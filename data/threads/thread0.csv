"Humans generally use natural language to communicate task requirements to each other. Ideally, natural language should also be usable for communicating goals to autonomous machines (e.g., robots) to minimize friction in task specification. However, understanding and mapping natural language goals to sequences of states and actions is challenging. Specifically, existing work along these lines has encountered difficulty in generalizing learned policies to new natural language goals and environments. In this paper, we propose a novel adversarial inverse reinforcement learning algorithm to learn a language-conditioned policy and reward function. To improve generalization of the learned policy and reward function, we use a variational goal generator to relabel trajectories and sample diverse goals during training. Our algorithm outperforms multiple baselines by a large margin on a vision-based natural language instruction following dataset (Room-2-Room), demonstrating a promising advance in enabling the use of natural language instructions in specifying agent goals.",0
"One approach to achieving human-like intelligence in artificial agents is through inverse reinforcement learning (IRL). IRL allows us to infer the reward function that drives an agent from observing its behavior over time. However, traditional IRL formulations assume that rewards are numerical values rather than natural language goals. This can lead to difficulties when trying to capture complex real world tasks where high level objectives need to be specified using natural languages.  Our work proposes a new framework called ""Inverse Reinforcement Learning with Natural Language Goals"" which combines deep learning techniques with classical planning algorithms. Our model learns a mapping from observed trajectories to natural language descriptions of their intended goal states, enabling effective use of natural language goals. We demonstrate our method on several simulated robotic manipulation tasks, showing significant improvement over baseline approaches.  We believe that our work has important implications for the development of autonomous systems capable of interacting naturally with humans and accomplishing complex tasks. With further refinement, our approach could enable greater collaboration between humans and machines, allowing them to work together towards common objectives expressed in natural language.",1
"Most deep neural networks are considered to be black boxes, meaning their output is hard to interpret. In contrast, logical expressions are considered to be more comprehensible since they use symbols that are semantically close to natural language instead of distributed representations. However, for high-dimensional input data such as images, the individual symbols, i.e. pixels, are not easily interpretable. We introduce the concept of first-order convolutional rules, which are logical rules that can be extracted using a convolutional neural network (CNN), and whose complexity depends on the size of the convolutional filter and not on the dimensionality of the input. Our approach is based on rule extraction from binary neural networks with stochastic local search. We show how to extract rules that are not necessarily short, but characteristic of the input, and easy to visualize. Our experiments show that the proposed approach is able to model the functionality of the neural network while at the same time producing interpretable logical rules.",0
"Title: Binary Neural Networks and convolutional rules applied to models for improved validation. Summary/Abstract: In recent years, deep learning has been used successfully in many applications such as image classification, natural language processing, robotics, and more. However, these powerful models have limitations because they can be difficult to interpret, debug or validate. One popular method of improving model transparency involves rule extraction techniques that extract explainable decision boundaries from complex systems such as neural networks. This paper focuses on rule extraction methods for binary neural networks using convolutional rules which have shown promising results in previous work. Using this approach, we propose new methodologies to validate binary neural network models by analyzing their corresponding extracted sets of rules. We evaluate our proposed approaches through several case studies covering diverse domains. Our experimental analysis demonstrates that utilizing convolutional rules leads to high quality decision boundaries that are both compact and comprehensible for interpreting neural network behaviors. Additionally, we observe significant improvements in accuracy over prior rule extraction algorithms while achieving scalability to large datasets. Overall, we present novel contributions towards enhancing trustworthiness of neural networks in critical applications where faith in automation decisions is crucial. Keywords: Deep Learning, Rule Extraction, Explainable Artificial Intelligence (XAI), Binary Classifiers, Convolutional Rules",1
"Deep neural networks have found widespread adoption in solving complex tasks ranging from image recognition to natural language processing. However, these networks make confident mispredictions when presented with data that does not belong to the training distribution, i.e. out-of-distribution (OoD) samples. In this paper we explore whether the property of Vicinal Risk Minimization (VRM) to smoothly interpolate between different class boundaries helps to train better OoD detectors. We apply VRM to existing OoD detection techniques and show their improved performance. We observe that existing OoD detectors have significant memory and compute overhead, hence we leverage VRM to develop an OoD detector with minimal overheard. Our detection method introduces an auxiliary class for classifying OoD samples. We utilize mixup in two ways to implement Vicinal Risk Minimization. First, we perform mixup within the same class and second, we perform mixup with Gaussian noise when training the auxiliary class. Our method achieves near competitive performance with significantly less compute and memory overhead when compared to existing OoD detection techniques. This facilitates the deployment of OoD detection on edge devices and expands our understanding of Vicinal Risk Minimization for use in training OoD detectors.",0
"In recent years, detecting out-of-distribution (OOD) samples has become increasingly important for ensuring the safety and reliability of machine learning models. Many existing OOD detection methods rely on using dense models pretrained on large datasets to extract high-dimensional features that capture complex patterns in the data. However, these approaches can be computationally expensive and may require extensive fine-tuning which limits their applicability to small-scale devices and low-resource settings. To address this issue, we propose vicinal risk minimization for lightweight OOD detection. Our method leverages a simple yet effective generative model trained using only a few parameters. We demonstrate through experiments on several benchmark datasets that our approach achieves state-of-the-art performance while requiring significantly fewer computational resources compared to previous methods. Furthermore, we provide insight into why and how our method works by analyzing the relationship between the complexity of the generative model and OOD detection accuracy. Overall, our work advances the field of OOD detection by showing that it is possible to achieve strong results without relying on complex deep learning algorithms.",1
"We present *-CFQ (""star-CFQ""): a suite of large-scale datasets of varying scope based on the CFQ semantic parsing benchmark, designed for principled investigation of the scalability of machine learning systems in a realistic compositional task setting. Using this suite, we conduct a series of experiments investigating the ability of Transformers to benefit from increased training size under conditions of fixed computational cost. We show that compositional generalization remains a challenge at all training sizes, and we show that increasing the scope of natural language leads to consistently higher error rates, which are only partially offset by increased training data. We further show that while additional training data from a related domain improves the accuracy in data-starved situations, this improvement is limited and diminishes as the distance from the related domain to the target domain increases.",0
"In recent years, machine learning has proven to be effective at solving complex problems that were once thought impossible for computers. However, one area where machine learning still struggles is in compositional tasks, which require an understanding of how different components fit together to form larger structures. This study examines the scalability of machine learning models on these types of tasks using a specific dataset known as Composition Flow Queries (CFQ). We investigate whether current methods are able to generalize beyond their training data and if they can handle increasingly difficult examples without breaking down. Our results demonstrate that while some models show promise, there remains significant room for improvement in terms of performance and scalability. These findings have implications for both researchers working on developing new techniques for handling compositional tasks and practitioners who need to choose appropriate models for real-world applications.",1
"The development of Information Technology has been increasingly changing the means of information exchange leading to the need of digitizing print documents. In the present era, there is a lot of fraud that often occur. To avoid account fraud there was verification using ID card extraction using OCR and NLP. Optical Character Recognition (OCR) is technology that used to generate text from image. With OCR we can extract Indonesian ID card or kartu tanda penduduk (KTP) into text too. This is using to make easier service operator to do data entry. To improve the accuracy we made text correction using Natural language Processing (NLP) method to fixing the text. With 50 Indonesian ID card image we got 0.78 F-score, and we need 4510 milliseconds to extract per ID card.",0
"This paper presents a method for extracting data from Indonesian identification cards using optical character recognition (OCR) and natural language processing techniques. To improve accuracy, we use both text detection and OCR to identify all the visible texts on an ID card image. We then apply post-processing steps such as non-alphabetic character removal, tokenization, named entity recognition, and normalization. Our approach outperforms previous methods in terms of precision and recall, demonstrating its effectiveness in handling real world ID card images with complex backgrounds and varied lighting conditions. Additionally, our system is able to automatically detect multiple languages used in Indonesia which makes it more robust to different types of identification documents within the country.",1
"Generating queries corresponding to natural language questions is a long standing problem. Traditional methods lack language flexibility, while newer sequence-to-sequence models require large amount of data. Schema-agnostic sequence-to-sequence models can be fine-tuned for a specific schema using a small dataset but these models have relatively low accuracy. We present a method that transforms the query generation problem into an intent classification and slot filling problem. This method can work using small datasets. For questions similar to the ones in the training dataset, it produces complex queries with high accuracy. For other questions, it can use a template-based approach or predict query pieces to construct the queries, still at a higher accuracy than sequence-to-sequence models. On a real-world dataset, a schema fine-tuned state-of-the-art generative model had 60\% exact match accuracy for the query generation task, while our method resulted in 92\% exact match accuracy.",0
"This research proposes a novel approach to generating complex database queries and API calls directly from natural language utterances using deep learning techniques. Current methods require manual engineering and domain-specific knowledge to create these mappings, which can result in limited coverage and error-prone systems. Our system takes advantage of recent advancements in large pre-trained transformer models to perform multi-step reasoning on natural text inputs without relying on handcrafted features. We evaluate our model on a variety of real-world use cases and show significant improvements over existing state-of-the art methods in terms of both accuracy and efficiency. Overall, we demonstrate that automatic generation of complex SQL/API calls has broad applications in natural language processing and human computer interaction.",1
"Neural sequence-to-sequence models are finding increasing use in editing of documents, for example in correcting a text document or repairing source code. In this paper, we argue that common seq2seq models (with a facility to copy single tokens) are not a natural fit for such tasks, as they have to explicitly copy each unchanged token. We present an extension of seq2seq models capable of copying entire spans of the input to the output in one step, greatly reducing the number of decisions required during inference. This extension means that there are now many ways of generating the same output, which we handle by deriving a new objective for training and a variation of beam search for inference that explicitly handles this problem. In our experiments on a range of editing tasks of natural language and source code, we show that our new model consistently outperforms simpler baselines.",0
Our approach makes use of an iterative and incremental procedure inspired on human copy editing and proof reading activities,1
"Automatic generation of video descriptions in natural language, also called video captioning, aims to understand the visual content of the video and produce a natural language sentence depicting the objects and actions in the scene. This challenging integrated vision and language problem, however, has been predominantly addressed for English. The lack of data and the linguistic properties of other languages limit the success of existing approaches for such languages. In this paper we target Turkish, a morphologically rich and agglutinative language that has very different properties compared to English. To do so, we create the first large scale video captioning dataset for this language by carefully translating the English descriptions of the videos in the MSVD (Microsoft Research Video Description Corpus) dataset into Turkish. In addition to enabling research in video captioning in Turkish, the parallel English-Turkish descriptions also enables the study of the role of video context in (multimodal) machine translation. In our experiments, we build models for both video captioning and multimodal machine translation and investigate the effect of different word segmentation approaches and different neural architectures to better address the properties of Turkish. We hope that the MSVD-Turkish dataset and the results reported in this work will lead to better video captioning and multimodal machine translation models for Turkish and other morphology rich and agglutinative languages.",0
"This paper presents a new dataset called MSVD-Turkish which is specifically designed for integrated vision and language research on Turkish data. The dataset consists of over 82,000 images from real-world scenes along with their corresponding captions written in both English and Turkish languages. The dataset includes annotations such as image tags, object detection bounding boxes, part-of-speech tags and dependency parse trees for Turkish sentences. Additionally, the dataset has been split into train, validation and test sets following standard splits used by other datasets. MSVD-Turkish is the first comprehensive multimodal dataset that provides support for vision and language tasks in Turkish. We hope that this dataset will pave the way for future work in natural language processing, computer vision and multilingual computing in Turkish.",1
"Despite being the workhorse of deep learning, the backpropagation algorithm is no panacea. It enforces sequential layer updates, thus preventing efficient parallelization of the training process. Furthermore, its biological plausibility is being challenged. Alternative schemes have been devised; yet, under the constraint of synaptic asymmetry, none have scaled to modern deep learning tasks and architectures. Here, we challenge this perspective, and study the applicability of Direct Feedback Alignment to neural view synthesis, recommender systems, geometric learning, and natural language processing. In contrast with previous studies limited to computer vision tasks, our findings show that it successfully trains a large range of state-of-the-art deep learning architectures, with performance close to fine-tuned backpropagation. At variance with common beliefs, our work supports that challenging tasks can be tackled in the absence of weight transport.",0
"Abstract: Despite the tremendous success of deep learning methods on many challenging tasks, there remains much work to be done if we want these models to align their objectives more closely with those of human users. One direction towards achieving this goal has been through ""direct feedback alignment"" (DFA), wherein feedback from real humans directly guides model behavior toward better performance. This process can scale up to modern architectures such as GPT-4, and can even generalize across unseen datasets at test time. Furthermore, integrating DFA into training provides numerous benefits over standalone fine-tuning that require no changes to the underlying model architecture. By combining both data augmentation via contrastive pretraining and direct user guidance during deployment, state of the art systems like GPT-4 are made even more effective while preserving interpretability and trustworthiness. Overall, successful use cases have emerged in domains as diverse as code generation, machine translation, language instruction, question answering, image generation, zero shot few-shot learning, text classification, summarization, recommendation, and more. These developments open exciting new possibilities for the next decade of applied research involving artificial intelligence with natural language understanding and generation abilities.  Here's my interpretation for this scientific paper: This study focuses on addressing a fundamental challenge facing current artificial intelligence technology -- ensuring their objectives better align with those of human users. To achieve this, the authors propose ""Direct Feedback Alignment,"" which involves using real human feedback to guide model behavior for improved accuracy. They demonstrate how this approach scales well for complex AI tasks, including large language models such as GPT-4. Their method significantly outperforms traditional approaches t",1
"Natural Language Search (NLS) extends the capabilities of search engines that perform keyword search allowing users to issue queries in a more ""natural"" language. The engine tries to understand the meaning of the queries and to map the query words to the symbols it supports like Persons, Organizations, Time Expressions etc.. It, then, retrieves the information that satisfies the user's need in different forms like an answer, a record or a list of records. We present an NLS system we implemented as part of the Search service of a major CRM platform. The system is currently in production serving thousands of customers. Our user studies showed that creating dynamic reports with NLS saved more than 50% of our user's time compared to achieving the same result with navigational search. We describe the architecture of the system, the particularities of the CRM domain as well as how they have influenced our design decisions. Among several submodules of the system we detail the role of a Deep Learning Named Entity Recognizer. The paper concludes with discussion over the lessons learned while developing this product.",0
"Title: Query understanding for natural language enterprise search  Abstract: Natural language querying has become increasingly important for modern enterprises due to the growing volume of data available and the need to find relevant information quickly and accurately. However, traditional keyword-based approaches often fail to capture the nuances of human language, resulting in poor retrieval performance and user dissatisfaction. In order to address these issues, we propose a novel approach that utilizes deep learning techniques to understand natural language queries and provide more accurate results. Our method leverages pre-trained transformer models to encode both queries and documents into high-dimensional vectors, allowing us to capture semantic relationships and contextual meaning. We then apply a bilateral matching algorithm to measure the similarity between queries and documents, which allows us to rank results according to relevance. Experiments on three large-scale datasets show that our method significantly outperforms state-of-the-art baselines in terms of mean average precision and other evaluation metrics, demonstrating the effectiveness of our approach. This work represents an important step towards improving the quality of natural language enterprise search and enabling users to access valuable information more efficiently.",1
"We propose Attention Grounder (AttnGrounder), a single-stage end-to-end trainable model for the task of visual grounding. Visual grounding aims to localize a specific object in an image based on a given natural language text query. Unlike previous methods that use the same text representation for every image region, we use a visual-text attention module that relates each word in the given query with every region in the corresponding image for constructing a region dependent text representation. Furthermore, for improving the localization ability of our model, we use our visual-text attention module to generate an attention mask around the referred object. The attention mask is trained as an auxiliary task using a rectangular mask generated with the provided ground-truth coordinates. We evaluate AttnGrounder on the Talk2Car dataset and show an improvement of 3.26% over the existing methods.",0
"Humans rely on their cars to transport them from one place to another every day. However, despite advances in automotive technology, there remains limited interaction between vehicles and pedestrians. In our paper, we propose a novel approach called ""AttnGrounder"" that enables real-time attention grounding between cars and pedestrians through natural language processing (NLP). Our system extracts and ranks relevant regions in images of approaching cars using Convolutional Neural Networks (CNN), allowing vehicles to focus their responses to specific queries made by pedestrians. This allows for faster and more accurate communication between humans and autonomous vehicles. Additionally, AttnGrounder provides a better understanding of human intentions, improving the overall safety and efficiency of road interactions. We evaluate our proposed method on a large dataset consisting of car-pedestrian scenarios and demonstrate significant improvements over baseline approaches. Our work bridges the gap between NLP and computer vision and has important implications for future generations of autonomous vehicles. Overall, AttnGrounder represents a crucial step towards safer and smarter urban environments where humans and machines can interact seamlessly.",1
"Deep neural network models represent the state-of-the-art methodologies for natural language processing. Here we build on top of these methodologies to incorporate temporal information and model how to review data changes with time. Specifically, we use the dynamic representations of recurrent point process models, which encode the history of how business or service reviews are received in time, to generate instantaneous language models with improved prediction capabilities. Simultaneously, our methodologies enhance the predictive power of our point process models by incorporating summarized review content representations. We provide recurrent network and temporal convolution solutions for modeling the review content. We deploy our methodologies in the context of recommender systems, effectively characterizing the change in preference and taste of users as time evolves. Source code is available at [1].",0
"""Recurrent point review models (RPRMs) are machine learning algorithms that allow researchers to analyze text data by breaking down complex documents into individual points and their corresponding reviews. These models have been found to be highly effective at identifying patterns and trends across large datasets, making them particularly useful in fields such as sentiment analysis and topic modeling. In this study, we present a detailed overview of RPRMs and explore their applications in several domains. We also discuss limitations and potential future directions for improving these powerful tools.""",1
"Large-scale network embedding is to learn a latent representation for each node in an unsupervised manner, which captures inherent properties and structural information of the underlying graph. In this field, many popular approaches are influenced by the skip-gram model from natural language processing. Most of them use a contrastive objective to train an encoder which forces the embeddings of similar pairs to be close and embeddings of negative samples to be far. A key of success to such contrastive learning methods is how to draw positive and negative samples. While negative samples that are generated by straightforward random sampling are often satisfying, methods for drawing positive examples remains a hot topic.   In this paper, we propose SCE for unsupervised network embedding only using negative samples for training. Our method is based on a new contrastive objective inspired by the well-known sparsest cut problem. To solve the underlying optimization problem, we introduce a Laplacian smoothing trick, which uses graph convolutional operators as low-pass filters for smoothing node representations. The resulting model consists of a GCN-type structure as the encoder and a simple loss function. Notably, our model does not use positive samples but only negative samples for training, which not only makes the implementation and tuning much easier, but also reduces the training time significantly.   Finally, extensive experimental studies on real world data sets are conducted. The results clearly demonstrate the advantages of our new model in both accuracy and scalability compared to strong baselines such as GraphSAGE, G2G and DGI.",0
"This paper presents a novel method for network embedding called ""Sparse Cut (SCE)"". The approach relies on solving a linear program which finds a sparsity preserving cut over each layer separately. We provide results that show the advantages of our model in terms of accuracy and interpretability on several data sets of varying sizes. We also compare against state of art methods showing competitive performance. Finally we look at visualizations of embeddings to aid qualitative analysis and confirm findings. Code available upon request.",1
"In order to build efficient deep recurrent neural architectures, it is essential to analyze the complexityof long distance dependencies (LDDs) of the dataset being modeled. In this paper, we presentdetailed analysis of the dependency decay curve exhibited by various datasets. The datasets sampledfrom a similar process (e.g. natural language, sequential MNIST, Strictlyk-Piecewise languages,etc) display variations in the properties of the dependency decay curve. Our analysis reveal thefactors resulting in these variations; such as (i) number of unique symbols in a dataset, (ii) size ofthe dataset, (iii) number of interacting symbols within a given LDD, and (iv) the distance betweenthe interacting symbols. We test these factors by generating synthesized datasets of the Strictlyk-Piecewise languages. Another advantage of these synthesized datasets is that they enable targetedtesting of deep recurrent neural architectures in terms of their ability to model LDDs with differentcharacteristics. We also demonstrate that analysing dependency decay curves can inform the selectionof optimal hyper-parameters for SOTA deep recurrent neural architectures. This analysis can directlycontribute to the development of more accurate and efficient sequential models.",0
"This study aimed to gain a better understanding of recurrent neural architectures (RNAs) through analyzing and synthesizing long distance dependencies (LDDs) in benchmark sequential datasets. RNAs have been found to perform well on tasks that involve processing sequences of data, such as natural language processing, speech recognition, and time series prediction. However, the mechanisms underlying their success remain unclear, particularly in relation to LDDs. To address this gap, we conducted three experiments using established benchmark datasets: sentiment analysis on movie reviews from IMDb; part-of-speech tagging for English sentences from the Penn Treebank dataset; and character-level language modeling for text corpora consisting of books from Project Gutenberg. Our first experiment sought to determine whether different RNA models were able to capture LDDs in these benchmark datasets. We then performed ablation studies to analyze which components of each architecture contributed most significantly to strong performance on sequence data. Finally, we explored the effectiveness of attention mechanisms in RNAs, as they have recently become popular tools for dealing with input sequences. Results showed a consistent advantage for specific types of RNAs across all three tasks, suggesting commonalities in the nature of long distance dependencies among diverse linguistic domains. These findings contribute to our overall comprehension of why some RNA designs excel at handling LDDs while others fail. By expanding knowledge of how RNAs process complex sequential data, our research can aid future developments in natural language processing and other related areas.",1
"Millions of unsolicited medical inquiries are received by pharmaceutical companies every year. It has been hypothesized that these inquiries represent a treasure trove of information, potentially giving insight into matters regarding medicinal products and the associated medical treatments. However, due to the large volume and specialized nature of the inquiries, it is difficult to perform timely, recurrent, and comprehensive analyses. Here, we propose a machine learning approach based on natural language processing and unsupervised learning to automatically discover key topics in real-world medical inquiries from customers. This approach does not require ontologies nor annotations. The discovered topics are meaningful and medically relevant, as judged by medical information specialists, thus demonstrating that unsolicited medical inquiries are a source of valuable customer insights. Our work paves the way for the machine-learning-driven analysis of medical inquiries in the pharmaceutical industry, which ultimately aims at improving patient care.",0
"Medical professionals are often bombarded with numerous questions from patients, their families, or other colleagues, which can lead to information overload. In our work, we aimed to address this challenge by developing a system that utilizes natural language processing (NLP) techniques to identify key topics discussed in short, real-world medical queries. To achieve this goal, we employed unsupervised machine learning algorithms that did not require labeled data. Our methodology comprised preprocessing and tokenization steps followed by topic modeling based on Latent Dirichlet Allocation (LDA). We evaluated the effectiveness of our approach using several metrics such as precision, recall, F1 score, and receiver operating characteristic curve analysis. Results showed high accuracy levels across all performance measures, demonstrating the potential of our system for helping healthcare providers manage and prioritize incoming medical inquiries more effectively. Overall, our study presents a promising direction for advancing NLP applications in healthcare communication management through unsupervised learning methods.",1
"As deep learning models become tasked with more and more decisions that impact human lives, such as criminal recidivism, loan repayment, and face recognition for law enforcement, bias is becoming a growing concern. Debiasing algorithms are typically split into three paradigms: pre-processing, in-processing, and post-processing. However, in computer vision or natural language applications, it is common to start with a large generic model and then fine-tune to a specific use-case. Pre- or in-processing methods would require retraining the entire model from scratch, while post-processing methods only have black-box access to the model, so they do not leverage the weights of the trained model. Creating debiasing algorithms specifically for this fine-tuning use-case has largely been neglected.   In this work, we initiate the study of a new paradigm in debiasing research, intra-processing, which sits between in-processing and post-processing methods. Intra-processing methods are designed specifically to debias large models which have been trained on a generic dataset and fine-tuned on a more specific task. We show how to repurpose existing in-processing methods for this use-case, and we also propose three baseline algorithms: random perturbation, layerwise optimization, and adversarial fine-tuning. All of our techniques can be used for all popular group fairness measures such as equalized odds or statistical parity difference. We evaluate these methods across three popular datasets from the AIF360 toolkit, as well as on the CelebA faces dataset. Our code is available at https://github.com/abacusai/intraprocessing_debiasing.",0
"As neural networks continue to gain popularity and influence in our daily lives, there has been increasing concern over their susceptibility to biases and errors. These problems can arise from many sources, including data collection techniques, model architecture, and training methods. To address these issues, researchers have developed a range of debiasing techniques that aim to identify and mitigate unwanted behavior patterns. While most previous efforts have focused on intervention strategies that operate at different stages of the learning process, such as preprocessing or post-processing steps, this study proposes a novel intra-processing approach that works within the network itself. This method examines the internal computations performed by individual neurons during inference time and modifies them accordingly. By targeting specific parts of the decision-making pipeline, we show how to significantly reduce bias while maintaining high levels of accuracy and efficiency. Our findings highlight the effectiveness of this new class of techniques and provide valuable insights into future directions for enhancing the reliability and robustness of machine learning systems.",1
"Referring expression comprehension (REC) aims to localize a target object in an image described by a referring expression phrased in natural language. Different from the object detection task that queried object labels have been pre-defined, the REC problem only can observe the queries during the test. It thus more challenging than a conventional computer vision problem. This task has attracted a lot of attention from both computer vision and natural language processing community, and several lines of work have been proposed, from CNN-RNN model, modular network to complex graph-based model. In this survey, we first examine the state of the art by comparing modern approaches to the problem. We classify methods by their mechanism to encode the visual and textual modalities. In particular, we examine the common approach of joint embedding images and expressions to a common feature space. We also discuss modular architectures and graph-based models that interface with structured graph representation. In the second part of this survey, we review the datasets available for training and evaluating REC systems. We then group results according to the datasets, backbone models, settings so that they can be fairly compared. Finally, we discuss promising future directions for the field, in particular the compositional referring expression comprehension that requires longer reasoning chain to address.",0
"Understood. Here’s a possible abstract for your paper on referring expression comprehension:  Referring expressions like pronouns, proper names, demonstratives, and quantifiers play crucial roles in human communication by enabling efficient reference to objects and events within a discourse context. However, modeling their interpretation has proven challenging due to diverse ambiguities arising from syntax, semantics, pragmatics, and world knowledge. As natural language processing models have advanced toward more ambitious applications like question answering, machine translation, and conversational AI, automating referent identification and disambiguation has become increasingly important. In this survey, we review current methods tackling referring expression comprehension problems involving noun phrases (NP), which constitute the majority of references in text. We organize these approaches into three major frameworks based on the key component they prioritize for solving the task—syntax, semantics, or contextual inference. For each framework, we describe popular algorithms, discuss benchmark datasets, compare state-of-the-art results, and highlight tradeoffs between performance and resource demands. By analyzing these methodological trends, promising research directions, and future challenges emerge that may further guide the development of high-performing and explainable referring expression comprehenders.",1
"Deep neural networks (DNNs) have been extremely successful in solving many challenging AI tasks in natural language processing, speech recognition, and computer vision nowadays. However, DNNs are typically computation intensive, memory demanding, and power hungry, which significantly limits their usage on platforms with constrained resources. Therefore, a variety of compression techniques (e.g. quantization, pruning, and knowledge distillation) have been proposed to reduce the size and power consumption of DNNs. Blockwise knowledge distillation is one of the compression techniques that can effectively reduce the size of a highly complex DNN. However, it is not widely adopted due to its long training time. In this paper, we propose a novel parallel blockwise distillation algorithm to accelerate the distillation process of sophisticated DNNs. Our algorithm leverages local information to conduct independent blockwise distillation, utilizes depthwise separable layers as the efficient replacement block architecture, and properly addresses limiting factors (e.g. dependency, synchronization, and load balancing) that affect parallelism. The experimental results running on an AMD server with four Geforce RTX 2080Ti GPUs show that our algorithm can achieve 3x speedup plus 19% energy savings on VGG distillation, and 3.5x speedup plus 29% energy savings on ResNet distillation, both with negligible accuracy loss. The speedup of ResNet distillation can be further improved to 3.87 when using four RTX6000 GPUs in a distributed cluster.",0
"Effective use of parallel processing techniques has become increasingly important as deep neural network models continue to grow larger and more complex. In this work, we propose a new method of knowledge distillation that leverages block-based partitioning to efficiently transfer knowledge from large teacher models to smaller student models using GPU acceleration. Our approach utilizes multiple parallel instances of the same model architecture, each trained on disjoint subsets of the data, allowing us to achieve efficient parallelization without sacrificing the fidelity of the transferred knowledge. Extensive experiments demonstrate the effectiveness of our method at compressing VGG-like and ResNet-like architectures while maintaining high accuracy, outperforming state-of-the-art knowledge distillation methods under similar conditions. Overall, our results suggest that parallel blockwise knowledge distillation represents a promising direction towards developing scalable and performant machine learning systems capable of handling big data problems in real-world settings.",1
"Visual captioning aims to generate textual descriptions given images or videos. Traditionally, image captioning models are trained on human annotated datasets such as Flickr30k and MS-COCO, which are limited in size and diversity. This limitation hinders the generalization capabilities of these models while also rendering them liable to making mistakes. Language models can, however, be trained on vast amounts of freely available unlabelled data and have recently emerged as successful language encoders and coherent text generators. Meanwhile, several unimodal and multimodal fusion techniques have been proven to work well for natural language generation and automatic speech recognition. Building on these recent developments, and with the aim of improving the quality of generated captions, the contribution of our work in this paper is two-fold: First, we propose a generic multimodal model fusion framework for caption generation as well as emendation where we utilize different fusion strategies to integrate a pretrained Auxiliary Language Model (AuxLM) within the traditional encoder-decoder visual captioning frameworks. Next, we employ the same fusion strategies to integrate a pretrained Masked Language Model (MLM), namely BERT, with a visual captioning model, viz. Show, Attend, and Tell, for emending both syntactic and semantic errors in captions. Our caption emendation experiments on three benchmark image captioning datasets, viz. Flickr8k, Flickr30k, and MSCOCO, show improvements over the baseline, indicating the usefulness of our proposed multimodal fusion strategies. Further, we perform a preliminary qualitative analysis on the emended captions and identify error categories based on the type of corrections.",0
"Artificial intelligence has made significant strides in recent years in various fields including computer vision, natural language processing, and machine learning. One area where these technologies can be used effectively is in visual captioning, which involves generating human-like descriptions of images using artificial neural networks (ANNs). In this study, we explore the use of fusion models that combine different types of inputs to improve the quality of generated image descriptions. We investigate two approaches: fusing pre-trained ANNs that have been trained on large datasets with fine-grained annotations, and integrating external knowledge sources such as scene graphs, object detections, and semantic representations into the model architecture. Our results show that both methods significantly enhance the performance of our fusion models compared to state-of-the-art systems. Furthermore, we analyze the impact of each contribution on the overall performance, demonstrating that integration of diverse contextual information improves the coherence and informativeness of the generated descriptions. Our work presents new insights into developing more advanced ANN architectures tailored for complex tasks like visual captioning by exploiting multimodal data resources.",1
"We address the problem of retrieving a specific moment from an untrimmed video by natural language. It is a challenging problem because a target moment may take place in the context of other temporal moments in the untrimmed video. Existing methods cannot tackle this challenge well since they do not fully consider the temporal contexts between temporal moments. In this paper, we model the temporal context between video moments by a set of predefined two-dimensional maps under different temporal scales. For each map, one dimension indicates the starting time of a moment and the other indicates the duration. These 2D temporal maps can cover diverse video moments with different lengths, while representing their adjacent contexts at different temporal scales. Based on the 2D temporal maps, we propose a Multi-Scale Temporal Adjacent Network (MS-2D-TAN), a single-shot framework for moment localization. It is capable of encoding the adjacent temporal contexts at each scale, while learning discriminative features for matching video moments with referring expressions. We evaluate the proposed MS-2D-TAN on three challenging benchmarks, i.e., Charades-STA, ActivityNet Captions, and TACoS, where our MS-2D-TAN outperforms the state of the art.",0
"This paper presents a novel approach for moment localization using natural language processing techniques. We introduce the concept of multi-scale 2D temporal adjacent networks (MANET), which integrate multiple levels of spatiotemporal context into a single framework. Our method combines feature extraction at different scales with a self-attention mechanism that captures global dependencies within the sequence. Experimental results on two benchmark datasets demonstrate that our model outperforms state-of-the-art methods by significant margins. Our analysis shows that this improvement comes from incorporating both spatial and temporal dependencies, as well as effectively handling noisy data and ambiguous annotations. Overall, we believe this work represents an important step towards more robust and accurate moment detection systems.",1
"We introduce the task of dense captioning in 3D scans from commodity RGB-D sensors. As input, we assume a point cloud of a 3D scene; the expected output is the bounding boxes along with the descriptions for the underlying objects. To address the 3D object detection and description problems, we propose Scan2Cap, an end-to-end trained method, to detect objects in the input scene and describe them in natural language. We use an attention mechanism that generates descriptive tokens while referring to the related components in the local context. To reflect object relations (i.e. relative spatial relations) in the generated captions, we use a message passing graph module to facilitate learning object relation features. Our method can effectively localize and describe 3D objects in scenes from the ScanRefer dataset, outperforming 2D baseline methods by a significant margin (27.61% CiDEr@0.5IoUimprovement).",0
"This should summarize the main contributions, methodology, results, and implications of your research in the provided space constraints (150-300 words).",1
"Extreme classification tasks are multi-label tasks with an extremely large number of labels (tags). These tasks are hard because the label space is usually (i) very large, e.g. thousands or millions of labels, (ii) very sparse, i.e. very few labels apply to each input document, and (iii) highly correlated, meaning that the existence of one label changes the likelihood of predicting all other labels. In this work, we propose a self-attention based variational encoder-model to extract the label-label and label-feature dependencies jointly and to predict labels for a given input. In more detail, we propose a non-autoregressive latent variable model and compare it to a strong autoregressive baseline that predicts a label based on all previously generated labels. Our model can therefore be used to predict all labels in parallel while still including both label-label and label-feature dependencies through latent variables, and compares favourably to the autoregressive baseline. We apply our models to four standard extreme classification natural language data sets, and one news videos dataset for automated label detection from a lexicon of semantic concepts. Experimental results show that although the autoregressive models, where use a given order of the labels for chain-order label prediction, work great for the small scale labels or the prediction of the highly ranked label, but our non-autoregressive model surpasses them by around 2% to 6% when we need to predict more labels, or the dataset has a larger number of the labels.",0
"In recent years, multi-label learning has become an increasingly important area of study as more data becomes available online. This field involves predicting multiple outputs (labels) at once, which can be particularly challenging due to label correlation and imbalance issues. One popular approach is autoregressive modeling, where each output prediction depends only on local information from nearby examples. On the other hand, non-autoregressive models aim to predict all outputs simultaneously using global information across the entire dataset. In this study, we compare and analyze the performance of both types of approaches on several benchmark datasets. Our results indicate that while autoregressive methods often perform better overall, certain situations may call for a non-autoregressive approach. We further investigate how varying hyperparameters and algorithmic components affect model performance. Finally, we discuss potential directions for future research in the field of multi-label learning.",1
"Recent advances in neural forecasting have produced major improvements in accuracy for probabilistic demand prediction. In this work, we propose novel improvements to the current state of the art by incorporating changes inspired by recent advances in Transformer architectures for Natural Language Processing. We develop a novel decoder-encoder attention for context-alignment, improving forecasting accuracy by allowing the network to study its own history based on the context for which it is producing a forecast. We also present a novel positional encoding that allows the neural network to learn context-dependent seasonality functions as well as arbitrary holiday distances. Finally we show that the current state of the art MQ-Forecaster (Wen et al., 2017) models display excess variability by failing to leverage previous errors in the forecast to improve accuracy. We propose a novel decoder-self attention scheme for forecasting that produces significant improvements in the excess variation of the forecast.",0
"""In MQTransformer we present a new deep learning architecture that can perform multi-horizon forecasting tasks with context dependence and feedback awareness."" (abstract) This sounds like it could use some improvement, here are my suggestions: ""Forecasting future outcomes is a challenging task, one that often requires taking into account past data as well as current conditions. In our work, we introduce MQTransformer, a novel deep learning architecture designed for performing multi-horizon forecasts while leveraging both contextual dependencies and feedback information. Our approach allows for greater accuracy by enabling the model to adaptively weight different parts of input sequences based on their relevance and temporal relationships.""",1
"Learning deep representations to solve complex machine learning tasks has become the prominent trend in the past few years. Indeed, Deep Neural Networks are now the golden standard in domains as various as computer vision, natural language processing or even playing combinatorial games. However, problematic limitations are hidden behind this surprising universal capability. Among other things, explainability of the decisions is a major concern, especially since deep neural networks are made up of a very large number of trainable parameters. Moreover, computational complexity can quickly become a problem, especially in contexts constrained by real time or limited resources. Therefore, understanding how information is stored and the impact this storage can have on the system remains a major and open issue. In this chapter, we introduce a method to transform deep neural network models into deep associative memories, with simpler, more explicable and less expensive operations. We show through experiments that these transformations can be done without penalty on predictive performance. The resulting deep associative memories are excellent candidates for artificial intelligence that is easier to theorize and manipulate.",0
"This paper presents the concept of ""DecisiveNets"" which can solve machine learning problems using deep associative memories. These networks use supervised training data consisting of inputs that have associated outputs provided by human experts. The authors show how these networks can achieve high accuracy on complex benchmark datasets by fine-tuning architectural hyperparameters. They also provide theoretical analysis showing why this approach works well. Finally, they demonstrate their technique on several real world applications including image classification, speech recognition, and playing games such as Go and Space Invaders. Overall, this work represents an exciting new direction for machine learning research.",1
"With the advancement of deep learning, artificial intelligence (AI) has made many breakthroughs in recent years and achieved superhuman performance in various tasks such as object detection, reading comprehension, and video games. Generative Modeling, such as various Generative Adversarial Networks (GAN) models, has been applied to generate paintings and music. Research in Natural Language Processing (NLP) also had a leap forward in 2018 since the release of the pre-trained contextual neural language models such as BERT and recently released GPT3. Despite the exciting AI applications aforementioned, AI is still significantly lagging behind humans in creativity, which is often considered the ultimate moonshot for AI. Our work is inspired by Chinese calligraphy, which is a unique form of visual art where the character itself is an aesthetic painting. We also draw inspirations from paintings of the Abstract Expressionist movement in the 1940s and 1950s, such as the work by American painter Franz Kline. In this paper, we present a creative framework based on Conditional Generative Adversarial Networks and Contextual Neural Language Model to generate abstract artworks that have intrinsic meaning and aesthetic value, which is different from the existing work, such as image captioning and text-to-image generation, where the texts are the descriptions of the images. In addition, we have publicly released a Chinese calligraphy image dataset and demonstrate our framework using a prototype system and a user study.",0
"This paper presents a novel framework called 'AbstractArtGen', which leverages transfer learning techniques and Generative Adversarial Networks (GAN) to generate abstract art. Utilizing calligraphic strokes as input allows for new variations on existing GAN architectures such as DALL-E and BigGAN. Experiments show that our approach can produce visually appealing abstract art. In addition, we release a dataset of abstract calligraphic images, which we believe could be used by researchers interested in advancing computer vision through creativity enhancement. Finally, future work would involve training more advanced models and investigating applications such as artist tools/toys, VR painting software, etc. We envision that our method has many possible real world applications for both artists and amateurs alike!",1
"Over the past few years, self-attention is shining in the field of deep learning, especially in the domain of natural language processing(NLP). Its impressive effectiveness, along with ubiquitous implementations, have aroused our interest in efficiently scheduling the data-flow of corresponding computations onto architectures with many computing units to realize parallel computing. In this paper, based on the theory of self-attention mechanism and state-of-the-art realization of self-attention in language models, we propose a general scheduling algorithm, which is derived from the optimum scheduling for small instances solved by a satisfiability checking(SAT) solver, to parallelize typical computations of self-attention. Strategies for further optimization on skipping redundant computations are put forward as well, with which reductions of almost 25% and 50% of the original computations are respectively achieved for two widely-adopted application schemes of self-attention. With the proposed optimization adopted, we have correspondingly come up with another two scheduling algorithms. The proposed algorithms are applicable regardless of problem sizes, as long as the number of input vectors is divisible to the number of computing units available in the architecture. Due to the complexity of proving the correctness of the algorithms mathematically for general cases, we have conducted experiments to reveal their validity, together with the superior quality of the solutions provided by which, by solving SAT problems for particular instances.",0
"In this work we present a novel parallel scheduling mechanism that significantly improves efficiency of state-of-the-art Transformer architectures. We demonstrate our method on real world datasets, providing significant speedups over serial implementations without any loss of accuracy in predictions. Our proposed approach generalizes well to arbitrary models, allowing practitioners to easily apply our technique to their existing codebase for large gains in inference time. Additionally, we provide insights into the inner workings of self-attention mechanisms through analysis and visualizations that further highlight the effectiveness of our approach. This study represents an important step towards making large language models more accessible for resource constrained environments.",1
"As one of the most well-known artificial feature sampler, the sliding window is widely used in scenarios where spatial and temporal information exists, such as computer vision, natural language process, data stream, and time series. Among which time series is common in many scenarios like credit card payment, user behavior, and sensors. General feature selection for features extracted by sliding window aggregate calls for time-consuming iteration to generate features, and then traditional feature selection methods are employed to rank them. The decision of key parameter, i.e. the period of sliding windows, depends on the domain knowledge and calls for trivial. Currently, there is no automatic method to handle the sliding window aggregate features selection. As the time consumption of feature generation with different periods and sliding windows is huge, it is very hard to enumerate them all and then select them.   In this paper, we propose a general framework using Markov Chain to solve this problem. This framework is very efficient and has high accuracy, such that it is able to perform feature selection on a variety of features and period options. We show the detail by 2 common sliding windows and 3 types of aggregation operators. And it is easy to extend more sliding windows and aggregation operators in this framework by employing existing theory about Markov Chain.",0
"This paper proposes a new approach for automatic feature selection for multi-period sliding window aggregate in time series analysis. Traditional methods for feature selection can be computationally expensive and require manual input from experts. Our method utilizes machine learning algorithms to automatically select relevant features for each aggregation period without requiring any prior knowledge. We evaluate our proposed method on several real world datasets and demonstrate that it outperforms existing methods while reducing computational complexity. Furthermore, we show that our method is capable of identifying meaningful patterns and trends in large scale data sets by accurately predicting future events based on historical data. Overall, our findings highlight the potential for automating complex time series analyses and unlocking insights into previously unknown phenomena.",1
"Machine learning has been developed dramatically and witnessed a lot of applications in various fields over the past few years. This boom originated in 2009, when a new model emerged, that is, the deep artificial neural network, which began to surpass other established mature models on some important benchmarks. Later, it was widely used in academia and industry. Ranging from image analysis to natural language processing, it fully exerted its magic and now become the state-of-the-art machine learning models. Deep neural networks have great potential in medical imaging technology, medical data analysis, medical diagnosis and other healthcare issues, and is promoted in both pre-clinical and even clinical stages. In this review, we performed an overview of some new developments and challenges in the application of machine learning to medical image analysis, with a special focus on deep learning in photoacoustic imaging. The aim of this review is threefold: (i) introducing deep learning with some important basics, (ii) reviewing recent works that apply deep learning in the entire ecological chain of photoacoustic imaging, from image reconstruction to disease diagnosis, (iii) providing some open source materials and other resources for researchers interested in applying deep learning to photoacoustic imaging.",0
"Photoacoustic imaging (PAI) has emerged as a promising noninvasive imaging modality that can combine optical contrast from endogenous molecules such as hemoglobin and lipids with ultrasonic resolution. Recently, deep learning techniques have been applied to PAI data analysis, leading to improved image quality, quantification accuracy, and speed. This survey provides a comprehensive overview of state-of-the-art applications of deep learning in PAI. After discussing background concepts and key challenges in PAI signal processing and reconstruction, we present various deep neural network architectures and strategies used for different tasks in PAI, including denoising, spectral unmixing, spatial resolution enhancement, scatter correction, temperature compensation, and whole-image regression for quantitative parameter mapping. We then review representative research works that apply these methods to a range of biomedical questions, such as breast cancer detection, vascular disease diagnosis, functional brain imaging, and small animal imaging. Finally, we highlight current limitations and future directions in using deep learning for PAI. Overall, our aim is to provide insights into the potential benefits of incorporating artificial intelligence into PAI research and clinical practice.",1
"Qualitative analysis of verbal data is of central importance in the learning sciences. It is labor-intensive and time-consuming, however, which limits the amount of data researchers can include in studies. This work is a step towards building a statistical machine learning (ML) method for achieving an automated support for qualitative analyses of students' writing, here specifically in score laboratory reports in introductory biology for sophistication of argumentation and reasoning. We start with a set of lab reports from an undergraduate biology course, scored by a four-level scheme that considers the complexity of argument structure, the scope of evidence, and the care and nuance of conclusions. Using this set of labeled data, we show that a popular natural language modeling processing pipeline, namely vector representation of words, a.k.a word embeddings, followed by Long Short Term Memory (LSTM) model for capturing language generation as a state-space model, is able to quantitatively capture the scoring, with a high Quadratic Weighted Kappa (QWK) prediction score, when trained in via a novel contrastive learning set-up. We show that the ML algorithm approached the inter-rater reliability of human analysis. Ultimately, we conclude, that machine learning (ML) for natural language processing (NLP) holds promise for assisting learning sciences researchers in conducting qualitative studies at much larger scales than is currently possible.",0
"We present a novel approach towards automatic coding of students' writing by training deep neural networks on high dimensional representations learned from text using contrastive representation learning methods based in the Wasserstein distance metric, thus allowing us to generate more accurate results than existing state of the art techniques. Our method leverages recent advances in unsupervised pretraining to learn effective initializations that speed up convergence and improve final performance. By evaluating our algorithm against several standard benchmarks, we demonstrate significant improvements across all measures including accuracy scores. Furthermore, we showcase how our framework can capture fine grained differences in language usage patterns between domains enabling applications such as discourse analysis. Our work serves as a step forward in bridging semantic gap by introducing new machine learning architectures capable of handling large scale problems while producing models with competitive computational overhead. Overall, these findings open possibilities in utilizing self supervision strategies for large language modeling tasks.",1
"The prosperity of computer vision (CV) and natural language procession (NLP) in recent years has spurred the development of deep learning in many other domains. The advancement in machine learning provides us with an alternative option besides the computationally expensive density functional theories (DFT). Kernel method and graph neural networks have been widely studied as two mainstream methods for property prediction. The promising graph neural networks have achieved comparable accuracy to the DFT method for specific objects in the recent study. However, most of the graph neural networks with high precision so far require fully connected graphs with pairwise distance distribution as edge information. In this work, we shed light on the Directed Graph Attention Neural Network (DGANN), which only takes chemical bonds as edges and operates on bonds and atoms of molecules. DGANN distinguishes from previous models with those features: (1) It learns the local chemical environment encoding by graph attention mechanism on chemical bonds. Every initial edge message only flows into every message passing trajectory once. (2) The transformer blocks aggregate the global molecular representation from the local atomic encoding. (3) The position vectors and coordinates are used as inputs instead of distances. Our model has matched or outperformed most baseline graph neural networks on QM9 datasets even without thorough hyper-parameters searching. Moreover, this work suggests that models directly utilizing 3D coordinates can still reach high accuracies for molecule representation even without rotational and translational invariance incorporated.",0
"This paper proposes a novel approach using directed graph attention neural networks (GAT) to predict molecular properties by incorporating three dimensional coordinates. We demonstrate that our method can outperform traditional GAT methods on several benchmark datasets including Tox21 and SIDER. Our results show that adding spatial information improves the accuracy of property prediction models. Additionally, we present analysis of how the model generates embeddings which further support our findings. By exploiting both chemical structure and geometric features, our proposed DAGNN achieves state of art performance across different types of datasets. The source code used in this research has been made publicly available on GitHub.",1
"The attention mechanism can refine the extracted feature maps and boost the classification performance of the deep network, which has become an essential technique in computer vision and natural language processing. However, the memory and computational costs of the dot-product attention mechanism increase quadratically with the spatio-temporal size of the input. Such growth hinders the usage of attention mechanisms considerably in application scenarios with large-scale inputs. In this Letter, we propose a Linear Attention Mechanism (LAM) to address this issue, which is approximately equivalent to dot-product attention with computational efficiency. Such a design makes the incorporation between attention mechanisms and deep networks much more flexible and versatile. Based on the proposed LAM, we re-factor the skip connections in the raw U-Net and design a Multi-stage Attention ResU-Net (MAResU-Net) for semantic segmentation from fine-resolution remote sensing images. Experiments conducted on the Vaihingen dataset demonstrated the effectiveness and efficiency of our MAResU-Net. Open-source code is available at https://github.com/lironui/Multistage-Attention-ResU-Net.",0
"This paper presents a novel deep learning architecture called Multi-Stage Attention Residual U-Net (ResU-Net) that significantly improves the accuracy of semantic segmentation for fine-resolution remote sensing images. Our model outperforms state-of-the-art models on several benchmark datasets by achieving higher pixel-wise accuracy and better visualization results. We propose a multi-scale attention mechanism that allows our network to adaptively focus on different scales during feature extraction, which further enhances the performance of the model. Additionally, we adopt a residual skip connection strategy to address the difficulty of training very deep networks while reducing computational cost. Comprehensive experiments demonstrate that the proposed method effectively exploits high resolution features from input images, leading to significant improvements in overall segmentation quality.",1
"For protein sequence datasets, unlabeled data has greatly outpaced labeled data due to the high cost of wet-lab characterization. Recent deep-learning approaches to protein prediction have shown that pre-training on unlabeled data can yield useful representations for downstream tasks. However, the optimal pre-training strategy remains an open question. Instead of strictly borrowing from natural language processing (NLP) in the form of masked or autoregressive language modeling, we introduce a new pre-training task: directly predicting protein profiles derived from multiple sequence alignments. Using a set of five, standardized downstream tasks for protein models, we demonstrate that our pre-training task along with a multi-task objective outperforms masked language modeling alone on all five tasks. Our results suggest that protein sequence models may benefit from leveraging biologically-inspired inductive biases that go beyond existing language modeling techniques in NLP.",0
"Abstract: In bioinformatics research, protein sequence models play an important role in analyzing biological data. However, due to the complexity of proteins, creating accurate models can be difficult. Here, we propose an alignment-based pre-training task called ""Profile Prediction"" that helps train these models more effectively by using already existing data. Our approach improves upon traditional techniques such as multiple sequence alignments (MSA) by taking advantage of modern deep learning methods. We evaluate our method on several benchmark datasets and demonstrate improved model accuracy compared to state-of-the-art methods. Our work has significant implications for the field of bioinformatics and could lead to more precise predictions for protein structures and functions.",1
"Video description involves the generation of the natural language description of actions, events, and objects in the video. There are various applications of video description by filling the gap between languages and vision for visually impaired people, generating automatic title suggestion based on content, browsing of the video based on the content and video-guided machine translation [86] etc.In the past decade, several works had been done in this field in terms of approaches/methods for video description, evaluation metrics,and datasets. For analyzing the progress in the video description task, a comprehensive survey is needed that covers all the phases of video description approaches with a special focus on recent deep learning approaches. In this work, we report a comprehensive survey on the phases of video description approaches, the dataset for video description, evaluation metrics, open competitions for motivating the research on the video description, open challenges in this field, and future research directions. In this survey, we cover the state-of-the-art approaches proposed for each and every dataset with their pros and cons. For the growth of this research domain,the availability of numerous benchmark dataset is a basic need. Further, we categorize all the dataset into two classes: open domain dataset and domain-specific dataset. From our survey, we observe that the work in this field is in fast-paced development since the task of video description falls in the intersection of computer vision and natural language processing. But still, the work in the video description is far from saturation stage due to various challenges like the redundancy due to similar frames which affect the quality of visual features, the availability of dataset containing more diverse content and availability of an effective evaluation metric.",0
"This could be used as a summary in your paper, or as text that can attract readers while submitting your work online. ABSTRACT: This paper provides a comprehensive review of recent methods and challenges associated with video description - a process that converts visual media content into natural language descriptions accessible by individuals who are visually impaired. By analyzing current techniques and approaches, we aimed to identify potential gaps and opportunities for future research. Our study revealed several promising directions in terms of technology development and research prioritization, such as advances in computer vision algorithms, machine learning models, crowdsourcing platforms, accessibility standards compliance, and user evaluation metrics. We conclude that continued progress requires collaboration among experts from diverse fields, including computer science, disability studies, psychology, education, sociology, and human-computer interaction. Future research should focus on creating personalized solutions tailored to individual needs and preferences, addressing social barriers, ensuring equal participation for underrepresented communities, and fostering sustainable partnerships across stakeholders. Ultimately, our goal is to contribute to building inclusive society where everyone has equal opportunity to enjoy multimedia content regardless of their abilities.",1
"While improvements in deep learning architectures have played a crucial role in improving the state of supervised and unsupervised learning in computer vision and natural language processing, neural network architecture choices for reinforcement learning remain relatively under-explored. We take inspiration from successful architectural choices in computer vision and generative modelling, and investigate the use of deeper networks and dense connections for reinforcement learning on a variety of simulated robotic learning benchmark environments. Our findings reveal that current methods benefit significantly from dense connections and deeper networks, across a suite of manipulation and locomotion tasks, for both proprioceptive and image-based observations. We hope that our results can serve as a strong baseline and further motivate future research into neural network architectures for reinforcement learning. The project website with code is at this link https://sites.google.com/view/d2rl/home.",0
"Recent advances in deep learning have shown that dense neural networks can achieve state-of-the art results on a wide range of tasks. However, these architectures are often computationally expensive during both training and inference which limits their use in real world applications where efficiency is critical. In this work, we propose the use of ""deep dense"" architectures as a solution to reduce computational cost while maintaining performance advantages over traditional shallow methods. These sparse but dense architectures balance the computational complexity against task accuracy, making them ideal for RL problems where speed of convergence is crucial. We evaluate our approach across several benchmark environments using OpenAI baselines and demonstrate that DDAs yield better or comparable average returns compared to other popular algorithms used by researchers today such as TD3, SAC, and PPO. Our analysis shows that sparsity and randomness play important roles in determining the success or failure of DDA agents within different environments. The future directions section highlights potential modifications which may further enhance performance along with ideas worth exploring beyond simple action selection from the current agent design. Overall, DDL provides an interesting alternative to existing methods for solving RL problems with promising performance gains and increased efficiency making it well suited for deployment in resource constrained devices/systems.",1
"Conversational interfaces for the detail-oriented retail fashion domain are more natural, expressive, and user friendly than classical keyword-based search interfaces. In this paper, we introduce the Fashion IQ dataset to support and advance research on interactive fashion image retrieval. Fashion IQ is the first fashion dataset to provide human-generated captions that distinguish similar pairs of garment images together with side-information consisting of real-world product descriptions and derived visual attribute labels for these images. We provide a detailed analysis of the characteristics of the Fashion IQ data, and present a transformer-based user simulator and interactive image retriever that can seamlessly integrate visual attributes with image features, user feedback, and dialog history, leading to improved performance over the state of the art in dialog-based image retrieval. We believe that our dataset will encourage further work on developing more natural and real-world applicable conversational shopping assistants.",0
"Our approach introduces a new dataset called FashionIQ that contains pairs of natural language descriptions (queries) of fashion items and their corresponding images retrieved from online product pages. We collected 246 queries and images and made them publicly available along with baseline results using existing retrieval models. This creates a valuable resource which can be used by other researchers studying image retrieval. Additionally we present initial experiments on fine tuning a state of the art textual embedding model trained on large scale static data so as to improve accuracy on this task. Furthermore, we analyze the characteristics of our new dataset comparing it against traditional datasets such as COCO and Flickr8k and showcase how it poses some unique challenges towards developing effective solutions. Finally, we aim at spurring further interest in creating more datasets focused specifically on NLF tasks given the recent rise in popularity of these applications. Overall, we believe that our work provides insightful contributions towards advancing the field of visual search by natural language feedback.",1
"ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.",0
"Abstract: This study investigates how underspecification can pose challenges for credibility in modern machine learning techniques such as deep neural networks (DNNs). Underspecification refers to cases where models fail to capture important aspects of data generating processes that are relevant to applications but may not show up clearly in available training data. We present theoretical arguments suggesting that underspecification can undermine interpretability and reliability in DNN predictions and we substantiate our claims through empirical studies on several benchmark datasets including image recognition and natural language processing tasks. Our findings suggest that current practices in DNN model selection and evaluation can overlook important forms of underspecification, highlighting the need for further research into methods that promote greater transparency and accountability in these systems.",1
"Classically, visual object tracking involves following a target object throughout a given video, and it provides us the motion trajectory of the object. However, for many practical applications, this output is often insufficient since additional semantic information is required to act on the video material. Example applications of this are surveillance and target-specific video summarization, where the target needs to be monitored with respect to certain predefined constraints, e.g., 'when standing near a yellow car'. This paper explores, tracking visual objects subjected to additional lingual constraints. Differently from Li et al., we impose additional lingual constraints upon tracking, which enables new applications of tracking. Whereas in their work the goal is to improve and extend upon tracking itself. To perform benchmarks and experiments, we contribute two datasets: c-MOT16 and c-LaSOT, curated through appending additional constraints to the frames of the original LaSOT and MOT16 datasets. We also experiment with two deep models SiamCT-DFG and SiamCT-CA, obtained through extending a recent state-of-the-art Siamese tracking method and adding modules inspired from the fields of natural language processing and visual question answering. Through experimental results, we show that the proposed model SiamCT-CA can significantly outperform its counterparts. Furthermore, our method enables the selective compression of videos, based on the validity of the constraint.",0
"This research presents a new approach to object tracking called ""Siamese Tracking with Lingual Object Constraints."" The goal of object tracking is to keep track of objects as they move through time in video sequences. In existing methods like the popular Siamese network architecture, the tracker compares a small window near each target location from frame to frame to find correspondences. These comparisons only consider appearance features like colors and textures. Our approach uses a novel constraint that ties together these feature windows at both locations by using a textual description of what the region looks like. We use off-the-shelf tools to extract such descriptions directly from natural language inputs provided by humans, which can describe complex features well beyond color and texture. Experiments show our method outperforms SOTA (State Of The Art) approaches on challenging benchmarks. Since we rely solely on human language to improve performance without engineering any specialized features, there may exist some applications where our approach could excel over other methods even further given appropriate linguistic guidance from experts. As more sophisticated models come online, it seems likely similar techniques using them to replace our simpler systems would yield even stronger results.",1
"Federated learning (FL) has attracted increasing attention in recent years. As a privacy-preserving collaborative learning paradigm, it enables a broader range of applications, especially for computer vision and natural language processing tasks. However, to date, there is limited research of federated learning on relational data, namely Knowledge Graph (KG). In this work, we present a modified version of the graph neural network algorithm that performs federated modeling over KGs across different participants. Specifically, to tackle the inherent data heterogeneity issue and inefficiency in algorithm convergence, we propose a novel optimization algorithm, named FedAlign, with 1) optimal transportation (OT) for on-client personalization and 2) weight constraint to speed up the convergence. Extensive experiments have been conducted on several widely used datasets. Empirical results show that our proposed method outperforms the state-of-the-art FL methods, such as FedAVG and FedProx, with better convergence.",0
"Federated relational data modeling (FRDM) allows distributed databases on multiple servers to collaborate by defining relationships between tables that exist on different physical locations within these servers. This approach requires solving the problem of basing alignment: determining how much each attribute from one table can describe other attributes from another related table while considering database integrity constraints such as functional dependencies. In this study, we propose a new methodology based on basis alignment and weight penalty functions which enables improved FRDM via more accurate description ability measurement. Our results demonstrate significant improvements over existing methods leading up to higher quality federation model creation processes.",1
"Most existing text-to-image generation methods adopt a multi-stage modular architecture which has three significant problems: 1) Training multiple networks increases the run time and affects the convergence and stability of the generative model; 2) These approaches ignore the quality of early-stage generator images; 3) Many discriminators need to be trained. To this end, we propose the Dual Attention Generative Adversarial Network (DTGAN) which can synthesize high-quality and semantically consistent images only employing a single generator/discriminator pair. The proposed model introduces channel-aware and pixel-aware attention modules that can guide the generator to focus on text-relevant channels and pixels based on the global sentence vector and to fine-tune original feature maps using attention weights. Also, Conditional Adaptive Instance-Layer Normalization (CAdaILN) is presented to help our attention modules flexibly control the amount of change in shape and texture by the input natural-language description. Furthermore, a new type of visual loss is utilized to enhance the image resolution by ensuring vivid shape and perceptually uniform color distributions of generated images. Experimental results on benchmark datasets demonstrate the superiority of our proposed method compared to the state-of-the-art models with a multi-stage framework. Visualization of the attention maps shows that the channel-aware attention module is able to localize the discriminative regions, while the pixel-aware attention module has the ability to capture the globally visual contents for the generation of an image.",0
"Here is my attempt at writing an abstract for your paper titled ""DTGAN: Dual Attention Generative Adversarial Networks for Text-to-Image Generation"":  Generating realistic images from text descriptions has been a challenging problem in computer vision for many years now. While there have been several approaches proposed that utilize generative adversarial networks (GANs) to tackle this task, most of them suffer from important limitations such as limited coherency between image regions, low resolution output images, etc. In this work we introduce a new architecture called dual attention GAN (DTGAN), which addresses these issues by using multiple attention mechanisms. We demonstrate through extensive experiments that our approach produces state-of-the art results on a number of popular datasets widely used for evaluating text-to-image generation models, while providing a more interpretable and explainable solution compared to alternative architectures like Stable Diffusion-Aware Discrete GANs (SD-DCGAN).  The key idea behind our method is to employ two separate attention modules: one focused on small spatial details and another targeted at large global features. This allows us to achieve high quality and coherent outputs that capture both local features present in individual objects, and the larger context surrounding them. Our contributions can be summarized as follows: i) we propose a novel deep convolutional GAN framework capable of producing sharp images conditioned on natural language texts; ii) our model employs multi-scale attention enabling better handling of diverse scales observed within objects/scenes along with more detailed focus on particular regions, e.g., focusing solely on face of a person or foliage in parks; iii) we conduct comprehensive evaluation over three benchmark datasets where our DTGAN provides unparalleled performance according to standard metrics commonly adopted in the community including human studies; iv) we provide visual inspection highlighting improved qualitative properties associated w",1
"Reinforcement learning provides a general framework for flexible decision making and control, but requires extensive data collection for each new task that an agent needs to learn. In other machine learning fields, such as natural language processing or computer vision, pre-training on large, previously collected datasets to bootstrap learning for new tasks has emerged as a powerful paradigm to reduce data requirements when learning a new task. In this paper, we ask the following question: how can we enable similarly useful pre-training for RL agents? We propose a method for pre-training behavioral priors that can capture complex input-output relationships observed in successful trials from a wide range of previously seen tasks, and we show how this learned prior can be used for rapidly learning new tasks without impeding the RL agent's ability to try out novel behaviors. We demonstrate the effectiveness of our approach in challenging robotic manipulation domains involving image observations and sparse reward functions, where our method outperforms prior works by a substantial margin.",0
"This paper introduces Parrot, a data driven approach that extracts statistical dependencies between states and actions to guide reinforcement learning (RL). By leveraging prior knowledge of these correlations, our method reduces sample complexity compared to traditional RL algorithms while still achieving competitive performance on Atari games and gridworld tasks. We show how to incorporate prior beliefs into deep neural networks by formulating them as Gaussian processes, enabling efficient optimization via gradient methods. Additionally, we provide theoretical analysis establishing conditions under which Parrot attains sublinear regret and derives sharper bounds than standard policy iteration based planning strategies. Our work demonstrates the utility of using preexisting knowledge within large datasets when learning new skills. -----  Parallel Programming Education at UIUC - Spring 2023 Syllabus -----------------------------------------------------------  Course Description: In this course, students will learn modern parallel programming techniques to solve real world problems efficiently on distributed systems. Through hands-on projects and lectures from industry experts, you will gain practical experience designing and implementing scalable applications using parallel computing frameworks like Hadoop and Spark. You will explore topics such as parallel computing fundamentals, MapReduce, YARN, shared files/databases across clusters, stream processing & SQL query engines, Kubernetes, resource management, security and best practices. At the end of this class, you should have a solid foundation for building production quality applications using distributed platforms.  Meeting Time: TBD | Location: Siebel Center Room #4147 Contact Person: Dr. Dewayne E. Perry (dewayn@illinois.edu) Office hours by appointment only Technical Support Contact: tba Online Presence: Piazza Q&A Forum, Github Repository, StackOverflow tag ""uiucparall...  Grading Breakdown: Individual homework assignments / quizzes | 20% Group Project Midterm | 25% Final Exam | 30% Class Participation + Homework Discussion | 25% ----------",1
"Reinforcement learning (RL), particularly in sparse reward settings, often requires prohibitively large numbers of interactions with the environment, thereby limiting its applicability to complex problems. To address this, several prior approaches have used natural language to guide the agent's exploration. However, these approaches typically operate on structured representations of the environment, and/or assume some structure in the natural language commands. In this work, we propose a model that directly maps pixels to rewards, given a free-form natural language description of the task, which can then be used for policy learning. Our experiments on the Meta-World robot manipulation domain show that language-based rewards significantly improves the sample efficiency of policy learning, both in sparse and dense reward settings.",0
"This paper presents PixL2R, a new method that enables reinforcement learning algorithms to leverage human feedback more effectively through natural language instructions. Existing approaches require extensive manual engineering of reward functions or rely heavily on demonstrations to learn complex tasks efficiently. In contrast, PixL2R maps high-level task descriptions into low-level pixel rewards using deep neural networks. These rewards guide reinforcement learning agents towards desired behaviors while minimizing expert supervision. We evaluate our approach on challenging benchmarks from robotics, computer vision, and game domains. Our results show consistent improvements over state-of-the-art methods that use only raw pixels as feedback. Furthermore, we demonstrate how PixL2R can adapt to changing task objectives online without requiring explicit fine-tuning. By bridging the gap between natural language guidance and efficient decision making in artificial intelligence, PixL2R has the potential to significantly simplify the development of reinforcement learning applications.",1
"Machine Learning (ML) techniques for image classification routinely require many labelled images for training the model and while testing, we ought to use images belonging to the same domain as those used for training. In this paper, we overcome the two main hurdles of ML, i.e. scarcity of data and constrained prediction of the classification model. We do this by introducing a visual classifier which uses a concept of transfer learning, namely Zero-Shot Learning (ZSL), and standard Natural Language Processing techniques. We train a classifier by mapping labelled images to their textual description instead of training it for specific classes. Transfer learning involves transferring knowledge across domains that are similar. ZSL intelligently applies the knowledge learned while training for future recognition tasks. ZSL differentiates classes as two types: seen and unseen classes. Seen classes are the classes upon which we have trained our model and unseen classes are the classes upon which we test our model. The examples from unseen classes have not been encountered in the training phase. Earlier research in this domain focused on developing a binary classifier but, in this paper, we present a multi-class classifier with a Zero-Shot Learning approach.",0
"Title: ""A Multi-Class Approach - Building a Visual Classifier based on Textual Descriptions using Zero-Shot Learning""  Abstract: In recent years, zero-shot learning (ZSL) has emerged as a promising approach to build classifiers that can recognize objects from images without requiring large amounts of labeled training data. However, most existing ZSL methods focus on binary classification tasks, where the goal is to predict whether an object belongs to one of two classes. In practice, many real-world applications require multi-class recognition, which involves identifying objects belonging to one of multiple distinct categories. In this paper, we propose a novel multi-class ZSL framework that leverages textual descriptions to learn visual representations of objects across different classes. Our method combines a pre-trained deep convolutional neural network (CNN) model with a semantic embedding module that maps textual attributes into a continuous vector space. We then use these vectors to project the image features learned by the CNN model onto a shared latent space, which allows us to perform joint zero-shot inference across all classes simultaneously. Experimental results on several benchmark datasets demonstrate that our proposed approach significantly outperforms state-of-the-art multi-class ZSL methods, achieving higher accuracy with better generalization capabilities. Our study shows that combining semantic embeddings with CNN models is a powerful strategy for building effective multi-class ZSL systems that can operate effectively even under limited availability of annotated examples per category. Overall, our work advances the state of art in ZSL and has important implications for various computer vision applications such as content-based image retrieval and object detection.",1
"Understanding images and text together is an important aspect of cognition and building advanced Artificial Intelligence (AI) systems. As a community, we have achieved good benchmarks over language and vision domains separately, however joint reasoning is still a challenge for state-of-the-art computer vision and natural language processing (NLP) systems. We propose a novel task to derive joint inference about a given image-text modality and compile the Visuo-Linguistic Question Answering (VLQA) challenge corpus in a question answering setting. Each dataset item consists of an image and a reading passage, where questions are designed to combine both visual and textual information i.e., ignoring either modality would make the question unanswerable. We first explore the best existing vision-language architectures to solve VLQA subsets and show that they are unable to reason well. We then develop a modular method with slightly better baseline performance, but it is still far behind human performance. We believe that VLQA will be a good benchmark for reasoning over a visuo-linguistic context. The dataset, code and leaderboard is available at https://shailaja183.github.io/vlqa/.",0
Title: VLQA Challenge,1
"Personality image captioning (PIC) aims to describe an image with a natural language caption given a personality trait. In this work, we introduce a novel formulation for PIC based on a communication game between a speaker and a listener. The speaker attempts to generate natural language captions while the listener encourages the generated captions to contain discriminative information about the input images and personality traits. In this way, we expect that the generated captions can be improved to naturally represent the images and express the traits. In addition, we propose to adapt the language model GPT2 to perform caption generation for PIC. This enables the speaker and listener to benefit from the language encoding capacity of GPT2. Our experiments show that the proposed model achieves the state-of-the-art performance for PIC.",0
"In recent years, there has been significant interest in developing artificial intelligence (AI) systems that can generate natural language descriptions of images, such as image captioning models. These models have shown promise for applications such as visual question answering and automated content creation, among others. However, many existing models rely on structured input or output formats, making them difficult to integrate into more general human communication tasks. Additionally, these models often lack a clear understanding of their own limitations, which can lead to unreliable or misleading outputs. This work proposes a new approach to image captioning based on structural and functional decomposition, which breaks down the problem into smaller subtasks and allows for greater transparency and control over the generated text. We show through experiments that our method significantly improves both accuracy and coherence compared to previous methods, while still maintaining high levels of fluency and descriptiveness. Our approach opens up exciting possibilities for future research in multi-modal AI communication.",1
"Kronecker Products (KP) have been used to compress IoT RNN Applications by 15-38x compression factors, achieving better results than traditional compression methods. However when KP is applied to large Natural Language Processing tasks, it leads to significant accuracy loss (approx 26%). This paper proposes a way to recover accuracy otherwise lost when applying KP to large NLP tasks, by allowing additional degrees of freedom in the KP matrix. More formally, we propose doping, a process of adding an extremely sparse overlay matrix on top of the pre-defined KP structure. We call this compression method doped kronecker product compression. To train these models, we present a new solution to the phenomenon of co-matrix adaption (CMA), which uses a new regularization scheme called co matrix dropout regularization (CMR). We present experimental results that demonstrate compression of a large language model with LSTM layers of size 25 MB by 25x with 1.4% loss in perplexity score. At 25x compression, an equivalent pruned network leads to 7.9% loss in perplexity score, while HMD and LMF lead to 15% and 27% loss in perplexity score respectively.",0
"Artificial intelligence (AI) has gained significant traction over recent years due to advancements in natural language processing (NLP). However, NLP models often suffer from large memory footprints that limit their applications on resource-constrained devices such as smartphones and embedded systems. To address this challenge, we propose compressing state-of-the-art NLP models using Doped Kronecker Products (DKP), which efficiently represent high-dimensional tensors through low-rank approximations without sacrificing accuracy. Our approach leverages Kronecker products by introducing sparse factors into each Kronecker factorization matrix, allowing us to adaptively trade off sparsity versus rank during compression. Experimental results demonstrate that our method consistently outperforms baseline methods across popular benchmark datasets while reducing model size up to 25%. This work bridges the gap between efficient AI deployments on edge devices and maintaining adequate performance, enabling new opportunities for mobile computing and IoT platforms where resources are limited but demand for advanced NLP capabilities continues to grow exponentially.",1
"The aim of image captioning is to generate textual description of a given image. Though seemingly an easy task for humans, it is challenging for machines as it requires the ability to comprehend the image (computer vision) and consequently generate a human-like description for the image (natural language understanding). In recent times, encoder-decoder based architectures have achieved state-of-the-art results for image captioning. Here, we present a heuristic of beam search on top of the encoder-decoder based architecture that gives better quality captions on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.",0
"In recent years, image caption generation has emerged as a critical task in computer vision research due to its applications in diverse areas such as content creation, accessibility, robotics, and more. Previous approaches have utilized various deep learning architectures like convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to generate descriptive captions for images. This paper presents ""Attention Beam,"" a novel approach that builds upon previous work by incorporating attention mechanisms into image captioning models to improve their accuracy and coherence. Our method leverages self-attention modules to focus on relevant parts of the image while generating text descriptions. Additionally, we propose a beam search algorithm that explores different interpretations of the input image during decoding, further improving our model's performance. Extensive experiments conducted using standard benchmark datasets demonstrate the effectiveness of our proposed approach compared to state-of-the-art methods. Overall, the results indicate that integrating attention techniques significantly enhances the quality of generated captions, yielding improved performance across multiple metrics. With Attention Beam, we aim to bridge the gap between computer vision and natural language processing tasks, advancing both fields towards realizing intelligent systems capable of seamless communication between humans and machines.",1
"We introduce the task of 3D object localization in RGB-D scans using natural language descriptions. As input, we assume a point cloud of a scanned 3D scene along with a free-form description of a specified target object. To address this task, we propose ScanRefer, learning a fused descriptor from 3D object proposals and encoded sentence embeddings. This fused descriptor correlates language expressions with geometric features, enabling regression of the 3D bounding box of a target object. We also introduce the ScanRefer dataset, containing 51,583 descriptions of 11,046 objects from 800 ScanNet scenes. ScanRefer is the first large-scale effort to perform object localization via natural language expression directly in 3D.",0
"We describe ScanRefer, which enables accurate localization of 3D objects in real world scenes via natural language requests from users. Our method relies on novel object proposal generation techniques that operate directly on raw depth maps by leveraging geometric features and learning latent representations guided by human annotator behavior. These proposals then undergo refinement through natural language queries issued by users in order to accurately localize objects within the scene. In addition to the benefits offered by direct operation on low cost sensors such as KinectFusion (avoiding expensive laser based scanners), our pipeline allows easy integration of semantic knowledge extracted from images into dense 3D scan understanding applications allowing users to interact naturally and efficiently with complex 3D environments. To demonstrate the effectiveness of our approach we conduct experiments quantitatively evaluating performance relative to previous state of the art methods across standard benchmarks and show qualitative improvements on challenging use cases involving cluttered indoor scenes and dynamic motion scenarios. Overall, our work presents significant advances in the ability to accurately localize objects in complex 3D spaces while offering new ways for humans to interface with these computational models.",1
"Dot-product attention has wide applications in computer vision and natural language processing. However, its memory and computational costs grow quadratically with the input size. Such growth prohibits its application on high-resolution inputs. To remedy this drawback, this paper proposes a novel efficient attention mechanism equivalent to dot-product attention but with substantially less memory and computational costs. Its resource efficiency allows more widespread and flexible integration of attention modules into a network, which leads to better accuracies. Empirical evaluations demonstrated the effectiveness of its advantages. Efficient attention modules brought significant performance boosts to object detectors and instance segmenters on MS-COCO 2017. Further, the resource efficiency democratizes attention to complex models, where high costs prohibit the use of dot-product attention. As an exemplar, a model with efficient attention achieved state-of-the-art accuracies for stereo depth estimation on the Scene Flow dataset. Code is available at https://github.com/cmsflash/efficient-attention.",0
"In recent years, neural networks have been widely applied to natural language processing tasks such as machine translation, question answering, summarization, sentiment analysis etc. One key component of these models is attention mechanism which allows them to selectively focus on relevant parts of input sequence. However, standard attention implementations suffer from quadratic complexity wrt input length, hindering their applicability to longer sequences. This paper proposes two methods to address this issue. First, we propose shifting matrix multiplications offline by precomputing attention weights for all positions at once using batch normalization. Secondly, we introduce efficient scaled sigmoids to replace softmax calculations. Our experiments show that applying these techniques results in significant speedups without loss of quality compared to existing baseline architectures. While our main goal is efficiency improvements, we see performance boosts over base attention even when computational costs are disregarded. We believe that the proposed methods have great potential for enabling more complex neural network architectures while maintaining tractability and feasibility for real world applications.",1
"There are many applications of Generative Adversarial Networks (GANs) in fields like computer vision, natural language processing, speech synthesis, and more. Undoubtedly the most notable results have been in the area of image synthesis and in particular in the generation of deepfake videos. While deepfakes have received much negative media coverage, they can be a useful technology in applications like entertainment, customer relations, or even assistive care. One problem with generating deepfakes is the requirement for a lot of image training data of the subject which is not an issue if the subject is a celebrity for whom many images already exist. If there are only a small number of training images then the quality of the deepfake will be poor. Some media reports have indicated that a good deepfake can be produced with as few as 500 images but in practice, quality deepfakes require many thousands of images, one of the reasons why deepfakes of celebrities and politicians have become so popular. In this study, we exploit the property of a GAN to produce images of an individual with variable facial expressions which we then use to generate a deepfake. We observe that with such variability in facial expressions of synthetic GAN-generated training images and a reduced quantity of them, we can produce a near-realistic deepfake videos.",0
"This paper proposes using Generative Adversarial Networks (GAN) to synthesize minimum training data for deep learning algorithms. Currently, creating realistic images from generative models requires large amounts of high quality labeled data that can take significant time to collect and annotate by hand. However, as GANs have become more advanced, they have proven capable of generating new samples that fit within previously learned distributions. The authors propose leveraging these advancements by pretraining their model on ImageNet data before fine tuning on the target dataset. In addition to reducing required training data, pretraining allows for larger image sizes, faster inference speed, and improved stability over state-of-the-art methods. Experimental results demonstrate the effectiveness of this method for a wide range of tasks including facial expression manipulation, object insertion into still images, and video frame generation. Overall, this work provides a promising direction towards enabling generators like Stable Diffusion which require less than ten minutes of human labeling per task while achieving similar performance compared to traditional methods requiring tens of thousands of labels.",1
"Variational autoencoders (VAEs) hold great potential for modelling text, as they could in theory separate high-level semantic and syntactic properties from local regularities of natural language. Practically, however, VAEs with autoregressive decoders often suffer from posterior collapse, a phenomenon where the model learns to ignore the latent variables, causing the sequence VAE to degenerate into a language model. In this paper, we argue that posterior collapse is in part caused by the lack of dispersion in encoder features. We provide empirical evidence to verify this hypothesis, and propose a straightforward fix using pooling. This simple technique effectively prevents posterior collapse, allowing model to achieve significantly better data log-likelihood than standard sequence VAEs. Comparing to existing work, our proposed method is able to achieve comparable or superior performances while being more computationally efficient.",0
"This paper describes some issues that can occur with Variational Autoencoders (VAEs) during training, especially when working on sequence data like text or time series. Specifically, two problems known as posterior collapse and encoder feature dispersion are discussed. These problems can cause poor results and make it difficult to effectively use these models for tasks such as generating new examples or unsupervised learning. The authors present methods for addressing these issues and improving the performance of VAEs in practice.",1
"Sequence labeling is a fundamental problem in machine learning, natural language processing and many other fields. A classic approach to sequence labeling is linear chain conditional random fields (CRFs). When combined with neural network encoders, they achieve very good performance in many sequence labeling tasks. One limitation of linear chain CRFs is their inability to model long-range dependencies between labels. High order CRFs extend linear chain CRFs by modeling dependencies no longer than their order, but the computational complexity grows exponentially in the order. In this paper, we propose the Neural Latent Dependency Model (NLDM) that models dependencies of arbitrary length between labels with a latent tree structure. We develop an end-to-end training algorithm and a polynomial-time inference algorithm of our model. We evaluate our model on both synthetic and real datasets and show that our model outperforms strong baselines.",0
This should describe the contents of the paper without revealing the authorship/affiliation.,1
"The last few years have seen an increased interest in deep learning (DL) due to its success in applications such as computer vision, natural language processing (NLP), and self-driving cars. Inspired by this success, this paper applied DL to predict flight demand and delays, which have been a concern for airlines and the other stakeholders in the National Airspace System (NAS). Demand and delay prediction can be formulated as a supervised learning problem, where, given an understanding of past historical demand and delays, a deep learning network can examine sequences of historic data to predict current and future sequences. With that in mind, we applied a well-known DL method, sequence to sequence (seq2seq), to solve the problem. Our results show that the seq2seq method can reduce demand prediction mean squared error (MSE) by 50%, compared to two classical baseline algorithms.",0
"This paper presents a deep learning approach to forecast flight delays based on historical data of flight demand and other relevant factors such as air traffic control situations and weather conditions. Our proposed model uses neural networks to extract features from large amounts of raw data, allowing us to make accurate predictions that can help optimize operations at busy airports. Through extensive evaluation, we demonstrate the effectiveness of our method compared to traditional statistical models. Overall, our results show promise for using machine learning algorithms to improve decision making in complex systems like air transportation.",1
"word2vec due to Mikolov \textit{et al.} (2013) is a word embedding method that is widely used in natural language processing. Despite its great success and frequent use, theoretical justification is still lacking. The main contribution of our paper is to propose a rigorous analysis of the highly nonlinear functional of word2vec. Our results suggest that word2vec may be primarily driven by an underlying spectral method. This insight may open the door to obtaining provable guarantees for word2vec. We support these findings by numerical simulations. One fascinating open question is whether the nonlinear properties of word2vec that are not captured by the spectral method are beneficial and, if so, by what mechanism.",0
"In natural language processing, word embeddings have become ubiquitous due to their ability to capture contextual relationships among words while maintaining numerical properties that can be used in downstream tasks such as machine translation. Word embeddings are typically learned by trained on large datasets of text using neural networks. However, one limitation of existing models is their opacity and difficulty of interpretation. To address this issue, we propose a method called Spectral Underpinnings Learn (SUL) which provides insight into how embedding dimensions can be associated with specific semantic features such as synonymy or hypernymy. We empirically evaluate SUL against several benchmarks and show its effectiveness in capturing meaningful relations. Finally, we demonstrate the utility of our approach through novel applications in NLP. Overall, SUL allows for better understanding and manipulation of word embeddings providing new possibilities in NLP research.",1
"We study the challenging problem of releasing a robot in a previously unseen environment, and having it follow unconstrained natural language navigation instructions. Recent work on the task of Vision-and-Language Navigation (VLN) has achieved significant progress in simulation. To assess the implications of this work for robotics, we transfer a VLN agent trained in simulation to a physical robot. To bridge the gap between the high-level discrete action space learned by the VLN agent, and the robot's low-level continuous action space, we propose a subgoal model to identify nearby waypoints, and use domain randomization to mitigate visual domain differences. For accurate sim and real comparisons in parallel environments, we annotate a 325m2 office space with 1.3km of navigation instructions, and create a digitized replica in simulation. We find that sim-to-real transfer to an environment not seen in training is successful if an occupancy map and navigation graph can be collected and annotated in advance (success rate of 46.8% vs. 55.9% in sim), but much more challenging in the hardest setting with no prior mapping at all (success rate of 22.5%).",0
"Increasingly capable artificial intelligence agents rely on large amounts of data obtained from simulated environments (sims) to train effectively before deployment in real scenarios. As these models perform well, they can provide valuable support to humans in various tasks that require processing natural language inputs and interacting with visual scenes to reach specific goals. However, transitioning simulation performance to real-world proficiency often requires additional training adapted to nuances present only in reality, which may differ significantly from those experienced by agents during their initial learning stages. This raises the question: how do we ensure that AIs trained primarily in sims are able to successfully transfer their skills into complex human worlds? To answer this question and bridge the gap between sim and real domains, this study presents several novel techniques aimed at enhancing the ability of AI agents to excel across both types of environment while preserving their efficiency in simulation settings. By leveraging insights from diverse fields such as computer vision, robotics, and cognitive psychology, our approach enhances traditional methods used to transfer knowledge acquired in simulations to more effective adaptation in true-to-life scenarios. We demonstrate the effectiveness of our proposals through extensive experiments conducted in challenging test cases with high variability, showing significant improvements over baseline approaches commonly utilized in current state-of-the-art systems. Additionally, we analyze the importance of certain design choices and components within the proposed framework and discuss future research directions towards even better sim-to-real generalization capabilities. Our work contributes to building robust agent architectures tha",1
"The ever-growing advances of deep learning in many areas including vision, recommendation systems, natural language processing, etc., have led to the adoption of Deep Neural Networks (DNNs) in production systems. The availability of large datasets and high computational power are the main contributors to these advances. The datasets are usually crowdsourced and may contain sensitive information. This poses serious privacy concerns as this data can be misused or leaked through various vulnerabilities. Even if the cloud provider and the communication link is trusted, there are still threats of inference attacks where an attacker could speculate properties of the data used for training, or find the underlying model architecture and parameters. In this survey, we review the privacy concerns brought by deep learning, and the mitigating techniques introduced to tackle these issues. We also show that there is a gap in the literature regarding test-time inference privacy, and propose possible future research directions.",0
"Deep learning has revolutionized many aspects of modern life by enabling more efficient data processing and analysis through artificial intelligence algorithms. However, these advancements come at the cost of privacy as deep learning relies on large amounts of sensitive personal data. Therefore, understanding privacy implications in deep learning becomes crucial to ensure secure deployments and ethical use cases. This survey provides an overview of existing research exploring tradeoffs between accuracy and privacy preservation techniques used in different architectures such as neural networks, convolutional networks, recurrent networks, generative adversarial networks (GANs), reinforcement learning, etc., along with recent approaches proposed to balance model utility and individuals’ rights to informational self-determination, such as federated learning and differential privacy. Additionally, we discuss open challenges in designing frameworks that can guarantee privacy while still maintaining high levels of efficiency and scalability. Ultimately, our goal is to inspire researchers and practitioners towards creating better informed systems and applications capable of balancing these competing priorities.",1
"Computational research on mental health disorders from written texts covers an interdisciplinary area between natural language processing and psychology. A crucial aspect of this problem is prevention and early diagnosis, as suicide resulted from depression being the second leading cause of death for young adults. In this work, we focus on methods for detecting the early onset of depression from social media texts, in particular from Reddit. To that end, we explore the eRisk 2018 dataset and achieve good results with regard to the state of the art by leveraging topic analysis and learned confidence scores to guide the decision process.",0
"This paper presents a method for detecting early onset depression symptoms through analysis of social media text data. Our approach utilizes learned confidence scores, which are used to evaluate the likelihood that a given sentence exhibits depressive language features. These confidence scores are then combined into a feature vector representing each user over time, providing insights into changes in their state of mind. We evaluated our model on a dataset consisting of daily updates posted by users on Twitter. Results indicate that our method can accurately identify early signs of depression with high accuracy. Future work includes expanding the range of depression symptom types analyzed as well as increasing the size of the dataset.",1
"Continual learning systems will interact with humans, with each other, and with the physical world through time -- and continue to learn and adapt as they do. An important open problem for continual learning is a large-scale benchmark that enables realistic evaluation of algorithms. In this paper, we study a natural setting for continual learning on a massive scale. We introduce the problem of personalized online language learning (POLL), which involves fitting personalized language models to a population of users that evolves over time. To facilitate research on POLL, we collect massive datasets of Twitter posts. These datasets, Firehose10M and Firehose100M, comprise 100 million tweets, posted by one million users over six years. Enabled by the Firehose datasets, we present a rigorous evaluation of continual learning algorithms on an unprecedented scale. Based on this analysis, we develop a simple algorithm for continual gradient descent (ConGraD) that outperforms prior continual learning methods on the Firehose datasets as well as earlier benchmarks. Collectively, the POLL problem setting, the Firehose datasets, and the ConGraD algorithm enable a complete benchmark for reproducible research on web-scale continual learning.",0
"This paper presents a novel approach to continual learning with web-scale natural language data. We address the challenge of how to effectively learn from large amounts of unstructured text data by introducing a new model that combines several state-of-the-art techniques. Our approach allows us to efficiently integrate both online and offline training data, enabling our model to continuously adapt to changes in the underlying distribution of the input data.  We evaluate our method on two benchmark datasets and show that it achieves significant improvements over existing approaches, outperforming them across all metrics. Furthermore, we demonstrate the practicality of our approach through experiments on real-world applications such as sentiment analysis and question answering. Overall, our work represents a major step forward towards building more robust and flexible NLP systems that can keep up with the rapidly evolving nature of modern languages.",1
"The neural attention mechanism plays an important role in many natural language processing applications. In particular, the use of multi-head attention extends single-head attention by allowing a model to jointly attend information from different perspectives. Without explicit constraining, however, multi-head attention may suffer from attention collapse, an issue that makes different heads extract similar attentive features, thus limiting the model's representation power. In this paper, for the first time, we provide a novel understanding of multi-head attention from a Bayesian perspective. Based on the recently developed particle-optimization sampling techniques, we propose a non-parametric approach that explicitly improves the repulsiveness in multi-head attention and consequently strengthens model's expressiveness. Remarkably, our Bayesian interpretation provides theoretical inspirations on the not-well-understood questions: why and how one uses multi-head attention. Extensive experiments on various attention models and applications demonstrate that the proposed repulsive attention can improve the learned feature diversity, leading to more informative representations with consistent performance improvement on various tasks.",0
"This paper seeks to rethink multi-head attention (MHA) from the perspective of Bayesian inference. MHA is a popular mechanism used in transformer models to enable them to efficiently process large amounts of data by focusing on different parts of the input at once. However, current implementations often suffer from computational costs that scale linearly with sequence length, making their use impractical for longer sequences. To address these limitations, we propose Repulsive Attention (RA), which adapts the softmax function commonly used to normalize probabilities into a repulsive distribution centered around zero. By doing so, RA enables MHA operations to focus only on informative features while suppressing less important ones, leading to more efficient computation without sacrificing accuracy. Experimental results show that our proposed method achieves substantial speedups over state-of-the art methods across a range of natural language processing tasks. Our approach offers a new direction for developing efficient and effective deep learning algorithms.",1
"Generating schema labels automatically for column values of data tables has many data science applications such as schema matching, and data discovery and linking. For example, automatically extracted tables with missing headers can be filled by the predicted schema labels which significantly minimizes human effort. Furthermore, the predicted labels can reduce the impact of inconsistent names across multiple data tables. Understanding the connection between column values and contextual information is an important yet neglected aspect as previously proposed methods treat each column independently. In this paper, we propose a context-aware semantic labeling method using both the column values and context. Our new method is based on a new setting for semantic labeling, where we sequentially predict labels for an input table with missing headers. We incorporate both the values and context of each data column using the pre-trained contextualized language model, BERT, that has achieved significant improvements in multiple natural language processing tasks. To our knowledge, we are the first to successfully apply BERT to solve the semantic labeling task. We evaluate our approach using two real-world datasets from different domains, and we demonstrate substantial improvements in terms of evaluation metrics over state-of-the-art feature-based methods.",0
"Recent advances in deep learning have led to significant improvements in language understanding tasks such as question answering and text generation. However, these models require large amounts of labeled training data and fine-tuning which can become computationally expensive. In this work we propose a novel approach that utilizes pre-trained deep contextualized language models (e.g. BERT) for semantic labeling without any retraining. Our method leverages existing relationships between input pairs of sentences using cosine similarity and a softmax function. We evaluate our proposed model on two benchmark datasets: MultiNLI and Quasar-T. Results show significant improvement over previous methods achieving state-of-the-art performance while requiring less computational resources during inference time. Our contributions provide a more efficient framework that enables researchers and practitioners to use powerful pre-trained language models without going through complex and time consuming fine-tuning procedures. This has important implications for various downstream NLP applications including but not limited to sentiment analysis, machine translation, chatbots, virtual assistants etc... Keywords: Natural Language Processing (NLP), Semantic Labeling, Fine-Grained Classification, BERT, Pre-Trained Models, Efficiency, State-Of-The Art Performance, Sentence Similarity, Cosine Similarity, Softmax Function.",1
"Despite the widespread application of recurrent neural networks (RNNs) across a variety of tasks, a unified understanding of how RNNs solve these tasks remains elusive. In particular, it is unclear what dynamical patterns arise in trained RNNs, and how those patterns depend on the training dataset or task. This work addresses these questions in the context of a specific natural language processing task: text classification. Using tools from dynamical systems analysis, we study recurrent networks trained on a battery of both natural and synthetic text classification tasks. We find the dynamics of these trained RNNs to be both interpretable and low-dimensional. Specifically, across architectures and datasets, RNNs accumulate evidence for each class as they process the text, using a low-dimensional attractor manifold as the underlying mechanism. Moreover, the dimensionality and geometry of the attractor manifold are determined by the structure of the training dataset; in particular, we describe how simple word-count statistics computed on the training dataset can be used to predict these properties. Our observations span multiple architectures and datasets, reflecting a common mechanism RNNs employ to perform text classification. To the degree that integration of evidence towards a decision is a common computational primitive, this work lays the foundation for using dynamical systems techniques to study the inner workings of RNNs.",0
"Text classification using Recurrent Neural Networks (RNNs) has gained significant attention due to their ability to capture complex temporal dependencies present in sequential data. However, one major challenge faced by these models is the vanishing gradient problem that arises during backpropagation through time (BPTT). In recent years, several techniques have been proposed to address this issue, including architectures such as Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs), as well as optimization methods like truncated BPTT and variational inference. Despite these advancements, there remains a need for better understanding of the geometric properties underlying these models, particularly those related to integration and differentiation operations performed within each time step. This paper presents a thorough analysis of the geometry of integration in text classification RNNs, shedding new light on the mechanics behind these powerful machine learning tools. By uncovering fundamental relationships among recurrence parameters, activation functions, and memory dynamics, we provide insights into the design and training of more effective and efficient RNNs for natural language processing tasks. Our findings offer valuable guidance for researchers and practitioners seeking to improve the performance and interpretability of state-of-the-art sequence prediction models.",1
"Matching information across image and text modalities is a fundamental challenge for many applications that involve both vision and natural language processing. The objective is to find efficient similarity metrics to compare the similarity between visual and textual information. Existing approaches mainly match the local visual objects and the sentence words in a shared space with attention mechanisms. The matching performance is still limited because the similarity computation is based on simple comparisons of the matching features, ignoring the characteristics of their distribution in the data. In this paper, we address this limitation with an efficient learning objective that considers the discriminative feature distributions between the visual objects and sentence words. Specifically, we propose a novel Adversarial Discriminative Domain Regularization (ADDR) learning framework, beyond the paradigm metric learning objective, to construct a set of discriminative data domains within each image-text pairs. Our approach can generally improve the learning efficiency and the performance of existing metrics learning frameworks by regulating the distribution of the hidden space between the matching pairs. The experimental results show that this new approach significantly improves the overall performance of several popular cross-modal matching techniques (SCAN, VSRN, BFAN) on the MS-COCO and Flickr30K benchmarks.",0
"In recent years, deep metric learning has emerged as a powerful technique for matching data across different modalities such as images, videos, audio, text, etc., enabling applications like cross-modal retrieval, video summarization, sentiment analysis, and so on. However, training deep metrics networks remain challenging due to the existence of domain gaps that cause significant degradation in performance. To address these issues, we propose a novel adversarial discriminative domain regularization approach that enhances the training process by explicitly modeling these domain discrepancies and aligning them effectively. Our method adopts two different types of discriminators (domain classifier and pseudo reconstructor) that are trained together to minimize their errors during the training phase while maximizing those of the main network. Experimental results on several benchmark datasets validate the effectiveness of our proposed method over existing approaches both quantitatively and qualitatively.",1
"Contrastive Learning has recently received interest due to its success in self-supervised representation learning in the computer vision domain. However, the origins of Contrastive Learning date as far back as the 1990s and its development has spanned across many fields and domains including Metric Learning and natural language processing. In this paper we provide a comprehensive literature review and we propose a general Contrastive Representation Learning framework that simplifies and unifies many different contrastive learning methods. We also provide a taxonomy for each of the components of contrastive learning in order to summarise it and distinguish it from other forms of machine learning. We then discuss the inductive biases which are present in any contrastive learning system and we analyse our framework under different views from various sub-fields of Machine Learning. Examples of how contrastive learning has been applied in computer vision, natural language processing, audio processing, and others, as well as in Reinforcement Learning are also presented. Finally, we discuss the challenges and some of the most promising future research directions ahead.",0
"Recent advancements in natural language processing have led to significant improvements in tasks such as machine translation, text classification, and generative modeling. However, current methods often struggle when faced with out-of-domain data, leading to poor generalization performance. In this work, we propose contrastive representation learning (CRL) as a framework that addresses these shortcomings by leveraging both positive and negative samples during training. Our approach allows models to learn more meaningful representations which can effectively capture fine-grained distinctions between different concepts. We evaluate our method on several benchmark datasets across various NLP tasks and demonstrate its effectiveness through extensive experiments. Additionally, we provide a comprehensive review of related works, highlighting key insights from existing literature and identifying future research directions. Overall, our results show that CRL provides a powerful alternative for developing robust and flexible NLP systems that can handle unseen scenarios and achieve superior performance compared to state-of-the-art baselines.",1
"Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks. In this paper, we present a new Transformer architecture, Extended Transformer Construction (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs. To scale attention to longer inputs, we introduce a novel global-local attention mechanism between global tokens and regular input tokens. We also show that combining global-local attention with relative position encodings and a Contrastive Predictive Coding (CPC) pre-training objective allows ETC to encode structured inputs. We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs.",0
"How would you write an abstract for the following research paper? Paper Title: ETC: Encoding Long and Structured Inputs in Transformers The authors present a new encoding method (ETC) for transformer models that efficiently processes long and structured inputs such as natural language documents and code snippets. Their method can handle inputs up to billions of tokens in length and effectively captures global dependencies without incurring computational overhead due to its novel attention mechanism design. In addition, they conduct extensive experiments on four real world NLP benchmark datasets, demonstrating significant improvements over competitive baselines, including state-of-the-art tokenizers. Overall, their work presents a major advancement towards enabling large pretraining on extremely long sequences, opening promising research directions to exploit them fully.",1
"We present VisualHints, a novel environment for multimodal reinforcement learning (RL) involving text-based interactions along with visual hints (obtained from the environment). Real-life problems often demand that agents interact with the environment using both natural language information and visual perception towards solving a goal. However, most traditional RL environments either solve pure vision-based tasks like Atari games or video-based robotic manipulation; or entirely use natural language as a mode of interaction, like Text-based games and dialog systems. In this work, we aim to bridge this gap and unify these two approaches in a single environment for multimodal RL. We introduce an extension of the TextWorld cooking environment with the addition of visual clues interspersed throughout the environment. The goal is to force an RL agent to use both text and visual features to predict natural language action commands for solving the final task of cooking a meal. We enable variations and difficulties in our environment to emulate various interactive real-world scenarios. We present a baseline multimodal agent for solving such problems using CNN-based feature extraction from visual hints and LSTMs for textual feature extraction. We believe that our proposed visual-lingual environment will facilitate novel problem settings for the RL community.",0
"Introduction: For decades now, humans have been fascinated by the ability of computers to learn from experience without explicit programming. This phenomenon, which has come to be known as machine learning, was initially based on supervised learning approaches where a model is trained using labeled data examples. However, more recently there have been developments in unsupervised and reinforcement learning that allow agents to improve their behavior without any human intervention at all. In these cases, the agent must discover optimal strategies solely through trial-and-error interactions with its environment. While impressive results have been achieved through these methods, they suffer from several limitations, including poor interpretability, lack of generalization across environments, limited exploration abilities, and brittleness under noisy feedback scenarios. One methodology that addresses some of these shortcomings is called “multimodal” RL (MRL), which incorporates multiple sensory modalities into decision making. Previous works have shown promising results with MRL agents performing better than those relying on single modalities only. Motivation: We present here a new system called Visual Hints that introduces visual hints to guide policy improvement in MRL agents. These hints can represent arbitrary pieces of information expressed either textually or graphically such as natural language descriptions, diagrams, icons, images, videos, etc. They offer an additional communication channel between users and MRL agents and extend existing textual natural language interfaces towards richer forms of expression. Our work leverages recent advances in multiagent systems and deep neural network architectures for natural image generation (such as DALL-E). Contributions: Our main contribution is develo",1
"Language modeling is a keystone task in natural language processing. When training a language model on sensitive information, differential privacy (DP) allows us to quantify the degree to which our private data is protected. However, training algorithms which enforce differential privacy often lead to degradation in model quality. We study the feasibility of learning a language model which is simultaneously high-quality and privacy preserving by tuning a public base model on a private corpus. We find that DP fine-tuning boosts the performance of language models in the private domain, making the training of such models possible.",0
"Our analysis shows that differentially private language models benefit from public pre-training. We consider both unstructured and structured data types, showing that pre-trained models on public datasets significantly improve model utility without sacrificing privacy guarantees. By fine-tuning these models further using the private dataset, we demonstrate significant accuracy gains compared to training from scratch while maintaining strong privacy guarantees. These results have important implications for enabling secure and private machine learning in real-world applications where strong privacy regulations exist.",1
"This article reviews meta-learning also known as learning-to-learn which seeks rapid and accurate model adaptation to unseen tasks with applications in highly automated AI, few-shot learning, natural language processing and robotics. Unlike deep learning, meta-learning can be applied to few-shot high-dimensional datasets and considers further improving model generalization to unseen tasks. Deep learning is focused upon in-sample prediction and meta-learning concerns model adaptation for out-of-sample prediction. Meta-learning can continually perform self-improvement to achieve highly autonomous AI. Meta-learning may serve as an additional generalization block complementary for original deep learning model. Meta-learning seeks adaptation of machine learning models to unseen tasks which are vastly different from trained tasks. Meta-learning with coevolution between agent and environment provides solutions for complex tasks unsolvable by training from scratch. Meta-learning methodology covers a wide range of great minds and thoughts. We briefly introduce meta-learning methodologies in the following categories: black-box meta-learning, metric-based meta-learning, layered meta-learning and Bayesian meta-learning framework. Recent applications concentrate upon the integration of meta-learning with other machine learning framework to provide feasible integrated problem solutions. We briefly present recent meta-learning advances and discuss potential future research directions.",0
The abstract should be informative but concise. Please keep it less than 20% of the overall text content.,1
"A class of recent semi-supervised learning (SSL) methods heavily rely on domain-specific data augmentations. In contrast, generative SSL methods involve unsupervised learning based on generative models by either joint-training or pre-training, and are more appealing from the perspective of being domain-agnostic, since they do not inherently require data augmentations. Joint-training estimates the joint distribution of observations and labels, while pre-training is taken over observations only. Recently, energy-based models (EBMs) have achieved promising results for generative modeling. Joint-training via EBMs for SSL has been explored with encouraging results across different data modalities. In this paper, we make two contributions. First, we explore pre-training via EBMs for SSL and compare it to joint-training. Second, a suite of experiments are conducted over domains of image classification and natural language labeling to give a realistic whole picture of the performances of EBM based SSL methods. It is found that joint-training EBMs outperform pre-training EBMs marginally but nearly consistently.",0
"This is a research paper that presents an investigation into semi-supervised learning techniques applied to image classification problems using deep convolutional neural networks (CNNs). In particular, we focus on methods that use self-supervision to learn representations that can generalize across different domains. We propose two novel approaches based on energy-based models (EBM): one approach involves joint training with both supervised and unsupervised objectives, while another involves pre-training the model with an EBM before fine-tuning on labeled data. Both methods achieve state-of-the-art results on three benchmark datasets, demonstrating their effectiveness in handling cross-domain semantic shifts. Our analyses show that our proposed frameworks effectively utilize unlabeled data and lead to better feature representation for improved performance compared to traditional fully-supervised learning. Overall, our work contributes new insights into how energy-based models can improve semi-supervised learning and generalization across diverse domains.",1
"Sign language is a gesture based symbolic communication medium among speech and hearing impaired people. It also serves as a communication bridge between non-impaired population and impaired population. Unfortunately, in most situations a non-impaired person is not well conversant in such symbolic languages which restricts natural information flow between these two categories of population. Therefore, an automated translation mechanism can be greatly useful that can seamlessly translate sign language into natural language. In this paper, we attempt to perform recognition on 30 basic Indian sign gestures. Gestures are represented as temporal sequences of 3D depth maps each consisting of 3D coordinates of 20 body joints. A recurrent neural network (RNN) is employed as classifier. To improve performance of the classifier, we use geometric transformation for alignment correction of depth frames. In our experiments the model achieves 84.81% accuracy.",0
This paper presents a novel approach for recognizing sign language gestures using recurrent neural networks (RNN) trained on data captured by depth sensors such as Microsoft’s Kinect camera. We train our network on a dataset consisting of 3D point cloud representations of hand movements taken from different angles. Our model takes these point clouds as input and outputs the corresponding sign gesture recognized at each frame of video footage. By incorporating prior knowledge of human kinematics into the RNN architecture through gait parameterization we achieve state of the art accuracy across all datasets. Through analysis of confusion matrices and ablation studies we can show that this method greatly outperforms traditional computer vision techniques such as feature extraction and clustering. Additionally our method generalizes well across many variants of sign languages making it adaptive to different cultures while simultaneously achieving high levels of recognition accuracy. We believe this work has significant impact on applications which require robust and accurate recognition of non verbal communication particularly those used within deaf communities where alternative means of communication are essential.,1
"The transformer has been extensively used in research domains such as computer vision, image processing, and natural language processing. The transformer, however, has not been actively used in graph neural networks. To this end, we introduce a transformer-based advanced GNN model, named UGformer, to learn graph representations. In particular, given an input graph, we present two UGformer variants. The first variant is to leverage the transformer on a set of sampled neighbors for each node, while the second is to leverage the transformer directly on the input graph. Experimental results demonstrate that our UGformer achieves state-of-the-art accuracies on well-known benchmark datasets for graph classification and inductive text classification. The code is available on Github: \url{https://github.com/daiquocnguyen/Graph-Transformer}.",0
"Title: ""Universal Graph Transformer Self-Attention Networks""  Abstract:  Recently there has been increasing interest in graph-based machine learning methods due to their ability to model complex relationships between data points in high dimensions. One promising approach is self-attention networks (SAN), which have been shown to achieve state-of-the-art performance on many natural language processing tasks by dynamically weighing the importance of different parts of input sequences. In this work, we present universal graph transformer SANs (UGTSAN) as a general framework that can handle any type of graphs while achieving competitive results across a wide range of benchmark datasets. Our UGTSAN architecture unifies recent advances in graph attention mechanisms by incorporating edge features into the self-attention mechanism using flexible graph transformers. We show experimentally that our method outperforms other approaches and provides robustness even under challenging conditions such as noisy edge features. Additionally, we analyze the behavior of UGTSAN and provide insights into how edge information influences attention patterns during inference. Overall, our research demonstrates the effectiveness of UGTSAN and represents a significant step forward towards bridging the gap between traditional node-focused GNNs and powerful SANs.",1
"We propose a novel lightweight generative adversarial network for efficient image manipulation using natural language descriptions. To achieve this, a new word-level discriminator is proposed, which provides the generator with fine-grained training feedback at word-level, to facilitate training a lightweight generator that has a small number of parameters, but can still correctly focus on specific visual attributes of an image, and then edit them without affecting other contents that are not described in the text. Furthermore, thanks to the explicit training signal related to each word, the discriminator can also be simplified to have a lightweight structure. Compared with the state of the art, our method has a much smaller number of parameters, but still achieves a competitive manipulation performance. Extensive experimental results demonstrate that our method can better disentangle different visual attributes, then correctly map them to corresponding semantic words, and thus achieve a more accurate image modification using natural language descriptions.",0
"""This research presents the development of lightweight generative adversarial networks (GAN) for text-guided image manipulation. The proposed method utilizes GANs as well as several techniques to reduce computational cost while maintaining high levels of performance. These techniques include reducing the number of model parameters and using low-rank decomposition. The results show that the method can generate images from scratch based on input text descriptions and perform effective editing tasks such as semantic feature enhancement, object removal, attribute modification and others without significant degradation compared to state-of-the-art models. This approach has potential applications across different domains including computer graphics, VFX, photo retouching, creative industries, etc.""",1
"Latent Dirichlet Allocation (LDA) is a topic model widely used in natural language processing and machine learning. Most approaches to training the model rely on iterative algorithms, which makes it difficult to run LDA on big corpora that are best analyzed in parallel and distributed computational environments. Indeed, current approaches to parallel inference either don't converge to the correct posterior or require storage of large dense matrices in memory. We present a novel sampler that overcomes both problems, and we show that this sampler is faster, both empirically and theoretically, than previous Gibbs samplers for LDA. We do so by employing a novel P\'olya-urn-based approximation in the sparse partially collapsed sampler for LDA. We prove that the approximation error vanishes with data size, making our algorithm asymptotically exact, a property of importance for large-scale topic models. In addition, we show, via an explicit example, that - contrary to popular belief in the topic modeling literature - partially collapsed samplers can be more efficient than fully collapsed samplers. We conclude by comparing the performance of our algorithm with that of other approaches on well-known corpora.",0
"In recent years, there has been increasing interest in developing models that can capture complex relationships among multiple variables. One such model is latent dirichlet allocation (LDA), which has proven successful in natural language processing tasks like text generation and topic modeling. However, LDA suffers from several limitations, including high computational cost, slow convergence, poor scalability, and difficulty dealing with missing data. To address these issues, we propose a novel algorithm called ""Pólya Urn Latent Dirichlet Allocation"" (PULDA). Our approach builds on classic sampling techniques and uses a double sparsity prior to improve performance and scalability. We demonstrate the effectiveness of our method using synthetic and real datasets, showing significant improvements over traditional approaches in terms of speed, accuracy, and robustness to missing values. Our work offers new opportunities for researchers and practitioners working in machine learning, data mining, and related fields to tackle more challenging problems involving large amounts of data and missing entries.",1
"Most recent successes on forecasting the people motion are based on LSTM models and all most recent progress has been achieved by modelling the social interaction among people and the people interaction with the scene. We question the use of the LSTM models and propose the novel use of Transformer Networks for trajectory forecasting. This is a fundamental switch from the sequential step-by-step processing of LSTMs to the only-attention-based memory mechanisms of Transformers. In particular, we consider both the original Transformer Network (TF) and the larger Bidirectional Transformer (BERT), state-of-the-art on all natural language processing tasks. Our proposed Transformers predict the trajectories of the individual people in the scene. These are ""simple"" model because each person is modelled separately without any complex human-human nor scene interaction terms. In particular, the TF model without bells and whistles yields the best score on the largest and most challenging trajectory forecasting benchmark of TrajNet. Additionally, its extension which predicts multiple plausible future trajectories performs on par with more engineered techniques on the 5 datasets of ETH + UCY. Finally, we show that Transformers may deal with missing observations, as it may be the case with real sensor data. Code is available at https://github.com/FGiuliari/Trajectory-Transformer.",0
"In recent years, there has been increasing interest in developing models capable of predicting future trajectories for objects in videos. This task is challenging due to the complex motion patterns exhibited by many types of objects, as well as the large number of potential factors that can influence their movements over time. One promising approach to tackling these issues is through the use of transformer networks, which have demonstrated impressive performance on several natural language processing tasks. In this work, we apply transformer architectures to the problem of trajectory forecasting and evaluate them on a variety of datasets. Our results show that our method achieves state-of-the art accuracy across all benchmarks considered, suggesting that transformers may provide a powerful tool for modeling spatio-temporal data. We conclude by discussing some possible extensions of our work and applications of our proposed framework in other areas.",1
"We are interested in understanding how well Transformer language models (TLMs) can perform reasoning tasks when trained on knowledge encoded in the form of natural language. We investigate their systematic generalization abilities on a logical reasoning task in natural language, which involves reasoning over relationships between entities grounded in first-order logical proofs. Specifically, we perform soft theorem-proving by leveraging TLMs to generate natural language proofs. We test the generated proofs for logical consistency, along with the accuracy of the final inference. We observe length-generalization issues when evaluated on longer-than-trained sequences. However, we observe TLMs improve their generalization performance after being exposed to longer, exhaustive proofs. In addition, we discover that TLMs are able to generalize better using backward-chaining proofs compared to their forward-chaining counterparts, while they find it easier to generate forward chaining proofs. We observe that models that are not trained to generate proofs are better at generalizing to problems based on longer proofs. This suggests that Transformers have efficient internal reasoning strategies that are harder to interpret. These results highlight the systematic generalization behavior of TLMs in the context of logical reasoning, and we believe this work motivates deeper inspection of their underlying reasoning strategies.",0
"This paper proposes a novel approach for measuring systematic generalization in neural proof generation with transformers (NPGT). NPGT systems generate logical proofs automatically by learning from human-written proofs. However, evaluating these models poses unique challenges due to their complex nature. Our method addresses these difficulties by using human evaluation to measure systematicity in NPGT output. We first collect a dataset of human-generated proof steps that exhibit different types of reasoning, including both systematic and unsystematic ones. Then we use a pretrained language model to rank the steps according to their perceived level of systematization. Finally, we evaluate our metric on several NPGT models trained with diverse methods and data sizes, demonstrating good alignment with human judgments. Our work provides insights into how humans reason about systematicity in proving and contributes towards building more effective NPGT systems.",1
"Every year physicians face an increasing demand of image-based diagnosis from patients, a problem that can be addressed with recent artificial intelligence methods. In this context, we survey works in the area of automatic report generation from medical images, with emphasis on methods using deep neural networks, with respect to: (1) Datasets, (2) Architecture Design, (3) Explainability and (4) Evaluation Metrics. Our survey identifies interesting developments, but also remaining challenges. Among them, the current evaluation of generated reports is especially weak, since it mostly relies on traditional Natural Language Processing (NLP) metrics, which do not accurately capture medical correctness.",0
"This survey provides an overview of recent advances in deep learning techniques for medical image analysis and automated report generation. We discuss current methods for designing explainable artificial intelligence (AI) models that can generate accurate and interpretable results for tasks such as disease classification and segmentation. Our review highlights key challenges faced by existing systems, including the need for more robust evaluation metrics and better understanding of human perception processes. To address these issues, we explore promising directions for future research, including the development of novel model architectures, new training strategies, and improved human-machine interfaces. Overall, our work seeks to advance the state-of-the-art in automatic image-based medical reporting through a deeper understanding of how humans interact with deep learning algorithms.",1
"First-order stochastic optimization methods are currently the most widely used class of methods for training deep neural networks. However, the choice of the optimizer has become an ad-hoc rule that can significantly affect the performance. For instance, SGD with momentum (SGD+M) is typically used in computer vision (CV) and Adam is used for training transformer models for Natural Language Processing (NLP). Using the wrong method can lead to significant performance degradation. Inspired by the dual averaging algorithm, we propose Modernized Dual Averaging (MDA), an optimizer that is able to perform as well as SGD+M in CV and as Adam in NLP. Our method is not adaptive and is significantly simpler than Adam. We show that MDA induces a decaying uncentered $L_2$-regularization compared to vanilla SGD+M and hypothesize that this may explain why it works on NLP problems where SGD+M fails.",0
"Recent advances in deep learning have led to significant improvements in many areas of artificial intelligence such as computer vision, natural language processing, and speech recognition. However, optimizing these complex models can pose a challenge due to their large number of parameters and often non-convex objective functions. In this work, we show that a simple yet effective algorithm called dual averaging can lead to surprisingly good results on several popular benchmark datasets. We compare our results against state-of-the-art methods from both industry and academia, demonstrating competitive performance across a wide range of architectures and tasks. Our findings suggest that dual averaging deserves further investigation as a general-purpose optimization technique for training neural networks, particularly given its ease of implementation and scalability. Additionally, our analysis sheds light on some of the factors that influence optimal hyperparameters, which could inform future research on neural network design. Overall, our work highlights the potential of simple algorithms like dual averaging for solving challenging problems in the field of machine learning.",1
"Many optimizers have been proposed for training deep neural networks, and they often have multiple hyperparameters, which make it tricky to benchmark their performance. In this work, we propose a new benchmarking protocol to evaluate both end-to-end efficiency (training a model from scratch without knowing the best hyperparameter) and data-addition training efficiency (the previously selected hyperparameters are used for periodically re-training the model with newly collected data). For end-to-end efficiency, unlike previous work that assumes random hyperparameter tuning, which over-emphasizes the tuning time, we propose to evaluate with a bandit hyperparameter tuning strategy. A human study is conducted to show that our evaluation protocol matches human tuning behavior better than the random search. For data-addition training, we propose a new protocol for assessing the hyperparameter sensitivity to data shift. We then apply the proposed benchmarking framework to 7 optimizers and various tasks, including computer vision, natural language processing, reinforcement learning, and graph mining. Our results show that there is no clear winner across all the tasks.",0
"Title: Progress in Neural Network Training: An Assessment of Modern Optimization Techniques  Neural networks have become increasingly popular due to their ability to perform complex tasks such as image classification and natural language processing. However, optimizing these models remains a challenge, particularly given the large number of hyperparameters that must be tuned manually. Many different optimization techniques have been proposed over time to address this issue, ranging from simple gradient descent methods to more advanced algorithms like stochastic gradient descent (SGD) and Adam.  In order to evaluate the effectiveness of these approaches and determine how far we have come in terms of model training, we propose a new evaluation protocol. Our method involves comparing several state-of-the-art optimization algorithms on a variety of datasets commonly used in machine learning research, including MNIST, CIFAR-10, and ImageNet. We assess each algorithm according to both accuracy metrics and computational efficiency measures, allowing us to provide a comprehensive analysis of their relative strengths and weaknesses.  Our results indicate that while there has been significant progress in optimizer design, many challenges still remain. Some of the most promising recent advancements include adaptive gradient methods like Adagrad and AdaDelta, which can automatically adjust step sizes based on the curvature of the loss surface. These algorithms consistently outperformed other popular techniques like SGD and Adam across all three benchmark datasets.  Overall, our findings demonstrate that while significant strides have been taken in neural network training, there remains ample room for further improvement. By evaluating existing techniques through our new protocol and identifying areas where current methods fall short, researchers can work towards developing even better optimization strategies. This will ultimately lead to even more accurate and efficient machine learning models, benefitting a wide range of applications.",1
"Pooling operations have shown to be effective on computer vision and natural language processing tasks. One challenge of performing pooling operations on graph data is the lack of locality that is not well-defined on graphs. Previous studies used global ranking methods to sample some of the important nodes, but most of them are not able to incorporate graph topology. In this work, we propose the topology-aware pooling (TAP) layer that explicitly considers graph topology. Our TAP layer is a two-stage voting process that selects more important nodes in a graph. It first performs local voting to generate scores for each node by attending each node to its neighboring nodes. The scores are generated locally such that topology information is explicitly considered. In addition, graph topology is incorporated in global voting to compute the importance score of each node globally in the entire graph. Altogether, the final ranking score for each node is computed by combining its local and global voting scores. To encourage better graph connectivity in the sampled graph, we propose to add a graph connectivity term to the computation of ranking scores. Results on graph classification tasks demonstrate that our methods achieve consistently better performance than previous methods.",0
"Graph Neural Networks (GNNs) have emerged as powerful models for graph data, such as social networks, molecular graphs, and knowledge graphs. GNNs learn representations by propagating signals along edges in the input graphs, where each node can aggregate context from its neighbors. In practice, pooling operations are often applied after multiple layers of message passing to reduce the size of the representation while preserving relevant structural information. Existing graph pooling methods typically operate at different levels of abstraction than message passing: they collapse nodes into supernodes without considering their relative positions in the original graph structure. We present topologically-aware graph pooling (TAGPool), which preserves both structural properties and learned embeddings during aggregation and collapse. Our method performs explicit clustering on the nodes before aggregation, based on some similarity measure that takes into account local connectivity patterns as well as global distances between nodes. By incorporating edge weights and node features throughout the entire process, our approach allows TAGPool to capture more detailed relationships between nearby nodes and distantly connected communities, leading to improved performance compared to other pooling methods on several benchmark datasets. Additionally, we show how pooling reduces computational complexity and memory requirements for large graphs, making inference faster and more scalable. Overall, TAGPool represents an important step forward for efficient graph neural network architecture design, particularly in domains where model interpretability is critical and high spatio-structural fidelity must be maintained.",1
"In the computational prediction of chemical compound properties, molecular descriptors and fingerprints encoded to low dimensional vectors are used. The selection of proper molecular descriptors and fingerprints is both important and challenging as the performance of such models is highly dependent on descriptors. To overcome this challenge, natural language processing models that utilize simplified molecular input line-entry system as input were studied, and several transformer-variant models achieved superior results when compared with conventional methods. In this study, we explored the structural differences of the transformer-variant model and proposed a new self-attention based model. The representation learning performance of the self-attention module was evaluated in a multi-task learning environment using imbalanced chemical datasets. The experiment results showed that our model achieved competitive outcomes on several benchmark datasets. The source code of our experiment is available at https://github.com/arwhirang/sa-mtl and the dataset is available from the same URL.",0
"Abstract: This research develops a methodology that utilizes self attention multi task learning (SML) algorithms in order to accurately predict chemical properties based on SMILE representations. Our approach effectively addresses challenges faced by previous methods such as model interpretability and generalization ability by focusing on feature interactions within molecules via self attention mechanisms. We demonstrate that our framework outperforms state-of-the art models across multiple benchmark datasets through extensive experiments. Moreover, we provide comprehensive analysis on both molecular level and feature level contributions which highlight the effectiveness of our proposed methodology. These findings have significant implications for the development of novel machine learning approaches for drug discovery and design applications where accurate prediction of chemical properties is crucial.",1
"In natural language processing (NLP), enormous pre-trained models like BERT have become the standard starting point for training on a range of downstream tasks, and similar trends are emerging in other areas of deep learning. In parallel, work on the lottery ticket hypothesis has shown that models for NLP and computer vision contain smaller matching subnetworks capable of training in isolation to full accuracy and transferring to other tasks. In this work, we combine these observations to assess whether such trainable, transferrable subnetworks exist in pre-trained BERT models. For a range of downstream tasks, we indeed find matching subnetworks at 40% to 90% sparsity. We find these subnetworks at (pre-trained) initialization, a deviation from prior NLP research where they emerge only after some amount of training. Subnetworks found on the masked language modeling task (the same task used to pre-train the model) transfer universally; those found on other tasks transfer in a limited fashion if at all. As large-scale pre-training becomes an increasingly central paradigm in deep learning, our results demonstrate that the main lottery ticket observations remain relevant in this context. Codes available at https://github.com/VITA-Group/BERT-Tickets.",0
"The Lottery Ticket Hypothesis states that even relatively small neural networks can match the performance of larger ones if properly pruned. In recent work we show how lottery tickets provide new insight into the structure of modern pre-training algorithms, but there remains room for improvement with respect to their ability to scale up. Here we study how structured knowledge encoded by powerful generative language models improves the quality of transferred knowledge, enabling larger language models such as GPT-2 to benefit from transfer learning. We find that incorporating external knowledge significantly boosts the accuracy of fine tuning on natural language understanding tasks, demonstrating the importance of explicitly reasoning over symbolic data. These results have implications both for our understanding of how artificial intelligence operates, as well as suggestions for future research directions aimed at developing systems which more closely mimick human cognition.",1
"In recent years, text-guided image manipulation has gained increasing attention in the image generation research field. Recent works have proposed to deal with a simplified setting where the input image only has a single object and the text modification is acquired by swapping image captions or labels. In this paper, we study a setting that allows users to edit an image with multiple objects using complex text instructions. In this image generation task, the inputs are a reference image and an instruction in natural language that describes desired modifications to the input image. We propose a GAN-based method to tackle this problem. The key idea is to treat text as neural operators to locally modify the image feature. We show that the proposed model performs favorably against recent baselines on three public datasets.",0
In recent years there has been significant progress towards creating large scale generative models that can synthesize novel image data from textual descriptions or other modalities. However many works still rely on powerful pretraining techniques like unpaired adversarial training to generate visually appealing images from simple text prompts which may often yield confusing results. This work explores using contrastive learning as an alternative to train these models without relying on such external signaling methods like GANs. By minimizing reconstruction error between pairs of augmented views we create a discriminator capable of producing diverse visual outputs for given natural language inputs which rival current state of the art results. We evaluate our method on three widely used benchmark datasets and perform both quantitative analysis and human judgement studies indicating that our approach outperforms previous methods under certain conditions when trained properly. Overall our contributions improve interpretability and performance of image generation models while greatly simplifying their training process. Further research should examine how to better integrate these advancements into broader applications involving NLP.,1
"Vision-and-Language Navigation (VLN) is a natural language grounding task where an agent learns to follow language instructions and navigate to specified destinations in real-world environments. A key challenge is to recognize and stop at the correct location, especially for complicated outdoor environments. Existing methods treat the STOP action equally as other actions, which results in undesirable behaviors that the agent often fails to stop at the destination even though it might be on the right path. Therefore, we propose Learning to Stop (L2Stop), a simple yet effective policy module that differentiates STOP and other actions. Our approach achieves the new state of the art on a challenging urban VLN dataset Touchdown, outperforming the baseline by 6.89% (absolute improvement) on Success weighted by Edit Distance (SED).",0
"In recent years there has been increasing interest in using natural language as an interface for controlling robots and other agents operating in complex environments such as cities. While significant progress has been made towards enabling autonomous urban navigation via vision alone, incorporating human level intelligence through language is still largely unexplored due to difficulties in real time interaction with large scale datasets. This work proposes the integration of deep learning methods specifically designed for image generation into language guided urban driving tasks which allow for quick adaptation while ensuring safety during testing. We demonstrate that our approach significantly outperforms state-of-the-art end-to-end trained systems on challenging real world trajectories without any explicit fine tuning or additional data collection. Our framework provides a simple and efficient solution towards developing intelligent vehicles capable of interpreting human instruction at runtime.",1
"Neural Networks and Deep Learning have seen an upsurge of research in the past decade due to the improved results. Generates text from the given image is a crucial task that requires the combination of both sectors which are computer vision and natural language processing in order to understand an image and represent it using a natural language. However existing works have all been done on a particular lingual domain and on the same set of data. This leads to the systems being developed to perform poorly on images that belong to specific locales' geographical context. TextMage is a system that is capable of understanding visual scenes that belong to the Bangladeshi geographical context and use its knowledge to represent what it understands in Bengali. Hence, we have trained a model on our previously developed and published dataset named BanglaLekhaImageCaptions. This dataset contains 9,154 images along with two annotations for each image. In order to access performance, the proposed model has been implemented and evaluated.",0
"""Captions have become important pieces of visual content that complement images by providing context and enhancing user experience. In recent years, advancements in deep learning technologies have made possible automatic image captioning systems capable of generating natural language descriptions of photos. While these models have been trained on English data, little work has focused on other languages such as Bangla which is widely spoken across South Asia. This paper proposes a novel model called TextMage that is tailored specifically for automatically generating high quality Bangla captions from images. Our approach combines state-of-the art object detection techniques with recurrent neural networks (RNN) and long short term memory (LSTM) architectures to generate accurate and concise captions. Experimental results show that our model significantly outperforms existing methods achieving a higher BLEU score compared to two strong baseline models.""",1
"We study learning of indexed families from positive data where a learner can freely choose a hypothesis space (with uniformly decidable membership) comprising at least the languages to be learned. This abstracts a very universal learning task which can be found in many areas, for example learning of (subsets of) regular languages or learning of natural languages. We are interested in various restrictions on learning, such as consistency, conservativeness or set-drivenness, exemplifying various natural learning restrictions.   Building on previous results from the literature, we provide several maps (depictions of all pairwise relations) of various groups of learning criteria, including a map for monotonicity restrictions and similar criteria and a map for restrictions on data presentation. Furthermore, we consider, for various learning criteria, whether learners can be assumed consistent.",0
"In this paper, we present a new method for indexing learning data that allows users to easily navigate and search through large amounts of educational material. Our approach uses maps as visual representations of the content, making it easy to see relationships and connections within the data. We demonstrate how these maps can be used to support exploratory learning and inquiry by enabling learners to follow their interests and engage with the materials in a more meaningful way. Our experiments show that our method effectively improves learning outcomes while reducing cognitive load. By providing a flexible tool for organizing and accessing knowledge, we aim to empower educators and students alike to unlock their full potential and achieve greater success.",1
"The quality of datasets is one of the key factors that affect the accuracy of aerodynamic data models. For example, in the uniformly sampled Burgers' dataset, the insufficient high-speed data is overwhelmed by massive low-speed data. Predicting high-speed data is more difficult than predicting low-speed data, owing to that the number of high-speed data is limited, i.e. the quality of the Burgers' dataset is not satisfactory. To improve the quality of datasets, traditional methods usually employ the data resampling technology to produce enough data for the insufficient parts in the original datasets before modeling, which increases computational costs. Recently, the mixtures of experts have been used in natural language processing to deal with different parts of sentences, which provides a solution for eliminating the need for data resampling in aerodynamic data modeling. Motivated by this, we propose the multi-task learning (MTL), a datasets quality-adaptive learning scheme, which combines task allocation and aerodynamic characteristics learning together to disperse the pressure of the entire learning task. The task allocation divides a whole learning task into several independent subtasks, while the aerodynamic characteristics learning learns these subtasks simultaneously to achieve better precision. Two experiments with poor quality datasets are conducted to verify the data quality-adaptivity of the MTL to datasets. The results show than the MTL is more accurate than FCNs and GANs in poor quality datasets.",0
"This paper presents the results of our research into using multi-task learning (MTL) techniques to improve aerodynamic data predictions. We show that by applying MTL methods, we can achieve more accurate predictions than traditional single task models. Our approach involves training the model on multiple tasks simultaneously, allowing it to learn better representations of the underlying features which then improves accuracy on downstream tasks such as predicting lift force coefficients, drag forces, or other relevant parameters of interest. Experimental results demonstrate that the proposed method significantly outperforms standard approaches across all evaluation metrics considered, making it a viable solution for real world applications where high quality predictions are critical.",1
"Despite the widely reported success of embedding-based machine learning methods on natural language processing tasks, the use of more easily interpreted engineered features remains common in fields such as cognitive impairment (CI) detection. Manually engineering features from noisy text is time and resource consuming, and can potentially result in features that do not enhance model performance. To combat this, we describe a new approach to feature engineering that leverages sequential machine learning models and domain knowledge to predict which features help enhance performance. We provide a concrete example of this method on a standard data set of CI speech and demonstrate that CI classification accuracy improves by 2.3% over a strong baseline when using features produced by this method. This demonstration provides an ex-ample of how this method can be used to assist classification in fields where interpretability is important, such as health care.",0
"In recent years, cognitive impairment has become an increasingly prevalent health concern worldwide due to the aging population. With early detection crucial for effective treatment and management, there exists a pressing need for accurate and reliable methods to identify individuals at risk. One promising approach involves analyzing subtle changes in speech patterns through feature extraction and subsequence classification. However, few studies have explored the effectiveness of this methodology on large datasets. To address this gap, we conducted a comprehensive analysis of two extensive databases containing audio recordings from individuals with mild cognitive impairment (MCI), Alzheimer's disease (AD), and normal controls. Our results demonstrate that our proposed model outperforms previous approaches by accurately classifying participants across all three groups with high sensitivity and specificity. Moreover, we identified key features that contribute to the most significant differences among these cohorts. These findings hold important clinical implications as our method can potentially serve as a noninvasive diagnostic tool for detecting cognitive decline in routine medical settings. Further research is necessary to establish its generalizability beyond the study populations; nevertheless, this work represents a critical step towards enhancing the early identification of individuals with MCI and AD. Ultimately, this guided approach could lead to better patient outcomes, increased resource allocation, and lower societal costs associated with managing cognitive disorders.",1
"This paper studies the task of temporal moment localization in a long untrimmed video using natural language query. Given a query sentence, the goal is to determine the start and end of the relevant segment within the video. Our key innovation is to learn a video feature embedding through a language-conditioned message-passing algorithm suitable for temporal moment localization which captures the relationships between humans, objects and activities in the video. These relationships are obtained by a spatial sub-graph that contextualizes the scene representation using detected objects and human features conditioned in the language query. Moreover, a temporal sub-graph captures the activities within the video through time. Our method is evaluated on three standard benchmark datasets, and we also introduce YouCookII as a new benchmark for this task. Experiments show our method outperforms state-of-the-art methods on these datasets, confirming the effectiveness of our approach.",0
"Automatically generated abstract from input... A natural language query issued against videos often requires localizing specific moments within them that can best respond to user intents. To accomplish moment localization effectively while satisfying diverse user requests, we need to locate semantically meaningful yet concise content units such as events, actions, entities, attributes, scenes, etc., rather than isolated objects of interest mentioned in textual prompts directly. In contrast to popular object detection pipelines relying on pre-trained detectors to first identify regions containing objects specified by users, our approach discovers object relationships through spatiotemporal reasoning across video frames based on high-level representations, then predicts the relevance of corresponding candidate segments automatically. We cast scene decomposition into a densely connected neural network and formulate each event or attribute inference as selecting a subset of nodes within individual layers or across multiple scales. Our framework achieves end-to-end learning towards unified moment retrieval under various modalities such as image, clip, segment or even sentence queries without domain-specific fine-tuning. Besides outperforming state-of-the-art methods under several evaluation metrics and demonstrating superior robustness in open VQA, we present human case studies where detailed explanations clarify how our method could significantly enhance user experience when interacting with complex multimedia data repositories for both exploration and presentation purposes. Overall, our findings highlight the importance of joint reasoning over space, time and semantics at different levels of granularity when resolving challenges in multimedia understanding and generating descriptive visual responses via human-like interactions.",1
"Most existing work that grounds natural language phrases in images starts with the assumption that the phrase in question is relevant to the image. In this paper we address a more realistic version of the natural language grounding task where we must both identify whether the phrase is relevant to an image and localize the phrase. This can also be viewed as a generalization of object detection to an open-ended vocabulary, introducing elements of few- and zero-shot detection. We propose an approach for this task that extends Faster R-CNN to relate image regions and phrases. By carefully initializing the classification layers of our network using canonical correlation analysis (CCA), we encourage a solution that is more discerning when reasoning between similar phrases, resulting in over double the performance compared to a naive adaptation on three popular phrase grounding datasets, Flickr30K Entities, ReferIt Game, and Visual Genome, with test-time phrase vocabulary sizes of 5K, 32K, and 159K, respectively.",0
"Incorporate the following key terms: phrase detection, open-ended queries, image-language networks, visual attention mechanisms, fine-grained analysis, deep contextual understanding, natural language processing. Use the past tense throughout except where specified otherwise. Provide at least one sentence outlining future work that could build on the proposed model/approach but preferably more if possible! Feel free to include any additional relevant details about state-of-the art systems or potential applications as well within this space limit constraint. This abstract should set up the content sections of our full length research article accordingly. Please keep in mind that I am looking for concise text without unnecessary repetition. Thank you so much! Here is an example of how I would like my references styled citation [1]. If there are more than one citations to include at once please indicate with square brackets e.g.[2],[4],[7] but feel free to format them differently from my example if better suited overall! Let me know and thanks again! --Cecibel-- The ability to detect phrases from images holds great potential for bridging the gap between human languages and computer vision. Current techniques for phrase detection rely heavily upon predefined templates and object detections, which fail to capture the nuances present in complex scenes. Recent advancements in image-language models have sought to address these limitations through fine-grained visual attention mechanisms and deep contextual understanding of both image and text data. However, existing approaches still struggle with the challenges presented by open-ended queries and real-world scenarios, as they require heavy computational resources and may generate irrelevant responses. Our proposed approach revisits the notion of image-langua",1
"Sign language translation (SLT) aims to interpret sign video sequences into text-based natural language sentences. Sign videos consist of continuous sequences of sign gestures with no clear boundaries in between. Existing SLT models usually represent sign visual features in a frame-wise manner so as to avoid needing to explicitly segmenting the videos into isolated signs. However, these methods neglect the temporal information of signs and lead to substantial ambiguity in translation. In this paper, we explore the temporal semantic structures of signvideos to learn more discriminative features. To this end, we first present a novel sign video segment representation which takes into account multiple temporal granularities, thus alleviating the need for accurate video segmentation. Taking advantage of the proposed segment representation, we develop a novel hierarchical sign video feature learning method via a temporal semantic pyramid network, called TSPNet. Specifically, TSPNet introduces an inter-scale attention to evaluate and enhance local semantic consistency of sign segments and an intra-scale attention to resolve semantic ambiguity by using non-local video context. Experiments show that our TSPNet outperforms the state-of-the-art with significant improvements on the BLEU score (from 9.58 to 13.41) and ROUGE score (from 31.80 to 34.96)on the largest commonly-used SLT dataset. Our implementation is available at https://github.com/verashira/TSPNet.",0
"This work presents a novel hierarchical approach to feature learning for sign language translation using temporal semantic pyramids (TSPs). Our proposed method, TSPNet, utilizes these hierarchically decomposed representations to capture spatial and temporal relationships within individual frames as well as across multiple time steps. We demonstrate that our model outperforms baseline models on two benchmark datasets by achieving state-of-the-art results. Through extensive ablative experiments, we show that each component of our model plays a crucial role in improving performance. Additionally, we conduct visualization studies to gain insight into the learned features and verify their effectiveness at capturing high-level concepts such as classemes and hand shapes. Overall, our work provides valuable contributions to both the field of machine learning and human computer interaction for individuals who communicate through American Sign Language.",1
"Models based on self-attention mechanisms have been successful in analyzing temporal data and have been widely used in the natural language domain. We propose a new model architecture for video face representation and recognition based on a self-attention mechanism. Our approach could be used for video with single and multiple identities. To the best of our knowledge, no one has explored the aggregation approaches that consider the video with multiple identities. The proposed approach utilizes existing models to get the face representation for each video frame, e.g., ArcFace and MobileFaceNet, and the aggregation module produces the aggregated face representation vector for video by taking into consideration the order of frames and their quality scores. We demonstrate empirical results on a public dataset for video face recognition called IJB-C to indicate that the self-attention aggregation network (SAAN) outperforms naive average pooling. Moreover, we introduce a new multi-identity video dataset based on the publicly available UMDFaces dataset and collected GIFs from Giphy. We show that SAAN is capable of producing a compact face representation for both single and multiple identities in a video. The dataset and source code will be publicly available.",0
"This study presents a new approach for representing and recognizing faces in videos using self-attention aggregation networks (SANs). With recent advancements in deep learning, there has been a growing interest in developing efficient methods for handling temporal data such as videos. However, most existing approaches rely on recurrent neural networks (RNNs) which can suffer from vanishing gradients and exploding activations when dealing with long sequence lengths. To address these issues, SANs were introduced as an alternative architecture that can effectively model sequential data without suffering from these problems. In this work, we apply SANs to the task of video face representation and recognition, achieving state-of-the-art results on several benchmark datasets. Our approach first extracts features from each frame in the video using a convolutional neural network (CNN), then applies a SAN module to aggregate the information across frames into a single high-dimensional vector. Finally, this vector is used as input to a standard CNN classifier for face verification. We evaluate our method on three publicly available datasets - YTF, CASIA NIR-VIS, and AgeDB - and show significant improvements over previous methods. Overall, our findings demonstrate the effectiveness of SANs for video face representation and recognition tasks, paving the way for future research in this area.",1
"We propose a novel type of balanced clustering algorithm to approximate attention. Attention complexity is reduced from $O(N^2)$ to $O(N \log N)$, where $N$ is the sequence length. Our algorithm, SMYRF, uses Locality Sensitive Hashing (LSH) in a novel way by defining new Asymmetric transformations and an adaptive scheme that produces balanced clusters. The biggest advantage of SMYRF is that it can be used as a drop-in replacement for dense attention layers without any retraining. On the contrary, prior fast attention methods impose constraints (e.g. queries and keys share the same vector representations) and require re-training from scratch. We apply our method to pre-trained state-of-the-art Natural Language Processing and Computer Vision models and we report significant memory and speed benefits. Notably, SMYRF-BERT outperforms (slightly) BERT on GLUE, while using $50\%$ less memory. We also show that SMYRF can be used interchangeably with dense attention before and after training. Finally, we use SMYRF to train GANs with attention in high resolutions. Using a single TPU, we were able to scale attention to 128x128=16k and 256x256=65k tokens on BigGAN on CelebA-HQ.",0
"In recent years, deep learning has achieved tremendous success in many fields due to the development of advanced neural network architectures that leverage the power of attention mechanisms. These mechanisms allow models to selectively focus on relevant input features during the prediction process, resulting in more accurate and robust behavior. However, current approaches still suffer from computational complexity, memory usage, and scalability issues, especially as we move towards larger datasets and more complex tasks. To address these challenges, we introduce SMYRF (Scalable Memory-Efficient eXtreme Relevance Filtering), a novel attention mechanism that uses asymmetric clustering to drastically reduce memory footprint and computation time while improving performance over state-of-the-art methods. Through extensive experiments across diverse domains such as image classification, machine translation, and question answering, our results demonstrate that SMYRF outperforms competitive baselines by achieving up to 4x speedup in inference time and up to 20% improvement in accuracy under low resource settings. Our work demonstrates the feasibility of efficient and effective extreme relevance filtering for large-scale language processing problems without compromising quality.",1
"Deep neural networks have demonstrated their superior performance in almost every Natural Language Processing task, however, their increasing complexity raises concerns. In particular, these networks require high expenses on computational hardware, and training budget is a concern for many. Even for a trained network, the inference phase can be too demanding for resource-constrained devices, thus limiting its applicability. The state-of-the-art transformer models are a vivid example. Simplifying the computations performed by a network is one way of relaxing the complexity requirements. In this paper, we propose an end to end binarized neural network architecture for the intent classification task. In order to fully utilize the potential of end to end binarization, both input representations (vector embeddings of tokens statistics) and the classifier are binarized. We demonstrate the efficiency of such architecture on the intent classification of short texts over three datasets and for text classification with a larger dataset. The proposed architecture achieves comparable to the state-of-the-art results on standard intent classification datasets while utilizing ~ 20-40% lesser memory and training time. Furthermore, the individual components of the architecture, such as binarized vector embeddings of documents or binarized classifiers, can be used separately with not necessarily fully binary architectures.",0
"Abstract: Binary classification using deep learning techniques has recently gained popularity due to their ability to learn complex representations directly from raw input data such as images or text. In particular, end to end binarized neural networks have shown promising results on several benchmark datasets while requiring significantly less computational resources compared to non-binary models that use full precision floating point weights. This work presents a detailed study comparing two different architectures; binary convolutional neural network (BCNN) and a novel binarized variant of a transformer architecture called Binary Transformer (BT), across multiple natural language processing tasks including sentiment analysis, question answering and machine translation. Our experiments demonstrate that both BCNN and BT achieve competitive accuracies to their corresponding full precision baselines, albeit at substantially reduced model sizes. Furthermore we showcase how these binarised models can effectively scale up to larger dataset sizes without sacrificing accuracy. These findings suggest that binarizing neural networks could lead to significant improvements in terms of memory efficiency and computational cost, making them more accessible for widespread deployment on resource constrained devices. Additionally our results provide insights into choosing optimal hyperparameters based on specific NLP task requirements.",1
"A visual relationship denotes a relationship between two objects in an image, which can be represented as a triplet of (subject; predicate; object). Visual relationship detection is crucial for scene understanding in images. Existing visual relationship detection datasets only contain true relationships that correctly describe the content in an image. However, distinguishing false visual relationships from true ones is also crucial for image understanding and grounded natural language processing. In this paper, we construct a visual relationship authenticity dataset, where both true and false relationships among all objects appeared in the captions in the Flickr30k entities image caption dataset are annotated. The dataset is available at https://github.com/codecreator2053/VR_ClassifiedDataset. We hope that this dataset can promote the study on both vision and language understanding.",0
"Abstract:  Visual relationship authenticity refers to the extent to which visual portrayals accurately represent the nature of their subject matter. For example, photographs of natural landscapes may contain elements that have been digitally manipulated or staged, making them less accurate representations of reality. Similarly, images used in advertising often use techniques such as retouching and composition to create idealized versions of products, services, and people. In order to study and quantify the degree of visual relationship authenticity present in images, a dataset would need to be constructed containing examples ranging from highly authentic to highly fabricated scenes. However, currently there exists no comprehensive dataset specifically designed to measure the level of authenticity in visual relationships across different domains including photography, illustration, painting, drawing, cartoon and computer graphics. This paper presents a detailed description of our work on constructing the first large scale comprehensive Visual Relationship Authenticity (VRA) dataset consisting of over 26k unique images spread into six classes of VRA. The dataset was collected using crowd sourcing techniques where we asked human annotators to score each image based on the perceptual criteria like light, color accuracy, depth, shape etc. Our results show high inter-rater agreement and correlation between automated features like contrast, color accuracy etc. We also provide experiments conducted on object classification tasks trained on this data set demonstrating that it helps improve performance. Overall, we believe the proposed VRA dataset can serve as a rich resource for researchers interested in studying how authenticity impacts viewer behavior, perception and cognition, as well as applications related to digital media forensics, content verification and social sciences.",1
"Measuring similarity between IP addresses is an important task in the daily operations of any enterprise network. Applications that depend on an IP similarity measure include measuring correlation between security alerts, building baselines for behavioral modelling, debugging network failures and tracking persistent attacks. However, IPs do not have a natural similarity measure by definition. Deep Learning architectures are a promising solution here since they are able to learn numerical representations for IPs directly from data, allowing various distance measures to be applied on the calculated representations. Current works have utilized Natural Language Processing (NLP) techniques for learning IP embeddings. However, these approaches have no proper way to handle out-of-vocabulary (OOV) IPs not seen during training. In this paper, we propose a novel approach for IP embedding using an adapted graph neural network (GNN) architecture. This approach has the advantages of working on the raw data, scalability and, most importantly, induction, i.e. the ability to measure similarity between previously unseen IPs. Using data from an enterprise network, our approach is able to identify similarities between local DNS servers and root DNS servers even though some of these machines are never encountered during the training phase.",0
"Increasingly complex enterprise networks require more advanced methods for detecting malicious behavior. One common method for intrusion detection is by monitoring network traffic patterns. Previous research has used graph neural networks (GNN) to analyze similarities in large scale graph data, however most existing solutions only compare nodes within a single time snapshot. Our proposed solution uses a GNN to continuously monitor traffic flow across multiple temporal snapshots. By comparing the similarity scores from different times, we can identify abnormal flows that might indicate potential security threats. We evaluated our approach on a real-world dataset containing over one million network events from a university network spanning several months. Experimental results show that our method outperforms traditional techniques such as BGPDR, HMM, and Deep Learning algorithms like Autoencoders. Additionally, our system can successfully detect previously unknown attacks with high accuracy. This work provides valuable insights into applying machine learning techniques to improve network security measures, ultimately leading to safer online experiences for all users.",1
"Recent technological advancements in the Internet and Social media usage have resulted in the evolution of faster and efficient platforms of communication. These platforms include visual, textual and speech mediums and have brought a unique social phenomenon called Internet memes. Internet memes are in the form of images with witty, catchy, or sarcastic text descriptions. In this paper, we present a multi-modal sentiment analysis system using deep neural networks combining Computer Vision and Natural Language Processing. Our aim is different than the normal sentiment analysis goal of predicting whether a text expresses positive or negative sentiment; instead, we aim to classify the Internet meme as a positive, negative, or neutral, identify the type of humor expressed and quantify the extent to which a particular effect is being expressed. Our system has been developed using CNN and LSTM and outperformed the baseline score.",0
"This work presents GundaPusuniL (GPL), our submission to SemEval-2020 Task 8: Multimodal Memotion Analysis. We describe how we built upon previous work on memotion analysis by using a multimodal approach that incorporates facial expressions, body language, vocal characteristics, lexical content, sentiment analysis, and external features such as weather conditions. We outline the different steps involved in creating GPL including data collection, annotation, feature extraction, model training, and evaluation metrics. Our results show promising performance compared to other submissions in terms of F1 score and overall accuracy. Our work demonstrates the effectiveness of a multi-modal approach towards understanding emotions conveyed through memes. Overall, this study contributes to the field of computer vision and natural language processing in the context of social media platforms where memes play an important role in shaping public discourse. This project describes the development and implementation of GuntaPusnil L(GPL), an AI tool designed for the analysis of meme emotions via multi modal cues. These multi modal cues includes face expression ,body movement , voice features and external details like weather etc .The authors collect and annotate data before performing Feature extraction and trainning their machine learning models. They compare their system’s result against benchmark dataset from semEVAL-2020 task 8 and achieved good f1 scroes and over all accuracies better than most of others. With these positive results they conclude that a multi modal approach can indeed give better insight into the emotions behind memes and hence aid in sentiment analysis in social media platforms. In addition to contribution to computer vision ,nlp research area ,this technology has a wide range of applications from marketing campagns monitoring to mental health checks",1
"Natural language descriptions of user interface (UI) elements such as alternative text are crucial for accessibility and language-based interaction in general. Yet, these descriptions are constantly missing in mobile UIs. We propose widget captioning, a novel task for automatically generating language descriptions for UI elements from multimodal input including both the image and the structural representations of user interfaces. We collected a large-scale dataset for widget captioning with crowdsourcing. Our dataset contains 162,859 language phrases created by human workers for annotating 61,285 UI elements across 21,750 unique UI screens. We thoroughly analyze the dataset, and train and evaluate a set of deep model configurations to investigate how each feature modality as well as the choice of learning strategies impact the quality of predicted captions. The task formulation and the dataset as well as our benchmark models contribute a solid basis for this novel multimodal captioning task that connects language and user interfaces.",0
"This paper presents a study on generating natural language descriptions for mobile user interface elements using deep learning techniques. In particular, we propose widget captioning which generates descriptive captions for UI controls such as buttons, input fields, images, etc., that can improve accessibility and usability for visually impaired users. Our approach uses convolutional neural networks (CNN) to extract visual features from screen shots of mobile interfaces followed by recurrent neural networks (RNN) to generate natural language descriptions. We evaluate our method on two benchmark datasets consisting of Android app screenshots and web pages and demonstrate significant improvements over state-of-the-art methods in terms of both automatic metrics and human evaluation. The generated descriptions are more accurate, concise, and informative, providing better assistance for blind users. Our work provides new insights into enhancing the accessibility and usability of mobile devices through artificial intelligence technology.",1
"Bolukbasi et al. (2016) presents one of the first gender bias mitigation techniques for word embeddings. Their method takes pre-trained word embeddings as input and attempts to isolate a linear subspace that captures most of the gender bias in the embeddings. As judged by an analogical evaluation task, their method virtually eliminates gender bias in the embeddings. However, an implicit and untested assumption of their method is that the bias sub-space is actually linear. In this work, we generalize their method to a kernelized, non-linear version. We take inspiration from kernel principal component analysis and derive a non-linear bias isolation technique. We discuss and overcome some of the practical drawbacks of our method for non-linear gender bias mitigation in word embeddings and analyze empirically whether the bias subspace is actually linear. Our analysis shows that gender bias is in fact well captured by a linear subspace, justifying the assumption of Bolukbasi et al. (2016).",0
"This paper presents research exploring the effectiveness of the linear subspace hypothesis in gender bias mitigation. Recent studies have suggested that algorithms can learn harmful biases from data collected by humans, leading to discriminatory decisions made based on demographic attributes such as gender. To address these issues, a promising approach has been proposed: training machine learning models on a subset of the original dataset containing a balanced distribution of sensitive classes (i.e., male/female). By selecting the features driving predictive accuracy, we aimed to discover whether this method could effectively reduce gender bias in outcomes. We evaluated our findings using experiments conducted on real-world datasets, focusing particularly on high-stakes applications where bias may result in unfair treatment, unequal opportunities, or unjustified societal costs. Our results suggest that the linear subspace hypothesis holds promise in reducing gender bias in predictive modeling tasks. While there remain challenges in achieving optimal balance across all protected groups, further development and refinement of these methods offer a path forward towards more equitable decision-making systems.",1
"Product descriptions in e-commerce platforms contain detailed and valuable information about retailers assortment. In particular, coding promotions within digital leaflets are of great interest in e-commerce as they capture the attention of consumers by showing regular promotions for different products. However, this information is embedded into images, making it difficult to extract and process for downstream tasks. In this paper, we present an end-to-end approach that classifies promotions within digital leaflets into their corresponding product categories using both visual and textual information. Our approach can be divided into three key components: 1) region detection, 2) text recognition and 3) text classification. In many cases, a single promotion refers to multiple product categories, so we introduce a multi-label objective in the classification head. We demonstrate the effectiveness of our approach for two separated tasks: 1) image-based detection of the descriptions for each individual promotion and 2) multi-label classification of the product categories using the text from the product descriptions. We train and evaluate our models using a private dataset composed of images from digital leaflets obtained by Nielsen. Results show that we consistently outperform the proposed baseline by a large margin in all the experiments.",0
"Digital leaflet analysis has recently attracted significant research attention due to the increasing popularity of online shopping and e-commerce platforms. In particular, automatic promotion recognition from digital leaflets plays an important role in assisting customers in making informed purchasing decisions by providing up-to-date information on available discounts, deals, and other marketing campaigns. However, existing approaches mainly focus on either textual or visual features extracted from the leaflet images, which limits their performance in handling complex promotion scenarios that often involve multiple labels and diverse layout designs. To address these limitations, we propose a novel multi-modal neural network model for jointly processing both textual and visual cues obtained from digital leaflets to accurately recognize different types of promotions. We evaluate our approach using three large datasets collected from real-world commercial websites and demonstrate its superiority over state-of-the-art methods across multiple metrics. Our findings have valuable implications for developing effective strategies towards enhancing the overall quality of multimedia content understanding tasks related to electronic commerce.",1
"The query-based moment retrieval is a problem of localising a specific clip from an untrimmed video according a query sentence. This is a challenging task that requires interpretation of both the natural language query and the video content. Like in many other areas in computer vision and machine learning, the progress in query-based moment retrieval is heavily driven by the benchmark datasets and, therefore, their quality has significant impact on the field. In this paper, we present a series of experiments assessing how well the benchmark results reflect the true progress in solving the moment retrieval task. Our results indicate substantial biases in the popular datasets and unexpected behaviour of the state-of-the-art models. Moreover, we present new sanity check experiments and approaches for visualising the results. Finally, we suggest possible directions to improve the temporal sentence grounding in the future. Our code for this paper is available at https://mayu-ot.github.io/hidden-challenges-MR .",0
"""Abstract: This study aimed at uncovering hidden challenges that face query-based video moment retrieval by conducting an in-depth analysis of three existing systems. These challenges were categorized into four main themes: representation challenge, indexing challenge, interaction challenge and evaluation challenge.""",1
"Simultaneously modeling source code and natural language has many exciting applications in automated software development and understanding. Pursuant to achieving such technology, we introduce PyMT5, the Python method text-to-text transfer transformer, which is trained to translate between all pairs of Python method feature combinations: a single model that can both predict whole methods from natural language documentation strings (docstrings) and summarize code into docstrings of any common style. We present an analysis and modeling effort of a large-scale parallel corpus of 26 million Python methods and 7.7 million method-docstring pairs, demonstrating that for docstring and method generation, PyMT5 outperforms similarly-sized auto-regressive language models (GPT2) which were English pre-trained or randomly initialized. On the CodeSearchNet test set, our best model predicts 92.1% syntactically correct method bodies, achieved a BLEU score of 8.59 for method generation and 16.3 for docstring generation (summarization), and achieved a ROUGE-L F-score of 24.8 for method generation and 36.7 for docstring generation.",0
"Title: Efficient Multi-Mode Translation of Natural Language and Python Code Using Transformers  Abstract: Natural language processing has seen significant advancements due to recent developments in deep learning techniques such as transformer models. One particular challenge faced by many users is translating natural language instructions into executable Python code, which can be tedious and error-prone. To address this problem, we propose PyMT5, a novel framework that leverages transformer architectures to facilitate efficient multi-modal translation from natural language input (e.g., English) to equivalent Python code output. This approach allows users to interactively write code snippets using fluent prose instead of laboriously coding each line manually. Our experiments demonstrate that our model outperforms other state-of-the-art systems on benchmark datasets across multiple evaluation metrics, making PyMT5 an effective tool for programmers, educators, researchers, and anyone seeking assistance with writing clean, readable Python code quickly and easily.",1
"Knowledge graph reasoning is a critical task in natural language processing. The task becomes more challenging on temporal knowledge graphs, where each fact is associated with a timestamp. Most existing methods focus on reasoning at past timestamps and they are not able to predict facts happening in the future. This paper proposes Recurrent Event Network (RE-NET), a novel autoregressive architecture for predicting future interactions. The occurrence of a fact (event) is modeled as a probability distribution conditioned on temporal sequences of past knowledge graphs. Specifically, our RE-NET employs a recurrent event encoder to encode past facts and uses a neighborhood aggregator to model the connection of facts at the same timestamp. Future facts can then be inferred in a sequential manner based on the two modules. We evaluate our proposed method via link prediction at future times on five public datasets. Through extensive experiments, we demonstrate the strength of RENET, especially on multi-step inference over future timestamps, and achieve state-of-the-art performance on all five datasets. Code and data can be found at https://github.com/INK-USC/RE-Net.",0
"An important problem in knowledge base construction involves identifying temporally structured events from textual descriptions. Prior work has focused on independent event extraction using supervised learning. We take advantage of a recent paradigm shift in deep graph neural networks that use autoencoders to learn continuous node embeddings, such as node2vec or EgoNN, allowing us to make efficient use of large graphs like those available today in commonsense reasoning benchmarks like ConceptNet (Gardner & Erban, 2014). Our model represents each entity in the graph as a latent vector and defines a recurrent network structure over time by optimizing autoregressive dependencies between hidden states at each step given input sequences. This architecture allows us to efficiently propagate influence through the graph while preserving smoothness, resulting in better generalization compared to heuristic baselines across several datasets. To evaluate our approach we focus on temporal substructures, examining both qualitative examples and quantitatively measuring precision/recall against gold standards for extracting subsequences of interest in the inference task. Overall, our contributions establish foundational techniques for scalably constructing knowledge bases from unstructured data via explicit sequence modeling that respects complex relational structures.",1
"Zero-shot learning aims to recognize instances of unseen classes, for which no visual instance is available during training, by learning multimodal relations between samples from seen classes and corresponding class semantic representations. These class representations usually consist of either attributes, which do not scale well to large datasets, or word embeddings, which lead to poorer performance. A good trade-off could be to employ short sentences in natural language as class descriptions. We explore different solutions to use such short descriptions in a ZSL setting and show that while simple methods cannot achieve very good results with sentences alone, a combination of usual word embeddings and sentences can significantly outperform current state-of-the-art.",0
"In recent years, zero-shot learning has emerged as a promising approach towards tackling one of the core challenges in artificial intelligence - transferring knowledge learned from one domain to another. While previous work in zero-shot learning focused on utilizing semantic representations such as concepts or attributes as input to perform inference across tasks and domains, this paper presents an alternative view by using sentences themselves as semantic representations. We show that sentence embeddings derived from transformer architectures like GPT can effectively encode task-agnostic information at both fine-grained and coarse-grained levels of abstraction. By leveraging these sentence embeddings, we demonstrate significantly improved performance over state-of-the-art models trained exclusively on existing benchmark datasets. Our approach offers promise for future research into bridging the gap between natural language understanding and task-oriented reasoning under novel circumstances without any explicit supervision. We believe our contribution points towards a more robust methodology for training intelligent systems capable of generalization beyond their original settings.",1
"To scale non-parametric extensions of probabilistic topic models such as Latent Dirichlet allocation to larger data sets, practitioners rely increasingly on parallel and distributed systems. In this work, we study data-parallel training for the hierarchical Dirichlet process (HDP) topic model. Based upon a representation of certain conditional distributions within an HDP, we propose a doubly sparse data-parallel sampler for the HDP topic model. This sampler utilizes all available sources of sparsity found in natural language - an important way to make computation efficient. We benchmark our method on a well-known corpus (PubMed) with 8m documents and 768m tokens, using a single multi-core machine in under four days.",0
"This paper presents a new approach to training hierarchical dirichlet process (HDP) topic models using sparse data. Our method leverages parallel computing techniques to improve scalability and efficiency. We propose a novel algorithm that uses a hierarchical variational inference framework to allow for better handling of sparsity and nonlinear dependencies. Experimental results on benchmark datasets show significantly improved performance over state-of-the-art methods. Implications for natural language processing applications are discussed. Keywords: hierarchical dirichlet process, topic modeling, sparse data, parallel computing, variational inference.",1
"Discovering concepts (or temporal abstractions) in an unsupervised manner from demonstration data in the absence of an environment is an important problem. Organizing these discovered concepts hierarchically at different levels of abstraction is useful in discovering patterns, building ontologies, and generating tutorials from demonstration data. However, recent work to discover such concepts without access to any environment does not discover relationships (or a hierarchy) between these discovered concepts. In this paper, we present a Transformer-based concept abstraction architecture UNHCLE (pronounced uncle) that extracts a hierarchy of concepts in an unsupervised way from demonstration data. We empirically demonstrate how UNHCLE discovers meaningful hierarchies using datasets from Chess and Cooking domains. Finally, we show how UNHCLE learns meaningful language labels for concepts by using demonstration data augmented with natural language for cooking and chess. All of our code is available at https://github.com/UNHCLE/UNHCLE",0
"Title: ""Unsupervised Hierarchical Concept Learning"" Paper Type: Research Article Abstract: This research article presents an unsupervised hierarchical concept learning approach that enables machines to identify high-level concepts from raw data without any prior supervision. Inspired by human cognition, our method leverages the idea of hierarchy and emergence to learn complex abstractions from simple inputs. By progressively constructing interpretable representations at different levels of abstraction, we can achieve meaningful knowledge discovery and organization. To demonstrate the effectiveness of our framework, we apply it to several real-world datasets across diverse domains, including images, text, and sensor measurements. Our experimental results showcase remarkable performance gains over state-of-the-art methods while maintaining scalability and robustness. With the increasing availability of large-scale datasets, our work paves the way towards efficient, automated machine understanding of complex phenomena. Keywords: Unsupervised learning, hierarchical representation, cognitive inspired models, self-organization, transfer learning. Title: ""Unlocking Knowledge through Emergent Hierarchy: An Unsupervised Approach"" Paper Type: Research Article Abstract: This study proposes a novel unsupervised technique to enable machines to infer meaningful concepts from raw data, mimicking how humans perceive and organize knowledge. Our approach builds on the premise that abstract thinking arises naturally from simpler representations through self-organized processes, leading to a nested hierarchy of concepts. We introduce a model based on these principles and validate its efficacy across a range of benchmarks spanning various application areas such as computer vision, natural language processing, and signal analysis. Results consistently outperform existing alternatives and exhibit greater adaptability under conditions with limited training resources, demonstrating the potential of this technology to facilitate large-scale, automatic intelligence extraction from big data repositories. By enabling computational systems to replicate fundamental aspects of human cognition, future developments might lead to more intelligent agents capable of collaborative problem-solving and advanced decision support. Keywords: Unsupervised learning, emergent hierarchy, artificial intelligence, deep neural networks, knowledge representation.",1
"In Natural Language Processing (NLP), pretrained language models (LMs) that are transferred to downstream tasks have been recently shown to achieve state-of-the-art results. However, standard fine-tuning can degrade the general-domain representations captured during pretraining. To address this issue, we introduce a new regularization technique, AFTER; domain Adversarial Fine-Tuning as an Effective Regularizer. Specifically, we complement the task-specific loss used during fine-tuning with an adversarial objective. This additional loss term is related to an adversarial classifier, that aims to discriminate between in-domain and out-of-domain text representations. In-domain refers to the labeled dataset of the task at hand while out-of-domain refers to unlabeled data from a different domain. Intuitively, the adversarial classifier acts as a regularizer which prevents the model from overfitting to the task-specific domain. Empirical results on various natural language understanding tasks show that AFTER leads to improved performance compared to standard fine-tuning.",0
"Fine-tuning models pretrained on large datasets has become the de facto standard for building state-of-the-art models in many natural language processing tasks such as sentiment analysis, question answering, and machine translation. However, fine-tuning can lead to overfitting and poor generalization performance if not properly regularized. In recent years, several approaches have been proposed to mitigate these issues by imposing additional constraints during training. One promising approach is domain adversarial fine-tuning which introduces a discriminator network that learns to distinguish whether the output is generated from a model trained on the target task or one trained on other domains. The main contribution of our work is showing that using domain adversarial fine-tuning effectively regularizes the model during training leading to improved performance across several NLP benchmarks while maintaining low computational overhead compared to traditional fine-tuning methods. Additionally, we explore different ways of designing the discriminator architecture and show that incorporating global context into the discriminator improves performance further demonstrating the effectiveness of our method. Our results suggest that domain adversarial fine-tuning could serve as an alternative to popular regularization techniques such as dropout or weight decay in certain settings thus opening up future research directions exploring the use of GAN like architectures for NLP problems beyond just sequence generation.",1
"Neural autoregressive sequence models are used to generate sequences in a variety of natural language processing (NLP) tasks, where they are evaluated according to sequence-level task losses. These models are typically trained with maximum likelihood estimation, which ignores the task loss, yet empirically performs well as a surrogate objective. Typical approaches to directly optimizing the task loss such as policy gradient and minimum risk training are based around sampling in the sequence space to obtain candidate update directions that are scored based on the loss of a single sequence. In this paper, we develop an alternative method based on random search in the parameter space that leverages access to the maximum likelihood gradient. We propose maximum likelihood guided parameter search (MGS), which samples from a distribution over update directions that is a mixture of random search around the current parameters and around the maximum likelihood gradient, with each direction weighted by its improvement in the task loss. MGS shifts sampling to the parameter space, and scores candidates using losses that are pooled from multiple sequences. Our experiments show that MGS is capable of optimizing sequence-level losses, with substantial reductions in repetition and non-termination in sequence completion, and similar improvements to those of minimum risk training in machine translation.",0
"Abstract: In recent years, machine learning has become increasingly important in many fields due to its ability to learn complex patterns from data and make accurate predictions. One of the main challenges in training these models is selecting the optimal hyperparameters that can lead to better generalization performance on new data. This paper proposes a method based on Model Length Estimation (MLE) for guiding the selection of these hyperparameters during training. By maximizing the expected log likelihood under the estimated distribution, our algorithm effectively balances exploration and exploitation of the model parameters space while taking into account the uncertainty of the current estimates. Experiments on several benchmark datasets show that our method consistently outperforms random search baseline and other state-of-the-art methods by achieving lower test perplexity or higher accuracy in language understanding tasks. Our results demonstrate the effectiveness of using MLE as a surrogate for optimizing the model performance, which may potentially have broader applications beyond sequence modeling problems.",1
"Referring image segmentation aims to predict the foreground mask of the object referred by a natural language sentence. Multimodal context of the sentence is crucial to distinguish the referent from the background. Existing methods either insufficiently or redundantly model the multimodal context. To tackle this problem, we propose a ""gather-propagate-distribute"" scheme to model multimodal context by cross-modal interaction and implement this scheme as a novel Linguistic Structure guided Context Modeling (LSCM) module. Our LSCM module builds a Dependency Parsing Tree suppressed Word Graph (DPT-WG) which guides all the words to include valid multimodal context of the sentence while excluding disturbing ones through three steps over the multimodal feature, i.e., gathering, constrained propagation and distributing. Extensive experiments on four benchmarks demonstrate that our method outperforms all the previous state-of-the-arts.",0
"In recent years, there has been significant progress in developing computer vision algorithms that can accurately segment objects from images. However, these methods typically rely on large amounts of annotated data and may struggle with object boundaries that are ambiguous or difficult to predict. In this work, we present a novel approach to referring image segmentation that leverages linguistic structure to guide context modeling and improve boundary accuracy.  Our method begins by analyzing natural language descriptions of the target object, which provide important cues for understanding the object's shape and appearance. We use this information to generate region proposals that correspond to potential object locations in the input image. Next, we apply a deep neural network to score each proposal based on how well it matches the given description, taking into account both local features within the proposed region as well as global contextual constraints.  Experimental results demonstrate the effectiveness of our approach across a range of challenging scenarios, including occlusions, cluttered backgrounds, and variations in lighting and viewpoint. Our model outperforms previous state-of-the-art methods on several benchmark datasets while requiring significantly fewer annotations during training. Overall, our work represents an important step towards enabling more flexible and robust referential image segmentation systems that can better accommodate complex real-world environments.",1
"Vision-and-language navigation requires an agent to navigate through a real 3D environment following natural language instructions. Despite significant advances, few previous works are able to fully utilize the strong correspondence between the visual and textual sequences. Meanwhile, due to the lack of intermediate supervision, the agent's performance at following each part of the instruction cannot be assessed during navigation. In this work, we focus on the granularity of the visual and language sequences as well as the traceability of agents through the completion of an instruction. We provide agents with fine-grained annotations during training and find that they are able to follow the instruction better and have a higher chance of reaching the target at test time. We enrich the benchmark dataset Room-to-Room (R2R) with sub-instructions and their corresponding paths. To make use of this data, we propose effective sub-instruction attention and shifting modules that select and attend to a single sub-instruction at each time-step. We implement our sub-instruction modules in four state-of-the-art agents, compare with their baseline models, and show that our proposed method improves the performance of all four agents.   We release the Fine-Grained R2R dataset (FGR2R) and the code at https://github.com/YicongHong/Fine-Grained-R2R.",0
"In recent years, there has been significant progress in developing artificial intelligence (AI) systems that can interact with humans in natural ways through language processing tasks such as question answering and conversation generation. However, existing models often struggle with understanding complex sub-instructions within instructions given by human users. For example, if a user asks an AI system to ""find me the book I need,"" the system may fail to identify which specific task needs to be performed first - whether to search online, open the catalog, or visit the library shelves. To address this challenge, we propose a novel approach called Sub-Instruction Aware Vision-and-Language Navigation (SAVLN). Our model utilizes both visual and textual inputs from the environment to predict sub-instructions required to complete the main instruction, without relying solely on predefined rules or heuristics. We evaluate our method using several benchmark datasets and demonstrate its superior performance compared to state-of-the-art methods. Overall, SAVLN represents a promising step towards creating more intelligent and flexible AI systems that can better interpret and execute complex human instructions.",1
"Conservation laws are considered to be fundamental laws of nature. It has broad applications in many fields, including physics, chemistry, biology, geology, and engineering. Solving the differential equations associated with conservation laws is a major branch in computational mathematics. The recent success of machine learning, especially deep learning in areas such as computer vision and natural language processing, has attracted a lot of attention from the community of computational mathematics and inspired many intriguing works in combining machine learning with traditional methods. In this paper, we are the first to view numerical PDE solvers as an MDP and to use (deep) RL to learn new solvers. As proof of concept, we focus on 1-dimensional scalar conservation laws. We deploy the machinery of deep reinforcement learning to train a policy network that can decide on how the numerical solutions should be approximated in a sequential and spatial-temporal adaptive manner. We will show that the problem of solving conservation laws can be naturally viewed as a sequential decision-making process, and the numerical schemes learned in such a way can easily enforce long-term accuracy. Furthermore, the learned policy network is carefully designed to determine a good local discrete approximation based on the current state of the solution, which essentially makes the proposed method a meta-learning approach. In other words, the proposed method is capable of learning how to discretize for a given situation mimicking human experts. Finally, we will provide details on how the policy network is trained, how well it performs compared with some state-of-the-art numerical solvers such as WENO schemes, and supervised learning based approach L3D and PINN, and how well it generalizes.",0
"Title: ""Discovering Optimal Control Policies for One-Dimensional Conservation Law Problems using Deep Reinforcement Learning""  This work presents a deep reinforcement learning (RL) approach for solving one-dimensional scalar conservation law problems. Conservation laws describe physical phenomena where quantities such as mass, energy, or momentum must remain constant over time and space. These equations have many applications in physics, engineering, and other fields but can be difficult to solve analytically or numerically for complex scenarios. As a result, efficient methods for approximating solutions that are accurate, robust, and easy to use are highly desired. Here, we focus on finding control policies that steer systems towards desired equilibrium states or trajectories while minimizing error or maximizing performance objectives like cost functions. To achieve these goals, our RL framework leverages deep neural networks to both approximate solution dynamics and guide policy search procedures based on temporal difference update rules. Our experiments demonstrate significant improvements over traditional trial-and-error approaches and offer promising initial results for scaling up to more challenging cases. Potential future directions could include extending these techniques beyond 1D problems to higher dimensions, incorporating domain knowledge to enhance generalization properties, or exploring alternative model architectures better suited for capturing essential system behaviors. This research ultimately advances our ability to design effective controllers for real-world conservation law problems using state-of-the-art machine learning tools.",1
"As Machine Learning technologies become increasingly used in contexts that affect citizens, companies as well as researchers need to be confident that their application of these methods will not have unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches to mitigating (social) biases and increase fairness in the Machine Learning literature. It organises approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, unsupervised learning, and natural language processing is also provided along with a selection of currently available open source libraries. The article concludes by summarising open challenges articulated as four dilemmas for fairness research.",0
"This paper provides a comprehensive survey of recent research on fairness in machine learning (ML). ML algorithms can perpetuate or even amplify existing societal biases, leading to unfair outcomes that may disproportionately harm certain groups. Addressing these concerns has become increasingly important as ML systems play an ever more prominent role in decision making across many domains such as hiring, lending, criminal justice, and education. We first provide an overview of different forms of bias found in training datasets and algorithmic models themselves. Next we discuss popular techniques used to mitigate these issues including postprocessing methods that debias test set predictions, regularization techniques during model training, data preprocessing methods like subpopulation selection and rebalancing, and adversarial methodologies for identifying and mitigating bias. Finally we conclude by highlighting open challenges for future work. Our goal is to serve as a reference resource for researchers and practitioners interested in addressing fairness concerns in their own work while simultaneously providing readers a broader understanding of current state-of-the art techniques for achieving fairer ML predictions.",1
"In deep neural nets, lower level embedding layers account for a large portion of the total number of parameters. Tikhonov regularization, graph-based regularization, and hard parameter sharing are approaches that introduce explicit biases into training in a hope to reduce statistical complexity. Alternatively, we propose stochastically shared embeddings (SSE), a data-driven approach to regularizing embedding layers, which stochastically transitions between embeddings during stochastic gradient descent (SGD). Because SSE integrates seamlessly with existing SGD algorithms, it can be used with only minor modifications when training large scale neural networks. We develop two versions of SSE: SSE-Graph using knowledge graphs of embeddings; SSE-SE using no prior information. We provide theoretical guarantees for our method and show its empirical effectiveness on 6 distinct tasks, from simple neural networks with one hidden layer in recommender systems, to the transformer and BERT in natural languages. We find that when used along with widely-used regularization methods such as weight decay and dropout, our proposed SSE can further reduce overfitting, which often leads to more favorable generalization results.",0
"Title: ""Regularizing embedding layers using stochastic shared embeddings""  This paper presents a new method for regularizing embedding layers in neural networks, which we call stochastic shared embeddings (SSE). SSE leverages the idea that multiple tasks can share common learned representations by randomly sampling from a fixed set of pretrained shared embeddings at each task-specific layer during training. By doing so, we aim to address two important challenges faced in NLP: lack of data for low-resource languages and domains, and the need to transfer knowledge across different natural language processing subtasks within the same model architecture. We evaluate our approach on several datasets and show consistent improvements over previous methods. Our findings demonstrate the effectiveness of SSE as a simple yet powerful technique for regularizing embedding layers and improving performance on challenging natural language understanding tasks.",1
"Recently, Deep Learning (DL) methods have shown an excellent performance in image captioning and visual question answering. However, despite their performance, DL methods do not learn the semantics of the words that are being used to describe a scene, making it difficult to spot incorrect words used in captions or to interchange words that have similar meanings. This work proposes a combination of DL methods for object detection and natural language processing to validate image's captions. We test our method in the FOIL-COCO data set, since it provides correct and incorrect captions for various images using only objects represented in the MS-COCO image data set. Results show that our method has a good overall performance, in some cases similar to the human performance.",0
"This paper presents a new methodology called Correction by Analyses, Pos Tagging and Interpretation of objects (CAPIO) that identifies objects from their noun names and uses natural language processing techniques such as part of speech tagging (POS tagging) to further refine object recognition. Unlike traditional methods that rely on complex image data analysis or prior knowledge of objects, our approach demonstrates significant improvement over state-of-the-art models when tested across multiple datasets while maintaining low computational requirements. By exploiting the correlation between common nouns and object features, we showcase how textual descriptions can serve as powerful alternatives towards real-time, accurate, and efficient object identification. Our proposed framework has far-reaching applications in automating tasks requiring object detection including robotics, surveillance, medicine, security, and consumer electronics industries, making technology more accessible and easier than ever before. We anticipate future research will explore multi-modal approaches that incorporate vision signals and linguistic cues simultaneously for even better performance. In summary, this work represents a groundbreaking advancement in computer vision enabling machines to recognize simple everyday objects through human-readable descriptions alone, laying the foundation for further breakthrough discoveries.",1
"Referring image segmentation aims at segmenting the foreground masks of the entities that can well match the description given in the natural language expression. Previous approaches tackle this problem using implicit feature interaction and fusion between visual and linguistic modalities, but usually fail to explore informative words of the expression to well align features from the two modalities for accurately identifying the referred entity. In this paper, we propose a Cross-Modal Progressive Comprehension (CMPC) module and a Text-Guided Feature Exchange (TGFE) module to effectively address the challenging task. Concretely, the CMPC module first employs entity and attribute words to perceive all the related entities that might be considered by the expression. Then, the relational words are adopted to highlight the correct entity as well as suppress other irrelevant ones by multimodal graph reasoning. In addition to the CMPC module, we further leverage a simple yet effective TGFE module to integrate the reasoned multimodal features from different levels with the guidance of textual information. In this way, features from multi-levels could communicate with each other and be refined based on the textual context. We conduct extensive experiments on four popular referring segmentation benchmarks and achieve new state-of-the-art performances.",0
"This research proposes an image segmentation method that utilizes cross-modal progressive comprehension techniques to improve accuracy. The proposed approach first generates high-resolution semantic feature maps by incorporating multi-scale contextual knowledge from external data sources such as object detection results, pretrained models, and human feedback. Then, these feature maps are iteratively updated based on a progressively refining strategy, which takes into account both visual cues and textual descriptions. Finally, a deep neural network is used to predict the pixelwise segmentation masks using the generated features. Experimental results demonstrate significant improvements compared to state-of-the-art methods, especially in challenging scenarios where limited supervision is available. The proposed framework has potential applications in areas such as computer vision and autonomous driving, where accurate scene understanding is crucial. Overall, this work contributes to the field of image segmentation by leveraging innovative cross-modal learning techniques to enhance performance under real-world constraints.",1
"Text is the most widely used means of communication today. This data is abundant but nevertheless complex to exploit within algorithms. For years, scientists have been trying to implement different techniques that enable computers to replicate some mechanisms of human reading. During the past five years, research disrupted the capacity of the algorithms to unleash the value of text data. It brings today, many opportunities for the insurance industry.Understanding those methods and, above all, knowing how to apply them is a major challenge and key to unleash the value of text data that have been stored for many years. Processing language with computer brings many new opportunities especially in the insurance sector where reports are central in the information used by insurers. SCOR's Data Analytics team has been working on the implementation of innovative tools or products that enable the use of the latest research on text analysis. Understanding text mining techniques in insurance enhances the monitoring of the underwritten risks and many processes that finally benefit policyholders.This article proposes to explain opportunities that Natural Language Processing (NLP) are providing to insurance. It details different methods used today in practice traces back the story of them. We also illustrate the implementation of certain methods using open source libraries and python codes that we have developed to facilitate the use of these techniques.After giving a general overview on the evolution of text mining during the past few years,we share about how to conduct a full study with text mining and share some examples to serve those models into insurance products or services. Finally, we explained in more details every step that composes a Natural Language Processing study to ensure the reader can have a deep understanding on the implementation.",0
"""Natural Language Processing (NLP) has emerged as a powerful tool in many industries, including insurance. NLP enables computers to process human language data, allowing them to perform tasks such as sentiment analysis, text summarization, speech recognition, and more. Insurance companies can leverage these capabilities to gain valuable insights from unstructured customer feedback, automate claims processes, personalize communication with customers, detect fraudulent activities, and improve underwriting decisions. This paper presents a comprehensive review of recent advances in NLP research relevant to the insurance industry. We cover topics such as social media monitoring, document classification, named entity recognition, machine translation, and question answering systems. Our findings highlight the potential benefits of applying NLP techniques in various areas within insurance companies and identify future research directions to address challenges faced by the industry.""",1
"Change Captioning is a task that aims to describe the difference between images with natural language. Most existing methods treat this problem as a difference judgment without the existence of distractors, such as viewpoint changes. However, in practice, viewpoint changes happen often and can overwhelm the semantic difference to be described. In this paper, we propose a novel visual encoder to explicitly distinguish viewpoint changes from semantic changes in the change captioning task. Moreover, we further simulate the attention preference of humans and propose a novel reinforcement learning process to fine-tune the attention directly with language evaluation rewards. Extensive experimental results show that our method outperforms the state-of-the-art approaches by a large margin in both Spot-the-Diff and CLEVR-Change datasets.",0
"This research presents a novel approach to caption generation using natural language processing techniques that utilizes viewpoint adaptation to improve performance on image descriptions. The proposed method uses a matching encoder architecture to map images onto text embeddings, which allows for efficient comparison of visual features from different perspectives. Experimental results show that the proposed model outperforms state-of-the-art methods in terms of accuracy and coherency of generated captions. Furthermore, qualitative analysis indicates that the model effectively captures spatial relationships and contextual information present in the input images. Overall, the study contributes to the development of advanced NLP techniques for caption generation tasks, providing a promising direction for future work in computer vision and multimedia systems.",1
"Anomaly detection in videos is a problem that has been studied for more than a decade. This area has piqued the interest of researchers due to its wide applicability. Because of this, there has been a wide array of approaches that have been proposed throughout the years and these approaches range from statistical-based approaches to machine learning-based approaches. Numerous surveys have already been conducted on this area but this paper focuses on providing an overview on the recent advances in the field of anomaly detection using Deep Learning. Deep Learning has been applied successfully in many fields of artificial intelligence such as computer vision, natural language processing and more. This survey, however, focuses on how Deep Learning has improved and provided more insights to the area of video anomaly detection. This paper provides a categorization of the different Deep Learning approaches with respect to their objectives. Additionally, it also discusses the commonly used datasets along with the common evaluation metrics. Afterwards, a discussion synthesizing all of the recent approaches is made to provide direction and possible areas for future research.",0
"This survey presents an overview of deep learning techniques used for video anomaly detection. With the increasing availability of cameras and videos, there has been a growing demand for automatic analysis of visual data, including detecting unusual events that might require immediate attention. Traditional approaches based on handcrafted features have proven limited in their ability to capture complex patterns in videos. In recent years, deep learning methods have shown great potential in addressing these limitations by leveraging large amounts of training data to learn robust representations of normal behavior. We provide a comprehensive review of different architectures designed specifically for unsupervised video anomaly detection tasks. Our analysis reveals commonalities across existing techniques, such as the use of temporal context models and reconstruction losses, while highlighting areas where further research can contribute to improving performance. Overall, our findings emphasize the importance of utilizing both convolutional neural networks (CNNs) and recurrent neural networks (RNNs), along with effective preprocessing steps, in achieving successful video anomaly detection using deep learning.",1
"The internal workings of modern deep learning models stay often unclear to an external observer, although spatial attention mechanisms are involved. The idea of this work is to translate these spatial attentions into natural language to provide a simpler access to the model's function. Thus, I took a neural image captioning model and measured the reactions to external modification in its spatial attention for three different interface methods: a fixation over the whole generation process, a fixation for the first time-steps and an addition to the generator's attention. The experimental results for bounding box based spatial attention vectors have shown that the captioning model reacts to method dependent changes in up to 52.65% and includes in 9.00% of the cases object categories, which were otherwise unmentioned. Afterwards, I established such a link to a hierarchical co-attention network for visual question answering by extraction of its word, phrase and question level spatial attentions. Here, generated captions for the word level included details of the question-answer pairs in up to 55.20% of the cases. This work indicates that spatial attention seen as an external interface for image caption generators is an useful method to access visual functions in natural language.",0
"In image captioning models, effective attention mechanisms can significantly improve performance by allowing the model to focus on relevant features in the input images. However, current approaches mainly rely on temporal self-attention within recurrent neural networks (RNNs), which limits their ability to capture spatial relationships across different regions in an image. To address this limitation, we propose using a new type of attention mechanism that operates directly on spatial feature maps. Our approach first applies convolutional layers to extract high-level representations from the raw pixel data, then applies multi-head attention over these representations to compute global context vectors for each region in the image. This allows our model to selectively attend to specific regions in the image based on their relevance to the task at hand. We evaluate our method against several strong baselines and demonstrate consistent improvement in both qualitative and quantitative metrics. Overall, our work shows the effectiveness of spatial attention mechanisms as an interface for image captioning models, offering significant potential for future research in this area.",1
"Recognition of Arabic characters is essential for natural language processing and computer vision fields. The need to recognize and classify the handwritten Arabic letters and characters are essentially required. In this paper, we present an algorithm for recognizing Arabic letters and characters based on using deep convolution neural networks (DCNN) and support vector machine (SVM). This paper addresses the problem of recognizing the Arabic handwritten characters by determining the similarity between the input templates and the pre-stored templates using both fully connected DCNN and dropout SVM. Furthermore, this paper determines the correct classification rate (CRR) depends on the accuracy of the corrected classified templates, of the recognized handwritten Arabic characters. Moreover, we determine the error classification rate (ECR). The experimental results of this work indicate the ability of the proposed algorithm to recognize, identify, and verify the input handwritten Arabic characters. Furthermore, the proposed system determines similar Arabic characters using a clustering algorithm based on the K-means clustering approach to handle the problem of multi-stroke in Arabic characters. The comparative evaluation is stated and the system accuracy reached 95.07% CRR with 4.93% ECR compared with the state of the art.",0
"This paper presents a novel approach to recognize handwritten Arabic characters using convolution neural networks (CNN) and support vector machines (SVM). The proposed method uses pre-trained CNN to extract features from the images of Arabic characters and then employs SVM as a classifier to identify the character classes. Experimental results show that our approach achieves state-of-the-art performance on several benchmark datasets, outperforming other methods such as Hidden Markov Models and Artificial Neural Networks. In addition, we conducted extensive analysis on different aspects of our system including feature extraction techniques, network architectures, hyperparameter tuning, and dataset size, which provide insights into designing more efficient handwritten character recognition systems. Our work demonstrates the potential of combining CNN and SVM for recognizing complex script languages like Arabic.",1
"Machine learning models have been successfully applied to a wide range of applications including computer vision, natural language processing, and speech recognition. A successful implementation of these models however, usually relies on deep neural networks (DNNs) which are treated as opaque black-box systems due to their incomprehensible complexity and intricate internal mechanism. In this work, we present a novel algorithm for explaining the predictions of a DNN using adversarial machine learning. Our approach identifies the relative importance of input features in relation to the predictions based on the behavior of an adversarial attack on the DNN. Our algorithm has the advantage of being fast, consistent, and easy to implement and interpret. We present our detailed analysis that demonstrates how the behavior of an adversarial attack, given a DNN and a task, stays consistent for any input test data point proving the generality of our approach. Our analysis enables us to produce consistent and efficient explanations. We illustrate the effectiveness of our approach by conducting experiments using a variety of DNNs, tasks, and datasets. Finally, we compare our work with other well-known techniques in the current literature.",0
"""Adversarial examples have been recently demonstrated as a powerful tool to reveal deep neural networks (DNN) intrinsic weaknesses. However these attacks rely on strong assumptions that might not always hold true such as access to models weights, knowledge on how many times models has been trained before using them against DNN etc... In this work we propose and evaluate a novel method which takes advantage of natural adversarial noise inherent in most datasets . Our experiments show that our approach improves robustness against state-of-the art adversarial methods by orders of magnitude."" ---",1
Finding a path free from obstacles that poses minimal risk is critical for safe navigation. People who are sighted and people who are visually impaired require navigation safety while walking on a sidewalk. In this research we developed an assistive navigation on a sidewalk by integrating sensory inputs using reinforcement learning. We trained a Sidewalk Obstacle Avoidance Agent (SOAA) through reinforcement learning in a simulated robotic environment. A Sidewalk Obstacle Conversational Agent (SOCA) is built by training a natural language conversation agent with real conversation data. The SOAA along with SOCA was integrated in a prototype device called augmented guide (AG). Empirical analysis showed that this prototype improved the obstacle avoidance experience about 5% from a base case of 81.29%,0
"Title: ""From Digital Training to Physical Navigation: Enabling Safe Mobility for Visually Impaired Individuals through Reinforcement Learning""  Abstract: Navigating unfamiliar environments can pose significant challenges for visually impaired individuals. With limited accessibility and safety features available on traditional navigation devices, new approaches are necessary to enhance their mobility experience. Recently, advances in deep reinforcement learning have made it possible to train agents that can learn from large amounts of data to perform complex tasks without explicit instructions. In this work, we propose using virtual reality (VR) as a tool to provide realistic training scenarios for visually impaired users, enhancing the effectiveness of reinforcement learning algorithms. Our approach leverages user feedback to adaptively generate novel obstacles and reward functions, ensuring that the learned policies generalize well to real-world situations. We evaluate our method using simulation experiments and real-world validation studies involving human participants. Results demonstrate significant improvements in task performance compared to state-of-the-art methods, paving the way towards safe and effective navigation assistance for visually impaired users. Overall, this research contributes to the development of accessible technologies for those with visual disabilities, promoting independence and confidence in daily activities.",1
"Deep learning methods have shown state of the art performance in a range of tasks from computer vision to natural language processing. However, it is well known that such systems are vulnerable to attackers who craft inputs in order to cause misclassification. The level of perturbation an attacker needs to introduce in order to cause such a misclassification can be extremely small, and often imperceptible. This is of significant security concern, particularly where misclassification can cause harm to humans.   We thus propose Deep Latent Defence, an architecture which seeks to combine adversarial training with a detection system. At its core Deep Latent Defence has a adversarially trained neural network. A series of encoders take the intermediate layer representation of data as it passes though the network and project it to a latent space which we use for detecting adversarial samples via a $k$-nn classifier. We present results using both grey and white box attackers, as well as an adaptive $L_{\infty}$ bounded attack which was constructed specifically to try and evade our defence. We find that even under the strongest attacker model that we have investigated our defence is able to offer significant defensive benefits.",0
"Includes the phrase “In this paper we present”. Doesn’t have any spelling/grammar mistakes. Is written in past tense. Is structured into three paragraphs: introduction, background section(briefly explain problem), contributions/results. Can you please complete this task?",1
"Attention-based models have demonstrated remarkable success in various natural language understanding tasks. However, efficient execution remains a challenge for these models which are memory-bound due to their massive number of parameters. We present GOBO, a model quantization technique that compresses the vast majority (typically 99.9%) of the 32-bit floating-point parameters of state-of-the-art BERT models and their variants to 3 bits while maintaining their accuracy. Unlike other quantization methods, GOBO does not require fine-tuning nor retraining to compensate for the quantization error. We present two practical hardware applications of GOBO. In the first GOBO reduces memory storage and traffic and as a result inference latency and energy consumption. This GOBO memory compression mechanism is plug-in compatible with many architectures; we demonstrate it with the TPU, Eyeriss, and an architecture using Tensor Cores-like units. Second, we present a co-designed hardware architecture that also reduces computation. Uniquely, the GOBO architecture maintains most of the weights in 3b even during computation, a property that: (1) makes the processing elements area efficient, allowing us to pack more compute power per unit area, (2) replaces most multiply-accumulations with additions, and (3) reduces the off-chip traffic by amplifying on-chip memory capacity.",0
"Despite significant advances in Natural Language Processing (NLP) technology over recent years, there remains an urgent need for efficient models that can make accurate predictions at low latencies while minimizing energy consumption. To address these challenges, we present GOBO - a novel attention-based model architecture that utilizes weight quantization techniques to achieve superior performance on multiple benchmark datasets. Our approach adapts the popular Transformer architecture and employs customized quantization techniques specific to each component of the original network. We report results using different metrics such as Perplexity (PPL), F1 score, Accuracy and ROC-AUC, which demonstrate the effectiveness of our method. These findings suggest that GOBO provides a compelling alternative to existing state-of-the-art methods, particularly in situations where fast inference times and reduced computational requirements are critical factors. This work has important implications for real-world applications of natural language processing, including virtual assistants and chatbots, allowing them to operate more efficiently in scenarios where resources are limited. Overall, our study represents a valuable contribution towards the development of high performing and energy efficient machine learning algorithms that satisfy the increasing demand for fast and accurate predictions from large scale data sets.",1
"Deep neural networks have achieved great success both in computer vision and natural language processing tasks. However, mostly state-of-art methods highly rely on external training or computing to improve the performance. To alleviate the external reliance, we proposed a gradient enhancement approach, conducted by the short circuit neural connections, to improve the gradient learning of deep neural networks. The proposed short circuit is a unidirectional connection that single back propagates the sensitive from the deep layer to the shallows. Moreover, the short circuit formulates to be a gradient truncation of its crossing layers which can plug into the backbone deep neural networks without introducing external training parameters. Extensive experiments demonstrate deep neural networks with our short circuit gain a large margin over the baselines on both computer vision and natural language processing tasks.",0
"Artificial neural networks can often learn complex functions effectively by minimizing a loss function through gradient descent on large datasets. However, they sometimes suffer from vanishing gradients during backpropagation. This causes difficulty in training deep architectures with increasing depth or model capacity. One common method to tackle this problem is to use batch normalization (BN). BN standardizes input activations to have zero mean and unit variance and provides important regularization benefits. In addition, recent work has shown that using normalized activation gradients as inputs for subsequent layers substantially speeds up convergence time. In this paper, we propose adding short circuits (SC) at each layer to further reduce computational costs while improving generalization performance. We demonstrate experimentally on CIFAR-10 and ImageNet benchmark datasets that our approach achieves improved accuracy over previous state-of-the-art methods involving BN plus residual connections, group normalization, and growth rate regularization. Our findings contribute new insights into designing effective deep learning models. Keywords: artificial intelligence, machine learning, deep learning, gradient vanishing, batch normalization, normalized activation gradients, short circuit connection, convolutional neural network, Residual Net, GrowthNet, Generalized Divisive Normalization, Group Normalization.",1
"Alzheimer's Dementia (AD) is an incurable, debilitating, and progressive neurodegenerative condition that affects cognitive function. Early diagnosis is important as therapeutics can delay progression and give those diagnosed vital time. Developing models that analyse spontaneous speech could eventually provide an efficient diagnostic modality for earlier diagnosis of AD. The Alzheimer's Dementia Recognition through Spontaneous Speech task offers acoustically pre-processed and balanced datasets for the classification and prediction of AD and associated phenotypes through the modelling of spontaneous speech. We exclusively analyse the supplied textual transcripts of the spontaneous speech dataset, building and comparing performance across numerous models for the classification of AD vs controls and the prediction of Mental Mini State Exam scores. We rigorously train and evaluate Support Vector Machines (SVMs), Gradient Boosting Decision Trees (GBDT), and Conditional Random Fields (CRFs) alongside deep learning Transformer based models. We find our top performing models to be a simple Term Frequency-Inverse Document Frequency (TF-IDF) vectoriser as input into a SVM model and a pre-trained Transformer based model `DistilBERT' when used as an embedding layer into simple linear models. We demonstrate test set scores of 0.81-0.82 across classification metrics and a RMSE of 4.58.",0
"This is a summary of natural language processing techniques used for predicting Alzheimer’s dementia. Several studies have explored the use of text features derived from spontaneous speech samples as indicators of cognitive decline, including those using lexical variables, such as word length, sentence structure and complexity measures; phonological patterns of speech rhythm and stress; discourse measures such as coherence and informativity, semantic variables including noun density, verb tense usage patterns and part-of-speech ratios, and pragmatic features, such as referentiality, relevance, and conversational cues. These findings provide a strong evidence base for further development and evaluation of new models that incorporate natural language processing methods into diagnostic algorithms for early detection of Alzheimer’s disease in general practice settings.",1
"We investigated the effect of different training scenarios on predicting the (retro)synthesis of chemical compounds using a text-like representation of chemical reactions (SMILES) and Natural Language Processing neural network Transformer architecture. We showed that data augmentation, which is a powerful method used in image processing, eliminated the effect of data memorization by neural networks, and improved their performance for the prediction of new sequences. This effect was observed when augmentation was used simultaneously for input and the target data simultaneously. The top-5 accuracy was 84.8% for the prediction of the largest fragment (thus identifying principal transformation for classical retro-synthesis) for the USPTO-50k test dataset and was achieved by a combination of SMILES augmentation and a beam search algorithm. The same approach provided significantly better results for the prediction of direct reactions from the single-step USPTO-MIT test set. Our model achieved 90.6% top-1 and 96.1% top-5 accuracy for its challenging mixed set and 97% top-5 accuracy for the USPTO-MIT separated set. It also significantly improved results for USPTO-full set single-step retrosynthesis for both top-1 and top-10 accuracies. The appearance frequency of the most abundantly generated SMILES was well correlated with the prediction outcome and can be used as a measure of the quality of reaction prediction.",0
"""Abstract"" (without quotes) followed by space should be the first word(s) of the Abstract field text box, which may then be followed by the actual content of your Abstract. Please follow these instructions carefully. State-of-the-art augmented NLP transformer models offer powerful tools for chemists seeking to solve complex synthesis problems with greater efficiency than traditional methods. Our study focuses on the development of two such models: RetroXL and ChemPlanner. Both systems employ deep learning techniques that enable them to process large quantities of data from a wide variety of sources and identify optimal pathways for synthesizing target molecules using available starting materials. RetroXL is particularly effective at addressing the problem of retrosynthetic analysis, breaking down target molecules into their precursors one step at a time until commercially viable routes are identified. In contrast, ChemPlanner offers a more comprehensive approach that can handle multi-step planning tasks involving multiple reactions, conditions, and reactants. Through rigorous testing and validation against benchmark datasets, we demonstrate the superior performance of our augmented NLP transformer models compared to existing alternatives, making them promising solutions for accelerating drug discovery and other chemical research applications.",1
"Mass utilization of body-worn cameras has led to a huge corpus of available egocentric video. Existing video summarization algorithms can accelerate browsing such videos by selecting (visually) interesting shots from them. Nonetheless, since the system user still has to watch the summary videos, browsing large video databases remain a challenge. Hence, in this work, we propose to generate a textual synopsis, consisting of a few sentences describing the most important events in a long egocentric videos. Users can read the short text to gain insight about the video, and more importantly, efficiently search through the content of a large video database using text queries. Since egocentric videos are long and contain many activities and events, using video-to-text algorithms results in thousands of descriptions, many of which are incorrect. Therefore, we propose a multi-task learning scheme to simultaneously generate descriptions for video segments and summarize the resulting descriptions in an end-to-end fashion. We Input a set of video shots and the network generates a text description for each shot. Next, visual-language content matching unit that is trained with a weakly supervised objective, identifies the correct descriptions. Finally, the last component of our network, called purport network, evaluates the descriptions all together to select the ones containing crucial information. Out of thousands of descriptions generated for the video, a few informative sentences are returned to the user. We validate our framework on the challenging UT Egocentric video dataset, where each video is between 3 to 5 hours long, associated with over 3000 textual descriptions on average. The generated textual summaries, including only 5 percent (or less) of the generated descriptions, are compared to groundtruth summaries in text domain using well-established metrics in natural language processing.",0
"This paper presents a method to automatically generate text synopses from egocentric videos captured by first-person cameras worn by users while performing activities such as cooking, exercising or shopping. Our approach combines computer vision techniques for action recognition, object detection and scene segmentation with natural language generation methods that take into account both visual content and audio narrations. We evaluate our system on several datasets collected under realistic settings and show significant improvements over baseline approaches.  Text Synopsis Generation for Egocentric Videos: Abstract ===============================================  This research proposes a novel solution for generating text summaries of first-person (egocentric) videos using wearable camera technology commonly used in everyday life activities like grocery shopping, exercise routines or meal preparations. By combining state-of-the-art computer vision algorithms for recognizing actions, detecting objects and separating scenes along with natural language processing models incorporating both video footage and audio commentary, our proposed method outperforms existing techniques. Extensive experimentation across multiple benchmark datasets verifies these findings, confirming the value added by jointly considering multiple modalities. Overall, our work offers promising insights towards automated multimedia indexing and retrieval solutions benefiting applications such as personal archiving, memory support and social sharing platforms.",1
"Recommendation has been a long-standing problem in many areas ranging from e-commerce to social websites. Most current studies focus only on traditional approaches such as content-based or collaborative filtering while there are relatively fewer studies in hybrid recommender systems. Due to the latest advances of deep learning achieved in different fields including computer vision and natural language processing, deep learning has also gained much attention in Recommendation Systems. There are several studies that utilize ID embeddings of users and items to implement collaborative filtering with deep neural networks. However, such studies do not take advantage of other categorical or continuous features of inputs. In this paper, we propose a new deep neural network architecture which consists of not only ID embeddings but also auxiliary information such as features of job postings and candidates for job recommendation system which is a reciprocal recommendation system. Experimental results on the dataset from a job-site show that the proposed method improves recommendation results over deep learning models utilizing ID embeddings.",0
"This paper presents a novel approach to recommendation systems using deep hybrid models, which combines both collaborative filtering and content-based filtering techniques. Our proposed model incorporates multiple neural networks with different architectures, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), to capture both temporal dynamics and contextual features of user preferences and item characteristics. In addition, we propose a new loss function that balances precision and recall to improve overall accuracy in the recommended items. Experimental results on two real-world datasets demonstrate significant improvement over state-of-the-art recommendation algorithms across various evaluation metrics. Our approach represents a promising direction for building more accurate and personalized recommendation systems that can effectively handle complex data types and dynamic user behavior patterns.",1
"Adversarial machine learning has exposed several security hazards of neural models and has become an important research topic in recent times. Thus far, the concept of an ""adversarial perturbation"" has exclusively been used with reference to the input space referring to a small, imperceptible change which can cause a ML model to err. In this work we extend the idea of ""adversarial perturbations"" to the space of model weights, specifically to inject backdoors in trained DNNs, which exposes a security risk of using publicly available trained models. Here, injecting a backdoor refers to obtaining a desired outcome from the model when a trigger pattern is added to the input, while retaining the original model predictions on a non-triggered input. From the perspective of an adversary, we characterize these adversarial perturbations to be constrained within an $\ell_{\infty}$ norm around the original model weights. We introduce adversarial perturbations in the model weights using a composite loss on the predictions of the original model and the desired trigger through projected gradient descent. We empirically show that these adversarial weight perturbations exist universally across several computer vision and natural language processing tasks. Our results show that backdoors can be successfully injected with a very small average relative change in model weight values for several applications.",0
"This paper explores the possibility that adversarial weight perturbation attacks can inject neural backdoors into machine learning systems. Previous research has shown that these types of attacks can cause models to behave unpredictably or produce incorrect outputs, but there hasn't been any investigation into whether they could deliberately steer the model towards specific behaviors without detection. We use both simulated data sets and real-world image classification tasks to test if adversarial examples can influence the outputs of trained machine learning models enough to create hidden vulnerabilities that activate under certain conditions. Our results indicate that while adversarial weight perturbation attacks may introduce some level of instability in the predictions made by the targeted models, their impact on the overall performance remains within acceptable levels compared to other forms of random errors commonly encountered in practice. However, further studies would need to evaluate their potential as stealthy attack vectors that evade detection by intrusion monitoring tools and human reviewers alike. In conclusion, we argue that better understanding of the threat landscape should guide future work aiming at improving robustness against adversarial inputs.",1
"We present a novel generalized zero-shot algorithm to recognize perceived emotions from gestures. Our task is to map gestures to novel emotion categories not encountered in training. We introduce an adversarial, autoencoder-based representation learning that correlates 3D motion-captured gesture sequence with the vectorized representation of the natural-language perceived emotion terms using word2vec embeddings. The language-semantic embedding provides a representation of the emotion label space, and we leverage this underlying distribution to map the gesture-sequences to the appropriate categorical emotion labels. We train our method using a combination of gestures annotated with known emotion terms and gestures not annotated with any emotions. We evaluate our method on the MPI Emotional Body Expressions Database (EBEDB) and obtain an accuracy of $58.43\%$. This improves the performance of current state-of-the-art algorithms for generalized zero-shot learning by $25$--$27\%$ on the absolute.",0
"This is an interesting approach to understanding emotions based on gestures. By using semantically conditioned zero shot perception combined with adversarial autoencoders, researchers were able to create an algorithm that could successfully identify unseen emotions in human subjects through their hand movements alone. The key was to train the system by providing examples of different types of gestures associated with specific emotions, such as fear, happiness, sadness, etc., so that the model would know how to recognize these feelings even if they had never seen them before. Ultimately, this method proved successful at identifying both positive and negative emotions expressed through simple gestures. Overall, this study provides valuable insights into the potential applications of machine learning for recognizing and interpreting nonverbal cues.",1
"The task of visual grounding requires locating the most relevant region or object in an image, given a natural language query. So far, progress on this task was mostly measured on curated datasets, which are not always representative of human spoken language. In this work, we deviate from recent, popular task settings and consider the problem under an autonomous vehicle scenario. In particular, we consider a situation where passengers can give free-form natural language commands to a vehicle which can be associated with an object in the street scene. To stimulate research on this topic, we have organized the \emph{Commands for Autonomous Vehicles} (C4AV) challenge based on the recent \emph{Talk2Car} dataset (URL: https://www.aicrowd.com/challenges/eccv-2020-commands-4-autonomous-vehicles). This paper presents the results of the challenge. First, we compare the used benchmark against existing datasets for visual grounding. Second, we identify the aspects that render top-performing models successful, and relate them to existing state-of-the-art models for visual grounding, in addition to detecting potential failure cases by evaluating on carefully selected subsets. Finally, we discuss several possibilities for future work.",0
"This abstract summarizes the content and conclusions of the Commands for Autonomous Vehicles workshop summary report, which was prepared by a panel from diverse stakeholder groups with experience spanning across technology research and development, manufacturing, policy making, and government agencies involved in autonomous vehicle regulation and enforcement. The C4AV workshop aimed to identify challenges surrounding advanced communication systems used for transmitting instructions between human drivers, passengers, other road users, or remote operators to highly automated vehicles with low or no occupancy as well as to develop recommendations for standards that would enable consistent commands among all parties. In particular, participants discussed two areas: the first concerned shortcomings related to human factors aspects (visual, audio, or haptic modality); while the second area addressed safety considerations arising due to technological complexity including cybersecurity issues. Based on discussions during five separate breakout sessions focused on these topics, participants recommended guidance for advancing harmonized international technical specifications, better harmonization of testing protocols, and improved public engagement efforts such as establishing common definitions and clear metrics that can support widespread adoption of connected vehicle communications through harmonized global technology standards. Ultimately, adopting universal languages for effective AV interactions, along with ensuring secure connectivity and guarding against potential hacking risks, should lead toward safer roads worldwide while fostering economic growth throughout the transportation ecosystem.",1
"BERT has recently attracted a lot of attention in natural language understanding (NLU) and achieved state-of-the-art results in various NLU tasks. However, its success requires large deep neural networks and huge amount of data, which result in long training time and impede development progress. Using stochastic gradient methods with large mini-batch has been advocated as an efficient tool to reduce the training time. Along this line of research, LAMB is a prominent example that reduces the training time of BERT from 3 days to 76 minutes on a TPUv3 Pod. In this paper, we propose an accelerated gradient method called LANS to improve the efficiency of using large mini-batches for training. As the learning rate is theoretically upper bounded by the inverse of the Lipschitz constant of the function, one cannot always reduce the number of optimization iterations by selecting a larger learning rate. In order to use larger mini-batch size without accuracy loss, we develop a new learning rate scheduler that overcomes the difficulty of using large learning rate. Using the proposed LANS method and the learning rate scheme, we scaled up the mini-batch sizes to 96K and 33K in phases 1 and 2 of BERT pretraining, respectively. It takes 54 minutes on 192 AWS EC2 P3dn.24xlarge instances to achieve a target F1 score of 90.5 or higher on SQuAD v1.1, achieving the fastest BERT training time in the cloud.",0
"Title: ""Accelerating Large Batch Optimization in BERT Preprocessing""  Optimizing deep learning models like BERT requires substantial computational resources and time. In order to reduce the amount of required computation while preserving performance, we propose a novel approach based on large batch gradient descent. This method allows us to train models more quickly by processing larger amounts of data during each iteration, leading to faster optimization overall. We demonstrate that our approach can optimize pretraining of BERT models in as little as 54 minutes using common consumer hardware. Additionally, we provide a detailed analysis of how different hyperparameters affect model accuracy and training speed. Our results show that this approach yields comparable performance compared to standard methods across multiple benchmark datasets, making it a promising tool for practitioners looking to accelerate their own deep learning projects.",1
"Text-to-Face (TTF) synthesis is a challenging task with great potential for diverse computer vision applications. Compared to Text-to-Image (TTI) synthesis tasks, the textual description of faces can be much more complicated and detailed due to the variety of facial attributes and the parsing of high dimensional abstract natural language. In this paper, we propose a Text-to-Face model that not only produces images in high resolution (1024x1024) with text-to-image consistency, but also outputs multiple diverse faces to cover a wide range of unspecified facial features in a natural way. By fine-tuning the multi-label classifier and image encoder, our model obtains the vectors and image embeddings which are used to transform the input noise vector sampled from the normal distribution. Afterwards, the transformed noise vector is fed into a pre-trained high-resolution image generator to produce a set of faces with the desired facial attributes. We refer to our model as TTF-HD. Experimental results show that TTF-HD generates high-quality faces with state-of-the-art performance.",0
"Title: Automatic Face Synthesis from Textual Descriptions using Attributes  Automating face synthesis through natural language descriptions has gained significant attention due to its many applications, including generating virtual avatars, animating characters, and creating computer graphics for movies and games. However, current methods for text-based generation often produce blurry faces that lack fine details, which can lead to a perception of poor quality and realism. To address these limitations, we present a novel framework for high-quality text-to-face (T2F) generation via attribute disentanglement. Our method generates highly detailed facial features by combining continuous facial attributes and corresponding attribute embeddings. In particular, our model learns discrete semantic attributes, such as hair color and glasses, while simultaneously learning their corresponding continuous values. This enables our approach to generate more accurate and diverse faces compared to previous T2F systems, resulting in improved visual fidelity and overall quality. We demonstrate the effectiveness of our method on two challenging benchmarks: UTKFace and FFHQ. Our results show significantly better performance in terms of both quantitative metrics and subjective evaluations by human raters. Overall, our work takes an important step towards automatic face synthesis from textual descriptions, opening up exciting opportunities for creative industries, entertainment, and education.",1
"Temporal grounding of natural language in untrimmed videos is a fundamental yet challenging multimedia task facilitating cross-media visual content retrieval. We focus on the weakly supervised setting of this task that merely accesses to coarse video-level language description annotation without temporal boundary, which is more consistent with reality as such weak labels are more readily available in practice. In this paper, we propose a \emph{Boundary Adaptive Refinement} (BAR) framework that resorts to reinforcement learning (RL) to guide the process of progressively refining the temporal boundary. To the best of our knowledge, we offer the first attempt to extend RL to temporal localization task with weak supervision. As it is non-trivial to obtain a straightforward reward function in the absence of pairwise granular boundary-query annotations, a cross-modal alignment evaluator is crafted to measure the alignment degree of segment-query pair to provide tailor-designed rewards. This refinement scheme completely abandons traditional sliding window based solution pattern and contributes to acquiring more efficient, boundary-flexible and content-aware grounding results. Extensive experiments on two public benchmarks Charades-STA and ActivityNet demonstrate that BAR outperforms the state-of-the-art weakly-supervised method and even beats some competitive fully-supervised ones.",0
"This research focuses on using reinforcement learning (RL) to ground natural language instructions in untrimmed videos. Traditional approaches require dense supervision at each time step of the task, which can be costly to obtain. In contrast, weak supervision allows the use of simpler sources such as textual descriptions or other low-cost annotations. Our approach learns to temporally localize referring expressions within video clips by taking advantage of existing large datasets that provide only weak temporal cues such as image-caption pairs, and extends them via RL. By leveraging imitation learning from human demonstrations, our method improves upon prior work while requiring significantly less data for training. Experiments show that our system outperforms previous methods in terms of accuracy and robustness to instruction variants, generalizing across different domains and instruction types. Overall, our study shows that combining weakly supervised learning with RL is a promising direction towards building intelligent agents capable of interacting with visual media in realistic settings.",1
"Memes are graphics and text overlapped so that together they present concepts that become dubious if one of them is absent. It is spread mostly on social media platforms, in the form of jokes, sarcasm, motivating, etc. After the success of BERT in Natural Language Processing (NLP), researchers inclined to Visual-Linguistic (VL) multimodal problems like memes classification, image captioning, Visual Question Answering (VQA), and many more. Unfortunately, many memes get uploaded each day on social media platforms that need automatic censoring to curb misinformation and hate. Recently, this issue has attracted the attention of researchers and practitioners. State-of-the-art methods that performed significantly on other VL dataset, tends to fail on memes classification. In this context, this work aims to conduct a comprehensive study on memes classification, generally on the VL multimodal problems and cutting edge solutions. We propose a generalized framework for VL problems. We cover the early and next-generation works on VL problems. Finally, we identify and articulate several open research issues and challenges. This is the first study that presents the generalized view of the advanced classification techniques concerning memes classification to the best of our knowledge. We believe this study presents a clear road-map for the Machine Learning (ML) research community to implement and enhance memes classification techniques.",0
"""Memes have become an integral part of modern communication on social media platforms. They allow users to express their ideas and emotions through visual content that can spread rapidly among communities. The classification of memes has thus become increasingly important, as it enables researchers to better understand how they are used by different groups of people. In this paper, we present a comprehensive survey of the state-of-the art multimodal meme classification methods. We provide an overview of existing approaches, including traditional techniques like image recognition and natural language processing (NLP), as well as more recent advances in deep learning algorithms. Additionally, we discuss open challenges in meme classification, such as handling variations in images and text, coping with the dynamic nature of memes, dealing with limited annotated data, and ensuring ethical considerations during the development process.""",1
"It is well observed that in deep learning and computer vision literature, visual data are always represented in a manually designed coding scheme (eg., RGB images are represented as integers ranging from 0 to 255 for each channel) when they are input to an end-to-end deep neural network (DNN) for any learning task. We boldly question whether the manually designed inputs are good for DNN training for different tasks and study whether the input to a DNN can be optimally learned end-to-end together with learning the weights of the DNN. In this paper, we propose the paradigm of {\em deep collective learning} which aims to learn the weights of DNNs and the inputs to DNNs simultaneously for given tasks. We note that collective learning has been implicitly but widely used in natural language processing while it has almost never been studied in computer vision. Consequently, we propose the lookup vision networks (Lookup-VNets) as a solution to deep collective learning in computer vision. This is achieved by associating each color in each channel with a vector in lookup tables. As learning inputs in computer vision has almost never been studied in the existing literature, we explore several aspects of this question through varieties of experiments on image classification tasks. Experimental results on four benchmark datasets, i.e., CIFAR-10, CIFAR-100, Tiny ImageNet, and ImageNet (ILSVRC2012) have shown several surprising characteristics of Lookup-VNets and have demonstrated the advantages and promise of Lookup-VNets and deep collective learning.",0
"In today’s world, deep neural networks (DNNs) have become increasingly popular due to their ability to learn highly complex patterns from large amounts of data. However, learning optimal inputs and weights jointly in these DNNs can still pose significant challenges. To address these issues, we propose the use of deep collective learning (DCL), which leverages both the strengths of human intelligence and artificial intelligence (AI). Our proposed method combines crowdsourcing and meta-learning techniques to iteratively refine input samples that maximize task performance while simultaneously updating network parameters using gradient descent optimization. This allows us to jointly optimize inputs and model parameters towards a better local optimum. Experimental results on several benchmark datasets demonstrate the superiority of our approach over existing methods across different evaluation metrics. Overall, our work represents a major step forward towards developing more efficient and effective approaches for training DNNs.",1
"The spectacular expansion of the Internet has led to the development of a new research problem in the field of natural language processing: automatic toxic comment detection, since many countries prohibit hate speech in public media. There is no clear and formal definition of hate, offensive, toxic and abusive speeches. In this article, we put all these terms under the umbrella of ""toxic"" speech. The contribution of this paper is the design of binary classification and regression-based approaches aiming to predict whether a comment is toxic or not. We compare different unsupervised word representations and different DNN based classifiers. Moreover, we study the robustness of the proposed approaches to adversarial attacks by adding one (healthy or toxic) word. We evaluate the proposed methodology on the English Wikipedia Detox corpus. Our experiments show that using BERT fine-tuning outperforms feature-based BERT, Mikolov's and fastText representations with different DNN classifiers.",0
"This sounds like a fascinating project that has the potential to make a real impact on online discussions. Here's my attempt at creating an abstract for your paper:  Online platforms have become increasingly popular as a means of communication and exchange of ideas, but they can also be plagued by negative behaviors such as cyberbullying and harassment. Toxicity often takes the form of offensive language, personal attacks, hate speech, and other forms of abuse, which can lead to a hostile environment for all users.  The goal of our research is to develop an automatic system capable of detecting toxic comments on social media sites. We use deep neural networks (DNN) to analyze text data and identify patterns associated with toxic behavior. Our approach involves preprocessing the input text data to remove any unwanted characters, followed by training a convolutional neural network (CNN) model. We then fine-tune the model using transfer learning to improve accuracy and reduce false positives.  Our experiments show promising results with a high precision rate of over 97% and recall rate of over 86%. Furthermore, we evaluate the performance of the model on a held-out test set containing examples that were not used during training. Results indicate that the model generalizes well across different domains and scenarios.  Overall, our proposed method represents a significant step towards identifying and addressing harmful content on social media platforms. By automating the process of toxic comment detection, we aim to create more inclusive and respectful online communities where diverse opinions can thrive without fear of abuse.",1
"Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of ""X-former"" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored ""X-former"" models, providing an organized and comprehensive overview of existing work and models across multiple domains.",0
"In recent years, transformer architectures have emerged as a dominant approach in natural language processing tasks due to their ability to capture complex relationships between input variables. However, the computational requirements of these models can be prohibitively high, limiting their application in real-world scenarios where latency and resource constraints must be considered. This survey presents an overview of state-of-the-art methods aimed at improving the efficiency of transformer models without sacrificing performance on downstream tasks. We explore techniques such as knowledge distillation, pruning, quantization, and novel attention mechanisms that enable efficient transformers while maintaining comparable accuracy compared to full-sized models. Our analysis provides insights into tradeoffs between efficiency gains and model quality across different benchmark datasets, enabling researchers and practitioners to make informed decisions regarding which approaches may be most suitable for their specific use cases. Overall, our study demonstrates the significant progress made towards achieving efficient transformer models, paving the way for widespread adoption in various domains requiring NLP capabilities.",1
"Few sample learning (FSL) is significant and challenging in the field of machine learning. The capability of learning and generalizing from very few samples successfully is a noticeable demarcation separating artificial intelligence and human intelligence since humans can readily establish their cognition to novelty from just a single or a handful of examples whereas machine learning algorithms typically entail hundreds or thousands of supervised samples to guarantee generalization ability. Despite the long history dated back to the early 2000s and the widespread attention in recent years with booming deep learning technologies, little surveys or reviews for FSL are available until now. In this context, we extensively review 300+ papers of FSL spanning from the 2000s to 2019 and provide a timely and comprehensive survey for FSL. In this survey, we review the evolution history as well as the current progress on FSL, categorize FSL approaches into the generative model based and discriminative model based kinds in principle, and emphasize particularly on the meta learning based FSL approaches. We also summarize several recently emerging extensional topics of FSL and review the latest advances on these topics. Furthermore, we highlight the important FSL applications covering many research hotspots in computer vision, natural language processing, audio and speech, reinforcement learning and robotic, data analysis, etc. Finally, we conclude the survey with a discussion on promising trends in the hope of providing guidance and insights to follow-up researches.",0
"""Learning from very few samples"" refers to machine learning algorithms that can learn tasks quickly on small datasets (few) and perform well (high accuracy). There has been recent progress towards making these models more robust by studying different ways to improve their performance such as using data augmentation which increases the dataset size by adding noise to original training set and/or combining multiple techniques together. This survey outlines three types of methods for increasing sample sizes: synthetic data generation, real images resizing and warping, and applying image transformations during inference time which makes predictions on transformed test data directly. These works can reduce label mismatch error which occurs when prediction errors are made due to differences between training and test distributions. This allows us to increase our model's ability to generalize better to unseen data without collecting extra annotations, reducing costs substantially. Finally, we discuss some future directions and open questions left in this area.",1
"The interest in Artificial Intelligence (AI) and its applications has seen unprecedented growth in the last few years. This success can be partly attributed to the advancements made in the sub-fields of AI such as Machine Learning (ML), Computer Vision (CV), and Natural Language Processing (NLP). The largest of the growths in these fields has been made possible with deep learning, a sub-area of machine learning, which uses the principles of artificial neural networks. This has created significant interest in the integration of vision and language. The tasks are designed such that they perfectly embrace the ideas of deep learning. In this survey, we focus on ten prominent tasks that integrate language and vision by discussing their problem formulations, methods, existing datasets, evaluation measures, and compare the results obtained with corresponding state-of-the-art methods. Our efforts go beyond earlier surveys which are either task-specific or concentrate only on one type of visual content, i.e., image or video. Furthermore, we also provide some potential future directions in this field of research with an anticipation that this survey brings in innovative thoughts and ideas to address the existing challenges and build new applications.",0
"As machine learning models trained on massive datasets achieve state-of-the-art performance across numerous domains such as object detection, image classification, natural language processing, among others, there has been increased interest in understanding how vision and text can work together to provide more comprehensive solutions to tasks than either modality alone could accomplish. This survey investigates recent trends in research that integrates visual information (such as images) and written language data (such as text). We examine common tasks, datasets, benchmarking efforts, and methodologies used by researchers working at the intersection of computer vision and natural language processing. Our goal is to inform interested practitioners and researchers about this exciting new field and inspire further innovations toward developing multi-modal artificial intelligence systems capable of reasoning about complex scenarios using all available modalities. The integration of vision and language research is an area of increasing interest due to advancements in deep learning techniques. This study examines current trends in the industry including popular tasks and methods, as well as benchmarking efforts and available resources. Research involving both visual information and written language requires collaboration from experts across multiple fields. Practitioners and researchers alike can benefit from a deeper understanding of these cutting edge developments in order to drive future progress towards advanced artificial intelligence systems that can reason effectively through varied forms of input data.",1
"Despite great recent advances in visual tracking, its further development, including both algorithm design and evaluation, is limited due to lack of dedicated large-scale benchmarks. To address this problem, we present LaSOT, a high-quality Large-scale Single Object Tracking benchmark. LaSOT contains a diverse selection of 85 object classes, and offers 1,550 totaling more than 3.87 million frames. Each video frame is carefully and manually annotated with a bounding box. This makes LaSOT, to our knowledge, the largest densely annotated tracking benchmark. Our goal in releasing LaSOT is to provide a dedicated high quality platform for both training and evaluation of trackers. The average video length of LaSOT is around 2,500 frames, where each video contains various challenge factors that exist in real world video footage,such as the targets disappearing and re-appearing. These longer video lengths allow for the assessment of long-term trackers. To take advantage of the close connection between visual appearance and natural language, we provide language specification for each video in LaSOT. We believe such additions will allow for future research to use linguistic features to improve tracking. Two protocols, full-overlap and one-shot, are designated for flexible assessment of trackers. We extensively evaluate 48 baseline trackers on LaSOT with in-depth analysis, and results reveal that there still exists significant room for improvement. The complete benchmark, tracking results as well as analysis are available at http://vision.cs.stonybrook.edu/~lasot/.",0
"Abstract Large scale object tracking (LSOT) has been gaining increasing attention due to its many real world applications, including video surveillance, robotics, autonomous driving, augmented reality, and visual effects. However, despite significant progress in recent years, LSOT remains a challenging problem that requires solving several open problems at the intersection of computer vision, machine learning, and data management.  In this work we present LaSOT, a large-scale single object tracking benchmark designed to evaluate trackers across multiple domains while providing a comprehensive understanding of their strengths and weaknesses. To create our dataset, we collected video clips from publicly available sources such as YouTube, Vimeo, and Dailymotion covering diverse scenarios like crowded urban streets, shopping malls, museums, sport stadiums, car racing events, parks, indoor environments etc. Each clip was annotated manually by carefully following objects through time using bounding boxes. We then randomly split this set into training/validation and test sets where the train set contains 8849 and validation set contains 227 videos having approximately 69K frames each. Finally, all frame annotations were verified to ensure high quality data.  Our experiments show that state-of-the-art trackers still suffer significantly on certain classes of motion patterns, e.g., fast moving objects, deformable objects, occlusions and dynamic background changes, and demonstrate the need for new approaches that can effectively model these behaviors. Overall, LaSOT provides valuable insights into current tracker limitations and highlights opportunities for future research directions in the area of LSOT. In addition, this benchmark would serve as a platform for developing novel algorithms and comparing them against existing ones.",1
"Historical Document Processing is the process of digitizing written material from the past for future use by historians and other scholars. It incorporates algorithms and software tools from various subfields of computer science, including computer vision, document analysis and recognition, natural language processing, and machine learning, to convert images of ancient manuscripts, letters, diaries, and early printed texts automatically into a digital format usable in data mining and information retrieval systems. Within the past twenty years, as libraries, museums, and other cultural heritage institutions have scanned an increasing volume of their historical document archives, the need to transcribe the full text from these collections has become acute. Since Historical Document Processing encompasses multiple sub-domains of computer science, knowledge relevant to its purpose is scattered across numerous journals and conference proceedings. This paper surveys the major phases of, standard algorithms, tools, and datasets in the field of Historical Document Processing, discusses the results of a literature review, and finally suggests directions for further research.",0
"Abstract  This survey provides a comprehensive overview of historical document processing techniques, tools, and trends. We begin by discussing the importance of preserving historical documents and their significance in understanding our past. We then delve into different types of historical documents that require processing, such as handwritten manuscripts, newspapers, government records, maps, and photographs. Next, we examine various approaches used in digital libraries and archives worldwide to process these materials. These methods range from manual data entry and image scanning to more advanced machine learning algorithms and computer vision techniques.  We also review several open-source software applications and programming languages commonly employed in historical document processing projects. These tools offer researchers accessibility, customizability, scalability, and interoperability within a growing community of users. In addition, we highlight key challenges facing practitioners today, including large data volumes, limited resources, inconsistent metadata quality, and varying levels of digitization across collections. Finally, we consider future directions for the field and suggest potential areas of growth based on current trends and emerging technologies.  Our hope is that this survey serves both seasoned professionals looking to stay up-to-date on recent advancements and newcomers seeking a broad introduction to historical document processing techniques, tools, and trends. Ultimately, we aim to foster a better appreciation of the intricacies involved in working with historical documents while providing valuable resources for readers interested in pursuing further inquiry.",1
"In this paper, we propose a novel method for video moment retrieval (VMR) that achieves state of the arts (SOTA) performance on R@1 metrics and surpassing the SOTA on the high IoU metric (R@1, IoU=0.7).   First, we propose to use a multi-head self-attention mechanism, and further a cross-attention scheme to capture video/query interaction and long-range query dependencies from video context. The attention-based methods can develop frame-to-query interaction and query-to-frame interaction at arbitrary positions and the multi-head setting ensures the sufficient understanding of complicated dependencies. Our model has a simple architecture, which enables faster training and inference while maintaining .   Second, We also propose to use multiple task training objective consists of moment segmentation task, start/end distribution prediction and start/end location regression task. We have verified that start/end prediction are noisy due to annotator disagreement and joint training with moment segmentation task can provide richer information since frames inside the target clip are also utilized as positive training examples.   Third, we propose to use an early fusion approach, which achieves better performance at the cost of inference time. However, the inference time will not be a problem for our model since our model has a simple architecture which enables efficient training and inference.",0
"Video moment retrieval refers to the process of searching through large collections of video footage using natural language queries. This can be used in a variety of applications such as surveillance, entertainment, and education. However, current state-of-the-art methods rely heavily on manual annotations which makes scaling these systems challenging. In our work, we propose a novel approach that uses deep learning techniques to automatically label keyframes from videos, allowing them to be searched by natural language queries without any human annotation required. We evaluate our system on several datasets and show significant improvements over baseline models in terms of both accuracy and scalability. Our results demonstrate the potential of using unsupervised learning methods for video moment retrieval tasks.",1
"We address the challenging task of cross-modal moment retrieval, which aims to localize a temporal segment from an untrimmed video described by a natural language query. It poses great challenges over the proper semantic alignment between vision and linguistic domains. Existing methods independently extract the features of videos and sentences and purely utilize the sentence embedding in the multi-modal fusion stage, which do not make full use of the potential of language. In this paper, we present Language Guided Networks (LGN), a new framework that leverages the sentence embedding to guide the whole process of moment retrieval. In the first feature extraction stage, we propose to jointly learn visual and language features to capture the powerful visual information which can cover the complex semantics in the sentence query. Specifically, the early modulation unit is designed to modulate the visual feature extractor's feature maps by a linguistic embedding. Then we adopt a multi-modal fusion module in the second fusion stage. Finally, to get a precise localizer, the sentence information is utilized to guide the process of predicting temporal positions. Specifically, the late guidance module is developed to linearly transform the output of localization networks via the channel attention mechanism. The experimental results on two popular datasets demonstrate the superior performance of our proposed method on moment retrieval (improving by 5.8\% in terms of Rank1@IoU0.5 on Charades-STA and 5.2\% on TACoS). The source code for the complete system will be publicly available.",0
"Effective cross-modal retrieval involves understanding the relationship between textual descriptions and visual representations, as well as modeling interactions between them. Existing methods have difficulty capturing complex semantic dependencies between modalities due to their reliance on hand-crafted features or limited attention mechanisms. We present a novel approach based on guiding neural networks towards human language priors using sentence embeddings trained on vast amounts of natural language data. Our method enables efficient moment matching by leveraging powerful pre-trained models and incorporating structured knowledge from large scale language processing tasks such as machine translation and question answering. Experiments demonstrate significant improvements over state-of-the-art baselines across multiple benchmark datasets, highlighting our approach's ability to effectively capture semantic nuances in both modalities and enable more accurate cross-modal retrieval. With wide ranging applicability spanning multimedia search, recommendation systems, and content creation tools, we envision Language Guided Networks (LGN) becoming a valuable toolkit for cross-disciplinary researchers addressing complex multimodal problems at the intersection of computer vision and NLP communities.",1
"Software developers routinely search for code using general-purpose search engines. However, these search engines cannot find code semantically unless it has an accompanying description. We propose a technique for semantic code search: A Convolutional Neural Network approach to code retrieval (CoNCRA). Our technique aims to find the code snippet that most closely matches the developer's intent, expressed in natural language. We evaluated our approach's efficacy on a dataset composed of questions and code snippets collected from Stack Overflow. Our preliminary results showed that our technique, which prioritizes local interactions (words nearby), improved the state-of-the-art (SOTA) by 5% on average, retrieving the most relevant code snippets in the top 3 (three) positions by almost 80% of the time. Therefore, our technique is promising and can improve the efficacy of semantic code retrieval.",0
"A new approach has been proposed that uses convolutional neural networks (CNN) to improve code retrieval performance by modeling source code files as images. Inspired by recent advances in computer vision, this method leverages the unique features and characteristics of code files that can be captured visually through the use of CNNs. This enables more accurate representation of code snippets and facilitates better matching against large code repositories. To evaluate the effectiveness of the proposed method, experiments were conducted on two benchmark datasets using standard metrics such as precision, recall, F1 score, and MAP (Mean Average Precision). Results showed significant improvement over baseline approaches and demonstrated the potential of the proposed method as a valuable tool for software developers. Overall, this work highlights the promising application of deep learning techniques in code retrieval tasks and suggests possible directions for future research.",1
"The ability to efficiently search for images over an indexed database is the cornerstone for several user experiences. Incorporating user feedback, through multi-modal inputs provide flexible and interaction to serve fine-grained specificity in requirements. We specifically focus on text feedback, through descriptive natural language queries. Given a reference image and textual user feedback, our goal is to retrieve images that satisfy constraints specified by both of these input modalities. The task is challenging as it requires understanding the textual semantics from the text feedback and then applying these changes to the visual representation. To address these challenges, we propose a novel architecture TRACE which contains a hierarchical feature aggregation module to learn the composite visio-linguistic representations. TRACE achieves the SOTA performance on 3 benchmark datasets: FashionIQ, Shoes, and Birds-to-Words, with an average improvement of at least ~5.7%, ~3%, and ~5% respectively in R@K metric. Our extensive experiments and ablation studies show that TRACE consistently outperforms the existing techniques by significant margins both quantitatively and qualitatively.",0
"We present TRACE (Transform Aggregate and Compose VisioLinguistic Representations), an innovative approach that enables image search using textual feedback without directly accessing visual information. By employing visually grounded representations (VGRs) that encode semantically rich knowledge, our method can effectively facilitate crossmodal retrieval tasks through accurate mapping between language inputs and VGRs. Our approach introduces two key components - Vision Language Prompt Ensembling Network (VLPEN) and Multimodal Cross-Modal Attention Fusion Module (MCA FM). VLPEN generates diverse yet meaningful prompts from natural language queries while MCA FM effectively aligns and fuses multimodal features for improved performance. We perform extensive evaluations on popular benchmark datasets and demonstrate the effectiveness of our proposed solution outperforming several state-of-the-art methods by significant margins. In summary, TRACE provides a transformative technique to enhance search capabilities by aggregating and compositing complex VGR representations guided by human feedback alone.",1
"Intensive care clinicians need reliable clinical practice tools to preempt unexpected critical events that might harm their patients in intensive care units (ICU), to pre-plan timely interventions, and to keep the patient's family well informed. The conventional statistical models are built by curating only a limited number of key variables, which means a vast unknown amount of potentially precious data remains unused. Deep learning models (DLMs) can be leveraged to learn from large complex datasets and construct predictive clinical tools. This retrospective study was performed using 42,818 hospital admissions involving 35,348 patients, which is a subset of the MIMIC-III dataset. Natural language processing (NLP) techniques were applied to build DLMs to predict in-hospital mortality (IHM) and length of stay =7 days (LOS). Over 75 million events across multiple data sources were processed, resulting in over 355 million tokens. DLMs for predicting IHM using data from all sources (AS) and chart data (CS) achieved an AUC-ROC of 0.9178 and 0.9029, respectively, and PR-AUC of 0.6251 and 0.5701, respectively. DLMs for predicting LOS using AS and CS achieved an AUC-ROC of 0.8806 and 0.8642, respectively, and PR-AUC of 0.6821 and 0.6575, respectively. The observed AUC-ROC difference between models was found to be significant for both IHM and LOS at p=0.05. The observed PR-AUC difference between the models was found to be significant for IHM and statistically insignificant for LOS at p=0.05. In this study, deep learning models were constructed using data combined from a variety of sources in Electronic Health Records (EHRs) such as chart data, input and output events, laboratory values, microbiology events, procedures, notes, and prescriptions. It is possible to predict in-hospital mortality with much better confidence and higher reliability from models built using all sources of data.",0
"This is an overview of the research study ""All Data Inclusive, Deep Learning Models to Predict Critical Events in the Medical Information Mart for Intensive Care III Database"". The aim of this study was to develop deep learning models capable of predicting critical events within intensive care units by utilizing data from the MIMIC III database. By leveraging innovative machine learning techniques, researchers sought to create accurate and reliable predictors that can assist healthcare providers in managing patient outcomes. Key findings highlighted the effectiveness of these models in accurately predicting critical events such as cardiac arrests, sepsis, acute renal failure, and more. Overall, the results demonstrate the potential benefits of using advanced artificial intelligence methods in improving healthcare delivery and patient safety.",1
"Deep neural networks for natural language processing tasks are vulnerable to adversarial input perturbations. In this paper, we present a versatile language for programmatically specifying string transformations -- e.g., insertions, deletions, substitutions, swaps, etc. -- that are relevant to the task at hand. We then present an approach to adversarially training models that are robust to such user-defined string transformations. Our approach combines the advantages of search-based techniques for adversarial training with abstraction-based techniques. Specifically, we show how to decompose a set of user-defined string transformations into two component specifications, one that benefits from search and another from abstraction. We use our technique to train models on the AG and SST2 datasets and show that the resulting models are robust to combinations of user-defined transformations mimicking spelling mistakes and other meaning-preserving transformations.",0
"This study explores the impact of programmable string transformations on text generation models and presents a methodology that leverages augmentation through automatic transformation synthesis during training to increase model robustness. We demonstrate how automatically derived transformations can provide valuable supplementary data, which enhances the performance of state-of-the-art language generators across several benchmark datasets and evaluation metrics. Our findings emphasize the importance of adaptability in training, as our approach consistently outperforms strong baselines while maintaining competitive results on untransformed input. Additionally, we offer analysis into the behavior of our trained systems under stressful user conditions such as noise injection and multi-step editing operations.",1
"Machine learning methods based on statistical principles have proven highly successful in dealing with a wide variety of data analysis and analytics tasks. Traditional data models are mostly concerned with independent identically distributed data. The recent success of end-to-end modelling scheme using deep neural networks equipped with effective structures such as convolutional layers or skip connections allows the extension to more sophisticated and structured practical data, such as natural language, images, videos, etc. On the application side, vector fields are an extremely useful type of data in empirical sciences, as well as signal processing, e.g. non-parametric transformations of 3D point clouds using 3D vector fields, the modelling of the fluid flow in earth science, and the modelling of physical fields.   This review article is dedicated to recent computational tools of vector fields, including vector data representations, predictive model of spatial data, as well as applications in computer vision, signal processing, and empirical sciences.",0
"In this paper, we review the current state of data modelling techniques as applied to vector fields. We begin by discussing the importance of vector field analysis across multiple disciplines and introduce key notation and concepts used throughout the text. Next, we explore common methods for visualizing vector fields, highlighting their strengths and weaknesses in terms of expressiveness, interpretability, and computational efficiency. Then, we describe popular approaches to generating vector field models using either analytical formulas or data fitting techniques. These range from simple mathematical functions (such as harmonic oscillators) to more complex physics-based simulations that capture fluid dynamics, elasticity, or other phenomena governed by partial differential equations. Finally, we examine applications of vector field data modelling, including simulation verification, optimization, control systems design, feature extraction, clustering analysis, anomaly detection, and machine learning. Throughout the article, we cite relevant case studies and research literature to illustrate real-world implementations of these tools and methodologies. Our goal is to provide readers with a comprehensive overview of vector field data modelling, enabling them to make informed decisions when choosing appropriate methods for specific applications. By distilling the essence of these techniques into a unified framework, we hope to stimulate new ideas, foster interdisciplinary collaboration, and ultimately advance our understanding of scientific and engineering problems involving vector fields.",1
"We propose a cross-modality manifold alignment procedure that leverages triplet loss to jointly learn consistent, multi-modal embeddings of language-based concepts of real-world items. Our approach learns these embeddings by sampling triples of anchor, positive, and negative data points from RGB-depth images and their natural language descriptions. We show that our approach can benefit from, but does not require, post-processing steps such as Procrustes analysis, in contrast to some of our baselines which require it for reasonable performance. We demonstrate the effectiveness of our approach on two datasets commonly used to develop robotic-based grounded language learning systems, where our approach outperforms four baselines, including a state-of-the-art approach, across five evaluation metrics.",0
"""Grounded language understanding requires models that can align different modalities such as vision, text, and sound. Prior work has shown the importance of cross-modality alignment for grounded language learning; however, these methods often rely on ad hoc feature extraction pipelines or have limited scalability due to computation constraints. To address this gap, we propose Practical Cross-Modal Manifold Alignment (PCMA), a novel framework that exploits multi-level representations from pre-trained visual and textual transformers to learn joint embeddings across modalities. Our approach combines multiple instance similarity learning, clustering objectives, and adversarial training, leading to more robust representations at lower computational cost compared to previous methods. We evaluate our model on two benchmark datasets, VG-Captions and MSVD, showing improved performance over competing baseline methods by significant margins. Furthermore, we demonstrate the transferability of PCMA using downstream tasks including VQA2.0 and NLVR$^2$, highlighting its effectiveness for real-world applications.""",1
"An infrastructure for multisite, geographically-distributed creation and collection of diverse, high-quality, curated and labeled radiology image data is crucial for the successful automated development, deployment, monitoring and continuous improvement of Artificial Intelligence (AI)/Machine Learning (ML) solutions in the real world. An interactive radiology reporting approach that integrates image viewing, dictation, natural language processing (NLP) and creation of hyperlinks between image findings and the report, provides localized labels during routine interpretation. These images and labels can be captured and centralized in a cloud-based system. This method provides a practical and efficient mechanism with which to monitor algorithm performance. It also supplies feedback for iterative development and quality improvement of new and existing algorithmic models. Both feedback and monitoring are achieved without burdening the radiologist. The method addresses proposed regulatory requirements for post-marketing surveillance and external data. Comprehensive multi-site data collection assists in reducing bias. Resource requirements are greatly reduced compared to dedicated retrospective expert labeling.",0
"Title: Improving Radiologist Experience Through Integration of Artificial Intelligence (AI) into Imaging Practice  Abstract: Advances in artificial intelligence (AI) have led to significant improvements in imaging analysis and diagnosis in radiology. However, integration of these tools has often been difficult due to lack of standardization and reporting infrastructure. This paper describes the development of a multisite report-based centralized infrastructure designed to facilitate feedback collection on AI algorithms applied clinically, monitor progress over time, and assess the impact of such systems on overall workflow and patient outcomes. By collecting data from multiple hospitals and imaging centers using a unified system, we aim to optimize the adoption of AI technologies in radiologic practice while improving communication among stakeholders. Results of this study will provide guidance for future implementation of AI-driven advancements in radiological research, leading to improved patient care through enhanced diagnostic accuracy and more efficient use of healthcare resources. Our method could serve as a model for successful incorporation of new technology into complex medical environments by streamlining quality assurance and monitoring processes.",1
"Large scale analysis of source code, and in particular scientific source code, holds the promise of better understanding the data science process, identifying analytical best practices, and providing insights to the builders of scientific toolkits. However, large corpora have remained unanalyzed in depth, as descriptive labels are absent and require expert domain knowledge to generate. We propose a novel weakly supervised transformer-based architecture for computing joint representations of code from both abstract syntax trees and surrounding natural language comments. We then evaluate the model on a new classification task for labeling computational notebook cells as stages in the data analysis process from data import to wrangling, exploration, modeling, and evaluation. We show that our model, leveraging only easily-available weak supervision, achieves a 38% increase in accuracy over expert-supplied heuristics and outperforms a suite of baselines. Our model enables us to examine a set of 118,000 Jupyter Notebooks to uncover common data analysis patterns. Focusing on notebooks with relationships to academic articles, we conduct the largest ever study of scientific code and find that notebook composition correlates with the citation count of corresponding papers.",0
"This paper presents a new approach for analyzing data using weakly supervised transformers. Our proposed method, called CORAL (COde RepresentAtion Learning), leverages the power of deep learning algorithms to learn representations of code that can be used for tasks such as program synthesis, bug detection, and software analysis. Unlike traditional approaches, which rely on large amounts of labeled training data, our method only requires weak supervision in the form of natural language queries and program examples. Using these inputs, we train a state-of-the-art transformer model to generate high-quality code representations that capture important semantic features of programs. In extensive experiments, we show that our system outperforms previous methods across a range of challenging benchmarks, demonstrating the effectiveness of our approach for solving real-world problems in programming languages.",1
"This paper is a note on new directions and methodologies for validation and explanation of Machine Learning (ML) models employed for retail credit scoring in finance. Our proposed framework draws motivation from the field of Artificial Intelligence (AI) security and adversarial ML where the need for certifying the performance of the ML algorithms in the face of their overwhelming complexity poses a need for rethinking the traditional notions of model architecture selection, sensitivity analysis and stress testing. Our point of view is that the phenomenon of adversarial perturbations when detached from the AI security domain, has purely algorithmic roots and fall within the scope of model risk assessment. We propose a model criticism and explanation framework based on adversarially generated counterfactual examples for tabular data. A counterfactual example to a given instance in this context is defined as a synthetically generated data point sampled from the estimated data distribution which is treated differently by a model. The counterfactual examples can be used to provide a black-box instance-level explanation of the model behaviour as well as studying the regions in the input space where the model performance deteriorates. Adversarial example generating algorithms are extensively studied in the image and natural language processing (NLP) domains. However, most financial data come in tabular format and naive application of the existing techniques on this class of datasets generates unrealistic samples. In this paper, we propose a counterfactual example generation method capable of handling tabular data including discrete and categorical variables. Our proposed algorithm uses a gradient-free optimization based on genetic algorithms and therefore is applicable to any classification model.",0
"Here is a possible abstract for the PermuteAttack paper:  Credit scorecard models are used by financial institutions to assess the risk of lending money to individual customers. These models rely on machine learning algorithms that analyze large amounts of data to identify patterns and make predictions. However, these models can be difficult to interpret and explain, which makes it challenging for stakeholders to understand how they work and why they make certain decisions. This lack of transparency can lead to mistrust and potentially harmful outcomes, such as discriminatory practices. In this study, we propose a method called PermuteAttack to counteract these issues by providing a causal explanation of credit scorecard models through counterfactual reasoning. We demonstrate how our approach can generate meaningful insights into model behavior and uncover potential biases, enabling more informed decision making and promoting fairer outcomes. Our findings have important implications for the development and deployment of machine learning systems in finance and other sensitive domains where transparency and accountability are crucial.",1
"Online display advertising is growing rapidly in recent years thanks to the automation of the ad buying process. Real-time bidding (RTB) allows the automated trading of ad impressions between advertisers and publishers through real-time auctions. In order to increase the effectiveness of their campaigns, advertisers should deliver ads to the users who are highly likely to be converted (i.e., purchase, registration, website visit, etc.) in the near future. In this study, we introduce and examine different models for estimating the probability of a user converting, given their history of visited URLs. Inspired by natural language processing, we introduce three URL embedding models to compute semantically meaningful URL representations. To demonstrate the effectiveness of the different proposed representation and conversion prediction models, we have conducted experiments on real logged events collected from an advertising platform.",0
"In online marketing, measuring conversion rate from clicks is crucial. There are many ways of targeting users more effectively and increase their response rates to ads. We propose using pre-trained models which learn vector representations for URLs as click targets. These can then be used to find similar sites, by analyzing patterns present in embedding space, and serve them those adverts instead. To show that these proposed methods work well, we compared predictions made using site classification against human annotations and performed better than previous approaches relying solely on metadata such as website type (blog, news etc.). These results suggest new possibilities for improving online advertisement targeting using novel techniques beyond simple keyword matching. More importantly they confirm that unsupervised learning for embedding discovery followed up with supervised training performs better in practice over alternative transfer learning paradigms involving fine tuning pre trained deep neural networks.",1
"In the era of big data, a large number of text data generated by the Internet has given birth to a variety of text representation methods. In natural language processing (NLP), text representation transforms text into vectors that can be processed by computer without losing the original semantic information. However, these methods are difficult to effectively extract the semantic features among words and distinguish polysemy in language. Therefore, a text feature representation model based on convolutional neural network (CNN) and variational autoencoder (VAE) is proposed to extract the text features and apply the obtained text feature representation on the text classification tasks. CNN is used to extract the features of text vector to get the semantics among words and VAE is introduced to make the text feature space more consistent with Gaussian distribution. In addition, the output of the improved word2vec model is employed as the input of the proposed model to distinguish different meanings of the same word in different contexts. The experimental results show that the proposed model outperforms in k-nearest neighbor (KNN), random forest (RF) and support vector machine (SVM) classification algorithms.",0
"In this paper, we present an intelligent text representation model based on Convolutional Neural Network Variational Autoencoders (CNN-VAEs) that can capture both local structure and global context from large amounts of natural language data. Our method utilizes the power of deep learning techniques like VAEs to extract meaningful features that preserve semantics, while at the same time enabling efficient processing of unstructured big data by leveraging parallel computing architectures such as Graphical Processing Units (GPUs). We evaluate our approach through extensive experiments using several benchmark datasets commonly used in NLP tasks, showing superior performance compared to state-of-the art methods in terms of accuracy, efficiency and interpretability. This work contributes to the field of Natural Language Processing (NLP), by providing a scalable solution for handling complex linguistic phenomena across different domains, ultimately paving the road towards more advanced artificial intelligence applications.",1
"Dialog State Tracking (DST) is one of the most crucial modules for goal-oriented dialogue systems. In this paper, we introduce FastSGT (Fast Schema Guided Tracker), a fast and robust BERT-based model for state tracking in goal-oriented dialogue systems. The proposed model is designed for the Schema-Guided Dialogue (SGD) dataset which contains natural language descriptions for all the entities including user intents, services, and slots. The model incorporates two carry-over procedures for handling the extraction of the values not explicitly mentioned in the current user utterance. It also uses multi-head attention projections in some of the decoders to have a better modelling of the encoder outputs. In the conducted experiments we compared FastSGT to the baseline model for the SGD dataset. Our model keeps the efficiency in terms of computational and memory consumption while improving the accuracy significantly. Additionally, we present ablation studies measuring the impact of different parts of the model on its performance. We also show the effectiveness of data augmentation for improving the accuracy without increasing the amount of computational resources.",0
"This is an outline: a) Introduction (no more than one paragraph): i) Explanation of schema-guided dialogue dataset (SGD), which involves agents interacting via natural language following a predefined structure or schema; ii) Importance of dialogue state tracking, or predicting the user’s intent in SGD; b) Background on previous work, including traditional methods and their limitations; c) Our proposed method using BERT and its advantages over existing approaches; d) Results comparing our model against several baselines and showing improved performance; e) Conclusion summarizing findings and potential future directions",1
"We introduce the task of Image-Set Visual Question Answering (ISVQA), which generalizes the commonly studied single-image VQA problem to multi-image settings. Taking a natural language question and a set of images as input, it aims to answer the question based on the content of the images. The questions can be about objects and relationships in one or more images or about the entire scene depicted by the image set. To enable research in this new topic, we introduce two ISVQA datasets - indoor and outdoor scenes. They simulate the real-world scenarios of indoor image collections and multiple car-mounted cameras, respectively. The indoor-scene dataset contains 91,479 human annotated questions for 48,138 image sets, and the outdoor-scene dataset has 49,617 questions for 12,746 image sets. We analyze the properties of the two datasets, including question-and-answer distributions, types of questions, biases in dataset, and question-image dependencies. We also build new baseline models to investigate new research challenges in ISVQA.",0
"Automatically answering questions based on images is a challenging task due to the ambiguity present in natural language queries and variability of objects within images. We propose a novel approach to question answering by considering image sets instead of single images, as they provide more contextual information to resolve the ambiguities. Our method consists of two components: (i) retrieving relevant images from large collections using keyword similarity search, and (ii) selecting the most informative subset of these retrieved images to generate answers using deep learning models. Experimental evaluation on four public datasets shows that our method outperforms state-of-the-art approaches both qualitatively and quantitatively while providing high diversity in generated results. Additionally, we demonstrate applications of our approach beyond Q&A into areas such as story generation and commonsense reasoning. These findings contribute towards developing intelligent systems capable of handling real-world complex tasks in multimedia data analysis.",1
"Video Moment Retrieval (VMR) is a task to localize the temporal moment in untrimmed video specified by natural language query. For VMR, several methods that require full supervision for training have been proposed. Unfortunately, acquiring a large number of training videos with labeled temporal boundaries for each query is a labor-intensive process. This paper explores methods for performing VMR in a weakly-supervised manner (wVMR): training is performed without temporal moment labels but only with the text query that describes a segment of the video. Existing methods on wVMR generate multi-scale proposals and apply query-guided attention mechanisms to highlight the most relevant proposal. To leverage the weak supervision, contrastive learning is used which predicts higher scores for the correct video-query pairs than for the incorrect pairs. It has been observed that a large number of candidate proposals, coarse query representation, and one-way attention mechanism lead to blurry attention maps which limit the localization performance. To handle this issue, Video-Language Alignment Network (VLANet) is proposed that learns sharper attention by pruning out spurious candidate proposals and applying a multi-directional attention mechanism with fine-grained query representation. The Surrogate Proposal Selection module selects a proposal based on the proximity to the query in the joint embedding space, and thus substantially reduces candidate proposals which leads to lower computation load and sharper attention. Next, the Cascaded Cross-modal Attention module considers dense feature interactions and multi-directional attention flow to learn the multi-modal alignment. VLANet is trained end-to-end using contrastive loss which enforces semantically similar videos and queries to gather. The experiments show that the method achieves state-of-the-art performance on Charades-STA and DiDeMo datasets.",0
"This paper presents VLANet (Video-Language Alignment Network), which addresses the problem of weakly-supervised video moment retrieval by aligning videos with natural language descriptions. Previous approaches have relied on costly human annotations or strong temporal supervision, but VLANet utilizes only weak metadata such as text summaries and audio transcriptions. Our method leverages both visual and semantic features through joint attention mechanisms, allowing for more accurate alignment. Extensive experiments demonstrate that our model outperforms state-of-the-art methods across multiple datasets while using fewer resources, making VLANet an effective tool for real-world applications.",1
"With the arising concerns for the AI systems provided with direct access to abundant sensitive information, researchers seek to develop more reliable AI with implicit information sources. To this end, in this paper, we introduce a new task called video description via two multi-modal cooperative dialog agents, whose ultimate goal is for one conversational agent to describe an unseen video based on the dialog and two static frames. Specifically, one of the intelligent agents - Q-BOT - is given two static frames from the beginning and the end of the video, as well as a finite number of opportunities to ask relevant natural language questions before describing the unseen video. A-BOT, the other agent who has already seen the entire video, assists Q-BOT to accomplish the goal by providing answers to those questions. We propose a QA-Cooperative Network with a dynamic dialog history update learning mechanism to transfer knowledge from A-BOT to Q-BOT, thus helping Q-BOT to better describe the video. Extensive experiments demonstrate that Q-BOT can effectively learn to describe an unseen video by the proposed model and the cooperative learning method, achieving the promising performance where Q-BOT is given the full ground truth history dialog.",0
"This paper presents a novel approach to describing unseen videos using multi-modal cooperative dialog agents. We propose a method that combines natural language processing techniques with computer vision algorithms to generate descriptions of video content in real-time. Our model consists of two components: a visual representation generator, which extracts features from raw video data, and a text generation module, which uses these features to produce natural language descriptions. Our system was trained on a large dataset of audio-visual pairs, allowing it to learn patterns in human speech and video imagery. In addition, our model can leverage external knowledge sources such as web articles or human feedback to improve the accuracy and completeness of its descriptions. Results show that our approach significantly outperforms baseline models across several metrics, including semantic coherence, readability, and overall quality. Overall, we believe that our work represents an important step towards developing more robust and adaptive systems for generating descriptive multimedia content.",1
"Significant advances have been made in recent years on Natural Language Processing with machines surpassing human performance in many tasks, including but not limited to Question Answering. The majority of deep learning methods for Question Answering targets domains with large datasets and highly matured literature. The area of Nuclear and Atomic energy has largely remained unexplored in exploiting non-annotated data for driving industry viable applications. Due to lack of dataset, a new dataset was created from the 7000 research papers on nuclear domain. This paper contributes to research in understanding nuclear domain knowledge which is then evaluated on Nuclear Question Answering Dataset (NQuAD) created by nuclear domain experts as part of this research. NQuAD contains 612 questions developed on 181 paragraphs randomly selected from the IGCAR research paper corpus. In this paper, the Nuclear Bidirectional Encoder Representational Transformers (NukeBERT) is proposed, which incorporates a novel technique for building BERT vocabulary to make it suitable for tasks with less training data. The experiments evaluated on NQuAD revealed that NukeBERT was able to outperform BERT significantly, thus validating the adopted methodology. Training NukeBERT is computationally expensive and hence we will be open-sourcing the NukeBERT pretrained weights and NQuAD for fostering further research work in the nuclear domain.",0
"This research presents a pre-trained language model called ""NukeBERT"" designed specifically for low resource nuclear domain tasks. NukeBERT was trained using a novel dataset consisting of both open source text from technical reports and classified documents provided by the National Nuclear Security Administration (NNSA). The proposed approach leverages BERT architecture which has been proven to outperform traditional language models on benchmark datasets, however fine tuning large language models can require vast amounts of computational resources that may not be available at every organization. Therefore, this work contributes a smaller scale version of BERT (base only) as well as a training process that allows users without access to cloud services to use common hardware for development and inference. Evaluation results on several downstream nuclear related classification and regression tasks demonstrate significant improvement over competitive baselines. In summary, this research provides a generalizable framework for developing industry specific natural language processing applications on limited budgets.",1
"Today, the dominant paradigm for training neural networks involves minimizing task loss on a large dataset. Using world knowledge to inform a model, and yet retain the ability to perform end-to-end training remains an open question. In this paper, we present a novel framework for introducing declarative knowledge to neural network architectures in order to guide training and prediction. Our framework systematically compiles logical statements into computation graphs that augment a neural network without extra learnable parameters or manual redesign. We evaluate our modeling strategy on three tasks: machine comprehension, natural language inference, and text chunking. Our experiments show that knowledge-augmented networks can strongly improve over baselines, especially in low-data regimes.",0
"Recent advancements in deep learning have resulted in state-of-the-art models that can perform complex tasks such as image classification, speech recognition, and natural language understanding. However, these models often struggle with tasks that require explicit reasoning and logical inference. In order to address this limitation, we propose augmenting neural networks with first-order logic (FOL), which allows them to reason explicitly about propositions and their relationships.  Our approach integrates FOL formulas into the training process of neural networks by adding a novel regularization term that encourages the model to satisfy the constraints imposed by the FOL formulas. This results in more robust and interpretable predictions by ensuring consistency with respect to known facts and logical rules. Our experimental evaluation demonstrates significant improvements on several benchmark datasets across multiple domains including computer vision, natural language processing, and knowledge reasoning.  Overall, our work shows promise towards enabling AI systems to possess more human-like reasoning abilities while maintaining high levels of efficiency and accuracy in their primary task performance. By bridging the gap between deep learning models and symbolic artificial intelligence systems, we take a step closer to creating intelligent agents capable of solving problems beyond those seen during training.",1
"Many recent advances in neural information retrieval models, which predict top-K items given a query, learn directly from a large training set of (query, item) pairs. However, they are often insufficient when there are many previously unseen (query, item) combinations, often referred to as the cold start problem. Furthermore, the search system can be biased towards items that are frequently shown to a query previously, also known as the 'rich get richer' (a.k.a. feedback loop) problem. In light of these problems, we observed that most online content platforms have both a search and a recommender system that, while having heterogeneous input spaces, can be connected through their common output item space and a shared semantic representation. In this paper, we propose a new Zero-Shot Heterogeneous Transfer Learning framework that transfers learned knowledge from the recommender system component to improve the search component of a content platform. First, it learns representations of items and their natural-language features by predicting (item, item) correlation graphs derived from the recommender system as an auxiliary task. Then, the learned representations are transferred to solve the target search retrieval task, performing query-to-item prediction without having seen any (query, item) pairs in training. We conduct online and offline experiments on one of the world's largest search and recommender systems from Google, and present the results and lessons learned. We demonstrate that the proposed approach can achieve high performance on offline search retrieval tasks, and more importantly, achieved significant improvements on relevance and user interactions over the highly-optimized production system in online experiments.",0
"This paper proposes a novel approach for zero-shot transfer learning from recommender systems to cold-start search retrieval, which addresses the challenging task of ranking documents in response to user queries without any training data. Our method leverages the vast amount of knowledge embedded in pretrained language models to map user intent into ranked lists of candidate documents that are relevant to their query. We demonstrate the effectiveness of our approach on two benchmark datasets and show significant improvements over strong baselines. Additionally, we provide a comprehensive analysis of the factors impacting performance and discuss potential future directions for research in this area. Overall, our work offers valuable insights into the promise and limitations of heterogeneous transfer learning methods for natural language processing tasks under constrained settings.",1
"Given a natural language query, teaching machines to ask clarifying questions is of immense utility in practical natural language processing systems. Such interactions could help in filling information gaps for better machine comprehension of the query. For the task of ranking clarification questions, we hypothesize that determining whether a clarification question pertains to a missing entry in a given post (on QA forums such as StackExchange) could be considered as a special case of Natural Language Inference (NLI), where both the post and the most relevant clarification question point to a shared latent piece of information or context. We validate this hypothesis by incorporating representations from a Siamese BERT model fine-tuned on NLI and Multi-NLI datasets into our models and demonstrate that our best performing model obtains a relative performance improvement of 40 percent and 60 percent respectively (on the key metric of Precision@1), over the state-of-the-art baseline(s) on the two evaluation sets of the StackExchange dataset, thereby, significantly surpassing the state-of-the-art.",0
"This study proposes an approach using natural language inference techniques to rank clarification questions from users seeking answers to their queries. The proposed method uses both lexical cues such as question type (e.g., yes/no vs. open ended) and semantic similarity based on Latent Semantic Analysis to determine the relevance of different follow-up questions. Experimental results show that our ranking model outperforms baseline models and achieves significant improvement in accuracy. The ability to accurately rank clarification questions can have significant implications for search engines, virtual assistants, chatbots, and other intelligent systems designed to respond to user requests for information. By understanding which questions need clarifying and in what order, these systems can provide more accurate responses to their human interlocutors. Our work represents one step toward improving the performance of natural language processing systems, ultimately leading to better communication between humans and computers.",1
"Transformer-based models have achieved stateof-the-art results in many tasks in natural language processing. However, such models are usually slow at inference time, making deployment difficult. In this paper, we develop an efficient algorithm to search for fast models while maintaining model quality. We describe a novel approach to decompose the Transformer architecture into smaller components, and propose a sampling-based one-shot architecture search method to find an optimal model for inference. The model search process is more efficient than alternatives, adding only a small overhead to training time. By applying our methods to BERT-base architectures, we achieve 10% to 30% speedup for pre-trained BERT and 70% speedup on top of a previous state-of-the-art distilled BERT model on Cloud TPU-v2 with a generally acceptable drop in performance.",0
"This paper presents a method for quickly searching for neural network architectures using one-shot architecture search. By composing smaller components such as layers and blocks into larger architectures, we can rapidly evaluate different designs and find high-performing models efficiently. Our approach uses a novel weight initialization scheme that improves performance and stability, allowing us to identify strong architectures in just a few shots. We demonstrate our framework on several benchmark datasets including image classification and machine translation tasks. Results show that our one-shot architecture search outperforms existing methods while requiring significantly fewer resources. Overall, this work advances the state of art in neural architecture search and provides valuable insights for designing effective deep learning systems.",1
"Cross-domain alignment between image objects and text sequences is key to many visual-language tasks, and it poses a fundamental challenge to both computer vision and natural language processing. This paper investigates a novel approach for the identification and optimization of fine-grained semantic similarities between image and text entities, under a weakly-supervised setup, improving performance over state-of-the-art solutions. Our method builds upon recent advances in optimal transport (OT) to resolve the cross-domain matching problem in a principled manner. Formulated as a drop-in regularizer, the proposed OT solution can be efficiently computed and used in combination with other existing approaches. We present empirical evidence to demonstrate the effectiveness of our approach, showing how it enables simpler model architectures to outperform or be comparable with more sophisticated designs on a range of vision-language tasks.",0
"In this paper, we introduce weakly supervised cross-domain alignment with optimal transport (WSCOT), a new framework for aligning two domains that have different distributions of features but share some common underlying structure. Our approach leverages both labeled data from one domain and unlabeled data from another, allowing us to learn a map between the two domains that preserves important geometric relationships while remaining robust to variations in feature density. By optimizing a novel regularized optimal transport objective function, we ensure that our learned mapping admits desirable properties such as smoothness, non-negativity, and invertibility, which can be used directly for applications like image registration and transfer learning. Extensive experiments on synthetic and real datasets demonstrate the effectiveness and efficiency of WSCOT relative to state-of-the-art methods under various settings. This work paves the way towards enabling reliable models trained on one domain to generalize well across multiple heterogeneous datasets, opening up exciting possibilities in machine learning research and practice.",1
"Symbolic regression is a powerful technique that can discover analytical equations that describe data, which can lead to explainable models and generalizability outside of the training data set. In contrast, neural networks have achieved amazing levels of accuracy on image recognition and natural language processing tasks, but are often seen as black-box models that are difficult to interpret and typically extrapolate poorly. Here we use a neural network-based architecture for symbolic regression called the Equation Learner (EQL) network and integrate it with other deep learning architectures such that the whole system can be trained end-to-end through backpropagation. To demonstrate the power of such systems, we study their performance on several substantially different tasks. First, we show that the neural network can perform symbolic regression and learn the form of several functions. Next, we present an MNIST arithmetic task where a separate part of the neural network extracts the digits. Finally, we demonstrate prediction of dynamical systems where an unknown parameter is extracted through an encoder. We find that the EQL-based architecture can extrapolate quite well outside of the training data set compared to a standard neural network-based architecture, paving the way for deep learning to be applied in scientific exploration and discovery.",0
"This paper presents a novel approach to integrating neural network-based symbolic regression into deep learning frameworks for scientific discovery. Traditionally, symbolic regression has been used as a standalone technique for modeling and predicting complex data sets in science and engineering. However, recent advances in deep learning have shown promise in achieving better accuracy and generalization performance compared to traditional methods such as linear regression and polynomial regression. By combining neural networks and symbolic regression techniques, we can leverage the strengths of both approaches while mitigating their respective weaknesses.  The proposed method uses a multi-objective evolutionary algorithm that searches over the space of neural networks and symbolic expressions. At each generation, new candidate solutions are generated by either mutating existing solutions or applying genetic operators such as crossbreeding or particle swarm optimization. These candidates are then evaluated using a set of objective functions that measure the accuracy, diversity, and interpretability of the models produced.  To demonstrate the effectiveness of our proposed methodology, we applied it to several real-world problems across different domains including physics, chemistry, biology, and finance. Our results show that the integrated system was able to accurately model complex relationships among inputs and outputs in these diverse fields. Furthermore, the symbolic representations obtained through our method were often interpretable and provided insights into the underlying mechanisms driving the observed phenomena.  In summary, this work represents an important step towards developing more powerful and flexible tools for scientific discovery. By leveraging cutting-edge machine learning technologies, we hope to enable scientists and engineers alike to uncover new knowledge from large datasets faster and more effectively than ever before.",1
"Identifying common patterns among events is a key ability in human and machine perception, as it underlies intelligent decision making. We propose an approach for learning semantic relational set abstractions on videos, inspired by human learning. We combine visual features with natural language supervision to generate high-level representations of similarities across a set of videos. This allows our model to perform cognitive tasks such as set abstraction (which general concept is in common among a set of videos?), set completion (which new video goes well with the set?), and odd one out detection (which video does not belong to the set?). Experiments on two video benchmarks, Kinetics and Multi-Moments in Time, show that robust and versatile representations emerge when learning to recognize commonalities among sets. We compare our model to several baseline algorithms and show that significant improvements result from explicitly learning relational abstractions with semantic supervision.",0
"This paper presents a method for modeling semantic relational set abstractions (SRSA) in videos. SRSA refers to the ability of humans to group objects based on their similarity and categorize them into sets that share common characteristics. While there have been previous attempts at modeling SRSA in text and images, few studies have explored how this process occurs in video content. Our approach involves identifying clusters of similar frames within a video sequence using a self-supervised learning algorithm, followed by applying a graph convolutional network to learn high-level representations of these clusters. These representations capture important features such as motion patterns, object dynamics, and spatio-temporal relationships among entities. Experimental results demonstrate the effectiveness of our approach, outperforming state-of-the-art methods on several benchmark datasets. Our findings suggest that our framework provides new insights into understanding human cognition and could potentially enhance applications in computer vision and multimedia processing.",1
"Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning. Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the deep learning is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide ""obviously"" interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that (1) clinicians and practitioners can subsequently approach these methods with caution, (2) insights into interpretability will be born with more considerations for medical practices, and (3) initiatives to push forward data-based, mathematically- and technically-grounded medical education are encouraged.",0
"""Abstract: Artificial intelligence (AI) has become increasingly popular in recent years due to its ability to automate complex tasks and solve problems that would otherwise require significant human effort. However, despite these advantages, many AI systems remain 'black boxes,' meaning that their inner workings are difficult for humans to understand. This lack of transparency can lead to concerns regarding safety, privacy, fairness, accountability, and trustworthiness, especially in high stakes applications such as medicine. As a result, there is growing interest in explainable artificial intelligence (XAI), which refers to AI methods that are able to provide clear explanations of how they reach their decisions. In this survey paper, we present an overview of the state-of-the-art in medical XAI, focusing specifically on computational models used for drug discovery and development."" ------------------- The main goal of our study was to conduct a comprehensive review of current research and applications of XAI techniques in the field of medicine. We focused on both the benefits and challenges associated with using XAI in healthcare settings. Our objectives were threefold: firstly, we aimed to identify key trends and directions emerging from existing literature; secondly, we sought to evaluate the limitations and potential risks posed by unexplained AI decisions in medical environments; thirdly, we discussed novel approaches and future perspectives for medical XAI. Given the growing importance of understanding how machine learning algorithms make predictions, we believe this survey fills a gap in understanding the role of explainability in shaping AI technologies and informing policy discussions related to them. By providing insights into the current landscape, our findings serve as a foundation for further exploration of the design choices surrounding interpretabi",1
"Manipulating visual attributes of images through human-written text is a very challenging task. On the one hand, models have to learn the manipulation without the ground truth of the desired output. On the other hand, models have to deal with the inherent ambiguity of natural language. Previous research usually requires either the user to describe all the characteristics of the desired image or to use richly-annotated image captioning datasets. In this work, we propose a novel unsupervised approach, based on image-to-image translation, that alters the attributes of a given image through a command-like sentence such as ""change the hair color to black"". Contrarily to state-of-the-art approaches, our model does not require a human-annotated dataset nor a textual description of all the attributes of the desired image, but only those that have to be modified. Our proposed model disentangles the image content from the visual attributes, and it learns to modify the latter using the textual description, before generating a new image from the content and the modified attribute representation. Because text might be inherently ambiguous (blond hair may refer to different shadows of blond, e.g. golden, icy, sandy), our method generates multiple stochastic versions of the same translation. Experiments show that the proposed model achieves promising performances on two large-scale public datasets: CelebA and CUB. We believe our approach will pave the way to new avenues of research combining textual and speech commands with visual attributes.",0
"In our research we aim to develop an image translation approach that requires only limited amounts of textual guidance to generate high quality images without using any supervision during training. This unsupervised model uses a novel formulation for self-supervised learning through cycle consistency, where two different mappings (forward and reverse) constrain each other. Our method can use large unlabelled datasets, thus enabling us to learn more complex representations on diverse domains with many examples. Extensive experiments show better results than comparative methods across multiple tasks including realism metrics, qualitative evaluation by human annotators, perceptual study, as well as applications such as face hallucination. We achieve quantitatively and visually compelling results even if only small numbers of texts are available. Ablation studies confirm the effectiveness of the proposed components, while analysing their contributions towards disentanglement properties, visual fidelity preservation and robustness against noisy texts. To summarize, we propose an innovative framework for image-to-image translation problems which significantly advances prior work with improved performance, generalization ability, robustness, and interpretability. We believe these advantages make it applicable to numerous scenarios beyond computer vision, where fine-grained control over generated outputs may otherwise require strong supervision and/or expert knowledge. As future work, extensions could explore additional types of guidance such as semantic maps, adversarial feedback, or reinforcement signals to further improve the flexibility and applicability of our technique under various circumstances.",1
"We addressed the challenging task of video question answering, which requires machines to answer questions about videos in a natural language form. Previous state-of-the-art methods attempt to apply spatio-temporal attention mechanism on video frame features without explicitly modeling the location and relations among object interaction occurred in videos. However, the relations between object interaction and their location information are very critical for both action recognition and question reasoning. In this work, we propose to represent the contents in the video as a location-aware graph by incorporating the location information of an object into the graph construction. Here, each node is associated with an object represented by its appearance and location features. Based on the constructed graph, we propose to use graph convolution to infer both the category and temporal locations of an action. As the graph is built on objects, our method is able to focus on the foreground action contents for better video question answering. Lastly, we leverage an attention mechanism to combine the output of graph convolution and encoded question features for final answer reasoning. Extensive experiments demonstrate the effectiveness of the proposed methods. Specifically, our method significantly outperforms state-of-the-art methods on TGIF-QA, Youtube2Text-QA, and MSVD-QA datasets. Code and pre-trained models are publicly available at: https://github.com/SunDoge/L-GCN",0
"Recent works have demonstrated that convolutional networks can effectively extract features from videos for answering questions about them. However, these approaches often require large amounts of annotated data and rely heavily on handcrafted feature extraction, making them difficult to adapt to new domains. To address these limitations, we propose a novel approach called location-aware graph convolutional networks (LAGCN) which can efficiently learn representations of visual content by leveraging structured knowledge graphs as well as raw video frames and their corresponding textual context. LAGCN integrates both bottom-up and top-down cues to improve its effectiveness in understanding complex relationships among entities within videos and their associated queries. We demonstrate state-of-the-art performance across three challenging benchmark datasets: CharadesQA, TGIF-QA, and VizWiz, using just small amounts of training data. Our results indicate that LAGCN provides a powerful framework for modeling spatio-temporal relationships in videos towards accurate question answering and holds great potential for a wide range of real-world applications.",1
"Zero-shot learning (ZSL) is commonly used to address the very pervasive problem of predicting unseen classes in fine-grained image classification and other tasks. One family of solutions is to learn synthesised unseen visual samples produced by generative models from auxiliary semantic information, such as natural language descriptions. However, for most of these models, performance suffers from noise in the form of irrelevant image backgrounds. Further, most methods do not allocate a calculated weight to each semantic patch. Yet, in the real world, the discriminative power of features can be quantified and directly leveraged to improve accuracy and reduce computational complexity. To address these issues, we propose a novel framework called multi-patch generative adversarial nets (MPGAN) that synthesises local patch features and labels unseen classes with a novel weighted voting strategy. The process begins by generating discriminative visual features from noisy text descriptions for a set of predefined local patches using multiple specialist generative models. The features synthesised from each patch for unseen classes are then used to construct an ensemble of diverse supervised classifiers, each corresponding to one local patch. A voting strategy averages the probability distributions output from the classifiers and, given that some patches are more discriminative than others, a discrimination-based attention mechanism helps to weight each patch accordingly. Extensive experiments show that MPGAN has significantly greater accuracy than state-of-the-art methods.",0
"In recent years, generative zero-shot learning (GZSL) has emerged as a promising approach to recognizing visual patches across multiple object classes without requiring labeled examples from these novel classes. However, state-of-the-art GZSL methods still face significant challenges such as low accuracy and limited generalization ability. To address these issues, we propose an ensemble learning perspective that combines multiple GZSL models into a unified framework, termed Ensemble GZSL (EGZSL). Our key insight is that different GZSL models capture complementary knowledge about the data distribution, which can be leveraged to improve overall performance. We demonstrate the effectiveness of our EGZSL framework on two popular benchmark datasets, CUB-200-2011 and SUN Fine Art, and show that it significantly outperforms baseline approaches by achieving higher rank-N recognition accuracies than any individual model alone. Furthermore, we conduct extensive ablation studies to investigate the impact of various design choices on the EGZSL framework's performance, shedding light on how to build more effective ensembles for future work in GZSL. This work represents a step forward in advancing the field of image classification using zero-shot learning techniques and offers new insights into the role of ensemble learning in improving performance under complex settings like those encountered in GZSL tasks.",1
"Most existing text-to-image synthesis tasks are static single-turn generation, based on pre-defined textual descriptions of images. To explore more practical and interactive real-life applications, we introduce a new task - Interactive Image Editing, where users can guide an agent to edit images via multi-turn textual commands on-the-fly. In each session, the agent takes a natural language description from the user as the input and modifies the image generated in the previous turn to a new design, following the user description. The main challenges in this sequential and interactive image generation task are two-fold: 1) contextual consistency between a generated image and the provided textual description; 2) step-by-step region-level modification to maintain visual consistency across the generated image sequence in each session. To address these challenges, we propose a novel Sequential Attention Generative Adversarial Net-work (SeqAttnGAN), which applies a neural state tracker to encode the previous image and the textual description in each turn of the sequence, and uses a GAN framework to generate a modified version of the image that is consistent with the preceding images and coherent with the description. To achieve better region-specific refinement, we also introduce a sequential attention mechanism into the model. To benchmark on the new task, we introduce two new datasets, Zap-Seq and DeepFashion-Seq, which contain multi-turn sessions with image-description sequences in the fashion domain. Experiments on both datasets show that the proposed SeqAttnGANmodel outperforms state-of-the-art approaches on the interactive image editing task across all evaluation metrics including visual quality, image sequence coherence, and text-image consistency.",0
"This paper proposes a novel interactive image editing framework based on generative adversarial networks (GANs). Our approach is designed to enable users to edit images by sequentially attending to different regions of interest, which allows for fine-grained control over the generated results. To achieve this goal, we introduce a new architecture called Sequential Attention Generative Adversarial Network (SAGAN), which combines attention mechanisms with a conditional GAN. Inspired by visual attention models, SAGAN enables the generator to focus on specific parts of the input image during the synthesis process, while preserving global coherence. We demonstrate that our method can effectively manipulate local details in real-time while maintaining global consistency and perceptual quality. Extensive experimental evaluations confirm the effectiveness of our method compared to state-of-the-art baselines under both quantitative and qualitative metrics. Overall, our work represents a significant step towards enabling more intuitive and efficient image editing interfaces through deep learning techniques.",1
"We consider the problem of segmenting image regions given a natural language phrase, and study it on a novel dataset of 77,262 images and 345,486 phrase-region pairs. Our dataset is collected on top of the Visual Genome dataset and uses the existing annotations to generate a challenging set of referring phrases for which the corresponding regions are manually annotated. Phrases in our dataset correspond to multiple regions and describe a large number of object and stuff categories as well as their attributes such as color, shape, parts, and relationships with other entities in the image. Our experiments show that the scale and diversity of concepts in our dataset poses significant challenges to the existing state-of-the-art. We systematically handle the long-tail nature of these concepts and present a modular approach to combine category, attribute, and relationship cues that outperforms existing approaches.",0
"In recent years, advances in computer vision have led to significant progress in image segmentation techniques that can automatically separate objects from their surroundings in images and videos. Despite these advancements, many methods still require manual annotation or specialized training data, which can be time consuming and limit their applicability in real-world scenarios. This paper presents a new method called PhraseCut, which leverages language as a powerful prior to solve this problem without any explicit supervision. By treating object boundaries as natural phrase boundaries within textual descriptions, our approach can effectively segregate objects of interest in unconstrained environments using only raw image input. Our experiments show that PhraseCut outperforms state-of-the-art models on several benchmark datasets while requiring less computational resources, making it an attractive option for deploying efficient image segmentation tools across diverse domains. Additionally, we demonstrate the versatility of our technique by applying it to challenging use cases such as fine-grained material recognition, person keypoint estimation, and medical imaging analysis, highlighting its broad potential beyond traditional bounding box predictions. Overall, this work represents a major step towards more generalizable artificial intelligence solutions through language, where machines can learn complex tasks solely from text descriptions rather than massive amounts of labeled examples.",1
"Textures in natural images can be characterized by color, shape, periodicity of elements within them, and other attributes that can be described using natural language. In this paper, we study the problem of describing visual attributes of texture on a novel dataset containing rich descriptions of textures, and conduct a systematic study of current generative and discriminative models for grounding language to images on this dataset. We find that while these models capture some properties of texture, they fail to capture several compositional properties, such as the colors of dots. We provide critical analysis of existing models by generating synthetic but realistic textures with different descriptions. Our dataset also allows us to train interpretable models and generate language-based explanations of what discriminative features are learned by deep networks for fine-grained categorization where texture plays a key role. We present visualizations of several fine-grained domains and show that texture attributes learned on our dataset offer improvements over expert-designed attributes on the Caltech-UCSD Birds dataset.",0
"Introduction: Textures play an important role in our daily lives, as they provide us with valuable information about objects that we come into contact with on a regular basis. For example, textures can indicate whether an object is rough or smooth, hard or soft, warm or cold, wet or dry, etc. However, describing these different types of textures is no easy feat - especially given the limited capabilities of natural language processing (NLP) systems today. This paper presents a novel approach to describe textures using natural language. Our method uses NLP techniques such as sentiment analysis, topic modeling, and named entity recognition to extract relevant features from the input descriptions. These features are then used to build a texture description database which can be queried by users to find relevant examples. We evaluate our method on several datasets containing textual descriptions of objects and compare it against state-of-the-art methods. Results show that our method achieves better accuracy than existing approaches while providing richer and more detailed descriptions of textures. Conclusion: In conclusion, this research has presented a new method for describing textures using natural language. By leveraging the power of NLP techniques like sentiment analysis, topic modeling, and named entity recognition, our method allows users to easily query a database of texture descriptions in order to find relevant examples. This work represents an important step towards developing more advanced NLP models capable of accurately describing complex real world phenomena such as textures. Future work includes expanding the scope of our dataset to cover a wider range of textures, and exploring other applications of our method beyond texture description. Overall, our work has shown promising results in tackling this challenging problem and paves the way for further advancements in the field of NLP.",1
"We present a new video understanding pentathlon challenge, an open competition held in conjunction with the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2020. The objective of the challenge was to explore and evaluate new methods for text-to-video retrieval-the task of searching for content within a corpus of videos using natural language queries. This report summarizes the results of the first edition of the challenge together with the findings of the participants.",0
"This research is focused on advancing video understanding by presenting new benchmarks and evaluation metrics that measure a model's ability across five key tasks - object detection, instance segmentation, semantic segmentation, motion estimation, and visual SLAM - which together span the entire pipeline of vision problems that arise in modern computer graphics applications. The proposed end-to-end system addresses these challenges through an iterative architecture where each task solution is used as input to improve performance of subsequent tasks. Results show significant improvement over traditional single-task networks in both speed and accuracy under realistic training settings, demonstrating the importance of pushing the boundaries of current methods beyond isolated solutions towards integrated systems able to solve complex scenarios arising in multimedia content creation and other domains involving large amounts of unstructured visual data.",1
"Overparameterized networks trained to convergence have shown impressive performance in domains such as computer vision and natural language processing. Pushing state of the art on salient tasks within these domains corresponds to these models becoming larger and more difficult for machine learning practitioners to use given the increasing memory and storage requirements, not to mention the larger carbon footprint. Thus, in recent years there has been a resurgence in model compression techniques, particularly for deep convolutional neural networks and self-attention based networks such as the Transformer.   Hence, this paper provides a timely overview of both old and current compression techniques for deep neural networks, including pruning, quantization, tensor decomposition, knowledge distillation and combinations thereof.   We assume a basic familiarity with deep learning architectures\footnote{For an introduction to deep learning, see ~\citet{goodfellow2016deep}}, namely, Recurrent Neural Networks~\citep[(RNNs)][]{rumelhart1985learning,hochreiter1997long}, Convolutional Neural Networks~\citep{fukushima1980neocognitron}~\footnote{For an up to date overview see~\citet{khan2019survey}} and Self-Attention based networks~\citep{vaswani2017attention}\footnote{For a general overview of self-attention networks, see ~\citet{chaudhari2019attentive}.},\footnote{For more detail and their use in natural language processing, see~\citet{hu2019introductory}}. Most of the papers discussed are proposed in the context of at least one of these DNN architectures.",0
"This paper provides an overview of neural network compression, which has become increasingly important as the size and complexity of deep learning models continue to grow. We discuss the motivation behind compressing neural networks, including reducing computational resources required for deployment, making them more deployable on edge devices, improving their interpretability, and enabling faster training processes. Then we provide a survey of existing methods for neural network compression including pruning techniques such as weight magnitude-based approaches, sparsity regularization, and iterative pruning; quantization techniques using low-bit precision representations of weights and activations; knowledge distillation techniques that train smaller student networks by mimicking the outputs of larger teacher networks; and model optimization techniques aimed at improving accuracy efficiency tradeoffs under resource constraints. Finally, we present challenges and future directions for research in neural network compression.",1
"Recently, large scale Transformer-based language models such as BERT, GPT-2, and XLNet have brought about exciting leaps in state-of-the-art results for many Natural Language Processing (NLP) tasks. One of the common trends in these recent models is a significant increase in model complexity, which introduces both more weights and computation. Moreover, with the advent of large-scale unsupervised datasets, training time is further extended due to the increased amount of data samples within a single training epoch. As a result, to train these models within a reasonable time, machine learning (ML) programmers often require advanced hardware setups such as the premium GPU-enabled NVIDIA DGX workstations or specialized accelerators such as Google's TPU Pods. Our work addresses this limitation and demonstrates that the BERT pre-trained model can be trained within 2 weeks on an academic-size cluster of widely available GPUs through careful algorithmic and software optimizations. In this paper, we present these optimizations on how to improve single device training throughput, distribute the training workload over multiple nodes and GPUs, and overcome the communication bottleneck introduced by the large data exchanges over the network. We show that we are able to perform pre-training on BERT within a reasonable time budget (12 days) in an academic setting, but with a much less expensive and less aggressive hardware resource requirement than in previously demonstrated industrial settings based on NVIDIA DGX machines or Google's TPU Pods.",0
"Title: Multi-node BERT pretraining: A cost-effective solution  This paper presents a multi-node pretraining approach that significantly reduces computational costs while maintaining high performance accuracy. The proposed method leverages distributed computing to train multiple instances of BERT (Bidirectional Encoder Representations from Transformers) on different nodes simultaneously. This parallel training setup enables faster convergence, which results in higher overall efficiency. To maximize resource utilization and minimize idle time, we introduce a dynamic node allocation strategy based on workload monitoring. Furthermore, our algorithm optimizes communication overhead through efficient data partitioning and adaptive batch size adjustment. We conduct experiments using large datasets such as BookCorpus and demonstrate that our approach achieves comparable perplexity scores at a fraction of the cost compared to single-node fine-tuning methods. Our findings have significant implications for natural language processing research and applications where scalability and affordability are crucial factors. Overall, the proposed multi-node BERT pretraining framework is a cost-effective alternative for advanced NLP tasks without compromising model quality.",1
"It has been found that software, like natural language texts, exhibits ""naturalness"", which can be captured by statistical language models. In recent years, neural language models have been proposed to represent the naturalness of software through deep learning. In this paper, we conduct an experimental evaluation of state-of-the-art neural language models for source code, including RNN-based models and Transformer-XL based models. Through experiments on a large-scale Python code corpus, we find that the Transformer-XL model outperforms RNN-based models (including LSTM and GRU models) in capturing the naturalness of software, with far less computational cost.",0
"Despite decades of research on natural language processing (NLP), little progress has been made in creating models that can effectively learn from source code data. This is largely due to the unique challenges presented by programming languages, such as their ambiguous nature and reliance on context. In recent years, deep learning techniques have shown promise in addressing these difficulties through advanced architectures like transformers. One such architecture, Transformer-XL, allows for powerful NLP tasks including sequence generation and classification, but has yet to be applied extensively in the domain of program synthesis.  This paper seeks to fill this gap by presenting a novel approach using Transformer-XL for language modelling of source code. Our method leverages the transformer's parallel processing capabilities and attention mechanism to capture rich relationships between tokens in the code. We then train our model on large amounts of real-world code corpora and evaluate its effectiveness on several benchmark datasets commonly used to assess program synthesis performance.  Our results show that our model significantly outperforms traditional NLP baselines across multiple metrics, demonstrating its ability to generate high-quality programs quickly and efficiently. Furthermore, we conduct experiments to analyze the impact of different hyperparameters and ablation studies to better understand how our model works at a deeper level. These analyses provide insight into which design choices may lead to improved performance in future work. Overall, our paper offers a promising new direction for research in programming language understanding and synthesis, paving the way for more robust tools for developers in areas such as automatic bug fixing, refactoring, and code completion.",1
"Detecting clinically relevant objects in medical images is a challenge despite large datasets due to the lack of detailed labels. To address the label issue, we utilize the scene-level labels with a detection architecture that incorporates natural language information. We present a challenging new set of radiologist paired bounding box and natural language annotations on the publicly available MIMIC-CXR dataset especially focussed on pneumonia and pneumothorax. Along with the dataset, we present a joint vision language weakly supervised transformer layer-selected one-stage dual head detection architecture (LITERATI) alongside strong baseline comparisons with class activation mapping (CAM), gradient CAM, and relevant implementations on the NIH ChestXray-14 and MIMIC-CXR dataset. Borrowing from advances in vision language architectures, the LITERATI method demonstrates joint image and referring expression (objects localized in the image using natural language) input for detection that scales in a purely weakly supervised fashion. The architectural modifications address three obstacles -- implementing a supervised vision and language detection method in a weakly supervised fashion, incorporating clinical referring expression natural language information, and generating high fidelity detections with map probabilities. Nevertheless, the challenging clinical nature of the radiologist annotations including subtle references, multi-instance specifications, and relatively verbose underlying medical reports, ensures the vision language detection task at scale remains stimulating for future investigation.",0
"This should include: Objective/aim (in past tense): In this study we aimed at detecting diseases like pneumonia and pneumothorax from X-rays by training deep neural nets on automatically generated weak labels that were derived from human readable reports. Methods: To generate these weak labels we used natural language processing techniques such as named entity recognition and relation extraction. These NER + RE pipelines then classified whether there was evidence of any pneumonia present - ""Positive"" and if so could differentiate between left lung and right lung involvement. For PTX we simply did presence detection. These models had F1 scores above 92%. Then for each image in our dataset we performed weakly supervised localization via attention over object proposals yielding bounding boxes and corresponding classification probabilities. Results: On average per image roughly half of all proposed regions were found positive and nearly 80% of those with non-zero objects had probability greater than 0.5 to have some label associated with them. After evaluating the model using cross validation we achieved high AP averages across all three settings: 76%, 84% and 85%. Conclusion: Our results suggest that although we only have weakly labeled data available which can lead to noisy pseudo ground truths, with proper preprocessing of both imagery and text sources strong performance can still be achieved detecting diseases through computer vision and natural language interfaces. This work presents a novel approach to detecting diseases from radiographs using weakly supervised methods. Despite having limited annotated data, the authors successfully trained neural networks to identify conditions such as pneumonia and pneumothorax by leveraging automatic label generation based on natural lan",1
"Person search by natural language aims at retrieving a specific person in a large-scale image pool that matches the given textual descriptions. While most of the current methods treat the task as a holistic visual and textual feature matching one, we approach it from an attribute-aligning perspective that allows grounding specific attribute phrases to the corresponding visual regions. We achieve success as well as the performance boosting by a robust feature learning that the referred identity can be accurately bundled by multiple attribute visual cues. To be concrete, our Visual-Textual Attribute Alignment model (dubbed as ViTAA) learns to disentangle the feature space of a person into subspaces corresponding to attributes using a light auxiliary attribute segmentation computing branch. It then aligns these visual features with the textual attributes parsed from the sentences by using a novel contrastive learning loss. Upon that, we validate our ViTAA framework through extensive experiments on tasks of person search by natural language and by attribute-phrase queries, on which our system achieves state-of-the-art performances. Code will be publicly available upon publication.",0
"One way that law enforcement agencies can improve their ability to track down suspects is through the use of artificial intelligence (AI) technology. For example, natural language processing algorithms can be used to analyze text data from social media posts, emails, and other sources to identify patterns related to criminal activity. This can help police officers quickly gather evidence and make arrests before criminals have time to flee or destroy evidence. However, these methods still rely on linguistics and cannot provide any visual information such as images, videos and audio files which could be vital in certain cases like facial recognition, behavior analysis, etc. In order to gain more accurate results, there needs to be a methodology which uses both textual(NLP) and visual(Image and video analysis) modalities simultaneously. We propose one possible solution named ViTAA i.e.,Visual - Textual Attribute Alignments in person search using Natural Language processing . ViTAA proposes an end-to-end neural network architecture that jointly models person attributes across textual descriptions, image features, and video frame-level feature maps. Our proposed approach aligns cross-modal attribute spaces to predict missing attributes based solely on available modality, thereby providing better alignment between textual attributes and vision representations. Our extensive experiments show significant improvements over state-of-the-art competitors on four benchmark datasets namely FlickrFaces, CU-FSL-V2 , PRW-YHZH and TGTPS. Overall, our work represents a crucial step towards integrating cross-modality knowledge into large scale multimodal analysis and shows promise for future development in the fields of Law Enforcement, Surveillance And Facial Recognition, Security systems",1
"There has been considerable and growing interest in applying machine learning for cyber defenses. One promising approach has been to apply natural language processing techniques to analyze logs data for suspicious behavior. A natural question arises to how robust these systems are to adversarial attacks. Defense against sophisticated attack is of particular concern for cyber defenses. In this paper, we develop a testing framework to evaluate adversarial robustness of machine learning cyber defenses, particularly those focused on log data. Our framework uses techniques from deep reinforcement learning and adversarial natural language processing. We validate our framework using a publicly available dataset and demonstrate that our adversarial attack does succeed against the target systems, revealing a potential vulnerability. We apply our framework to analyze the influence of different levels of dropout regularization and find that higher dropout levels increases robustness. Moreover 90% dropout probability exhibited the highest level of robustness by a significant margin, which suggests unusually high dropout may be necessary to properly protect against adversarial attacks.",0
"Improving machine learning (ML) cyber defense models requires understanding how attackers can potentially break them. This research focuses on adversarial robustness analysis using log data from deployed ML systems. We investigate methods for detecting and mitigating adversarial attacks that try to evade defenses by making small modifications to input data. Our approach uses logs generated during testing and deployment to identify cases where inputs were altered maliciously and assess the impact of these changes on model predictions. To evaluate our techniques, we apply them to real-world security datasets consisting of both benign and malicious traffic, demonstrating their effectiveness at detecting and blocking adversarial examples while maintaining high accuracy in classifying legitimate traffic. Additionally, we provide insights into the behavior of adversaries based on the patterns observed in the modified traffic, which could aid in the development of more resilient ML defenses. Overall, our work advances the state of art in adversarial robustness evaluation and provides valuable recommendations for practitioners seeking to enhance the security posture of their ML applications.",1
"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",0
"Abstract: In recent years, transfer learning has emerged as a powerful approach for training deep neural networks on natural language processing tasks, leveraging the knowledge gained from pretraining on large amounts of data from other domains or languages. Despite its success, there remain several limitations and challenges associated with applying pretrained models directly without further fine-tuning. For example, pretrained models may have difficulty adapting to small datasets or specific domain constraints, leading to poor generalization performance. Additionally, existing text transformers use different architectures or objectives for code generation than those used during pretraining, which can cause a significant decrease in performance. This paper presents a unified text-to-text transformer architecture that addresses these issues by enabling seamless adaptation to multiple text generation tasks while maintaining strong generalization ability across diverse scenarios. We evaluate our model against state-of-the-art alternatives using benchmark datasets from machine translation, summarization, question answering, and content generation, achieving competitive results. Our findings demonstrate that our proposed method represents a step forward in pushing the limits of transfer learning for natural language processing. Keywords: Natural Language Processing (NLP), Transfer Learning, Pre-training, Fine-Tuning, Text Generation Tasks",1
"Given an image, generating its natural language description (i.e., caption) is a well studied problem. Approaches proposed to address this problem usually rely on image features that are difficult to interpret. Particularly, these image features are subdivided into global and local features, where global features are extracted from the global representation of the image, while local features are extracted from the objects detected locally in an image. Although, local features extract rich visual information from the image, existing models generate captions in a blackbox manner and humans have difficulty interpreting which local objects the caption is aimed to represent. Hence in this paper, we propose a novel framework for the image captioning with an explicit object (e.g., knowledge graph entity) selection process while still maintaining its end-to-end training ability. The model first explicitly selects which local entities to include in the caption according to a human-interpretable mask, then generate proper captions by attending to selected entities. Experiments conducted on the MSCOCO dataset demonstrate that our method achieves good performance in terms of the caption quality and diversity with a more interpretable generating process than previous counterparts.",0
"This paper describes how to integrate image captioning models with rule-based entity masking systems using transfer learning from pre-trained object detection models. We evaluate two popular image caption generation methods: UpDn (Universal Dependency Parser) and Show, Attend and Tell (SAT). These models are then fine-tuned on a new task by incorporating entity recognition and slot filling techniques commonly found in rule-based systems. Our approach results in higher accuracy compared to using either method separately. Finally, we explore ways to further improve performance through hybrid approaches that combine both model types. The result is a more robust system capable of handling a variety of complex tasks in real-world settings. Keywords: image captioning, natural language processing, entity recognition, rule-based systems, machine learning This research paper presents a novel technique for integrating image captioning models with rule-based entity masking systems. By leveraging transfer learning from pre-trained object detection models, the authors aim to enhance the accuracy of existing image caption generation methods such as UpDn and SAT. The proposed approach involves fine-tuning these models to include entity recognition and slot filling abilities common in rule-based systems. Experiments showcase the effectiveness of this hybrid solution, resulting in improved performance over standalone methods. Further optimizations could lead to even better results in challenging scenarios. Overall, the integration of image captioning and rule-based entity masking provides a promising direction for building reliable NLP systems suited for real-world applications.",1
"The outbreak of the novel coronavirus disease 2019 (COVID-19), caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has been continuously affecting human lives and communities around the world in many ways, from cities under lockdown to new social experiences. Although in most cases COVID-19 results in mild illness, it has drawn global attention due to the extremely contagious nature of SARS-CoV-2. Governments and healthcare professionals, along with people and society as a whole, have taken any measures to break the chain of transition and flatten the epidemic curve. In this study, we used multiple data sources, i.e., PubMed and ArXiv, and built several machine learning models to characterize the landscape of current COVID-19 research by identifying the latent topics and analyzing the temporal evolution of the extracted research themes, publications similarity, and sentiments, within the time-frame of January- May 2020. Our findings confirm the types of research available in PubMed and ArXiv differ significantly, with the former exhibiting greater diversity in terms of COVID-19 related issues and the latter focusing more on intelligent systems/tools to predict/diagnose COVID-19. The special attention of the research community to the high-risk groups and people with complications was also confirmed.",0
"In recent years, COVID-19 has emerged as one of the most significant public health crises in history. As researchers have raced against time to develop treatments, vaccines, and other interventions to combat the virus, there has been an explosion of scientific literature on the topic. To better understand how knowledge about COVID-19 is evolving over time, we developed a method that uses machine learning and natural language processing (NLP) techniques to analyze large amounts of text data from articles, papers, reports, news stories, social media posts, and more.  Our approach involved extracting features from the text data using NLP tools such as named entity recognition, part-of-speech tagging, dependency parsing, and sentiment analysis. We then fed these feature vectors into a machine learning algorithm to build models that could predict changes in key variables over time, including the number of cases, deaths, and medical publications related to the disease. Our results show that our models were able to accurately capture trends in all three variables across different regions and countries. For example, our model predicted the rise in case numbers during seasonal flu months, and identified the peak number of medical publications coinciding with global lockdown measures. Overall, our work demonstrates the potential of machine learning and NLP methods to uncover insights from vast collections of text data, shedding new light on the dynamic nature of COVID-19 research. While many challenges remain, our findings represent an important step forward towards understanding this critical issue at a deeper level.",1
"Adversarial attacks of deep neural networks have been intensively studied on image, audio, natural language, patch, and pixel classification tasks. Nevertheless, as a typical, while important real-world application, the adversarial attacks of online video object tracking that traces an object's moving trajectory instead of its category are rarely explored. In this paper, we identify a new task for the adversarial attack to visual tracking: online generating imperceptible perturbations that mislead trackers along an incorrect (Untargeted Attack, UA) or specified trajectory (Targeted Attack, TA). To this end, we first propose a \textit{spatial-aware} basic attack by adapting existing attack methods, i.e., FGSM, BIM, and C&W, and comprehensively analyze the attacking performance. We identify that online object tracking poses two new challenges: 1) it is difficult to generate imperceptible perturbations that can transfer across frames, and 2) real-time trackers require the attack to satisfy a certain level of efficiency. To address these challenges, we further propose the spatial-aware online incremental attack (a.k.a. SPARK) that performs spatial-temporal sparse incremental perturbations online and makes the adversarial attack less perceptible. In addition, as an optimization-based method, SPARK quickly converges to very small losses within several iterations by considering historical incremental perturbations, making it much more efficient than basic attacks. The in-depth evaluation on state-of-the-art trackers (i.e., SiamRPN++ with AlexNet, MobileNetv2, and ResNet-50, and SiamDW) on OTB100, VOT2018, UAV123, and LaSOT demonstrates the effectiveness and transferability of SPARK in misleading the trackers under both UA and TA with minor perturbations.",0
"SPARK is a new attack designed to disrupt computer vision tracking systems by exploiting spatial vulnerabilities that arise during online deployment. Unlike traditional methods which rely on brute force or physical attacks to interfere with sensors, SPARK employs cunning strategies to gradually degrade the performance of the tracking system without arousing suspicion. By leveraging advanced understanding of how these trackers function, SPARK can calculate precise timing and location of perturbations to achieve maximum impact on tracking accuracy while remaining covert. Furthermore, as it operates incrementally over time, SPARK adapts to changing conditions to maintain effectiveness even under updated countermeasures. Experimental evaluation demonstrates SPARK’s success against multiple state-of-the art visual tracking algorithms and hardware platforms, making it a potent threat to security applications dependent on reliable object detection. Ultimately, our research serves as a call for further investigation into defenses capable of mitigating emerging threats in the rapidly evolving field of adversarial machine learning.",1
"Vision-and-Language Navigation (VLN) requires an agent to find a specified spot in an unseen environment by following natural language instructions. Dominant methods based on supervised learning clone expert's behaviours and thus perform better on seen environments, while showing restricted performance on unseen ones. Reinforcement Learning (RL) based models show better generalisation ability but have issues as well, requiring large amount of manual reward engineering is one of which. In this paper, we introduce a Soft Expert Reward Learning (SERL) model to overcome the reward engineering designing and generalisation problems of the VLN task. Our proposed method consists of two complementary components: Soft Expert Distillation (SED) module encourages agents to behave like an expert as much as possible, but in a soft fashion; Self Perceiving (SP) module targets at pushing the agent towards the final destination as fast as possible. Empirically, we evaluate our model on the VLN seen, unseen and test splits and the model outperforms the state-of-the-art methods on most of the evaluation metrics.",0
"This work presents a method for training agents in vision-and-language navigation tasks using soft expert rewards. The proposed approach leverages human demonstrations to provide guidance to the agent during training, allowing it to learn more quickly and effectively than with traditional reinforcement learning algorithms alone. The resulting agent is able to outperform state-of-the-art methods on several benchmark datasets, achieving significantly higher success rates and reduced failure modes. Overall, the use of soft expert rewards represents a promising new direction for improving performance in complex vision-and-language tasks.",1
"The task of retrieving video content relevant to natural language queries plays a critical role in effectively handling internet-scale datasets. Most of the existing methods for this caption-to-video retrieval problem do not fully exploit cross-modal cues present in video. Furthermore, they aggregate per-frame visual features with limited or no temporal information. In this paper, we present a multi-modal transformer to jointly encode the different modalities in video, which allows each of them to attend to the others. The transformer architecture is also leveraged to encode and model the temporal information. On the natural language side, we investigate the best practices to jointly optimize the language embedding together with the multi-modal transformer. This novel framework allows us to establish state-of-the-art results for video retrieval on three datasets. More details are available at http://thoth.inrialpes.fr/research/MMT.",0
"This study presents a novel approach for video retrieval using multi-modal transformers. Traditional methods have relied on handcrafted features, while recent deep learning approaches mostly focus on image data. We address the unique challenges posed by videos, such as high-dimensionality, temporal dependencies, and varying modalities (audio, visual), using a single end-to-end model that efficiently captures interdependencies across all components. Our system builds upon state-of-the-art techniques from natural language processing (NLP) and computer vision (CV). We present experiments evaluating our method against other models and baselines, demonstrating improved performance on standard benchmarks. Our findings contribute towards efficient large-scale search, summarization, recommendation, and annotation applications.",1
"Though deep neural networks (DNNs) have shown superiority over other techniques in major fields like computer vision, natural language processing, robotics, recently, it has been proven that they are vulnerable to adversarial attacks. The addition of a simple, small and almost invisible perturbation to the original input image can be used to fool DNNs into making wrong decisions. With more attack algorithms being designed, a need for defending the neural networks from such attacks arises. Retraining the network with adversarial images is one of the simplest techniques. In this paper, we evaluate the effectiveness of such a retraining strategy in defending against adversarial attacks. We also show how simple algorithms like KNN can be used to determine the labels of the adversarial images needed for retraining. We present the results on two standard datasets namely, CIFAR-10 and TinyImageNet.",0
"""Adversarial attacks on machine learning models have become increasingly popular as a means of evaluating their robustness against adversaries seeking to manipulate output decisions. In response, researchers have proposed a variety of defenses aimed at improving model robustness against such attacks. One promising approach involves retraining the model using adversarial examples as additional training data. This strategy aims to teach the model how to better recognize patterns associated with malicious input perturbations, thereby reducing the impact of adversarial attacks. This work presents an evaluation of a simple retraining strategy as a defense against common white box and black box attacks across multiple datasets and models. Our results demonstrate that retraining can provide significant improvements in robustness compared to unprotected baseline models. Furthermore, we find that even relatively small amounts of adversarial augmentation can result in substantial gains in terms of adversary mitigation without requiring large quantities of additional training data or computational resources. Overall, our study provides insight into the effectiveness of retraining strategies as a defense against adversarial attacks in real-world applications."" Note: I wrote an original abstract based on the topic you provided earlier. If you want me to use any specific text from your previous drafts or papers, please let me know!",1
"With online calendar services gaining popularity worldwide, calendar data has become one of the richest context sources for understanding human behavior. However, event scheduling is still time-consuming even with the development of online calendars. Although machine learning based event scheduling models have automated scheduling processes to some extent, they often fail to understand subtle user preferences and complex calendar contexts with event titles written in natural language. In this paper, we propose Neural Event Scheduling Assistant (NESA) which learns user preferences and understands calendar contexts, directly from raw online calendars for fully automated and highly effective event scheduling. We leverage over 593K calendar events for NESA to learn scheduling personal events, and we further utilize NESA for multi-attendee event scheduling. NESA successfully incorporates deep neural networks such as Bidirectional Long Short-Term Memory, Convolutional Neural Network, and Highway Network for learning the preferences of each user and understanding calendar context based on natural languages. The experimental results show that NESA significantly outperforms previous baseline models in terms of various evaluation metrics on both personal and multi-attendee event scheduling tasks. Our qualitative analysis demonstrates the effectiveness of each layer in NESA and learned user preferences.",0
"The proposed research focuses on improving event scheduling through the learning of user preferences and understanding of calendar contexts. By utilizing machine learning algorithms, we aim to develop a system that can accurately predict which events a given user would like to attend, as well as which ones may conflict with other important engagements already scheduled in their calendars. To accomplish this goal, our approach collects data from multiple sources such as social media activity, online reviews, and past event participation history. This data is then analyzed using advanced techniques to identify patterns and create models for personalized recommendations. Additionally, the system incorporates features such as location awareness and real-time updates to ensure accuracy and relevance during the decision making process. Ultimately, our aim is to provide users with more efficient and effective event scheduling experiences by minimizing conflicts, reducing planning time, and increasing overall satisfaction.",1
"Multi-stage training and knowledge transfer, from a large-scale pretraining task to various finetuning tasks, have revolutionized natural language processing and computer vision resulting in state-of-the-art performance improvements. In this paper, we develop a multi-stage influence function score to track predictions from a finetuned model all the way back to the pretraining data. With this score, we can identify the pretraining examples in the pretraining task that contribute most to a prediction in the finetuning task. The proposed multi-stage influence function generalizes the original influence function for a single model in (Koh & Liang, 2017), thereby enabling influence computation through both pretrained and finetuned models. We study two different scenarios with the pretrained embeddings fixed or updated in the finetuning tasks. We test our proposed method in various experiments to show its effectiveness and potential applications.",0
"This research proposes a novel approach to modeling social influence that incorporates both individual characteristics and social network structures. By doing so, we aim to provide more accurate predictions of how individuals within a group may behave under different circumstances and how these behaviors can impact others. Our method involves developing a multi-stage influence function which accounts for variations in social behavior among individuals at each stage. We demonstrate the effectiveness of our approach using simulations on synthetic datasets and evaluate our results through comparison with other well-known models of social influence. Ultimately, the insights gained from our study contribute to a better understanding of human interactions and decision making processes, with potential applications in fields such as marketing, public health, and policy-making.",1
"Generating natural language descriptions for videos, i.e., video captioning, essentially requires step-by-step reasoning along the generation process. For example, to generate the sentence ""a man is shooting a basketball"", we need to first locate and describe the subject ""man"", next reason out the man is ""shooting"", then describe the object ""basketball"" of shooting. However, existing visual reasoning methods designed for visual question answering are not appropriate to video captioning, for it requires more complex visual reasoning on videos over both space and time, and dynamic module composition along the generation process. In this paper, we propose a novel visual reasoning approach for video captioning, named Reasoning Module Networks (RMN), to equip the existing encoder-decoder framework with the above reasoning capacity. Specifically, our RMN employs 1) three sophisticated spatio-temporal reasoning modules, and 2) a dynamic and discrete module selector trained by a linguistic loss with a Gumbel approximation. Extensive experiments on MSVD and MSR-VTT datasets demonstrate the proposed RMN outperforms the state-of-the-art methods while providing an explicit and explainable generation process. Our code is available at https://github.com/tgc1997/RMN.",0
In summary:,1
"Vision-and-Language Navigation (VLN) is a task where agents must decide how to move through a 3D environment to reach a goal by grounding natural language instructions to the visual surroundings. One of the problems of the VLN task is data scarcity since it is difficult to collect enough navigation paths with human-annotated instructions for interactive environments. In this paper, we explore the use of counterfactual thinking as a human-inspired data augmentation method that results in robust models. Counterfactual thinking is a concept that describes the human propensity to create possible alternatives to life events that have already occurred. We propose an adversarial-driven counterfactual reasoning model that can consider effective conditions instead of low-quality augmented data. In particular, we present a model-agnostic adversarial path sampler (APS) that learns to sample challenging paths that force the navigator to improve based on the navigation performance. APS also serves to do pre-exploration of unseen environments to strengthen the model's ability to generalize. We evaluate the influence of APS on the performance of different VLN baseline models using the room-to-room dataset (R2R). The results show that the adversarial training process with our proposed APS benefits VLN models under both seen and unseen environments. And the pre-exploration process can further gain additional improvements under unseen environments.",0
"This paper presents a new approach to robot navigation that combines computer vision techniques with natural language processing and reinforcement learning. We propose a novel algorithm called counterfactual path sampling, which generates hypothetical paths through the environment by making small perturbations to the current state of the world. These counterfactual paths can then be used to guide the robot towards a goal location while also providing relevant visual cues from the surroundings. Our experiments show that our method outperforms traditional navigation algorithms on a variety of tasks, including indoor and outdoor environments with both textual and image instructions. Overall, we believe that our work represents an important step forward towards creating robots that can effectively navigate complex and dynamic environments using multiple sources of input. This paper describes a new approach to robot navigation that utilizes computer vision techniques in combination with natural language processing (NLP) and reinforcement learning. Dubbed ""counterfactual path sampling,"" the proposed method involves generating imaginary routes that deviate slightly from the existing conditions in order to aid the robot in reaching a desired destination. By doing so, the system can provide the machine with supplementary visual clues in addition to verbal commands. During testing, the authors found their innovative technique was more effective than established navigation methods when confronted with varying settings, whether they involved written or pictorial directions. Ultimately, the authors believe their findings have significant implications for the development of advanced autonomous systems capable of navigating intricate and unpredictable scenarios.",1
"Recent advances in facial expression synthesis have shown promising results using diverse expression representations including facial action units. Facial action units for an elaborate facial expression synthesis need to be intuitively represented for human comprehension, not a numeric categorization of facial action units. To address this issue, we utilize human-friendly approach: use of natural language where language helps human grasp conceptual contexts. In this paper, therefore, we propose a new facial expression synthesis model from language-based facial expression description. Our method can synthesize the facial image with detailed expressions. In addition, effectively embedding language features on facial features, our method can control individual word to handle each part of facial movement. Extensive qualitative and quantitative evaluations were conducted to verify the effectiveness of the natural language.",0
"In recent years, there has been significant progress in generating realistic images and videos that capture human expressions. However, most existing approaches rely on predefined templates or handcrafted parameters, limiting their ability to synthesize facial expressions from descriptions provided by humans. To address this limitation, we propose a novel method for comprehensive facial expression synthesis using natural language descriptions. Our approach involves training an encoder-decoder network to generate high-resolution images directly from textual descriptions. We employ two separate networks to encode both local features (e.g., mouth shape) and global attributes (e.g., gender), which allows us to better capture fine details and overall context. We further improve our results through attention mechanisms that focus the decoder on important parts of the input description. Extensive evaluations demonstrate that our method outperforms state-of-the-art methods in terms of visual quality and diversity, as well as alignment with human preferences. Our framework provides a powerful tool for capturing complex facial expressions using interpretable language input, opening up new possibilities for applications such as avatar creation, virtual reality, and therapy support. The supplementary materials provide additional analyses and comparisons.",1
"Logical connectives and their implications on the meaning of a natural language sentence are a fundamental aspect of understanding. In this paper, we investigate whether visual question answering (VQA) systems trained to answer a question about an image, are able to answer the logical composition of multiple such questions. When put under this \textit{Lens of Logic}, state-of-the-art VQA models have difficulty in correctly answering these logically composed questions. We construct an augmentation of the VQA dataset as a benchmark, with questions containing logical compositions and linguistic transformations (negation, disjunction, conjunction, and antonyms). We propose our {Lens of Logic (LOL)} model which uses question-attention and logic-attention to understand logical connectives in the question, and a novel Fr\'echet-Compatibility Loss, which ensures that the answers of the component questions and the composed question are consistent with the inferred logical operation. Our model shows substantial improvement in learning logical compositions while retaining performance on VQA. We suggest this work as a move towards robustness by embedding logical connectives in visual understanding.",0
"Title: Investigating Visual Question Answering through the lens of Logical Reasoning Abstract: Despite significant advances in visual question answering (VQA), many state-of-the-art models still struggle to perform well on challenging questions that require logical reasoning. To address this gap, we propose a new approach that leverages logic to better understand and solve VQA problems. Our method uses automated theorem proving techniques to analyze the structure of natural language queries and determine their validity based on first-order logic principles. We then use these insights to guide the search process for relevant image regions and apply semantic parsing to extract meaning from the resulting images. Experimental results demonstrate that our approach significantly improves performance over baseline methods across several benchmark datasets, indicating that incorporating logical reasoning into VQA can lead to more effective solutions.",1
"Embodied AI has been recently gaining attention as it aims to foster the development of autonomous and intelligent agents. In this paper, we devise a novel embodied setting in which an agent needs to explore a previously unknown environment while recounting what it sees during the path. In this context, the agent needs to navigate the environment driven by an exploration goal, select proper moments for description, and output natural language descriptions of relevant objects and scenes. Our model integrates a novel self-supervised exploration module with penalty, and a fully-attentive captioning model for explanation. Also, we investigate different policies for selecting proper moments for explanation, driven by information coming from both the environment and the navigation. Experiments are conducted on photorealistic environments from the Matterport3D dataset and investigate the navigation and explanation capabilities of the agent as well as the role of their interactions.",0
"This research explores how self-supervision can be applied to improve navigation and recounting tasks. We present a methodology that allows machines to learn from user interaction data without explicit guidance, which enables them to adapt to individual users' needs and preferences. Our approach combines both goal-oriented and recollection-based strategies to create more robust models for navigation and recall. The results show significant improvements over previous methods in terms of accuracy and flexibility, as well as reduced reliance on large amounts of annotated data. In summary, our work demonstrates the potential of self-supervised learning techniques for enhancing autonomous agents' ability to navigate complex environments and effectively recount their experiences. The implications of these findings could have far-reaching impacts in fields such as robotics, education, and healthcare where efficient navigation and effective communication are essential.",1
"In the past decade, deep learning (DL) has achieved unprecedented success in numerous fields including computer vision, natural language processing, and healthcare. In particular, DL is experiencing an increasing development in applications for advanced medical image analysis in terms of analysis, segmentation, classification, and furthermore. On the one hand, tremendous needs that leverage the power of DL for medical image analysis are arising from the research community of a medical, clinical, and informatics background to jointly share their expertise, knowledge, skills, and experience. On the other hand, barriers between disciplines are on the road for them often hampering a full and efficient collaboration. To this end, we propose our novel open-source platform, i.e., MeDaS -- the MeDical open-source platform as Service. To the best of our knowledge, MeDaS is the first open-source platform proving a collaborative and interactive service for researchers from a medical background easily using DL related toolkits, and at the same time for scientists or engineers from information sciences to understand the medical knowledge side. Based on a series of toolkits and utilities from the idea of RINV (Rapid Implementation aNd Verification), our proposed MeDaS platform can implement pre-processing, post-processing, augmentation, visualization, and other phases needed in medical image analysis. Five tasks including the subjects of lung, liver, brain, chest, and pathology, are validated and demonstrated to be efficiently realisable by using MeDaS.",0
"""An Open-Source Platform for Medicine and Informatics Integration""  There has been increasing recognition that integrating medical practice with informatics could greatly improve healthcare delivery by facilitating access to patient data from multiple sources and allowing for more efficient analysis of that data. Despite these potential benefits, however, integration remains challenging due to barriers such as disparate systems and data formats. To address this challenge, we present MedaS - an open-source platform designed to bridge the gap between medicine and informatics. Our platform enables easy integration of different databases and provides tools for querying and analyzing clinical data. By leveraging machine learning algorithms and natural language processing techniques, MeDAS can process free text and identify relevant concepts, providing insights into key trends within large datasets. We demonstrate the utility of our system through several case studies on real clinical data, highlighting how MeDAS enables novel applications for personalized treatments and improved clinical research workflows. Overall, MeDAS represents a major step towards enabling cross-disciplinary collaboration and accelerates innovations in precision medicine.",1
"The joint understanding of vision and language has been recently gaining a lot of attention in both the Computer Vision and Natural Language Processing communities, with the emergence of tasks such as image captioning, image-text matching, and visual question answering. As both images and text can be encoded as sets or sequences of elements -- like regions and words -- proper reduction functions are needed to transform a set of encoded elements into a single response, like a classification or similarity score. In this paper, we propose a novel fully-attentive reduction method for vision and language. Specifically, our approach computes a set of scores for each element of each modality employing a novel variant of cross-attention, and performs a learnable and cross-modal reduction, which can be used for both classification and ranking. We test our approach on image-text matching and visual question answering, building fair comparisons with other reduction choices, on both COCO and VQA 2.0 datasets. Experimentally, we demonstrate that our approach leads to a performance increase on both tasks. Further, we conduct ablation studies to validate the role of each component of the approach.",0
"Artificial intelligence has made significant progress in natural language processing (NLP), but one of the main challenges remains the integration of vision data into NLP models. Recent advancements have focused on developing attention mechanisms that can jointly attend to both modalities; however, these methods still struggle to effectively combine linguistic knowledge with visual cues. In this work, we propose a novel aggregation function called Multimodal Attention Gating (MAG) that incorporates linguistic features into feature maps learned by convolutional neural networks (CNNs). Our method outperforms state-of-the art methods on three benchmark datasets: ReferItGame, Flickr8K, and SBU Caption dataset. We achieve better performance than previous models on all metrics, including BLEU4, METEOR, ROUGEL, CIDEr, and SPICE. Our contributions showcase the effectiveness of our proposed approach in combining vision and language through a simple yet powerful gating mechanism. By highlighting the benefits of multimodal attention over individual attention heads and pooling operations, we provide insights into future research directions in this rapidly growing field of study.",1
"The ability to perform effective planning is crucial for building an instruction-following agent. When navigating through a new environment, an agent is challenged with (1) connecting the natural language instructions with its progressively growing knowledge of the world; and (2) performing long-range planning and decision making in the form of effective exploration and error correction. Current methods are still limited on both fronts despite extensive efforts. In this paper, we introduce the Evolving Graphical Planner (EGP), a model that performs global planning for navigation based on raw sensory input. The model dynamically constructs a graphical representation, generalizes the action space to allow for more flexible decision making, and performs efficient planning on a proxy graph representation. We evaluate our model on a challenging Vision-and-Language Navigation (VLN) task with photorealistic images and achieve superior performance compared to previous navigation architectures. For instance, we achieve a 53% success rate on the test split of the Room-to-Room navigation task through pure imitation learning, outperforming previous navigation architectures by up to 5%.",0
"This project presents an approach to automating multi-step visual navigation tasks that can operate in complex real-world environments. Our work builds on previous graph-based planners, but extends them by reasoning over both object interactions and human instructions using attention mechanisms across multiple modalities (visual input from cameras, natural language input). We address challenges such as ambiguous inputs and sparse rewards through meta-learning that modifies reward functions during planning to improve task completion. Through simulations and live deployment experiments we demonstrate that our evolved graphical planning algorithm outperforms baseline systems on both accuracy and speed metrics relative to manual intervention.  The goal of this research was to develop a method for performing multi-step visual navigation tasks in complex real world scenarios. Previous graph-based planner approaches have been limited in their ability to reason about object interactions and interpret verbal instructions provided by humans. To overcome these limitations, the authors propose a novel evolutionary graphical planning system which combines vision and language processing components, utilizing attention mechanisms to fuse information from different sensory sources. The proposed system uses meta-learning techniques to adapt its objective function during planning, enabling better performance on challenging problems involving incomplete or noisy data. Simulation results and real-world deployment tests showed significant improvements compared to state-of-the-art methods, achieving higher levels of accuracy and efficiency without explicit human guidance. The study suggests that this new hybrid approach has great potential for autonomous robots operating in uncertain environments where accurate understanding of visual scenes and natural language commands is crucial.",1
"In image captioning where fluency is an important factor in evaluation, e.g., $n$-gram metrics, sequential models are commonly used; however, sequential models generally result in overgeneralized expressions that lack the details that may be present in an input image. Inspired by the idea of the compositional neural module networks in the visual question answering task, we introduce a hierarchical framework for image captioning that explores both compositionality and sequentiality of natural language. Our algorithm learns to compose a detail-rich sentence by selectively attending to different modules corresponding to unique aspects of each object detected in an input image to include specific descriptions such as counts and color. In a set of experiments on the MSCOCO dataset, the proposed model outperforms a state-of-the art model across multiple evaluation metrics, more importantly, presenting visually interpretable results. Furthermore, the breakdown of subcategories $f$-scores of the SPICE metric and human evaluation on Amazon Mechanical Turk show that our compositional module networks effectively generate accurate and detailed captions.",0
"""Image captioning is one of the most challenging tasks in computer vision, requiring both visual processing capabilities as well as natural language generation skills. Recent advances in neural network architectures have allowed models to generate accurate image descriptions; however, these state-of-the-art models lack the ability to describe complex scenes in a compositional manner. In this work, we propose using Compositional Neural Module Network (CNMN), which learns to compose primitive concepts into a coherent scene representation that can capture global semantics while focusing on local details. Our approach first preprocesses the input images to extract region features and then applies modules at different levels of abstraction to build hierarchical representations. We evaluate our method on four widely used benchmark datasets and achieve significant improvements over previous methods, demonstrating the effectiveness of CNMN for generating compositional image captions.""",1
"We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is $x\Phi(x)$, where $\Phi(x)$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs ($x\mathbf{1}_{x0}$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.",0
Artificial Intelligence - Assistant: Understood! What would you like me to write?,1
"With great advances in vision and natural language processing, the generation of image captions becomes a need. In a recent paper, Mathews, Xie and He [1], extended a new model to generate styled captions by separating semantics and style. In continuation of this work, here a new captioning model is developed including an image encoder to extract the features, a mixture of recurrent networks to embed the set of extracted features to a set of words, and a sentence generator that combines the obtained words as a stylized sentence. The resulted system that entitled as Mixture of Recurrent Experts (MoRE), uses a new training algorithm that derives singular value decomposition (SVD) from weighting matrices of Recurrent Neural Networks (RNNs) to increase the diversity of captions. Each decomposition step depends on a distinctive factor based on the number of RNNs in MoRE. Since the used sentence generator gives a stylized language corpus without paired images, our captioning model can do the same. Besides, the styled and diverse captions are extracted without training on a densely labeled or styled dataset. To validate this captioning model, we use Microsoft COCO which is a standard factual image caption dataset. We show that the proposed captioning model can generate a diverse and stylized image captions without the necessity of extra-labeling. The results also show better descriptions in terms of content accuracy.",0
"This paper presents a new approach to image caption generation using deep learning techniques. We introduce a novel architecture that combines a singular value decomposition (SVD) based mixture model with recurrent neural networks (RNNs). Our method, called SVD-based Mixture of Recurrent Experts (MRE), enables efficient processing of images by selectively attending to relevant regions and capturing diverse semantic concepts.  The proposed framework is composed of two main components: a VGG16 CNN backbone for feature extraction and region proposals generation; and a novel attention mechanism that uses an RNN-based module and a learned latent space for selecting informative regions. The key innovation lies in our use of a weighted sum of experts (recurrent units) as well as the ability to attend to different aspects of each expert to achieve diversity in image understanding and generate more accurate captions.  We evaluate our model on several benchmark datasets including MSCOCO and Flickr8K, and show that our model outperforms state-of-the-art approaches in terms of automatic metrics such as BLEU score and CIDEr score, as well as human evaluations. Additionally, we conduct ablation studies to demonstrate the effectiveness of individual components and their contributions towards achieving high performance results.  Our findings suggest that combining the power of deep neural networks with traditional computer vision methods like SVD can lead to significant improvements in image captioning tasks. Furthermore, our work highlights the importance of considering both content relevance and stylistic coherence for generating natural language descriptions of complex visual scenes. Overall, our research has important implications for enabling computers to better interpret and describe visual information from real world environments.",1
"E-commerce customers in developing nations like India tend to follow no fixed format while entering shipping addresses. Parsing such addresses is challenging because of a lack of inherent structure or hierarchy. It is imperative to understand the language of addresses, so that shipments can be routed without delays. In this paper, we propose a novel approach towards understanding customer addresses by deriving motivation from recent advances in Natural Language Processing (NLP). We also formulate different pre-processing steps for addresses using a combination of edit distance and phonetic algorithms. Then we approach the task of creating vector representations for addresses using Word2Vec with TF-IDF, Bi-LSTM and BERT based approaches. We compare these approaches with respect to sub-region classification task for North and South Indian cities. Through experiments, we demonstrate the effectiveness of generalized RoBERTa model, pre-trained over a large address corpus for language modelling task. Our proposed RoBERTa model achieves a classification accuracy of around 90% with minimal text preprocessing for sub-region classification task outperforming all other approaches. Once pre-trained, the RoBERTa model can be fine-tuned for various downstream tasks in supply chain like pincode suggestion and geo-coding. The model generalizes well for such tasks even with limited labelled data. To the best of our knowledge, this is the first of its kind research proposing a novel approach of understanding customer addresses in e-commerce domain by pre-training language models and fine-tuning them for different purposes.",0
"Incorporating context into deep learning models has been shown to significantly improve their performance on natural language processing tasks such as sentiment analysis, machine translation, and question answering. However, few attempts have been made at applying these methods to address classification in e-commerce, which poses unique challenges due to the diversity of products sold and the sheer volume of online transactions taking place daily. This study explores the feasibility of using deep contextual embeddings to tackle the problem of address classification in an e-commerce setting. We present a novel model architecture that leverages pretrained deep contextual representations to capture the nuances of human languages found in addresses. Our experimental results demonstrate the effectiveness of our proposed approach, achieving state-of-the-art accuracy on three benchmark datasets commonly used for evaluation in the field of address recognition. Overall, our work provides insights into how advanced NLP techniques can be applied to real-world applications in e-commerce, paving the way for future research in this area.",1
"Advertising and feed ranking are essential to many Internet companies such as Facebook. Among many real-world advertising and feed ranking systems, click through rate (CTR) prediction plays a central role. In recent years, many neural network based CTR models have been proposed and achieved success such as Factorization-Machine Supported Neural Networks, DeepFM and xDeepFM. Many of them contain two commonly used components: embedding layer and MLP hidden layers. On the other side, gating mechanism is also widely applied in many research fields such as computer vision(CV) and natural language processing(NLP). Some research has proved that gating mechanism improves the trainability of non-convex deep neural networks. Inspired by these observations, we propose a novel model named GateNet which introduces either the feature embedding gate or the hidden gate to the embedding layer or hidden layers of DNN CTR models, respectively. The feature embedding gate provides a learnable feature gating module to select salient latent information from the feature-level. The hidden gate helps the model to implicitly capture the high-order interaction more effectively. Extensive experiments conducted on three real-world datasets demonstrate its effectiveness to boost the performance of various state-of-the-art models such as FM, DeepFM and xDeepFM on all datasets.",0
"This paper presents a novel approach for click-through rate (CTR) prediction using deep neural networks. We introduce GateNet, a gating mechanism that dynamically adjusts the importance of different features based on their relevance to the target variable. Our model outperforms previous state-of-the-art models across several CTR prediction benchmark datasets, achieving significant improvements over baseline methods by up to 7%. In addition, we provide detailed analysis of the performance of our model compared to other feature selection techniques commonly used in the literature. Overall, our results demonstrate the effectiveness of our proposed method for improved CTR prediction accuracy.",1
"Visual question answering is concerned with answering free-form questions about an image. Since it requires a deep linguistic understanding of the question and the ability to associate it with various objects that are present in the image, it is an ambitious task and requires techniques from both computer vision and natural language processing. We propose a novel method that approaches the task by performing context-driven, sequential reasoning based on the objects and their semantic and spatial relationships present in the scene. As a first step, we derive a scene graph which describes the objects in the image, as well as their attributes and their mutual relationships. A reinforcement agent then learns to autonomously navigate over the extracted scene graph to generate paths, which are then the basis for deriving answers. We conduct a first experimental study on the challenging GQA dataset with manually curated scene graphs, where our method almost reaches the level of human performance.",0
"Scene graphs provide an excellent representation of complex scenes, but pose significant challenges due to their large scale and rich structure. We present a novel scene graph reasoning framework that utilizes deep learning techniques for visual question answering on large scene graphs. Our model combines hierarchical attention mechanisms with graph convolutional networks, enabling efficient inference over both local and global context. To evaluate our method, we create a new dataset consisting of scene graphs paired with natural language questions and corresponding answers. Experimental results show a clear improvement compared to baseline methods across multiple metrics.",1
"Graph representation learning has emerged as a powerful technique for addressing real-world problems. Various downstream graph learning tasks have benefited from its recent developments, such as node classification, similarity search, and graph classification. However, prior arts on graph representation learning focus on domain specific problems and train a dedicated model for each graph dataset, which is usually non-transferable to out-of-domain data. Inspired by the recent advances in pre-training from natural language processing and computer vision, we design Graph Contrastive Coding (GCC) -- a self-supervised graph neural network pre-training framework -- to capture the universal network topological properties across multiple networks. We design GCC's pre-training task as subgraph instance discrimination in and across networks and leverage contrastive learning to empower graph neural networks to learn the intrinsic and transferable structural representations. We conduct extensive experiments on three graph learning tasks and ten graph datasets. The results show that GCC pre-trained on a collection of diverse datasets can achieve competitive or better performance to its task-specific and trained-from-scratch counterparts. This suggests that the pre-training and fine-tuning paradigm presents great potential for graph representation learning.",0
"In recent years, graph neural networks (GNNs) have emerged as powerful models for learning representations on graphs. While their success has been demonstrated across diverse application domains, training GNNs remains challenging due to their high computational cost and sensitivity to hyperparameters. To address these issues, we propose GCC, a novel method that leverages contrastive coding techniques to pre-train GNNs efficiently and effectively. By encoding nodes using localized neighborhood information and minimizing the cross-entropy loss against random augmentations, our approach enables end-to-end optimization without relying on explicit supervision signals. We evaluate GCC extensively on multiple benchmark datasets for node classification, graph classification, and link prediction tasks. Results show that it achieves state-of-the-art performance while requiring fewer epochs and parameters compared to strong baseline methods, including those trained under full supervision. These findings highlight the promise of self-supervised pre-training as a viable strategy for enhancing the effectiveness and efficiency of GNN training. Our study contributes new insights into graph representation learning that can inspire future work in this growing research area.",1
"Machine learning models tend to over-rely on statistical shortcuts. These spurious correlations between parts of the input and the output labels does not hold in real-world settings. We target this issue on the recent open-ended visual counting task which is well suited to study statistical shortcuts. We aim to develop models that learn a proper mechanism of counting regardless of the output label. First, we propose the Modifying Count Distribution (MCD) protocol, which penalizes models that over-rely on statistical shortcuts. It is based on pairs of training and testing sets that do not follow the same count label distribution such as the odd-even sets. Intuitively, models that have learned a proper mechanism of counting on odd numbers should perform well on even numbers. Secondly, we introduce the Spatial Counting Network (SCN), which is dedicated to visual analysis and counting based on natural language questions. Our model selects relevant image regions, scores them with fusion and self-attention mechanisms, and provides a final counting score. We apply our protocol on the recent dataset, TallyQA, and show superior performances compared to state-of-the-art models. We also demonstrate the ability of our model to select the correct instances to count in the image. Code and datasets are available: https://github.com/cdancette/spatial-counting-network",0
"""In this research paper, we present a novel approach for open-ended visual counting that addresses the shortcomings of existing statistical methods used in computer vision tasks. These limitations often result in significant errors and poor accuracy, particularly in complex scenes where objects are partially occluded or appear similar to one another. Our proposed method overcomes these challenges by utilizing advanced deep learning techniques and prior knowledge from humans in the form of annotated datasets to improve object detection and counting performance. Experimental results demonstrate the effectiveness of our approach on real-world benchmarks and showcase its superiority compared to state-of-the-art methods. This work paves the way for further advancements in visual understanding and has significant implications for applications such as autonomous vehicles and surveillance systems.""",1
"Image retrieval with natural language feedback offers the promise of catalog search based on fine-grained visual features that go beyond objects and binary attributes, facilitating real-world applications such as e-commerce. Our Modality-Agnostic Attention Fusion (MAAF) model combines image and text features and outperforms existing approaches on two visual search with modifying phrase datasets, Fashion IQ and CSS, and performs competitively on a dataset with only single-word modifications, Fashion200k. We also introduce two new challenging benchmarks adapted from Birds-to-Words and Spot-the-Diff, which provide new settings with rich language inputs, and we show that our approach without modification outperforms strong baselines. To better understand our model, we conduct detailed ablations on Fashion IQ and provide visualizations of the surprising phenomenon of words avoiding ""attending"" to the image region they refer to.",0
"This paper presents a novel approach to addressing the challenges associated with modality fusion in visual search tasks that incorporate both image and text data. We propose a modality-agnostic attention fusion mechanism designed to effectively integrate information from both modalities while accounting for their inherent differences in representation. Our method utilizes self-attention mechanisms to learn attended representations of each modality independently before fusing them into a single representation. An additional attention layer allows for fine-grained adjustments based on the specific requirements of the task at hand. To evaluate our approach, we conduct extensive experiments on two benchmark datasets for product retrieval and instance search. Experimental results demonstrate the effectiveness of our proposed framework, which outperforms state-of-the-art methods by significant margins across all metrics. These findings contribute to the development of more effective and efficient solutions for multimodal learning problems involving vision and language inputs.",1
"Emergent communication in artificial agents has been studied to understand language evolution, as well as to develop artificial systems that learn to communicate with humans. We show that agents performing a cooperative navigation task in various gridworld environments learn an interpretable communication protocol that enables them to efficiently, and in many cases, optimally, solve the task. An analysis of the agents' policies reveals that emergent signals spatially cluster the state space, with signals referring to specific locations and spatial directions such as ""left"", ""up"", or ""upper left room"". Using populations of agents, we show that the emergent protocol has basic compositional structure, thus exhibiting a core property of natural language.",0
In many real world scenarios agents have to work together to achieve a common goal. They must coordinate their actions and make decisions based on partial observability of each other’s state. We study how emergence of communication can improve performance in such environments. We propose a novel method where agents learn to communicate using continuous rewards obtained during training time. Our model achieves better than human level performance on popular benchmark multi-agent navigation tasks. To our knowledge this is the first study that successfully demonstrates improvement from learned communication alone without any prior heuristics or handcrafted rules guiding agent interaction. ------------------------------------------- How would you like your name to appear? -------------- Please provide your full name so I can address you properly.,1
"The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyper-parameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.",0
"The purpose of our work is to introduce the novelty that is layer normalization (LN) into the transformer architecture. Our goal was to study its impact on both classification tasks and text generation. Firstly, we provide some context into how these two tasks have been performed previously using transformers. Then we describe layer normalization in detail. Following that, we explore how LN may improve upon existing techniques used in previous work through extensive experimentation, which covers: i) comparisons against vanilla self attention; ii) analysing different hyperparameters; iii) measuring performance degradation with increased model sizes. Lastly, we share our conclusions from all the results gathered throughout the paper. We believe our findings suggest promising directions towards improving large language models and achieving new state-of-the-art results across natural language processing applications powered by the transformer family.",1
"Video captioning works on the two fundamental concepts, feature detection and feature composition. While modern day transformers are beneficial in composing features, they lack the fundamental problems of selecting and understanding of the contents. As the feature length increases, it becomes increasingly important to include provisions for improved capturing of the pertinent contents. In this work, we have introduced a new concept of Self-Aware Composition Transformer (SACT) that is capable of generating Multinomial Attention (MultAtt) which is a way of generating distributions of various combinations of frames. Also, multi-head attention transformer works on the principle of combining all possible contents for attention, which is good for natural language classification, but has limitations for video captioning. Video contents have repetitions and require parsing of important contents for better content composition. In this work, we have introduced SACT for more selective attention and combined them for different attention heads for better capturing of the usable contents for any applications. To address the problem of diversification and encourage selective utilization, we propose the Self-Aware Composition Transformer model for dense video captioning and apply the technique on two benchmark datasets like ActivityNet and YouCookII.",0
"This is an excellent research paper that presents SACT: Self-Aware Multi-Space Feature Composition Transformer for Multinomial Attention for Video Captioning. By introducing self-awareness into feature composition transformers, the authors demonstrate how to effectively model complex relationships between multiple spaces without sacrificing computational efficiency. Using multinomial attention as a building block allows for fine-grained control over different types of relations while reducing computational overhead compared to full softmax attention. Overall, the proposed approach achieves state-of-the-art performance on three popular benchmark datasets, making it a valuable contribution to the field of video captioning. In conclusion, SACT represents a significant step forward in improving the accuracy and scalability of visual representation learning models.",1
"State-of-the-art machine learning methods exhibit limited compositional generalization. At the same time, there is a lack of realistic benchmarks that comprehensively measure this ability, which makes it challenging to find and evaluate improvements. We introduce a novel method to systematically construct such benchmarks by maximizing compound divergence while guaranteeing a small atom divergence between train and test sets, and we quantitatively compare this method to other approaches for creating compositional generalization benchmarks. We present a large and realistic natural language question answering dataset that is constructed according to this method, and we use it to analyze the compositional generalization ability of three machine learning architectures. We find that they fail to generalize compositionally and that there is a surprisingly strong negative correlation between compound divergence and accuracy. We also demonstrate how our method can be used to create new compositionality benchmarks on top of the existing SCAN dataset, which confirms these findings.",0
"This paper presents a comprehensive method for measuring compositional generalization (CG), which refers to the ability of machine learning models to make accurate predictions based on novel combinations of features that were not seen during training. Existing methods have limitations in terms of scalability, interpretability, and applicability to real-world datasets. Our proposed approach addresses these issues by using statistical measures to quantify CG across different levels of data granularity, while providing insights into the underlying patterns driving model performance. We validate our methodology using three diverse case studies from biomedical research, image classification, and natural language processing, demonstrating its effectiveness in capturing different aspects of CG across varying contexts. Our results highlight the importance of considering both qualitative and quantitative factors when evaluating CG in machine learning systems, ultimately leading to better informed decisions regarding their deployment in complex and dynamic environments.",1
"We propose Pixel-BERT to align image pixels with text by deep multi-modal transformers that jointly learn visual and language embedding in a unified end-to-end framework. We aim to build a more accurate and thorough connection between image pixels and language semantics directly from image and sentence pairs instead of using region-based image features as the most recent vision and language tasks. Our Pixel-BERT which aligns semantic connection in pixel and text level solves the limitation of task-specific visual representation for vision and language tasks. It also relieves the cost of bounding box annotations and overcomes the unbalance between semantic labels in visual task and language semantic. To provide a better representation for down-stream tasks, we pre-train a universal end-to-end model with image and sentence pairs from Visual Genome dataset and MS-COCO dataset. We propose to use a random pixel sampling mechanism to enhance the robustness of visual representation and to apply the Masked Language Model and Image-Text Matching as pre-training tasks. Extensive experiments on downstream tasks with our pre-trained model show that our approach makes the most state-of-the-arts in downstream tasks, including Visual Question Answering (VQA), image-text retrieval, Natural Language for Visual Reasoning for Real (NLVR). Particularly, we boost the performance of a single model in VQA task by 2.17 points compared with SOTA under fair comparison.",0
"One of the key challenges in computer vision is aligning image pixels with textual descriptions. This can be difficult because images contain many details that may not be relevant to their description, while text may lack the necessary precision to describe every aspect of an image. In order to address this issue, we propose using deep multi-modal transformer models to align image pixels with text. Our approach, which we call ""Pixel-BERT,"" leverages recent advances in natural language processing (NLP) and computer vision (CV), allowing us to achieve state-of-the-art performance on several benchmarks. We demonstrate that our method outperforms other methods in both quantitative metrics and human evaluation, proving that Pixel-BERT provides a powerful tool for solving the problem of pixel-text alignment. Overall, our work represents an important step forward in improving the accuracy of NLP and CV systems, with implications for a wide range of applications including image generation, question answering, and more.",1
"Neural Networks (NNs) have provided state-of-the-art results for many challenging machine learning tasks such as detection, regression and classification across the domains of computer vision, speech recognition and natural language processing. Despite their success, they are often implemented in a frequentist scheme, meaning they are unable to reason about uncertainty in their predictions. This article introduces Bayesian Neural Networks (BNNs) and the seminal research regarding their implementation. Different approximate inference methods are compared, and used to highlight where future research can improve on current methods.",0
"This survey provides an introduction to the field of Bayesian neural networks (BNNs), which have gained increasing attention in recent years due to their ability to incorporate prior knowledge into model training and inference. BNNs extend traditional feedforward artificial neural networks by using probabilistic models that enable uncertainty quantification and allow predictions to be made under incomplete or uncertain data. They offer several advantages over classical methods, including the ability to handle noise, missing data, and high-dimensional inputs. We provide an overview of different approaches to BNNs, including variational methods, Markov chain Monte Carlo sampling, and hierarchical models. We discuss the benefits and limitations of each approach as well as open challenges faced by researchers working on improving performance and scalability. Finally, we highlight key applications of BNNs across diverse domains such as computer vision, natural language processing, robotics, and medical diagnosis. Our aim is to introduce readers to the exciting possibilities offered by BNNs and encourage further exploration of this rapidly developing area.",1
"Image captioning is a multimodal problem that has drawn extensive attention in both the natural language processing and computer vision community. In this paper, we present a novel image captioning architecture to better explore semantics available in captions and leverage that to enhance both image representation and caption generation. Our models first construct caption-guided visual relationship graphs that introduce beneficial inductive bias using weakly supervised multi-instance learning. The representation is then enhanced with neighbouring and contextual nodes with their textual and visual features. During generation, the model further incorporates visual relationships using multi-task learning for jointly predicting word and object/predicate tag sequences. We perform extensive experiments on the MSCOCO dataset, showing that the proposed framework significantly outperforms the baselines, resulting in the state-of-the-art performance under a wide range of evaluation metrics.",0
"This paper investigates methods for improving image captioning by leveraging better use of human generated captions during training and testing. The proposed approach uses pairs of images and their corresponding captions as input to train an encoder-decoder model architecture that generates descriptive and accurate image captions. We evaluate our method on standard benchmark datasets and demonstrate improved performance over baseline models trained without access to captions. Our results show that utilizing paired image-caption data effectively enhances the learning process and leads to more informative and diverse captions. Overall, we believe that our work presents a significant contribution towards solving the challenges posed by open-domain image captioning.",1
"Self-attention has emerged as a vital component of state-of-the-art sequence-to-sequence models for natural language processing in recent years, brought to the forefront by pre-trained bi-directional Transformer models. Its effectiveness is partly due to its non-sequential architecture, which promotes scalability and parallelism but limits the model to inputs of a bounded length. In particular, such architectures perform poorly on algorithmic tasks, where the model must learn a procedure which generalizes to input lengths unseen in training, a capability we refer to as inductive generalization. Identifying the computational limits of existing self-attention mechanisms, we propose I-BERT, a bi-directional Transformer that replaces positional encodings with a recurrent layer. The model inductively generalizes on a variety of algorithmic tasks where state-of-the-art Transformer models fail to do so. We also test our method on masked language modeling tasks where training and validation sets are partitioned to verify inductive generalization. Out of three algorithmic and two natural language inductive generalization tasks, I-BERT achieves state-of-the-art results on four tasks.",0
"Here's an example: This abstract presents I-BERT, a novel approach to inductively generalizing transformers to arbitrary context lengths. Our method addresses several limitations of existing approaches by adapting input embeddings to different linguistic tasks without fine-tuning the entire network architecture. We show that our model consistently outperforms state-of-the-art models across a range of NLP benchmarks, including language modeling, sentiment analysis, and question answering. Additionally, we demonstrate that I-BERT can effectively generalize to unseen domains through transfer learning. These results highlight the effectiveness and versatility of our approach, making I-BERT a powerful tool for NLP researchers and practitioners alike.",1
"Pretraining has become a standard technique in computer vision and natural language processing, which usually helps to improve performance substantially. Previously, the most dominant pretraining method is transfer learning (TL), which uses labeled data to learn a good representation network. Recently, a new pretraining approach -- self-supervised learning (SSL) -- has demonstrated promising results on a wide range of applications. SSL does not require annotated labels. It is purely conducted on input data by solving auxiliary tasks defined on the input data examples. The current reported results show that in certain applications, SSL outperforms TL and the other way around in other applications. There has not been a clear understanding on what properties of data and tasks render one approach outperforms the other. Without an informed guideline, ML researchers have to try both methods to find out which one is better empirically. It is usually time-consuming to do so. In this work, we aim to address this problem. We perform a comprehensive comparative study between SSL and TL regarding which one works better under different properties of data and tasks, including domain difference between source and target tasks, the amount of pretraining data, class imbalance in source data, and usage of target data for additional pretraining, etc. The insights distilled from our comparative studies can help ML researchers decide which method to use based on the properties of their applications.",0
"As deep learning has become increasingly popular over the past few years, researchers have been exploring two promising approaches: transfer learning and self-supervised learning. Both methods aim to improve model performance by leveraging pre-existing knowledge from other tasks, but they differ significantly in their goals and implementation strategies. In ""Transfer Learning or Self-Supervised Learning?"" we provide an overview of these two paradigms and compare them in terms of advantages, limitations, and practical applications. We discuss how each method can address some of the challenges that arise when training deep neural networks on small datasets, such as limited data availability, class imbalance, or domain shift. By comparing case studies and examining state-of-the-art results across different domains, our analysis provides insights into which approach works better under certain conditions and helps readers make informed decisions when choosing among the available options. Ultimately, both transfer learning and self-supervised learning have shown great potential in improving models' generalization ability; however, their success depends on careful design choices and adaptations based on specific use cases. With this work, we hope to promote a broader understanding of these important concepts within the machine learning community and inspire future innovation.",1
"Differentially private stochastic gradient descent (DPSGD) is a variation of stochastic gradient descent based on the Differential Privacy (DP) paradigm which can mitigate privacy threats arising from the presence of sensitive information in training data. One major drawback of training deep neural networks with DPSGD is a reduction in the model's accuracy. In this paper, we propose an alternative method for preserving data privacy based on introducing noise through learnable probability distributions, which leads to a significant improvement in the utility of the resulting private models. We also demonstrate that normalization layers have a large beneficial impact on the performance of deep neural networks with noisy parameters. In particular, we show that contrary to general belief, a large amount of random noise can be added to the weights of neural networks without harming the performance, once the networks are augmented with normalization layers. We hypothesize that this robustness is a consequence of the scale invariance property of normalization operators. Building on these observations, we propose a new algorithmic technique for training deep neural networks under very low privacy budgets by sampling weights from Gaussian distributions and utilizing batch or layer normalization techniques to prevent performance degradation. Our method outperforms previous approaches, including DPSGD, by a substantial margin on a comprehensive set of experiments on Computer Vision and Natural Language Processing tasks. In particular, we obtain a 20 percent accuracy improvement over DPSGD on the MNIST and CIFAR10 datasets with DP-privacy budgets of $\varepsilon = 0.05$ and $\varepsilon = 2.0$, respectively. Our code is available online: https://github.com/uds-lsv/SIDP.",0
"In recent years, machine learning has become increasingly popular as a tool for data analysis and decision making. However, many datasets used in training these models contain sensitive information that must be protected from disclosure. One approach to addressing this issue is through differential privacy, which allows statistical queries to be answered while ensuring that individual records cannot be identified. This paper presents a new method for training deep neural networks (DNNs) using robust optimization techniques with a focus on protecting the underlying data through differential privacy. Our proposed method achieves state-of-the-art performance on several benchmark tasks while providing strong guarantees on the privacy of the input data. We demonstrate the effectiveness of our approach by applying it to both image classification and natural language processing problems, showing improved accuracy over previous methods. Additionally, we provide a comprehensive evaluation of the tradeoffs involved in achieving high levels of privacy protection versus model accuracy. Overall, our work contributes important advances towards enabling secure and private use of machine learning models in practice.",1
"Recent work has shown that topological enhancements to recurrent neural networks (RNNs) can increase their expressiveness and representational capacity. Two popular enhancements are stacked RNNs, which increases the capacity for learning non-linear functions, and bidirectional processing, which exploits acausal information in a sequence. In this work, we explore the delayed-RNN, which is a single-layer RNN that has a delay between the input and output. We prove that a weight-constrained version of the delayed-RNN is equivalent to a stacked-RNN. We also show that the delay gives rise to partial acausality, much like bidirectional networks. Synthetic experiments confirm that the delayed-RNN can mimic bidirectional networks, solving some acausal tasks similarly, and outperforming them in others. Moreover, we show similar performance to bidirectional networks in a real-world natural language processing task. These results suggest that delayed-RNNs can approximate topologies including stacked RNNs, bidirectional RNNs, and stacked bidirectional RNNs - but with equivalent or faster runtimes for the delayed-RNNs.",0
"This paper explores the application of delayed recurrent neural networks (DRNNs) as approximations for stacked and bidirectional recurrent architectures. We show that DRNNs can effectively capture complex temporal dependencies by propagating gradients backwards through time with a small delay at each layer. Our experimental results demonstrate that DRNNs achieve comparable performance to state-of-the-art models on challenging benchmark tasks while requiring significantly fewer parameters and computational resources. In addition, we provide theoretical insights into the benefits of using DRNNs over traditional RNNs and showcase their potential applications in real-world scenarios such as speech recognition and language translation. Overall, our work advances the understanding of deep learning methods applied to sequential data and highlights the value of using approximation techniques to balance model complexity and efficiency.",1
"We present a new large-scale multilingual video description dataset, VATEX, which contains over 41,250 videos and 825,000 captions in both English and Chinese. Among the captions, there are over 206,000 English-Chinese parallel translation pairs. Compared to the widely-used MSR-VTT dataset, VATEX is multilingual, larger, linguistically complex, and more diverse in terms of both video and natural language descriptions. We also introduce two tasks for video-and-language research based on VATEX: (1) Multilingual Video Captioning, aimed at describing a video in various languages with a compact unified captioning model, and (2) Video-guided Machine Translation, to translate a source language description into the target language using the video information as additional spatiotemporal context. Extensive experiments on the VATEX dataset show that, first, the unified multilingual model can not only produce both English and Chinese descriptions for a video more efficiently, but also offer improved performance over the monolingual models. Furthermore, we demonstrate that the spatiotemporal video context can be effectively utilized to align source and target languages and thus assist machine translation. In the end, we discuss the potentials of using VATEX for other video-and-language research.",0
Abstract:,1
"Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses $O(n^2)$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from $O(n^2)$ to $O(n)$ in both time and space. The resulting linear transformer, the \textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.",0
"Here we present ""Linformer"", a new attention architecture that operates on linear computation and memory complexity compared to previous nonlinear self-attention methods, achieving comparable performance across a range of natural language processing (NLP) tasks while offering significant computational advantages. Our approach first projects input sequences into a low-dimensional space using a trainable linear layer, which allows us to compute softmax scores efficiently and perform dot product based attention with O(n^2), where n is length of input sequence. Our experimental results show that our proposed method outperforms other popular transformer architectures like Transformers XL and MLM-T5, as well as traditional recurrent neural networks such as LSTMs, demonstrating promise for scaling NLP models to larger datasets while maintaining state-of-the-art performance. Overall, we believe the simplicity and efficiency offered by the proposed model have significant implications for real world deployments of large-scale NLP applications in areas including machine translation, question answering, and text generation.",1
"The multinomial and related distributions have long been used to model categorical, count-based data in fields ranging from bioinformatics to natural language processing. Commonly utilized variants include the standard multinomial and the Dirichlet multinomial distributions due to their computational efficiency and straightforward parameter estimation process. However, these distributions make strict assumptions about the mean, variance, and covariance between the categorical features being modeled. If these assumptions are not met by the data, it may result in poor parameter estimates and loss in accuracy for downstream applications like classification. Here, we explore efficient parameter estimation and supervised classification methods using an alternative distribution, called the Beta-Liouville multinomial, which relaxes some of the multinomial assumptions. We show that the Beta-Liouville multinomial is comparable in efficiency to the Dirichlet multinomial for Newton-Raphson maximum likelihood estimation, and that its performance on simulated data matches or exceeds that of the multinomial and Dirichlet multinomial distributions. Finally, we demonstrate that the Beta-Liouville multinomial outperforms the multinomial and Dirichlet multinomial on two out of four gold standard datasets, supporting its use in modeling data with low to medium class overlap in a supervised classification context.",0
"Title: ""Fast Maximum Likelihood Estimation and Supervised Classification for the Beta-Liouville Multinomial""  Abstract: In many real-world applications such as natural language processing, bioinformatics, and finance, data often follow non-negative multinomial distributions. These models have proven to be effective in modeling count data subjected to constraints, which arise due to limited resources, budgets, regulations, ethical considerations, etc. However, estimation and classification tasks using these distributions can be computationally expensive and time consuming, particularly when dealing with large datasets. To address this challenge, we propose the use of the beta-Liouville distribution, a generalization of the negative binomial (NB) distribution that offers better fits in certain situations while preserving tractability. Our approach utilizes efficient algorithms for maximum likelihood estimation based on moments matching, which allows us to accurately estimate parameters even for small sample sizes. Additionally, we present a new method for supervised classification by leveraging the connection between the beta-Liouville and beta distributions. This enables us to develop fast decision rules based on linear discriminant analysis. We evaluate our methods using simulated and real data sets across diverse domains, demonstrating their superiority over benchmark approaches in terms of accuracy, speed, and scalability. Overall, our work contributes novel insights into handling complex multinomial problems through flexible yet practical probability models that offer significant computational advantages without compromising statistical efficiency.",1
"Neural Architecture Search (NAS) is a promising and rapidly evolving research area. Training a large number of neural networks requires an exceptional amount of computational power, which makes NAS unreachable for those researchers who have limited or no access to high-performance clusters and supercomputers. A few benchmarks with precomputed neural architectures performances have been recently introduced to overcome this problem and ensure more reproducible experiments. However, these benchmarks are only for the computer vision domain and, thus, are built from the image datasets and convolution-derived architectures. In this work, we step outside the computer vision domain by leveraging the language modeling task, which is the core of natural language processing (NLP). Our main contribution is as follows: we have provided search space of recurrent neural networks on the text datasets and trained 14k architectures within it; we have conducted both intrinsic and extrinsic evaluation of the trained models using datasets for semantic relatedness and language understanding evaluation; finally, we have tested several NAS algorithms to demonstrate how the precomputed results can be utilized. We believe that our results have high potential of usage for both NAS and NLP communities.",0
"This paper presents a benchmark framework (https://github.com/google-research/nasonlp) that enables easy and fast evaluation on multiple NLG tasks by providing datasets, baselines, etc...This framework should enable new research into the field as it is an important step towards understanding how to make machine learning models better without doing as much work as current state-of-the-art methods require.",1
"In the real world, linguistic agents are also embodied agents: they perceive and act in the physical world. The notion of Language Grounding questions the interactions between language and embodiment: how do learning agents connect or ground linguistic representations to the physical world ? This question has recently been approached by the Reinforcement Learning community under the framework of instruction-following agents. In these agents, behavioral policies or reward functions are conditioned on the embedding of an instruction expressed in natural language. This paper proposes another approach: using language to condition goal generators. Given any goal-conditioned policy, one could train a language-conditioned goal generator to generate language-agnostic goals for the agent. This method allows to decouple sensorimotor learning from language acquisition and enable agents to demonstrate a diversity of behaviors for any given instruction. We propose a particular instantiation of this approach and demonstrate its benefits.",0
"In recent years, there has been significant interest in grounding language semantics in perceptual input data. This process, known as language grounding, involves mapping natural language instructions into low-level actions that can manipulate objects in the environment. One popular approach to language grounding is through Reinforcement Learning (RL) algorithms, which learn to map linguistic inputs to physical outputs by trial and error. However, current RL methods have limitations in their ability to generalize across different domains and tasks, and often struggle with complex goal specification and planning.  This paper proposes a new approach to language grounding called ""Language-Conditioned Goal Generation"" (LCGG), which addresses these shortcomings in existing approaches. LCGG combines semantic parsing and deep RL to generate goals that are conditioned on the given instruction, allowing agents to effectively plan and execute actions even in unfamiliar environments. Our method utilizes pre-trained object detection models to identify relevant entities in the environment, enabling agents to reason over spatial relationships and make plans based on the identified objects.  We evaluate our model on three challenging task sets: Blocks World, SOKOBAN, and Room Escaper. Results show that our method significantly outperforms state-of-the-art baselines on all three datasets, demonstrating the effectiveness of LCGG at generating effective goals using natural language instructions alone. Furthermore, we conduct human studies to compare the quality of generated goals by real users against those produced by our model. Results suggest that our model can produce goals that are both easier for humans to follow and more likely to result in successful problem solving. Overall, our work represents an important step towards improving the robustness and flexibility of language grounding methods in artificial intelligence.",1
"This paper introduces RTEx, a novel methodology for a) ranking radiography exams based on their probability to contain an abnormality, b) generating abnormality tags for abnormal exams, and c) providing a diagnostic explanation in natural language for each abnormal exam. The task of ranking radiography exams is an important first step for practitioners who want to identify and prioritize those radiography exams that are more likely to contain abnormalities, for example, to avoid mistakes due to tiredness or to manage heavy workload (e.g., during a pandemic). We used two publicly available datasets to assess our methodology and demonstrate that for the task of ranking it outperforms its competitors in terms of NDCG@k. For each abnormal radiography exam RTEx generates a set of abnormality tags alongside an explanatory diagnostic text to explain the tags and guide the medical expert. Our tagging component outperforms two strong competitor methods in terms of F1. Moreover, the diagnostic captioning component of RTEx, which exploits the already extracted tags to constrain the captioning process, outperforms all competitors with respect to clinical precision and recall.",0
"""This paper presents a new approach for ranking, tagging, and explanatory diagnostic captioning of radiology exams called RTEX. In recent years, there has been a growing need for more efficient and accurate methods for analyzing medical images such as x-rays, CT scans, and MRIs. Traditional methods rely on human experts to manually analyze these images, which can be time-consuming and error-prone.  The proposed methodology addresses these limitations by leveraging advanced artificial intelligence techniques, specifically deep learning and computer vision algorithms, to automate many aspects of the analysis process. RTEX combines multiple state-of-the-art models to provide a comprehensive assessment that includes both quantitative measures and qualitative annotations. These features make it well suited for use in clinical settings where speed and accuracy are crucial for patient diagnosis and care.  We evaluate our approach using three different datasets representing various types of medical imagery. Our results demonstrate that RTEX performs favorably compared to existing methods and achieves competitive performance across all metrics. This work represents a step towards making medical image interpretation faster and more accurate while reducing human errors.""",1
"Although stochastic optimization is central to modern machine learning, the precise mechanisms underlying its success, and in particular, the precise role of the stochasticity, still remain unclear. Modelling stochastic optimization algorithms as discrete random recurrence relations, we show that multiplicative noise, as it commonly arises due to variance in local rates of convergence, results in heavy-tailed stationary behaviour in the parameters. A detailed analysis is conducted for SGD applied to a simple linear regression problem, followed by theoretical results for a much larger class of models (including non-linear and non-convex) and optimizers (including momentum, Adam, and stochastic Newton), demonstrating that our qualitative results hold much more generally. In each case, we describe dependence on key factors, including step size, batch size, and data variability, all of which exhibit similar qualitative behavior to recent empirical results on state-of-the-art neural network models from computer vision and natural language processing. Furthermore, we empirically demonstrate how multiplicative noise and heavy-tailed structure improve capacity for basin hopping and exploration of non-convex loss surfaces, over commonly-considered stochastic dynamics with only additive noise and light-tailed structure.",0
"In recent years, there has been increasing interest in understanding how randomness affects the performance of optimization algorithms in large-scale problems. In particular, multiplicative noise models have emerged as a powerful tool for capturing complex dependencies between variables and modeling uncertainty. This work investigates how such models can lead to heavy tail distributions in certain classes of nonconvex optimization problems. Our results show that multiplicative noise can significantly alter the landscape of these problems, leading to new challenges in designing efficient algorithms. We provide novel theoretical insights into the nature of these difficulties, along with experimental evidence supporting our findings. These results contribute to a deeper understanding of the interplay between randomness and optimization, and have important implications for solving real-world problems where data uncertainties may play a critical role.",1
"Graphs are the natural data structure to represent relational and structural information in many domains. To cover the broad range of graph-data applications including graph classification as well as graph generation, it is desirable to have a general and flexible model consisting of an encoder and a decoder that can handle graph data. Although the representative encoder-decoder model, Transformer, shows superior performance in various tasks especially of natural language processing, it is not immediately available for graphs due to their non-sequential characteristics. To tackle this incompatibility, we propose GRaph-Aware Transformer (GRAT), the first Transformer-based model which can encode and decode whole graphs in end-to-end fashion. GRAT is featured with a self-attention mechanism adaptive to the edge information and an auto-regressive decoding mechanism based on the two-path approach consisting of sub-graph encoding path and node-and-edge generation path for each decoding step. We empirically evaluated GRAT on multiple setups including encoder-based tasks such as molecule property predictions on QM9 datasets and encoder-decoder-based tasks such as molecule graph generation in the organic molecule synthesis domain. GRAT has shown very promising results including state-of-the-art performance on 4 regression tasks in QM9 benchmark.",0
"This paper presents the Graph-Aware Transformer (GAT), a new architecture that incorporates graph structure into the Transformer model. GAT uses attention mechanisms to weigh different neighborhoods of vertices based on their relation to the target vertex, allowing the model to learn better representations of graphs. We evaluate our approach on several benchmark datasets and show significant improvements over baseline models. Our results demonstrate the effectiveness of using attention mechanisms to capture global relationships in graphs. Additionally, we provide analysis comparing GAT to traditional Graph Neural Networks and other state-of-the-art approaches, highlighting its strengths and weaknesses. Overall, GAT provides a powerful tool for graph representation learning that can outperform current methods in many cases.",1
"By describing the features and abstractions of our world, language is a crucial tool for human learning and a promising source of supervision for machine learning models. We use language to improve few-shot visual classification in the underexplored scenario where natural language task descriptions are available during training, but unavailable for novel tasks at test time. Existing models for this setting sample new descriptions at test time and use those to classify images. Instead, we propose language-shaped learning (LSL), an end-to-end model that regularizes visual representations to predict language. LSL is conceptually simpler, more data efficient, and outperforms baselines in two challenging few-shot domains.",0
"This research explores how natural language can be used to shape visual representations for few-shot classification tasks. We propose a method that utilizes textual descriptions as guidance during training, enabling models to more effectively generalize to unseen examples from new classes. Our approach improves upon existing methods by introducing a novel attention mechanism, which focuses on relevant aspects of the input data rather than simply amplifying noise. Experiments demonstrate that our model outperforms strong baseline methods across several benchmark datasets, providing evidence of the effectiveness of our approach. These results highlight the potential for future work that combines language and vision processing techniques in order to enhance machine learning performance. Overall, we believe that shaping visual representations through language has significant potential applications in areas such as computer vision, robotics, and conversational agents, among others.",1
"Semantic code search is the task of retrieving relevant code given a natural language query. While related to other information retrieval tasks, it requires bridging the gap between the language used in code (often abbreviated and highly technical) and natural language more suitable to describe vague concepts and ideas.   To enable evaluation of progress on code search, we are releasing the CodeSearchNet Corpus and are presenting the CodeSearchNet Challenge, which consists of 99 natural language queries with about 4k expert relevance annotations of likely results from CodeSearchNet Corpus. The corpus contains about 6 million functions from open-source code spanning six programming languages (Go, Java, JavaScript, PHP, Python, and Ruby). The CodeSearchNet Corpus also contains automatically generated query-like natural language for 2 million functions, obtained from mechanically scraping and preprocessing associated function documentation. In this article, we describe the methodology used to obtain the corpus and expert labels, as well as a number of simple baseline solutions for the task.   We hope that CodeSearchNet Challenge encourages researchers and practitioners to study this interesting task further and will host a competition and leaderboard to track the progress on the challenge. We are also keen on extending CodeSearchNet Challenge to more queries and programming languages in the future.",0
"""Code search is a fundamental task in software development, yet despite decades of research, the state of the art remains limited. To drive progress in the field, we introduced the CodeSearchNet challenge, inviting participants from academia and industry to submit their best code search approaches. In this paper, we present an evaluation of the submissions received in response to our call, assessing both retrieval effectiveness (how well systems retrieve relevant results) and diversification (how well systems avoid repetition). We find that while some participant systems achieve promising performance, there is still significant room for improvement across all aspects of code search functionality. Our analysis highlights key areas where future work should focus, including handling duplicate code fragments, ranking relevance scores more effectively, improving scalability to meet real-world demands, leveraging additional modalities beyond plain text queries, and addressing the challenges posed by open source repositories. Ultimately, our study provides valuable insights into the current state of semantic code search technology, informing ongoing research efforts aimed at advancing the capabilities of these essential tools.""",1
"We propose a multi-head attention mechanism as a blending layer in a neural network model that translates natural language to a high level behavioral language for indoor robot navigation. We follow the framework established by (Zang et al., 2018a) that proposes the use of a navigation graph as a knowledge base for the task. Our results show significant performance gains when translating instructions on previously unseen environments, therefore, improving the generalization capabilities of the model.",0
"In this work we propose an approach that uses natural language instructions as input in order to guide robotic behavior. We use a multi head attention mechanism which allows us to process different types of instruction simultaneously. Our experiments demonstrate the ability to translate natural language instructions into motion commands and navigate through challenging environments successfully. Finally our method outperforms baseline approaches using LSTMs on both accuracy and speed metrics. This work has applications across multiple domains such as autonomous robots used in agriculture mining, surveillance etc. -----",1
"Learning disentangled representations of natural language is essential for many NLP tasks, e.g., conditional text generation, style transfer, personalized dialogue systems, etc. Similar problems have been studied extensively for other forms of data, such as images and videos. However, the discrete nature of natural language makes the disentangling of textual representations more challenging (e.g., the manipulation over the data space cannot be easily achieved). Inspired by information theory, we propose a novel method that effectively manifests disentangled representations of text, without any supervision on semantics. A new mutual information upper bound is derived and leveraged to measure dependence between style and content. By minimizing this upper bound, the proposed method induces style and content embeddings into two independent low-dimensional spaces. Experiments on both conditional text generation and text-style transfer demonstrate the high quality of our disentangled representation in terms of content and style preservation.",0
"This is a task that requires understanding of both natural language processing (NLP) and computer science. Here's an example:  Improved text representation learning has important applications across the NLP domain, but often relies on complex architectures and large datasets. In this work, we propose a simple yet effective method based on maximizing mutual information between input tokens and their contextualized representations. By leveraging variational Bayesian inference and KL divergence regularization, our approach achieves state-of-the-art results on several benchmark tasks while reducing computational cost and model size. Our findings shed light on the importance of information-based guidance in disentangling high-level abstractions from raw inputs, paving the way towards more efficient and interpretable models in NLP.",1
"Grounding free-form textual queries necessitates an understanding of these textual phrases and its relation to the visual cues to reliably reason about the described locations. Spatial attention networks are known to learn this relationship and focus its gaze on salient objects in the image. Thus, we propose to utilize spatial attention networks for image-level visual-textual fusion preserving local (word) and global (phrase) information to refine region proposals with an in-network Region Proposal Network (RPN) and detect single or multiple regions for a phrase query. We focus only on the phrase query - ground truth pair (referring expression) for a model independent of the constraints of the datasets i.e. additional attributes, context etc. For such referring expression dataset ReferIt game, our Multi-region Attention-assisted Grounding network (MAGNet) achieves over 12\% improvement over the state-of-the-art. Without the context from image captions and attribute information in Flickr30k Entities, we still achieve competitive results compared to the state-of-the-art.",0
"In natural language processing (NLP), query grounding refers to the process of mapping natural language queries to relevant regions within an image or video frame. This task is challenging due to ambiguity and inconsistency in human expressions, as well as the diversity of objects in images and videos. Previous approaches have mainly focused on either global object detection or regional attention mechanisms alone, but these methods often suffer from limited performance or tradeoffs between efficiency and accuracy. To address this gap, we propose MAGNet - Multi-region Attention Assisted Grounding of Natural Language Queries at phrase level. Our method leverages multi-scale features extracted through region proposal networks, enabling both precise object localization and sentence-level reasoning. We employ a hierarchical attention mechanism that effectively integrates different levels of information, including visual features, spatial relationships, semantic categories, and syntactic structure. With extensive experiments, our approach achieves state-of-the-art results on three benchmark datasets, demonstrating significant improvements over prior arts. Moreover, ablation studies reveal the importance of individual components, such as different attentional modules or alternative feature representations. Our work contributes insights into better NLP models by designing more effective ways to leverage external knowledge sources, and improving their interpretability via interaction and collaboration across diverse domains. By providing rich understanding of complex scenes and flexible handling of natural language variations, MAGNet opens up new possibilities for advanced applications such as visually assisted chatbots and human-robot interfaces.",1
"Beyond the common difficulties faced in the natural image captioning, medical report generation specifically requires the model to describe a medical image with a fine-grained and semantic-coherence paragraph that should satisfy both medical commonsense and logic. Previous works generally extract the global image features and attempt to generate a paragraph that is similar to referenced reports; however, this approach has two limitations. Firstly, the regions of primary interest to radiologists are usually located in a small area of the global image, meaning that the remainder parts of the image could be considered as irrelevant noise in the training procedure. Secondly, there are many similar sentences used in each medical report to describe the normal regions of the image, which causes serious data bias. This deviation is likely to teach models to generate these inessential sentences on a regular basis. To address these problems, we propose an Auxiliary Signal-Guided Knowledge Encoder-Decoder (ASGK) to mimic radiologists' working patterns. In more detail, ASGK integrates internal visual feature fusion and external medical linguistic information to guide medical knowledge transfer and learning. The core structure of ASGK consists of a medical graph encoder and a natural language decoder, inspired by advanced Generative Pre-Training (GPT). Experiments on the CX-CHR dataset and our COVID-19 CT Report dataset demonstrate that our proposed ASGK is able to generate a robust and accurate report, and moreover outperforms state-of-the-art methods on both medical terminology classification and paragraph generation metrics.",0
"This paper presents an auxiliary signal-guided knowledge encoder-decoder (ASKED) model that generates medical reports by encoding structured information from various sources into a latent space representation. Our proposed ASKED architecture utilizes knowledge graphs to guide the decoding process by generating meaningful embeddings. We demonstrate the effectiveness of our approach through quantitative evaluation on multiple metrics such as perplexity, F1 score, and human evaluation. Furthermore, we showcase the applicability of our methodology across different medical domains including radiology and cardiology. Overall, our study highlights the potential benefits of using ASKED models for automating medical report generation tasks and facilitating clinical decision making processes. By leveraging cutting-edge NLP techniques and domain-specific knowledge bases, our framework offers a step forward towards realizing these goals.",1
"It has witnessed a growing demand for efficient representation learning on point clouds in many 3D computer vision applications. Behind the success story of convolutional neural networks (CNNs) is that the data (e.g., images) are Euclidean structured. However, point clouds are irregular and unordered. Various point neural networks have been developed with isotropic filters or using weighting matrices to overcome the structure inconsistency on point clouds. However, isotropic filters or weighting matrices limit the representation power. In this paper, we propose a permutable anisotropic convolutional operation (PAI-Conv) that calculates soft-permutation matrices for each point using dot-product attention according to a set of evenly distributed kernel points on a sphere's surface and performs shared anisotropic filters. In fact, dot product with kernel points is by analogy with the dot-product with keys in Transformer as widely used in natural language processing (NLP). From this perspective, PAI-Conv can be regarded as the transformer for point clouds, which is physically meaningful and is robust to cooperate with the efficient random point sampling method. Comprehensive experiments on point clouds demonstrate that PAI-Conv produces competitive results in classification and semantic segmentation tasks compared to state-of-the-art methods.",0
"This paper presents a new method for learning from point clouds using permutations and convolutional layers. Our approach builds upon previous work on convolutions over ordered sequences, but we extend it to allow for flexible rotational equivariance by applying learned permutations at each layer. These permutations allow our model to learn meaningful features that can handle local changes in orientation while remaining invariant under global transformations. We demonstrate the effectiveness of our method through extensive experiments on several benchmark datasets, showing significant improvements over state-of-the-art methods. By incorporating permutation into convolutional networks, we achieve better generalization and robustness, leading to more accurate results for tasks such as object classification, part segmentation, and scene completion.",1
"Transformer-based models have become ubiquitous in natural language processing thanks to their large capacity, innate parallelism and high performance. The contextualizing component of a Transformer block is the $\textit{pairwise dot-product}$ attention that has a large $\Omega(L^2)$ memory requirement for length $L$ sequences, limiting its ability to process long documents. This has been the subject of substantial interest recently, where multiple approximations were proposed to reduce the quadratic memory requirement using sparse attention matrices. In this work, we propose to augment sparse Transformer blocks with a dense attention-based $\textit{global memory}$ of length $M$ ($\ll L$) which provides an aggregate global view of the entire input sequence to each position. Our augmentation has a manageable $O(M\cdot(L+M))$ memory overhead, and can be seamlessly integrated with prior sparse solutions. Moreover, global memory can also be used for sequence compression, by representing a long input sequence with the memory representations only. We empirically show that our method leads to substantial improvement on a range of tasks, including (a) synthetic tasks that require global reasoning, (b) masked language modeling, and (c) reading comprehension.",0
"This is an excellent research article that discusses the benefits of global memory augmentations (GMA) for transformers. Incorporating external memories into deep learning models has been shown to improve performance on tasks like natural language processing and computer vision. However, there are currently few papers exploring the use of GMAs with pretrained language models. The authors aim to address this gap by analyzing the effects of applying GMA methods in transformer architectures used in large language models such as BERT and GPT. Their experiments show that adding global context representations improves both perplexity measures and downstream task accuracy, making the case for future work exploring alternative GMA approaches for these types of language models. Overall, this well-written paper provides valuable insights for anyone interested in advancing the state of art in NLP using deep learning techniques.",1
"Detection and description of keypoints from an image is a well-studied problem in Computer Vision. Some methods like SIFT, SURF or ORB are computationally really efficient. This paper proposes a solution for a particular case study on object recognition of industrial parts based on hierarchical classification. Reducing the number of instances leads to better performance, indeed, that is what the use of the hierarchical classification is looking for. We demonstrate that this method performs better than using just one method like ORB, SIFT or FREAK, despite being fairly slower.",0
"This system uses human expert knowledge (in this case provided by Kovesi) on image feature detection and description to detect features. Then it selects one of three descriptor algorithms: SIFT, SURF or BRIEF. Next these descriptors are used to match and retrieve images from databases using Eigenfaces methodologies. Note that there are many parameters involved and our current research path has concentrated on finding optimal parameter settings for each detector/descriptor combination. There is still plenty of work left but I think we can eventually create some very strong systems out of this technology. In future papers hopefully we can describe how to find good hyperparameter values given training data since the process remains unchanged. Abstract:  This study presents an expert system for selecting and applying suitable 2D image feature detectors and descriptors for content-based image retrieval tasks. Specifically, the proposed system leverages human expert knowledge on feature detection to first identify key points within images. Subsequently, the system employs machine learning techniques such as Eigenfaces to classify and rank candidate descriptors based on their effectiveness at matching similar image features across large datasets. Our approach takes into account multiple types of detectors and descriptors including SIFT, SURF, and BRIEF, which are well established methods in computer vision research. Although promising results have been achieved through initial testing, further refinement is required before real-world implementation in search and retrieval applications. Ongoing efforts aim to address remaining challenges and improve accuracy, particularly through optimizing various associated parameters governing performance and efficiency under different conditions. Ultimately, integrating automation alongside expertise holds significant potential for enhancing the robustness and scalability of content-based image retrieval systems.",1
"Text based Visual Question Answering (TextVQA) is a recently raised challenge that requires a machine to read text in images and answer natural language questions by jointly reasoning over the question, Optical Character Recognition (OCR) tokens and visual content. Most of the state-of-the-art (SoTA) VQA methods fail to answer these questions because of i) poor text reading ability; ii) lacking of text-visual reasoning capacity; and iii) adopting a discriminative answering mechanism instead of a generative one which is hard to cover both OCR tokens and general text tokens in the final answer. In this paper, we propose a structured multimodal attention (SMA) neural network to solve the above issues. Our SMA first uses a structural graph representation to encode the object-object, object-text and text-text relationships appearing in the image, and then design a multimodal graph attention network to reason over it. Finally, the outputs from the above module are processed by a global-local attentional answering module to produce an answer that covers tokens from both OCR and general text iteratively. Our proposed model outperforms the SoTA models on TextVQA dataset and all three tasks of ST-VQA dataset. To provide an upper bound for our method and a fair testing base for further works, we also provide human-annotated ground-truth OCR annotations for the TextVQA dataset, which were not given in the original release.",0
"Title: Structured Multimodal Attentions for TextVQA (Visual Question Answering) -------------------- Introduction: In recent years, significant advances have been made in the field of computer vision through the development of deep learning models such as Convolutional Neural Networks (CNNs). These models have proven to be highly effective at tasks such as object detection, image classification, and semantic segmentation. However, one area where these models still struggle is Visual Question Answering (VQA), which involves answering natural language questions about images. This can be attributed to the fact that VQA requires understanding both visual content and natural language, making it a challenging task even for human observers. To address this issue, we propose a novel approach called ""Structured Multimodal Attentions"" that utilizes attention mechanisms to effectively combine visual and textual features for improved performance on VQA tasks. Methodology/Approach: Our proposed method consists of three main components: visual feature extraction using a pretrained CNN model, textual feature embedding using a recurrent neural network (RNN), and structured multimodal attentions to integrate these two types of features. We first extract high-level image representations from the CNN by averaging the output of all convolutional layers followed by global pooling. We then use an RNN to encode each question into a fixed-length vector representation, which is passed through additional fully connected layers to produce final logits. For integrating the visual and textual features, we design three different attentional modules based on cross-modal interactions, i.e., self-attention networks, co-attention networks, and a hybrid version combining elements from both. Results/Findings: Evaluations demonstrate t",1
"To successfully build a deep learning model, it will need a large amount of labeled data. However, labeled data are hard to collect in many use cases. To tackle this problem, a bunch of data augmentation methods have been introduced recently and have demonstrated successful results in computer vision, natural language and so on. For financial trading data, to our best knowledge, successful data augmentation framework has rarely been studied. Here we propose a Modified Local Search Attack Sampling method to augment the candlestick data, which is a very important tool for professional trader. Our results show that the proposed method can generate high-quality data which are hard to distinguish by human and will open a new way for finance community to employ existing machine learning techniques even if the dataset is small.",0
"""Data augmentation techniques have been shown to improve performance in many machine learning tasks by increasing the size and diversity of training data. In finance, candlestick analysis plays an important role in decision making and prediction for traders and investors. This study explores how data augmentation can enhance the accuracy of deep neural networks trained on candlestick data. We propose a new method called Deep Candlestick Learner (DCL), which combines traditional technical indicators with deep learning models and uses data augmentation to increase the number of features. Our experiments demonstrate that using DCL significantly improves the accuracy of predicting stock price movements compared to other state-of-the-art methods.""",1
"Machine Learning and Inference methods have become ubiquitous in our attempt to induce more abstract representations of natural language text, visual scenes, and other messy, naturally occurring data, and support decisions that depend on it. However, learning models for these tasks is difficult partly because generating the necessary supervision signals for it is costly and does not scale. This paper describes several learning paradigms that are designed to alleviate the supervision bottleneck. It will illustrate their benefit in the context of multiple problems, all pertaining to inducing various levels of semantic representations from text.",0
"Artificial intelligence (AI) has had tremendous impacts on many industries but one thing that hampers its growth is the reliance on labeled training data which comes at significant costs due to the need for humans involvement. To address these limitations, the authors propose a new paradigm called incidental supervision wherein instead of relying solely on handlabeled data, they use other forms of indirect supervisions. This includes transfer learning from related tasks, natural language processing to generate synthetic labels, self-supervised pretraining, active learning and human feedback. They evaluate their methods on real world datasets across four domains including sentiment analysis, question answering, recommendation system and summarization showing that their proposed method outperforms baselines by large margins on most tasks while using significantly less annotated data. In conclusion, incidental supervision can overcome some of the challenges faced by traditional supervised machine learning by allowing models to learn more efficiently from indirect annotations and adapt better to changes. Overall, this research presents a new approach with great potential applications in areas like education and healthcare.",1
"Automatic search of neural architectures for various vision and natural language tasks is becoming a prominent tool as it allows to discover high-performing structures on any dataset of interest. Nevertheless, on more difficult domains, such as dense per-pixel classification, current automatic approaches are limited in their scope - due to their strong reliance on existing image classifiers they tend to search only for a handful of additional layers with discovered architectures still containing a large number of parameters. In contrast, in this work we propose a novel solution able to find light-weight and accurate segmentation architectures starting from only few blocks of a pre-trained classification network. To this end, we progressively build up a methodology that relies on templates of sets of operations, predicts which template and how many times should be applied at each step, while also generating the connectivity structure and downsampling factors. All these decisions are being made by a recurrent neural network that is rewarded based on the score of the emitted architecture on the holdout set and trained using reinforcement learning. One discovered architecture achieves 63.2% mean IoU on CamVid and 67.8% on CityScapes having only 270K parameters. Pre-trained models and the search code are available at https://github.com/DrSleep/nas-segm-pytorch.",0
"This abstract is based on the following paper: Xu et al., Template-based automatic search of compact semantic segmentation architectures, CVPR, June 2021. [PDF] (<https://arxiv.org/abs/2008.06457>) For clarity and brevity, we will refer to this as XTCS21 in the rest of this document. The problem addressed by XTCS21 is that current methods for designing neural networks use handcrafted designs which can take months or years. They propose using genetic algorithms to automatically evolve models with only a few hundred parameters, significantly reducing the time required while maintaining accuracy. Specifically, they focus on improving U-Net, which has proven effective at high resolution image segmentation tasks such as cell detection and instance segmentation. This work contributes to the fields of computer vision and machine learning. Prior art has used genetic algorithms or evolutionary computation to generate novel network architectures from scratch; however, these approaches typically produce inefficient architectures with high computational costs. In contrast, XTCS21 uses pretrained deep network features as initial population templates, allowing efficient evolution within constrained design spaces. Moreover, their framework allows easy deployment into different computing environments, including CPUs and mobile devices, without retraining. Finally, a comprehensive study compares performance against state-of-the-art models trained on large datasets, demonstrating competitive results on multiple benchmark datasets across several biological imaging domains. These findings have practical implications for accelerating model development across many scientific domains where pixelwise annotations are available but manual laborious architecture engineering remains a bottleneck. Future research directions include expanding generalization beyond biomedical images, exploring parameter sharing vs full fine-tuning within generatio",1
"Multi-hop knowledge based question answering (KBQA) is a complex task for natural language understanding. Many KBQA approaches have been proposed in recent years, and most of them are trained based on labeled reasoning path. This hinders the system's performance as many correct reasoning paths are not labeled as ground truth, and thus they cannot be learned. In this paper, we introduce an end-to-end KBQA system which can leverage multiple reasoning paths' information and only requires labeled answer as supervision. We conduct experiments on several benchmark datasets containing both single-hop simple questions as well as muti-hop complex questions, including WebQuestionSP (WQSP), ComplexWebQuestion-1.1 (CWQ), and PathQuestion-Large (PQL), and demonstrate strong performance.",0
"This paper presents a complex Knowledge Base Question Answering (KBQA) system that uses multiple reasoning paths to find answers to natural language queries. Our approach combines logical inference, rule matching, semantic analysis, and machine learning techniques to provide accurate responses. We demonstrate the effectiveness of our method by comparing it against several state-of-the-art systems on benchmark datasets and show significant improvement in both accuracy and speed. Additionally, we discuss potential applications of our KBQA system and highlight future research directions in this area. Overall, our work contributes towards building intelligent question answering systems capable of handling complex knowledge bases.",1
"The ability to take into account the characteristics - also called features - of observations is essential in Natural Language Processing (NLP) problems. Hidden Markov Chain (HMC) model associated with classic Forward-Backward probabilities cannot handle arbitrary features like prefixes or suffixes of any size, except with an independence condition. For twenty years, this default has encouraged the development of other sequential models, starting with the Maximum Entropy Markov Model (MEMM), which elegantly integrates arbitrary features. More generally, it led to neglect HMC for NLP. In this paper, we show that the problem is not due to HMC itself, but to the way its restoration algorithms are computed. We present a new way of computing HMC based restorations using original Entropic Forward and Entropic Backward (EFB) probabilities. Our method allows taking into account features in the HMC framework in the same way as in the MEMM framework. We illustrate the efficiency of HMC using EFB in Part-Of-Speech Tagging, showing its superiority over MEMM based restoration. We also specify, as a perspective, how HMCs with EFB might appear as an alternative to Recurrent Neural Networks to treat sequential data with a deep architecture.",0
"This paper provides a unified framework for hidden markov chain (HMC) based part-of-speech tagging (POST). POST has been widely used in natural language processing tasks such as dependency parsing and machine translation. While traditional HMCs have proven effective for modeling sequential data, they require expert knowledge on how to define states, which can become impractical for large vocabulary sizes. To address these issues, we propose using entropic forward-backward algorithm to learn the parameters of HMC models automatically from raw text data. We show that our approach achieves better accuracy compared to state-of-the-art techniques. Our experiments demonstrate that our method outperforms other popular POST methods across multiple languages. Finally, we make code available upon request to allow replication by others. Overall, this research shows promise towards developing more efficient NLP systems without relying extensively on human efforts. Further studies might explore new applications where POST could improve performance once the HMC model is integrated into a complete architecture.",1
"In recent years, there has been an exponential growth in the number of complex documents and texts that require a deeper understanding of machine learning methods to be able to accurately classify texts in many applications. Many machine learning approaches have achieved surpassing results in natural language processing. The success of these learning algorithms relies on their capacity to understand complex models and non-linear relationships within data. However, finding suitable structures, architectures, and techniques for text classification is a challenge for researchers. In this paper, a brief overview of text classification algorithms is discussed. This overview covers different text feature extractions, dimensionality reduction methods, existing algorithms and techniques, and evaluations methods. Finally, the limitations of each technique and their application in the real-world problem are discussed.",0
"Title: Text classification algorithms: A survey ===============================  Abstract: Text classification refers to predicting which class a given text instance belongs to based on its content. In recent years, there has been significant progress in developing advanced algorithms that can accurately classify texts into predefined categories by leveraging machine learning techniques such as supervised and unsupervised learning, deep learning, etc. This survey provides an overview of several state-of-the-art text classification algorithms, their advantages, disadvantages, data requirements, and applications. We discuss traditional methods like decision trees, random forests, Naive Bayes, support vector machines (SVMs), as well as more modern approaches such as artificial neural networks (ANNs), convolutional neural networks (CNNs), recurrent neural networks (RNNs) and Transformers. Our aim here is to provide researchers, developers and practitioners who work with text data a concise yet comprehensive guide to selecting appropriate text classification methods for different types of natural language processing tasks at hand.",1
"Time series modeling techniques based on deep learning have seen many advancements in recent years, especially in data-abundant settings and with the central aim of learning global models that can extract patterns across multiple time series. While the crucial importance of appropriate data pre-processing and scaling has often been noted in prior work, most studies focus on improving model architectures. In this paper we empirically investigate the effect of data input and output transformations on the predictive performance of several neural forecasting architectures. In particular, we investigate the effectiveness of several forms of data binning, i.e. converting real-valued time series into categorical ones, when combined with feed-forward, recurrent neural networks, and convolution-based sequence models. In many non-forecasting applications where these models have been very successful, the model inputs and outputs are categorical (e.g. words from a fixed vocabulary in natural language processing applications or quantized pixel color intensities in computer vision). For forecasting applications, where the time series are typically real-valued, various ad-hoc data transformations have been proposed, but have not been systematically compared. To remedy this, we evaluate the forecasting accuracy of instances of the aforementioned model classes when combined with different types of data scaling and binning. We find that binning almost always improves performance (compared to using normalized real-valued inputs), but that the particular type of binning chosen is of lesser importance.",0
"Abstract: In recent years, time series forecasting has become increasingly important due to the vast amounts of data generated by modern technologies. This study examines the effectiveness of discretization as a method for improving neural time series models. Specifically, we empirically evaluate the impact of different approaches to discretizing continuous time series variables on model accuracy and robustness. Our results suggest that discretization can indeed improve model performance, particularly for high frequency datasets where traditional methods may struggle to capture patterns and trends effectively. Further analysis reveals that the choice of binning strategy plays a crucial role in determining model outcomes, highlighting the importance of careful consideration when selecting appropriate discretization techniques for specific use cases. Overall, our findings demonstrate that discretization can enhance the quality of neural network predictions and offer insights into optimal implementation practices for improved decision making in the field of time series forecasting.",1
"We study the problem of jointly reasoning about language and vision through a navigation and spatial reasoning task. We introduce the Touchdown task and dataset, where an agent must first follow navigation instructions in a real-life visual urban environment, and then identify a location described in natural language to find a hidden object at the goal position. The data contains 9,326 examples of English instructions and spatial descriptions paired with demonstrations. Empirical analysis shows the data presents an open challenge to existing methods, and qualitative linguistic analysis shows that the data displays richer use of spatial reasoning compared to related resources.",0
"This paper presents an approach to natural language navigation that enables autonomous agents to navigate through complex urban environments by interacting with humans using natural language instructions. Our method uses deep learning techniques to identify objects and their relationships within street scenes, allowing our agent to localize itself, move toward a given destination, and avoid obstacles in real time. We evaluate our system on challenging outdoor driving datasets collected from the streets of large cities. Our results show that our agent significantly improves upon existing state-of-the-art methods while reducing reliance on GPS and other sensors. In addition, we demonstrate how our approach can effectively handle variations in human languages and dialects. Overall, our work represents a significant step forward towards enabling intelligent robots and vehicles that can naturally communicate with humans in diverse and dynamic visual environments.",1
"This paper introduces the task of visual question answering for remote sensing data (RSVQA). Remote sensing images contain a wealth of information which can be useful for a wide range of tasks including land cover classification, object counting or detection. However, most of the available methodologies are task-specific, thus inhibiting generic and easy access to the information contained in remote sensing data. As a consequence, accurate remote sensing product generation still requires expert knowledge. With RSVQA, we propose a system to extract information from remote sensing data that is accessible to every user: we use questions formulated in natural language and use them to interact with the images. With the system, images can be queried to obtain high level information specific to the image content or relational dependencies between objects visible in the images. Using an automatic method introduced in this article, we built two datasets (using low and high resolution data) of image/question/answer triplets. The information required to build the questions and answers is queried from OpenStreetMap (OSM). The datasets can be used to train (when using supervised methods) and evaluate models to solve the RSVQA task. We report the results obtained by applying a model based on Convolutional Neural Networks (CNNs) for the visual part and on a Recurrent Neural Network (RNN) for the natural language part to this task. The model is trained on the two datasets, yielding promising results in both cases.",0
"This paper proposes the use of deep learning models specifically designed for remote sensing data called RSVQA (Remote Sensing Vision Question Answering), which has shown promising results on real datasets over traditional QA methods. We discuss recent progresses from academia in solving complex visual question answering problems using deep neural networks. Our model is trained to extract features from remotely sensed images and provide accurate answers through a multi step process involving multiple layers of reasoning. Results show that our method outperforms previous state-of-the-art approaches by at least 20% accuracy while requiring less computational time. Our findings suggest that integrating advanced computer vision models into QA frameworks provides significant improvements in performance especially on high resolution imagery. Overall, RSVQA serves as a benchmark that future research can build upon in order to achieve greater advancements.",1
"We present the task of Spatio-Temporal Video Question Answering, which requires intelligent systems to simultaneously retrieve relevant moments and detect referenced visual concepts (people and objects) to answer natural language questions about videos. We first augment the TVQA dataset with 310.8K bounding boxes, linking depicted objects to visual concepts in questions and answers. We name this augmented version as TVQA+. We then propose Spatio-Temporal Answerer with Grounded Evidence (STAGE), a unified framework that grounds evidence in both spatial and temporal domains to answer questions about videos. Comprehensive experiments and analyses demonstrate the effectiveness of our framework and how the rich annotations in our TVQA+ dataset can contribute to the question answering task. Moreover, by performing this joint task, our model is able to produce insightful and interpretable spatio-temporal attention visualizations. Dataset and code are publicly available at: http: //tvqa.cs.unc.edu, https://github.com/jayleicn/TVQAplus",0
"This work presents a novel approach to video question answering (VQA) that incorporates spatio-temporal grounding to improve performance. VQA tasks involve understanding natural language questions related to images or videos and generating accurate answers. While existing methods have achieved some success in these tasks, there remains room for improvement. In particular, current approaches often struggle with spatial and temporal reasoning due to limitations in their attention mechanisms and lack of explicit context modeling. To address these issues, we propose a new method called TVQA+ that integrates both spatial and temporal grounding into the VQA pipeline. Our approach first predicts a bounding box around the object mentioned in the query by using region proposal networks and then extracts the relevant snippet from the video based on the predicted time interval. We then feed these snippets along with corresponding question text through a transformer network to obtain the final answer prediction. Our results show significant improvements over state-of-the-art VQA models across multiple datasets, demonstrating the effectiveness of our proposed method for solving complex VQA problems requiring spatio-temporal reasoning.",1
"In the last few years, many different methods have been focusing on using deep recurrent neural networks for natural language generation. The most widely used sequence-to-sequence neural methods are word-based: as such, they need a pre-processing step called delexicalization (conversely, relexicalization) to deal with uncommon or unknown words. These forms of processing, however, give rise to models that depend on the vocabulary used and are not completely neural.   In this work, we present an end-to-end sequence-to-sequence model with attention mechanism which reads and generates at a character level, no longer requiring delexicalization, tokenization, nor even lowercasing. Moreover, since characters constitute the common ""building blocks"" of every text, it also allows a more general approach to text generation, enabling the possibility to exploit transfer learning for training. These skills are obtained thanks to two major features: (i) the possibility to alternate between the standard generation mechanism and a copy one, which allows to directly copy input facts to produce outputs, and (ii) the use of an original training pipeline that further improves the quality of the generated texts.   We also introduce a new dataset called E2E+, designed to highlight the copying capabilities of character-based models, that is a modified version of the well-known E2E dataset used in the E2E Challenge. We tested our model according to five broadly accepted metrics (including the widely used BLEU), showing that it yields competitive performance with respect to both character-based and word-based approaches.",0
"This paper presents a novel approach to generating text based on input characters using a copy mechanism and customized model training. Traditional data-to-text generators often rely solely on pre-trained models that generate sentences without considering the specific requirements of each task. Our method addresses these limitations by incorporating a copy mechanism that retrieves matching text segments from a large corpus and uses them to guide generation. We also introduce a finetuning technique that tailors the generator to match the desired length and complexity of the output text. Experimental results show that our approach significantly outperforms baseline methods in terms of fluency, relevance, coherence, and accuracy. Furthermore, we demonstrate how our system can effectively handle complex tasks such as question answering and language translation. Overall, our work advances the state of art in character-based text generation, opening up new possibilities for natural language processing applications.",1
"Deep Convolutional Neural Network (CNN) is a special type of Neural Networks, which has shown exemplary performance on several competitions related to Computer Vision and Image Processing. Some of the exciting application areas of CNN include Image Classification and Segmentation, Object Detection, Video Processing, Natural Language Processing, and Speech Recognition. The powerful learning ability of deep CNN is primarily due to the use of multiple feature extraction stages that can automatically learn representations from the data. The availability of a large amount of data and improvement in the hardware technology has accelerated the research in CNNs, and recently interesting deep CNN architectures have been reported. Several inspiring ideas to bring advancements in CNNs have been explored, such as the use of different activation and loss functions, parameter optimization, regularization, and architectural innovations. However, the significant improvement in the representational capacity of the deep CNN is achieved through architectural innovations. Notably, the ideas of exploiting spatial and channel information, depth and width of architecture, and multi-path information processing have gained substantial attention. Similarly, the idea of using a block of layers as a structural unit is also gaining popularity. This survey thus focuses on the intrinsic taxonomy present in the recently reported deep CNN architectures and, consequently, classifies the recent innovations in CNN architectures into seven different categories. These seven categories are based on spatial exploitation, depth, multi-path, width, feature-map exploitation, channel boosting, and attention. Additionally, the elementary understanding of CNN components, current challenges, and applications of CNN are also provided.",0
"In recent years, deep convolutional neural networks (CNN) have emerged as one of the most popular architectures in computer vision tasks such as image classification, object detection, and segmentation. These models have achieved state-of-the-art performance by using multiple convolutional layers followed by pooling and nonlinear activation functions. However, designing efficient CNN architectures requires careful consideration of factors such as model size, computational cost, and parameter sharing. This survey aims to provide an overview of the latest advancements in CNN architecture designs and highlight their strengths and limitations. We present a comprehensive analysis of several prominent architectures including ResNet, DenseNet, U-Net, MobileNet, ShuffleNet, and SqueezeNet, among others. Our aim is to assist researchers and practitioners in selecting appropriate network topologies based on their specific task requirements while providing insights into future research directions.",1
"This paper focuses on the problem of unsupervised alignment of hierarchical data such as ontologies or lexical databases. This is a problem that appears across areas, from natural language processing to bioinformatics, and is typically solved by appeal to outside knowledge bases and label-textual similarity. In contrast, we approach the problem from a purely geometric perspective: given only a vector-space representation of the items in the two hierarchies, we seek to infer correspondences across them. Our work derives from and interweaves hyperbolic-space representations for hierarchical data, on one hand, and unsupervised word-alignment methods, on the other. We first provide a set of negative results showing how and why Euclidean methods fail in this hyperbolic setting. We then propose a novel approach based on optimal transport over hyperbolic spaces, and show that it outperforms standard embedding alignment techniques in various experiments on cross-lingual WordNet alignment and ontology matching tasks.",0
"In this paper we present Un supervised hierarchy matching with optimal transport over hyperbolic spaces (UMHOTHS), a novel approach to solving one of computer vision’s most challenging problems: how to accurately identify objects within complex images. Traditional methods have relied on manually labeled data to train machine learning models, but these approaches can be prohibitively time-consuming and expensive. UMHOTHS instead uses unlabeled data, leveraging recent advances in optimal transport theory to learn representations of hierarchical features that capture high-level abstractions and low-level details simultaneously. We demonstrate the effectiveness of our method using benchmark datasets such as ImageNet and MNIST, showing significantly improved performance compared to state-of-the-art algorithms that require manual labeling. Our work paves the way for more efficient and accurate object recognition systems in a wide range of applications from medical imaging to self-driving cars.",1
"Existing attention mechanisms are trained to attend to individual items in a collection (the memory) with a predefined, fixed granularity, e.g., a word token or an image grid. We propose area attention: a way to attend to areas in the memory, where each area contains a group of items that are structurally adjacent, e.g., spatially for a 2D memory such as images, or temporally for a 1D memory such as natural language sentences. Importantly, the shape and the size of an area are dynamically determined via learning, which enables a model to attend to information with varying granularity. Area attention can easily work with existing model architectures such as multi-head attention for simultaneously attending to multiple areas in the memory. We evaluate area attention on two tasks: neural machine translation (both character and token-level) and image captioning, and improve upon strong (state-of-the-art) baselines in all the cases. These improvements are obtainable with a basic form of area attention that is parameter free.",0
"This paper presents a novel approach to improving performance on natural language processing tasks using area attention mechanisms. We introduce a neural network architecture that integrates spatial reasoning into traditional sequence models, allowing them to attend to specific regions within input sequences. Our model achieves state-of-the-art results across several benchmark datasets including CoNLL2003, SciTail, and RTE. Furthermore, we demonstrate that our method can effectively handle inputs with varying lengths without any special preprocessing steps, making it well-suited for real world applications where data may come in different shapes and sizes. Overall, this work contributes new insights into the field of natural language understanding and shows promise for further improvements through spatially guided attentional processes.",1
"Deep Learning (DL) is one of the hottest trends in machine learning as DL approaches produced results superior to the state-of-the-art in problematic areas such as image processing and natural language processing (NLP). To foster the growth of DL, several open source frameworks appeared providing implementations of the most common DL algorithms. These frameworks vary in the algorithms they support and in the quality of their implementations. The purpose of this work is to provide a qualitative and quantitative comparison among three of the most popular and most comprehensive DL frameworks (namely Google's TensorFlow, University of Montreal's Theano and Microsoft's CNTK). The ultimate goal of this work is to help end users make an informed decision about the best DL framework that suits their needs and resources. To ensure that our study is as comprehensive as possible, we conduct several experiments using multiple benchmark datasets from different fields (image processing, NLP, etc.) and measure the performance of the frameworks' implementations of different DL algorithms. For most of our experiments, we find out that CNTK's implementations are superior to the other ones under consideration.",0
"Deep Learning (DL) has gained significant traction in recent years due to its successes in many complex real world problems such as image recognition, natural language processing etc. There exist several Open Source DL Frameworks available today which allow us to build and train Machine Learning models easily without having prior expertise on specific hardware architectures or software development. In this work we aim to provide a comprehensive comparison of these Open Source DL Frameworks based on different performance metrics such as accuracy, speed and memory usage. We have used popular datasets widely used by researchers as well as industry practitioners to evaluate these frameworks under various scenarios. Our experimental results show that each framework performs differently in terms of model size, latency & throughput, depending on the type of problem at hand. Additionally, our evaluation process will provide insights into factors affecting the deployment of machine learning algorithms across multiple domains. By providing an overview of popular DL Frameworks and highlighting their respective strengths & weaknesses, developers can make informed decisions while designing ML systems tailored towards solving diverse challenges emerging from applications across fields like Healthcare, Finance, Retail amongst others. This research work would serve both academia as well as industries interested in building efficient Artificial Intelligence Systems using cutting edge DL technology.",1
"The recent developments and growing interest in neural-symbolic models has shown that hybrid approaches can offer richer models for Artificial Intelligence. The integration of effective relational learning and reasoning methods is one of the key challenges in this direction, as neural learning and symbolic reasoning offer complementary characteristics that can benefit the development of AI systems. Relational labelling or link prediction on knowledge graphs has become one of the main problems in deep learning-based natural language processing research. Moreover, other fields which make use of neural-symbolic techniques may also benefit from such research endeavours. There have been several efforts towards the identification of missing facts from existing ones in knowledge graphs. Two lines of research try and predict knowledge relations between two entities by considering all known facts connecting them or several paths of facts connecting them. We propose a neural-symbolic graph neural network which applies learning over all the paths by feeding the model with the embedding of the minimal subset of the knowledge graph containing such paths. By learning to produce representations for entities and facts corresponding to word embeddings, we show how the model can be trained end-to-end to decode these representations and infer relations between entities in a multitask approach. Our contribution is two-fold: a neural-symbolic methodology leverages the resolution of relational inference in large graphs, and we also demonstrate that such neural-symbolic model is shown more effective than path-based approaches",0
"Abstract:  This research presents a neural-symbolic relational reasoning approach that enables effective link inference and computation from knowledge bases represented as graph models. Our method leverages recent advances in deep learning and symbolic artificial intelligence, allowing for joint modeling of both logical and probabilistic aspects of knowledge representation. This hybrid approach ensures scalability while maintaining expressiveness and interpretability. We demonstrate our framework by addressing two important tasks: (i) predicting missing links in a given graph using machine learning techniques; and (ii) computing answers to queries involving complex relations by integrating logic solvers into the training process. Experimental results on several benchmark datasets show promising performance improvements over existing methods, highlighting the effectiveness of our proposed approach.",1
"Although adaptive optimization algorithms such as Adam show fast convergence in many machine learning tasks, this paper identifies a problem of Adam by analyzing its performance in a simple non-convex synthetic problem, showing that Adam's fast convergence would possibly lead the algorithm to local minimums. To address this problem, we improve Adam by proposing a novel adaptive gradient descent algorithm named AdaX. Unlike Adam that ignores the past gradients, AdaX exponentially accumulates the long-term gradient information in the past during training, to adaptively tune the learning rate. We thoroughly prove the convergence of AdaX in both the convex and non-convex settings. Extensive experiments show that AdaX outperforms Adam in various tasks of computer vision and natural language processing and can catch up with Stochastic Gradient Descent.",0
"""Adaptive Gradient Descent with Exponential Long Term Memory"" presents a new algorithm called ""AdaX"" that combines adaptive gradient descent with exponential long term memory. This algorithm offers several key benefits over traditional gradient descent methods, including improved accuracy and faster convergence rates. In particular, AdaX uses exponential smoothing to maintain a running average of past gradients, which allows it to effectively adjust learning rates based on recent data trends. Additionally, the use of exponential decay ensures that older gradients have less influence on the current model parameters than more recent ones, thereby enabling the algorithm to focus on relevant training examples. Experiments conducted using diverse datasets demonstrate that AdaX outperforms state-of-the-art optimization techniques, such as Adagrad and Adam, and can reduce the number of iterations required by up to 98%. Overall, the results indicate that AdaX has significant potential for improving deep neural network training across a variety of applications.",1
"This paper presents several strategies that can improve neural network-based predictive methods for MOOC student course trajectory modeling, applying multiple ideas previously applied to tackle NLP (Natural Language Processing) tasks. In particular, this paper investigates LSTM networks enhanced with two forms of regularization, along with the more recently introduced Transformer architecture.",0
"Abstract: MOOCs have become increasingly popular over recent years as more individuals turn towards online resources to learn new skills. Students often enroll in multiple courses simultaneously, making their course trajectories difficult to analyze. However, advances in natural language processing (NLP) can be utilized to overcome these challenges by modeling student behavior patterns in real-time. This study proposes a novel approach that combines NLP techniques such as topic modeling, word embedding and transfer learning algorithms, to accurately predict future actions taken by students who attend massive open online courses (MOOCS). Our results demonstrate that our proposed method achieves higher accuracy compared to traditional methods and provides valuable insights into how students navigate through complex educational pathways. Our work contributes to the growing field of MOOC analytics and has significant implications for education providers looking to optimize student engagement.",1
"Recurrent Neural Networks (RNNs) have been shown to be valuable for constructing Intrusion Detection Systems (IDSs) for network data. They allow determining if a flow is malicious or not already before it is over, making it possible to take action immediately. However, considering the large number of packets that has to be inspected, for example in cloud/fog and edge computing, the question of computational efficiency arises. We show that by using a novel Reinforcement Learning (RL)-based approach called SparseIDS, we can reduce the number of consumed packets by more than three fourths while keeping classification accuracy high. To minimize the computational expenses of the RL-based sampling we show that a shared neural network can be used for both the classifier and the RL logic. Thus, no additional resources are consumed by the sampling in deployment. Comparing to various other sampling techniques, SparseIDS consistently achieves higher classification accuracy by learning to sample only relevant packets. A major novelty of our RL-based approach is that it can not only skip up to a predefined maximum number of samples like other approaches proposed in the domain of Natural Language Processing but can even skip arbitrarily many packets in one step. This enables saving even more computational resources for long sequences. Inspecting SparseIDS's behavior of choosing packets shows that it adopts different sampling strategies for different attack types and network flows. Finally we build an automatic steering mechanism that can guide SparseIDS in deployment to achieve a desired level of sparsity.",0
"Title: ""Learning Adaptive Network Intrusion Detection with Reward Shaping""  In today's rapidly evolving security landscape, network intrusion detection systems (NIDS) must balance accurate threat detection with minimizing resource overhead. One approach to achieving these conflicting goals is by using packet sampling, where NIDS selectively inspect packets from traffic flows based on their potential risk. However, traditional static packet selection policies can be insufficient in dynamic environments, as attack patterns change over time and new vulnerabilities emerge. To address this challenge, we propose SparseIDS, which integrates deep reinforcement learning into a distributed architecture that adapts to changing network conditions. By combining reward shaping with sparse, event-based sampling decisions, our system achieves state-of-the art performance across multiple metrics while reducing processing overhead by up to 98%. Our results showcase the effectiveness of incorporating explicit rewards within the context of network intrusion detection, emphasizing the importance of modeling both short-term benefits and longer-term objectives towards building more robust security platforms.",1
"Voice Assistants aim to fulfill user requests by choosing the best intent from multiple options generated by its Automated Speech Recognition and Natural Language Understanding sub-systems. However, voice assistants do not always produce the expected results. This can happen because voice assistants choose from ambiguous intents - user-specific or domain-specific contextual information reduces the ambiguity of the user request. Additionally the user information-state can be leveraged to understand how relevant/executable a specific intent is for a user request. In this work, we propose a novel Energy-based model for the intent ranking task, where we learn an affinity metric and model the trade-off between extracted meaning from speech utterances and relevance/executability aspects of the intent. Furthermore we present a Multisource Denoising Autoencoder based pretraining that is capable of learning fused representations of data from multiple sources. We empirically show our approach outperforms existing state of the art methods by reducing the error-rate by 3.8%, which in turn reduces ambiguity and eliminates undesired dead-ends leading to better user experience. Finally, we evaluate the robustness of our algorithm on the intent ranking task and show our algorithm improves the robustness by 33.3%.",0
"""Learning to rank intents in voice assistants"" explores methods for improving the accuracy of task completion by virtual assistants through intent ranking. Virtual assistants have become increasingly popular as they simplify tasks such as setting alarms, sending messages, finding nearby businesses, making phone calls, and controlling smart home devices. Despite their utility, these assistants often fail to accurately complete tasks due to incorrect interpretation of user requests. In this study, we propose an approach that uses large amounts of data from user interactions and machine learning algorithms to improve intent ranking models. By using natural language understanding techniques to better identify and categorize user intentions, our model outperforms existing state-of-the-art systems on public benchmarks. This research has implications in both academia and industry as it presents new possibilities for developing more accurate and reliable digital assistants.",1
"Despite the great success of Convolutional Neural Networks (CNNs) in Computer Vision and Natural Language Processing, the working mechanism behind CNNs is still under extensive discussions and research. Driven by a strong demand for the theoretical explanation of neural networks, some researchers utilize information theory to provide insight into the black box model. However, to the best of our knowledge, employing information theory to quantitatively analyze and qualitatively visualize neural networks has not been extensively studied in the visualization community. In this paper, we combine information entropies and visualization techniques to shed light on how CNN works. Specifically, we first introduce a data model to organize the data that can be extracted from CNN models. Then we propose two ways to calculate entropy under different circumstances. To provide a fundamental understanding of the basic building blocks of CNNs (e.g., convolutional layers, pooling layers, normalization layers) from an information-theoretic perspective, we develop a visual analysis system, CNNSlicer. CNNSlicer allows users to interactively explore the amount of information changes inside the model. With case studies on the widely used benchmark datasets (MNIST and CIFAR-10), we demonstrate the effectiveness of our system in opening the blackbox of CNNs.",0
"This paper proposes an information-theoretic visual analysis framework for studying convolutional neural networks (CNNs). CNNs have been widely used in image processing tasks but their inner workings remain largely opaque. Our proposed framework provides a systematic approach for understanding how different components of a CNN contribute to its performance. We develop three novel methods based on mutual information, namely channel relevance analysis, feature importance maps, and multi-scale decomposition. These tools provide unique insights into the functioning of CNNs and reveal previously unknown aspects of their behavior. Experiments performed on several benchmark datasets demonstrate that our methods can effectively measure and explain the performance of CNNs under different settings. Overall, our work paves the way for more interpretable machine learning models and contributes to the growing field of explainability research.",1
"We develop a language-guided navigation task set in a continuous 3D environment where agents must execute low-level actions to follow natural language navigation directions. By being situated in continuous environments, this setting lifts a number of assumptions implicit in prior work that represents environments as a sparse graph of panoramas with edges corresponding to navigability. Specifically, our setting drops the presumptions of known environment topologies, short-range oracle navigation, and perfect agent localization. To contextualize this new task, we develop models that mirror many of the advances made in prior settings as well as single-modality baselines. While some of these techniques transfer, we find significantly lower absolute performance in the continuous setting -- suggesting that performance in prior `navigation-graph' settings may be inflated by the strong implicit assumptions.",0
"In recent years, the field of artificial intelligence has made significant strides in developing algorithms that can perform complex tasks such as image recognition and natural language processing. One area where these advances have been particularly impactful is in the development of vision-based navigation systems. These systems use computer vision techniques to identify objects in a scene and determine how they relate to each other, allowing robots to navigate through complex environments autonomously. However, current vision-based navigation approaches often rely on predefined maps or graph representations of the environment, which may not be accurate or up-to-date. This can lead to errors and limitations in performance. Therefore, there is a need for new methods that enable more flexible and effective vision-based navigation in continuous environments. In our work, we propose a novel approach that combines vision and natural language processing to allow robots to interpret instructions and navigate through real-world scenarios without relying on predefined graphs or maps. Our method uses deep learning models to extract relevant features from both visual and textual input streams and integrate them into a single representation. We evaluate our approach using experiments conducted in diverse indoor and outdoor environments and demonstrate that our system significantly improves navigation accuracy compared to state-of-the-art methods. Overall, our research presents a step forward towards enabling robots to effectively operate in dynamic and unstructured settings.",1
"One of deep learning's attractive benefits is the ability to automatically extract relevant features for a target problem from largely raw data, instead of utilizing human engineered and error prone handcrafted features. While deep learning has shown success in fields such as image classification and natural language processing, its application for feature extraction on raw network packet data for intrusion detection is largely unexplored. In this paper we modify a Word2Vec approach, used for text processing, and apply it to packet data for automatic feature extraction. We call this approach Packet2Vec. For the classification task of benign versus malicious traffic on a 2009 DARPA network data set, we obtain an area under the curve (AUC) of the receiver operating characteristic (ROC) between 0.988-0.996 and an AUC of the Precision/Recall curve between 0.604-0.667.",0
"In ""Packet2Vec: Utilizing Word2Vec for Feature Extraction in Packet Data,"" we propose a new approach to feature extraction that leverages recent advances in natural language processing (NLP) techniques such as word embeddings. This method, which we refer to as ""Packet2Vec"", enables efficient representation of large volumes of packet data by condensing high-dimensional features into low-dimensional, meaningful representations that can be used for analysis and classification tasks. By adapting NLP methods to network traffic analysis, our approach offers several benefits over traditional feature engineering techniques including improved interpretability, scalability and potential robustness to changes in data sources. We evaluate the effectiveness of our proposed model using real-world datasets and demonstrate significant improvements over baseline models across multiple performance metrics. These findings suggest that our method has great potential for use in cybersecurity applications where accurate detection and prediction of threats is critical.",1
"Multi-task learning (mtl) provides state-of-the-art results in many applications of computer vision and natural language processing. In contrast to single-task learning (stl), mtl allows for leveraging knowledge between related tasks improving prediction results on the main task (in contrast to an auxiliary task) or all tasks. However, there is a limited number of comparative studies on applying mtl architectures for regression and time series problems taking recent advances of mtl into account. An interesting, non-linear problem is the forecast of the expected power generation for renewable power plants. Therefore, this article provides a comparative study of the following recent and important mtl architectures: Hard parameter sharing, cross-stitch network, sluice network (sn). They are compared to a multi-layer perceptron model of similar size in an stl setting. Additionally, we provide a simple, yet effective approach to model task specific information through an embedding layer in an multi-layer perceptron, referred to as task embedding. Further, we introduce a new mtl architecture named emerging relation network (ern), which can be considered as an extension of the sluice network. For a solar power dataset, the task embedding achieves the best mean improvement with 14.9%. The mean improvement of the ern and the sn on the solar dataset is of similar magnitude with 14.7% and 14.8%. On a wind power dataset, only the ern achieves a significant improvement of up to 7.7%. Results suggest that the ern is beneficial when tasks are only loosely related and the prediction problem is more non-linear. Contrary, the proposed task embedding is advantageous when tasks are strongly correlated. Further, the task embedding provides an effective approach with reduced computational effort compared to other mtl architectures.",0
"The multi-task regression problem has been studied extensively by researchers due to its importance in machine learning applications such as natural language processing, computer vision, and speech recognition. However, most existing methods focus on using hand-engineered features which may not capture all relevant aspects of the task at hand. In recent years, deep neural networks have shown promising results in solving complex tasks but require large amounts of data and computational resources. To address these issues, we propose a novel approach called emerging relation network (ERN) that models relationships between tasks in order to perform better in low-data scenarios. ERN uses graph convolutional layers to learn task dependencies and utilizes task embedding to project each task into a shared latent space where interactions can occur among related tasks. Our experiments show improved performance compared to state-of-the-art baseline models across multiple benchmark datasets while requiring less computing power. Our work paves the way towards efficient solutions for multi-task regression problems in limited data settings.",1
"While the success of deep neural networks (DNNs) is well-established across a variety of domains, our ability to explain and interpret these methods is limited. Unlike previously proposed local methods which try to explain particular classification decisions, we focus on global interpretability and ask a universally applicable question: given a trained model, which features are the most important? In the context of neural networks, a feature is rarely important on its own, so our strategy is specifically designed to leverage partial covariance structures and incorporate variable dependence into feature ranking. Our methodological contributions in this paper are two-fold. First, we propose an effect size analogue for DNNs that is appropriate for applications with highly collinear predictors (ubiquitous in computer vision). Second, we extend the recently proposed ""RelATive cEntrality"" (RATE) measure (Crawford et al., 2019) to the Bayesian deep learning setting. RATE applies an information theoretic criterion to the posterior distribution of effect sizes to assess feature significance. We apply our framework to three broad application areas: computer vision, natural language processing, and social science.",0
"Abstract: This paper presents a method for interpreting deep neural networks (DNNs) by evaluating variable importance. DNNs have been shown to produce excellent results on complex tasks but their opaque nature makes it difficult for practitioners to explain how they arrive at decisions. This study proposes an approach that uses sensitivity analysis to quantify the impact of each input feature on model predictions. We demonstrate the effectiveness of our method using several case studies and show that it can identify meaningful patterns in the data that influence network behavior. Our findings contribute to the growing field of interpretability research, which seeks to improve transparency and trustworthiness in machine learning models.",1
"In recent years, the emerging topics of recommender systems that take advantage of natural language processing techniques have attracted much attention, and one of their applications is the Conversational Recommender System (CRS). Unlike traditional recommender systems with content-based and collaborative filtering approaches, CRS learns and models user's preferences through interactive dialogue conversations. In this work, we provide a summarization of the recent evolution of CRS, where deep learning approaches are applied to CRS and have produced fruitful results. We first analyze the research problems and present key challenges in the development of Deep Conversational Recommender Systems (DCRS), then present the current state of the field taken from the most recent researches, including the most common deep learning models that benefit DCRS. Finally, we discuss future directions for this vibrant area.",0
"In recent years there has been growing interest in conversational recommender systems (RS), which can interactively provide personalized recommendations within natural language conversations, enabling users to state their preferences or goals as part of the conversation process. These goal-oriented RS aim to assist users by providing advice, explanations, suggestions, opinions, feedback or other forms of guidance while respecting their privacy concerns and minimizing disruption to the ongoing dialogue. Unlike traditional web search engines that rely on static keywords and lack interaction, deep conversational recommendation models take advantage of human-AI interactions and leverage large amounts of user data to generate more accurate predictions through fine-grained user modeling. This paper presents a comprehensive survey of current research trends and open challenges in conversational recommendation systems, including aspects such as natural language understanding, active learning, knowledge representation and reasoning, explainability, context awareness, social intelligence, multi-turn dialogue, and evaluation metrics. By shedding light on these important issues, we hope to inspire further exploration into this nascent but rapidly evolving field.",1
"This paper presents a framework to recognize temporal compositions of atomic actions in videos. Specifically, we propose to express temporal compositions of actions as semantic regular expressions and derive an inference framework using probabilistic automata to recognize complex actions as satisfying these expressions on the input video features. Our approach is different from existing works that either predict long-range complex activities as unordered sets of atomic actions, or retrieve videos using natural language sentences. Instead, the proposed approach allows recognizing complex fine-grained activities using only pretrained action classifiers, without requiring any additional data, annotations or neural network training. To evaluate the potential of our approach, we provide experiments on synthetic datasets and challenging real action recognition datasets, such as MultiTHUMOS and Charades. We conclude that the proposed approach can extend state-of-the-art primitive action classifiers to vastly more complex activities without large performance degradation.",0
"This paper presents a novel approach to inferring temporal compositions of actions using probabilistic automata. By combining principles from the domains of machine learning and control theory, we develop a framework that enables efficient inference of complex action sequences based on observed data. Our method builds upon a recently introduced model called a probabilistic automaton, which represents a distribution over states rather than a single state as in traditional automata. We propose algorithms for both offline and online inference tasks, including joint inference of multiple composition variables under general uncertainty. Through experiments involving simulated robots, we demonstrate our framework can accurately identify high-level action plans and adapt to changes in environments. Our results suggest wide applicability of the proposed technique across numerous fields requiring decision making under uncertain conditions. While there exist prior works tackling related problems such as hierarchical task decomposition and active exploration, our paper offers new insights into composing actions by exploiting rich representations derived from recent advances in deep reinforcement learning. Overall, we believe this work contributes significantly towards developing intelligent systems capable of learning and acting effectively in uncertain real-world settings.",1
"Referring expression comprehension aims to localize the object instance described by a natural language expression. Current referring expression methods have achieved good performance. However, none of them is able to achieve real-time inference without accuracy drop. The reason for the relatively slow inference speed is that these methods artificially split the referring expression comprehension into two sequential stages including proposal generation and proposal ranking. It does not exactly conform to the habit of human cognition. To this end, we propose a novel Realtime Cross-modality Correlation Filtering method (RCCF). RCCF reformulates the referring expression comprehension as a correlation filtering process. The expression is first mapped from the language domain to the visual domain and then treated as a template (kernel) to perform correlation filtering on the image feature map. The peak value in the correlation heatmap indicates the center points of the target box. In addition, RCCF also regresses a 2-D object size and 2-D offset. The center point coordinates, object size and center point offset together to form the target bounding box. Our method runs at 40 FPS while achieving leading performance in RefClef, RefCOCO, RefCOCO+ and RefCOCOg benchmarks. In the challenging RefClef dataset, our methods almost double the state-of-the-art performance (34.70% increased to 63.79%). We hope this work can arouse more attention and studies to the new cross-modality correlation filtering framework as well as the one-stage framework for referring expression comprehension.",0
"In real-time referring expression comprehension (REC), understanding the correlation between different modalities has been proved efficient for improving robustness and accuracy performance on REC tasks. With more data sources available, finding effective ways to integrate cross-modal information into REC pipelines has become increasingly challenging. This work presents a novel method that performs real-time correlation filtering between visual features from video frames and speech features extracted using Automatic Speech Recognition (ASR). Our approach uses a two-stream neural network architecture where each stream processes one modality respectively. We then introduce a correlation filter module which applies a spatio-temporal attention mechanism over both streams, enabling them to attend to relevant information across modalities and generate weighted feature maps. These weighted feature maps are concatenated into a final representation used by the downstream model for REC. Experimental results demonstrate significant improvement compared to traditional fusion methods like Early Fusion and Late Fusion. The proposed method shows better robustness against incorrect ASR outputs than previous works, providing competitive performance at inference time with faster speed. Additionally, our system achieves state-of-the-art performance on the popular benchmark dataset Charades REC without requiring any external pre-training. By proposing a novel correlation filtering mechanism based on self-attention and introducing early vision-speech interaction, we contribute to reducing the gap between academic research in REC and industrial applications demanding low latency processing times, scalability, and high precision.",1
"Deep learning continues to revolutionize an ever-growing number of critical application areas including healthcare, transportation, finance, and basic sciences. Despite their increased predictive power, model transparency and human explainability remain a significant challenge due to the ""black box"" nature of modern deep learning models. In many cases the desired balance between interpretability and performance is predominately task specific. Human-centric domains such as healthcare necessitate a renewed focus on understanding how and why these frameworks are arriving at critical and potentially life-or-death decisions. Given the quantity of research and empirical successes of deep learning for computer vision, most of the existing interpretability research has focused on image processing techniques. Comparatively, less attention has been paid to interpreting deep learning frameworks using sequential data. Given recent deep learning advancements in highly sequential domains such as natural language processing and physiological signal processing, the need for deep sequential explanations is at an all-time high. In this paper, we review current techniques for interpreting deep learning techniques involving sequential data, identify similarities to non-sequential methods, and discuss current limitations and future avenues of sequential interpretability research.",0
"This paper focuses on the challenges and solutions for understanding deep learning models that operate on sequential data. These models, including Recurrent Neural Networks (RNNs) and Long Short Term Memory networks (LSTM), have shown great promise in tasks such as speech recognition, natural language processing, and computer vision. However, they can be difficult to interpret due to their complex internal mechanisms and ability to learn highly nonlinear representations. To address these issues, we propose a new framework called ""Sequential Interpretability"". In this framework, interpreting the predictions made by the model becomes synonymous with determining how a particular input causes its output in each step of processing. We demonstrate our approach using several case studies from diverse domains, highlighting promising applications and future research directions. Ultimately, our work provides insights into improving human trust and confidence in decision making based on black box models and paves the way towards developing explainable deep learning systems.",1
"Convolutional Neural Networks (CNNs) have become common in many fields including computer vision, speech recognition, and natural language processing. Although CNN hardware accelerators are already included as part of many SoC architectures, the task of achieving high accuracy on resource-restricted devices is still considered challenging, mainly due to the vast number of design parameters that need to be balanced to achieve an efficient solution. Quantization techniques, when applied to the network parameters, lead to a reduction of power and area and may also change the ratio between communication and computation. As a result, some algorithmic solutions may suffer from lack of memory bandwidth or computational resources and fail to achieve the expected performance due to hardware constraints. Thus, the system designer and the micro-architect need to understand at early development stages the impact of their high-level decisions (e.g., the architecture of the CNN and the amount of bits used to represent its parameters) on the final product (e.g., the expected power saving, area, and accuracy). Unfortunately, existing tools fall short of supporting such decisions.   This paper introduces a hardware-aware complexity metric that aims to assist the system designer of the neural network architectures, through the entire project lifetime (especially at its early stages) by predicting the impact of architectural and micro-architectural decisions on the final product. We demonstrate how the proposed metric can help evaluate different design alternatives of neural network models on resource-restricted devices such as real-time embedded systems, and to avoid making design mistakes at early stages.",0
"""The development of neural network architectures has seen tremendous growth over recent years due to advances in deep learning techniques. However, as the size and complexity of these models increase, understanding their behavior becomes more challenging, leading to difficulties in designing efficient and effective networks. To address these issues, we propose the use of a hardware-aware complexity metric called HCM (Hardware-Aware Complexity Metric) that estimates the computational cost required by each architecture based on parameters like number of operations, weights, depth, width, and connections between layers. By incorporating these characteristics into our metric, HCM provides insights into the performance and efficiency of different neural network architectures under specific hardware conditions such as GPUs and TPUs. Our experimental results demonstrate the effectiveness of using HCM in selecting appropriate architectures for given hardware constraints, achieving better predictive accuracy while reducing training time and improving computational efficiency.""",1
"Image classification has been studied extensively, but there has been limited work in using unconventional, external guidance other than traditional image-label pairs for training. We present a set of methods for leveraging information about the semantic hierarchy embedded in class labels. We first inject label-hierarchy knowledge into an arbitrary CNN-based classifier and empirically show that availability of such external semantic information in conjunction with the visual semantics from images boosts overall performance. Taking a step further in this direction, we model more explicitly the label-label and label-image interactions using order-preserving embeddings governed by both Euclidean and hyperbolic geometries, prevalent in natural language, and tailor them to hierarchical image classification and representation learning. We empirically validate all the models on the hierarchical ETHEC dataset.",0
"This paper presents a novel approach to hierarchical image classification based on entailment cone embeddings (ECE). ECE represents concepts as cones over vector spaces and captures their relationships through logical entailment rules. By leveraging these relationships in a hierarchy, we can effectively classify images into multiple levels of abstraction. We first introduce our conceptual framework and discuss existing approaches to entailment-based image classification. Then, we present our proposed methodology that integrates ECE into a deep convolutional neural network architecture. Our model achieves state-of-the-art performance on three benchmark datasets for semantic image segmentation, scene recognition, and fine-grained object detection. Furthermore, we demonstrate how our method improves upon previous methods by evaluating its robustness to noisy inputs and visualizing the learned feature representations. Finally, we provide insights into future directions for research related to high-level visual reasoning and cognitive artificial intelligence systems. Overall, our work advances knowledge representation techniques in computer vision towards more human-like understanding and decision making.",1
"Neural network models are widely used in solving many challenging problems, such as computer vision, personalized recommendation, and natural language processing. Those models are very computationally intensive and reach the hardware limit of the existing server and IoT devices. Thus, finding better model architectures with much less amount of computation while maximally preserving the accuracy is a popular research topic. Among various mechanisms that aim to reduce the computation complexity, identifying the zero values in the model weights and in the activations to avoid computing them is a promising direction.   In this paper, we summarize the current status of the research on the computation of sparse neural networks, from the perspective of the sparse algorithms, the software frameworks, and the hardware accelerations. We observe that the search for the sparse structure can be a general methodology for high-quality model explorations, in addition to a strategy for high-efficiency model execution. We discuss the model accuracy influenced by the number of weight parameters and the structure of the model. The corresponding models are called to be located in the weight dominated and structure dominated regions, respectively. We show that for practically complicated problems, it is more beneficial to search large and sparse models in the weight dominated region. In order to achieve the goal, new approaches are required to search for proper sparse structures, and new sparse training hardware needs to be developed to facilitate fast iterations of sparse models.",0
"Artificial neural networks have revolutionized many areas of computer science by providing new tools for solving complex problems that were previously difficult to tackle. However, these models often require large amounts of computational resources to store their parameters and operate effectively. This can limit their use in resource-constrained systems such as smartphones, embedded devices, and internet-of-things (IoT) nodes. To address these limitations, researchers have proposed sparse artificial neural networks (ANN), which significantly reduce storage requirements while preserving accuracy. These developments have opened up opportunities for creating new hardware tailored specifically for ANN computing, enabling more efficient implementation of modern machine learning algorithms. This article presents an analysis of recent advances in sparse ANN computation and discusses how they may inspire future hardware design. The authors examine current methods used to represent and manipulate sparse ANNs efficiently, highlighting potential optimization strategies that could improve energy efficiency and performance. They also explore emerging techniques for hardware acceleration of sparsity computations, including neuromorphic chips and analog circuits, emphasizing their advantages over traditional digital architectures. By reviewing the latest findings and trends in this rapidly evolving field, we aim to provide insights into promising directions for research in both algorithmic and hardware aspects of sparse ANN computation. Our work serves as a starting point for readers who wish to delve deeper into the subject and contribute to shaping the next generation of intelligent systems.",1
"The extraction of main content from web pages is an important task for numerous applications, ranging from usability aspects, like reader views for news articles in web browsers, to information retrieval or natural language processing. Existing approaches are lacking as they rely on large amounts of hand-crafted features for classification. This results in models that are tailored to a specific distribution of web pages, e.g. from a certain time frame, but lack in generalization power. We propose a neural sequence labeling model that does not rely on any hand-crafted features but takes only the HTML tags and words that appear in a web page as input. This allows us to present a browser extension which highlights the content of arbitrary web pages directly within the browser using our model. In addition, we create a new, more current dataset to show that our model is able to adapt to changes in the structure of web pages and outperform the state-of-the-art model.",0
"In our work we address the problem of automatically identifying boilerplate content and removing it from large collections such as websites and documents on the internet. We explore the use of machine learning techniques for sequence labeling problems and present results on benchmark data sets demonstrating their effectiveness. Our approach uses recurrent neural networks (RNNs) to model the dependencies between the tokens and leverages several different features in order to improve performance. We evaluate these models against prior methods and demonstrate that they achieve significant improvements over state-of-the-art approaches. Finally, we discuss future directions and potential applications of this research.",1
"Neural networks are surprisingly good at interpolating and perform remarkably well when the training set examples resemble those in the test set. However, they are often unable to extrapolate patterns beyond the seen data, even when the abstractions required for such patterns are simple. In this paper, we first review the notion of extrapolation, why it is important and how one could hope to tackle it. We then focus on a specific type of extrapolation which is especially useful for natural language processing: generalization to sequences that are longer than the training ones. We hypothesize that models with a separate content- and location-based attention are more likely to extrapolate than those with common attention mechanisms. We empirically support our claim for recurrent seq2seq models with our proposed attention on variants of the Lookup Table task. This sheds light on some striking failures of neural models for sequences and on possible methods to approaching such issues.",0
"This paper presents a new method called location attention that enables deep learning models to extrapolate beyond their training data more effectively. Inspired by recent work on self-attention mechanisms, we show how to modify existing models such as recurrent neural networks (RNNs) and transformers so they can attend to specific locations within input sequences during inference. Our approach introduces lightweight modifications that do not require retraining and have no additional parameters, making them suitable for deployment in constrained environments where computational resources are limited. We evaluate our model on several benchmark tasks, including language generation, image classification, and machine translation, demonstrating significant improvements over baseline methods that rely solely on vanilla RNNs or transformers. By explicitly reasoning about spatial relationships between elements in input sequences, location attention enables better generalization to longer inputs than traditional sequence processing algorithms.",1
"Joint feature selection and classification in an online setting is essential for time-sensitive decision making. However, most existing methods treat this coupled problem independently. Specifically, online feature selection methods can handle either streaming features or data instances offline to produce a fixed set of features for classification, while online classification methods classify incoming instances using full knowledge about the feature space. Nevertheless, all existing methods utilize a set of features, common for all data instances, for classification. Instead, we propose a framework to perform joint feature selection and classification on-the-fly, so as to minimize the number of features evaluated for every data instance and maximize classification accuracy. We derive the optimum solution of the associated optimization problem and analyze its structure. Two algorithms are proposed, ETANA and F-ETANA, which are based on the optimum solution and its properties. We evaluate the performance of the proposed algorithms on several public datasets, demonstrating (i) the dominance of the proposed algorithms over the state-of-the-art, and (ii) its applicability to broad range of application domains including clinical research and natural language processing.",0
"This should be able to be understood by someone who has no background in machine learning. For example, explain that joint feature selection means looking at many features together as one set instead of individually. What kind of models are you comparing? Explain what ""on-the-fly"" refers to. Finally, end by summarizing the contributions made by your research. This is meant to entice readers into wanting to learn more about the subject matter while understanding enough about the subject from reading only this summary to hold their own in casual conversation. Please focus on explaining the important aspects without bogging down in technical details or jargon if possible. In the event that some necessary detail needs including please provide additional explanation alongside it. Thank you!",1
"One of the primary challenges limiting the applicability of deep learning is its susceptibility to learning spurious correlations rather than the underlying mechanisms of the task of interest. The resulting failure to generalise cannot be addressed by simply using more data from the same distribution. We propose an auxiliary training objective that improves the generalization capabilities of neural networks by leveraging an overlooked supervisory signal found in existing datasets. We use pairs of minimally-different examples with different labels, a.k.a counterfactual or contrasting examples, which provide a signal indicative of the underlying causal structure of the task. We show that such pairs can be identified in a number of existing datasets in computer vision (visual question answering, multi-label image classification) and natural language processing (sentiment analysis, natural language inference). The new training objective orients the gradient of a model's decision function with pairs of counterfactual examples. Models trained with this technique demonstrate improved performance on out-of-distribution test sets.",0
In this paper we present a method that uses counterfactuals and gradient supervision to learn which interventions make a difference. We first describe how counterfactuals can be used to identify what would have happened if a particular event had occurred differently. Next we explain how these counterfactuals can then be leveraged to guide learning by specifying whether a change was beneficial or harmful. Our approach builds on prior work using counterfactuals in machine learning and incorporates ideas from adversarial training. Extensive experimentation shows our method leads to substantially better results than alternative approaches. Finally we discuss implications and future directions. -----,1
"In recent years, topological data analysis has been utilized for a wide range of problems to deal with high dimensional noisy data. While text representations are often high dimensional and noisy, there are only a few work on the application of topological data analysis in natural language processing. In this paper, we introduce a novel algorithm to extract topological features from word embedding representation of text that can be used for text classification. Working on word embeddings, topological data analysis can interpret the embedding high-dimensional space and discover the relations among different embedding dimensions. We will use persistent homology, the most commonly tool from topological data analysis, for our experiment. Examining our topological algorithm on long textual documents, we will show our defined topological features may outperform conventional text mining features.",0
"Abstract: In recent years, word embeddings have become a popular tool for capturing the semantic meaning of words in natural language processing tasks. However, most current methods used for extracting topological features from these vectors ignore their distributional nature. We present a novel method of extracting topological features by considering both local and global structure of the vector space that preserves the directionality of relations among words. Our approach utilizes normalized cut graphs which can efficiently capture the relationships between different topics within large embedding spaces. Experimental results on several benchmark datasets show that our method outperforms state-of-the art techniques significantly. This work provides new insights into how we can better exploit the geometric properties of high dimensional spaces like those formed by word embeddings. Keywords: word embeddings, topology, vector spaces, neural networks, NLP.",1
"Grounding referring expressions in images aims to locate the object instance in an image described by a referring expression. It involves a joint understanding of natural language and image content, and is essential for a range of visual tasks related to human-computer interaction. As a language-to-vision matching task, the core of this problem is to not only extract all the necessary information (i.e., objects and the relationships among them) in both the image and referring expression, but also make full use of context information to align cross-modal semantic concepts in the extracted information. Unfortunately, existing work on grounding referring expressions fails to accurately extract multi-order relationships from the referring expression and associate them with the objects and their related contexts in the image. In this paper, we propose a Cross-Modal Relationship Extractor (CMRE) to adaptively highlight objects and relationships (spatial and semantic relations) related to the given expression with a cross-modal attention mechanism, and represent the extracted information as a language-guided visual relation graph. In addition, we propose a Gated Graph Convolutional Network (GGCN) to compute multimodal semantic contexts by fusing information from different modes and propagating multimodal information in the structured relation graph. Experimental results on three common benchmark datasets show that our Cross-Modal Relationship Inference Network, which consists of CMRE and GGCN, significantly surpasses all existing state-of-the-art methods. Code is available at https://github.com/sibeiyang/sgmn/tree/master/lib/cmrin_models",0
"In natural language processing tasks such as grounding referring expressions in images, understanding the relationships between different entities can provide valuable context for making accurate predictions. However, current methods often rely on pre-trained visual representations that may lack sufficient relationship information. To address this issue, we propose a novel approach called relationship-embedded representation learning (RERL) which incorporates relationships between entities into visual representations. Our method leverages large amounts of data from knowledge graphs along with image-text pairs to learn embeddings that capture both visual features and semantic relationships. Experiments demonstrate significant improvements over baseline models on challenging referenceless image question answering benchmarks, showing the effectiveness of our RERL approach. By enhancing representations with relational information, we open up new possibilities for grounded NLP research.",1
"Grounding referring expressions aims to locate in an image an object referred to by a natural language expression. The linguistic structure of a referring expression provides a layout of reasoning over the visual contents, and it is often crucial to align and jointly understand the image and the referring expression. In this paper, we propose a scene graph guided modular network (SGMN), which performs reasoning over a semantic graph and a scene graph with neural modules under the guidance of the linguistic structure of the expression. In particular, we model the image as a structured semantic graph, and parse the expression into a language scene graph. The language scene graph not only decodes the linguistic structure of the expression, but also has a consistent representation with the image semantic graph. In addition to exploring structured solutions to grounding referring expressions, we also propose Ref-Reasoning, a large-scale real-world dataset for structured referring expression reasoning. We automatically generate referring expressions over the scene graphs of images using diverse expression templates and functional programs. This dataset is equipped with real-world visual contents as well as semantically rich expressions with different reasoning layouts. Experimental results show that our SGMN not only significantly outperforms existing state-of-the-art algorithms on the new Ref-Reasoning dataset, but also surpasses state-of-the-art structured methods on commonly used benchmark datasets. It can also provide interpretable visual evidences of reasoning. Data and code are available at https://github.com/sibeiyang/sgmn",0
"This paper focuses on referring expression reasoning in natural language understanding tasks that involve graph structures. We propose a novel approach for solving these problems by combining symbolic and statistical methods in order to capture both the structural and contextual aspects of reference resolution. Our method involves creating a semantic representation using dependency parsing and entity linking to obtain entities, relations, and events from the text. Then, we use logical inference techniques based on first-order logic to infer additional relationships and constraints between entities. Finally, we apply machine learning algorithms to predict the intended referent for each referring expression. We evaluate our model on several benchmark datasets and demonstrate state-of-the-art performance in terms of accuracy and robustness compared to existing approaches. Overall, our work advances the field of NLP by introducing a new framework for solving complex graph-structured referring expression reasoning tasks.",1
"Supervised machine learning (ML) algorithms are aimed at maximizing classification performance under available energy and storage constraints. They try to map the training data to the corresponding labels while ensuring generalizability to unseen data. However, they do not integrate meaning-based relationships among labels in the decision process. On the other hand, natural language processing (NLP) algorithms emphasize the importance of semantic information. In this paper, we synthesize the complementary advantages of supervised ML and NLP algorithms into one method that we refer to as SECRET (Semantically Enhanced Classification of REal-world Tasks). SECRET performs classifications by fusing the semantic information of the labels with the available data: it combines the feature space of the supervised algorithms with the semantic space of the NLP algorithms and predicts labels based on this joint space. Experimental results indicate that, compared to traditional supervised learning, SECRET achieves up to 14.0% accuracy and 13.1% F1 score improvements. Moreover, compared to ensemble methods, SECRET achieves up to 12.7% accuracy and 13.3% F1 score improvements. This points to a new research direction for supervised classification based on incorporation of semantic information.",0
"This paper presents a new methodology for enhancing classification tasks using semantic data processing techniques. By leveraging advanced machine learning algorithms and natural language processing tools, we demonstrate how to effectively integrate semantically rich annotations into standard training datasets to improve performance on downstream classification tasks. Our approach involves first extracting high-quality representations from diverse sources such as text corpora, knowledge graphs, and online lexica. These representations can then be used to generate novel features that capture important relationships and dependencies among concepts, allowing models to learn more complex structures and make better predictions. We evaluate our framework on several real world application domains including image classification, sentiment analysis, and named entity recognition, showing significant improvements over state-of-the-art baselines across all benchmark tests. Additionally, we discuss potential use cases where our method could have practical impact, such as automating content moderation and improving search engine accuracy. Overall, our work highlights the importance of incorporating semantic information into traditional machine learning pipelines to achieve superior results in various applications.",1
"Recently, the bidirectional encoder representations from transformers (BERT) model has attracted much attention in the field of natural language processing, owing to its high performance in language understanding-related tasks. The BERT model learns language representation that can be adapted to various tasks via pre-training using a large corpus in an unsupervised manner. This study proposes the language and action learning using multimodal BERT (lamBERT) model that enables the learning of language and actions by 1) extending the BERT model to multimodal representation and 2) integrating it with reinforcement learning. To verify the proposed model, an experiment is conducted in a grid environment that requires language understanding for the agent to act properly. As a result, the lamBERT model obtained higher rewards in multitask settings and transfer settings when compared to other models, such as the convolutional neural network-based model and the lamBERT model without pre-training.",0
"This paper introduces Lambert, a transformer architecture designed specifically for language tasks involving multimodality and sequential actions. We demonstrate that by fine-tuning Lambert on large scale datasets, the model can achieve state-of-the-art results across multiple domains such as image generation, natural language understanding and question answering, machine translation, semantic segmentation and speech recognition. Furthermore, we showcase how Lambert achieves these performances without any pre-training on massive amounts of data like other deep learning models. This shows great promise for researchers who have limited access to computational resources but still wish to build models capable of tackling challenging real world problems. With the release of our codebase, dataset and detailed documentation, we hope that the community at large can take advantage of these advancements made possible through Lambert. As more applications continue to emerge, we believe that Lambert will become an important component in developing artificial intelligence systems capable of handling complex human interactions.",1
"Benefiting from advances in machine vision and natural language processing techniques, current image captioning systems are able to generate detailed visual descriptions. For the most part, these descriptions represent an objective characterisation of the image, although some models do incorporate subjective aspects related to the observer's view of the image, such as sentiment; current models, however, usually do not consider the emotional content of images during the caption generation process. This paper addresses this issue by proposing novel image captioning models which use facial expression features to generate image captions. The models generate image captions using long short-term memory networks applying facial features in addition to other visual features at different time steps. We compare a comprehensive collection of image captioning models with and without facial features using all standard evaluation metrics. The evaluation metrics indicate that applying facial features with an attention mechanism achieves the best performance, showing more expressive and more correlated image captions, on an image caption dataset extracted from the standard Flickr 30K dataset, consisting of around 11K images containing faces. An analysis of the generated captions finds that, perhaps unexpectedly, the improvement in caption quality appears to come not from the addition of adjectives linked to emotional aspects of the images, but from more variety in the actions described in the captions.",0
"Image captioning has become increasingly important as computers process more image data than ever before. Existing approaches rely on object detection models such as Faster R-CNN, which identify objects within images but provide little context beyond that. Recent work has attempted to improve upon these methods by integrating knowledge from external datasets into the caption generation model to improve output coherence and accuracy. However, few attempts have been made to incorporate facial expression analysis into the image description process. We propose a novel approach based on deep attention mechanism that combines facial expression recognition and object detection techniques to generate natural language descriptions of images containing faces. Our experiments demonstrate that the integration of facial expressions improves the performance of existing captioning systems, providing richer and more detailed descriptions that capture both the visual content and emotional state of subjects in the image. By harnessing the power of attention mechanisms and fine-grained feature learning, our method achieves superior results compared to current state-of-the-art captioning techniques, setting a new benchmark in image captioning research.",1
"The goal of the YouMakeup VQA Challenge 2020 is to provide a common benchmark for fine-grained action understanding in domain-specific videos e.g. makeup instructional videos. We propose two novel question-answering tasks to evaluate models' fine-grained action understanding abilities. The first task is \textbf{Facial Image Ordering}, which aims to understand visual effects of different actions expressed in natural language to the facial object. The second task is \textbf{Step Ordering}, which aims to measure cross-modal semantic alignments between untrimmed videos and multi-sentence texts. In this paper, we present the challenge guidelines, the dataset used, and performances of baseline models on the two proposed tasks. The baseline codes and models are released at \url{https://github.com/AIM3-RUC/YouMakeup_Baseline}.",0
"This paper presents the ""YouMakeup"" video question answering (VQA) challenge, which focuses on fine-grained action understanding in domain-specific videos. The goal of the challenge is to encourage researchers to develop models that can effectively extract information from videos related to makeup tutorials, such as identifying specific actions performed by the individuals in the videos. To achieve this, the authors introduce a dataset consisting of over 700 YouTube videos along with annotations of frames and questions related to different aspects of makeup application. They then evaluate several state-of-the-art VQA algorithms using their newly created dataset and show promising results towards achieving fine-grained action understanding in these types of videos. Additionally, they provide insights into how these methods could potentially be improved further through additional training data or new model architectures. Overall, this work contributes to advancing our understanding of how machines can effectively process visual information from complex, real-world domains like beauty tutorials.",1
"Image classification has been studied extensively but there has been limited work in the direction of using non-conventional, external guidance other than traditional image-label pairs to train such models. In this thesis we present a set of methods to leverage information about the semantic hierarchy induced by class labels. In the first part of the thesis, we inject label-hierarchy knowledge to an arbitrary classifier and empirically show that availability of such external semantic information in conjunction with the visual semantics from images boosts overall performance. Taking a step further in this direction, we model more explicitly the label-label and label-image interactions by using order-preserving embedding-based models, prevalent in natural language, and tailor them to the domain of computer vision to perform image classification. Although, contrasting in nature, both the CNN-classifiers injected with hierarchical information, and the embedding-based models outperform a hierarchy-agnostic model on the newly presented, real-world ETH Entomological Collection image dataset https://www.research-collection.ethz.ch/handle/20.500.11850/365379.",0
"This paper introduces a deep neural network which is designed to learn representations that capture hierarchical relationships in images. In contrast to previous work, we propose that a single image can contain multiple high level concepts which can each correspond to different objects, attributes or even actions happening simultaneously. Our framework incorporates these multi-level labels into training and demonstrates significant improvements on standard benchmark datasets. We hope that our method will open up new possibilities for computer vision research by enabling models to better understand complex scenes and relationships within them.",1
"Recent developments in image classification and natural language processing, coupled with the rapid growth in social media usage, have enabled fundamental advances in detecting breaking events around the world in real-time. Emergency response is one such area that stands to gain from these advances. By processing billions of texts and images a minute, events can be automatically detected to enable emergency response workers to better assess rapidly evolving situations and deploy resources accordingly. To date, most event detection techniques in this area have focused on image-only or text-only approaches, limiting detection performance and impacting the quality of information delivered to crisis response teams. In this paper, we present a new multimodal fusion method that leverages both images and texts as input. In particular, we introduce a cross-attention module that can filter uninformative and misleading components from weak modalities on a sample by sample basis. In addition, we employ a multimodal graph-based approach to stochastically transition between embeddings of different multimodal pairs during training to better regularize the learning process as well as dealing with limited training data by constructing new matched pairs from different samples. We show that our method outperforms the unimodal approaches and strong multimodal baselines by a large margin on three crisis-related tasks.",0
"This multimodal categorization method can identify more types of events than previous work by considering both text and visual content, using models that are fine tuned on social media data, and utilizing pretraining object detection to provide additional context. Our approach achieves competitive results across multiple tasks including event classification and named entity recognition, outperforming prior state-of-the-art methods which rely solely on text or only use basic visual features. We further demonstrate how our proposed model can effectively detect crisis events without any human intervention, making it highly applicable to real world situations where timely identification of emergencies is crucial. Finally, we discuss future directions for improvement and applications of the proposed framework in other domains besides crisis events.",1
"This paper presents a framework for the analysis of changes in visual streams: ordered sequences of images, possibly separated by significant time gaps. We propose a new approach to incorporating unlabeled data into training to generate natural language descriptions of change. We also develop a framework for estimating the time of change in visual stream. We use learned representations for change evidence and consistency of perceived change, and combine these in a regularized graph cut based change detector. Experimental evaluation on visual stream datasets, which we release as part of our contribution, shows that representation learning driven by natural language descriptions significantly improves change detection accuracy, compared to methods that do not rely on language.",0
"In this paper we present a framework for detecting change in visual streams using machine learning techniques. Our approach uses deep neural networks to learn representations that capture changes in appearance over time. We evaluate our method on several benchmark datasets and demonstrate its effectiveness compared to other state-of-the-art methods. Our work has applications in video surveillance, object tracking, and anomaly detection in streams of images and videos. By describing and understanding how changes occur in visual streams, we can gain insights into complex phenomena such as human behavior, environmental events, and technical processes. This research contributes to the broader field of computer vision by providing new tools for analyzing streaming data from cameras and sensors. Overall, our results showcase the potential of machine learning algorithms to detect and describe changes in high-dimensional image sequences.",1
"We address the problem of video grounding from natural language queries. The key challenge in this task is that one training video might only contain a few annotated starting/ending frames that can be used as positive examples for model training. Most conventional approaches directly train a binary classifier using such imbalance data, thus achieving inferior results. The key idea of this paper is to use the distances between the frame within the ground truth and the starting (ending) frame as dense supervisions to improve the video grounding accuracy. Specifically, we design a novel dense regression network (DRN) to regress the distances from each frame to the starting (ending) frame of the video segment described by the query. We also propose a simple but effective IoU regression head module to explicitly consider the localization quality of the grounding results (i.e., the IoU between the predicted location and the ground truth). Experimental results show that our approach significantly outperforms state-of-the-arts on three datasets (i.e., Charades-STA, ActivityNet-Captions, and TACoS).",0
"This paper presents a novel approach to video grounding using dense regression networks (DRNs). We introduce an end-to-end trainable model that uses DRNs to generate spatially accurate bounding boxes for objects detected in videos. Our method employs an attention mechanism to weigh different features of the input frames based on their relevance to the target object, allowing for efficient computation during inference. To overcome the difficulties in training such deep models due to imbalanced labels and class distributions, we propose a multi-task learning framework which utilizes auxiliary losses to guide the network towards better localization performance. Extensive experiments conducted on two benchmark datasets demonstrate significant improvements over existing methods in terms of both accuracy and speed. Furthermore, our approach provides interpretability by generating explicit predictions for all locations in the image plane, rather than relying solely on post hoc saliency maps as done by most previous works. Overall, we believe that our work represents a significant step forward in advancing the state of the art in video grounding.",1
"In this paper we present an approach and a benchmark for visual reasoning in robotics applications, in particular small object grasping and manipulation. The approach and benchmark are focused on inferring object properties from visual and text data. It concerns small household objects with their properties, functionality, natural language descriptions as well as question-answer pairs for visual reasoning queries along with their corresponding scene semantic representations. We also present a method for generating synthetic data which allows to extend the benchmark to other objects or scenes and propose an evaluation protocol that is more challenging than in the existing datasets. We propose a reasoning system based on symbolic program execution. A disentangled representation of the visual and textual inputs is obtained and used to execute symbolic programs that represent a 'reasoning process' of the algorithm. We perform a set of experiments on the proposed benchmark and compare to results for the state of the art methods. These results expose the shortcomings of the existing benchmarks that may lead to misleading conclusions on the actual performance of the visual reasoning systems.",0
"In the field of computer vision, object perception involves identifying objects within an image and understanding their properties such as shape, color, and texture. Recent advancements in deep learning have led to significant improvements in object detection accuracy; however, these approaches still lack accurate reasoning capabilities for more complex tasks such as understanding object relationships and contextual dependencies. Therefore, there exists a need for developing new benchmarks that can evaluate the performance of visual reasoning models on complex object recognition scenarios. This work presents a new benchmark called ""SHOP-VRB"" which stands for Semantic Hierarchical Object Parser based on Visual Relationship Benchmarking. The proposed benchmark consists of a large dataset containing over 67k annotated images and challenges related to object detection, segmentation, parsing and relationship modeling. Our evaluation shows that state-of-the-art methods fail to perform well on many of the tasks presented by our benchmark demonstrating the value of SHOP-VRB in driving future research in the direction of improving object representation and reasoning abilities in Vision and CV systems.",1
"Generative adversarial networks (GANs) are increasingly attracting attention in the computer vision, natural language processing, speech synthesis and similar domains. However, evaluating the performance of GANs is still an open and challenging problem. Existing evaluation metrics primarily measure the dissimilarity between real and generated images using automated statistical methods. They often require large sample sizes for evaluation and do not directly reflect human perception of image quality. In this work, we introduce an evaluation metric called Neuroscore, for evaluating the performance of GANs, that more directly reflects psychoperceptual image quality through the utilization of brain signals. Our results show that Neuroscore has superior performance to the current evaluation metrics in that: (1) It is more consistent with human judgment; (2) The evaluation process needs much smaller numbers of samples; and (3) It is able to rank the quality of images on a per GAN basis. A convolutional neural network (CNN) based neuro-AI interface is proposed to predict Neuroscore from GAN-generated images directly without the need for neural responses. Importantly, we show that including neural responses during the training phase of the network can significantly improve the prediction capability of the proposed model. Codes and data can be referred at this link: https://github.com/villawang/Neuro-AI-Interface.",0
"""This work presents a neuro-inspired interface that enables direct evaluation and control of generative adversarial networks (GANs). By leveraging human brain signals and machine learning techniques, we develop an interactive system that provides real-time feedback on GAN performance while enabling users to manipulate key parameters for improved results. Our approach significantly enhances user experience by allowing for intuitive exploration of complex image generation pipelines without requiring extensive technical expertise. We demonstrate effectiveness through experiments showing improvement over state-of-the-art baseline systems.""",1
"Learning to navigate in a visual environment following natural-language instructions is a challenging task, because the multimodal inputs to the agent are highly variable, and the training data on a new task is often limited. In this paper, we present the first pre-training and fine-tuning paradigm for vision-and-language navigation (VLN) tasks. By training on a large amount of image-text-action triplets in a self-supervised learning manner, the pre-trained model provides generic representations of visual environments and language instructions. It can be easily used as a drop-in for existing VLN frameworks, leading to the proposed agent called Prevalent. It learns more effectively in new tasks and generalizes better in a previously unseen environment. The performance is validated on three VLN tasks. On the Room-to-Room benchmark, our model improves the state-of-the-art from 47% to 51% on success rate weighted by path length. Further, the learned representation is transferable to other VLN tasks. On two recent tasks, vision-and-dialog navigation and ""Help, Anna!"" the proposed Prevalent leads to significant improvement over existing methods, achieving a new state of the art.",0
"In recent years, deep learning has achieved remarkable results in computer vision tasks such as object recognition, image segmentation, and caption generation. However, these models typically require task specific architectures that are tuned on large amounts of data. Therefore, they can’t easily generalize to new visual domains where no labeled training examples exist. In contrast, humans have the ability to navigate environments using only natural language instructions and a single view of each scene. To achieve this kind of robustness in artificial agents, we need to design models that can learn from both visual input and textual supervision. This paper presents a novel approach called VisNav pre-training which leverages unsupervised representations learned by predicting random patches of images paired with randomly selected captions. We demonstrate the effectiveness of our method on two challenging navigation benchmarks: COCO-DRIVING \& CARLA and show how transferring learned knowledge can significantly improve performance compared to strong baselines trained from scratch. Additionally, we conduct ablation studies to analyze the impact of different components of our method on the final performance. Our work sets the stage for future research in building generic agents capable of navigating diverse real-world environments guided solely by human language commands.",1
"In many scenarios, humans prefer a text-based representation of quantitative data over numerical, tabular, or graphical representations. The attractiveness of textual summaries for complex data has inspired research on data-to-text systems. While there are several data-to-text tools for time series, few of them try to mimic how humans summarize for time series. In this paper, we propose a model to create human-like text descriptions for time series. Our system finds patterns in time series data and ranks these patterns based on empirical observations of human behavior using utility estimation. Our proposed utility estimation model is a Bayesian network capturing interdependencies between different patterns. We describe the learning steps for this network and introduce baselines along with their performance for each step. The output of our system is a natural language description of time series that attempts to match a human's summary of the same data.",0
"Title: ""Human-like Time Series Summaries via Trend Utility Estimation""Authors: [Your Name], [Coauthor(s) Name]Publication: SubmittedDate: Abstract:Time series summarization refers to generating concise yet informative descriptions of time series data, which can aid humans in understanding complex patterns and trends quickly. Inspired by human cognition mechanisms, we present Trend Utility Estimation (TUE), an approach that synthesizes two key components for time series summarization: temporal trending analysis and importance evaluation. Temporal trending analysis focuses on modeling how time series evolve over time and identifying key events that impact its behavior. Importance evaluation ranks the significance of different parts relative to each other and overall within context. Our method uses a neural network architecture called TimeTrender to jointly learn both aspects from raw data without requiring any additional supervision or domain knowledge. Extensive experiments across multiple datasets demonstrate TUE outperforms baseline methods significantly and generates more meaningful and interpretable summaries with respect to popular metrics and expert evaluations alike. Overall, our work presents an exciting step towards creating AI systems capable of producing summaries that better align with human expectations.Future directions involve incorporating external features like textual descriptions and exploring applications in diverse domains such as finance, healthcare, and environmental monitoring. We believe this research has strong potential to enhance decision making, facilitate information retrieval tasks, and enable users to gain insights into their data effectively and efficiently.Keywords: Time series summarization, Trend utility estimation, Neural networks, Unsupervised learning",1
"A sum-product network (SPN) is a probabilistic model, based on a rooted acyclic directed graph, in which terminal nodes represent univariate probability distributions and non-terminal nodes represent convex combinations (weighted sums) and products of probability functions. They are closely related to probabilistic graphical models, in particular to Bayesian networks with multiple context-specific independencies. Their main advantage is the possibility of building tractable models from data, i.e., models that can perform several inference tasks in time proportional to the number of links in the graph. They are somewhat similar to neural networks and can address the same kinds of problems, such as image processing and natural language understanding. This paper offers a survey of SPNs, including their definition, the main algorithms for inference and learning from data, the main applications, a brief review of software libraries, and a comparison with related models",0
"Abstract This paper surveys sum-product networks. Sum-product networks are powerful machine learning models that have been used successfully on many applications. They can learn complex functions from data and make accurate predictions. In this paper, we give an overview of the theory behind sum-product networks as well as their connections to other types of neural network architectures such as belief propagation nets. We discuss how sum-product networks are trained using gradient descent methods and illustrate their capabilities through experiments on several benchmark datasets.",1
"Overparameterized neural networks can be highly accurate on average on an i.i.d. test set yet consistently fail on atypical groups of the data (e.g., by learning spurious correlations that hold on average but not in such groups). Distributionally robust optimization (DRO) allows us to learn models that instead minimize the worst-case training loss over a set of pre-defined groups. However, we find that naively applying group DRO to overparameterized neural networks fails: these models can perfectly fit the training data, and any model with vanishing average training loss also already has vanishing worst-case training loss. Instead, the poor worst-case performance arises from poor generalization on some groups. By coupling group DRO models with increased regularization---a stronger-than-typical L2 penalty or early stopping---we achieve substantially higher worst-group accuracies, with 10-40 percentage point improvements on a natural language inference task and two image tasks, while maintaining high average accuracies. Our results suggest that regularization is important for worst-group generalization in the overparameterized regime, even if it is not needed for average generalization. Finally, we introduce a stochastic optimization algorithm, with convergence guarantees, to efficiently train group DRO models.",0
"Distributional robustness has emerged as a key concept in deep learning research due to its ability to improve the generalization performance of neural networks under distribution shifts that occur during deployment. In this paper, we focus on group shifts which refer to changes in the marginal distributions of different groups in the training data. We propose a novel regularization technique called distributionally robust neural networks (DRNN) to address the problem of worst-case generalization in the presence of group shifts. Our approach involves adding a divergence term based on the Cauchy loss function to the standard cross entropy loss used in logistic regression. This regularizes the model by encouraging a uniform decrease in error across all groups, thus reducing sensitivity to any particular group shift. Experimental results show that DRNN achieves significantly better generalization accuracy than state-of-the-art models on several benchmark datasets while maintaining competitive performance on clean test sets. These findings highlight the importance of considering both individual and group robustness in designing robust machine learning systems.",1
"Convolutional Neural Network (CNN) is one of the most significant networks in the deep learning field. Since CNN made impressive achievements in many areas, including but not limited to computer vision and natural language processing, it attracted much attention both of industry and academia in the past few years. The existing reviews mainly focus on the applications of CNN in different scenarios without considering CNN from a general perspective, and some novel ideas proposed recently are not covered. In this review, we aim to provide novel ideas and prospects in this fast-growing field as much as possible. Besides, not only two-dimensional convolution but also one-dimensional and multi-dimensional ones are involved. First, this review starts with a brief introduction to the history of CNN. Second, we provide an overview of CNN. Third, classic and advanced CNN models are introduced, especially those key points making them reach state-of-the-art results. Fourth, through experimental analysis, we draw some conclusions and provide several rules of thumb for function selection. Fifth, the applications of one-dimensional, two-dimensional, and multi-dimensional convolution are covered. Finally, some open issues and promising directions for CNN are discussed to serve as guidelines for future work.",0
"In recent years, convolutional neural networks (CNNs) have achieved remarkable successes in numerous domains such as image recognition, object detection, natural language processing and more. They are now widely used across multiple industries thanks to their excellent performance on complex problems that were previously hard to solve. This survey provides an overview of CNN architecture analysis, applications and prospects from both theoretical researchers and practitioners’ perspectives, aiming at providing insights into some fundamental questions surrounding these systems. With a focus on key results, the main body of existing work and open problems under discussion, we hope to inspire further investigation into one or more topics in order to provide new contributions to the field. Lastly, future directions where we can see CNN technologies having impact are discussed. The paper highlights the most prominent issues and trends shaping the study of CNN technology and explores the ways they influence our understanding of deep learning models.",1
"Vision-Language Navigation (VLN) is a task where agents learn to navigate following natural language instructions. The key to this task is to perceive both the visual scene and natural language sequentially. Conventional approaches exploit the vision and language features in cross-modal grounding. However, the VLN task remains challenging, since previous works have neglected the rich semantic information contained in the environment (such as implicit navigation graphs or sub-trajectory semantics). In this paper, we introduce Auxiliary Reasoning Navigation (AuxRN), a framework with four self-supervised auxiliary reasoning tasks to take advantage of the additional training signals derived from the semantic information. The auxiliary tasks have four reasoning objectives: explaining the previous actions, estimating the navigation progress, predicting the next orientation, and evaluating the trajectory consistency. As a result, these additional training signals help the agent to acquire knowledge of semantic representations in order to reason about its activity and build a thorough perception of the environment. Our experiments indicate that auxiliary reasoning tasks improve both the performance of the main task and the model generalizability by a large margin. Empirically, we demonstrate that an agent trained with self-supervised auxiliary reasoning tasks substantially outperforms the previous state-of-the-art method, being the best existing approach on the standard benchmark.",0
"Abstract: This paper presents a new approach to vision-language navigation using self-supervised auxiliary reasoning tasks. The proposed method combines visual representations learned from large amounts of data with task-specific reasoning skills to enable agents to effectively navigate complex environments by following natural language instructions. We demonstrate the effectiveness of our approach through extensive experiments on several challenging benchmark datasets, showing that it significantly outperforms existing state-of-the-art methods. Our findings have important implications for the development of intelligent systems capable of understanding and acting upon both visual and linguistic information.",1
"Source code summarizing is a task of writing short, natural language descriptions of source code behavior during run time. Such summaries are extremely useful for software development and maintenance but are expensive to manually author,hence it is done for small fraction of the code that is produced and is often ignored. Automatic code documentation can possibly solve this at a low cost. This is thus an emerging research field with further applications to program comprehension, and software maintenance. Traditional methods often relied on cognitive models that were built in the form of templates and by heuristics and had varying degree of adoption by the developer community. But with recent advancements, end to end data-driven approaches based on neural techniques have largely overtaken the traditional techniques. Much of the current landscape employs neural translation based architectures with recurrence and attention which is resource and time intensive training procedure. In this paper, we employ neural techniques to solve the task of source code summarizing and specifically compare NMT based techniques to more simplified and appealing Transformer architecture on a dataset of Java methods and comments. We bring forth an argument to dispense the need of recurrence in the training procedure. To the best of our knowledge, transformer based models have not been used for the task before. With supervised samples of more than 2.1m comments and code, we reduce the training time by more than 50% and achieve the BLEU score of 17.99 for the test set of examples.",0
"Automatically summarizing source code has been a long standing challenge in software engineering research. Recent work on deep learning approaches have used neural architectures such as sequence models (e.g., LSTMs) and convolutional models that operate directly on tokenized sequences to produce summaries. We present a new approach that utilizes attention mechanisms from transformer networks to capture relationships between tokens across different lines of code. We call our method ""DeepCode"" which stands for deep summarization of code files by attending to key words found in natural language summarizations written by human experts. We collected a dataset containing bug reports and associated java code files and corresponding summaries. Our results show that both the use of self attention and pretraining are essential components of successful summary generation and that our model can effectively summarize the relevant portions of large Java programs where the original documentation fails to provide insight. Furthermore we demonstrate how our approach can effectively generate summaries of code that is incorrect or incomplete. While correctness of synthesized programs remains open problem, generating informative summaries is often valuable in and of itself for understanding potential bugs and issues programmers face day to day. Finally we evaluate our approach against other state of art summarization techniques which are only capable of producing short descriptions lacking important details required for program comprehension. We believe that by leveraging recent advancements in the field of natural language processing, we are one step closer towards providing developers the tools they need to better understand and maintain complex software systems at scale.",1
"In the Vision-and-Language Navigation (VLN) task, an agent with egocentric vision navigates to a destination given natural language instructions. The act of manually annotating these instructions is timely and expensive, such that many existing approaches automatically generate additional samples to improve agent performance. However, these approaches still have difficulty generalizing their performance to new environments. In this work, we investigate the popular Room-to-Room (R2R) VLN benchmark and discover that what is important is not only the amount of data you synthesize, but also how you do it. We find that shortest path sampling, which is used by both the R2R benchmark and existing augmentation methods, encode biases in the action space of the agent which we dub as action priors. We then show that these action priors offer one explanation toward the poor generalization of existing works. To mitigate such priors, we propose a path sampling method based on random walks to augment the data. By training with this augmentation strategy, our agent is able to generalize better to unknown environments compared to the baseline, significantly improving model performance in the process.",0
"This paper addresses generalization errors made by vision-and-language navigation agents when given novel natural language instructions that differ from those seen during training. We propose a new method called Take the Scenic Route (TSR) which improves the agent’s ability to handle these unseen instruction types at test time by explicitly incorporating knowledge of high level spatial relationships into learning objectives. Evaluations on multiple datasets demonstrate significant improvements over strong baselines across all metrics. Our approach has wide applicability as it can work within existing models without modifying their internal architectures making it easy to integrate into production systems. Furthermore, our TSR module enables robust zero shot transfer performance while significantly reducing computation costs over standard fine tuning approaches. Lastly, we provide analysis showing how TSR modules implicitly learn complex spatio-temporal reasoning skills through interaction with raw image inputs making them interpretable.",1
"We present ALFRED (Action Learning From Realistic Environments and Directives), a benchmark for learning a mapping from natural language instructions and egocentric vision to sequences of actions for household tasks. ALFRED includes long, compositional tasks with non-reversible state changes to shrink the gap between research benchmarks and real-world applications. ALFRED consists of expert demonstrations in interactive visual environments for 25k natural language directives. These directives contain both high-level goals like ""Rinse off a mug and place it in the coffee maker."" and low-level language instructions like ""Walk to the coffee maker on the right."" ALFRED tasks are more complex in terms of sequence length, action space, and language than existing vision-and-language task datasets. We show that a baseline model based on recent embodied vision-and-language tasks performs poorly on ALFRED, suggesting that there is significant room for developing innovative grounded visual language understanding models with this benchmark.",0
"In recent years, there has been significant progress in developing task-oriented conversational agents that can interpret natural language instructions and perform actions accordingly. These systems rely on large pre-trained models that require massive amounts of data to achieve high accuracy. However, these models often struggle to generalize well across different domains and tasks. Therefore, there remains a need for benchmark datasets that evaluate how effectively these models can handle grounded instruction interpretation for everyday tasks. This paper introduces ALFRED, a new benchmark dataset containing over 14k annotated instructions for performing tasks such as setting alarms, composing messages, making phone calls, etc. We propose several evaluation metrics designed to measure different aspects of a model’s performance in interpreting grounded instructions, including overall correctness, partial credit, informativeness, fluency, variety, conciseness, specificity, etc. Our results show that current state-of-the-art models have room for improvement in terms of interpreting grounded instructions for everyday tasks. Overall, our work provides researchers with valuable insights into improving grounded instruction interpretation by conversational agents.",1
"The underlying structure of natural language is hierarchical; words combine into phrases, which in turn form clauses. An awareness of this hierarchical structure can aid machine learning models in performing many linguistic tasks. However, most such models just process text sequentially and there is no bias towards learning hierarchical structure encoded into their architecture. In this paper, we extend the recent transformer model (Vaswani et al., 2017) by enabling it to learn hierarchical representations. To achieve this, we adapt the ordering mechanism introduced in Shen et al., 2018, to the self-attention module of the transformer architecture. We train our new model on language modelling and then apply it to the task of unsupervised parsing. We achieve reasonable results on the freely available subset of the WSJ10 dataset with an F1-score of about 50%.",0
"This paper presents a novel approach to unsupervised parsing using a hierarchical transformer model. Our method leverages the strengths of both recursive neural networks (RNN) and attention mechanisms by introducing a two-level hierarchy within the transformer architecture. At the first level, we use multi-head self-attention to identify relationships between individual tokens, while at the second level, we use RNN layers to capture longer-range dependencies between nested phrases. We show that our approach outperforms previous state-of-the-art methods on several benchmark datasets across different languages and domains. Furthermore, we demonstrate the effectiveness of our model in generating human-like parse trees in zero-shot settings, without any supervision. Overall, this work represents an important step towards achieving fully unsupervised natural language understanding.",1
"Artificial Neural Networks (ANNs) replaced conventional software systems in various domains such as machine translation, natural language processing, and image processing. So, why do we need an repository for artificial neural networks? Those systems are developed with labeled data and we have strong dependencies between the data that is used for training and testing our network. Another challenge is the data quality as well as reuse-ability. There we are trying to apply concepts from classic software engineering that is not limited to the model, while data and code haven't been dealt with mostly in other projects. The first question that comes to mind might be, why don't we use GitHub, a well known widely spread tool for reuse, for our issue. And the reason why is that GitHub, although very good in its class is not developed for machine learning appliances and focuses more on software reuse. In addition to that GitHub does not allow to execute the code directly on the platform which would be very convenient for collaborative work on one project.",0
"In recent years, there has been growing interest in artificial neural networks (ANN) due to their ability to perform complex tasks such as image classification, speech recognition, and natural language processing. However, one major challenge facing researchers who work on these systems is that they often have to start from scratch every time they want to build a new model, which can be extremely time consuming and resource intensive. To address this issue, we propose a repository for reusing artifacts of artificial neural networks. This repository would allow researchers to share and reuse artifacts related to ANN training such as data preprocessing pipelines, hyperparameter tuning scripts, initialization strategies, and more. By doing so, developers could significantly reduce the amount of time spent developing new models, leading to faster innovation in this rapidly evolving field. Additionally, the sharing of artifacts within the community would foster greater collaboration among researchers and improve reproducibility of results. Our proposed repository is designed with extensibility in mind, allowing future contributions by other members of the ANN development community, thereby enabling broad adoption and continuous growth over time. Ultimately, our goal is to create a centralized hub for ANN artifacts, where developers can quickly find high quality materials to use in their own projects, paving the way towards even more advanced machine learning models.",1
"The goal of weakly-supervised video moment retrieval is to localize the video segment most relevant to the given natural language query without access to temporal annotations during training. Prior strongly- and weakly-supervised approaches often leverage co-attention mechanisms to learn visual-semantic representations for localization. However, while such approaches tend to focus on identifying relationships between elements of the video and language modalities, there is less emphasis on modeling relational context between video frames given the semantic context of the query. Consequently, the above-mentioned visual-semantic representations, built upon local frame features, do not contain much contextual information. To address this limitation, we propose a Latent Graph Co-Attention Network (LoGAN) that exploits fine-grained frame-by-word interactions to reason about correspondences between all possible pairs of frames, given the semantic context of the query. Comprehensive experiments across two datasets, DiDeMo and Charades-Sta, demonstrate the effectiveness of our proposed latent co-attention model where it outperforms current state-of-the-art (SOTA) weakly-supervised approaches by a significant margin. Notably, it even achieves a 11% improvement to Recall@1 accuracy over strongly-supervised SOTA methods on DiDeMo.",0
"Abstract: This paper proposes a novel method called LoGAN (Latent Graph Co-Attention Network) for weakly-supervised video moment retrieval tasks. The method leverages graph convolutional networks (GCNs) combined with co-attention mechanisms to effectively capture both spatial and temporal relationships within video frames. Additionally, a latent variable model is introduced that captures global contextual dependencies among multiple GCN blocks, providing a more robust representation of complex videos. The proposed framework achieves state-of-the-art performance on two challenging benchmark datasets, demonstrating its effectiveness in addressing weakly supervised video moment retrieval problems.",1
"Recent research advances in Computer Vision and Natural Language Processing have introduced novel tasks that are paving the way for solving AI-complete problems. One of those tasks is called Visual Question Answering (VQA). A VQA system must take an image and a free-form, open-ended natural language question about the image, and produce a natural language answer as the output. Such a task has drawn great attention from the scientific community, which generated a plethora of approaches that aim to improve the VQA predictive accuracy. Most of them comprise three major components: (i) independent representation learning of images and questions; (ii) feature fusion so the model can use information from both sources to answer visual questions; and (iii) the generation of the correct answer in natural language. With so many approaches being recently introduced, it became unclear the real contribution of each component for the ultimate performance of the model. The main goal of this paper is to provide a comprehensive analysis regarding the impact of each component in VQA models. Our extensive set of experiments cover both visual and textual elements, as well as the combination of these representations in form of fusion and attention mechanisms. Our major contribution is to identify core components for training VQA models so as to maximize their predictive performance.",0
"This research paper presents a comprehensive analysis of visual question answering (VQA) architectures, focusing on their components and how they contribute to the overall performance of VQA models. In particular, we explore three key aspects: attention mechanisms, fusion techniques, and external knowledge sources. We first provide an overview of recent developments in VQA, highlighting common approaches and trends. Then, we delve into each component area and discuss representative methods, evaluating their strengths and limitations through detailed experiments. Our findings show that different combinations of these components can lead to significant improvements or degradations in VQA accuracy. Furthermore, we identify areas where future work could focus on making progress toward solving challenges unique to VQA. Overall, our results offer valuable insights for researchers looking to build better VQAs and advance the field as a whole.",1
"We introduce a new task, Video-and-Language Inference, for joint multimodal understanding of video and text. Given a video clip with aligned subtitles as premise, paired with a natural language hypothesis based on the video content, a model needs to infer whether the hypothesis is entailed or contradicted by the given video clip. A new large-scale dataset, named Violin (VIdeO-and-Language INference), is introduced for this task, which consists of 95,322 video-hypothesis pairs from 15,887 video clips, spanning over 582 hours of video. These video clips contain rich content with diverse temporal dynamics, event shifts, and people interactions, collected from two sources: (i) popular TV shows, and (ii) movie clips from YouTube channels. In order to address our new multimodal inference task, a model is required to possess sophisticated reasoning skills, from surface-level grounding (e.g., identifying objects and characters in the video) to in-depth commonsense reasoning (e.g., inferring causal relations of events in the video). We present a detailed analysis of the dataset and an extensive evaluation over many strong baselines, providing valuable insights on the challenges of this new task.",0
"Abstract: Natural language processing has seen significant advancements over the past decade through the development of large-scale datasets such as ImageNet, COCO, and OpenImages. However, there still exists a gap in understanding how humans interpret visual content and integrate that knowledge into natural language descriptions. This work addresses this gap by introducing VIOLIN (Visual Interactions And Object Localization for Images Named), a benchmark dataset consisting of image-caption pairs generated through crowdsourced human interactions.  The unique contribution of VIOLIN lies in its focus on capturing fine-grained visual details using explicit object localization and interaction instructions from human annotators. Each instance in the dataset contains four components - an input video frame, multiple reference captions describing different aspects of the scene, bounding boxes for objects mentioned in the captions, and a set of instructions specifying where and how the objects interact within the frame. By designing tasks based on these annotations, we aim to encourage research in developing models capable of accurately inferring relationships among actors, actions, and objects within videos at scale.  We demonstrate the utility of our dataset by training and evaluating state-of-the-art vision-language architectures on several benchmark tasks, including Referring Expression Segmentation, Instance Segmentation, Visual Grounding, V2C (Video to Caption) Generation, C2V (Caption to Video) Retrieval, and Joint Segmentation and Ranking. These results show consistent improvements compared to previous datasets across all tasks, thereby establishing VIOLIN's effectiveness as a comprehensive resource for advancing computer vision and natural language processing research. Overall, VIOLIN represents an important step towards bridging the gap between vision and language, enabling the creation of systems that can truly understand and generate complex visual narratives.",1
"We explore the task of Video Object Grounding (VOG), which grounds objects in videos referred to in natural language descriptions. Previous methods apply image grounding based algorithms to address VOG, fail to explore the object relation information and suffer from limited generalization. Here, we investigate the role of object relations in VOG and propose a novel framework VOGNet to encode multi-modal object relations via self-attention with relative position encoding. To evaluate VOGNet, we propose novel contrasting sampling methods to generate more challenging grounding input samples, and construct a new dataset called ActivityNet-SRL (ASRL) based on existing caption and grounding datasets. Experiments on ASRL validate the need of encoding object relations in VOG, and our VOGNet outperforms competitive baselines by a significant margin.",0
"In recent years, there has been growing interest in developing methods for grounding natural language descriptions to visual content. One approach that has gained popularity is video object grounding, which involves identifying objects mentioned in natural language queries within a video sequence. This task presents several challenges due to the complexity of real-world scenes, variations in viewpoint, occlusions, and cluttered backgrounds. To address these issues, we present a novel method based on semantic roles, which provide structured representations of actions and their participants. Our framework first extracts semantic roles from the query sentence using a pretrained NLP model, then matches them against the spatio-temporal intervals predicted by a convolutional neural network detector trained on static images. We evaluate our proposed method on a benchmark dataset and show significant improvements over state-of-the-art approaches. Additionally, we demonstrate the effectiveness of our approach in downstream applications such as video question answering. Overall, our results highlight the potential of leveraging linguistic knowledge encoded in semantic roles for robust video object grounding.",1
"In this paper, our focus is the connection and influence of language technologies on the research in neurolinguistics. We present a review of brain imaging-based neurolinguistic studies with a focus on the natural language representations, such as word embeddings and pre-trained language models. Mutual enrichment of neurolinguistics and language technologies leads to development of brain-aware natural language representations. The importance of this research area is emphasized by medical applications.",0
"This paper describes the development of data-driven models that can accurately predict how changes in brain structure are related to cognitive outcomes (like perception, attention, memory) and vice versa. We then use these predictions as training signals for our language processing neural network. Finally, we evaluate each component on a benchmark dataset. Results indicate promising performance on both datasets suggesting generalizability across different populations and tasks. By using real-world neuroimaging data rather than artificially constructed stimuli used in previous studies, we were able to demonstrate improved cross domain transfer. Our methodology suggests several future directions including evaluation on clinical cohorts. These results provide support for the idea that language acquisition requires continuous interaction between linguistic input from outside the body (either external or imagined speech scenarios), and internal representations stored in working memory which are updated based on feedback. In conclusion, the present study provides new insights into the underlying mechanisms of language development by integrating behavioral, physiological, and neurological indices of brain function. In summary, this work advances understanding of language learning through the integration of multiple sources of data. By developing accurate predictive models informed by cutting-edge neuroscience research, we established a foundation for building and evaluating powerful language processing systems. Ultimately, our findings highlight the importance of considering complex interactions among various domains in order to achieve comprehensive knowledge of human cognition and communication processes.",1
"Technology and the fruition of cultural heritage are becoming increasingly more entwined, especially with the advent of smart audio guides, virtual and augmented reality, and interactive installations. Machine learning and computer vision are important components of this ongoing integration, enabling new interaction modalities between user and museum. Nonetheless, the most frequent way of interacting with paintings and statues still remains taking pictures. Yet images alone can only convey the aesthetics of the artwork, lacking is information which is often required to fully understand and appreciate it. Usually this additional knowledge comes both from the artwork itself (and therefore the image depicting it) and from an external source of knowledge, such as an information sheet. While the former can be inferred by computer vision algorithms, the latter needs more structured data to pair visual content with relevant information. Regardless of its source, this information still must be be effectively transmitted to the user. A popular emerging trend in computer vision is Visual Question Answering (VQA), in which users can interact with a neural network by posing questions in natural language and receiving answers about the visual content. We believe that this will be the evolution of smart audio guides for museum visits and simple image browsing on personal smartphones. This will turn the classic audio guide into a smart personal instructor with which the visitor can interact by asking for explanations focused on specific interests. The advantages are twofold: on the one hand the cognitive burden of the visitor will decrease, limiting the flow of information to what the user actually wants to hear; and on the other hand it proposes the most natural way of interacting with a guide, favoring engagement.",0
"In recent years, there has been increasing interest in developing computer vision techniques for cultural heritage applications such as art history research, museum collections management, conservation science, and education. One important task in this area is visual question answering (VQA), which involves understanding natural language questions and generating answers based on images from cultural heritage repositories. VQA can support a wide range of use cases including enhancing accessibility and engagement through virtual tours, improving museum operations by automatically identifying objects, classifying content, and reducing human effort required for manual indexing, and contributing to art historical scholarship by providing large-scale quantitative analysis and comparison of visual works. This paper presents a survey of current approaches in VQA for cultural heritage and discusses their strengths and limitations. We review existing datasets and benchmark results, identify opportunities for further work, and outline future directions for the field. Our goal is to provide researchers and practitioners with a comprehensive overview of state-of-the-art methods, tools, and resources for using machine learning in cultural heritage VQA.",1
"Discrete structure rules for validating molecular structures are usually limited to fulfillment of the octet rule or similar simple deterministic heuristics. We propose a model, inspired by language modeling from natural language processing, with the ability to learn from a collection of undirected molecular graphs, enabling fitting of any underlying structure rule present in the collection. We introduce an adaption to the popular Transformer model, which can learn relationships between atoms and bonds. To our knowledge, the Transformer adaption is the first model that is trained to solve the unsupervised task of recovering partially observed molecules. In this work, we assess how different degrees of information impact performance w.r.t. to fitting the QM9 dataset, which conforms to the octet rule, and to fitting the ZINC dataset, which contains hypervalent molecules and ions requiring the model to learn a more complex structure rule. More specifically, we test a full discrete graph with bond order information, a full discrete graph with only connectivity, a bag-of-neighbors, a bag-of-atoms, and a count-based unigram statistics. These results provide encouraging evidence that neural networks, even when only connectivity is available, can learn arbitrary molecular structure rules specific to a dataset, as the Transformer adaption surpasses a strong octet rule baseline on the ZINC dataset.",0
"This research proposes a method for autoencoding undirected molecular graphs using neural networks. Molecules play a crucial role in our lives as they form the basis for drugs, materials, and energy sources. Understanding their properties at the molecular level can lead to significant advancements in various fields such as chemistry, biology, and medicine.  One approach to studying molecules is through graph representation, where each node represents an atom and edge represents a chemical bond. These graphs have unique topological features that make them challenging to model. Traditional graph encoding methods lack sufficient capacity to capture all relevant structural details, resulting in limited performance in downstream tasks. To overcome these limitations, we employ deep learning techniques to design a novel autoencoder architecture capable of preserving important information from the original molecular graph during the encoding process.  Our proposed method adopts an unsupervised strategy by training the autoencoder on large datasets of diverse molecules without any labels. We use variational inference to optimize the encoder network, which maps high-dimensional data (molecular graphs) into a lower-dimensional latent space while maintaining critical chemical structure information.  Evaluation results demonstrate the effectiveness of our model, outperforming state-of-the-art methods on several benchmark tests, including property prediction and clustering of similar molecules based on latent embeddings learned by the autoencoder. Our findings indicate that the proposed algorithm could serve as a valuable tool for exploring new ways to represent and analyze molecular structures, paving the way for improved drug discovery, material development, and other applications.",1
"This paper investigates the idea of encoding object-centered representations in the design of the reward function and policy architectures of a language-guided reinforcement learning agent. This is done using a combination of object-wise permutation invariant networks inspired from Deep Sets and gated-attention mechanisms. In a 2D procedurally-generated world where agents targeting goals in natural language navigate and interact with objects, we show that these architectures demonstrate strong generalization capacities to out-of-distribution goals. We study the generalization to varying numbers of objects at test time and further extend the object-centered architectures to goals involving relational reasoning.",0
"This is an important problem that needs solving. If we can solve it generality becomes feasible across many domains, including deep reinforcement learning (RL). Many existing methods either use large amounts of data/computation to solve the task at hand or rely on heuristics which become brittle over simple changes to environments. Our method uses standard tricks from computer graphics as well as recent advances in modeling sets using neural networks. We prove our theory works via experiments on challenging continuous control tasks where models achieve high rewards without explicit representation learning.",1
"Referring expression comprehension (REC) aims to localize a text-related region in a given image by a referring expression in natural language. Existing methods focus on how to build convincing visual and language representations independently, which may significantly isolate visual and language information. In this paper, we argue that for REC the referring expression and the target region are semantically correlated and subject, location and relationship consistency exist between vision and language.On top of this, we propose a novel approach called MutAtt to construct mutual guidance between vision and language, which treat vision and language equally thus yield compact information matching. Specifically, for each module of subject, location and relationship, MutAtt builds two kinds of attention-based mutual guidance strategies. One strategy is to generate vision-guided language embedding for the sake of matching relevant visual feature. The other reversely generates language-guided visual feature to match relevant language embedding. This mutual guidance strategy can effectively guarantees the vision-language consistency in three modules. Experiments on three popular REC datasets demonstrate that the proposed approach outperforms the current state-of-the-art methods.",0
"Understandably, referring expression comprehension (REC) has recently received significant attention from natural language processing (NLP), due largely to advancements in large pretrained models like BERT, GPT-2, and RoBERTa that excel at capturing global context. Despite these successes, REC remains challenging, particularly as expressions grow more complex and rely on multiple modalities such as images and texts. To address this challenge, our research introduces MultiModal Unified Text And Transformers For Text-Image (MUTATT).  Our method takes advantage of multi-modal inputs by using both textual and visual cues in tandem to better represent referred entities. This approach effectively integrates the strengths of each modality without relying solely on concatenation or feature extraction techniques commonly used in NLP. Our core innovation lies in how we train an encoder-decoder framework inspired by transformers to mutually guide the understanding of visual and textual evidence during inference time. By doing so, our model accurately determines referents across diverse genres and domains while outperforming previous state-of-the-art methods on benchmark datasets.  In summary, our work demonstrates the effectiveness of multi-modality in enhancing REC through MUTATT’s novel combination of encoding both textual and visual data within an iterative architecture. These findings have practical applications ranging from conversational agents to image search engines, further emphasizing their importance in advancing NLP research. We hope this study encourages future exploration into the intricate relationships between vision and language and promotes continued progress towards human-level intelligence for artificial systems.",1
"Referring expression comprehension (REC) and segmentation (RES) are two highly-related tasks, which both aim at identifying the referent according to a natural language expression. In this paper, we propose a novel Multi-task Collaborative Network (MCN) to achieve a joint learning of REC and RES for the first time. In MCN, RES can help REC to achieve better language-vision alignment, while REC can help RES to better locate the referent. In addition, we address a key challenge in this multi-task setup, i.e., the prediction conflict, with two innovative designs namely, Consistency Energy Maximization (CEM) and Adaptive Soft Non-Located Suppression (ASNLS). Specifically, CEM enables REC and RES to focus on similar visual regions by maximizing the consistency energy between two tasks. ASNLS supresses the response of unrelated regions in RES based on the prediction of REC. To validate our model, we conduct extensive experiments on three benchmark datasets of REC and RES, i.e., RefCOCO, RefCOCO+ and RefCOCOg. The experimental results report the significant performance gains of MCN over all existing methods, i.e., up to +7.13% for REC and +11.50% for RES over SOTA, which well confirm the validity of our model for joint REC and RES learning.",0
"This paper presents a multi-task collaborative network (MTCN) approach to referral expression comprehension and segmentation that jointly tackles referring expression recognition and segmentation tasks in natural language processing. We train MTCN on large-scale datasets containing annotated referring expressions along with their corresponding ground truth regions, enabling the model to learn both global semantic features as well as local visual features during training. Our evaluation shows significantly improved performance over baseline models for both tasks, demonstrating the effectiveness of our method for jointly understanding and exploiting textual descriptions and image content. Furthermore, we demonstrate robustness against variations in referring expression types, achieving state-of-the-art results across all three benchmarks used for evaluation: RefCOCO+, RefCOCOg and Charades. Overall, our work serves as a step forward towards automated NLP and computer vision systems that can truly comprehend human input by leveraging multiple modalities effectively.",1
"Vision-dialog navigation posed as a new holy-grail task in vision-language disciplinary targets at learning an agent endowed with the capability of constant conversation for help with natural language and navigating according to human responses. Besides the common challenges faced in visual language navigation, vision-dialog navigation also requires to handle well with the language intentions of a series of questions about the temporal context from dialogue history and co-reasoning both dialogs and visual scenes. In this paper, we propose the Cross-modal Memory Network (CMN) for remembering and understanding the rich information relevant to historical navigation actions. Our CMN consists of two memory modules, the language memory module (L-mem) and the visual memory module (V-mem). Specifically, L-mem learns latent relationships between the current language interaction and a dialog history by employing a multi-head attention mechanism. V-mem learns to associate the current visual views and the cross-modal memory about the previous navigation actions. The cross-modal memory is generated via a vision-to-language attention and a language-to-vision attention. Benefiting from the collaborative learning of the L-mem and the V-mem, our CMN is able to explore the memory about the decision making of historical navigation actions which is for the current step. Experiments on the CVDN dataset show that our CMN outperforms the previous state-of-the-art model by a significant margin on both seen and unseen environments.",0
"In this paper we describe two neural architectures which integrate cross-modal memory features into vision-dialog navigation tasks. These models explore high level concepts such as object affordances, relationships, and attributes, while utilizing external knowledge from text corpora. Through extensive experimentation on three challenging datasets - VDDB, ImageNet, and COCO - our work demonstrates significant improvements over baseline methods, achieving state-of-the art results across all benchmarks. Our contributions include: (i) new techniques that enable better integration of cross-modal memories; (ii) enhanced retrieval through semantic embeddings derived from large language models; and (iii) improved understanding of visual contexts via incorporating higher order representations. Furthermore, we analyze the performance gains associated with different aspects of our approach, including data augmentation strategies, model size, depth, and ensemble ensembling. Finally, ablation studies confirm the effectiveness of each component within our systems. Overall, these advancements bring us closer to realizing human-like capabilities in AI agents interacting with complex environments under open-domain conditions.",1
"Deep learning has been shown to be successful in a number of domains, ranging from acoustics, images, to natural language processing. However, applying deep learning to the ubiquitous graph data is non-trivial because of the unique characteristics of graphs. Recently, substantial research efforts have been devoted to applying deep learning methods to graphs, resulting in beneficial advances in graph analysis techniques. In this survey, we comprehensively review the different types of deep learning methods on graphs. We divide the existing methods into five categories based on their model architectures and training strategies: graph recurrent neural networks, graph convolutional networks, graph autoencoders, graph reinforcement learning, and graph adversarial methods. We then provide a comprehensive overview of these methods in a systematic manner mainly by following their development history. We also analyze the differences and compositions of different methods. Finally, we briefly outline the applications in which they have been used and discuss potential future research directions.",0
"Deep learning techniques have recently shown promising results in many areas such as computer vision, natural language processing, speech recognition, and predictive analytics among others. In particular, graphs provide a powerful mathematical representation framework that captures many complex relationships found across diverse application domains. Motivated by these recent successes, we present a comprehensive survey of deep learning techniques over graph data structures with an emphasis on both theoretical developments and real world applications. We focus on a wide range of problems from node classification, link prediction to clustering, dimensionality reduction, and anomaly detection that can be framed over graph structured inputs. Additionally, we discuss different approaches to build graph convolutional neural networks (GCNN), which constitute one core component of deep learning models. To achieve this goal, the current study covers fundamental concepts behind GCNNs including filtering process, convolution, pooling, and nonlinear activations along with advanced architectural designs tailored for specific graph properties like attention mechanisms and multi-scale representations. Finally, we review existing benchmark datasets commonly used for evaluating and comparing performance of graph embedding methods under the light of future research directions. We aim at providing readers with a concise yet self-contained guide to graph based machine learning and identify open challenges ahead paving the path towards further development in this exciting new field.",1
"This paper studies the problem of temporal moment localization in a long untrimmed video using natural language as the query. Given an untrimmed video and a sentence as the query, the goal is to determine the starting, and the ending, of the relevant visual moment in the video, that corresponds to the query sentence. While previous works have tackled this task by a propose-and-rank approach, we introduce a more efficient, end-to-end trainable, and {\em proposal-free approach} that relies on three key components: a dynamic filter to transfer language information to the visual domain, a new loss function to guide our model to attend the most relevant parts of the video, and soft labels to model annotation uncertainty. We evaluate our method on two benchmark datasets, Charades-STA and ActivityNet-Captions. Experimental results show that our approach outperforms state-of-the-art methods on both datasets.",0
"This proposed method addresses temporal moment localization tasks by framing them as sequence labeling problems in which we learn a guided attention mechanism that selectively focuses on informative features across multiple modalities (e.g., visual features derived from frames, audio features extracted via speech recognition) to accurately predict the precise time segment associated with a natural language query (e.g., “find where the car turns red”). Our approach utilizes crossmodal input to boost performance via self-supervised learning, leverages existing annotated data to pretrain our model, and exploits iteratively trained decoders to yield enhanced feature representations capturing both intra-modality correlations and inter-modality interactions. Experiments demonstrate the effectiveness of our approach over several challenging datasets: Charades, ActivityNet Captions, and TRECVID MED2014. This work presents an important advancement towards realizing flexible video search applications that can robustly interpret complex natural language queries without requiring handcrafted proposals to first identify relevant content within videos.",1
"Visual question answering by using information from multiple modalities has attracted more and more attention in recent years. However, it is a very challenging task, as the visual content and natural language have quite different statistical properties. In this work, we present a method called Adversarial Multimodal Network (AMN) to better understand video stories for question answering. In AMN, as inspired by generative adversarial networks, we propose to learn multimodal feature representations by finding a more coherent subspace for video clips and the corresponding texts (e.g., subtitles and questions). Moreover, we introduce a self-attention mechanism to enforce the so-called consistency constraints in order to preserve the self-correlation of visual cues of the original video clips in the learned multimodal representations. Extensive experiments on the MovieQA dataset show the effectiveness of our proposed AMN over other published state-of-the-art methods.",0
"In recent years, there has been increasing interest in developing natural language processing (NLP) systems capable of understanding complex questions and generating accurate answers. One important application of NLP is question answering, which involves generating textual responses that accurately address user queries. To achieve this goal, it is essential to consider multiple modalities such as images, audio clips, and video snippets. However, current approaches mainly focus on incorporating single modality into their model designs, leading to limited performance compared to human capabilities. We propose an adversarial multimodal network architecture that integrates visual and textual features effectively, enabling the system to generate more informative answers. Our proposed model leverages generative adversarial networks to balance interdependencies among diverse modalities, resulting in better alignment between different types of representations. Extensive experiments demonstrate significant improvements over state-of-the-art baselines, achieving new benchmarks across four challenging movie QA datasets. By providing deeper insights into multimodal learning and generation techniques, our work opens up exciting opportunities for future research in multimodal conversational AI.",1
"The ability to generate natural language explanations conditioned on the visual perception is a crucial step towards autonomous agents which can explain themselves and communicate with humans. While the research efforts in image and video captioning are giving promising results, this is often done at the expense of the computational requirements of the approaches, limiting their applicability to real contexts. In this paper, we propose a fully-attentive captioning algorithm which can provide state-of-the-art performances on language generation while restricting its computational demands. Our model is inspired by the Transformer model and employs only two Transformer layers in the encoding and decoding stages. Further, it incorporates a novel memory-aware encoding of image regions. Experiments demonstrate that our approach achieves competitive results in terms of caption quality while featuring reduced computational demands. Further, to evaluate its applicability on autonomous agents, we conduct experiments on simulated scenes taken from the perspective of domestic robots.",0
"This paper presents a novel architecture called Smart (Shallow memory - awareness transformer), which addresses the challenge of explainability in deep learning models applied in robotics by introducing a shallow memory module into the attention mechanism. In our approach, we use a small recurrent neural network as a memory buffer to store temporarily relevant information during inference while training. We then apply a simple gating mechanism based on the output of this buffer, to selectively incorporate contextual knowledge from distant parts of the input sequence into the current step’s computation. Our experiments demonstrate that using Smart in combination with a pre-trained BERT model significantly improves zero shot response quality on standard benchmark datasets while requiring less computational resources compared to other approaches such as LLM fine tuning, making it suitable for deployment in resource constrained environments. Furthermore, analysis of internal states shows that our method indeed leads to more meaningful representation capturing spatial and temporal relationships in textual inputs, resulting in improved transparency.",1
"Most image captioning frameworks generate captions directly from images, learning a mapping from visual features to natural language. However, editing existing captions can be easier than generating new ones from scratch. Intuitively, when editing captions, a model is not required to learn information that is already present in the caption (i.e. sentence structure), enabling it to focus on fixing details (e.g. replacing repetitive words). This paper proposes a novel approach to image captioning based on iterative adaptive refinement of an existing caption. Specifically, our caption-editing model consisting of two sub-modules: (1) EditNet, a language module with an adaptive copy mechanism (Copy-LSTM) and a Selective Copy Memory Attention mechanism (SCMA), and (2) DCNet, an LSTM-based denoising auto-encoder. These components enable our model to directly copy from and modify existing captions. Experiments demonstrate that our new approach achieves state-of-art performance on the MS COCO dataset both with and without sequence-level training.",0
"This may sound tricky, but here goes! Please keep in mind that I am only able to write based on my current knowledge cutoff date of September 2021, so any more recent developments would need to be factored into your analysis.  Image caption generation has been an active area of research for many years, with numerous approaches proposed by academia and industry alike. While these approaches have achieved promising results, they often fail to account for human preferences when generating descriptions, which can lead to important details being overlooked or omitted altogether. In response, we present ""Show, Edit & Tell,"" a novel framework that leverages both visual and textual modalities to generate high-quality image captions that better align with human expectations.  Our approach begins by showing a user a set of images along with their generated captions and asking them to identify missing or incorrect elements. These edits serve as valuable feedback that our system uses to refine subsequent generations until satisfactory captions are reached. By incorporating both visual and textual input throughout the process, our model learns to better capture salient features and contextual relationships, resulting in more accurate and informative captions overall.  To evaluate the efficacy of our framework, we conduct experiments across four benchmark datasets spanning diverse content domains such as food, landscapes, and scenes containing objects. We compare our results against several state-of-the-art baseline methods using both automatic metrics (such as BLEU, ROUGE, METEOR) as well as manual evaluation from Amazon Mechanical Turk annotators. Our evaluations consistently demonstrate significant improvements across all metrics, confirming that our framework produces higher quality captions compared to existing techniques.  In summary, our work presents a new approach t",1
"Important information that relates to a specific topic in a document is often organized in tabular format to assist readers with information retrieval and comparison, which may be difficult to provide in natural language. However, tabular data in unstructured digital documents, e.g., Portable Document Format (PDF) and images, are difficult to parse into structured machine-readable format, due to complexity and diversity in their structure and style. To facilitate image-based table recognition with deep learning, we develop the largest publicly available table recognition dataset PubTabNet (https://github.com/ibm-aur-nlp/PubTabNet), containing 568k table images with corresponding structured HTML representation. PubTabNet is automatically generated by matching the XML and PDF representations of the scientific articles in PubMed Central Open Access Subset (PMCOA). We also propose a novel attention-based encoder-dual-decoder (EDD) architecture that converts images of tables into HTML code. The model has a structure decoder which reconstructs the table structure and helps the cell decoder to recognize cell content. In addition, we propose a new Tree-Edit-Distance-based Similarity (TEDS) metric for table recognition, which more appropriately captures multi-hop cell misalignment and OCR errors than the pre-established metric. The experiments demonstrate that the EDD model can accurately recognize complex tables solely relying on the image representation, outperforming the state-of-the-art by 9.7% absolute TEDS score.",0
"This paper presents a system that automatically recognizes tables from images. We introduce a large dataset of over four million labeled table boundaries extracted from the web which we use to train our neural network models. Our models achieve high accuracy on both binary (table/no table) and more challenging multi-label classification tasks including row segmentation, column segmentation and cell detection. We demonstrate the effectiveness of our approach through quantitative evaluations against baseline methods as well as human judgments. Additionally, we provide qualitative examples of how our system can correctly identify and extract complex tables embedded within larger documents. Overall, our work shows promising results for future applications such as automating financial reporting, summarization, and search engines.",1
"Deep learning natural language processing models often use vector word embeddings, such as word2vec or GloVe, to represent words. A discrete sequence of words can be much more easily integrated with downstream neural layers if it is represented as a sequence of continuous vectors. Also, semantic relationships between words, learned from a text corpus, can be encoded in the relative configurations of the embedding vectors. However, storing and accessing embedding vectors for all words in a dictionary requires large amount of space, and may stain systems with limited GPU memory. Here, we used approaches inspired by quantum computing to propose two related methods, {\em word2ket} and {\em word2ketXS}, for storing word embedding matrix during training and inference in a highly efficient way. Our approach achieves a hundred-fold or more reduction in the space required to store the embeddings with almost no relative drop in accuracy in practical natural language processing tasks.",0
"Abstract: One of the most pressing challenges faced by natural language processing (NLP) models today is balancing efficiency with accuracy. While current approaches like dense vector representations have achieved high levels of performance on NLP tasks, they come at the cost of significant memory requirements and computational overhead. In this work, we propose Word2Ket, a novel space-efficient approach to representing words as quantum entangled states. By leveraging principles from quantum mechanics and information theory, we show that Word2Ket can reduce the memory footprint of embedding vectors while maintaining their expressiveness and relevance to downstream NLP applications. We evaluate our method using standard benchmarks on several text classification datasets, demonstrating competitive results compared to state-of-the-art methods. Our contributions highlight the potential of integrating insights from physics into NLP to address fundamental tradeoffs between model size and performance.",1
"Video description is the automatic generation of natural language sentences that describe the contents of a given video. It has applications in human-robot interaction, helping the visually impaired and video subtitling. The past few years have seen a surge of research in this area due to the unprecedented success of deep learning in computer vision and natural language processing. Numerous methods, datasets and evaluation metrics have been proposed in the literature, calling the need for a comprehensive survey to focus research efforts in this flourishing new direction. This paper fills the gap by surveying the state of the art approaches with a focus on deep learning models; comparing benchmark datasets in terms of their domains, number of classes, and repository size; and identifying the pros and cons of various evaluation metrics like SPICE, CIDEr, ROUGE, BLEU, METEOR, and WMD. Classical video description approaches combined subject, object and verb detection with template based language models to generate sentences. However, the release of large datasets revealed that these methods can not cope with the diversity in unconstrained open domain videos. Classical approaches were followed by a very short era of statistical methods which were soon replaced with deep learning, the current state of the art in video description. Our survey shows that despite the fast-paced developments, video description research is still in its infancy due to the following reasons. Analysis of video description models is challenging because it is difficult to ascertain the contributions, towards accuracy or errors, of the visual features and the adopted language model in the final description. Existing datasets neither contain adequate visual diversity nor complexity of linguistic structures. Finally, current evaluation metrics ...",0
"This survey paper provides an overview of current methods used in video description research field, including both textual and visual approaches. We review several publicly available datasets that have been created specifically for the task of video description generation, as well as some more general image and text corpora that can also be employed for similar purposes. We evaluate these techniques by using appropriate evaluation metrics, which enable us to compare their performance on different levels of granularity from sentence-level to summary-level. Our primary aim is to provide readers with a comprehensive understanding of state-of-the art technologies, resources, and methodologies involved in video description so they can make informed decisions on how best to proceed with their own research projects in this area. Additionally, we identify open challenges and potential future directions to encourage further development of this rapidly evolving field. Finally, our findings indicate that while many existing solutions show promising results, there remains significant room for improvement in terms of accuracy, coherency, and diversity of generated descriptions.",1
"Referring expression comprehension (REF) aims at identifying a particular object in a scene by a natural language expression. It requires joint reasoning over the textual and visual domains to solve the problem. Some popular referring expression datasets, however, fail to provide an ideal test bed for evaluating the reasoning ability of the models, mainly because 1) their expressions typically describe only some simple distinctive properties of the object and 2) their images contain limited distracting information. To bridge the gap, we propose a new dataset for visual reasoning in context of referring expression comprehension with two main features. First, we design a novel expression engine rendering various reasoning logics that can be flexibly combined with rich visual properties to generate expressions with varying compositionality. Second, to better exploit the full reasoning chain embodied in an expression, we propose a new test setting by adding additional distracting images containing objects sharing similar properties with the referent, thus minimising the success rate of reasoning-free cross-domain alignment. We evaluate several state-of-the-art REF models, but find none of them can achieve promising performance. A proposed modular hard mining strategy performs the best but still leaves substantial room for improvement. We hope this new dataset and task can serve as a benchmark for deeper visual reasoning analysis and foster the research on referring expression comprehension.",0
"Developer are developing more advanced robots that can perform tasks independently. But there is still lack of research focus on natural language understanding and referential reasoning which prevent these machines from performing human like tasks. In this work we introduce Cops-ref task and dataset aimed at benchmarking compositional referring expression comprehension. We showcase some key findings from our study such as compositionality phenomena in object detection. Finally, we release large scale datasets to the community hoping it would spark advancements in this direction.",1
"Home design is a complex task that normally requires architects to finish with their professional skills and tools. It will be fascinating that if one can produce a house plan intuitively without knowing much knowledge about home design and experience of using complex designing tools, for example, via natural language. In this paper, we formulate it as a language conditioned visual content generation problem that is further divided into a floor plan generation and an interior texture (such as floor and wall) synthesis task. The only control signal of the generation process is the linguistic expression given by users that describe the house details. To this end, we propose a House Plan Generative Model (HPGM) that first translates the language input to a structural graph representation and then predicts the layout of rooms with a Graph Conditioned Layout Prediction Network (GC LPN) and generates the interior texture with a Language Conditioned Texture GAN (LCT-GAN). With some post-processing, the final product of this task is a 3D house model. To train and evaluate our model, we build the first Text-to-3D House Model dataset.",0
"Intelligent Home 3D (IHD) is an innovative computer program that generates highly detailed and accurate three-dimensional house designs using only natural language inputs provided by users. Unlike traditional CAD software which requires extensive technical expertise, IHD allows individuals without any previous experience to create custom home plans quickly and easily. By utilizing advanced artificial intelligence algorithms and machine learning techniques, IHD can interpret even vague or incomplete descriptions and generate fully realized, scaled models complete with realistic textures and lighting effects. Furthermore, IHD employs cutting-edge optimization strategies to minimize errors and maximize design efficiency throughout the development process. Overall, IHD represents a significant breakthrough in architectural computing technology, offering a user-friendly, cost-effective alternative to expensive professional services. As such, IHD has great potential applications across numerous industries and fields, including architecture, interior design, urban planning, and more.",1
"In this dissertation, we cover some recent advances in collaborative filtering and ranking. In chapter 1, we give a brief introduction of the history and the current landscape of collaborative filtering and ranking; chapter 2 we first talk about pointwise collaborative filtering problem with graph information, and how our proposed new method can encode very deep graph information which helps four existing graph collaborative filtering algorithms; chapter 3 is on the pairwise approach for collaborative ranking and how we speed up the algorithm to near-linear time complexity; chapter 4 is on the new listwise approach for collaborative ranking and how the listwise approach is a better choice of loss for both explicit and implicit feedback over pointwise and pairwise loss; chapter 5 is about the new regularization technique Stochastic Shared Embeddings (SSE) we proposed for embedding layers and how it is both theoretically sound and empirically effectively for 6 different tasks across recommendation and natural language processing; chapter 6 is how we introduce personalization for the state-of-the-art sequential recommendation model with the help of SSE, which plays an important role in preventing our personalized model from overfitting to the training data; chapter 7, we summarize what we have achieved so far and predict what the future directions can be; chapter 8 is the appendix to all the chapters.",0
"Collaborative filtering (CF) and ranking have been two dominant approaches for personalized recommendation systems due to their success in improving user satisfaction through tailoring recommendations to individual preferences, as well as their scalability across large datasets. In recent years, there has been significant advancement in CF methods that use implicit feedback data such as clicks and views which are more widely available than explicit ratings but noisier and sparser. Additionally, advanced techniques like deep learning and transfer learning have been incorporated into these models, enabling better performance under cold starting scenarios where insufficient interactions exist, leading to improved accuracy overall. This work presents an overview of the current state of collaborative filtering and ranking and identifies some potential future research directions towards next generation intelligent recommender systems that can adapt and evolve on the fly using cognitive computing technologies, multi modal sensor inputs, and cross domain knowledge integration. Ultimately, we aim to provide readers with new perspectives on how cutting edge research developments in artificial intelligence are shaping the evolution of modern day recommender systems.",1
"We present PrecisionBatching, a quantized inference algorithm for speeding up neural network execution on traditional hardware platforms at low bitwidths without the need for retraining or recalibration. PrecisionBatching decomposes a neural network into individual bitlayers and accumulates them using fast 1-bit operations while maintaining activations in full precision. PrecisionBatching not only facilitates quantized inference at low bitwidths ( 8 bits) without the need for retraining/recalibration, but also 1) enables traditional hardware platforms the ability to realize inference speedups at a finer granularity of quantization (e.g: 1-16 bit execution) and 2) allows accuracy and speedup tradeoffs at runtime by exposing the number of bitlayers to accumulate as a tunable parameter. Across a variety of applications (MNIST, language modeling, natural language inference) and neural network architectures (fully connected, RNN, LSTM), PrecisionBatching yields end-to-end speedups of over 8x on a GPU within a  1% error margin of the full precision baseline, outperforming traditional 8-bit quantized inference by over 1.5x-2x at the same error tolerance.",0
Abstract:,1
"We propose Sparse Sinkhorn Attention, a new efficient and sparse method for learning to attend. Our method is based on differentiable sorting of internal representations. Concretely, we introduce a meta sorting network that learns to generate latent permutations over sequences. Given sorted sequences, we are then able to compute quasi-global attention with only local windows, improving the memory efficiency of the attention module. To this end, we propose new algorithmic innovations such as Causal Sinkhorn Balancing and SortCut, a dynamic sequence truncation method for tailoring Sinkhorn Attention for encoding and/or decoding purposes. Via extensive experiments on algorithmic seq2seq sorting, language modeling, pixel-wise image generation, document classification and natural language inference, we demonstrate that our memory efficient Sinkhorn Attention method is competitive with vanilla attention and consistently outperforms recently proposed efficient Transformer models such as Sparse Transformers.",0
"Title: ""Sparse Sinkhorn Attention""  Abstract: This paper presents a new method for attention mechanisms called Sparse Sinkhorn Attention (SSA). We introduce SSA as an extension of traditional softmax-based methods by incorporating a novel approach based on optimal transport theory. SSA uses entropic regularization combined with matrix balancing techniques from the Sinkhorn-Knopp algorithm to enforce sparsity in attention weights while preserving their interpretability. Our proposed model outperforms state-of-the-art models across several benchmarks, including both sequence-to-sequence generation tasks and text classification tasks.  We explore three key challenges associated with self-attention modules that current models fail to address. Firstly, we tackle the problem of computational cost by developing an efficient approximation technique for calculating exact attention weights. Secondly, we address the difficulty of overfitting due to large capacity models by introducing sparsity through entropy maximization. Lastly, we aim to improve generalizability by ensuring that attention weights have meaningful interpretable structure. This is achieved through minimizing discrepancies between predicted probabilities and observed token frequencies during training using the Sinkhorn-Knopp algorithm. Empirical results demonstrate superior performance compared to strong baselines across multiple domains, proving the effectiveness of our proposal.",1
"We propose a federated learning framework to handle heterogeneous client devices which do not conform to the population data distribution. The approach hinges upon a parameterized superquantile-based objective, where the parameter ranges over levels of conformity. We present an optimization algorithm and establish its convergence to a stationary point. We show how to practically implement it using secure aggregation by interleaving iterations of the usual federated averaging method with device filtering. We conclude with numerical experiments on neural networks as well as linear models on tasks from computer vision and natural language processing.",0
"""Device heterogeneity is a key challenge in federated learning - a popular distributed machine learning approach that enables decentralized data processing while preserving privacy. This paper proposes a novel superquantile methodology to tackle device heterogeneity issues in federated learning. By leveraging quantiles, we identify discrepancies between different client devices and aggregate them accordingly, which improves performance and model robustness under diverse conditions. We evaluate our approach using real datasets and demonstrate significant improvements compared to traditional aggregation methods. Our results have important implications for developing efficient and effective federated learning models that can handle diverse client environments."" The following paper presents a new approach to addressing device heterogeneity in federated learning. This is a critical issue since federated learning allows for decentralized data processing without compromising individual privacy. However, differences across clients affects training accuracy and convergence speed, and existing solutions often fail at balancing tradeoffs between these factors. To solve this problem, we propose a superquantile method based on identifying device discrepancies through quantiles and selecting appropriate aggregations. Empirical studies validate the effectiveness of our approach outperforming current techniques in terms of both accuracy and efficiency. These findings provide valuable insights into devising superior federated learning systems that adapt better to varying client configurations. Overall, this research advances knowledge in the field by offering innovative tools to optimize models amidst diversity challenges.",1
"The lottery ticket hypothesis proposes that over-parameterization of deep neural networks (DNNs) aids training by increasing the probability of a ""lucky"" sub-network initialization being present rather than by helping the optimization process (Frankle & Carbin, 2019). Intriguingly, this phenomenon suggests that initialization strategies for DNNs can be improved substantially, but the lottery ticket hypothesis has only previously been tested in the context of supervised learning for natural image tasks. Here, we evaluate whether ""winning ticket"" initializations exist in two different domains: natural language processing (NLP) and reinforcement learning (RL).For NLP, we examined both recurrent LSTM models and large-scale Transformer models (Vaswani et al., 2017). For RL, we analyzed a number of discrete-action space tasks, including both classic control and pixel control. Consistent with workin supervised image classification, we confirm that winning ticket initializations generally outperform parameter-matched random initializations, even at extreme pruning rates for both NLP and RL. Notably, we are able to find winning ticket initializations for Transformers which enable models one-third the size to achieve nearly equivalent performance. Together, these results suggest that the lottery ticket hypothesis is not restricted to supervised learning of natural images, but rather represents a broader phenomenon in DNNs.",0
"Title: ""Playing the Lottery with Rewards and Multiple Languages: An Analysis of Lottery Tickets in Reinforcement Learning and Natural Language Processing""  Abstract: In many countries across the world, playing the lottery has become a popular form of entertainment and potential source of income. However, while players may enjoy the excitement that comes with trying their luck, there can be significant challenges associated with understanding how different factors, such as language, can impact the odds of winning. In order to better understand these complex relationships, researchers have begun exploring the use of reinforcement learning (RL) and natural language processing (NLP) techniques to study lottery tickets and other similar phenomena. This work promises to provide insights into both theoretical and applied problems related to decision making under uncertainty, and could potentially lead to new methods for evaluating risk and reward in environments where the underlying rules remain partially hidden from view. By bridging these two fields, scientists hope to create powerful tools capable of analyzing vast amounts of data and providing accurate predictions to aid in decision making and strategic planning.",1
"This paper significantly improves on, and finishes to validate, an approach proposed in previous research in which safety outcomes were predicted from attributes with machine learning. Like in the original study, we use Natural Language Processing (NLP) to extract fundamental attributes from raw incident reports and machine learning models are trained to predict safety outcomes. The outcomes predicted here are injury severity, injury type, body part impacted, and incident type. However, unlike in the original study, safety outcomes were not extracted via NLP but were provided by independent human annotations, eliminating any potential source of artificial correlation between predictors and predictands. Results show that attributes are still highly predictive, confirming the validity of the original approach. Other improvements brought by the current study include the use of (1) a much larger dataset featuring more than 90,000 reports, (2) two new models, XGBoost and linear SVM (Support Vector Machines), (3) model stacking, (4) a more straightforward experimental setup with more appropriate performance metrics, and (5) an analysis of per-category attribute importance scores. Finally, the injury severity outcome is well predicted, which was not the case in the original study. This is a significant advancement.",0
"This should provide some basic context and give readers a general idea of what your paper is about without requiring them to read it in full before deciding if they want to do so. Please note that I am trained on papers published in technical journals, conferences, as well as online magazines for popular science outreach. Therefore my expectation for quality may differ from other instructors you have interacted with previously. Also please keep in mind that while generating an engaging abstract can increase interest in reading your paper, ultimately a high quality research result with novel contributions must back up any claims made in order for it to gain wider recognition, citations and influence within your field.  AI-based prediction models have become increasingly relevant in recent years due to their potential to improve safety and efficiency in construction projects. In particular, using machine learning algorithms to predict independent construction safety outcomes based on universal attributes could revolutionize the industry by enabling proactive risk management strategies tailored to specific sites, materials, and contractor teams. However, despite promising advancements in this area, few studies have explored the feasibility of developing such predictive models for use across diverse construction environments, materials, site conditions, and workforce characteristics.  This study seeks to address this gap in knowledge by investigating whether artificial intelligence (AI) can accurately forecast construction safety outcomes from a range of universally applicable variables, including location data, building materials, labor supply chains, and weather patterns. To accomplish this, we propose constructing and testing a novel, deep neural network model capable of processing large volumes of disparate input parameters to generate predictions for construction safety incidents and accidents. By demonstrating th",1
"Many applications, including natural language processing, sensor networks, collaborative filtering, and federated learning, call for estimating discrete distributions from data collected in batches, some of which may be untrustworthy, erroneous, faulty, or even adversarial.   Previous estimators for this setting ran in exponential time, and for some regimes required a suboptimal number of batches. We provide the first polynomial-time estimator that is optimal in the number of batches and achieves essentially the best possible estimation accuracy.",0
This paper presents a new algorithm for learning discrete distributions from batches of data. The algorithm optimizes robustness under distribution shift while minimizing regret. We analyze the theoretical guarantees of our method and show that it outperforms existing state-of-the-art algorithms across a range of environments. Our results have important implications for practitioners looking to design systems that can learn and adapt quickly in changing environments.,1
"Extracting context from visual representations is of utmost importance in the advancement of Computer Science. Representation of such a format in Natural Language has a huge variety of applications such as helping the visually impaired etc. Such an approach is a combination of Computer Vision and Natural Language techniques which is a hard problem to solve. This project aims to compare different approaches for solving the image captioning problem. In specific, the focus was on comparing two different types of models: Encoder-Decoder approach and a Multi-model approach. In the encoder-decoder approach, inject and merge architectures were compared against a multi-modal image captioning approach based primarily on object detection. These approaches have been compared on the basis on state of the art sentence comparison metrics such as BLEU, GLEU, Meteor, and Rouge on a subset of the Google Conceptual captions dataset which contains 100k images. On the basis of this comparison, we observed that the best model was the Inception injected encoder model. This best approach has been deployed as a web-based system. On uploading an image, such a system will output the best caption associated with the image.",0
"This article proposes a new image understanding method that converts images into natural language descriptions using deep learning techniques. By training a convolutional neural network on large volumes of labeled data, we are able to generate high quality captions which accurately describe the contents of each image. Our proposed model uses multiple layers of abstraction, allowing it to capture both low level features such as colors and shapes, as well as higher level concepts such as objects and scenes. We evaluate our model against state of the art methods on several benchmark datasets and demonstrate superior performance across all metrics. Additionally, we showcase the robustness of our model by testing it on challenging scenarios where existing models struggle. Overall, our work represents a significant step towards truly intelligent image understanding systems. -----  Please provide feedback! Your critique would be most valuable. If you have any other requests please don't hesitate to ask. I am here to assist.",1
"Classical person re-identification approaches assume that a person of interest has appeared across different cameras and can be queried by one of the existing images. However, in real-world surveillance scenarios, frequently no visual information will be available about the queried person. In such scenarios, a natural language description of the person by a witness will provide the only source of information for retrieval. In this work, person re-identification using both vision and language information is addressed under all possible gallery and query scenarios. A two stream deep convolutional neural network framework supervised by cross entropy loss is presented. The weights connecting the second last layer to the last layer with class probabilities, i.e., logits of softmax layer are shared in both networks. Canonical Correlation Analysis is performed to enhance the correlation between the two modalities in a joint latent embedding space. To investigate the benefits of the proposed approach, a new testing protocol under a multi modal ReID setting is proposed for the test split of the CUHK-PEDES and CUHK-SYSU benchmarks. The experimental results verify the merits of the proposed system. The learnt visual representations are more robust and perform 22\% better during retrieval as compared to a single modality system. The retrieval with a multi modal query greatly enhances the re-identification capability of the system quantitatively as well as qualitatively.",0
"This paper presents a new approach to person re-identification using convolutional neural networks (CNNs) that incorporates both visual features from images and textual descriptions generated by natural language processing techniques. By combining these two sources of data, we can improve the accuracy of person re-id systems compared to those that rely solely on visual features. Our method uses pre-trained CNN models as a baseline, then fine-tunes them on our combined dataset. We evaluate our system on several benchmark datasets and demonstrate improved performance over state-of-the-art methods. In addition to providing detailed results and analysis, we also discuss potential applications of our method beyond just person re-id. Overall, we believe that our work represents a significant step forward in the development of intelligent computer vision systems that can effectively combine multiple types of data to solve complex tasks.",1
"Text-based games -- in which an agent interacts with the world through textual natural language -- present us with the problem of combinatorially-sized action-spaces. Most current reinforcement learning algorithms are not capable of effectively handling such a large number of possible actions per turn. Poor sample efficiency, consequently, results in agents that are unable to pass bottleneck states, where they are unable to proceed because they do not see the right action sequence to pass the bottleneck enough times to be sufficiently reinforced. Building on prior work using knowledge graphs in reinforcement learning, we introduce two new game state exploration strategies. We compare our exploration strategies against strong baselines on the classic text-adventure game, Zork1, where prior agent have been unable to get past a bottleneck where the agent is eaten by a Grue.",0
"In recent years, text-based adventures have seen a resurgence in popularity due to their simplicity, accessibility, and potential for novel gameplay mechanics. One challenge facing agents operating in these environments is how to explore efficiently while minimizing risk from unknown hazards such as grues. This paper proposes several exploration strategies designed to maximize coverage and reduce danger, using techniques inspired by human navigation methods and adaptive search algorithms. We evaluate the effectiveness of our approaches through simulation and compare them against traditional random sampling. Our results demonstrate that our strategies significantly outperform existing methods in terms of both speed and safety, making them valuable tools for players and developers alike. Overall, this work contributes new insights into navigating uncertain, textually-represented domains and has implications for artificial intelligence more broadly.",1
"Incorporating hierarchical structures like constituency trees has been shown to be effective for various natural language processing (NLP) tasks. However, it is evident that state-of-the-art (SOTA) sequence-based models like the Transformer struggle to encode such structures inherently. On the other hand, dedicated models like the Tree-LSTM, while explicitly modeling hierarchical structures, do not perform as efficiently as the Transformer. In this paper, we attempt to bridge this gap with ""Hierarchical Accumulation"" to encode parse tree structures into self-attention at constant time complexity. Our approach outperforms SOTA methods in four IWSLT translation tasks and the WMT'14 English-German translation task. It also yields improvements over Transformer and Tree-LSTM on three text classification tasks. We further demonstrate that using hierarchical priors can compensate for data shortage, and that our model prefers phrase-level attentions over token-level attentions.",0
"Incorporate some key ideas from the body of your paper into the abstract. Be sure all important terms and concepts appear here that would need definitions in the introduction section if not already explained within the paper itself. If you are unsure about how these constraints apply to papers in a particular field other than the natural sciences please ask a member of staff! Additionally, some guidelines on writing style for a computer science abstract can be found at https://www.cs.cmu.edu/afs/andrew/public/CourseFiles/15748-spr2006/papers/abstract_guidelines_vldb09.pdf The use of tree structures for visual representation has been growing exponentially over recent years due to their ability to capture high level abstractions as well as low level details. This work presents a new model called TATHAN which allows for hierarchical attention mechanisms within neural networks to more effectively reason over structured data such as trees and graphs. We demonstrate the superiority of our method over baseline models on three challenging task categories: machine translation, question answering and sentiment analysis. Our main contributions are summarized below: (i) we introduce a novel approach to incorporating hierarchical structure into neural network architectures through our proposed HAN module; (ii) using TATHAN, we show significant improvements compared to state of the art results across multiple datasets in two languages; (iii) ablation studies illustrate which design choices lead to better performance. Overall, this research furthers our understanding of modeling tree structured representations using deep learning techniques.",1
"Transformer based Very Large Language Models (VLLMs) like BERT, XLNet and RoBERTa, have recently shown tremendous performance on a large variety of Natural Language Understanding (NLU) tasks. However, due to their size, these VLLMs are extremely resource intensive and cumbersome to deploy at production time. Several recent publications have looked into various ways to distil knowledge from a transformer based VLLM (most commonly BERT-Base) into a smaller model which can run much faster at inference time. Here, we propose a novel set of techniques which together produce a task-specific hybrid convolutional and transformer model, WaLDORf, that achieves state-of-the-art inference speed while still being more accurate than previous distilled models.",0
"This should be based on your work so far, if you have any questions ask!",1
"Attention based Transformer architecture has enabled significant advances in the field of natural language processing. In addition to new pre-training techniques, recent improvements crucially rely on working with a relatively larger embedding dimension for tokens. Unfortunately, this leads to models that are prohibitively large to be employed in the downstream tasks. In this paper we identify one of the important factors contributing to the large embedding size requirement. In particular, our analysis highlights that the scaling between the number of heads and the size of each head in the current architecture gives rise to a low-rank bottleneck in attention heads, causing this limitation. We further validate this in our experiments. As a solution we propose to set the head size of an attention unit to input sequence length, and independent of the number of heads, resulting in multi-head attention layers with provably more expressive power. We empirically show that this allows us to train models with a relatively smaller embedding dimension and with better performance scaling.",0
"Multi-Head Attention (MHA) has been a popular design choice for many deep learning architectures such as Transformers due to their ability to capture interdependencies between multiple input modalities effectively. However, the recent trend towards increasing the number of attention heads raises concerns regarding computational efficiency and effectiveness. In particular, we observe that while having more attention heads can improve model performance under certain conditions, there exists a low-rank bottleneck during training which hinders further improvement. To address this issue, we propose a novel method based on Singular Value Decomposition (SVD) to control the rank of each head separately. Experimental results show significant improvements over state-of-the-art MHA models across several benchmark datasets, demonstrating the efficacy of our approach. Our findings provide important insights into the nature of multi-head attention mechanisms and have implications for future research in developing efficient deep learning architectures.",1
"Transfer learning has become the de facto standard in computer vision and natural language processing, especially where labeled data is scarce. Accuracy can be significantly improved by using pre-trained models and subsequent fine-tuning. In visual reasoning tasks, such as image question answering, transfer learning is more complex. In addition to transferring the capability to recognize visual features, we also expect to transfer the system's ability to reason. Moreover, for video data, temporal reasoning adds another dimension. In this work, we formalize these unique aspects of transfer learning and propose a theoretical framework for visual reasoning, exemplified by the well-established CLEVR and COG datasets. Furthermore, we introduce a new, end-to-end differentiable recurrent model (SAMNet), which shows state-of-the-art accuracy and better performance in transfer learning on both datasets. The improved performance of SAMNet stems from its capability to decouple the abstract multi-step reasoning from the length of the sequence and its selective attention enabling to store only the question-relevant objects in the external memory.",0
"Abstract: While transfer learning has been widely applied to image classification tasks, there have been limited efforts focused on developing effective approaches for transfer learning for visual and relational reasoning tasks. We aim to fill this gap by presenting two novel algorithms based on meta learning that can effectively adapt pretrained models for both task-agnostic visual understanding tasks such as object detection, instance segmentation, pose estimation, and semantic correspondence; and then evaluate them extensively across several benchmark datasets showing significant improvements over state-of-the-art baselines. Additionally, we show that these methods generalize well to real world applications, including robotic grasping and scene text transcription. Our results demonstrate the effectiveness of our approach in promoting more efficient training through knowledge transfer, which ultimately leads to better performing systems while requiring less data compared to fully supervised fine tuning alone.",1
"Video captioning is an advanced multi-modal task which aims to describe a video clip using a natural language sentence. The encoder-decoder framework is the most popular paradigm for this task in recent years. However, there exist some problems in the decoder of a video captioning model. We make a thorough investigation into the decoder and adopt three techniques to improve the performance of the model. First of all, a combination of variational dropout and layer normalization is embedded into a recurrent unit to alleviate the problem of overfitting. Secondly, a new online method is proposed to evaluate the performance of a model on a validation set so as to select the best checkpoint for testing. Finally, a new training strategy called professional learning is proposed which uses the strengths of a captioning model and bypasses its weaknesses. It is demonstrated in the experiments on Microsoft Research Video Description Corpus (MSVD) and MSR-Video to Text (MSR-VTT) datasets that our model has achieved the best results evaluated by BLEU, CIDEr, METEOR and ROUGE-L metrics with significant gains of up to 18% on MSVD and 3.5% on MSR-VTT compared with the previous state-of-the-art models.",0
"Title: An In Depth Examination of the Text-to-Image Encoding Process in Video Captioning Tools Abstract: With video becoming such an important medium for communication today, there has been increased focus on making videos accessible to all individuals regardless of their ability to hear. One approach that has gained popularity recently is text-based representation of images in videos through automatic image description generation models like YouDescribe and automatic speech recognition (ASR) systems like CAVE. However, these methods have limitations which prevent them from delivering accurate results consistently. Researchers have proposed using convolutional neural networks (CNNs) pre-trained on large scale datasets to tackle this problem. These CNN based approaches can learn complex representations directly from pixels without any external supervision by mapping input image features to corresponding captions. The proposed work delves deeper into decoder architecture used in most state-of-the art image captioning frameworks with Transformer architectures and explores two alternative decoders that outperform traditional Transformer architecture by significant margins. The study concludes that decoding methodology holds great significance in determining performance. This research paves way for future works to identify better encoder architectures thereby enhancing overall image captioning capabilities.",1
"The rapid growth of video on the internet has made searching for video content using natural language queries a significant challenge. Human-generated queries for video datasets `in the wild' vary a lot in terms of degree of specificity, with some queries describing specific details such as the names of famous identities, content from speech, or text available on the screen. Our goal is to condense the multi-modal, extremely high dimensional information from videos into a single, compact video representation for the task of video retrieval using free-form text queries, where the degree of specificity is open-ended.   For this we exploit existing knowledge in the form of pre-trained semantic embeddings which include 'general' features such as motion, appearance, and scene features from visual content. We also explore the use of more 'specific' cues from ASR and OCR which are intermittently available for videos and find that these signals remain challenging to use effectively for retrieval. We propose a collaborative experts model to aggregate information from these different pre-trained experts and assess our approach empirically on five retrieval benchmarks: MSR-VTT, LSMDC, MSVD, DiDeMo, and ActivityNet. Code and data can be found at www.robots.ox.ac.uk/~vgg/research/collaborative-experts/. This paper contains a correction to results reported in the previous version.",0
"This paper presents a method for retrieving video content using representations that have been generated by collaborating experts. Traditionally, retrieval systems rely on textual descriptions or tags assigned by users, but these can often be inaccurate or incomplete. In contrast, our approach leverages knowledge from multiple experts who work together to create high quality representations of videos, which are then used as input for a state-of-the-art retrieval system. We demonstrate significant improvements over baseline methods, achieving better recall and precision across several datasets. Our results show the potential benefits of collaborative expertise for improving video retrieval performance, offering new possibilities for building effective multimedia search systems.",1
"Biomedical research papers use significantly different language and jargon when compared to typical English text, which reduces the utility of pre-trained NLP models in this domain. Meanwhile Medline, a database of biomedical abstracts, introduces nearly a million new documents per-year. Applications that could benefit from understanding this wealth of publicly available information, such as scientific writing assistants, chat-bots, or descriptive hypothesis generation systems, require new domain-centered approaches. A conditional language model, one that learns the probability of words given some a priori criteria, is a fundamental building block in many such applications. We propose a transformer-based conditional language model with a shallow encoder ""condition"" stack, and a deep ""language model"" stack of multi-headed attention blocks. The condition stack encodes metadata used to alter the output probability distribution of the language model stack. We sample this distribution in order to generate biomedical abstracts given only a proposed title, an intended publication year, and a set of keywords. Using typical natural language generation metrics, we demonstrate that this proposed approach is more capable of producing non-trivial relevant entities within the abstract body than the 1.5B parameter GPT-2 language model.",0
"Automatically generating summaries from scientific papers has been a topic of interest for several years now. In recent times, deep learning techniques have played a significant role in achieving high performance in various natural language processing tasks, such as text classification and question answering. A novel approach called conditional biomedical abstract generation (CBAG) was proposed by Hua et al., which utilizes a pre-trained LSTM (Long Short Term Memory) model fine-tuned on a large corpus of PubMed articles with highlights. The resulting model, named BioHiglight, demonstrated superior performance over traditional methods like sentence extraction and rule-based systems. The authors evaluated their method on two datasets consisting of biochemistry and cancer research articles. Experimental results showed that BioHiglight was able to effectively generate concise yet informative summaries outperforming other state-of-the-art models in terms of ROUGE scores. Additionally, human evaluations confirmed the quality of generated summaries deeming them useful and relevant. Furthermore, analysis of generated summaries revealed interesting insights into how they differ from extractive methods and the benefits of using BioHiglight instead. This study presents CBAG - a promising solution towards enhancing literature review processes and facilitating information retrieval, bridging the gap between automatic summary generation and manual reviews while ensuring high-quality outputs. With future advancements, this technique could potentially play a critical role in medical decision making, scientific discovery, and drug development.",1
"We present GluonCV and GluonNLP, the deep learning toolkits for computer vision and natural language processing based on Apache MXNet (incubating). These toolkits provide state-of-the-art pre-trained models, training scripts, and training logs, to facilitate rapid prototyping and promote reproducible research. We also provide modular APIs with flexible building blocks to enable efficient customization. Leveraging the MXNet ecosystem, the deep learning models in GluonCV and GluonNLP can be deployed onto a variety of platforms with different programming languages. The Apache 2.0 license has been adopted by GluonCV and GluonNLP to allow for software distribution, modification, and usage.",0
"This paper introduces two new open-source frameworks developed by Amazon, GluonCV and GluonNLP, which allow users to easily create and deploy deep learning models for computer vision (CV) and natural language processing (NLP), respectively. GluonCV is designed specifically for CV tasks such as object detection, segmentation, and classification, while GluonNLP focuses on NLP applications like text classification and translation. Both frameworks utilize the Gluon interface, providing developers with a consistent programming experience across both domains. Additionally, these libraries leverage popular deep learning engines, including TensorFlow and Caffe, allowing practitioners to seamlessly use their preferred tools within the same framework. Finally, we demonstrate the effectiveness of these frameworks through several case studies using real-world datasets and benchmarks, validating their potential impact in these increasingly important fields.",1
"The goal of this paper is to embed controllable factors, i.e., natural language descriptions, into image-to-image translation with generative adversarial networks, which allows text descriptions to determine the visual attributes of synthetic images. We propose four key components: (1) the implementation of part-of-speech tagging to filter out non-semantic words in the given description, (2) the adoption of an affine combination module to effectively fuse different modality text and image features, (3) a novel refined multi-stage architecture to strengthen the differential ability of discriminators and the rectification ability of generators, and (4) a new structure loss to further improve discriminators to better distinguish real and synthetic images. Extensive experiments on the COCO dataset demonstrate that our method has a superior performance on both visual realism and semantic consistency with given descriptions.",0
"Abstract: This paper presents an image-to-image translation model that can generate realistic images based on natural language text descriptions. Unlike previous models that rely solely on visual inputs, our method uses both visual and textual cues to produce more accurate and diverse outputs. We use convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to capture features from the input image and text description respectively, then fuse them together to create a new feature vector which serves as input to the generator network. Our experiments show that using text guidance improves the quality and diversity of generated images over models without text guidance. Moreover, we demonstrate the effectiveness of our model on several tasks including object manipulation, style transfer, and hallucination, showing its wide range of applications in computer graphics and vision domains.",1
"We extend the idea of word pieces in natural language models to machine learning tasks on opaque ids. This is achieved by applying hash functions to map each id to multiple hash tokens in a much smaller space, similarly to a Bloom filter. We show that by applying a multi-layer Transformer to these Bloom filter digests, we are able to obtain models with high accuracy. They outperform models of a similar size without hashing and, to a large degree, models of a much larger size trained using sampled softmax with the same computational budget. Our key observation is that it is important to use a multi-layer Transformer for Bloom filter digests to remove ambiguity in the hashed input. We believe this provides an alternative method to solving problems with large vocabulary size.",0
"As I don’t have access to your paper titled “Superbloom: Bloom filter meets Transformer”, could you please provide me the text of your paper so that I can write an abstract based on the content? Otherwise, here is an example of how you might rewrite the abstract without using the exact same language as the original one, which was written before Bloom filters met transformers:  Abstract: This work presents a novel application of Bloom filters, a probabilistic data structure used to test whether an element is likely present in a set. Our approach combines the power of Bloom filters with recent advances in natural language processing using the popular open source library Open Assistant, resulting in a new technique we call ""superblooming"". We demonstrate how superblooming can improve the efficiency and accuracy of natural language processing tasks by experimenting with different configurations and comparing them against traditional methods. Our results show promising potential for applications in areas such as sentiment analysis, question answering, and machine translation, among others. Overall, our study highlights the benefits of adapting existing techniques to meet emerging challenges and paves the way for future research exploring the intersection of computer science and human languages.",1
"Boolean matrix has been used to represent digital information in many fields, including bank transaction, crime records, natural language processing, protein-protein interaction, etc. Boolean matrix factorization (BMF) aims to find an approximation of a binary matrix as the Boolean product of two low rank Boolean matrices, which could generate vast amount of information for the patterns of relationships between the features and samples. Inspired by binary matrix permutation theories and geometric segmentation, we developed a fast and efficient BMF approach called MEBF (Median Expansion for Boolean Factorization). Overall, MEBF adopted a heuristic approach to locate binary patterns presented as submatrices that are dense in 1's. At each iteration, MEBF permutates the rows and columns such that the permutated matrix is approximately Upper Triangular-Like (UTL) with so-called Simultaneous Consecutive-ones Property (SC1P). The largest submatrix dense in 1 would lies on the upper triangular area of the permutated matrix, and its location was determined based on a geometric segmentation of a triangular. We compared MEBF with other state of the art approaches on data scenarios with different sparsity and noise levels. MEBF demonstrated superior performances in lower reconstruction error, and higher computational efficiency, as well as more accurate sparse patterns than popular methods such as ASSO, PANDA and MP. We demonstrated the application of MEBF on both binary and non-binary data sets, and revealed its further potential in knowledge retrieving and data denoising.",0
"In the modern era, data has become increasingly important as businesses seek to improve their operations by leveraging insights from large datasets. One technique that allows us to extract meaningful patterns from these datasets is boolean matrix factorization (BMF), which decomposes a binary matrix into two low rank matrices. This decomposition can then be used for tasks such as image retrieval and recommender systems. However, existing methods face significant limitations, including high computational complexity and memory requirements, making them unsuitable for large scale applications. To address these challenges, we propose a novel method called geometric segmentation based BMF (GSBMF). Our approach combines traditional BMF techniques with geometric clustering principles, resulting in fast and efficient matrix factorization without sacrificing accuracy. We demonstrate the effectiveness of our algorithm through extensive experiments on real world datasets, showing that GSBMF outperforms state of the art methods while requiring less time and memory. Overall, our work represents a major step forward in enabling businesses to harness the power of big data analytics.",1
"Early detection of psychological distress is key to effective treatment. Automatic detection of distress, such as depression, is an active area of research. Current approaches utilise vocal, facial, and bodily modalities. Of these, the bodily modality is the least investigated, partially due to the difficulty in extracting bodily representations from videos, and partially due to the lack of viable datasets. Existing body modality approaches use automatic categorization of expressions to represent body language as a series of specific expressions, much like words within natural language. In this dissertation I present a new type of feature, within the body modality, that represents meta information of gestures, such as speed, and use it to predict a non-clinical depression label. This differs to existing work by representing overall behaviour as a small set of aggregated meta features derived from a person's movement. In my method I extract pose estimation from videos, detect gestures within body parts, extract meta information from individual gestures, and finally aggregate these features to generate a small feature vector for use in prediction tasks. I introduce a new dataset of 65 video recordings of interviews with self-evaluated distress, personality, and demographic labels. This dataset enables the development of features utilising the whole body in distress detection tasks. I evaluate my newly introduced meta-features for predicting depression, anxiety, perceived stress, somatic stress, five standard personality measures, and gender. A linear regression based classifier using these features achieves a 82.70% F1 score for predicting depression within my novel dataset.",0
"In recent years there has been growing interest in using computer vision techniques to extract features from human body gestures as part of affective computing applications. Body gestures have proven to provide important cues about a person’s emotional state and can serve as valuable input to systems that aim to recognize and respond to human emotions. However, existing approaches often rely on complex models that require significant computational resources or lack robustness under different environmental conditions, which limits their applicability in real-world scenarios. This paper presents a novel framework that leverages vision-based methods to extract met ... Show more",1
"Implementing enterprise process automation often requires significant technical expertise and engineering effort. It would be beneficial for non-technical users to be able to describe a business process in natural language and have an intelligent system generate the workflow that can be automatically executed. A building block of process automations are If-Then programs. In the consumer space, sites like IFTTT and Zapier allow users to create automations by defining If-Then programs using a graphical interface. We explore the efficacy of modeling If-Then programs as a sequence learning task. We find Seq2Seq approaches have high potential (performing strongly on the Zapier recipes) and can serve as a promising approach to more complex program synthesis challenges.",0
"Title: ""Evaluating Sequence-to-Sequence Learning Models for If-Then Program Synthesis""  Abstract: This research explores the use of sequence-to-sequence learning models for the task of if-then program synthesis. We evaluate several state-of-the-art sequence-to-sequence models and compare their performance on a dataset of programming problems. Our results show that these models are capable of generating correct and efficient code solutions for many problem instances, but there remain limitations in handling certain types of logic and control flow constructs. We provide insights into how different model architectures and training methods affect the quality of generated programs. Overall, our findings demonstrate the potential of using machine learning techniques for automated program synthesis tasks while highlighting areas where further improvements are needed.",1
"Undirected neural sequence models such as BERT (Devlin et al., 2019) have received renewed interest due to their success on discriminative natural language understanding tasks such as question-answering and natural language inference. The problem of generating sequences directly from these models has received relatively little attention, in part because generating from undirected models departs significantly from conventional monotonic generation in directed sequence models. We investigate this problem by proposing a generalized model of sequence generation that unifies decoding in directed and undirected models. The proposed framework models the process of generation rather than the resulting sequence, and under this framework, we derive various neural sequence models as special cases, such as autoregressive, semi-autoregressive, and refinement-based non-autoregressive models. This unification enables us to adapt decoding algorithms originally developed for directed sequence models to undirected sequence models. We demonstrate this by evaluating various handcrafted and learned decoding strategies on a BERT-like machine translation model (Lample & Conneau, 2019). The proposed approach achieves constant-time translation results on par with linear-time translation results from the same undirected sequence model, while both are competitive with the state-of-the-art on WMT'14 English-German translation.",0
"Abstract: Sequence generation is a fundamental task in natural language processing (NLP) that involves generating text one token at a time. Recently, deep learning approaches such as recurrent neural networks (RNNs) and transformers have achieved state-of-the-art results on several sequence generation tasks. However, existing methods often require specialized architectures or heuristics tailored to specific types of data, limiting their general applicability. In this work, we propose a generalized framework for sequence generation that can accommodate different types of input sequences and output spaces. Our approach leverages recent advances in self-attention mechanisms, enabling efficient computation over large sequences while maintaining parallelism across tokens. We evaluate our framework using two common NLP benchmark datasets, showing competitive performance compared to state-of-the-art models. Furthermore, our framework provides insights into how attention weights relate to downstream probabilities, shedding light on the nature of these connections in sequence prediction problems. Overall, our work presents a flexible and powerful toolkit for sequence generation in NLP, applicable beyond specific domains or model classes.",1
"Distributed word embeddings have yielded state-of-the-art performance in many NLP tasks, mainly due to their success in capturing useful semantic information. These representations assign only a single vector to each word whereas a large number of words are polysemous (i.e., have multiple meanings). In this work, we approach this critical problem in lexical semantics, namely that of representing various senses of polysemous words in vector spaces. We propose a topic modeling based skip-gram approach for learning multi-prototype word embeddings. We also introduce a method to prune the embeddings determined by the probabilistic representation of the word in each topic. We use our embeddings to show that they can capture the context and word similarity strongly and outperform various state-of-the-art implementations.",0
"Recent research has shown that topic models can capture meaningful underlying themes in text data. However, existing methods suffer from limitations related to interpretability, sparsity, and scalability. In this paper, we address these challenges by introducing multi sense embeddings from topic models (MSE). Our approach involves training topic models on large datasets and encoding topics as low-dimensional vectors in semantic spaces using pre-trained language models like Word2Vec and GloVe. We then use clustering algorithms to identify multiple senses associated with each topic vector and develop novel visualization techniques to provide insights into their contextual meanings. Experimental results demonstrate the effectiveness of our method in improving interpretability, reducing sparsity, and scaling up topic modeling to handle massive datasets. Additionally, MSE outperforms state-of-the-art topic models across various evaluation metrics. Overall, our work presents a significant advancement towards enabling human-like understanding of complex texts through topic modeling.",1
"Generative adversarial networks (GANs) are increasingly attracting attention in the computer vision, natural language processing, speech synthesis and similar domains. Arguably the most striking results have been in the area of image synthesis. However, evaluating the performance of GANs is still an open and challenging problem. Existing evaluation metrics primarily measure the dissimilarity between real and generated images using automated statistical methods. They often require large sample sizes for evaluation and do not directly reflect human perception of image quality. In this work, we describe an evaluation metric we call Neuroscore, for evaluating the performance of GANs, that more directly reflects psychoperceptual image quality through the utilization of brain signals. Our results show that Neuroscore has superior performance to the current evaluation metrics in that: (1) It is more consistent with human judgment; (2) The evaluation process needs much smaller numbers of samples; and (3) It is able to rank the quality of images on a per GAN basis. A convolutional neural network (CNN) based neuro-AI interface is proposed to predict Neuroscore from GAN-generated images directly without the need for neural responses. Importantly, we show that including neural responses during the training phase of the network can significantly improve the prediction capability of the proposed model. Materials related to this work are provided at https://github.com/villawang/Neuro-AI-Interface.",0
"Artificial intelligence (AI) has been used to create generative models that can generate new data based on existing examples, such as images, audio files, text, etc. These generative models often use deep neural networks called generative adversarial networks (GANs), which consist of two sub-networks competing against each other – one generating fake data and another discriminating real from generated data. GANs have shown great potential in many fields but evaluating their performance remains challenging due to their lack of transparency. This study presents a novel approach using neuro-artificial intelligence interfaces to evaluate the output quality of GANs by simulating how humans would perceive them. We propose a system called Synthetic-Neuroscore (SynNS) that combines psychology-based evaluation methods and artificial intelligence techniques like neuroimaging analysis to provide more accurate and interpretable results. Experiments were conducted comparing our proposed method with subjective human ratings and established objective metrics demonstrating its effectiveness in measuring generation fidelity, perceptual plausibility, memorability, and overall pleasantness. Our findings suggest that combining neuroscientific knowledge with AI technology could enhance the evaluation capabilities and facilitate better applications of GANs in various domains. The development of SynNS represents a first step towards building a comprehensive framework for assessing AI systems through neuro-AI interface approaches.",1
"We consider referring image segmentation. It is a problem at the intersection of computer vision and natural language understanding. Given an input image and a referring expression in the form of a natural language sentence, the goal is to segment the object of interest in the image referred by the linguistic query. To this end, we propose a dual convolutional LSTM (ConvLSTM) network to tackle this problem. Our model consists of an encoder network and a decoder network, where ConvLSTM is used in both encoder and decoder networks to capture spatial and sequential information. The encoder network extracts visual and linguistic features for each word in the expression sentence, and adopts an attention mechanism to focus on words that are more informative in the multimodal interaction. The decoder network integrates the features generated by the encoder network at multiple levels as its input and produces the final precise segmentation mask. Experimental results on four challenging datasets demonstrate that the proposed network achieves superior segmentation performance compared with other state-of-the-art methods.",0
"In this paper, we propose a novel dual convolutional LSTM network for referring image segmentation. We address the problem of referring expression comprehension in images by using two different branches of our model: one branch focuses on spatial attention for localization, while another branch predicts semantic attributes relevant to the task at hand. Our architecture uses recurrent layers to integrate temporal context and fuse these separate sources of information together for accurate object instance segmentation. Experimental results demonstrate that our approach outperforms previous state-of-the-art methods for both refining object boundaries and predicting semantic attributes for referring expressions. This work has implications for applications such as robotic manipulation tasks and visual question answering systems.",1
"Understanding intrinsic patterns and predicting spatiotemporal characteristics of cities require a comprehensive representation of urban neighborhoods. Existing works relied on either inter- or intra-region connectivities to generate neighborhood representations but failed to fully utilize the informative yet heterogeneous data within neighborhoods. In this work, we propose Urban2Vec, an unsupervised multi-modal framework which incorporates both street view imagery and point-of-interest (POI) data to learn neighborhood embeddings. Specifically, we use a convolutional neural network to extract visual features from street view images while preserving geospatial similarity. Furthermore, we model each POI as a bag-of-words containing its category, rating, and review information. Analog to document embedding in natural language processing, we establish the semantic similarity between neighborhood (""document"") and the words from its surrounding POIs in the vector space. By jointly encoding visual, textual, and geospatial information into the neighborhood representation, Urban2Vec can achieve performances better than baseline models and comparable to fully-supervised methods in downstream prediction tasks. Extensive experiments on three U.S. metropolitan areas also demonstrate the model interpretability, generalization capability, and its value in neighborhood similarity analysis.",0
"This article presents the model called ""Urban2Vec"" which represents urban areas using street view imagery as well as points of interest (POIs) data to generate more accurate embeddings than existing approaches based on either type of data alone. By combining these two types of data, Urban2Vec captures both visual and functional aspects of neighborhoods for improved geospatial understanding. The authors evaluate their approach on two datasets from different cities and demonstrate that Urban2Vec outperforms state-of-the-art alternatives across multiple metrics, including spatial proximity measures, retrieval performance, and clustering tasks. Overall, the results highlight the benefits of leveraging diverse sources of urban information to improve multi-modal representations for advanced applications such as geocomputing, location recommendation, and environmental planning.",1
"Variance reduction methods such as SVRG and SpiderBoost use a mixture of large and small batch gradients to reduce the variance of stochastic gradients. Compared to SGD, these methods require at least double the number of operations per update to model parameters. To reduce the computational cost of these methods, we introduce a new sparsity operator: The random-top-k operator. Our operator reduces computational complexity by estimating gradient sparsity exhibited in a variety of applications by combining the top-k operator and the randomized coordinate descent operator. With this operator, large batch gradients offer an extra benefit beyond variance reduction: A reliable estimate of gradient sparsity. Theoretically, our algorithm is at least as good as the best algorithm (SpiderBoost), and further excels in performance whenever the random-top-k operator captures gradient sparsity. Empirically, our algorithm consistently outperforms SpiderBoost using various models on various tasks including image classification, natural language processing, and sparse matrix factorization. We also provide empirical evidence to support the intuition behind our algorithm via a simple gradient entropy computation, which serves to quantify gradient sparsity at every iteration.",0
"Here is my attempt at writing an abstract for a scientific paper on variance reduction methods that use sparse gradients:  Title: ""Optimization Techniques for Machine Learning""  Abstract: This research investigates several novel optimization techniques for machine learning models trained using gradient descent. In particular, we focus on reducing computational complexity by exploiting sparsity in model parameters and gradients. Our methods significantly improve convergence rates while retaining comparable generalization performance. We compare our results against existing approaches in popular deep learning frameworks and demonstrate their effectiveness across a range of real-world applications. Overall, these variance reduction techniques offer promising improvements over traditional optimization methods in modern machine learning practice.",1
"Interactive Fiction games are text-based simulations in which an agent interacts with the world purely through natural language. They are ideal environments for studying how to extend reinforcement learning agents to meet the challenges of natural language understanding, partial observability, and action generation in combinatorially-large text-based action spaces. We present KG-A2C, an agent that builds a dynamic knowledge graph while exploring and generates actions using a template-based action space. We contend that the dual uses of the knowledge graph to reason about game state and to constrain natural language generation are the keys to scalable exploration of combinatorially large natural language actions. Results across a wide variety of IF games show that KG-A2C outperforms current IF agents despite the exponential increase in action space size.",0
"This paper presents a new model that combines graph constrained reinforcement learning with natural language action spaces. By leveraging recent advances in deep learning, we aim to improve the performance of traditional RL algorithms on challenging problems such as robotics manipulation tasks or high level decision making across multiple domains. Our approach provides a flexible framework which can capture complex, task dependent relationships among variables while improving stability and sample efficiency compared to existing methods. We evaluate our method using two classical control benchmarks and show that it significantly outperforms other state-of-the-art algorithms. Overall, this work demonstrates how combining constraint reasoning with deep learning techniques can lead to more effective solutions in RL applications.",1
"In natural language processing, it has been observed recently that generalization could be greatly improved by finetuning a large-scale language model pretrained on a large unlabeled corpus. Despite its recent success and wide adoption, finetuning a large pretrained language model on a downstream task is prone to degenerate performance when there are only a small number of training instances available. In this paper, we introduce a new regularization technique, to which we refer as ""mixout"", motivated by dropout. Mixout stochastically mixes the parameters of two models. We show that our mixout technique regularizes learning to minimize the deviation from one of the two models and that the strength of regularization adapts along the optimization trajectory. We empirically evaluate the proposed mixout and its variants on finetuning a pretrained language model on downstream tasks. More specifically, we demonstrate that the stability of finetuning and the average accuracy greatly increase when we use the proposed approach to regularize finetuning of BERT on downstream tasks in GLUE.",0
"Abstract: Deep neural networks have become highly effective models for Natural Language Processing (NLP) tasks due to their ability to learn complex relationships within vast amounts of data. However, these models often require significant computational resources during training which can make them impractical for many users. In this work we propose Mixout as a lightweight regularization technique that allows us to finetune large scale pretrained language models with minimal computational overhead while maintaining strong performance on NLP benchmarks such as GLUE. Our approach works by randomly dropping out parts of the input text at runtime as well as select layers from the model itself. By doing so we induce robustness against noise and encourage the network to generalize better through learned representations rather than overfitting to specific inputs. Experimental results show that our proposed method improves upon prior state-of-the-art methods for fine tuning large LLMs without requiring additional compute. Our code has been made publicly available for further experimentation. -----Please provide a summary for ""Mixout: Effective Regularization to Fine Tune Large Scale Pre Trained Language Models""",1
"Learning from one or few visual examples is one of the key capabilities of humans since early infancy, but is still a significant challenge for modern AI systems. While considerable progress has been achieved in few-shot learning from a few image examples, much less attention has been given to the verbal descriptions that are usually provided to infants when they are presented with a new object. In this paper, we focus on the role of additional semantics that can significantly facilitate few-shot visual learning. Building upon recent advances in few-shot learning with additional semantic information, we demonstrate that further improvements are possible by combining multiple and richer semantics (category labels, attributes, and natural language descriptions). Using these ideas, we offer the community new results on the popular miniImageNet and CUB few-shot benchmarks, comparing favorably to the previous state-of-the-art results for both visual only and visual plus semantics-based approaches. We also performed an ablation study investigating the components and design choices of our approach.",0
"In recent years, there have been significant advancements in artificial intelligence (AI) with regards to few-shot learning, which enables machines to learn from small amounts of data. However, most existing approaches assume that all examples within each task share the same semantic meaning, which may not always be true in real-world scenarios where tasks can have multiple interpretations. This study presents an approach that addresses this limitation by enabling AIs to learn multiple tasks with different semantics using only a few examples per task. We first introduce a novel multi-semantics model that learns to generate descriptions of tasks given their input images. Using these generated descriptions as guidance, we then train an agent to perform both seen and unseen tasks without requiring large datasets. Our experiments demonstrate that our approach significantly outperforms state-of-the-art methods on several benchmarks while reducing computational resources. Overall, our work represents baby steps towards achieving more generalizable machine learning systems that can adapt to diverse environments.",1
"Generative adversarial networks (GANs) are a hot research topic recently. GANs have been widely studied since 2014, and a large number of algorithms have been proposed. However, there is few comprehensive study explaining the connections among different GANs variants, and how they have evolved. In this paper, we attempt to provide a review on various GANs methods from the perspectives of algorithms, theory, and applications. Firstly, the motivations, mathematical representations, and structure of most GANs algorithms are introduced in details. Furthermore, GANs have been combined with other machine learning algorithms for specific applications, such as semi-supervised learning, transfer learning, and reinforcement learning. This paper compares the commonalities and differences of these GANs methods. Secondly, theoretical issues related to GANs are investigated. Thirdly, typical applications of GANs in image processing and computer vision, natural language processing, music, speech and audio, medical field, and data science are illustrated. Finally, the future open research problems for GANs are pointed out.",0
"Abstract: This paper provides an overview of generative adversarial networks (GAN), which have emerged as one of the most significant developments in deep learning. GAN consist of two subnetworks; one generates data such as images or sounds while the other attempts to detect whether the generated sample is real or fake. By training these two components against each other, the generator learns how to create more convincing samples that can fool the discriminator network. This has resulted in numerous breakthroughs across diverse fields from computer graphics to natural language processing, where they are used for tasks such as image synthesis, super resolution, text generation, and translation. The study conducts an exhaustive analysis of different variations of GAN architectures, discussing their strengths and weaknesses, applications, pitfalls and challenges encountered during training. Additionally, recent advancements in algorithms, theory and techniques employed to tackle limitations faced by researchers are presented and critically analyzed. Finally, potential future directions and open research questions are provided, concluding that despite current challenges faced by practitioners, GAN continues to remain among the fastest growing field with immense opportunities awaiting innovators in industry.",1
"One of the key challenges in designing machine learning systems is to determine the right balance amongst several objectives, which also oftentimes are incommensurable and conflicting. For example, when designing deep neural networks (DNNs), one often has to trade-off between multiple objectives, such as accuracy, energy consumption, and inference time. Typically, there is no single configuration that performs equally well for all objectives. Consequently, one is interested in identifying Pareto-optimal designs. Although different multi-objective optimization algorithms have been developed to identify Pareto-optimal configurations, state-of-the-art multi-objective optimization methods do not consider the different evaluation costs attending the objectives under consideration. This is particularly important for optimizing DNNs: the cost arising on account of assessing the accuracy of DNNs is orders of magnitude higher than that of measuring the energy consumption of pre-trained DNNs. We propose FlexiBO, a flexible Bayesian optimization method, to address this issue. We formulate a new acquisition function based on the improvement of the Pareto hyper-volume weighted by the measurement cost of each objective. Our acquisition function selects the next sample and objective that provides maximum information gain per unit of cost. We evaluated FlexiBO on 7 state-of-the-art DNNs for object detection, natural language processing, and speech recognition. Our results indicate that, when compared to other state-of-the-art methods across the 7 architectures we tested, the Pareto front obtained using FlexiBO has, on average, a 28.44% higher contribution to the true Pareto front and achieves 25.64% better diversity.",0
"This paper presents FlexiBO, a novel method that takes advantage of multi-objective optimization techniques using hyperparameter tuning of deep neural networks (DNNs) while focusing on cost minimization. In particular, we introduce a flexible weighted sum approach for optimizing DNNs under multiple objectives such as accuracy and efficiency. Our framework effectively balances model quality and computational costs by incorporating an adaptive learning rate scheme into our objective function. Through extensive experiments, we demonstrate that our algorithm significantly outperforms state-of-the-art single-objective methods across multiple datasets and architectures. Furthermore, we showcase how FlexiBO can identify models tailored towards specific use cases without sacrificing overall performance. Our work highlights the importance of considering both objectives during the training process and provides insights into designing efficient machine learning systems.",1
"Video moment retrieval is to search the moment that is most relevant to the given natural language query. Existing methods are mostly trained in a fully-supervised setting, which requires the full annotations of temporal boundary for each query. However, manually labeling the annotations is actually time-consuming and expensive. In this paper, we propose a novel weakly-supervised moment retrieval framework requiring only coarse video-level annotations for training. Specifically, we devise a proposal generation module that aggregates the context information to generate and score all candidate proposals in one single pass. We then devise an algorithm that considers both exploitation and exploration to select top-K proposals. Next, we build a semantic completion module to measure the semantic similarity between the selected proposals and query, compute reward and provide feedbacks to the proposal generation module for scoring refinement. Experiments on the ActivityCaptions and Charades-STA demonstrate the effectiveness of our proposed method.",0
"We present a weakly supervised method for video moment retrieval using semantic completion networks (SCN). Our approach leverages natural language queries to search for moments within unlabeled videos that match specific concepts or scenarios described by the query. SCNs have been shown to effectively encode both visual and textual features, making them well suited for our task. We train a network to predict missing visual elements based on inputted text descriptions, allowing us to retrieve semantically relevant moments from a given video collection. Through extensive experiments, we demonstrate the effectiveness of our approach compared to several state-of-the-art methods for video moment retrieval under weak supervision. Our results highlight the promise of utilizing SCNs as a powerful tool for weakly-supervised learning in computer vision tasks such as video retrieval and annotation.",1
"The recent success of deep neural networks (DNNs) for function approximation in reinforcement learning has triggered the development of Deep Reinforcement Learning (DRL) algorithms in various fields, such as robotics, computer games, natural language processing, computer vision, sensing systems, and wireless networking. Unfortunately, DNNs suffer from high computational cost and memory consumption, which limits the use of DRL algorithms in systems with limited hardware resources. In recent years, pruning algorithms have demonstrated considerable success in reducing the redundancy of DNNs in classification tasks. However, existing algorithms suffer from a significant performance reduction in the DRL domain. In this paper, we develop the first effective solution to the performance reduction problem of pruning in the DRL domain, and establish a working algorithm, named Policy Pruning and Shrinking (PoPS), to train DRL models with strong performance while achieving a compact representation of the DNN. The framework is based on a novel iterative policy pruning and shrinking method that leverages the power of transfer learning when training the DRL model. We present an extensive experimental study that demonstrates the strong performance of PoPS using the popular Cartpole, Lunar Lander, Pong, and Pacman environments. Finally, we develop an open source software for the benefit of researchers and developers in related fields.",0
"This paper presents PoPS (Policy Pruning and Shrinking), a new method that improves deep reinforcement learning by selectively pruning and shrinking policies. In previous works on policy search such as Proximal Policy Optimization and REINFORCE, the standard approach consists of iteratively updating the entire policy distribution until convergence. However, these methods can suffer from high sample complexity, slow convergence, and instability. In contrast, our proposed method learns compact policies directly from scratch without any assumption on the functional form of the solution. We show experimental results demonstrating the effectiveness of PoPS across multiple benchmark tasks. Our findings suggest that PoPS is more efficient than baseline approaches while achieving better performance in terms of both accuracy and sample efficiency. Furthermore, we provide insights into how different components of the algorithm affect its behavior, which could guide future work in this area.",1
"The success stories from deep learning models increase every day spanning different tasks from image classification to natural language understanding. With the increasing popularity of these models, scientists spend more and more time finding the optimal parameters and best model architectures for their tasks. In this paper, we focus on the ingredient that feeds these machines: the data. We hypothesize that the data ordering affects how well a model performs. To that end, we conduct experiments on an image classification task using ImageNet dataset and show that some data orderings are better than others in terms of obtaining higher classification accuracies. Experimental results show that independent of model architecture, learning rate and batch size, ordering of the data significantly affects the outcome. We show these findings using different metrics: NDCG, accuracy @ 1 and accuracy @ 5. Our goal here is to show that not only parameters and model architectures but also the data ordering has a say in obtaining better results.",0
"The impact of data ordering on image classification has been studied extensively over recent years. In order to accurately assess its impact, we must first consider how images are typically presented in a dataset. One common method involves presenting images in a randomized sequence, meaning that no particular arrangement exists, while another popular option sees pictures grouped by content matter into different categories. This grouping approach often employs features such as shape, texture, color and more recently deep neural network embeddings to make categorical distinctions. In addition, there have been advancements in how these groups can then be shuffled and adjusted to maximize their utility. As a result, there has never been a greater need than now to understand how ordering influences performance across multiple image recognition models. To address this issue, our research compares and contrasts model accuracy under four ordered configurations: (1) randomly permuted images; (2) manually arranged contents by experts;(3)automatically sorted based on feature embeddings;and (4) datasets composed from both man-made and automated procedures. Experimental results demonstrate that each approach provides varying levels of improvement compared against baseline non-ordered benchmarks. Ultimately, we aim to provide insights into which strategies work best according to specific scenarios and goals.",1
"One of the long-term challenges of robotics is to enable robots to interact with humans in the visual world via natural language, as humans are visual animals that communicate through language. Overcoming this challenge requires the ability to perform a wide variety of complex tasks in response to multifarious instructions from humans. In the hope that it might drive progress towards more flexible and powerful human interactions with robots, we propose a dataset of varied and complex robot tasks, described in natural language, in terms of objects visible in a large set of real images. Given an instruction, success requires navigating through a previously-unseen environment to identify an object. This represents a practical challenge, but one that closely reflects one of the core visual problems in robotics. Several state-of-the-art vision-and-language navigation, and referring-expression models are tested to verify the difficulty of this new task, but none of them show promising results because there are many fundamental differences between our task and previous ones. A novel Interactive Navigator-Pointer model is also proposed that provides a strong baseline on the task. The proposed model especially achieves the best performance on the unseen test split, but still leaves substantial room for improvement compared to the human performance.",0
"In recent years, there has been growing interest in developing robots that can navigate through complex environments while interacting naturally with humans. One key challenge facing these robotic systems is the ability to communicate effectively with human users about their surroundings. This paper introduces a novel approach called REVERIE (Remote Embodied Visual Referring Expression in Real Indoor Environments) which enables robots to refer to specific objects and locations within indoor environments using natural language. Our method relies on advanced computer vision techniques such as object detection and segmentation, and leverages the robot's embodiment to provide reliable visual cues. We demonstrate the effectiveness of our approach via experiments conducted in real indoor environments and show that our system outperforms baseline methods in terms of accuracy and user satisfaction. Our work represents an important step towards creating more versatile and communicative robots that can seamlessly integrate into daily human life.",1
"For joint inference over multiple variables, a variety of structured prediction techniques have been developed to model correlations among variables and thereby improve predictions. However, many classical approaches suffer from one of two primary drawbacks: they either lack the ability to model high-order correlations among variables while maintaining computationally tractable inference, or they do not allow to explicitly model known correlations. To address this shortcoming, we introduce `Graph Structured Prediction Energy Networks,' for which we develop inference techniques that allow to both model explicit local and implicit higher-order correlations while maintaining tractability of inference. We apply the proposed method to tasks from the natural language processing and computer vision domain and demonstrate its general utility.",0
"Here's your requested abstract:  In recent years, graph structured prediction energy networks (GSPEN) have emerged as a powerful methodology for modeling complex dependencies among variables in machine learning tasks. GSPEN represents each variable as a node on a directed graph, where the edges represent relationships that encode prior knowledge about the problem at hand. By doing so, it allows models to capture both local and global features of the data, resulting in improved accuracy compared to traditional predictive methods such as linear regression or decision trees. In addition, GSPEN can incorporate domain expertise into their structure by encoding human insights into its design process. As a result, these models are particularly well suited for use cases in which reliable predictions are crucial but obtaining labeled training data is difficult or expensive. However, there remain several open challenges associated with deploying GSPEN in practice. This work seeks to address some of these issues by proposing novel algorithms and techniques that enhance the performance of GSPEN in real-world settings while preserving its interpretability and ease of implementation. Our findings demonstrate the effectiveness of our approach across a range of applications including natural language processing, computer vision, and bioinformatics, offering promising new directions for future research in this area.",1
"We present a robust aggregation approach to make federated learning robust to settings when a fraction of the devices may be sending corrupted updates to the server. The proposed approach relies on a robust secure aggregation oracle based on the geometric median, which returns a robust aggregate using a constant number of calls to a regular non-robust secure average oracle. The robust aggregation oracle is privacy-preserving, similar to the secure average oracle it builds upon. We provide experimental results of the proposed approach with linear models and deep networks for two tasks in computer vision and natural language processing. The robust aggregation approach is agnostic to the level of corruption; it outperforms the classical aggregation approach in terms of robustness when the level of corruption is high, while being competitive in the regime of low corruption.",0
"In distributed learning systems such as federated learning, communication efficiency is crucial because of the constraints on network bandwidth and latency. Existing aggregation methods often fail to meet these requirements due to their use of complex, computationally expensive algorithms that require multiple rounds of communication between clients and servers. This can lead to high computational costs, increased memory usage, and slow convergence rates. To address these issues, we propose a new method called robust aggregation (RA) which uses low-rank matrix approximation techniques to reduce the complexity of the aggregation process. Our approach improves both accuracy and communication efficiency by exploiting sparse data structures while maintaining strong privacy guarantees through differential privacy mechanisms. We demonstrate the effectiveness of our method using simulations on several datasets and benchmarks, showing significant improvements over existing state-of-the-art approaches. Our results show that RA outperforms other methods in terms of both computational cost and privacy protection, making it well suited for real-world applications where fast and accurate model training under limited resources and tight deadlines is critical.",1
"Nowadays, the use of machine learning models is becoming a utility in many applications. Companies deliver pre-trained models encapsulated as application programming interfaces (APIs) that developers combine with third party components and their own models and data to create complex data products to solve specific problems. The complexity of such products and the lack of control and knowledge of the internals of each component used cause unavoidable effects, such as lack of transparency, difficulty in auditability, and emergence of potential uncontrolled risks. They are effectively black-boxes. Accountability of such solutions is a challenge for the auditors and the machine learning community. In this work, we propose a wrapper that given a black-box model enriches its output prediction with a measure of uncertainty. By using this wrapper, we make the black-box auditable for the accuracy risk (risk derived from low quality or uncertain decisions) and at the same time we provide an actionable mechanism to mitigate that risk in the form of decision rejection; we can choose not to issue a prediction when the risk or uncertainty in that decision is significant. Based on the resulting uncertainty measure, we advocate for a rejection system that selects the more confident predictions, discarding those more uncertain, leading to an improvement in the trustability of the resulting system. We showcase the proposed technique and methodology in a practical scenario where a simulated sentiment analysis API based on natural language processing is applied to different domains. Results demonstrate the effectiveness of the uncertainty computed by the wrapper and its high correlation to bad quality predictions and misclassifications.",0
"An essential aspect of machine learning algorithms is their ability to provide accurate predictions on new data points. However, these models can sometimes make errors that may have negative consequences for users and society as a whole. To address this issue, researchers have proposed several methods such as model interpretability and transparency techniques, but they often lack scalability and efficiency. In this work, we introduce the concept of Dirichlet uncertainty wrappers to improve the accountability and auditability of machine learning models by providing explicit quantification of the aleatoric uncertainty inherent in each prediction. Our approach combines the advantages of both predictive uncertainty estimation methods and Bayesian decision theory principles to enable explainable decisions with reliable confidence bounds. We evaluate our method using experiments on real datasets, demonstrating significant improvements over baseline approaches in terms of classification performance, calibration measures, and interpretability metrics. Our framework provides a general toolbox applicable across different domains and has important implications for ethical considerations related to automation in critical applications, such as healthcare, finance, and law enforcement. By encouraging practitioners to implement explicit Aleatory uncertainty estimation, the goal is to contribute towards a more responsible use of machine learning technology in society.",1
"Significant advances have been made in artificial systems by using biological systems as a guide. However, there is often little interaction between computational models for emergent communication and biological models of the emergence of language. Many researchers in language origins and emergent communication take compositionality as their primary target for explaining how simple communication systems can become more like natural language. However, there is reason to think that compositionality is the wrong target on the biological side, and so too the wrong target on the machine-learning side. As such, the purpose of this paper is to explore this claim. This has theoretical implications for language origins research more generally, but the focus here will be the implications for research on emergent communication in computer science and machine learning---specifically regarding the types of programmes that might be expected to work and those which will not. I further suggest an alternative approach for future research which focuses on reflexivity, rather than compositionality, as a target for explaining how simple communication systems may become more like natural language. I end by providing some reference to the language origins literature that may be of some use to researchers in machine learning.",0
"This would make a fine introduction section. Please provide a summary of the main points you would like me to cover in the rest of the abstract. In addition to an overview of the research topic I should also discuss any hypotheses presented and how the authors intend on testing them. If there aren't specific hypotheses then I can focus more generally on contributions. Also, please clarify if there should be suggestions for future work.",1
"Vision and language are two fundamental capabilities of human intelligence. Humans routinely perform tasks through the interactions between vision and language, supporting the uniquely human capacity to talk about what they see or hallucinate a picture on a natural-language description. The valid question of how language interacts with vision motivates us researchers to expand the horizons of computer vision area. In particular, ""vision to language"" is probably one of the most popular topics in the past five years, with a significant growth in both volume of publications and extensive applications, e.g., captioning, visual question answering, visual dialog, language navigation, etc. Such tasks boost visual perception with more comprehensive understanding and diverse linguistic representations. Going beyond the progresses made in ""vision to language,"" language can also contribute to vision understanding and offer new possibilities of visual content creation, i.e., ""language to vision."" The process performs as a prism through which to create visual content conditioning on the language inputs. This paper reviews the recent advances along these two dimensions: ""vision to language"" and ""language to vision."" More concretely, the former mainly focuses on the development of image/video captioning, as well as typical encoder-decoder structures and benchmarks, while the latter summarizes the technologies of visual content creation. The real-world deployment or services of vision and language are elaborated as well.",0
"This comprehensive review examines the current state of vision and language research through two main lenses - visual perception and content creation. We begin by discussing recent advances in computer vision systems that allow machines to interpret images and videos. These developments have enabled new applications such as object detection, image generation, and video understanding. Next, we explore how natural language processing has been integrated into these systems, allowing them to generate human-like descriptions of multimedia data. Finally, we look at some exciting future directions for this field, including multimodal learning and cross-modal reasoning, which promise even more advanced abilities for artificial intelligence systems. Overall, our goal is to provide readers with a broad overview of this rapidly evolving area of study.",1
"Deep neural networks (DNNs) have emerged as a popular mathematical tool for function approximation due to their capability of modelling highly nonlinear functions. Their applications range from image classification and natural language processing to learning-based control. Despite their empirical successes, there is still a lack of theoretical understanding of the representative power of such deep architectures. In this work, we provide a theoretical analysis of the expressiveness of fully-connected, feedforward DNNs with 1-Lipschitz activation functions. In particular, we characterize the expressiveness of a DNN by its Lipchitz constant. By leveraging random matrix theory, we show that, given sufficiently large and randomly distributed weights, the expected upper and lower bounds of the Lipschitz constant of a DNN and hence their expressiveness increase exponentially with depth and polynomially with width, which gives rise to the benefit of the depth of DNN architectures for efficient function approximation. This observation is consistent with established results based on alternative expressiveness measures of DNNs. In contrast to most of the existing work, our analysis based on the Lipschitz properties of DNNs is applicable to a wider range of activation nonlinearities and potentially allows us to make sensible comparisons between the complexity of a DNN and the function to be approximated by the DNN. We consider this work to be a step towards understanding the expressive power of DNNs and towards designing appropriate deep architectures for practical applications such as system control.",0
"This can include background details such as why Lipschitz constants are used in deep neural network architectures, any related research that has been conducted before, and how your work builds off of those findings (if applicable). Please include at least three citations to academic literature within the abstract itself. Output should be less than 60K characters. *This* study examines the expressiveness of several popular deep neural network architectures by analyzing their Lipschitz constants. We aim to provide insights into the capacity of these models to represent complex functions and distinguish among different families of networks based on their generalization performance. Our analysis leverages the existing body of research exploring the properties of deep learning algorithms with theoretical guarantees and extends upon previous studies investigating the connection between model expressivity and robustness to overparameterization. Using state-of-the-art methods for estimating Lipschitz constants, we evaluate a range of commonly used DNNs across multiple datasets and tasks, from image classification to language generation. Our results demonstrate clear distinctions in terms of expressive power among architecture classes, highlighting specific design choices critical to achieving high-fidelity function approximation. These findings contribute to our understanding of the tradeoffs between model complexity and expressivity, providing valuable guidance for future developments in artificial intelligence. Key contributions include evidence linking strong regularizers and implicit biases to favorable behavior, thus pointing towards avenues for further improving DNN capabilities. Collectively, our research provides actionable recommendations for practitioners seeking optimal architecture designs and offers important implications for both theory and applications of machine learning.",1
"Learning an embedding for a large collection of items is a popular approach to overcome the computational limitations associated to one-hot encodings. The aim of item embedding is to learn a low dimensional space for the representations, able to capture with its geometry relevant features or relationships for the data at hand. This can be achieved for example by exploiting adjacencies among items in large sets of unlabelled data. In this paper we interpret in an Information Geometric framework the item embeddings obtained from conditional models. By exploiting the $\alpha$-geometry of the exponential family, first introduced by Amari, we introduce a family of natural $\alpha$-embeddings represented by vectors in the tangent space of the probability simplex, which includes as a special case standard approaches available in the literature. A typical example is given by word embeddings, commonly used in natural language processing, such as Word2Vec and GloVe. In our analysis, we show how the $\alpha$-deformation parameter can impact on standard evaluation tasks.",0
"This paper explores natural language processing techniques that leverage large amounts of data to produce high-quality vector representations of words, phrases, sentences, and paragraphs. We investigate both the effectiveness and feasibility of training embeddings on unstructured text, such as found in books, articles, websites, social media posts, emails, instant messages, etc., which accounts for over 98% of all written content produced by humans every day. Our findings suggest that these embeddings capture meaningful relationships and patterns across a wide range of domains, including but not limited to: topic modeling; sentiment analysis; named entity recognition; machine translation; paraphrasing and question answering. They can even outperform handcrafted features and other state-of-the-art methods on some tasks while requiring significantly less effort and resources. Furthermore, we demonstrate how simple linear algebra operations applied to these vectors yield interpretable semantic insights, allowing us to study complex linguistic phenomena like metaphors, idioms, and homonymy at scale. Finally, we provide open source code and pretrained models so others may build upon our work and adapt it to their needs.",1
"Learning text representation is crucial for text classification and other language related tasks. There are a diverse set of text representation networks in the literature, and how to find the optimal one is a non-trivial problem. Recently, the emerging Neural Architecture Search (NAS) techniques have demonstrated good potential to solve the problem. Nevertheless, most of the existing works of NAS focus on the search algorithms and pay little attention to the search space. In this paper, we argue that the search space is also an important human prior to the success of NAS in different applications. Thus, we propose a novel search space tailored for text representation. Through automatic search, the discovered network architecture outperforms state-of-the-art models on various public datasets on text classification and natural language inference tasks. Furthermore, some of the design principles found in the automatic network agree well with human intuition.",0
"Abstract: In recent years, text representation has emerged as one of the most crucial components in natural language processing (NLP). As a result, several models have been developed based on different techniques such as recurrent neural networks (RNNs) or transformers, which require manual architectural engineering to achieve state-of-the-art results. To tackle this issue, we introduce TextNAS, a novel automated model search method tailored specifically for text representation tasks. We begin by designing a search space consisting of operation building blocks that can mix and match existing architectures like RNNs, CNNs, or Transformers. Subsequently, using reinforcement learning principles, our algorithm selects architectures from this search space that maximize validation performance while adhering to constraints, such as computational efficiency or parameter size budget. Finally, we present extensive evaluations across multiple benchmark datasets, demonstrating TextNAS’ superiority over both strong handcrafted baselines and even randomly sampled models within the same search space. With these promising results, TextNAS paves the way towards more efficient NLP architecture development through automated methods, ultimately leading to stronger AI systems overall.",1
"Recently, pre-trained language representation flourishes as the mainstay of the natural language understanding community, e.g., BERT. These pre-trained language representations can create state-of-the-art results on a wide range of downstream tasks. Along with continuous significant performance improvement, the size and complexity of these pre-trained neural models continue to increase rapidly. Is it possible to compress these large-scale language representation models? How will the pruned language representation affect the downstream multi-task transfer learning objectives? In this paper, we propose Reweighted Proximal Pruning (RPP), a new pruning method specifically designed for a large-scale language representation model. Through experiments on SQuAD and the GLUE benchmark suite, we show that proximal pruned BERT keeps high accuracy for both the pre-training task and the downstream multiple fine-tuning tasks at high prune ratio. RPP provides a new perspective to help us analyze what large-scale language representation might learn. Additionally, RPP makes it possible to deploy a large state-of-the-art language representation model such as BERT on a series of distinct devices (e.g., online servers, mobile phones, and edge devices).",0
"In recent years, there has been significant interest in developing large language models that can generate human-like text based on input prompts. However, training these models requires massive amounts of computational resources and data. To address this challenge, researchers have proposed various techniques such as pruning and distillation to reduce model size without sacrificing performance. One popular method is proximal pruning, which removes unimportant connections in deep neural networks while preserving their accuracy. In this work, we propose reweighted proximal pruning (RPP), a novel technique that enhances the efficiency of proximal pruning by dynamically adjusting the strength of each connection during the pruning process. We evaluate RPP on several benchmark datasets and demonstrate that it achieves higher compression ratios compared to existing methods, without negatively impacting model performance. Our findings suggest that RPP can serve as a powerful tool for scaling up large language models to even larger sizes and reducing their memory footprint in real-world applications. Overall, this work contributes to our understanding of efficient model optimization techniques for large language representation tasks.",1
"In this paper, we propose a novel controllable text-to-image generative adversarial network (ControlGAN), which can effectively synthesise high-quality images and also control parts of the image generation according to natural language descriptions. To achieve this, we introduce a word-level spatial and channel-wise attention-driven generator that can disentangle different visual attributes, and allow the model to focus on generating and manipulating subregions corresponding to the most relevant words. Also, a word-level discriminator is proposed to provide fine-grained supervisory feedback by correlating words with image regions, facilitating training an effective generator which is able to manipulate specific visual attributes without affecting the generation of other content. Furthermore, perceptual loss is adopted to reduce the randomness involved in the image generation, and to encourage the generator to manipulate specific attributes required in the modified text. Extensive experiments on benchmark datasets demonstrate that our method outperforms existing state of the art, and is able to effectively manipulate synthetic images using natural language descriptions. Code is available at https://github.com/mrlibw/ControlGAN.",0
"One of the most recent advances in artificial intelligence is text-to-image generation using deep neural networks (DNNs). These DNNs have the ability to generate realistic images directly from natural language descriptions by learning semantic representations in unsupervised settings. However, one problem associated with these approaches is their lack of interpretability and difficulty in controlling specific attributes within generated images. In our work, we propose a new architecture that combines VAEs with attention mechanisms that allow fine-grained control over different aspects of generated images while maintaining high image quality. We evaluate our approach on several benchmark datasets and demonstrate significant improvement compared to state-of-the-art methods. Our method has potential applications in various domains such as computer graphics, robotics, and multimedia content creation.",1
"Exchanging gradients is a widely used method in modern multi-node machine learning system (e.g., distributed training, collaborative learning). For a long time, people believed that gradients are safe to share: i.e., the training data will not be leaked by gradient exchange. However, we show that it is possible to obtain the private training data from the publicly shared gradients. We name this leakage as Deep Leakage from Gradient and empirically validate the effectiveness on both computer vision and natural language processing tasks. Experimental results show that our attack is much stronger than previous approaches: the recovery is pixel-wise accurate for images and token-wise matching for texts. We want to raise people's awareness to rethink the gradient's safety. Finally, we discuss several possible strategies to prevent such deep leakage. The most effective defense method is gradient pruning.",0
"Outline: I. Introduction II. Background III. Motivation IV. Methodology V. Results VI. Discussion VII. Related Work VIII. Conclusion Abstract: This study presents an analysis of the phenomenon known as deep leakage from gradients (DLG). DLG refers to the unwanted disclosure of sensitive information through machine learning models that utilize gradient descent during training. While the issue has been widely reported in recent years, there is little comprehensive research on the subject. To address this gap in knowledge, we conducted a thorough investigation into the prevalence of DLG across different types of machine learning algorithms and datasets. Our results showed that DLG can occur even with relatively low levels of access to model parameters and gradients. We also found that traditional privacy protection methods, such as differential privacy, were insufficient in preventing DLG. Overall, our findings highlight the urgent need for further research into effective mitigation strategies for DLG in order to ensure the secure use of machine learning models. Keywords: deep leakage from gradients, machine learning, security breaches, gradient descent, differential privacy",1
"Reasoning with knowledge expressed in natural language and Knowledge Bases (KBs) is a major challenge for Artificial Intelligence, with applications in machine reading, dialogue, and question answering. General neural architectures that jointly learn representations and transformations of text are very data-inefficient, and it is hard to analyse their reasoning process. These issues are addressed by end-to-end differentiable reasoning systems such as Neural Theorem Provers (NTPs), although they can only be used with small-scale symbolic KBs. In this paper we first propose Greedy NTPs (GNTPs), an extension to NTPs addressing their complexity and scalability limitations, thus making them applicable to real-world datasets. This result is achieved by dynamically constructing the computation graph of NTPs and including only the most promising proof paths during inference, thus obtaining orders of magnitude more efficient models. Then, we propose a novel approach for jointly reasoning over KBs and textual mentions, by embedding logic facts and natural language sentences in a shared embedding space. We show that GNTPs perform on par with NTPs at a fraction of their cost while achieving competitive link prediction results on large datasets, providing explanations for predictions, and inducing interpretable models. Source code, datasets, and supplementary material are available online at https://github.com/uclnlp/gntp.",0
"This paper presents a novel framework that allows differentiable reasoning over large knowledge bases and natural language data. We demonstrate how our approach can learn representations of complex relationships within knowledge graphs as well as generate human-like responses in natural language tasks such as question answering and text generation. Our model uses a differentiable reasoner which can take into account uncertainty and make predictions using probabilistic inference. We evaluate our method against several benchmark datasets and show competitive results compared to state-of-the-art models. Our work paves the way towards more flexible and powerful artificial intelligence systems capable of learning from vast amounts of structured and unstructured data sources. This paper describes a new method for integrating structured and unstructured data, specifically focusing on leveraging natural language techniques for understanding large knowledge graphs.  The authors propose a ""differentiable reasoner"" that allows machine learning algorithms to be trained on these combined datasets to improve accuracy and flexibility. This system enables the use of probabilities and uncertain reasoning throughout its calculations and predictions, allowing for better decision making across diverse input domains. They test their method against established baselines and find significant improvements in performance on multiple NLP evaluation metrics.  Overall, this research offers important advances in merging disparate but relevant data types in order to enhance AI capabilities and offer real world benefits. While there is still much work to be done in refining and extending this technique, the initial results indicate tremendous potential value and impact going forward.",1
"Altering the content of an image with photo editing tools is a tedious task for an inexperienced user. Especially, when modifying the visual attributes of a specific object in an image without affecting other constituents such as background etc. To simplify the process of image manipulation and to provide more control to users, it is better to utilize a simpler interface like natural language. Therefore, in this paper, we address the challenge of manipulating images using natural language description. We propose the Two-sidEd Attentive conditional Generative Adversarial Network (TEA-cGAN) to generate semantically manipulated images while preserving other contents such as background intact. TEA-cGAN uses fine-grained attention both in the generator and discriminator of Generative Adversarial Network (GAN) based framework at different scales. Experimental results show that TEA-cGAN which generates 128x128 and 256x256 resolution images outperforms existing methods on CUB and Oxford-102 datasets both quantitatively and qualitatively.",0
"Image manipulation through natural language has been an active area of research in recent years, driven by the growing demand for interactive systems that can generate images based on textual descriptions. One promising approach to achieve this goal is by utilizing generative adversarial networks (GANs) which consist of two subnetworks, a generator and a discriminator, that work together to produce realistic-looking images. In this paper, we propose a novel method for image manipulation called Two-Sided Attentive Conditional GAN (TSAC-GAN), designed to incorporate attention mechanisms into both the generator and discriminator modules of the network. Our proposed TSAC-GAN architecture outperforms previous methods in terms of visual quality, generating more coherent and plausible images from textual prompts. We demonstrate the effectiveness of our model on several benchmark datasets, including CelebAHQ, LSUN Churches, and DALL-E2, achieving state-of-the-art results. Our contributions not only advance the field of image generation but also provide insights into how human attentional processes may influence perception, potentially inspiring new directions for future research in cognitive science and neuroscience. This study paves the way for building intelligent systems capable of creating customized visual content under user guidance.",1
"App classification is useful in a number of applications such as adding apps to an app store or building a user model based on the installed apps. Presently there are a number of existing methods to classify apps based on a given taxonomy on the basis of their text metadata. However, text based methods for app classification may not work in all cases, such as when the text descriptions are in a different language, or missing, or inadequate to classify the app. One solution in such cases is to utilize the app images to supplement the text description. In this paper, we evaluate a number of approaches in which app images can be used to classify the apps. In one approach, we use Optical character recognition (OCR) to extract text from images, which is then used to supplement the text description of the app. In another, we use pic2vec to convert the app images into vectors, then train an SVM to classify the vectors to the correct app label. In another, we use the captionbot.ai tool to generate natural language descriptions from the app images. Finally, we use a method to detect and label objects in the app images and use a voting technique to determine the category of the app based on all the images. We compare the performance of our image-based techniques to classify a number of apps in our dataset. We use a text based SVM app classifier as our base and obtained an improved classification accuracy of 96% for some classes when app images are added.",0
"This paper presents a study evaluating the usage of images for app classification. The aim was to investigate whether using images could improve the accuracy of classifying apps compared to traditional text-based methods. To achieve this goal, we collected a dataset of Android apps consisting of both app icons and screenshots. We then trained several machine learning models on this data and evaluated their performance using cross-validation techniques. Our results show that incorporating image features significantly improves the accuracy of app classification over text-only approaches. Furthermore, our analysis reveals interesting patterns in how different types of images are related to specific categories of apps, which can inform future research directions in this field. Overall, this work demonstrates the potential of image-based classification for mobile applications and highlights opportunities for further advancements in this area.",1
"We propose a new contextual-compositional neural network layer that handles out-of-vocabulary (OOV) words in natural language processing (NLP) tagging tasks. This layer consists of a model that attends to both the character sequence and the context in which the OOV words appear. We show that our model learns to generate task-specific \textit{and} sentence-dependent OOV word representations without the need for pre-training on an embedding table, unlike previous attempts. We insert our layer in the state-of-the-art tagging model of \citet{plank2016multilingual} and thoroughly evaluate its contribution on 23 different languages on the task of jointly tagging part-of-speech and morphosyntactic attributes. Our OOV handling method successfully improves performances of this model on every language but one to achieve a new state-of-the-art on the Universal Dependencies Dataset 1.4.",0
"This paper presents a method for generating representations that can capture meaning from specialized out-of-vocabulary (OOV) words found in natural language texts. We propose attending both form (i.e., characters) and context when encoding tokens into vectors. To achieve this goal, we first encode characters in isolation using Convolutional Neural Networks (CNN), and then combine them with their surrounding text by applying self attention mechanisms. We showcase how our approach significantly outperforms strong baselines on two challenging benchmark datasets: OneBench (word similarity and analogy tasks) and SciERC (event extraction). Our results suggest that combining character ngrams and contextual knowledge yields more robust representations for OOV words in downstream Natural Language Processing applications such as those mentioned above. Additionally, we provide insights into which types of words benefit most from this technique (short vs. long OOV words), which architectures work better at capturing the different parts of speech in these sequences, whether pretraining on external data helps boost performance, etc. These findings have implications beyond mere model accuracy metrics, since they help us better understand the complex interplay between language's formal properties (letter strings) and its semantic ones (contextual meanings). This paper proposes a new method for representing out-of-vocabulary (OOV) words in natural language processing tasks. OOV words pose significant challenges because they occur infrequently and lack sufficient context to allow models to accurately predict their meaning. To address this issue, we introduce a novel approach based on combining character-level features with contextual information. Specifically, we use convolutional neural networks (CNN) to encode characters in isolation and then apply self-attention mechanisms to incorporate contextual information. Experimental results demonstrate that our approach consistently outperforms state-ofthe-art baseline methods across multiple benchmark datasets, including OneBench and SciERC. Further analysis reveals important insights into the factors affecting representation quality, such as word length and part of speech tagging. Overall, our study contributes valuable new perspectives on the intricate interactions between linguistic structure and semantics, with potential benefits for future NLP research.",1
"Our research focuses on studying and developing methods for reducing the dimensionality of large datasets, common in biomedical applications. A major problem when learning information about patients based on genetic sequencing data is that there are often more feature variables (genetic data) than observations (patients). This makes direct supervised learning difficult. One way of reducing the feature space is to use latent Dirichlet allocation in order to group genetic variants in an unsupervised manner. Latent Dirichlet allocation is a common model in natural language processing, which describes a document as a mixture of topics, each with a probability of generating certain words. This can be generalized as a Bayesian tensor decomposition to account for multiple feature variables. While we made some progress improving and modifying these methods, our significant contributions are with hierarchical topic modeling. We developed distinct methods of incorporating hierarchical topic modeling, based on nested Chinese restaurant processes and Pachinko Allocation Machine, into Bayesian tensor decompositions. We apply these models to predict whether or not patients have autism spectrum disorder based on genetic sequencing data. We examine a dataset from National Database for Autism Research consisting of paired siblings -- one with autism, and the other without -- and counts of their genetic variants. Additionally, we linked the genes with their Reactome biological pathways. We combine this information into a tensor of patients, counts of their genetic variants, and the membership of these genes in pathways. Once we decompose this tensor, we use logistic regression on the reduced features in order to predict if patients have autism. We also perform a similar analysis of a dataset of patients with one of four common types of cancer (breast, lung, prostate, and colorectal).",0
"Here is an example of how you can write an abstract for your paper:  Conditional hierarchical Bayesian methods have been shown to provide powerful approaches for solving complex problems across a variety of domains. One such method that has recently gained attention is conditional hierarchical Bayesian Tucker decomposition (CHBTD). CHBTD represents a multivariate dataset as a core tensor with auxiliary tensors, which capture relationships among variables of interest and allow for parsimonious modeling by sharing factors across multiple datasets. In addition, CHBTD enables more flexible estimation than traditional Tucker decompositions due to its ability to accommodate missing data and incorporate prior knowledge through informative hyperpriors. This paper describes applications of CHBTD to real-world problems, highlights key advances over existing techniques, and discusses future research directions in this area. We illustrate the utility of CHBTD on both synthetic and real datasets, demonstrating its potential to improve prediction accuracy and interpretability in challenging settings. Our findings suggest that CHBTD offers promising opportunities for data science practitioners working with large, high-dimensional datasets in fields ranging from neuroscience to finance. Overall, our work provides evidence that CHBTD is a valuable tool for data analysis that warrants further exploration in different application areas.",1
"We propose a scalable Bayesian preference learning method for jointly predicting the preferences of individuals as well as the consensus of a crowd from pairwise labels. Peoples' opinions often differ greatly, making it difficult to predict their preferences from small amounts of personal data. Individual biases also make it harder to infer the consensus of a crowd when there are few labels per item. We address these challenges by combining matrix factorisation with Gaussian processes, using a Bayesian approach to account for uncertainty arising from noisy and sparse data. Our method exploits input features, such as text embeddings and user metadata, to predict preferences for new items and users that are not in the training set. As previous solutions based on Gaussian processes do not scale to large numbers of users, items or pairwise labels, we propose a stochastic variational inference approach that limits computational and memory costs. Our experiments on a recommendation task show that our method is competitive with previous approaches despite our scalable inference approximation. We demonstrate the method's scalability on a natural language processing task with thousands of users and items, and show improvements over the state of the art on this task. We make our software publicly available for future work.",0
This sounds like an interesting area of research! If you could provide me some details on the specifics of your work I would be happy to assist you with writing an abstract.,1
"A key requirement for supervised machine learning is labeled training data, which is created by annotating unlabeled data with the appropriate class. Because this process can in many cases not be done by machines, labeling needs to be performed by human domain experts. This process tends to be expensive both in time and money, and is prone to errors. Additionally, reviewing an entire labeled dataset manually is often prohibitively costly, so many real world datasets contain mislabeled instances.   To address this issue, we present in this paper a non-parametric end-to-end pipeline to find mislabeled instances in numerical, image and natural language datasets. We evaluate our system quantitatively by adding a small number of label noise to 29 datasets, and show that we find mislabeled instances with an average precision of more than 0.84 when reviewing our system's top 1\% recommendation. We then apply our system to publicly available datasets and find mislabeled instances in CIFAR-100, Fashion-MNIST, and others. Finally, we publish the code and an applicable implementation of our approach.",0
"Mislabeled instances can significantly affect the performance of machine learning models trained on classification datasets. These labels may either have been incorrectly assigned by mistake, or were deliberately altered. In this paper, we present an approach that enables us to identify instances that are likely mislabeled in large datasets using an active learner system. Our proposed method combines multiple techniques to address different types of label noise in real scenarios. We use uncertainty sampling with Monte Carlo Dropout to assess how uncertain the model is when making predictions. By doing so, we select instances that represent potential areas where our current knowledge might be lacking due to noisy labels. Then, we implement a new algorithm named ""Average Uncertainty Sampling"" which dynamically reweights instances during interaction. This improves label accuracy while minimizing user effort required for label correction. Experiments conducted on several benchmark datasets demonstrate significant improvements over existing methods under various levels of label noise. Additionally, the effectiveness of our approach is validated through human evaluation studies. In summary, this work presents an effective solution for identifying and correcting mislabeled instances in classification datasets used for training machine learning models.",1
"Recent advancements in language representation models such as BERT have led to a rapid improvement in numerous natural language processing tasks. However, language models usually consist of a few hundred million trainable parameters with embedding space distributed across multiple layers, thus making them challenging to be fine-tuned for a specific task or to be transferred to a new domain. To determine whether there are task-specific neurons that can be exploited for unsupervised transfer learning, we introduce a method for selecting the most important neurons to solve a specific classification task. This algorithm is further extended to multi-source transfer learning by computing the importance of neurons for several single-source transfer learning scenarios between different subsets of data sources. Besides, a task-specific fingerprint for each data source is obtained based on the percentage of the selected neurons in each layer. We perform extensive experiments in unsupervised transfer learning for sentiment analysis, natural language inference and sentence similarity, and compare our results with the existing literature and baselines. Significantly, we found that the source and target data sources with higher degrees of similarity between their task-specific fingerprints demonstrate a better transferability property. We conclude that our method can lead to better performance using just a few hundred task-specific and interpretable neurons.",0
"This paper presents a novel method for unsupervised transfer learning using a technique called BERT neuron selection. We explore the idea that by carefully selecting which individual BERT neurons to use as features, we can improve performance on downstream tasks without needing additional labeled data from those specific domains. Our approach uses a combination of statistical analysis and domain knowledge to identify important neurons for different types of problems. Experimental results demonstrate that our selected feature sets consistently outperform strong baseline methods across several benchmark datasets. These findings have implications for how we think about designing and training neural networks for natural language processing tasks, opening up new possibilities for adapting models to diverse domains and use cases. Overall, our work suggests that careful consideration of individual neurons can lead to more effective transfer learning strategies, paving the way for even stronger artificial intelligence systems in the future.",1
"Recent times have witnessed sharp improvements in reinforcement learning tasks using deep reinforcement learning techniques like Deep Q Networks, Policy Gradients, Actor Critic methods which are based on deep learning based models and back-propagation of gradients to train such models. An active area of research in reinforcement learning is about training agents to play complex video games, which so far has been something accomplished only by human intelligence. Some state of the art performances in video game playing using deep reinforcement learning are obtained by processing the sequence of frames from video games, passing them through a convolutional network to obtain features and then using recurrent neural networks to figure out the action leading to optimal rewards. The recurrent neural network will learn to extract the meaningful signal out of the sequence of such features. In this work, we propose a method utilizing a transformer network which have recently replaced RNNs in Natural Language Processing (NLP), and perform experiments to compare with existing methods.",0
"Recent advancements in deep learning have led to significant improvements in many fields such as computer vision, natural language processing and robotics. One area where these models can be particularly beneficial is game playing, which has been a popular research topic since the early days of artificial intelligence. In this paper we present a new method based on reinforcement learning using transformer architectures for training agents capable of competitive performance in a wide range of games. Our approach shows state-of-the-art results across several benchmarks. This work contributes to our understanding of how transformers, which were originally designed for sequence data, may be adapted to learn complex policies. We discuss some implications of these findings for other domains, including their potential impact on improving human decision making. Overall, these results demonstrate that there is still room for improvement in current approaches towards developing intelligent systems that can match human performance in challenging tasks.",1
"Typical active learning strategies are designed for tasks, such as classification, with the assumption that the output space is mutually exclusive. The assumption that these tasks always have exactly one correct answer has resulted in the creation of numerous uncertainty-based measurements, such as entropy and least confidence, which operate over a model's outputs. Unfortunately, many real-world vision tasks, like visual question answering and image captioning, have multiple correct answers, causing these measurements to overestimate uncertainty and sometimes perform worse than a random sampling baseline. In this paper, we propose a new paradigm that estimates uncertainty in the model's internal hidden space instead of the model's output space. We specifically study a manifestation of this problem for visual question answer generation (VQA), where the aim is not to classify the correct answer but to produce a natural language answer, given an image and a question. Our method overcomes the paraphrastic nature of language. It requires a semantic space that structures the model's output concepts and that enables the usage of techniques like dropout-based Bayesian uncertainty. We build a visual-semantic space that embeds paraphrases close together for any existing VQA model. We empirically show state-of-art active learning results on the task of VQA on two datasets, being 5 times more cost-efficient on Visual Genome and 3 times more cost-efficient on VQA 2.0.",0
"This is how I would write an abstract: Title: ""Deep Bayesian Active Learning for Multiple Correct Answers"" The goal of active learning is to maximize the amount learned from limited data by selecting informative samples. In traditional active learning settings, only one correct answer exists per question; however, many modern applications involve multiple possible answers. To address these cases, we introduce a deep Bayesian method that can select queries which allow us to distinguish among competing hypotheses without resorting to heuristics such as uncertainty sampling or query-by-committee. We frame active learning as approximate inference in a deep latent variable model, using recent developments in variational Bayesian methods that facilitate optimization via backpropagation through stochastic computations. Our experiments demonstrate state-of-the-art performance on several benchmark datasets with multiple outputs, including image classification, speech recognition, and machine translation tasks. Code is publicly available at [website]. This work tackles the challenge of active learning in settings where more than one answer could potentially be correct. By adopting a Bayesian approach that models the distribution over all possible answers, our proposed method is able to efficiently learn from just a few examples selected through a novel optimisation scheme tailored for the task at hand. Experimental results across a range of application domains show promising performance against strong baselines.",1
"Searching persons in large-scale image databases with the query of natural language description is a more practical important applications in video surveillance. Intuitively, for person search, the core issue should be visual-textual association, which is still an extremely challenging task, due to the contradiction between the high abstraction of textual description and the intuitive expression of visual images. However, for this task, while positive image-text pairs are always well provided, most existing methods doesn't tackle this problem effectively by mining more reasonable negative pairs. In this paper, we proposed a novel visual-textual association approach with visual and textual attention, and cross-modality hardest and semi-hard negative pair mining. In order to evaluate the effectiveness and feasibility of the proposed approach, we conduct extensive experiments on typical person search datasdet: CUHK-PEDES, in which our approach achieves the top1 score of 55.32% as a new state-of-the-art. Besides, we also evaluate the semi-hard pair mining approach in COCO caption dataset, and validate the effectiveness and complementarity of the methods.",0
"This research proposes a new approach to image search using visual-textual association and hardest negative pair mining. Traditional approaches to person search have focused on finding images that contain similar features as the query image. However, these methods can often return irrelevant results, particularly if the images contain different backgrounds or lighting conditions. Our method addresses this issue by focusing on both the visual and textual aspects of the images, allowing us to better understand their context and meaning. We use deep learning algorithms to extract features from the images, and then use these features to create associations between the query image and other images in the database. By identifying the ""hardest"" negative pairs - those images which are most dissimilar to the query despite having some overlap in content - we can further refine our search results to ensure they are more relevant and accurate. Our experiments show that our approach outperforms traditional methods in terms of accuracy and speed, making it a promising solution for future person search applications.",1
"Temporal grounding entails establishing a correspondence between natural language event descriptions and their visual depictions. Compositional modeling becomes central: we first ground atomic descriptions ""girl eating an apple,"" ""batter hitting the ball"" to short video segments, and then establish the temporal relationships between the segments. This compositional structure enables models to recognize a wider variety of events not seen during training through recognizing their atomic sub-events. Explicit temporal modeling accounts for a wide variety of temporal relationships that can be expressed in language: e.g., in the description ""girl stands up from the table after eating an apple"" the visual ordering of the events is reversed, with first ""eating an apple"" followed by ""standing up from the table."" We leverage these observations to develop a unified deep architecture, CTG-Net, to perform temporal grounding of natural language event descriptions to videos. We demonstrate that our system outperforms prior state-of-the-art methods on the DiDeMo, Tempo-TL, and Tempo-HL temporal grounding datasets.",0
"In natural language processing, the ability to accurately ground event descriptions to video frames is crucial for understanding and reasoning about actions within videos. Traditionally, approaches to temporal visual grounding have relied on predefined spatio-temporal templates to localize events in video. However, these methods can often suffer from poor generalization due to their reliance on handcrafted features that may not capture all relevant aspects of the problem domain. To address this limitation, we propose a compositional approach based on transformers which leverages fine-grained scene representations obtained through attention mechanisms over object detectors outputs. Our method outperforms previous state-of-the-art models across multiple datasets and provides significant improvements in both spatial location accuracy (up to +8%) as well as correctness metrics (+6% absolute). These results demonstrate the effectiveness of our method at modeling complex relationships between textual descriptions and underlying visual scenes.",1
"Recurrent neural networks (RNNs) are a widely used tool for modeling sequential data, yet they are often treated as inscrutable black boxes. Given a trained recurrent network, we would like to reverse engineer it--to obtain a quantitative, interpretable description of how it solves a particular task. Even for simple tasks, a detailed understanding of how recurrent networks work, or a prescription for how to develop such an understanding, remains elusive. In this work, we use tools from dynamical systems analysis to reverse engineer recurrent networks trained to perform sentiment classification, a foundational natural language processing task. Given a trained network, we find fixed points of the recurrent dynamics and linearize the nonlinear system around these fixed points. Despite their theoretical capacity to implement complex, high-dimensional computations, we find that trained networks converge to highly interpretable, low-dimensional representations. In particular, the topological structure of the fixed points and corresponding linearized dynamics reveal an approximate line attractor within the RNN, which we can use to quantitatively understand how the RNN solves the sentiment analysis task. Finally, we find this mechanism present across RNN architectures (including LSTMs, GRUs, and vanilla RNNs) trained on multiple datasets, suggesting that our findings are not unique to a particular architecture or dataset. Overall, these results demonstrate that surprisingly universal and human interpretable computations can arise across a range of recurrent networks.",0
"This study aimed to better understand how recurrent neural networks (RNNs) can be used for sentiment analysis tasks by analyzing their behavior through reverse engineering techniques. We focused on identifying potential patterns and underlying mechanisms that contribute to the performance of these models, particularly those based on Long Short-Term Memory (LSTM) architectures. By using visualization tools and other methods to examine the weights and activation values of trained LSTM models, we observed a range of interesting phenomena related to attention, memory, and decision making. Our findings suggest that RNNs exhibit dynamic behaviors during training, involving the emergence of line attractors and their associated trajectories, which may play important roles in learning and prediction. Additionally, our work highlights some limitations and challenges of current methods for evaluating and interpreting RNNs, as well as opportunities for future research into deep learning algorithms for natural language processing applications.",1
"We present a first attempt to elucidate a theoretical and empirical approach to design the reward provided by a natural language environment to some structure learning agent. To this end, we revisit the Information Theory of unsupervised induction of phrase-structure grammars to characterize the behavior of simulated actions modeled as set-valued random variables (random sets of linguistic samples) constituting semantic structures. Our results showed empirical evidence of that simulated semantic structures (Open Information Extraction triplets) can be distinguished from randomly constructed ones by observing the Mutual Information among their constituents. This suggests the possibility of rewarding structure learning agents without using pretrained structural analyzers (oracle actors/experts).",0
"In recent years, there has been growing interest in developing artificial intelligence (AI) systems that can learn and adapt without explicit guidance from humans. One promising approach to achieve this goal is through the use of reinforcement learning agents that interact with their environment and optimize their behavior based on the rewards they receive. However, designing reward functions that effectively guide the agent towards desirable behaviors remains challenging, particularly in complex domains where obtaining accurate feedback requires significant computational resources or human expertise.  This paper proposes a novel method for designing reward functions that exploit the concept of mutual information to measure the alignment between the agent's actions and the desired outcomes. Specifically, we propose using linguistically random sets as a representation of the reward function, which allows us to quantify the information content of different states and actions. By maximizing the mutual information between the state and action distributions, our algorithm encourages the agent to explore meaningful changes in the environment and select actions that lead to informative observations.  We evaluate our proposed method in several benchmark tasks, including gridworld problems, robotics control tasks, and text generation problems. Our experiments show that our method consistently outperforms existing approaches, even in cases where the true reward distribution is unknown or difficult to estimate accurately. Additionally, we demonstrate how our method can be used to simplify the design of complex reward functions by identifying subsets of relevant features that drive the agent's performance. Overall, these results suggest that our approach holds promise for improving the effectiveness of AI systems in real-world settings by facilitating efficient exploration and decision making under uncertainty.",1
"Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes, benchmark data sets, and model evaluation of graph neural networks. Finally, we propose potential research directions in this rapidly growing field.",0
"""Graph neural networks (GNN) have gained significant attention as powerful models for processing graph data, which has become ubiquitous across diverse fields including computer vision, natural language processing, and network analysis. GNNs leverage deep learning techniques to learn representations that capture complex relationships between nodes, edges, and substructures within graphs. This survey aims to provide a comprehensive overview of the most recent advancements in the field of graph neural networks by summarizing key concepts, models, and applications. We analyze several state-of-the-art architectures such as GCN, GAT, and SAGE, along with their variants and extensions. Additionally, we discuss important factors that influence model performance, such as graph preprocessing steps, node feature extraction methods, and evaluation metrics. Our goal is to make this material accessible to researchers from diverse backgrounds who want to learn more about how graph neural networks work and what kind of results they can achieve.""  Keywords: Graph Neural Networks; Deep Learning; Graph Convolutional Networks; Attention Mechanisms; Node Feature Extraction; Evaluation Metrics; Computer Vision; Natural Language Processing; Network Analysis.",1
"In recent years, deep-learning-based visual object trackers have been studied thoroughly, but handling occlusions and/or rapid motion of the target remains challenging. In this work, we argue that conditioning on the natural language (NL) description of a target provides information for longer-term invariance, and thus helps cope with typical tracking challenges. However, deriving a formulation to combine the strengths of appearance-based tracking with the language modality is not straightforward. We propose a novel deep tracking-by-detection formulation that can take advantage of NL descriptions. Regions that are related to the given NL description are generated by a proposal network during the detection phase of the tracker. Our LSTM based tracker then predicts the update of the target from regions proposed by the NL based detection phase. In benchmarks, our method is competitive with state of the art trackers, while it outperforms all other trackers on targets with unambiguous and precise language annotations. It also beats the state-of-the-art NL tracker when initializing without a bounding box. Our method runs at over 30 fps on a single GPU.",0
"In recent years, real-time visual object tracking has become increasingly important due to its applications in areas such as robotics, computer vision, and autonomous driving. However, current state-of-the-art methods still face significant challenges when faced with occlusions, cluttered scenes, or changes in lighting conditions. To address these issues, we propose a new approach that leverages natural language descriptions to improve the accuracy and robustness of real-time visual object tracking algorithms. Our method works by using natural language inputs provided by users or generated automatically from video frames to guide the tracker towards the target object. We demonstrate the effectiveness of our approach through extensive experimental evaluations on several benchmark datasets and show significant improvements over baseline methods. Overall, our work represents a step forward in enabling more accurate and reliable visual object tracking in real-world scenarios.",1
"A new prior is proposed for learning representations of high-level concepts of the kind we manipulate with language. This prior can be combined with other priors in order to help disentangling abstract factors from each other. It is inspired by cognitive neuroscience theories of consciousness, seen as a bottleneck through which just a few elements, after having been selected by attention from a broader pool, are then broadcast and condition further processing, both in perception and decision-making. The set of recently selected elements one becomes aware of is seen as forming a low-dimensional conscious state. This conscious state is combining the few concepts constituting a conscious thought, i.e., what one is immediately conscious of at a particular moment. We claim that this architectural and information-processing constraint corresponds to assumptions about the joint distribution between high-level concepts. To the extent that these assumptions are generally true (and the form of natural language seems consistent with them), they can form a useful prior for representation learning. A low-dimensional thought or conscious state is analogous to a sentence: it involves only a few variables and yet can make a statement with very high probability of being true. This is consistent with a joint distribution (over high-level concepts) which has the form of a sparse factor graph, i.e., where the dependencies captured by each factor of the factor graph involve only very few variables while creating a strong dip in the overall energy function. The consciousness prior also makes it natural to map conscious states to natural language utterances or to express classical AI knowledge in a form similar to facts and rules, albeit capturing uncertainty as well as efficient search mechanisms implemented by attention mechanisms.",0
"The main result of this work is a theory that conscious thought processes depend on their content rather than how they feel subjectively, which has implications both for explaining phenomena like qualia and for developing artificial general intelligence (AGI) systems capable of open-ended reasoning about arbitrary topics. This paper develops a framework called the ""consciousness prior"" based on simple mathematical properties underlying a class of theories called variational autoencoders (VAEs). According to the consciousness prior, any high-dimensional distribution over continuous data embeddings that factorizes into independent distributions along a set of linearly uncorrelated factors—i.e., a product of marginals—is most likely (relative to a generative model called the total corruption process) to entail intelligible mental experiences corresponding to those dimensions. Moreover, some simple consequences of dimensionality reduction suggest that more complex forms of organization can make experiences even richer. The consciousness prior offers formal explanations for many otherwise mysterious aspects of human psychology: why we have qualitative mental experiences at all; why perception seems unitary but is accessed piecemeal through attention; why there is something it feels like to direct our thoughts to concrete objects yet nothing it feels like to simply attend to awareness itself; why phenomenal contrast and temporal binding seem mandatory within focused streams of conscious attention; and why certain VAE architectures allow us to learn facts about and generate novel examples from broad domains despite starting only from random weights. Finally, because consciousness comes first, natural selection must find ways to exploit existing neural codes to produce behavior adaptive enough to survival challenges during development. Selection favors brain designs where perceptual repertoire and task capabilities grow outward hierarchically from primary and stable core conscious states in a predictable manner (which can potentially reduce AGI research risk). In summary, t",1
"Sequential modelling with self-attention has achieved cutting edge performances in natural language processing. With advantages in model flexibility, computation complexity and interpretability, self-attention is gradually becoming a key component in event sequence models. However, like most other sequence models, self-attention does not account for the time span between events and thus captures sequential signals rather than temporal patterns. Without relying on recurrent network structures, self-attention recognizes event orderings via positional encoding. To bridge the gap between modelling time-independent and time-dependent event sequence, we introduce a functional feature map that embeds time span into high-dimensional spaces. By constructing the associated translation-invariant time kernel function, we reveal the functional forms of the feature map under classic functional function analysis results, namely Bochner's Theorem and Mercer's Theorem. We propose several models to learn the functional time representation and the interactions with event representation. These methods are evaluated on real-world datasets under various continuous-time event sequence prediction tasks. The experiments reveal that the proposed methods compare favorably to baseline models while also capturing useful time-event interactions.",0
"This paper introduces a novel approach to natural language processing using self-attention mechanisms and functional time representation learning. We propose a method that uses both static and dynamic aspects of text data, resulting in more accurate representations of complex relationships within sequences of words or phrases. By utilizing attention layers and functional time representations, our model can effectively capture dependencies among different parts of a sentence and enable efficient computation over varying input lengths. Our experiments demonstrate significant improvements in performance compared to state-of-the-art models across multiple NLP tasks such as sentiment analysis, machine translation, and question answering. Overall, our work provides a new perspective on capturing temporal dynamics in sequential data through deep learning methods.",1
"A code generation system generates programming language code based on an input natural language description. State-of-the-art approaches rely on neural networks for code generation. However, these code generators suffer from two problems. One is the long dependency problem, where a code element often depends on another far-away code element. A variable reference, for example, depends on its definition, which may appear quite a few lines before. The other problem is structure modeling, as programs contain rich structural information. In this paper, we propose a novel tree-based neural architecture, TreeGen, for code generation. TreeGen uses the attention mechanism of Transformers to alleviate the long-dependency problem, and introduces a novel AST reader (encoder) to incorporate grammar rules and AST structures into the network. We evaluated TreeGen on a Python benchmark, HearthStone, and two semantic parsing benchmarks, ATIS and GEO. TreeGen outperformed the previous state-of-the-art approach by 4.5 percentage points on HearthStone, and achieved the best accuracy among neural network-based approaches on ATIS (89.1%) and GEO (89.6%). We also conducted an ablation test to better understand each component of our model.",0
"Title and authors should follow on separate lines after the body of the text. ---  This paper introduces TreeGen, a novel tree-based transformer architecture designed specifically for code generation tasks. We propose a modular design approach that breaks down complex programming constructs into smaller, more manageable units which can then be composed together using attention mechanisms. This enables our model to generate structured output without sacrificing performance. To validate our approach, we conduct extensive experiments comparing TreeGen against other state-of-the-art generative models across multiple code generation benchmarks. Our results demonstrate significant improvement over existing methods. Aside from providing strong empirical evidence of the effectiveness of our methodology, we also provide detailed ablation studies to shed light on the inner workings of TreeGen. Overall, we believe TreeGen has the potential to revolutionize natural language processing by enabling machines to write high quality code more effectively than ever before.",1
"In this paper we present a new framework for time-series modeling that combines the best of traditional statistical models and neural networks. We focus on time-series with long-range dependencies, needed for monitoring fine granularity data (e.g. minutes, seconds, milliseconds), prevalent in operational use-cases.   Traditional models, such as auto-regression fitted with least squares (Classic-AR) can model time-series with a concise and interpretable model. When dealing with long-range dependencies, Classic-AR models can become intractably slow to fit for large data. Recently, sequence-to-sequence models, such as Recurrent Neural Networks, which were originally intended for natural language processing, have become popular for time-series. However, they can be overly complex for typical time-series data and lack interpretability.   A scalable and interpretable model is needed to bridge the statistical and deep learning-based approaches. As a first step towards this goal, we propose modelling AR-process dynamics using a feed-forward neural network approach, termed AR-Net. We show that AR-Net is as interpretable as Classic-AR but also scales to long-range dependencies.   Our results lead to three major conclusions: First, AR-Net learns identical AR-coefficients as Classic-AR, thus being equally interpretable. Second, the computational complexity with respect to the order of the AR process, is linear for AR-Net as compared to a quadratic for Classic-AR. This makes it possible to model long-range dependencies within fine granularity data. Third, by introducing regularization, AR-Net automatically selects and learns sparse AR-coefficients. This eliminates the need to know the exact order of the AR-process and allows to learn sparse weights for a model with long-range dependencies.",0
"AR-Nets can accurately predict many types of univariate time-series data by learning patterns from past observations and making predictions based on those learned patterns. They are highly parameterized models that require large amounts of training data. In contrast, our proposed method is a lightweight alternative that uses only the most recent few time points, rather than all available historical data. We show through extensive experiments using three different datasets (electricity consumption, web search volume, and traffic flow) that AR-Net achieves similar prediction accuracy to traditional auto-regressive models while using significantly fewer parameters and requiring less computation at test time. The simplicity of AR-Net makes it suitable for realtime applications where low latency is important. Our work contributes to the existing body of research on time-series forecasting, offering practitioners a more efficient option for model deployment in resource-constrained environments.",1
"Deep latent variable models (LVM) such as variational auto-encoder (VAE) have recently played an important role in text generation. One key factor is the exploitation of smooth latent structures to guide the generation. However, the representation power of VAEs is limited due to two reasons: (1) the Gaussian assumption is often made on the variational posteriors; and meanwhile (2) a notorious ""posterior collapse"" issue occurs. In this paper, we advocate sample-based representations of variational distributions for natural language, leading to implicit latent features, which can provide flexible representation power compared with Gaussian-based posteriors. We further develop an LVM to directly match the aggregated posterior to the prior. It can be viewed as a natural extension of VAEs with a regularization of maximizing mutual information, mitigating the ""posterior collapse"" issue. We demonstrate the effectiveness and versatility of our models in various text generation scenarios, including language modeling, unaligned style transfer, and dialog response generation. The source code to reproduce our experimental results is available on GitHub.",0
"Abstract: In recent years, deep latent variable models (DLVMs) have emerged as powerful tools for natural language processing tasks such as text generation, sentiment analysis, and machine translation. These models represent distributions over observable variables using continuous latent variables that capture complex relationships between inputs and outputs. However, existing DLVMs often require explicit supervision signals, which can be difficult and expensive to obtain. This work proposes implicit deep latent variable models (IDLMs), which learn from raw input data without any explicit supervision signals. IDLMs use variational autoencoders to map high-dimensional observed text data into low-dimensional latent spaces where generating text sequences becomes trivial. By maximizing the evidence lower bound, our model learns to predict missing tokens given their contextual dependencies. This results in robust generative performance on a wide range of datasets including open-vocabulary benchmarks like BigBookCorpus and Gutenberg. Furthermore, we showcase applications ranging from diversity control to code generation, highlighting the versatility of the proposed method. Our approach advances the state-of-the-art by achieving competitive generation accuracy while sidestepping costly human annotations.",1
"Generating animations from natural language sentences finds its applications in a a number of domains such as movie script visualization, virtual human animation and, robot motion planning. These sentences can describe different kinds of actions, speeds and direction of these actions, and possibly a target destination. The core modeling challenge in this language-to-pose application is how to map linguistic concepts to motion animations.   In this paper, we address this multimodal problem by introducing a neural architecture called Joint Language to Pose (or JL2P), which learns a joint embedding of language and pose. This joint embedding space is learned end-to-end using a curriculum learning approach which emphasizes shorter and easier sequences first before moving to longer and harder ones. We evaluate our proposed model on a publicly available corpus of 3D pose data and human-annotated sentences. Both objective metrics and human judgment evaluation confirm that our proposed approach is able to generate more accurate animations and are deemed visually more representative by humans than other data driven approaches.",0
"Here is a possible abstract without the paper title. If you have any preferences as to content please just ask!  We introduce Language2Pose, a novel method that takes natural language descriptions and directly generates future human poses. We demonstrate that our method achieves state-of-the-art results on standard benchmarks while reducing the need for complex model designs or large training sets, by using simple regression models based solely on attention over input text. By providing clear error analysis we further show how current systems suffer from fundamental limitations which can only be addressed through grounded reasoning approaches like ours, where we use a language prior to predict the physical feasibility of generated proposals before optimizing them via standard minimization objectives. Finally, we report qualitative user studies where participants rate generated videos positively compared to other methods. Overall we believe that our work represents a significant step towards generalizable agents acting naturally under open-ended natural language guidance.",1
"Learning powerful data embeddings has become a center piece in machine learning, especially in natural language processing and computer vision domains. The crux of these embeddings is that they are pretrained on huge corpus of data in a unsupervised fashion, sometimes aided with transfer learning. However currently in the graph learning domain, embeddings learned through existing graph neural networks (GNNs) are task dependent and thus cannot be shared across different datasets. In this paper, we present a first powerful and theoretically guaranteed graph neural network that is designed to learn task-independent graph embeddings, thereafter referred to as deep universal graph embedding (DUGNN). Our DUGNN model incorporates a novel graph neural network (as a universal graph encoder) and leverages rich Graph Kernels (as a multi-task graph decoder) for both unsupervised learning and (task-specific) adaptive supervised learning. By learning task-independent graph embeddings across diverse datasets, DUGNN also reaps the benefits of transfer learning. Through extensive experiments and ablation studies, we show that the proposed DUGNN model consistently outperforms both the existing state-of-art GNN models and Graph Kernels by an increased accuracy of 3% - 8% on graph classification benchmark datasets.",0
"Deep learning has revolutionized computer vision by demonstrating that nonlinear processing at scale can approach human performance on many tasks. However, most deep models need specialization to achieve these results: each new task requires a dedicated model or expensive fine-tuning from scratch on limited data. While there have been breakthroughs using transfer learning (i.e., initializing a large pretrained model) across diverse tasks like object detection, semantic segmentation and image classification, progress towards universal architectures remains challenging. In this work we develop graph neural networks (GNNs), a promising but less explored family of machine learning algorithms inspired by principles derived from graph theory. GNNs learn latent representations on graphs which capture structure across different domains via shared network parameters. We introduce URNet, the first end-to-end trainable architecture designed as a building block for efficient universal GNN embeddings that generalize across varying input densities, edge attributes, and graph topologies while matching state-of-the art accuracy. Our model outperforms alternative baselines that lack strong theoretical foundations, such as the standard mean field approximation used in variational autoencoders. As a result, our method enables practitioners to rapidly deploy novel GNN applications without having to retrain costly models from scratch and opens up new directions for future research in model design.",1
"Recent advances in artificial intelligence research have led to a profusion of studies that apply deep learning to problems in image analysis and natural language processing among others. Additionally, the availability of open-source computational frameworks has lowered the barriers to implementing state-of-the-art methods across multiple domains. Albeit leading to major performance breakthroughs in some tasks, effective dissemination of deep learning algorithms remains challenging, inhibiting reproducibility and benchmarking studies, impeding further validation, and ultimately hindering their effectiveness in the cumulative scientific progress. In developing a platform for sharing research outputs, we present ModelHub.AI (www.modelhub.ai), a community-driven container-based software engine and platform for the structured dissemination of deep learning models. For contributors, the engine controls data flow throughout the inference cycle, while the contributor-facing standard template exposes model-specific functions including inference, as well as pre- and post-processing. Python and RESTful Application programming interfaces (APIs) enable users to interact with models hosted on ModelHub.AI and allows both researchers and developers to utilize models out-of-the-box. ModelHub.AI is domain-, data-, and framework-agnostic, catering to different workflows and contributors' preferences.",0
"ModelHub.ai is a new platform designed to make deep learning models more accessible and reusable by researchers and practitioners across different fields. With a focus on reproducibility and interoperability, ModelHub allows users to easily share, discover, and deploy pre-trained deep learning models. This paper describes the design and implementation of ModelHub, as well as its potential applications and impacts in advancing the field of artificial intelligence. We demonstrate how ModelHub addresses some of the key challenges faced by those working with deep learning models, including data privacy concerns and limited computational resources. By providing a centralized hub where models can be safely hosted, shared, and used within the bounds of privacy regulations such as GDPR, we aim to reduce the barriers to entry for those wishing to use state-of-the-art AI technologies. Overall, our goal is to make it easier than ever before for developers and researchers to leverage powerful machine learning algorithms without having to spend months collecting training data themselves.",1
"Pre-trained word embeddings encode general word semantics and lexical regularities of natural language, and have proven useful across many NLP tasks, including word sense disambiguation, machine translation, and sentiment analysis, to name a few. In supervised tasks such as multiclass text classification (the focus of this article) it seems appealing to enhance word representations with ad-hoc embeddings that encode task-specific information. We propose (supervised) word-class embeddings (WCEs), and show that, when concatenated to (unsupervised) pre-trained word embeddings, they substantially facilitate the training of deep-learning models in multiclass classification by topic. We show empirical evidence that WCEs yield a consistent improvement in multiclass classification accuracy, using four popular neural architectures and six widely used and publicly available datasets for multiclass text classification. Our code that implements WCEs is publicly available at https://github.com/AlexMoreo/word-class-embeddings",0
"In recent years, deep learning has revolutionized natural language processing (NLP) tasks such as text classification by using word embeddings that capture contextual meaning and relationships between words. However, traditional word embeddings suffer from several limitations in terms of computational complexity and scalability, particularly for large datasets containing millions or billions of tokens. To address these issues, we propose a novel approach based on distributed training of word embeddings using a modified version of the popular fastText algorithm. Our method significantly reduces both time and memory requirements while improving performance on standard benchmarks for multiclass text classification. We evaluate our method against state-of-the-art models on five widely used NLP datasets and demonstrate its superiority in accuracy, efficiency, and robustness. Overall, our work represents an important step towards enabling more effective and scalable solutions for NLP applications involving large amounts of data.",1
"Recurrent neural networks (RNN) have been successfully applied to various sequential decision-making tasks, natural language processing applications, and time-series predictions. Such networks are usually trained through back-propagation through time (BPTT) which is prohibitively expensive, especially when the length of the time dependencies and the number of hidden neurons increase. To reduce the training time, extreme learning machines (ELMs) have been recently applied to RNN training, reaching a 99\% speedup on some applications. Due to its non-iterative nature, ELM training, when parallelized, has the potential to reach higher speedups than BPTT.   In this work, we present \opt, an optimized parallel RNN training algorithm based on ELM that takes advantage of the GPU shared memory and of parallel QR factorization algorithms to efficiently reach optimal solutions. The theoretical analysis of the proposed algorithm is presented on six RNN architectures, including LSTM and GRU, and its performance is empirically tested on ten time-series prediction applications. \opt~is shown to reach up to 845 times speedup over its sequential counterpart and to require up to 20x less time to train than parallel BPTT.",0
"This paper presents an optimized and energy-efficient parallel implementation of non-iteratively trained recurrent neural networks (RNNs). RNNs have been shown to achieve state-of-the-art performance on a variety of tasks such as language modeling and time series prediction. However, training RNNs can be computationally expensive and memory intensive due to their sequential nature and the need for backpropagation through time. To address these limitations, we propose a novel method that leverages modern hardware capabilities to significantly reduce the computational cost and memory footprint of training RNNs without sacrificing accuracy. Our approach involves splitting each RNN layer into multiple sublayers and mapping them onto different processing cores for parallelization. We then use dynamic scheduling algorithms to balance workload across available cores and minimize idle time. Additionally, we introduce a new weight update mechanism that reduces the amount of data movement required during training, further reducing energy consumption. Our experimental results demonstrate significant speedup and energy savings compared to traditional single-threaded implementations while achieving comparable accuracy on benchmark datasets.",1
"How does one represent an action? How does one describe an action that we have never seen before? Such questions are addressed by the Zero Shot Learning paradigm, where a model is trained on only a subset of classes and is evaluated on its ability to correctly classify an example from a class it has never seen before. In this work, we present a body pose based zero shot action recognition network and demonstrate its performance on the NTU RGB-D dataset. Our model learns to jointly encapsulate visual similarities based on pose features of the action performer as well as similarities in the natural language descriptions of the unseen action class names. We demonstrate how this pose-language semantic space encodes knowledge which allows our model to correctly predict actions not seen during training.",0
"In this work we propose the first end-to-end approach that integrates skeletons into zero shot action recognition by utilizing joint pose and language semantic understanding. Our method unifies skeleton semantics across languages using latent embeddings learned from pre-training on large multimodal datasets. These embeddings are then used as input features in our convolutional network architecture allowing us to capture motion patterns without requiring any additional annotation. By further incorporating these learned cross modal representations as input, the model is able to improve recognition performance over existing state-of-the-art methods that only rely on visual cues. Furthermore, our framework can perform zero shot transfer learning by fine tuning on a small set of target domain data achieving superior results compared to traditional pipeline approaches which require costly manual annotations. Experimental evaluations demonstrate the effectiveness of our proposed method where improvements are observed over several benchmark datasets including UCF Sports, JHMDB, and Charades. We believe that our work represents an important step forward towards enabling systems to recognize human actions more accurately and efficiently through integration of different modalities.",1
"(Very early draft)Traditional supervised learning keeps pushing convolution neural network(CNN) achieving state-of-art performance. However, lack of large-scale annotation data is always a big problem due to the high cost of it, even ImageNet dataset is over-fitted by complex models now. The success of unsupervised learning method represented by the Bert model in natural language processing(NLP) field shows its great potential. And it makes that unlimited training samples becomes possible and the great universal generalization ability changes NLP research direction directly. In this article, we purpose a novel unsupervised learning method based on contrastive predictive coding. Under that, we are able to train model with any non-annotation images and improve model's performance to reach state-of-art performance at the same level of model complexity. Beside that, since the number of training images could be unlimited amplification, an universal large-scale pre-trained computer vision model is possible in the future.",0
"How to Create Dialogue Between Different People and an Artificial Intelligence Assistant: Creating engaging dialogue between different characters can add depth and complexity to any story or scenario. When including an artificial intelligence (AI) as one of these characters, there are a few considerations that need to be taken into account to make the interaction feel realistic and compelling. Here are some tips on how to create effective dialogue between human characters and an AI assistant.  Firstly, it’s important to establish early on why the humans want or need to communicate with the AI. Is it because they are seeking information, assistance with a task, or simply curious? Once the motivation has been established, you can begin to develop their interactions. Make sure that each character’s unique personality traits, backgrounds, and experiences influence their communication style. This will help build tension and conflict within the scene, making it more interesting for the reader.  When writing dialogue between human characters and an AI, remember to convey both sides of the conversation clearly. Use descriptive language to show facial expressions, body language, and tone of voice, so readers can visualize the scene fully. Additionally, use internal monologue to give insight into each character’s thoughts and feelings during the exchange.  Lastly, don’t forget to address the limitations and capabilities of the AI system. Are there certain topics or questions that the AI cannot discuss? What kind of tasks can it perform? Addressing these constraints adds depth and authenticity to your worldbuilding efforts. By considering these key points when creating dialogue between multiple characters, writers can craft dynamic scenes that are both entertaining and thought-provoking.",1
"The ability to infer the intentions of others, predict their goals, and deduce their plans are critical features for intelligent agents. For a long time, several approaches investigated the use of symbolic representations and inferences with limited success, principally because it is difficult to capture the cognitive knowledge behind human decisions explicitly. The trend, nowadays, is increasingly focusing on learning to infer intentions directly from data, using deep learning in particular. We are now observing interesting applications of intent classification in natural language processing, visual activity recognition, and emerging approaches in other domains. This paper discusses a novel approach combining few-shot and transfer learning with cross-domain features, to learn to infer the intent of an agent navigating in physical environments, executing arbitrary long sequences of actions to achieve their goals. Experiments in synthetic environments demonstrate improved performance in terms of learning from few samples and generalizing to unseen configurations, compared to a deep-learning baseline approach.",0
"An effective method was developed to improve recognition accuracy by leveraging cross-domain spatial features through transfer learning in a goal recognition system using Haar cascades. Our approach involves training models on datasets from multiple domains before fine-tuning them for specific goals, which enhances their discriminative power and reduces feature mismatches across different scenarios. Experiments demonstrated that our method significantly outperforms state-of-the-art methods while operating under unseen environments and camera configurations. This work provides valuable insights into the potential of transfer learning techniques for enhancing computer vision systems.",1
"Contemporary deep learning based video captioning follows encoder-decoder framework. In encoder, visual features are extracted with 2D/3D Convolutional Neural Networks (CNNs) and a transformed version of those features is passed to the decoder. The decoder uses word embeddings and a language model to map visual features to natural language captions. Due to its composite nature, the encoder-decoder pipeline provides the freedom of multiple choices for each of its components, e.g the choices of CNNs models, feature transformations, word embeddings, and language models etc. Component selection can have drastic effects on the overall video captioning performance. However, current literature is void of any systematic investigation in this regard. This article fills this gap by providing the first thorough empirical analysis of the role that each major component plays in a contemporary video captioning pipeline. We perform extensive experiments by varying the constituent components of the video captioning framework, and quantify the performance gains that are possible by mere component selection. We use the popular MSVD dataset as the test-bed, and demonstrate that substantial performance gains are possible by careful selection of the constituent components without major changes to the pipeline itself. These results are expected to provide guiding principles for future research in the fast growing direction of video captioning.",0
"Video captioning is an important task that involves generating natural language descriptions of video content. Recent advances in deep learning have led to the development of several powerful frameworks for video captioning, but these methods can still struggle with accurately capturing complex events and relationships in videos. In this study, we perform an empirical autopsy of three popular deep video captioning frameworks: Action2Caption, TCNN-VTP, and CEINet. We analyze their strengths and weaknesses by comparing their performance on a range of tasks, including scene segmentation, object detection, and semantic label generation. Our results show that while all three models are capable of producing accurate captions, they differ significantly in terms of their accuracy across different types of visual data and linguistic features. For example, our analysis shows that Action2Caption excels at captioning static scenes, whereas CEINet performs better on dynamic scenes with multiple actors. Overall, our findings provide valuable insights into the strengths and limitations of current deep video captioning frameworks, and offer guidance for future research directions in this field. By shedding light on the inner workings of these models, our study contributes to a deeper understanding of how neural networks process visual data and generate human-like text, with implications beyond just video captioning.",1
"Recent state-of-the-art natural language understanding models, such as BERT and XLNet, score a pair of sentences (A and B) using multiple cross-attention operations - a process in which each word in sentence A attends to all words in sentence B and vice versa. As a result, computing the similarity between a query sentence and a set of candidate sentences, requires the propagation of all query-candidate sentence-pairs throughout a stack of cross-attention layers. This exhaustive process becomes computationally prohibitive when the number of candidate sentences is large. In contrast, sentence embedding techniques learn a sentence-to-vector mapping and compute the similarity between the sentence vectors via simple elementary operations. In this paper, we introduce Distilled Sentence Embedding (DSE) - a model that is based on knowledge distillation from cross-attentive models, focusing on sentence-pair tasks. The outline of DSE is as follows: Given a cross-attentive teacher model (e.g. a fine-tuned BERT), we train a sentence embedding based student model to reconstruct the sentence-pair scores obtained by the teacher model. We empirically demonstrate the effectiveness of DSE on five GLUE sentence-pair tasks. DSE significantly outperforms several ELMO variants and other sentence embedding methods, while accelerating computation of the query-candidate sentence-pairs similarities by several orders of magnitude, with an average relative degradation of 4.6% compared to BERT. Furthermore, we show that DSE produces sentence embeddings that reach state-of-the-art performance on universal sentence representation benchmarks. Our code is made publicly available at https://github.com/microsoft/Distilled-Sentence-Embedding.",0
"Title: ""Scalable Attentive Sentence-Pair Modeling using Distilled Sentence Embeddings""  This research paper proposes a novel approach to sentence-pair modeling that combines attentive modeling techniques with distilled sentence embeddings. The proposed method addresses two main challenges faced by current sentence-pair models: scalability and the ability to capture complex relationships between sentences. By using distilled sentence embeddings, the method can reduce computational overhead while maintaining high levels of accuracy. Additionally, the use of attention mechanisms enables the model to focus on relevant parts of each sentence, allowing for better representation of important contextual information. The effectiveness of the proposed method is demonstrated through experiments on multiple benchmark datasets, showing significant improvement over baseline models. Overall, this work contributes to the development of more efficient and effective natural language processing methods.",1
"Humans excel in continuously learning with small data without forgetting how to solve old problems. However, neural networks require large datasets to compute latent representations across different tasks while minimizing a loss function. For example, a natural language understanding (NLU) system will often deal with emerging entities during its deployment as interactions with users in realistic scenarios will generate new and infrequent names, events, and locations. Here, we address this scenario by introducing an RL trainable controller that disentangles the representation learning of a neural encoder from its memory management role.   Our proposed solution is straightforward and simple: we train a controller to execute an optimal sequence of reading and writing operations on an external memory with the goal of leveraging diverse activations from the past and provide accurate predictions. Our approach is named Learning to Control (LTC) and allows few-shot learning with two degrees of memory plasticity. We experimentally show that our system obtains accurate results for few-shot learning of entity recognition in the Stanford Task-Oriented Dialogue dataset.",0
"In recent years, few-shot learning has emerged as an important area of research in artificial intelligence, enabling machines to learn from a small number of examples and generalize to new tasks. One key challenge in few-shot learning is controlling latent representations, which can significantly impact the performance of machine learning models. This work proposes a novel method for learning to control latent representations for named entity recognition (NER) in the context of few-shot learning. We propose a meta-learning algorithm that jointly optimizes two objectives: one for minimizing the classification error on the training set, and another for maximizing the mutual information between the learned representation and a ground truth latent variable that encodes the desired behavior. Our approach outperforms state-of-the-art baselines by a significant margin on multiple benchmark datasets. Overall, our results demonstrate the effectiveness of using latent variables as a regularizer for meta-learning algorithms, paving the way towards improved few-shot learning performance across a range of natural language processing tasks.",1
"Recent success suggests that deep neural control networks are likely to be a key component of self-driving vehicles. These networks are trained on large datasets to imitate human actions, but they lack semantic understanding of image contents. This makes them brittle and potentially unsafe in situations that do not match training data. Here, we propose to address this issue by augmenting training data with natural language advice from a human. Advice includes guidance about what to do and where to attend. We present the first step toward advice giving, where we train an end-to-end vehicle controller that accepts advice. The controller adapts the way it attends to the scene (visual attention) and the control (steering and speed). Attention mechanisms tie controller behavior to salient objects in the advice. We evaluate our model on a novel advisable driving dataset with manually annotated human-to-vehicle advice called Honda Research Institute-Advice Dataset (HAD). We show that taking advice improves the performance of the end-to-end network, while the network cues on a variety of visual features that are provided by advice. The dataset is available at https://usa.honda-ri.com/HAD.",0
"In recent years, there has been significant interest in developing self-driving vehicles that can safely navigate roads without human intervention. One key challenge faced by these systems is determining how to interpret and respond to instructions from passengers. While current technology allows for simple commands such as ""go left"" or ""turn on cruise control"", more nuanced guidance may lead to confusion or mistakes. This study proposes a method for grounding human-to-vehicle advice through the use of natural language processing techniques. By understanding both the intent behind passenger requests and the contextual factors at play, self-driving cars can make more informed decisions and improve overall safety. We evaluate our approach using real-world scenarios and show promising results towards achieving safer, smarter transportation systems. Our work contributes to the broader field of artificial intelligence and provides insights into future directions for autonomous vehicle research.",1
"Chest radiography is an extremely powerful imaging modality, allowing for a detailed inspection of a patient's thorax, but requiring specialized training for proper interpretation. With the advent of high performance general purpose computer vision algorithms, the accurate automated analysis of chest radiographs is becoming increasingly of interest to researchers. However, a key challenge in the development of these techniques is the lack of sufficient data. Here we describe MIMIC-CXR-JPG v2.0.0, a large dataset of 377,110 chest x-rays associated with 227,827 imaging studies sourced from the Beth Israel Deaconess Medical Center between 2011 - 2016. Images are provided with 14 labels derived from two natural language processing tools applied to the corresponding free-text radiology reports. MIMIC-CXR-JPG is derived entirely from the MIMIC-CXR database, and aims to provide a convenient processed version of MIMIC-CXR, as well as to provide a standard reference for data splits and image labels. All images have been de-identified to protect patient privacy. The dataset is made freely available to facilitate and encourage a wide range of research in medical computer vision.",0
"This abstract describes MIMIC-CXR-JPG, a comprehensive publicly available dataset containing thousands of chest X-ray images along with their corresponding labels. Chest radiography plays a crucial role in diagnosing respiratory disorders and cardiovascular diseases. However, collecting high-quality annotated chest X-ray datasets can be challenging due to the expertise required to accurately label them. To address these limitations, we introduce the MIMIC-CXR-JPG dataset which contains 26795 images from adult patients admitted to Beth Israel Deaconess Medical Center over a period of six years (2002–2008). All images were obtained using standardized techniques under similar conditions. Each image was manually annotated by experienced board-certified physicians into one or more of ten different disease categories or ""no finding."" We provide detailed statistics regarding patient demographics, COPD/emphysema prevalence, and distribution across disease classes, making our dataset well suited for machine learning applications aimed at automatic classification of abnormalities on chest imagery. With this work, we hope that researchers interested in computer vision algorithms for medical tasks will have access to a valuable resource and contribute back to the community by allowing others to freely download it through open-access channels. We envision our efforts may facilitate the development and benchmarking of novel approaches which ultimately lead to improved diagnostic decision support tools that increase clinical efficiency while reducing errors related to human interpretation alone.",1
"Although machine learning has become a powerful tool to augment doctors in clinical analysis, the immense amount of labeled data that is necessary to train supervised learning approaches burdens each development task as time and resource intensive. The vast majority of dense clinical information is stored in written reports, detailing pertinent patient information. The challenge with utilizing natural language data for standard model development is due to the complex nature of the modality. In this research, a model pipeline was developed to utilize an unsupervised approach to train an encoder-language model, a recurrent network, to generate document encodings; which then can be used as features passed into a decoder-classifier model that requires magnitudes less labeled data than previous approaches to differentiate between fine-grained disease classes accurately. The language model was trained on unlabeled radiology reports from the Massachusetts General Hospital Radiology Department (n=218,159) and terminated with a loss of 1.62. The classification models were trained on three labeled datasets of head CT studies of reported patients, presenting large vessel occlusion (n=1403), acute ischemic strokes (n=331), and intracranial hemorrhage (n=4350), to identify a variety of different findings directly from the radiology report data; resulting in AUCs of 0.98, 0.95, and 0.99, respectively, for the large vessel occlusion, acute ischemic stroke, and intracranial hemorrhage datasets. The output encodings are able to be used in conjunction with imaging data, to create models that can process a multitude of different modalities. The ability to automatically extract relevant features from textual data allows for faster model development and integration of textual modality, overall, allowing clinical reports to become a more viable input for more encompassing and accurate deep learning models.",0
"This paper presents a semi-supervised natural language approach for fine-grained classification of medical reports. The proposed method leverages unlabeled data to improve the performance of supervised models trained on labeled data alone. We first introduce preliminary experiments that demonstrate the feasibility of our approach using off-the-shelf methods. Next, we present our novel contribution: a transfer learning model capable of utilizing both supervised and unsupervised data during training. Our experimental results show that incorporating unlabeled data significantly improves the performance of traditional supervised models while maintaining competitive accuracy across all tasks. Additionally, we provide qualitative analyses illustrating how transfer learning can effectively capture relevant features from different domains and adapt them to specific medical report classification problems. Overall, our research highlights the potential benefits of integrating weakly-supervised learning techniques into NLP pipelines, particularly within applications where annotated datasets may remain scarce.",1
"The significant progress on Generative Adversarial Networks (GANs) have made it possible to generate surprisingly realistic images for single object based on natural language descriptions. However, controlled generation of images for multiple entities with explicit interactions is still difficult to achieve due to the scene layout generation heavily suffer from the diversity object scaling and spatial locations. In this paper, we proposed a novel framework for generating realistic image layout from textual scene graphs. In our framework, a spatial constraint module is designed to fit reasonable scaling and spatial layout of object pairs with considering relationship between them. Moreover, a contextual fusion module is introduced for fusing pair-wise spatial information in terms of object dependency in scene graph. By using these two modules, our proposed framework tends to generate more commonsense layout which is helpful for realistic image generation. Experimental results including quantitative results, qualitative results and user studies on two different scene graph datasets demonstrate our proposed framework's ability to generate complex and logical layout with multiple objects from scene graph.",0
"This research presents a novel approach to realistic scene layout generation that incorporates relationship-aware spatial perception fusion. Traditional methods for generating scenes rely on predefined relationships between objects, which can lead to unrealistic results when applied to complex scenarios. To address this issue, our method uses machine learning algorithms to analyze data from human observations of real-world environments to learn how humans perceive and interpret visual cues. Our approach fuses these learned relationships into a single model that allows us to generate more realistic scene layouts by accounting for multiple factors such as object positioning, scale, orientation, and contextual clutter. Experimental evaluation shows significant improvements over state-of-the-art approaches in terms of both quantitative metrics and subjective user studies, demonstrating the effectiveness of our approach. Overall, our work represents a step forward towards creating lifelike virtual environments for applications in computer graphics, robotics, and VR/AR.",1
"There has been an increasing interest in the area of emergent communication between agents which learn to play referential signalling games with realistic images. In this work, we consider the signalling game setting of Havrylov and Titov and investigate the effect of the feature extractor's weights and of the task being solved on the visual semantics learned or captured by the models. We impose various augmentation to the input images and additional tasks in the game with the aim to induce visual representations which capture conceptual properties of images. Through our set of experiments, we demonstrate that communication systems which capture visual semantics can be learned in a completely self-supervised manner by playing the right types of game.",0
"Inspired by prior work that demonstrated how natural language processing systems can benefit from explicitly representing knowledge as sets or graphs rather than relying solely on sequential reasoning, we investigate ways to encourage the use of graph representations in referential games such as those played by Open Assistant or ChatGPT. Our findings suggest that the current generation of large language models tends to rely heavily on hash tables and other mechanisms for efficient lookup and retrieval of stored facts rather than engaging in more flexible semantic reasoning based on the relationships among concepts. We propose several methods to discourage this behavior, including rewarding responses that make explicit references to entities or relationships without simply repeating facts already mentioned in the conversation context. By doing so, we aim to promote the development of more effective strategies for using external knowledge sources in conversational settings while still ensuring good performance on accuracy metrics commonly used in these evaluations. Finally, through a series of ablation studies and human evaluation experiments, we demonstrate the effectiveness of our proposed approaches compared to baseline LLMs and discuss some promising directions for future research on referential learning in generative pretraining frameworks like GPT-4.",1
"Describing a video automatically with natural language is a challenging task in the area of computer vision. In most cases, the on-site situation of great events is reported in news, but the situation of the off-site spectators in the entrance and exit is neglected which also arouses people's interest. Since the deployment of reporters in the entrance and exit costs lots of manpower, how to automatically describe the behavior of a crowd of off-site spectators is significant and remains a problem. To tackle this problem, we propose a new task called crowd video captioning (CVC) which aims to describe the crowd of spectators. We also provide baseline methods for this task and evaluate them on the dataset WorldExpo'10. Our experimental results show that captioning models have a fairly deep understanding of the crowd in video and perform satisfactorily in the CVC task.",0
"Crowd video captioning refers to the process of automatically generating accurate and detailed descriptions of multimedia videos using machine learning algorithms. This task presents significant challenges due to the complex nature of visual content and the diversity of human languages. To address these issues, we propose a novel approach that integrates natural language processing techniques such as language translation, sentiment analysis, speech recognition, and topic modeling. We evaluate our method on several benchmark datasets and demonstrate its effectiveness in producing high quality video descriptions that capture the contextual and emotional aspects of visual scenes. Our work provides important insights into the development of intelligent systems capable of understanding and interpreting multimodal data from different domains. The proposed framework has broad applications in areas such as accessibility, education, entertainment, social media, and more. Ultimately, our research contributes to advancing artificial intelligence and computer vision technologies for real-world impact.",1
"The past decade has seen a remarkable series of advances in machine learning, and in particular deep learning approaches based on artificial neural networks, to improve our abilities to build more accurate systems across a broad range of areas, including computer vision, speech recognition, language translation, and natural language understanding tasks. This paper is a companion paper to a keynote talk at the 2020 International Solid-State Circuits Conference (ISSCC) discussing some of the advances in machine learning, and their implications on the kinds of computational devices we need to build, especially in the post-Moore's Law-era. It also discusses some of the ways that machine learning may also be able to help with some aspects of the circuit design process. Finally, it provides a sketch of at least one interesting direction towards much larger-scale multi-task models that are sparsely activated and employ much more dynamic, example- and task-based routing than the machine learning models of today.",0
"Title: ""The Artificial Intelligence Revolution and its implications on computer architecture""  Abstract: This paper explores the recent advancements made in artificial intelligence (AI) technology, specifically deep learning methods such as convolutional neural networks (CNNs). These techniques have achieved unprecedented accuracy across multiple domains including image classification, natural language processing, speech recognition and even games like Go. As these models continue to grow in size and complexity they demand ever greater computational resources to run efficiently, imposing significant challenges for system designers. In order to meet their demands, new hardware architectures tailored for machine learning workloads must be designed. We discuss some promising approaches such as specialized processors, memory hierarchies, parallelization strategies and cloud computing platforms. Although current state-of-the art models achieve remarkable results, there are still many open questions regarding model interpretability and generality. We highlight key directions that future research might take towards developing more robust, explainable and efficient AI systems able to perform well under diverse conditions beyond the limited training distributions.  Note that you may want to change the title from 'artificial intelligence revolution' if your focus is only on deep learning, which I assumed was the case given the CNN examples mentioned in the body but please clarify if you would prefer otherwise.",1
"In drug-discovery-related tasks such as virtual screening, machine learning is emerging as a promising way to predict molecular properties. Conventionally, molecular fingerprints (numerical representations of molecules) are calculated through rule-based algorithms that map molecules to a sparse discrete space. However, these algorithms perform poorly for shallow prediction models or small datasets. To address this issue, we present SMILES Transformer. Inspired by Transformer and pre-trained language models from natural language processing, SMILES Transformer learns molecular fingerprints through unsupervised pre-training of the sequence-to-sequence language model using a huge corpus of SMILES, a text representation system for molecules. We performed benchmarks on 10 datasets against existing fingerprints and graph-based methods and demonstrated the superiority of the proposed algorithms in small-data settings where pre-training facilitated good generalization. Moreover, we define a novel metric to concurrently measure model accuracy and data efficiency.",0
"Here is my attempt at writing an abstract for a paper about developing a pre-trained molecular fingerprint using deep learning techniques that can assist in low data drug discovery projects. Abstract: Recent advancements in deep learning have shown promising results in a variety of applications including drug discovery. In particular, transformer architecture has emerged as a powerful tool for encoding sequence information such as protein sequences into embeddings. Motivated by these successes, we propose SMILES Transformer, a novel method based on transformers that encodes molecules represented by Simplified Molecular Input Line Entry System (SMILES) strings into fixed length representations called molecular fingerprints. These molecular fingerprints encode structural features of each molecule which could aid researchers in tasks such as similarity search, clustering and virtual screening among others. We evaluate our model on several benchmark datasets consisting of diverse chemical compounds from the ZINC database and the USPTO patents dataset and demonstrate better performance than competitive baselines on multiple metrics. Our proposed approach provides an efficient solution to generate robust fingerprints even when limited amounts of training data is available. By providing a simple yet effective technique, we aim to enable faster discovery of new drugs for real world health challenges. Is there any other detail you would like me to add?",1
"This paper explores the task of interactive image retrieval using natural language queries, where a user progressively provides input queries to refine a set of retrieval results. Moreover, our work explores this problem in the context of complex image scenes containing multiple objects. We propose Drill-down, an effective framework for encoding multiple queries with an efficient compact state representation that significantly extends current methods for single-round image retrieval. We show that using multiple rounds of natural language queries as input can be surprisingly effective to find arbitrarily specific images of complex scenes. Furthermore, we find that existing image datasets with textual captions can provide a surprisingly effective form of weak supervision for this task. We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.",0
"This paper presents ""Drill-down,"" a method that allows users to interactively retrieve complex scenes from large databases using natural language queries. We propose three key ideas: (i) decomposing complex scenes into simpler subscenas, which can then be queried individually; (ii) automatically determining the appropriate level of detail for each query based on factors such as scene complexity and user preference; and (iii) providing intuitive interaction techniques to support exploration of retrieved scenes. Our approach has been evaluated through experiments involving real datasets and human subjects, demonstrating significant improvements over traditional keyword-based methods for retrieving complex scenes. The results show that our method significantly reduces search time while increasing user satisfaction and accuracy, making it suitable for applications such as video surveillance analysis, urban planning, and virtual reality design. Overall, the proposed method represents an important step towards enabling more effective human-machine collaboration for high-level data analytics tasks.",1
"Autonomous reinforcement learning agents, like children, do not have access to predefined goals and reward functions. They must discover potential goals, learn their own reward functions and engage in their own learning trajectory. Children, however, benefit from exposure to language, helping to organize and mediate their thought. We propose LE2 (Language Enhanced Exploration), a learning algorithm leveraging intrinsic motivations and natural language (NL) interactions with a descriptive social partner (SP). Using NL descriptions from the SP, it can learn an NL-conditioned reward function to formulate goals for intrinsically motivated goal exploration and learn a goal-conditioned policy. By exploring, collecting descriptions from the SP and jointly learning the reward function and the policy, the agent grounds NL descriptions into real behavioral goals. From simple goals discovered early to more complex goals discovered by experimenting on simpler ones, our agent autonomously builds its own behavioral repertoire. This naturally occurring curriculum is supplemented by an active learning curriculum resulting from the agent's intrinsic motivations. Experiments are presented with a simulated robotic arm that interacts with several objects including tools.",0
"Recent advances in natural language processing have focused on developing models that can ground language in real-world objects and events. This task, known as ""language grounding,"" requires agents to understand the meaning of linguistic concepts in terms of their referents in the world. However, current approaches often rely heavily on supervised learning from pre-labeled data, which limits their ability to generalize to new situations and domains.  In this work, we propose a novel approach to language grounding that combines social interactions and curiosity-driven multi-goal learning. We assume the existence of an intelligent agent that learns by interacting with humans in natural environments. To account for the importance of both intrinsic and extrinsic motivations in driving human behavior, our model uses a hybrid reward system consisting of a state-dependent intrinsic reward and a goal-based extrinsic reward.  Through a set of empirical studies, we demonstrate the effectiveness of our approach in grounding language using social interactions and curiosity-driven exploration. Our results show that agents trained using this method perform significantly better than baseline models on a variety of evaluation metrics, including zero-shot transfer to unseen tasks and languages.  Our findings suggest that social interactions and curiosity play crucial roles in enabling agents to learn more robust representations of the world and its connections to language. By combining these mechanisms with multi-goal reinforcement learning, we are able to create agents that can effectively ground language in complex real-world environments.",1
"The massive progress of machine learning has seen its application over a variety of domains in the past decade. But how do we develop a systematic, scalable and modular strategy to validate machine-learning systems? We present, to the best of our knowledge, the first approach, which provides a systematic test framework for machine-learning systems that accepts grammar-based inputs. Our OGMA approach automatically discovers erroneous behaviours in classifiers and leverages these erroneous behaviours to improve the respective models. OGMA leverages inherent robustness properties present in any well trained machine-learning model to direct test generation and thus, implementing a scalable test generation methodology. To evaluate our OGMA approach, we have tested it on three real world natural language processing (NLP) classifiers. We have found thousands of erroneous behaviours in these systems. We also compare OGMA with a random test generation approach and observe that OGMA is more effective than such random test generation by up to 489%.",0
"This study presents a new approach to testing machine learning systems called ""grammar based directed testing."" In traditional software testing methods, test cases are designed to verify that individual features work as expected. However, these methods often fail to account for interactions between different components of a system. With grammar based directed testing, we use formal grammars to specify sets of possible inputs, ensuring that all relevant combinations have been tested. We apply our methodology to two real-world case studies: image classification using convolutional neural networks (CNNs), and natural language processing (NLP) using recurrent neural networks (RNNs). Results show significant improvements over existing testing methods in terms of detecting previously unknown failure modes, reducing test suite sizes, and improving code coverage. Additionally, because our approach operates on high level specifications, rather than low level implementation details, results are easily portable across similar problems from other domains. Our work demonstrates the value of incorporating grammatical models into the design process itself - resulting in more reliable and robust systems overall.",1
"In this paper, we introduce a contextual grounding approach that captures the context in corresponding text entities and image regions to improve the grounding accuracy. Specifically, the proposed architecture accepts pre-trained text token embeddings and image object features from an off-the-shelf object detector as input. Additional encoding to capture the positional and spatial information can be added to enhance the feature quality. There are separate text and image branches facilitating respective architectural refinements for different modalities. The text branch is pre-trained on a large-scale masked language modeling task while the image branch is trained from scratch. Next, the model learns the contextual representations of the text tokens and image objects through layers of high-order interaction respectively. The final grounding head ranks the correspondence between the textual and visual representations through cross-modal interaction. In the evaluation, we show that our model achieves the state-of-the-art grounding accuracy of 71.36% over the Flickr30K Entities dataset. No additional pre-training is necessary to deliver competitive results compared with related work that often requires task-agnostic and task-specific pre-training on cross-modal dadasets. The implementation is publicly available at https://gitlab.com/necla-ml/grounding.",0
"This paper presents a novel approach to grounding natural language entities in images by leveraging contextual information that goes beyond traditional object detection methods. Our method utilizes deep learning techniques such as neural networks and attention mechanisms to learn relationships between text and image data. We evaluate our model on several benchmark datasets and show significant improvements over baseline models. Additionally, we demonstrate the applicability of our approach to real world applications such as visual question answering and image caption generation. Overall, our work contributes to the growing field of computer vision and natural language processing, pushing the boundaries of how machines can understand and interpret complex human inputs.",1
"Automatically describing video content with natural language has been attracting much attention in CV and NLP communities. Most existing methods predict one word at a time, and by feeding the last generated word back as input at the next time, while the other generated words are not fully exploited. Furthermore, traditional methods optimize the model using all the training samples in each epoch without considering their learning situations, which leads to a lot of unnecessary training and can not target the difficult samples. To address these issues, we propose a text-based dynamic attention model named TDAM, which imposes a dynamic attention mechanism on all the generated words with the motivation to improve the context semantic information and enhance the overall control of the whole sentence. Moreover, the text-based dynamic attention mechanism and the visual attention mechanism are linked together to focus on the important words. They can benefit from each other during training. Accordingly, the model is trained through two steps: ""starting from scratch"" and ""checking for gaps"". The former uses all the samples to optimize the model, while the latter only trains for samples with poor control. Experimental results on the popular datasets MSVD and MSR-VTT demonstrate that our non-ensemble model outperforms the state-of-the-art video captioning benchmarks.",0
"Automatic video captioning has been gaining attention due to its potential benefits such as accessibility for individuals with hearing impairments, indexing large amounts of video data for easier retrieval and navigation, and automatic subtitling across languages. In recent years, deep learning-based approaches have shown great promise in improving the quality of generated captions. However, most current methods suffer from limitations such as temporal incoherence, hallucination errors, and limited context awareness, which hinder their effectiveness in real-world applications.  In this work, we propose a novel approach that combines text-based dynamic attention mechanisms with stepwise learning techniques to tackle these challenges and improve the accuracy and coherency of video captions. Our method employs a combination of short-term and long-term dependencies to capture the complex interplay between different features in the video sequence and generate more accurate and meaningful captions. We demonstrate our model's superiority through extensive experiments on two benchmark datasets, showing significant improvements over state-of-the-art methods in both qualitative analysis and quantitative metrics.  Our contributions can be summarized as follows: (i) introduction of novel text-based dynamic attention mechanisms that adaptively assign weights to input elements based on contextual similarity; (ii) implementation of stepwise learning strategies that optimize performance by iteratively refining decisions at each time step; (iii) comprehensive evaluation against competitive baselines using standard benchmark measures, resulting in consistent improvement in captioning accuracy and coherency; and (iv) open source code release to encourage reproducibility and future research in the field of automatic video captioning. Overall, our proposed framework represents a promising advancement towards developing robust and reliable solutions for automatic video captioning systems.",1
"Program synthesis of general-purpose source code from natural language specifications is challenging due to the need to reason about high-level patterns in the target program and low-level implementation details at the same time. In this work, we present PATOIS, a system that allows a neural program synthesizer to explicitly interleave high-level and low-level reasoning at every generation step. It accomplishes this by automatically mining common code idioms from a given corpus, incorporating them into the underlying language for neural synthesis, and training a tree-based neural synthesizer to use these idioms during code generation. We evaluate PATOIS on two complex semantic parsing datasets and show that using learned code idioms improves the synthesizer's accuracy.",0
"Title: Program Synthesis and Semantic Parsing with Learned Code Idioms  This paper presents a novel approach for program synthesis and semantic parsing using learned code idioms. Our method leverages deep learning techniques to extract common patterns and idiomatic expressions from large corpuses of human-written code, which are then used as prior knowledge to guide the search process during program synthesis and semantic parsing. By explicitly modeling these idioms, our system can generate more concise, readable, and efficient programs compared to existing approaches that rely solely on general language models or hand-coded rules.  Our approach consists of two main components: (1) a code idiom extraction module that learns idiomatic patterns by analyzing large datasets of code; and (2) a guided search algorithm that uses the extracted idioms to rank candidate programs and facilitate the generation process. We evaluate our method through extensive experiments on several benchmark tasks involving both program synthesis and semantic parsing, demonstrating significant improvements over state-of-the-art baselines in terms of accuracy, compactness, and runtime performance.  Overall, this work represents an important step towards developing intelligent systems capable of generating high-quality code with minimal user intervention. With the increasing importance of software development across many domains, our approach has the potential to greatly impact the productivity and efficiency of developers worldwide.",1
"Accurately annotating large scale dataset is notoriously expensive both in time and in money. Although acquiring low-quality-annotated dataset can be much cheaper, it often badly damages the performance of trained models when using such dataset without particular treatment. Various methods have been proposed for learning with noisy labels. However, most methods only handle limited kinds of noise patterns, require auxiliary information or steps (e.g. , knowing or estimating the noise transition matrix), or lack theoretical justification. In this paper, we propose a novel information-theoretic loss function, $\mathcal{L}_{DMI}$, for training deep neural networks robust to label noise. The core of $\mathcal{L}_{DMI}$ is a generalized version of mutual information, termed Determinant based Mutual Information (DMI), which is not only information-monotone but also relatively invariant. \emph{To the best of our knowledge, $\mathcal{L}_{DMI}$ is the first loss function that is provably robust to instance-independent label noise, regardless of noise pattern, and it can be applied to any existing classification neural networks straightforwardly without any auxiliary information}. In addition to theoretical justification, we also empirically show that using $\mathcal{L}_{DMI}$ outperforms all other counterparts in the classification task on both image dataset and natural language dataset include Fashion-MNIST, CIFAR-10, Dogs vs. Cats, MR with a variety of synthesized noise patterns and noise amounts, as well as a real-world dataset Clothing1M. Codes are available at https://github.com/Newbeeer/L_DMI .",0
"Title: An Information-Theoretic Perspective on Robust Loss Functions Abstract Machine learning models are becoming increasingly important tools for making decisions and automating tasks across industries from healthcare to finance. However, these models can only provide accurate results if they are trained using appropriate loss functions. In recent years, researchers have focused on developing noise-robust loss functions that can handle noisy labels more effectively than traditional approaches like cross entropy. This paper takes an information-theoretical perspective on robust loss functions and introduces a new measure called Least Discernible Mean Increment (L_DMI). We show how this measure provides a lower bound on mutual information, which leads to better generalization performance compared to existing approaches. Our experiments on real datasets demonstrate the effectiveness of L_DMI as a noise-robust loss function in both regression and classification settings. Overall, our work offers insights into the properties of robust loss functions that make them effective at handling label uncertainty while minimizing information loss during training.",1
"As a general-purpose generative model architecture, VAE has been widely used in the field of image and natural language processing. VAE maps high dimensional sample data into continuous latent variables with unsupervised learning. Sampling in the latent variable space of the feature, VAE can construct new image or text data. As a general-purpose generation model, the vanilla VAE can not fit well with various data sets and neural networks with different structures. Because of the need to balance the accuracy of reconstruction and the convenience of latent variable sampling in the training process, VAE often has problems known as ""posterior collapse"". images reconstructed by VAE are also often blurred. In this paper, we analyze the main cause of these problem, which is the lack of mutual information between the sample variable and the latent feature variable during the training process. To maintain mutual information in model training, we propose to use the auxiliary softmax multi-classification network structure to improve the training effect of VAE, named VAE-AS. We use MNIST and Omniglot data sets to test the VAE-AS model. Based on the test results, It can be show that VAE-AS has obvious effects on the mutual information adjusting and solving the posterior collapse problem.",0
"In recent years, Variational Autoencoders (VAEs) have emerged as a powerful tool for generative modeling tasks, such as image generation and data compression. However, one limitation of VAEs is their difficulty in handling multi-label classification problems, where multiple labels can be assigned to each input sample. To address this issue, we propose using an Auxiliary Softmax Multiclassifier (ASM) to improve the performance of VAEs on these types of problems. Our approach involves training two parallel models: a VAE that encodes the input into a latent space, and an ASM that maps inputs directly to label predictions. We use both reconstruction loss and cross entropy loss from the ASM as a form of regularization during training. Experimental results show that our method significantly outperforms other state-of-the-art methods for multi-label learning with VAEs. Furthermore, we demonstrate the effectiveness of our model by applying it to several real-world datasets and showing improved performance across all metrics. Overall, our work highlights the potential of combining VAEs and auxiliary classifiers for more effective multi-label learning.",1
"Deep learning achieves state-of-the-art results in many tasks in computer vision and natural language processing. However, recent works have shown that deep networks can be vulnerable to adversarial perturbations, which raised a serious robustness issue of deep networks. Adversarial training, typically formulated as a robust optimization problem, is an effective way of improving the robustness of deep networks. A major drawback of existing adversarial training algorithms is the computational overhead of the generation of adversarial examples, typically far greater than that of the network training. This leads to the unbearable overall computational cost of adversarial training. In this paper, we show that adversarial training can be cast as a discrete time differential game. Through analyzing the Pontryagin's Maximal Principle (PMP) of the problem, we observe that the adversary update is only coupled with the parameters of the first layer of the network. This inspires us to restrict most of the forward and back propagation within the first layer of the network during adversary updates. This effectively reduces the total number of full forward and backward propagation to only one for each group of adversary updates. Therefore, we refer to this algorithm YOPO (You Only Propagate Once). Numerical experiments demonstrate that YOPO can achieve comparable defense accuracy with approximately 1/5 ~ 1/4 GPU time of the projected gradient descent (PGD) algorithm. Our codes are available at https://https://github.com/a1600012888/YOPO-You-Only-Propagate-Once.",0
"Increasing training data size often leads to improvements on benchmark tasks but is computationally expensive due to the need for retraining models from scratch. We introduce the concept of adversarial propagation that leverages pretrained models to accelerate adversarial fine tuning using only a few gradient steps. Our method starts by generating adversarial examples of the input images using pretrained generative models such as DALL-E. This step helps us create more diverse set of adversarial examples than traditional methods that rely solely on adding noise. Then we use these adversarial examples along with their corresponding clean inputs to update the parameters of our model. Since the gradients flow backwards through unchanged pretrained weights during propagation, this process only takes a handful of iterations compared to full fine tuning where all layers are updated together. By doing so, the performance gap between fine tuned models trained with larger dataset versus smaller datasets shrinks considerably resulting in substantial speedup of up to 6x without sacrificing accuracy. Finally, our experiments show that minimal impact on memory usage means that this technique can easily scale to massive image classification datasets like ImageNet.",1
"In recent years, huge amounts of unstructured textual data on the Internet are a big difficulty for AI algorithms to provide the best recommendations for users and their search queries. Since the Internet became widespread, a lot of research has been done in the field of Natural Language Processing (NLP) and machine learning. Almost every solution transforms documents into Vector Space Models (VSM) in order to apply AI algorithms over them. One such approach is based on Case-Based Reasoning (CBR). Therefore, the most important part of those systems is to compute the similarity between numerical data points. In 2016, the new similarity TS-SS metric is proposed, which showed state-of-the-art results in the field of textual mining for unsupervised learning. However, no one before has investigated its performances for supervised learning (classification task). In this work, we devised a CBR system capable of finding the most similar documents for a given query aiming to investigate performances of the new state-of-the-art metric, TS-SS, in addition to the two other geometrical similarity measures --- Euclidean distance and Cosine similarity --- that showed the best predictive results over several benchmark corpora. The results show surprising inappropriateness of TS-SS measure for high dimensional features.",0
"This paper presents a new approach to finding the most similar textual documents based on case-based reasoning. Traditional approaches such as cosine similarity require vast computational resources and are limited by their linear time complexity which makes them unsuitable for large datasets. Our proposed methodology uses the concept of cases stored in memory along with learned experiences from previous computations to find the closest matches in realtime without requiring significant computational power. We demonstrate through experimental results that our approach outperforms standard methods significantly both in terms of accuracy and speed making it ideal for big data applications where scalability and efficiency are critical factors. Furthermore, we discuss the potential for future improvements and extensions to apply this technique beyond the scope of document retrieval. By providing an efficient solution for high precision similarity search, this work has wide ranging implications across all domains dealing with natural language processing such as machine translation, question answering and sentiment analysis amongst others.",1
"Emotion recognition is a classic field of research with a typical setup extracting features and feeding them through a classifier for prediction. On the other hand, generative models jointly capture the distributional relationship between emotions and the feature profiles. Relatively recently, Generative Adversarial Networks (GANs) have surfaced as a new class of generative models and have shown considerable success in modeling distributions in the fields of computer vision and natural language understanding. In this work, we experiment with variants of GAN architectures to generate feature vectors corresponding to an emotion in two ways: (i) A generator is trained with samples from a mixture prior. Each mixture component corresponds to an emotional class and can be sampled to generate features from the corresponding emotion. (ii) A one-hot vector corresponding to an emotion can be explicitly used to generate the features. We perform analysis on such models and also propose different metrics used to measure the performance of the GAN models in their ability to generate realistic synthetic samples. Apart from evaluation on a given dataset of interest, we perform a cross-corpus study where we study the utility of the synthetic samples as additional training data in low resource conditions.",0
"In this paper, we propose a novel method for modeling feature representations for affective speech using generative adversarial networks (GANs). We aim to capture both acoustic and linguistic features that can be used as input for machine learning models. Traditionally, feature extraction methods rely on handcrafted rules or predefined heuristics to extract relevant features from audio signals. However, these methods often struggle with capturing complex interactions across different domains. GANs provide us with a powerful alternative by allowing the training of deep neural networks to generate high quality data. We utilize two subnetworks: one generator network which takes raw speech inputs and generates highly realistic speech features; another discriminator network which determines whether the generated features are authentic or fake. Our experiments show promising results and demonstrate that our approach outperforms traditional feature extraction methods while producing more accurate emotional classification. Our proposed framework has great potential for future research in fields such as speech recognition, natural language processing, and human-computer interaction.",1
"Matrices satisfying the Restricted Isometry Property (RIP) play an important role in the areas of compressed sensing and statistical learning. RIP matrices with optimal parameters are mainly obtained via probabilistic arguments, as explicit constructions seem hard. It is therefore interesting to ask whether a fixed matrix can be incorporated into a construction of restricted isometries. In this paper, we construct a new broad ensemble of random matrices with dependent entries that satisfy the restricted isometry property. Our construction starts with a fixed (deterministic) matrix $X$ satisfying some simple stable rank condition, and we show that the matrix $XR$, where $R$ is a random matrix drawn from various popular probabilistic models (including, subgaussian, sparse, low-randomness, satisfying convex concentration property), satisfies the RIP with high probability. These theorems have various applications in signal recovery, random matrix theory, dimensionality reduction, etc. Additionally, motivated by an application for understanding the effectiveness of word vector embeddings popular in natural language processing and machine learning applications, we investigate the RIP of the matrix $XR^{(l)}$ where $R^{(l)}$ is formed by taking all possible (disregarding order) $l$-way entrywise products of the columns of a random matrix $R$.",0
"The authors study the effect that correlations can have on the performance of Compressive Sensing (CS) algorithms in recovering signals from undersampled linear measurements using the concept of Restricted Isometry Property (RIP). Their main finding is that CS can still successfully reconstruct signals even in cases where there are strong correlations among some of the signal components. This counterintuitive result has important implications for applications such as image compression and video processing, where real-world data often exhibits high degrees of correlation. In their analysis, the authors employ tools from random matrix theory, including concentration bounds on the singular values of Gaussian matrices. These results provide theoretical support for the use of CS methods in practice and highlight promising directions for future research.",1
"In practical applications of machine learning, it is often desirable to identify and abstain on examples where the model's predictions are likely to be incorrect. Much of the prior work on this topic focused on out-of-distribution detection or performance metrics such as top-k accuracy. Comparatively little attention was given to metrics such as area-under-the-curve or Cohen's Kappa, which are extremely relevant for imbalanced datasets. Abstention strategies aimed at top-k accuracy can produce poor results on these metrics when applied to imbalanced datasets, even when all examples are in-distribution. We propose a framework to address this gap. Our framework leverages the insight that calibrated probability estimates can be used as a proxy for the true class labels, thereby allowing us to estimate the change in an arbitrary metric if an example were abstained on. Using this framework, we derive computationally efficient metric-specific abstention algorithms for optimizing the sensitivity at a target specificity level, the area under the ROC, and the weighted Cohen's Kappa. Because our method relies only on calibrated probability estimates, we further show that by leveraging recent work on domain adaptation under label shift, we can generalize to test-set distributions that may have a different class imbalance compared to the training set distribution. On various experiments involving medical imaging, natural language processing, computer vision and genomics, we demonstrate the effectiveness of our approach. Source code available at https://github.com/blindauth/abstention. Colab notebooks reproducing results available at https://github.com/blindauth/abstention_experiments.",0
"In many real-world applications, datasets tend to be imbalanced, meaning that one class has significantly more instances than others. This can cause issues during model training as well as in prediction, leading to lower accuracy and potentially skewed results. One popular approach to addressing this problem is abstaining from making predictions for certain classes based on their prevalence in the data. However, determining which classes should be excluded and how frequently to exclude them remains challenging. To tackle these issues, we propose a flexible and adaptive framework for abstention under class imbalance. Our method takes into account both dataset characteristics and desired performance metrics to dynamically determine when and which classes to abstain from predicting. We evaluate our framework using several benchmark datasets and compare its effectiveness against other methods. Results show that our approach consistently outperforms existing techniques in terms of accuracy, precision, recall, and G-mean. Furthermore, we demonstrate that our framework can handle different types of imbalance and achieve better calibration. Overall, our work provides a powerful tool for handling imbalanced data in practice by offering a simple yet effective solution for implementing strategic abstention.",1
"Transformer architectures show significant promise for natural language processing. Given that a single pretrained model can be fine-tuned to perform well on many different tasks, these networks appear to extract generally useful linguistic features. A natural question is how such networks represent this information internally. This paper describes qualitative and quantitative investigations of one particularly effective model, BERT. At a high level, linguistic features seem to be represented in separate semantic and syntactic subspaces. We find evidence of a fine-grained geometric representation of word senses. We also present empirical descriptions of syntactic representations in both attention matrices and individual word embeddings, as well as a mathematical argument to explain the geometry of these representations.",0
"In this paper we show how to visualize and measure the geometry of BERT (Bidirectional Encoder Representations from Transformers). We use techniques from information theory and manifold learning to analyze the high-dimensional parameter space of BERT and reveal hidden patterns that cannot easily be seen through traditional means such as word embeddings. Our approach allows us to identify clusters within the embedding space corresponding to semantically meaningful categories, as well as directions corresponding to semantic relationships like synonymy and antonymy. These results provide new insights into the inner workings of large language models and open up opportunities for further analysis and optimization of these systems.",1
"Program comprehension is a fundamental task in software development and maintenance processes. Software developers often need to understand a large amount of existing code before they can develop new features or fix bugs in existing programs. Being able to process programming language code automatically and provide summaries of code functionality accurately can significantly help developers to reduce time spent in code navigation and understanding, and thus increase productivity. Different from natural language articles, source code in programming languages often follows rigid syntactical structures and there can exist dependencies among code elements that are located far away from each other through complex control flows and data flows. Existing studies on tree-based convolutional neural networks (TBCNN) and gated graph neural networks (GGNN) are not able to capture essential semantic dependencies among code elements accurately. In this paper, we propose novel tree-based capsule networks (TreeCaps) and relevant techniques for processing program code in an automated way that encodes code syntactical structures and captures code dependencies more accurately. Based on evaluation on programs written in different programming languages, we show that our TreeCaps-based approach can outperform other approaches in classifying the functionalities of many programs.",0
"Abstract: Deep learning has made significant progress in recent years due to advancements in neural network architectures such as capsules. Capsules have been shown to capture more detailed features than traditional convolutional networks (ConvNets) while reducing computational complexity and increasing interpretability. However, there still exist challenges related to processing program source code using these deep learning models. In particular, traditional ConvNet architectures struggle to identify meaningful patterns at varying scales within the codebase, leading to suboptimal performance for tasks like bug detection, security vulnerability analysis, and refactoring suggestions. To address this limitation, we propose TreeCaps: tree-structured capsule networks for effective program source code processing. Our approach exploits both spatial hierarchies present in code elements and structural relationships between components at different levels, enabling our model to learn more nuanced representations tailored towards software engineering tasks. We evaluate our method on multiple benchmark datasets against state-of-the-art baselines, demonstrating improved accuracy and robustness across several programming language contexts. This work paves the way for applying advanced deep learning techniques to tackle real-world challenges faced by developers during development and maintenance phases, thereby facilitating the construction of efficient, maintainable software systems.",1
"In this paper, we present Gamma-LSTM, an enhanced long short term memory (LSTM) unit, to enable learning of hierarchical representations through multiple stages of temporal abstractions. Gamma memory, a hierarchical memory unit, forms the central memory of Gamma-LSTM with gates to regulate the information flow into various levels of hierarchy, thus providing the unit with a control to pick the appropriate level of hierarchy to process the input at a given instant of time. We demonstrate better performance of Gamma-LSTM model regular and stacked LSTMs in two settings (pixel-by-pixel MNIST digit classification and natural language inference) placing emphasis on the ability to generalize over long sequences.",0
"This paper presents a new architecture that builds upon the widely used Long Short-Term Memory (LSTM) network by integrating external knowledge into its cell memory state through an attention mechanism over semantic concepts extracted from text corpora. By doing so, we show significant improvements compared to standard LSTMs for natural language understanding tasks such as sentiment analysis, machine translation, question answering, and text classification on three benchmark datasets commonly used for these tasks. Our method achieves state-of-the-art results on most of these benchmarks while being simple to implement and requiring little additional computational cost at inference time. Furthermore, our findings offer insight into how models can effectively integrate prior knowledge from pre-trained sources without compromising their ability to generalize across different domains or languages. Overall, our work suggests a promising direction towards building more capable sequence models able to handle complex patterns in sequential data beyond the scope of traditional LSTM architectures alone.",1
"Cancer is one of the leading cause of death, worldwide. Many believe that genomic data will enable us to better predict the survival time of these patients, which will lead to better, more personalized treatment options and patient care. As standard survival prediction models have a hard time coping with the high-dimensionality of such gene expression (GE) data, many projects use some dimensionality reduction techniques to overcome this hurdle. We introduce a novel methodology, inspired by topic modeling from the natural language domain, to derive expressive features from the high-dimensional GE data. There, a document is represented as a mixture over a relatively small number of topics, where each topic corresponds to a distribution over the words; here, to accommodate the heterogeneity of a patient's cancer, we represent each patient (~document) as a mixture over cancer-topics, where each cancer-topic is a mixture over GE values (~words). This required some extensions to the standard LDA model eg: to accommodate the ""real-valued"" expression values - leading to our novel ""discretized"" Latent Dirichlet Allocation (dLDA) procedure. We initially focus on the METABRIC dataset, which describes breast cancer patients using the r=49,576 GE values, from microarrays. Our results show that our approach provides survival estimates that are more accurate than standard models, in terms of the standard Concordance measure. We then validate this approach by running it on the Pan-kidney (KIPAN) dataset, over r=15,529 GE values - here using the mRNAseq modality - and find that it again achieves excellent results. In both cases, we also show that the resulting model is calibrated, using the recent ""D-calibrated"" measure. These successes, in two different cancer types and expression modalities, demonstrates the generality, and the effectiveness, of this approach.",0
"In recent years, there has been growing interest in using gene expression data to predict survival outcomes for cancer patients. However, analyzing large datasets of gene expression profiles can be challenging due to their high dimensionality and complexity. To address these issues, we propose a topic modeling approach that identifies latent patterns in gene expression data and uses them to improve survival prediction accuracy. Our method first preprocesses the raw microarray data by performing background correction, normalization, and summarizing the data into a lower dimension representation. We then apply Latent Dirichlet Allocation (LDA), a popular topic modeling technique, to identify groups of genes that consistently co-occur across different samples. Finally, we use survival analysis techniques to examine the association between these latent topics and patient outcome. Experimental results on two publicly available breast cancer datasets demonstrate that our method significantly improves survival prediction accuracy compared to traditional approaches that rely solely on individual genes or handcrafted features. This work represents an important step towards developing more accurate prognostic tools for cancer patients.  Keywords: Gene expression profiling; Topic models; Breast cancer; Survival analysis; Bioinformatics.",1
"Deep Neural Networks are often though to lack interpretability due to the distributed nature of their internal representations. In contrast, humans can generally justify, in natural language, for their answer to a visual question with simple common sense reasoning. However, human introspection abilities have their own limits as one often struggles to justify for the recognition process behind our lowest level feature recognition ability: for instance, it is difficult to precisely explain why a given texture seems more characteristic of the surface of a finger nail rather than a plastic bottle. In this paper, we showcase an application in which deep learning models can actually help human experts justify for their own low-level visual recognition process: We study the problem of assessing the adhesive potency of copper sheets from microscopic pictures of their surface. Although highly trained material experts are able to qualitatively assess the surface adhesive potency, they are often unable to precisely justify for their decision process. We present a model that, under careful design considerations, is able to provide visual clues for human experts to understand and justify for their own recognition process. Not only can our model assist human experts in their interpretation of the surface characteristics, we show how this model can be used to test different hypothesis of the copper surface response to different manufacturing processes.",0
"""Visual processing techniques have become increasingly important as technology advances and new methods emerge that allow humans to better interpret and analyze complex data sets derived from sensory inputs. However, these technologies often require expert knowledge to effectively utilize, which can lead to difficulties when interpreting results accurately. In order to address this challenge, researchers developed an artificial intelligence system designed to assist human experts in understanding the implications of their visual data analysis. This work presents a detailed evaluation of such a system applied to analyzing the potential adhesiveness of copper surfaces. By examining key parameters involved in determining whether a sample exhibits sufficient adhesion strength, including roughness and oxidation level, the authors demonstrate how the AI system was able to provide valuable insights into the nature of the material being investigated. Results show promising improvements over existing methodologies in terms of speed and accuracy in identifying areas of low adhesive quality. Ultimately, the findings suggest that incorporating advanced AI systems into human expert workflows has significant advantages in achieving optimal outcomes.""",1
"Word embeddings learnt from large corpora have been adopted in various applications in natural language processing and served as the general input representations to learning systems. Recently, a series of post-processing methods have been proposed to boost the performance of word embeddings on similarity comparison and analogy retrieval tasks, and some have been adapted to compose sentence representations. The general hypothesis behind these methods is that by enforcing the embedding space to be more isotropic, the similarity between words can be better expressed. We view these methods as an approach to shrink the covariance/gram matrix, which is estimated by learning word vectors, towards a scaled identity matrix. By optimising an objective in the semi-Riemannian manifold with Centralised Kernel Alignment (CKA), we are able to search for the optimal shrinkage parameter, and provide a post-processing method to smooth the spectrum of learnt word vectors which yields improved performance on downstream tasks.",0
"Abstract: This research study investigates different post-processing methods for pre-trained word embeddings to improve their performance in downstream natural language processing tasks. We compare several popular techniques including mean averaging, weighted averaging, and ensembling and evaluate them using three NLP tasks – sentiment analysis, part-of-speech tagging and named entity recognition. Our results show that while all post-processing methods lead to some improvement over individual fine-tuned models, there is little difference in overall performance across these methods. Furthermore, we find evidence to suggest that ensemble models perform better than any single model regardless of whether they have been subjected to additional post-processing. These findings provide valuable insights into the efficacy of post-processing methods for word embeddings and offer guidance for future research in this area. Keywords: word embeddings, post-processing methods, natural language processing, sentiment analysis, part-of-speech tagging, named entity recognition.  -----Hello! I am here to assist you. How can I help?",1
"Visual grounding, a task to ground (i.e., localize) natural language in images, essentially requires composite visual reasoning. However, existing methods over-simplify the composite nature of language into a monolithic sentence embedding or a coarse composition of subject-predicate-object triplet. In this paper, we propose to ground natural language in an intuitive, explainable, and composite fashion as it should be. In particular, we develop a novel modular network called Neural Module Tree network (NMTree) that regularizes the visual grounding along the dependency parsing tree of the sentence, where each node is a neural module that calculates visual attention according to its linguistic feature, and the grounding score is accumulated in a bottom-up direction where as needed. NMTree disentangles the visual grounding from the composite reasoning, allowing the former to only focus on primitive and easy-to-generalize patterns. To reduce the impact of parsing errors, we train the modules and their assembly end-to-end by using the Gumbel-Softmax approximation and its straight-through gradient estimator, accounting for the discrete nature of module assembly. Overall, the proposed NMTree consistently outperforms the state-of-the-arts on several benchmarks. Qualitative results show explainable grounding score calculation in great detail.",0
"In recent years, neural network architecture design has seen significant advances driven by techniques such as autoencoders, growth processes, reinforcement learning, and evolutionary algorithms [4, 7]. These methods have demonstrated success at discovering novel architectures that outperform hand-crafted designs across various domains such as computer vision, natural language processing, and generative modeling [9, 6, 8]. We posit that the discovery of effective modules and their composition, which we define as connecting experts through attention mechanisms, remains one important direction towards better automating deep learning research. Motivated by these developments, we introduce LearnToAssembly (L2A), a new method enabling gradient descent training of module trees whose nodes correspond to arbitrary computational graphs including popular operators such as convolutions, self-attention blocks, feedforward networks, residual connections, dropouts etc. L2A leverages continuous relaxations via softmax functions to enable backpropagation through non-differentiable operations like topology changes. Further, we develop new components for the system, including ones that generate structured latent spaces for neural modules based on human feedback from visual grounding tasks [10]. We demonstrate the efficacy of our framework using two downstream applications: generating images conditioned on text prompts and image synthesis from scene graphs. Across numerous experiments, we observe state-of-the-art performance compared against prior arts trained under comparable settings. We believe L2A can serve as a foundation for future work aiming to assemble customized models tailored for particular use cases. -----",1
"How can we teach a robot to predict what will happen next for an activity it has never seen before? We address this problem of zero-shot anticipation by presenting a hierarchical model that generalizes instructional knowledge from large-scale text-corpora and transfers the knowledge to the visual domain. Given a portion of an instructional video, our model predicts coherent and plausible actions multiple steps into the future, all in rich natural language. To demonstrate the anticipation capabilities of our model, we introduce the Tasty Videos dataset, a collection of 2511 recipes for zero-shot learning, recognition and anticipation.",0
"Here is my attempt: Instructional activities play an important role in education as they provide students with guidance on how to perform tasks effectively. However, these activities can sometimes lack sufficient feedback to support learners in achieving their goals. To address this challenge, we propose a new method that integrates zero-shot learning and natural language processing techniques to enable intelligent agents to anticipate user needs during instructional activities. Our approach leverages contextual cues from both the learner’s environment and past interactions to make predictions about which actions should be recommended next. We evaluate our method through several experiments involving different types of instructional scenarios and demonstrate its effectiveness in improving task completion rates and reducing cognitive load on learners. Our findings suggest that zero-shot anticipation has great potential for enhancing personalized educational experiences. What would you like me to change?",1
"Time Series Classification (TSC) has been an important and challenging task in data mining, especially on multivariate time series and multi-view time series data sets. Meanwhile, transfer learning has been widely applied in computer vision and natural language processing applications to improve deep neural network's generalization capabilities. However, very few previous works applied transfer learning framework to time series mining problems. Particularly, the technique of measuring similarities between source domain and target domain based on dynamic representation such as density estimation with importance sampling has never been combined with transfer learning framework. In this paper, we first proposed a general adaptive transfer learning framework for multi-view time series data, which shows strong ability in storing inter-view importance value in the process of knowledge transfer. Next, we represented inter-view importance through some time series similarity measurements and approximated the posterior distribution in latent space for the importance sampling via density estimation techniques. We then computed the matrix norm of sampled importance value, which controls the degree of knowledge transfer in pre-training process. We further evaluated our work, applied it to many other time series classification tasks, and observed that our architecture maintained desirable generalization ability. Finally, we concluded that our framework could be adapted with deep learning techniques to receive significant model performance improvements.",0
"This paper presents a novel approach to multi-view time series classification using adaptive transfer learning. We introduce a framework that allows multiple models trained on different views to collaborate and share knowledge, improving their overall performance. Our method uses a Bayesian model selection approach to dynamically allocate computational resources based on the importance of each view, enabling efficient use of computational power. Experimental results demonstrate the effectiveness of our method, which outperforms state-of-the-art baseline methods across several benchmark datasets. Our work shows promise for applications such as sensor fault detection and predictive maintenance.",1
"In this paper, we investigate a novel problem of telling the difference between image pairs in natural language. Compared to previous approaches for single image captioning, it is challenging to fetch linguistic representation from two independent visual information. To this end, we have proposed an effective encoder-decoder caption framework based on Hyper Convolution Net. In addition, a series of novel feature fusing techniques for pairwise visual information fusing are introduced and a discriminating referee is proposed to evaluate the pipeline. Because of the lack of appropriate datasets to support this task, we have collected and annotated a large new dataset with Amazon Mechanical Turk (AMT) for generating captions in a pairwise manner (with 14764 images and 26710 image pairs in total). The dataset is the first one on the relative difference caption task that provides descriptions in free language. We evaluate the effectiveness of our model on two datasets in the field and it outperforms the state-of-the-art approach by a large margin.",0
"In recent years, visual descriptors have become increasingly important in image recognition tasks as they provide discriminative features that can distinguish objects from one another. However, existing methods often struggle in accurately describing subtle differences between similar images, leading to poor performance on fine-grained classification tasks such as species identification in biological imaging. To address these limitations, we propose a new approach called ""Tell-The-Difference"" (TTD) which focuses on finding the most salient regions between two input images for accurate differentiation. Our method incorporates both generative models and discriminative mechanisms to ensure precise localization of differences at pixel level while preserving global object structures. We further introduce an innovative refereeing mechanism that filters out irrelevant distinctions proposed by the TTD model, ensuring only meaningful discrepancies are highlighted for improved accuracy in visual descriptor generation. Experimental results on several benchmark datasets demonstrate the effectiveness of our approach in achieving state-of-the-art performance on challenging fine-grained classification problems. Overall, the proposed approach provides a promising solution towards enhancing visual descriptor quality for complex pattern recognition tasks.",1
"In order to successfully perform tasks specified by natural language instructions, an artificial agent operating in a visual world needs to map words, concepts, and actions from the instruction to visual elements in its environment. This association is termed as Task-Oriented Grounding. In this work, we propose a novel Dynamic Attention Network architecture for the efficient multi-modal fusion of text and visual representations which can generate a robust definition of state for the policy learner. Our model assumes no prior knowledge from visual and textual domains and is an end to end trainable. For a 3D visual world where the observation changes continuously, the attention on the visual elements tends to be highly co-related from a one-time step to the next. We term this as ""Dynamic Attention"". In this work, we show that Dynamic Attention helps in achieving grounding and also aids in the policy learning objective. Since most practical robotic applications take place in the real world where the observation space is continuous, our framework can be used as a generalized multi-modal fusion unit for robotic control through natural language. We show the effectiveness of using 1D convolution over Gated Attention Hadamard product on the rate of convergence of the network. We demonstrate that the cell-state of a Long Short Term Memory (LSTM) is a natural choice for modeling Dynamic Attention and shows through visualization that the generated attention is very close to how humans tend to focus on the environment.",0
"Title: Dynamic Attention Networks for Task Oriented Grounding Abstract: In natural language processing, grounding refers to identifying the concrete objects referred to by linguistic expressions. This task can benefit from task-oriented approaches that fine-tune models on specific domains, but existing methods often use static attention mechanisms which may limit their performance. We propose dynamic attention networks (DAN), a novel model architecture designed specifically for task-oriented grounding that dynamically adjusts its focus during inference time. Our network uses convolutional neural networks to encode input sentences and images, combined with LSTM units that maintain a sequential history of features over time. By comparing these representations, our DAN model computes distributions over possible reference assignments for each expression in realtime, allowing for more flexible and accurate reasoning than other static approaches. Through extensive experimentation across four diverse datasets spanning multiple domains, we demonstrate the effectiveness of our approach compared to prior state-of-the-art methods, confirming the promise of dynamic attention networks for grounding tasks under different domain assumptions and levels of supervision.",1
"Owing to their ability to both effectively integrate information over long time horizons and scale to massive amounts of data, self-attention architectures have recently shown breakthrough success in natural language processing (NLP), achieving state-of-the-art results in domains such as language modeling and machine translation. Harnessing the transformer's ability to process long time horizons of information could provide a similar performance boost in partially observable reinforcement learning (RL) domains, but the large-scale transformers used in NLP have yet to be successfully applied to the RL setting. In this work we demonstrate that the standard transformer architecture is difficult to optimize, which was previously observed in the supervised learning setting but becomes especially pronounced with RL objectives. We propose architectural modifications that substantially improve the stability and learning speed of the original Transformer and XL variant. The proposed architecture, the Gated Transformer-XL (GTrXL), surpasses LSTMs on challenging memory environments and achieves state-of-the-art results on the multi-task DMLab-30 benchmark suite, exceeding the performance of an external memory architecture. We show that the GTrXL, trained using the same losses, has stability and performance that consistently matches or exceeds a competitive LSTM baseline, including on more reactive tasks where memory is less critical. GTrXL offers an easy-to-train, simple-to-implement but substantially more expressive architectural alternative to the standard multi-layer LSTM ubiquitously used for RL agents in partially observable environments.",0
"Neural networks have revolutionized fields such as computer vision and natural language processing over the past decade by providing state-of-the-art performance on numerous tasks. One specific architecture that has gained popularity recently due to their impressive results is transformers. While these models exhibit remarkable performance on many benchmark datasets, they remain challenging to optimize effectively because of their architectural complexity, making them difficult to apply directly to reinforcement learning problems. In this work, we focus on stabilizing transformer training through effective regularization methods and propose several novel techniques targeted at addressing common issues encountered during optimization. Our approach includes a weight decay method tailored specifically towards transformer parameters, dropout layers inserted into various positions within the model, and a mixed objective function combining the main reward objective with an auxiliary reconstruction loss term. By applying these techniques, we improve the stability and efficiency of training and achieve significant improvements in both training convergence speed and overall task performance compared to existing methods. We evaluate our algorithm across three diverse domains: image classification using CIFAR-10 and ImageNet, machine translation using the WMT2014 English-German dataset, and text generation with the LAMBADA corpus. Experimental results consistently demonstrate the effectiveness of our method in all three settings, leading us to believe that the proposed framework could provide a solid foundation for future research in RL applied to other high-level cognitive functions involving sequential decision making processes.",1
"We present a new task of query auto-completion for estimating instance probabilities. We complete a user query prefix conditioned upon an image. Given the complete query, we fine tune a BERT embedding for estimating probabilities of a broad set of instances. The resulting instance probabilities are used for selection while being agnostic to the segmentation or attention mechanism. Our results demonstrate that auto-completion using both language and vision performs better than using only language, and that fine tuning a BERT embedding allows to efficiently rank instances in the image. In the spirit of reproducible research we make our data, models, and code available.",0
This paper presents a model that generates natural language queries automatically to ask questions related to instance probabilities. The method uses visual context provided by a user as input and suggests text to formulate a question. The system then evaluates the likelihood of different possible answers and outputs the predicted response. We evaluate our approach on several benchmark datasets and show significant improvements over baseline methods. Our results demonstrate the effectiveness of our query auto-completion model for estimating instance probabilities using only visual cues. Implications and future work directions are discussed.  Is there anything else I can assist you with?,1
"Referring expressions are natural language descriptions that identify a particular object within a scene and are widely used in our daily conversations. In this work, we focus on segmenting the object in an image specified by a referring expression. To this end, we propose an end-to-end trainable comprehension network that consists of the language and visual encoders to extract feature representations from both domains. We introduce the spatial-aware dynamic filters to transfer knowledge from text to image, and effectively capture the spatial information of the specified object. To better communicate between the language and visual modules, we employ a caption generation network that takes features shared across both domains as input, and improves both representations via a consistency that enforces the generated sentence to be similar to the given referring expression. We evaluate the proposed framework on two referring expression datasets and show that our method performs favorably against the state-of-the-art algorithms.",0
"Object segmentation refers to the task of identifying each object in an image, including details such as its shape and size. A common method used in object segmentation involves referring expressions, which refer to textual descriptions that describe specific parts or aspects of objects within images. In recent years, there has been significant progress made in developing methods that can effectively identify these expressions, but still more work remains to be done on accurately segmenting objects based on these expressions. This paper proposes a new approach called Caption-Aware Consistency (CAC) to improve the accuracy of object segmentation using referring expressions. By leveraging large amounts of data from existing datasets, CAC is able to learn patterns in how referring expressions refer to objects in images and use this knowledge to guide the segmentation process. Experimental results demonstrate the effectiveness of our proposed method compared to state-of-the-art techniques in object segmentation using referring expressions. Our research contributes towards advancing computer vision technology and improving the ability of machines to interpret visual content.",1
"We investigate time-dependent data analysis from the perspective of recurrent kernel machines, from which models with hidden units and gated memory cells arise naturally. By considering dynamic gating of the memory cell, a model closely related to the long short-term memory (LSTM) recurrent neural network is derived. Extending this setup to $n$-gram filters, the convolutional neural network (CNN), Gated CNN, and recurrent additive network (RAN) are also recovered as special cases. Our analysis provides a new perspective on the LSTM, while also extending it to $n$-gram convolutional filters. Experiments are performed on natural language processing tasks and on analysis of local field potentials (neuroscience). We demonstrate that the variants we derive from kernels perform on par or even better than traditional neural methods. For the neuroscience application, the new models demonstrate significant improvements relative to the prior state of the art.",0
"This work presents recent advances in kernel-based approaches for sequence modeling problems. In particular we examine the connections between kernels used in neural networks such as convolutional neural networks (CNNs) for image processing, recurrent neural networks (RNNs), and transformers which have been adapted for natural language processing (NLP). We compare these methods with traditional time domain signal processing techniques that rely on Fourier analysis and frequency domain decomposition methods like Wavelet Transform. Our results demonstrate the effectiveness of kernel-based models over classical methods even at high sampling rates above Nyquist limit. We show how different choices of kernel functions impact accuracy and efficiency particularly related to computation complexity in terms of memory accesses and floating point operations. Finally we explore new directions for combining data-driven deep learning based representations along with physics informed loss functions leading towards more interpretable and robust systems suitable for control engineering applications.",1
"Interactive programming with interleaved code snippet cells and natural language markdown is recently gaining popularity in the form of Jupyter notebooks, which accelerate prototyping and collaboration. To study code generation conditioned on a long context history, we present JuICe, a corpus of 1.5 million examples with a curated test set of 3.7K instances based on online programming assignments. Compared with existing contextual code generation datasets, JuICe provides refined human-curated data, open-domain code, and an order of magnitude more training data. Using JuICe, we train models for two tasks: (1) generation of the API call sequence in a code cell, and (2) full code cell generation, both conditioned on the NL-Code history up to a particular code cell. Experiments using current baseline code generation models show that both context and distant supervision aid in generation, and that the dataset is challenging for current systems.",0
"Title: ""Distant Learning"" - An Introduction",1
"ICU readmission is associated with longer hospitalization, mortality and adverse outcomes. An early recognition of ICU re-admission can help prevent patients from worse situation and lower treatment cost. As the abundance of Electronics Health Records (EHR), it is popular to design clinical decision tools with machine learning technique manipulating on healthcare large scale data. We designed data-driven predictive models to estimate the risk of ICU readmission. The discharge summary of each hospital admission was carefully represented by natural language processing techniques. Unified Medical Language System (UMLS) was further used to standardize inconsistency of discharge summaries. 5 machine learning classifiers were adopted to construct predictive models. The best configuration yielded a competitive AUC of 0.748. Our work suggests that natural language processing of discharge summaries is capable to send clinicians warning of unplanned 30-day readmission upon discharge.",0
"This paper presents a novel approach to predicting 30-day intensive care unit (ICU) re-admissions using natural language processing and machine learning techniques. We used data from electronic health records and structured fields, as well as unstructured text in notes and discharge summaries, collected over several years at a hospital ICU. Our methods involve extracting features related to patient demographics, clinical variables, and physician notes, and training models on these features using standard classification algorithms. Results show that our method outperforms baseline approaches by achieving high accuracy in predicting early ICU readmission risk among critically ill patients. The findings have important implications for improving quality of care and reducing healthcare costs through better resource allocation and targeted interventions aimed at preventing unnecessary readmissions.",1
"This paper explores a novel approach to achieving emergent compositional communication in multi-agent systems. We propose a training regime implementing template transfer, the idea of carrying over learned biases across contexts. In our method, a sender-receiver pair is first trained with disentangled loss functions and then the receiver is transferred to train a new sender with a standard loss. Unlike other methods (e.g. the obverter algorithm), our approach does not require imposing inductive biases on the architecture of the agents. We experimentally show the emergence of compositional communication using topographical similarity, zero-shot generalization and context independence as evaluation metrics. The presented approach is connected to an important line of work in semiotics and developmental psycholinguistics: it supports a conjecture that compositional communication is scaffolded on simpler communication protocols.",0
"""Developmentally motivated emergence of compositional communication via template transfer"" - This study explores the developmental origins of human linguistic ability through examining two aspects: compositionality (the tendency to break down meaning into simpler units) and communicative systems that employ compositional principles (such as language). We investigated infants aged 6 months old who received visual cueing from adults while learning novel nonspeech vocalizations. Our results showcased how exposure to composed templates can encourage early signatures of compositionality before conventional language acquisition begins. Furthermore, our research has implications regarding infant cognitive processing and suggests new directions in understanding the evolutionary underpinnings of human languages. Overall, this work highlights the importance of considering both individual characteristics and environmental factors during language development studies.",1
"Deep learning models have achieved huge success in numerous fields, such as computer vision and natural language processing. However, unlike such fields, it is hard to apply traditional deep learning models on the graph data due to the 'node-orderless' property. Normally, adjacency matrices will cast an artificial and random node-order on the graphs, which renders the performance of deep models on graph classification tasks extremely erratic, and the representations learned by such models lack clear interpretability. To eliminate the unnecessary node-order constraint, we propose a novel model named Isomorphic Neural Network (IsoNN), which learns the graph representation by extracting its isomorphic features via the graph matching between input graph and templates. IsoNN has two main components: graph isomorphic feature extraction component and classification component. The graph isomorphic feature extraction component utilizes a set of subgraph templates as the kernel variables to learn the possible subgraph patterns existing in the input graph and then computes the isomorphic features. A set of permutation matrices is used in the component to break the node-order brought by the matrix representation. Three fully-connected layers are used as the classification component in IsoNN. Extensive experiments are conducted on benchmark datasets, the experimental results can demonstrate the effectiveness of ISONN, especially compared with both classic and state-of-the-art graph classification methods.",0
"In IsoNN, we propose an approach based on Neural Network (NN) for learning from graph data without losing the topological structure of the original graph using the Isomorphism theory. Our model can learn both from the node representation and edge representation of the graphs simultaneously and achieve state-of-the-art performance on several benchmark datasets. We use two submodels, one for node representation learning and the other for edge representation learning, which share weights of each layer. An additional loss term enforces the learned embeddings to satisfy pairwise constraints that should hold if the embedding were a faithful representation of the graph structure. This enables us to preserve the relationships among nodes while learning their representations. Finally, our model achieves superior results compared to other popular baseline methods demonstrating the effectiveness of jointly considering both node and edge properties during representation learning tasks. To summarize, IsoNN represents an exciting new development in the area of graph neural networks by combining the powerful expressive capabilities of deep learning architectures with the inherent geometry of graph data. Through rigorous experimentation across multiple benchmark datasets, we demonstrate that our proposed method significantly outperforms competing approaches, validating the utility of our design choices. Further research into the efficacy and application potential of these models remains ongoing.",1
"Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs. While some post-hoc fixes have been proposed, in particular top-$k$ and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving superior generations using standard greedy or beam search. According to human evaluations, our approach with standard beam search also outperforms the currently popular decoding methods of nucleus sampling or beam blocking, thus providing a strong alternative to existing techniques.",0
"In recent years, there has been significant interest in using artificial intelligence (AI) systems to generate human-like text. One popular method for achieving this goal is through the use of deep learning algorithms that can learn patterns from large amounts of data and then generate new sentences based on those patterns. However, these models often struggle with generating coherent and diverse text, as they tend to overfit to the training data and produce repetitive or predictable outputs. To address this issue, we propose a novel approach called unlikelihood training. This technique involves training neural networks to minimize their likelihood of producing common or likely sequences of tokens, which encourages them to explore more unusual but still grammatically correct options. We demonstrate the effectiveness of our approach by applying it to several benchmark datasets and show that it consistently produces higher quality and more diverse text compared to baseline methods. Additionally, we provide an analysis of the generated texts and discuss possible applications of this technology in real-world scenarios such as language translation or content creation. Overall, our work represents an important step forward towards creating powerful AI systems capable of generating natural sounding and creative text.",1
"One of the primary challenges of visual storytelling is developing techniques that can maintain the context of the story over long event sequences to generate human-like stories. In this paper, we propose a hierarchical deep learning architecture based on encoder-decoder networks to address this problem. To better help our network maintain this context while also generating long and diverse sentences, we incorporate natural language image descriptions along with the images themselves to generate each story sentence. We evaluate our system on the Visual Storytelling (VIST) dataset and show that our method outperforms state-of-the-art techniques on a suite of different automatic evaluation metrics. The empirical results from this evaluation demonstrate the necessities of different components of our proposed architecture and shows the effectiveness of the architecture for visual storytelling.",0
"This paper presents a hierarchical approach for visual storytelling using image description. We propose a framework that breaks down the process of generating descriptions into two levels: object level and scene level. At the object level, we use existing approaches for describing individual objects within an image, such as identifying and labeling objects and their attributes. At the scene level, we generate contextualized summaries by integrating multiple object-level descriptions together. Our method relies on data from large scale photo sharing websites and can be used across different domains. Experiments show that our approach generates high quality descriptions that effectively convey the key elements of each story. Additionally, we provide insights into how our system could potentially be improved through user studies and evaluation metrics. Overall, our framework is a significant contribution to the field of computer vision and has applications in many areas including accessibility, automation, education, entertainment, healthcare, market research, safety, retail and security.",1
"We present SentiMATE, a novel end-to-end Deep Learning model for Chess, employing Natural Language Processing that aims to learn an effective evaluation function assessing move quality. This function is pre-trained on the sentiment of commentary associated with the training moves and is used to guide and optimize the agent's game-playing decision making. The contributions of this research are three-fold: we build and put forward both a classifier which extracts commentary describing the quality of Chess moves in vast commentary datasets, and a Sentiment Analysis model trained on Chess commentary to accurately predict the quality of said moves, to then use those predictions to evaluate the optimal next move of a Chess agent. Both classifiers achieve over 90 % classification accuracy. Lastly, we present a Chess engine, SentiMATE, which evaluates Chess moves based on a pre-trained sentiment evaluation function. Our results exhibit strong evidence to support our initial hypothesis - ""Can Natural Language Processing be used to train a novel and sample efficient evaluation function in Chess Engines?"" - as we integrate our evaluation function into modern Chess engines and play against agents with traditional Chess move evaluation functions, beating both random agents and a DeepChess implementation at a level-one search depth - representing the number of moves a traditional Chess agent (employing the alpha-beta search algorithm) looks ahead in order to evaluate a given chess state.",0
"SentiMATE can learn from natural language instructions and execute them effectively, which makes it suitable as a partner for chess players who prefer learning by asking questions instead of reading books or watching videos. Players using SentiMATE can ask any question they have regarding chess positions and techniques, even for complex scenarios like endgame puzzles that involve deep tactics and strategy. Additionally, since SentiMATE can interpret emotions behind player queries and provide personalized responses, it enhances the human touch required while mastering such a creative subject like chess. Furthermore, as chess knowledge evolves over time, so does SentiMATE by absorbing new trends and strategies from online resources without additional coding, making it perpetually relevant. In summary, SentiMATE provides a superior interactive experience for beginners, intermediate, and advanced level chess enthusiasts alike compared to existing tools and methods, hence enabling more people to enjoy the beauty of chess while improving their skills rapidly.",1
"In Vision-and-Language Navigation (VLN), an embodied agent needs to reach a target destination with the only guidance of a natural language instruction. To explore the environment and progress towards the target location, the agent must perform a series of low-level actions, such as rotate, before stepping ahead. In this paper, we propose to exploit dynamic convolutional filters to encode the visual information and the lingual description in an efficient way. Differently from some previous works that abstract from the agent perspective and use high-level navigation spaces, we design a policy which decodes the information provided by dynamic convolution into a series of low-level, agent friendly actions. Results show that our model exploiting dynamic filters performs better than other architectures with traditional convolution, being the new state of the art for embodied VLN in the low-level action space. Additionally, we attempt to categorize recent work on VLN depending on their architectural choices and distinguish two main groups: we call them low-level actions and high-level actions models. To the best of our knowledge, we are the first to propose this analysis and categorization for VLN.",0
"In recent years, deep learning has revolutionized computer vision, allowing algorithms to achieve state-of-the-art results on tasks such as object detection, image classification, and semantic segmentation. However, these systems often rely heavily on large amounts of annotated data and can struggle with generalization across different environments and conditions. To address these issues, we propose a novel approach that combines embodied visual perception with natural language guidance to navigate in dynamic real-world scenarios. Our method uses convolutional neural networks (CNNs) equipped with attention modules to dynamically select relevant features from raw sensor input based on contextual cues provided by human instructions. We evaluate our system on two challenging navigation tasks: following guided tours through unseen indoor environments and executing multi-step directions within dense urban settings. Results show significant improvements over baseline methods that rely solely on vision or language alone, demonstrating the effectiveness of integrating multiple sources of information in complex navigation tasks. Overall, our work advances the field of robotics by enabling robots to operate autonomously in real-world situations while interactively communicating and adapting to changing environments under human supervision.",1
"Overparameterized transformer networks have obtained state of the art results in various natural language processing tasks, such as machine translation, language modeling, and question answering. These models contain hundreds of millions of parameters, necessitating a large amount of computation and making them prone to overfitting. In this work, we explore LayerDrop, a form of structured dropout, which has a regularization effect during training and allows for efficient pruning at inference time. In particular, we show that it is possible to select sub-networks of any depth from one large network without having to finetune them and with limited impact on performance. We demonstrate the effectiveness of our approach by improving the state of the art on machine translation, language modeling, summarization, question answering, and language understanding benchmarks. Moreover, we show that our approach leads to small BERT-like models of higher quality compared to training from scratch or using distillation.",0
"This paper introduces ""Structured Dropout"" as a means of reducing depth requirements for transformer models like GPT-2 at training time while allowing more complex computations during inference. Using a mixture model approach to drop individual gates within the attention mechanism based on the input data's complexity, we find that up to 96% of self attention gates can be safely dropped without harm to performance. By enabling these structural gate drops only when necessary rather than always doing so (as typically done), our method significantly boosts computational efficiency over existing approaches that use conventional random Gaussian noise injection or static pruning. In addition to improving GPU memory efficiency and runtime speedup with negligible loss in accuracy across several NLP benchmark datasets, our technique provides researchers greater flexibility by providing control over where they want to apply their computation resources given specific problem characteristics such as dataset size, task difficulty, etc., thus potentially advancing more effective usage of hardware resources while maintaining high quality predictions overall. Code release details are provided at https://github.com/facebookresearch/StructuredDropout.",1
"The growing ubiquity of Social Media data offers an attractive perspective for improving the quality of machine learning-based models in several fields, ranging from Computer Vision to Natural Language Processing. In this paper we focus on Facebook posts paired with reactions of multiple users, and we investigate their relationships with classes of emotions that are typically considered in the task of emotion detection. We are inspired by the idea of introducing a connection between reactions and emotions by means of First-Order Logic formulas, and we propose an end-to-end neural model that is able to jointly learn to detect emotions and predict Facebook reactions in a multi-task environment, where the logic formulas are converted into polynomial constraints. Our model is trained using a large collection of unsupervised texts together with data labeled with emotion classes and Facebook posts that include reactions. An extended experimental analysis that leverages a large collection of Facebook posts shows that the tasks of emotion classification and reaction prediction can both benefit from their interaction.",0
"This study focuses on developing a joint framework that can detect emotions from natural language texts as well as predict the corresponding Facebook reactions elicited by those texts. While previous studies have investigated individual tasks such as emotion detection and reaction prediction separately, our work uniquely bridges these two related but distinct fields. We demonstrate the effectiveness of our proposed method through extensive experiments on benchmark datasets consisting of annotated tweets paired with associated Facebook reactions data. Our findings suggest that leveraging both emotion detection and reaction prediction can lead to improved performance compared to state-of-the-art approaches. We believe our work has important implications for social media platforms seeking to better understand user engagement and sentiment analysis applications in other areas including mental health monitoring and customer service.",1
"This study compares the effectiveness and robustness of multi-class categorization of Amazon product data using transfer learning on pre-trained contextualized language models. Specifically, we fine-tuned BERT and XLNet, two bidirectional models that have achieved state-of-the-art performance on many natural language tasks and benchmarks, including text classification. While existing classification studies and benchmarks focus on binary targets, with the exception of ordinal ranking tasks, here we examine the robustness of such models as the number of classes grows from 1 to 20. Our experiments demonstrate an approximately linear decrease in performance metrics (i.e., precision, recall, $F_1$ score, and accuracy) with the number of class labels. BERT consistently outperforms XLNet using identical hyperparameters on the entire range of class label quantities for categorizing products based on their textual descriptions. BERT is also more affordable than XLNet in terms of the computational cost (i.e., time and memory) required for training. In all cases studied, the performance degradation rates were estimated to be 1% per additional class label.",0
"Title: Enhancing Multi-Class Categorization Accuracy through Fine-Tuned Pre-Trained Contextualized Language Models  In recent years, transfer learning has become increasingly popular as a technique used to leverage pre-trained models that have been trained on large datasets to solve problems in domains where labeled training data may be scarce. One such application of transfer learning involves fine-tuning these pre-trained models for multi-class categorization tasks. In this work, we investigate how the use of fine-tuned contextualized language models (CLMs) can improve the robustness of multi-class classification algorithms in text-based applications. We focus specifically on two well-known CLM architectures, BERT and GPT, which have recently gained significant attention due to their impressive performance on natural language processing tasks.  Our research demonstrates that fine-tuning these pre-trained models significantly improves the accuracy of multi-class classifiers across a variety of evaluation metrics. By using transfer learning techniques and exploiting the rich representation learned during pre-training, we were able to achieve substantial improvements over traditional approaches that rely solely on task-specific training. Our results suggest that utilizing pre-trained language models to augment the feature extraction process in text classification problems leads to more robust solutions that generalize better across different domains and languages. This finding holds true even when compared to state-of-the-art methods that employ advanced deep neural network architectures custom designed for specific problem types.  Overall, our study provides valuable insights into how practitioners can effectively leverage pre-trained language models to enhance the efficiency and accuracy of their machine learning pipelines. With continued advances in model architecture design, algorithm development, and dataset availability, there remains ample room for further improvement in this area. As such, future research should focus on exploring new ways to optimize the tradeoff between p",1
"Existing methods using generative adversarial approaches for Zero-Shot Learning (ZSL) aim to generate realistic visual features from class semantics by a single generative network, which is highly under-constrained. As a result, the previous methods cannot guarantee that the generated visual features can truthfully reflect the corresponding semantics. To address this issue, we propose a novel method named Cycle-consistent Adversarial Networks for Zero-Shot Learning (CANZSL). It encourages a visual feature generator to synthesize realistic visual features from semantics, and then inversely translate back synthesized the visual feature to corresponding semantic space by a semantic feature generator. Furthermore, in this paper a more challenging and practical ZSL problem is considered where the original semantics are from natural language with irrelevant words instead of clean semantics that are widely used in previous work. Specifically, a multi-modal consistent bidirectional generative adversarial network is trained to handle unseen instances by leveraging noise in the natural language. A forward one-to-many mapping from one text description to multiple visual features is coupled with an inverse many-to-one mapping from the visual space to the semantic space. Thus, a multi-modal cycle-consistency loss between the synthesized semantic representations and the ground truth can be learned and leveraged to enforce the generated semantic features to approximate to the real distribution in semantic space. Extensive experiments are conducted to demonstrate that our method consistently outperforms state-of-the-art approaches on natural language-based zero-shot learning tasks.",0
"This paper presents CANZSL: a method that addresses both problems simultaneously by proposing cycle-consistency constraints through adversarial training. In particular, we introduce two discriminator networks that predict whether textual inputs are generated from ground-truth descriptions or synthesized outputs of the generator network trained on source tasks. Through alternating optimization steps, our approach enforces cycle consistency between domains and ensures zero-shot generalization capability. We demonstrate state-of-the-art results on three benchmark datasets without any taskspecific fine tuning, providing evidence that cycle-consistent adversarial learning could indeed serve as a cornerstone of future ZSL approaches. By enabling more effective transfer across visual domains, our work sheds new light on how to achieve true generalization among natural language understanding agents",1
"Information in electronic health records (EHR), such as clinical narratives, examination reports, lab measurements, demographics, and other patient encounter entries, can be transformed into appropriate data representations that can be used for downstream clinical machine learning tasks using representation learning. Learning better representations is critical to improve the performance of downstream tasks. Due to the advances in machine learning, we now can learn better and meaningful representations from EHR through disentangling the underlying factors inside data and distilling large amounts of information and knowledge from heterogeneous EHR sources. In this chapter, we first introduce the background of learning representations and reasons why we need good EHR representations in machine learning for medicine and healthcare in Section 1. Next, we explain the commonly-used machine learning and evaluation methods for representation learning using a deep learning approach in Section 2. Following that, we review recent related studies of learning patient state representation from EHR for clinical machine learning tasks in Section 3. Finally, in Section 4 we discuss more techniques, studies, and challenges for learning natural language representations when free texts, such as clinical notes, examination reports, or biomedical literature are used. We also discuss challenges and opportunities in these rapidly growing research fields.",0
"This paper examines how representation learning can improve natural language processing (NLP) applications for electronic health records (EHRs). We discuss recent advances in using neural network models such as transformers and graph networks for NLP tasks like named entity recognition (NER), part-of-speech (POS) tagging, sentiment analysis, and question answering. We evaluate several different approaches on a large dataset of EHR texts, including transfer learning from pretrained models, fine-tuning, and multi-task learning. Our experiments show that these methods can significantly improve performance over traditional feature-based systems, achieving state-of-the-art results on multiple benchmark datasets. Furthermore, we demonstrate the practical utility of our approach by applying it to real-world clinical use cases, such as identifying drug side effects and extracting diagnosis codes from doctor's notes. Overall, our work highlights the potential of using representation learning techniques for improving NLP applications in the domain of EHRs, with promising implications for improved patient care and medical research.",1
"Standard autoregressive seq2seq models are easily trained by max-likelihood, but tend to show poor results under small-data conditions. We introduce a class of seq2seq models, GAMs (Global Autoregressive Models), which combine an autoregressive component with a log-linear component, allowing the use of global \textit{a priori} features to compensate for lack of data. We train these models in two steps. In the first step, we obtain an \emph{unnormalized} GAM that maximizes the likelihood of the data, but is improper for fast inference or evaluation. In the second step, we use this GAM to train (by distillation) a second autoregressive model that approximates the \emph{normalized} distribution associated with the GAM, and can be used for fast inference and evaluation. Our experiments focus on language modelling under synthetic conditions and show a strong perplexity reduction of using the second autoregressive model over the standard one.",0
"In recent years, there has been growing interest in developing machine learning models that can learn from sequences of data without requiring large amounts of training data. One approach to achieving this goal is through the use of autoregressive models, which make predictions based on past observations as well as current input features. While autoregressive models have shown promising results in several domains, they often suffer from limited performance due to their dependence on handcrafted feature engineering techniques. To address these limitations, we propose using global autoregressive models, which incorporate both local dependencies (e.g., adjacent time steps) and more complex nonlinear relationships across different parts of the sequence. We show how such models can achieve state-of-the-art performance across multiple benchmark datasets while requiring significantly less training data than other approaches. Our experimental analysis demonstrates the effectiveness of our proposed methodology and highlights its potential applications in fields such as natural language processing and video prediction tasks.",1
"Systems that can associate images with their spoken audio captions are an important step towards visually grounded language learning. We describe a scalable method to automatically generate diverse audio for image captioning datasets. This supports pretraining deep networks for encoding both audio and images, which we do via a dual encoder that learns to align latent representations from both modalities. We show that a masked margin softmax loss for such models is superior to the standard triplet loss. We fine-tune these models on the Flickr8k Audio Captions Corpus and obtain state-of-the-art results---improving recall in the top 10 from 29.6% to 49.5%. We also obtain human ratings on retrieval outputs to better assess the impact of incidentally matching image-caption pairs that were not associated in the data, finding that automatic evaluation substantially underestimates the quality of the retrieved results.",0
"In recent years, there has been increasing interest in using large-scale data sets to train deep neural networks on audio-visual representations that can encode complex relationships between spoken language and visual inputs. One major challenge facing researchers working in this area is the lack of transcriptions of speech signals in many existing datasets. This presents a significant hurdle, as these transcriptions are often used to fine-tune models, evaluate performance metrics, and perform other critical tasks. To address this issue, we present a novel approach based on visually grounding spoken input. Our method involves training a convolutional neural network (CNN) model to classify segments of video clips according to their content, such as whether they show objects, scenes, actions, or interactions. We then use this model to extract a high-level visual feature vector for each spoken utterance, which serves as a proxy for textual transcription. By applying this approach to two publicly available datasets containing millions of spoken phrases, we demonstrate that our system can effectively learn rich, semantically meaningful representations that capture key aspects of human communication. Through a comprehensive evaluation of our methods, we show that they significantly outperform baseline systems and state-of-the-art alternatives, particularly in terms of zero-shot transfer learning and cross-modal retrieval accuracy. Overall, this work represents an important step forward towards developing more effective, scalable solutions for natural language processing under challenging real-world conditions where transcripts may be scarce or nonexistent.",1
"Personal assistant AI systems such as Siri, Cortana, and Alexa have become widely used as a means to accomplish tasks through natural language commands. However, components in these systems generally rely on supervised machine learning algorithms that require large amounts of hand-annotated training data, which is expensive and time consuming to collect. The ability to incorporate unsupervised, weakly supervised, or distantly supervised data holds significant promise in overcoming this bottleneck. In this paper, we describe a framework that leverages user engagement signals (user behaviors that demonstrate a positive or negative response to content) to automatically create granular entity labels for training data augmentation. Strategies such as multi-task learning and validation using an external knowledge base are employed to incorporate the engagement annotated data and to boost the model's accuracy on a sequence labeling task. Our results show that learning from data automatically labeled by user engagement signals achieves significant accuracy gains in a production deep learning system, when measured on both the sequence labeling task as well as on user facing results produced by the system end-to-end. We believe this is the first use of user engagement signals to help generate training data for a sequence labeling task on a large scale, and can be applied in practical settings to speed up new feature deployment when little human annotated data is available.",0
"In order to create accurate and reliable virtual assistants that can effectively respond to user requests, robust entity labeling systems must be established within these artificial intelligence (AI) platforms. Due to the large amounts of data required by traditional supervised learning methods to generate high-quality entity labels, alternative approaches have been sought after in recent years. One promising approach involves utilizing engagement signals from users as they interact with the virtual assistant. These signals provide valuable insights into which entities may be more relevant to certain queries and tasks, allowing researchers to leverage them for effective entity labeling. By examining how users engage with a virtual assistant during natural language interactions, we can gain deeper understanding of their intentions and needs. This leads to improved entity recognition accuracy, enabling the development of virtual assistants better suited to fulfilling human preferences and expectations. Ultimately, employing user engagement signals in the process of entity labeling represents an important step towards creating highly capable AI systems able to support people’s diverse information needs while also contributing to advancements in natural language processing and machine learning.",1
"Referring expression comprehension aims to locate the object instance described by a natural language referring expression in an image. This task is compositional and inherently requires visual reasoning on top of the relationships among the objects in the image. Meanwhile, the visual reasoning process is guided by the linguistic structure of the referring expression. However, existing approaches treat the objects in isolation or only explore the first-order relationships between objects without being aligned with the potential complexity of the expression. Thus it is hard for them to adapt to the grounding of complex referring expressions. In this paper, we explore the problem of referring expression comprehension from the perspective of language-driven visual reasoning, and propose a dynamic graph attention network to perform multi-step reasoning by modeling both the relationships among the objects in the image and the linguistic structure of the expression. In particular, we construct a graph for the image with the nodes and edges corresponding to the objects and their relationships respectively, propose a differential analyzer to predict a language-guided visual reasoning process, and perform stepwise reasoning on top of the graph to update the compound object representation at every node. Experimental results demonstrate that the proposed method can not only significantly surpass all existing state-of-the-art algorithms across three common benchmark datasets, but also generate interpretable visual evidences for stepwisely locating the objects referred to in complex language descriptions.",0
"Incorporate at least five key features from the paper into your abstract. Use complete sentences. Follow IEEE formatting guidelines for references, citation, tables/figures, etc. Please write the abstract as if you were writing for a scientific journal (e.g., NIPS). This paper presents a novel approach to referring expression comprehension that dynamically weighs the importance of graph nodes during inference. We propose a dynamic graph attention mechanism that updates node weights based on the current state of the graph and the relationship between entities. This allows our model to effectively identify relevant information in complex graphs and achieve state-of-the-art results on benchmark datasets. Our contributions include: 1) introducing a new graph attention mechanism that adapts to changing context; 2) improving performance over strong baseline models on three challenging REC tasks; 3) providing extensive analysis and ablation studies to validate design choices; 4) making code and data available for replication and extension. Overall, our work demonstrates the effectiveness of dynamic attention mechanisms for referring expression comprehension and sets a new bar for performance in this task.",1
"With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in machine learning, natural language processing, and deep learning that addresses such challenges in different subdomains. With the commercialization of these systems, researchers are becoming aware of the biases that these applications can contain and have attempted to address them. In this survey we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined in order to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and how they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.",0
"Title: ""An Investigation into Biases and Unfairness in Machine Learning""  Abstract: This study examines biases and unfairness that arise in machine learning models, particularly those related to human decision making processes. Artificial intelligence (AI) systems have been used in many applications ranging from medical diagnosis to criminal justice, where decisions made by these systems can greatly impact human lives. However, these AI systems can inherit and amplify societal biases, resulting in discriminatory decisions or outcomes against certain groups of individuals based on their demographics such as race, gender, ethnicity, religion, etc. Despite growing concern over bias in AI algorithms, little research has investigated the causes and effects of unfair biases across different domains and scenarios, leading to an incomplete understanding of how such problems arise and persist. We conduct experiments on large public datasets that exhibit biased behaviors under different conditions, evaluate state-of-the-art debiasing techniques and fairness metrics, analyze case studies highlighting unfair AI decisions and discuss potential remedies to prevent harmful consequences of unintended bias amplification in AI systems. By improving our knowledge of fairness and bias in AI algorithms and decision-making processes, we aim to develop more transparent and accountable systems that promote social welfare, mitigate adverse impacts on vulnerable communities, and reduce trust gaps towards future technological advancements.",1
"Recently, generating adversarial examples has become an important means of measuring robustness of a deep learning model. Adversarial examples help us identify the susceptibilities of the model and further counter those vulnerabilities by applying adversarial training techniques. In natural language domain, small perturbations in the form of misspellings or paraphrases can drastically change the semantics of the text. We propose a reinforcement learning based approach towards generating adversarial examples in black-box settings. We demonstrate that our method is able to fool well-trained models for (a) IMDB sentiment classification task and (b) AG's news corpus news categorization task with significantly high success rates. We find that the adversarial examples generated are semantics-preserving perturbations to the original text.",0
"Title: Adversarial Attacks on Text Classification Models using Deep Reinforcement Learning  This paper presents a novel approach to generating adversarial examples for text classification models that leverages deep reinforcement learning. While traditional black-box attacks rely heavily on heuristics and often require significant computational resources, our proposed method offers more control over the perturbations introduced into the input text. By formulating the attack as a Markov Decision Process (MDP) and training a reinforcement learner to maximize the model's prediction error, we can generate targeted adversarial examples that are both effective and efficient to obtain. Our experimental results demonstrate the effectiveness of our approach against state-of-the-art text classifiers and showcase its potential as a powerful tool for evaluating the robustness of natural language processing systems. Overall, this work contributes to the broader understanding of the vulnerabilities of text classification models and highlights the importance of developing more resilient systems in real-world applications.",1
"Neural sequence generation is typically performed token-by-token and left-to-right. Whenever a token is generated only previously produced tokens are taken into consideration. In contrast, for problems such as sequence classification, bidirectional attention, which takes both past and future tokens into consideration, has been shown to perform much better. We propose to make the sequence generation process bidirectional by employing special placeholder tokens. Treated as a node in a fully connected graph, a placeholder token can take past and future tokens into consideration when generating the actual output token. We verify the effectiveness of our approach experimentally on two conversational tasks where the proposed bidirectional model outperforms competitive baselines by a large margin.",0
"In recent years, generative models have made significant progress in natural language processing tasks such as text generation and machine translation. However, these models often struggle with generating coherent and fluent sequences that consider future contextual dependencies. This paper presents a novel approach to addressing this challenge by incorporating attention mechanisms into bidirectional sequence generation models. Our method attends to future tokens during training and inference to capture longer term dependencies and generate more consistent and accurate outputs. We evaluate our model on several benchmark datasets and demonstrate improved performance over state-of-the-art methods. Additionally, we provide ablation studies to further analyze the contribution of different components within our framework. Overall, our work represents an important step towards developing more capable generative models that can better handle complex linguistic structures and produce higher quality output.",1
"We present a machine learning pipeline and model that uses the entire uncurated EHR for prediction of in-hospital mortality at arbitrary time intervals, using all available chart, lab and output events, without the need for pre-processing or feature engineering. Data for more than 45,000 American ICU patients from the MIMIC-III database were used to develop an ICU mortality prediction model. All chart, lab and output events were treated by the model in the same manner inspired by Natural Language Processing (NLP). Patient events were discretized by percentile and mapped to learnt embeddings before being passed to a Recurrent Neural Network (RNN) to provide early prediction of in-patient mortality risk. We compared mortality predictions with the Simplified Acute Physiology Score II (SAPS II) and the Oxford Acute Severity of Illness Score (OASIS). Data were split into an independent test set (10%) and a ten-fold cross-validation was carried out during training to avoid overfitting. 13,233 distinct variables with heterogeneous data types were included without manual selection or pre-processing. Recordings in the first few hours of a patient's stay were found to be strongly predictive of mortality, outperforming models using SAPS II and OASIS scores within just 2 hours and achieving a state of the art Area Under the Receiver Operating Characteristic (AUROC) value of 0.80 (95% CI 0.79-0.80) at 12 hours vs 0.70 and 0.66 for SAPS II and OASIS at 24 hours respectively. Our model achieves a very strong performance of AUROC 0.86 (95% CI 0.85-0.86) for in-patient mortality prediction after 48 hours on the MIMIC-III dataset. Predictive performance increases over the first 48 hours of the ICU stay, but suffers from diminishing returns, providing rationale for time-limited trials of critical care and suggesting that the timing of decision making can be optimised and individualised.",0
"In critical care medicine, accurate predictions of patient outcomes can greatly impact decisions regarding treatment and management. While there have been many attempts at developing predictive models using time series data, existing methods often require extensive feature engineering and/or manual selection of relevant variables, which may limit their generalizability across different patient populations and clinical settings. This work proposes a novel method that utilizes machine learning algorithms to generate dynamic predictions directly from raw physiological signals collected during hospital stays in the Intensive Care Unit (ICU). By bypassing traditional pre-processing steps such as signal filtering and manual feature extraction, our approach allows for real-time, bedside monitoring and evaluation of patients' condition over time, ultimately leading to more informed clinical decision making. Through comprehensive evaluations on large datasets obtained from multiple ICUs worldwide, we demonstrate the superiority of our model compared to current state-of-the-art methods in terms of accuracy and robustness. Our study has important implications for improving patient outcomes and advancing personalized healthcare practices in high-acuity medical environments.",1
Automatic question generation is an important problem in natural language processing. In this paper we propose a novel adaptive copying recurrent neural network model to tackle the problem of question generation from sentences and paragraphs. The proposed model adds a copying mechanism component onto a bidirectional LSTM architecture to generate more suitable questions adaptively from the input data. Our experimental results show the proposed model can outperform the state-of-the-art question generation methods in terms of BLEU and ROUGE evaluation scores.,0
"Artificial intelligence (AI) has made significant progress in recent years due to advancements in machine learning techniques such as deep neural networks. However, one key challenge that remains is developing algorithms capable of generating meaningful questions from large amounts of data. This paper presents a novel approach using adaptive copying neural networks (ACNN), which can learn to generate questions dynamically during training based on the input data.  The ACNN architecture consists of two main components: a generative model that learns to predict the next token in a sequence, and an encoder that extracts relevant features from the input text. During training, the network uses reinforcement learning to optimize a reward function measuring the quality of the generated questions. To encourage exploration and prevent overfitting, we use an ensemble of models with different random initializations and update them via stochastic gradient descent.  We evaluate our method on three datasets consisting of natural language queries, code snippets, and programming exercises respectively. Results show that our algorithm significantly outperforms strong baselines across all metrics including question generation accuracy, coverage, diversity, informativeness, and user satisfaction. Furthermore, qualitative analysis indicates that our generated questions are more specific, better structured, and closer to human expectations compared to state-of-the-art methods.  Overall, our work introduces a new paradigm for natural language processing by combining copying mechanisms with dynamic adaptation to achieve effective question generation. Our findings contribute to both theoretical understanding of adaptive systems and practical applications in software engineering and online education. Future directions could involve incorporating external knowledge bases, handling multi-modal inputs, and designing more advanced reward functions to improve generalization and scalability.",1
"Image captioning models are usually evaluated on their ability to describe a held-out set of images, not on their ability to generalize to unseen concepts. We study the problem of compositional generalization, which measures how well a model composes unseen combinations of concepts when describing images. State-of-the-art image captioning models show poor generalization performance on this task. We propose a multi-task model to address the poor performance, that combines caption generation and image--sentence ranking, and uses a decoding mechanism that re-ranks the captions according their similarity to the image. This model is substantially better at generalizing to unseen combinations of concepts compared to state-of-the-art captioning models.",0
"In recent years, there has been significant progress in image captioning using deep learning techniques. However, most state-of-the-art methods rely heavily on strong supervision and may struggle when faced with images that differ significantly from those seen during training. To address these limitations, we propose a novel approach based on compositional generalization, which allows models to generate accurate and diverse descriptions of unseen scenes by combining their understanding of individual concepts. Our method leverages self-attention mechanisms to selectively focus on relevant regions in the scene and hierarchically compose concept sequences into a final description. We evaluate our model against several baselines on two popular benchmarks and demonstrate that it achieves superior performance across multiple metrics while requiring only weakly supervised training data. Overall, our results suggest that enabling machines to exhibit human-like compositionality in visual understanding tasks holds great promise for improving real-world applications such as autonomous systems, robotics, and content generation.",1
"NeMo (Neural Modules) is a Python framework-agnostic toolkit for creating AI applications through re-usability, abstraction, and composition. NeMo is built around neural modules, conceptual blocks of neural networks that take typed inputs and produce typed outputs. Such modules typically represent data layers, encoders, decoders, language models, loss functions, or methods of combining activations. NeMo makes it easy to combine and re-use these building blocks while providing a level of semantic correctness checking via its neural type system. The toolkit comes with extendable collections of pre-built modules for automatic speech recognition and natural language processing. Furthermore, NeMo provides built-in support for distributed training and mixed precision on latest NVIDIA GPUs. NeMo is open-source https://github.com/NVIDIA/NeMo",0
"Title: Introduction to NeMo - Building AI Applications using Neural Modules  Abstract: In recent years, there has been significant progress made towards developing artificial intelligence (AI) technology that can perform tasks that were previously only possible for humans to accomplish. However, creating these types of systems often requires extensive expertise in machine learning, programming, and mathematics, making them difficult for non-experts to build. In order to address this issue, we present a new open source software framework called NeMo which allows users to easily build AI applications by connecting pre-trained neural modules together. This modular approach enables users to create complex AI systems without having to train each individual component from scratch. We discuss how NeMo streamlines the process of designing and deploying AI applications through a high-level interface that provides easy access to cutting-edge deep learning techniques. Additionally, our system allows developers to share their creations with others by publishing models as web services, enabling collaboration on large-scale projects. Overall, we believe that NeMo represents an important step forward for the democratization of AI development and we look forward to seeing the innovative solutions that emerge from its use.",1
"Finding an optimal assignment between two sets of objects is a fundamental problem arising in many applications, including the matching of `bag-of-words' representations in natural language processing and computer vision. Solving the assignment problem typically requires cubic time and its pairwise computation is expensive on large datasets. In this paper, we develop an algorithm which can find an optimal assignment in linear time when the cost function between objects is represented by a tree distance. We employ the method to approximate the edit distance between two graphs by matching their vertices in linear time. To this end, we propose two tree distances, the first of which reflects discrete and structural differences between vertices, and the second of which can be used to compare continuous labels. We verify the effectiveness and efficiency of our methods using synthetic and real-world datasets.",0
"Abstract: In graph matching problems, we aim to find correspondences between nodes in two graphs that preserve certain relationships. These relationships can be represented by a linear assignment problem, where each node has an assigned label from a set of potential labels, and pairs of corresponding nodes must receive compatible labels according to some compatibility matrix. Existing algorithms either solve these assignments approximately using heuristics or use integer programming methods which can take exponential time. In contrast, our proposed method computes optimal solutions in linear time under reasonable assumptions on the problem structure. Specifically, we assume that edges incident to one vertex cannot form a cycle, and there exists at least one perfect match (a complete collection of edge matches). Under these assumptions, we show how to compute optimal assignments efficiently using elementary operations on matrices. We then demonstrate empirically that these assumptions hold for many real world applications. Our results provide significant performance improvements over state-of-the-art approaches while guaranteeing optimality.",1
"While models for Visual Question Answering (VQA) have steadily improved over the years, interacting with one quickly reveals that these models lack consistency. For instance, if a model answers ""red"" to ""What color is the balloon?"", it might answer ""no"" if asked, ""Is the balloon red?"". These responses violate simple notions of entailment and raise questions about how effectively VQA models ground language. In this work, we introduce a dataset, ConVQA, and metrics that enable quantitative evaluation of consistency in VQA. For a given observable fact in an image (e.g. the balloon's color), we generate a set of logically consistent question-answer (QA) pairs (e.g. Is the balloon red?) and also collect a human-annotated set of common-sense based consistent QA pairs (e.g. Is the balloon the same color as tomato sauce?). Further, we propose a consistency-improving data augmentation module, a Consistency Teacher Module (CTM). CTM automatically generates entailed (or similar-intent) questions for a source QA pair and fine-tunes the VQA model if the VQA's answer to the entailed question is consistent with the source QA pair. We demonstrate that our CTM-based training improves the consistency of VQA models on the ConVQA datasets and is a strong baseline for further research.",0
"In this paper we tackle the problem of question consistency in Visual Question Answering (VQA). We propose a novel method that generates entailment questions from both image features and textual descriptions simultaneously, which can improve answer consistency by correcting incorrect answers given previously. Our approach uses a generative model trained on large scale entailment datasets which produces meaningful and relevant questions to ask. Experiments show improvement over current state of art methods achieving new highs in terms of accuracy and answering ability. In conclusion our work opens up possibilities for better visual reasoning abilities in machines.",1
"It is always well believed that parsing an image into constituent visual patterns would be helpful for understanding and representing an image. Nevertheless, there has not been evidence in support of the idea on describing an image with a natural-language utterance. In this paper, we introduce a new design to model a hierarchy from instance level (segmentation), region level (detection) to the whole image to delve into a thorough image understanding for captioning. Specifically, we present a HIerarchy Parsing (HIP) architecture that novelly integrates hierarchical structure into image encoder. Technically, an image decomposes into a set of regions and some of the regions are resolved into finer ones. Each region then regresses to an instance, i.e., foreground of the region. Such process naturally builds a hierarchal tree. A tree-structured Long Short-Term Memory (Tree-LSTM) network is then employed to interpret the hierarchal structure and enhance all the instance-level, region-level and image-level features. Our HIP is appealing in view that it is pluggable to any neural captioning models. Extensive experiments on COCO image captioning dataset demonstrate the superiority of HIP. More remarkably, HIP plus a top-down attention-based LSTM decoder increases CIDEr-D performance from 120.1% to 127.2% on COCO Karpathy test split. When further endowing instance-level and region-level features from HIP with semantic relation learnt through Graph Convolutional Networks (GCN), CIDEr-D is boosted up to 130.6%.",0
"This should provide a brief summary and conclusion that gives the reader a sense of your main findings and contribution to the field. If you want to say something like ""This work introduces"" please introduce yourself first (""We/I...""). Thank you!",1
"Recently, kernelized locality sensitive hashcodes have been successfully employed as representations of natural language text, especially showing high relevance to biomedical relation extraction tasks. In this paper, we propose to optimize the hashcode representations in a nearly unsupervised manner, in which we only use data points, but not their class labels, for learning. The optimized hashcode representations are then fed to a supervised classifier following the prior work. This nearly unsupervised approach allows fine-grained optimization of each hash function, which is particularly suitable for building hashcode representations generalizing from a training set to a test set. We empirically evaluate the proposed approach for biomedical relation extraction tasks, obtaining significant accuracy improvements w.r.t. state-of-the-art supervised and semi-supervised approaches.",0
"In natural language processing (NLP), relation extraction involves identifying relationships between entities mentioned in text. Recent research has focused on using neural network models and deep learning techniques to improve the accuracy and scalability of relation extraction systems. One key challenge faced by these approaches is the need for large amounts of labeled data for training.  In this work, we present a nearly-unsupervised approach to generating hashcode representations for relation extraction that requires only minimal supervision. Our method leverages unlabeled data to learn features representing relations within sentence pairs. These features are then used as input to train a hash function, which produces compact binary embeddings capturing both semantic and syntactic aspects of the underlying dependencies. We show that our hashcodes outperform traditional feature engineering methods while requiring less annotation effort. Additionally, experiments demonstrate their effectiveness for downstream tasks such as classification and named entity recognition. Overall, our results indicate that minimally supervised learning can significantly reduce the cost and complexity of NLP applications like relation extraction without compromising performance.",1
"Visualization refers to our ability to create an image in our head based on the text we read or the words we hear. It is one of the many skills that makes reading comprehension possible. Convolutional Neural Networks (CNN) are an excellent tool for recognizing and classifying text documents. In addition, it can generate images conditioned on natural language. In this work, we utilize CNNs capabilities to generate realistic images representative of the text illustrating the semantic concept. We conducted various experiments to highlight the capacity of the proposed model to generate representative images of the text descriptions used as input to the proposed model.",0
"This study examines the effectiveness of visualization techniques on reading comprehension among college students. Previous research has suggested that creating mental images while reading can improve understanding and retention of textual material. However, there remains a lack of consensus regarding the most effective visualization strategies and their impact on different types of texts and learning styles.  The present investigation utilized an experimental design to explore the relationship between visualization and reading comprehension across various domains, including science, social sciences, arts/humanities, math/computer science, and business/economics. Participants were randomly assigned to either an experimental group, which received training in visualization techniques prior to engaging in various reading tasks, or a control group, which did not receive such training but completed similar reading assignments.  Results indicated that participants who engaged in visualization exercises displayed significantly higher levels of reading comprehension compared to those in the control group. Furthermore, findings showed that certain visualization strategies may be more effective depending on the subject area and type of text. For instance, spatial visualization was found to enhance comprehension of scientific texts, while imagery relatedness was associated with improved comprehension of humanities content.  Overall, these results suggest that incorporating visualization techniques into educational practices could potentially enhance student performance and academic achievement. By encouraging learners to create vivid representations of written materials, educators may facilitate greater engagement with coursework and increase active recall during exams or assessments. Further research is recommended to explore additional applications of visualization in education and examine other potential benefits for diverse populations.",1
"Significant advances have been made in Natural Language Processing (NLP) modelling since the beginning of 2018. The new approaches allow for accurate results, even when there is little labelled data, because these NLP models can benefit from training on both task-agnostic and task-specific unlabelled data. However, these advantages come with significant size and computational costs. This workshop paper outlines how our proposed convolutional student architecture, having been trained by a distillation process from a large-scale model, can achieve 300x inference speedup and 39x reduction in parameter count. In some cases, the student model performance surpasses its teacher on the studied tasks.",0
"Deep learning models have revolutionized natural language processing by offering state-of-the art performance on challenging tasks such as question answering, machine translation, sentiment analysis, and more. In recent years, transformer architectures like BERT and GPT have become particularly popular due to their strong results and ability to model interdependencies across words without relying on sequentiality. However, one major challenge in using deep learning models for NLP tasks is that they typically require large amounts of labeled data to achieve high accuracy. This can make them difficult to apply in practice, especially for smaller datasets where label scarcity is a concern. To address this issue, we propose a new method called ""label-scarce distillation"" which leverages unlabeled data in addition to limited labeled data to train accurate and efficient text classifiers based on pre-trained transformers. Our approach outperforms previous methods in terms of accuracy while requiring significantly fewer labels during training, making it well-suited for real-world applications. We evaluate our approach on several benchmark datasets and demonstrate its effectiveness in achieving competitive results even when only a few labeled examples are available. Overall, our work shows promise in enabling broader use of deep learning models for natural language processing in practical settings.",1
"Research in natural language processing proceeds, in part, by demonstrating that new models achieve superior performance (e.g., accuracy) on held-out test data, compared to previous results. In this paper, we demonstrate that test-set performance scores alone are insufficient for drawing accurate conclusions about which model performs best. We argue for reporting additional details, especially performance on validation data obtained during model development. We present a novel technique for doing so: expected validation performance of the best-found model as a function of computation budget (i.e., the number of hyperparameter search trials or the overall training time). Using our approach, we find multiple recent model comparisons where authors would have reached a different conclusion if they had used more (or less) computation. Our approach also allows us to estimate the amount of computation required to obtain a given accuracy; applying it to several recently published results yields massive variation across papers, from hours to weeks. We conclude with a set of best practices for reporting experimental results which allow for robust future comparisons, and provide code to allow researchers to use our technique.",0
"In recent years, there has been growing concern within the scientific community regarding the lack of transparency and reproducibility in published research results. One major contributing factor to this issue is the limited reporting of experimental methods and data analysis procedures in scientific publications. This can lead to ambiguity in how experiments were conducted and analyzed, making it difficult for readers to fully evaluate the validity and reliability of the reported findings. Therefore, there is a need for improved standards in scientific reporting that promote openness and clarity in communicating experimental results. Here, we propose that requiring authors to ""show their work"" by providing detailed descriptions of their experimental methods and data analyses would address these concerns and improve the quality of science. We discuss existing guidelines and initiatives aimed at promoting transparent reporting practices, highlight the benefits of improved experimental reporting, and provide recommendations on how journal editors and reviewers could encourage more comprehensive reporting without overburdening authors. By implementing these measures, we hope to foster greater collaboration, reduce wasteful duplication of effort, and ultimately advance scientific progress through better communication of research outcomes.",1
"There have been a few recent methods proposed in text to video moment retrieval using natural language queries, but requiring full supervision during training. However, acquiring a large number of training videos with temporal boundary annotations for each text description is extremely time-consuming and often not scalable. In order to cope with this issue, in this work, we introduce the problem of learning from weak labels for the task of text to video moment retrieval. The weak nature of the supervision is because, during training, we only have access to the video-text pairs rather than the temporal extent of the video to which different text descriptions relate. We propose a joint visual-semantic embedding based framework that learns the notion of relevant segments from video using only video-level sentence descriptions. Specifically, our main idea is to utilize latent alignment between video frames and sentence descriptions using Text-Guided Attention (TGA). TGA is then used during the test phase to retrieve relevant moments. Experiments on two benchmark datasets demonstrate that our method achieves comparable performance to state-of-the-art fully supervised approaches.",0
"One method that has been developed to retrieve moments from videos based on text queries involves using weak supervision. This approach leverages large amounts of unlabeled video data along with limited human annotations to train models capable of accurately identifying specific video segments relevant to user inputs. In this paper, we present a comprehensive review of current state-of-the-art methods for achieving high performance in video moment retrieval tasks through weak supervision techniques. Our work provides both technical details and empirical evaluations of these systems, as well as insights into potential future directions for research in this area. Overall, our study demonstrates that significant progress can be made towards effective video moment retrieval even with minimal human input, making this approach highly valuable for real-world applications such as media browsing, education, entertainment, security, and more.",1
"While Reinforcement Learning (RL) approaches lead to significant achievements in a variety of areas in recent history, natural language tasks remained mostly unaffected, due to the compositional and combinatorial nature that makes them notoriously hard to optimize. With the emerging field of Text-Based Games (TBGs), researchers try to bridge this gap. Inspired by the success of RL algorithms on Atari games, the idea is to develop new methods in a restricted game world and then gradually move to more complex environments. Previous work in the area of TBGs has mainly focused on solving individual games. We, however, consider the task of designing an agent that not just succeeds in a single game, but performs well across a whole family of games, sharing the same theme. In this work, we present our deep RL agent--LeDeepChef--that shows generalization capabilities to never-before-seen games of the same family with different environments and task descriptions. The agent participated in Microsoft Research's ""First TextWorld Problems: A Language and Reinforcement Learning Challenge"" and outperformed all but one competitor on the final test set. The games from the challenge all share the same theme, namely cooking in a modern house environment, but differ significantly in the arrangement of the rooms, the presented objects, and the specific goal (recipe to cook). To build an agent that achieves high scores across a whole family of games, we use an actor-critic framework and prune the action-space by using ideas from hierarchical reinforcement learning and a specialized module trained on a recipe database.",0
"LeDeepChef is a deep reinforcement learning agent designed specifically for families of text-based games. In these types of games, players interact with characters within a story by making choices that affect how the game progresses. While traditional agents have relied on manually crafted features or human expertise to solve such problems, our approach trains an end-to-end model that can automatically learn representations and make decisions directly from raw game states. We evaluate our method on several games including Zork, Amnesia, and Porcumon, demonstrating state-of-the-art performance across diverse environments and settings. Our work shows that using large neural networks and advanced training techniques like deep Q-networks can significantly improve agent capabilities without requiring any external knowledge of game rules or specific domain assumptions. This represents a major step towards general solutions for complex decision-making problems in textual domains where no explicit models exist.",1
"A recent trend observed in traditionally challenging fields such as computer vision and natural language processing has been the significant performance gains shown by deep learning (DL). In many different research fields, DL models have been evolving rapidly and become ubiquitous. Despite researchers' excitement, unfortunately, most software developers are not DL experts and oftentimes have a difficult time following the booming DL research outputs. As a result, it usually takes a significant amount of time for the latest superior DL models to prevail in industry. This issue is further exacerbated by the common use of sundry incompatible DL programming frameworks, such as Tensorflow, PyTorch, Theano, etc. To address this issue, we propose a system, called Model Asset Exchange (MAX), that avails developers of easy access to state-of-the-art DL models. Regardless of the underlying DL programming frameworks, it provides an open source Python library (called the MAX framework) that wraps DL models and unifies programming interfaces with our standardized RESTful APIs. These RESTful APIs enable developers to exploit the wrapped DL models for inference tasks without the need to fully understand different DL programming frameworks. Using MAX, we have wrapped and open-sourced more than 30 state-of-the-art DL models from various research fields, including computer vision, natural language processing and signal processing, etc. In the end, we selectively demonstrate two web applications that are built on top of MAX, as well as the process of adding a DL model to MAX.",0
"Abstract: With the advent of increasingly powerful computer hardware such as graphics processing units (GPUs), deep learning has emerged as one of the most promising areas within artificial intelligence research. To take advantage of these advances, practitioners require access to high quality models, data sets, code snippets, and associated documentation. Model Asset eXchange is an open source platform that enables easy sharing and dissemination of machine learning assets. This infrastructure provides users with direct access to pretrained deep neural network architectures, and allows developers to rapidly create custom solutions without having to retrain existing networks from scratch. In addition, developers can contribute their own creations to enrich the community repository, thereby facilitating continuous improvement through crowd sourcing. Our work shows how Model Asset eXchange simplifies model discovery, selection, and deployment - providing significant time savings for both novice and experienced users alike. Our goal is to encourage more widespread adoption by making deep learning easier, faster, cheaper, and more accessible than ever before.",1
"The recent success of natural language understanding (NLU) systems has been troubled by results highlighting the failure of these models to generalize in a systematic and robust way. In this work, we introduce a diagnostic benchmark suite, named CLUTRR, to clarify some key issues related to the robustness and systematicity of NLU systems. Motivated by classic work on inductive logic programming, CLUTRR requires that an NLU system infer kinship relations between characters in short stories. Successful performance on this task requires both extracting relationships between entities, as well as inferring the logical rules governing these relationships. CLUTRR allows us to precisely measure a model's ability for systematic generalization by evaluating on held-out combinations of logical rules, and it allows us to evaluate a model's robustness by adding curated noise facts. Our empirical results highlight a substantial performance gap between state-of-the-art NLU models (e.g., BERT and MAC) and a graph neural network model that works directly with symbolic inputs---with the graph-based model exhibiting both stronger generalization and greater robustness.",0
"This research presents a new benchmark dataset for evaluating inductive reasoning models on natural language text. We focus on capturing the complexity of real-world reading comprehension tasks by creating challenging questions requiring multiple inferences across paragraphs or documents. Our evaluation suite, called ""CLUTRR,"" contains over one million machine-generated questions and answers derived from diverse topics such as science, history, geography, current events, and common knowledge. We conduct experiments comparing leading question answering systems, showing that our benchmark poses significant challenges beyond those seen in existing datasets, including TriviaQA, NaturalQuestions, BingQA, CNN-NewsQuiz, and others. In particular, we find that current state-of-the-art models struggle to handle multi-hop inference scenarios present in CLUTRR, suggesting opportunities for future model improvement. Overall, our work provides an important resource for the NLP community and helps guide progress towards more advanced reading understanding capacities in machines.",1
"Deep Learning has attracted considerable attention across multiple application domains, including computer vision, signal processing and natural language processing. Although quite a few single node deep learning frameworks exist, such as tensorflow, pytorch and keras, we still lack a complete processing structure that can accommodate large scale data processing, version control, and deployment, all while staying agnostic of any specific single node framework. To bridge this gap, this paper proposes a new, higher level framework, i.e. Nemesyst, which uses databases along with model sequentialisation to allow processes to be fed unique and transformed data at the point of need. This facilitates near real-time application and makes models available for further training or use at any node that has access to the database simultaneously. Nemesyst is well suited as an application framework for internet of things aggregated control systems, deploying deep learning techniques to optimise individual machines in massive networks. To demonstrate this framework, we adopted a case study in a novel domain; deploying deep learning to optimise the high speed control of electrical power consumed by a massive internet of things network of retail refrigeration systems in proportion to load available on the UK National Grid (a demand side response). The case study demonstrated for the first time in such a setting how deep learning models, such as Recurrent Neural Networks (vanilla and Long-Short-Term Memory) and Generative Adversarial Networks paired with Nemesyst, achieve compelling performance, whilst still being malleable to future adjustments as both the data and requirements inevitably change over time.",0
"In recent years there has been increasing demand on providing efficient, secure and reliable data storage solutions which can support IoT devices applications in different industries such as Agriculture , Manufacturing, Healthcare etc . To overcome the bottlenecks that traditional relational databases have encountered due to their limited scalability capabilities and high latency rates caused by the large amount of generated data from IoT systems Nemsyt was developed . Nemesyst employs deep learning algorithms to handle complex relationships among datasets and enables efficient processing time by minimizing communication overhead through parallel computing frameworks . In addition, our proposed approach utilizes hierarchical architectures that integrate machine learning models into refrigeration control processes at different levels, enabling adaptive decision making based on real-time sensor readings and environmental factors . This work aimed to provide a comprehensive study evaluating the use of Nemesyst framework applied on an IoT enabled food retailing system using a case scenario where data collected from temperature sensors are analyzed over Cloud servers equipped with powerful GPUs to run real-time analytics tasks . Experimental results demonstrated that Nemesyst achieved significant improvement across all performance metrics including accuracy and overall model quality compared to state-of-the-art techniques . Our findings suggested that deploying Nemesyst could greatly enhance the scalability and reliability of future Industry 4.0 applications while ensuring low end-to-end latencies for real-time monitoring purposes . Moreover, our hybrid-parallelism approach would allow organizations to reduce their infrastructure footprint by leveraging existing hardware investments thereby leading to considerable cost savin",1
"Deep learning systems thrive on abundance of labeled training data but such data is not always available, calling for alternative methods of supervision. One such method is expectation regularization (XR) (Mann and McCallum, 2007), where models are trained based on expected label proportions. We propose a novel application of the XR framework for transfer learning between related tasks, where knowing the labels of task A provides an estimation of the label proportion of task B. We then use a model trained for A to label a large corpus, and use this corpus with an XR loss to train a model for task B. To make the XR framework applicable to large-scale deep-learning setups, we propose a stochastic batched approximation procedure. We demonstrate the approach on the task of Aspect-based Sentiment classification, where we effectively use a sentence-level sentiment predictor to train accurate aspect-based predictor. The method improves upon fully supervised neural system trained on aspect-level data, and is also cumulative with LM-based pretraining, as we demonstrate by improving a BERT-based Aspect-based Sentiment model.",0
"Transfer learning has emerged as a powerful technique for training neural networks on new tasks using pre-trained models. In this work, we present a novel method for transferring knowledge from related tasks by leveraging the expected label proportions of the target task. Our approach utilizes these proportions to adjust the importance given to each task during multi-task training, effectively guiding the model towards the most relevant information. We demonstrate the effectiveness of our method through extensive experiments on three benchmark datasets across two domains: image classification and sentiment analysis. Results show that our approach outperforms state-of-the-art methods in both settings, achieving significant improvements over strong baselines while requiring less computational resources. Overall, this study represents an important step forward in developing effective transfer learning techniques for related tasks, opening up exciting opportunities for future research in this area.",1
"We propose weakly supervised language localization networks (WSLLN) to detect events in long, untrimmed videos given language queries. To learn the correspondence between visual segments and texts, most previous methods require temporal coordinates (start and end times) of events for training, which leads to high costs of annotation. WSLLN relieves the annotation burden by training with only video-sentence pairs without accessing to temporal locations of events. With a simple end-to-end structure, WSLLN measures segment-text consistency and conducts segment selection (conditioned on the text) simultaneously. Results from both are merged and optimized as a video-sentence matching problem. Experiments on ActivityNet Captions and DiDeMo demonstrate that WSLLN achieves state-of-the-art performance.",0
"This research presents Weakly Supervised Natural Language Localization (WSLLN) networks as a novel approach to localizing natural language inputs without relying on large amounts of annotated data. The method leverages weak supervision through distant supervision and pre-training techniques to learn representations that capture linguistically relevant features for text location prediction. Experimental results demonstrate the effectiveness of WSLLN compared to strong baselines, including models trained on fully supervised datasets and those fine-tuned with additional unlabeled corpora. Overall, our work shows promise for improving NLP applications by enabling effective model training under limited labeling resources.",1
"The scientific literature is a rich source of information for data mining with conceptual knowledge graphs; the open science movement has enriched this literature with complementary source code that implements scientific models. To exploit this new resource, we construct a knowledge graph using unsupervised learning methods to identify conceptual entities. We associate source code entities to these natural language concepts using word embedding and clustering techniques. Practical naming conventions for methods and functions tend to reflect the concept(s) they implement. We take advantage of this specificity by presenting a novel process for joint clustering text concepts that combines word-embeddings, nonlinear dimensionality reduction, and clustering techniques to assist in understanding, organizing, and comparing software in the open science ecosystem. With our pipeline, we aim to assist scientists in building on existing models in their discipline when making novel models for new phenomena. By combining source code and conceptual information, our knowledge graph enhances corpus-wide understanding of scientific literature.",0
"In this paper we present a framework for un supervised construction of knowledge graphs from natural language text and source code. We demonstrate that our method can extract valuable structured information from both kinds of data without relying on labeled training data for either domain. Our approach consists of two main components: a graph generator which transforms input into a form suitable for learning; and a model that predicts edges in the resulting graph given embeddings of nodes as well as their neighbours, and that generates node features using a denoising autoencoder architecture conditioned on these predicted edges. Experiments on several datasets show improved results over alternative methods that don't use code, and strong performance even compared to those that do.",1
"Frameworks for writing, compiling, and optimizing deep learning (DL) models have recently enabled progress in areas like computer vision and natural language processing. Extending these frameworks to accommodate the rapidly diversifying landscape of DL models and hardware platforms presents challenging tradeoffs between expressivity, composability, and portability. We present Relay, a new compiler framework for DL. Relay's functional, statically typed intermediate representation (IR) unifies and generalizes existing DL IRs to express state-of-the-art models. The introduction of Relay's expressive IR requires careful design of domain-specific optimizations, addressed via Relay's extension mechanisms. Using these extension mechanisms, Relay supports a unified compiler that can target a variety of hardware platforms. Our evaluation demonstrates Relay's competitive performance for a broad class of models and devices (CPUs, GPUs, and emerging accelerators). Relay's design demonstrates how a unified IR can provide expressivity, composability, and portability without compromising performance.",0
"Sure! Here is a possible draft abstract for your paper titled ""Relay: A High-level Compiler for Deep Learning"": Deep learning has become an increasingly popular approach to machine learning due to its ability to achieve state-of-the-art results on many tasks, but writing deep learning models can still require a great deal of specialized knowledge and expertise. One solution to this problem is to use high-level programming languages that provide libraries and frameworks specifically designed for deep learning, such as TensorFlow or PyTorch. However, these systems have some drawbacks, including limited control over low-level performance optimizations, difficulties in debugging and profiling deep learning programs, and limitations in flexibility. In order to address these issues, we present Relay, a new compiler for deep learning that allows users to write their models using familiar Python functions and data structures while providing automatic differentiation, GPU acceleration, and support for modern deep learning techniques like attention mechanisms and transformers. Our evaluation shows that Relay provides significant speedup compared to manual implementation of deep learning models while maintaining model quality. We believe Relay represents an important step forward in making deep learning more accessible and flexible for researchers and practitioners alike.",1
"A phrase grounding system localizes a particular object in an image referred to by a natural language query. In previous work, the phrases were restricted to have nouns that were encountered in training, we extend the task to Zero-Shot Grounding(ZSG) which can include novel, ""unseen"" nouns. Current phrase grounding systems use an explicit object detection network in a 2-stage framework where one stage generates sparse proposals and the other stage evaluates them. In the ZSG setting, generating appropriate proposals itself becomes an obstacle as the proposal generator is trained on the entities common in the detection and grounding datasets. We propose a new single-stage model called ZSGNet which combines the detector network and the grounding system and predicts classification scores and regression parameters. Evaluation of ZSG system brings additional subtleties due to the influence of the relationship between the query and learned categories; we define four distinct conditions that incorporate different levels of difficulty. We also introduce new datasets, sub-sampled from Flickr30k Entities and Visual Genome, that enable evaluations for the four conditions. Our experiments show that ZSGNet achieves state-of-the-art performance on Flickr30k and ReferIt under the usual ""seen"" settings and performs significantly better than baseline in the zero-shot setting.",0
"Title: Zero-shot grounding of objects from natural language queries: A deep learning approach  Abstract: In recent years, there has been increasing interest in developing methods that can automatically ground natural language queries to their corresponding visual representations. This problem, known as zero-shot object retrieval (ZSOR), involves finding the most relevant image(s) among a large collection given only a textual description of the target object. While many approaches have achieved promising results, they often rely on extensive training data and laborious feature engineering. In this work, we propose a novel solution based on deep learning techniques that requires no explicit supervision beyond the raw input data itself. Our method learns a joint embedding space for images and texts by maximizing mutual information between them, which effectively captures the semantic relationship between vision and language. We then use this shared representation to perform ZSOR without any fine-tuning or additional resources. Experiments on standard benchmark datasets show that our model significantly outperforms state-of-the-art algorithms under both zero-shot and few-shot settings, demonstrating the effectiveness and flexibility of our approach. This work opens up new possibilities for building versatile systems capable of handling diverse tasks related to image-text matching and grounding.",1
"How to aggregate multi-view representations of a 3D object into an informative and discriminative one remains a key challenge for multi-view 3D object retrieval. Existing methods either use view-wise pooling strategies which neglect the spatial information across different views or employ recurrent neural networks which may face the efficiency problem. To address these issues, we propose an effective and efficient framework called View N-gram Network (VNN). Inspired by n-gram models in natural language processing, VNN divides the view sequence into a set of visual n-grams, which involve overlapping consecutive view sub-sequences. By doing so, spatial information across multiple views is captured, which helps to learn a discriminative global embedding for each 3D object. Experiments on 3D shape retrieval benchmarks, including ModelNet10, ModelNet40 and ShapeNetCore55 datasets, demonstrate the superiority of our proposed method.",0
"Title: View N-gram Networks for 3D Object Retrieieval Abstract In recent years, there has been growing interest in developing techniques that can effectively retrieve three-dimensional (3D) objects from large databases based on partial, noisy, or incomplete input queries. Traditional approaches rely heavily on feature extraction and matching algorithms which often struggle with such noisy inputs and may fail to return accurate results. Our approach addresses these challenges by leveraging view n-gram networks - graphs constructed over sets of images rendered from different views of each object in the database. These graphs capture rich visual features across multiple scales and orientations, enabling effective retrieval even when only partial or noisy query data is available. We propose several novel graph construction methods tailored specifically for view n-grams, as well as efficient indexing schemes to support fast nearest neighbor search. Extensive experiments conducted on two benchmark datasets demonstrate significant improvement compared to previous state-of-the-art methods under varying levels of noise, demonstrating the effectiveness of our approach in tackling real-world 3D object retrieval problems. This research holds promise for applications ranging from computer vision and graphics to robotics and virtual reality. Keywords: 3D object retrieval; view n-gram networks; graph construction; index",1
"The collaborative ranking problem has been an important open research question as most recommendation problems can be naturally formulated as ranking problems. While much of collaborative ranking methodology assumes static ranking data, the importance of temporal information to improving ranking performance is increasingly apparent. Recent advances in deep learning, especially the discovery of various attention mechanisms and newer architectures in addition to widely used RNN and CNN in natural language processing, have allowed us to make better use of the temporal ordering of items that each user has engaged with. In particular, the SASRec model, inspired by the popular Transformer model in natural languages processing, has achieved state-of-art results in the temporal collaborative ranking problem and enjoyed more than 10x speed-up when compared to earlier CNN/RNN-based methods. However, SASRec is inherently an un-personalized model and does not include personalized user embeddings. To overcome this limitation, we propose a Personalized Transformer (SSE-PT) model, outperforming SASRec by almost 5% in terms of NDCG@10 on 5 real-world datasets. Furthermore, after examining some random users' engagement history and corresponding attention heat maps used during the inference stage, we find our model is not only more interpretable but also able to focus on recent engagement patterns for each user. Moreover, our SSE-PT model with a slight modification, which we call SSE-PT++, can handle extremely long sequences and outperform SASRec in ranking results with comparable training speed, striking a balance between performance and speed requirements. Code and data are open sourced at https://github.com/wuliwei9278/SSE-PT.",0
"Temporal Collaborative Ranking via personalized transformer is a new machine learning model that uses temporal features such as time stamps and sequential relationships to rank items in a collaborative ranking task. This approach builds on recent advances in natural language processing using transformers, which have achieved state-of-the-art performance in many text understanding tasks. In our work, we focus specifically on applications where temporal context can provide additional insights for collaboration rankings, such as recommender systems, team formation, or project planning. We demonstrate through experiments that our method improves upon baseline models by incorporating temporal signals into the collaborative ranking process. Our results show that temporal collaborative ranking leads to better recommendations and more accurate predictions of collaborator success compared to traditional approaches without considering temporal dynamics. Overall, this research contributes to the growing literature on combining deep learning techniques with domain-specific knowledge in important real-world problems involving collaboration.  Abstract: This study proposes a novel machine learning model called Temporal Collaborative Ranking via Personalized Transformer (TCRPT) for improved performance in collaborative ranking tasks that involve temporal elements such as time stamped data or sequential dependencies. TCRPT leverages recent advancements in natural language processing using transformer architectures and incorporates temporal cues to enhance the accuracy of recommendations, partner selection, scheduling, and other applications requiring effective collaboration assessments. Through experimental evaluation, our results confirm that integrating temporal information significantly outperforms existing methods that ignore these dimensions, demonstrating the effectiveness of our approach across various domains requiring collaboration analysis. These findings offer valuable insights for future research exploring complementary strategies blending machine learning with domain expertise to address crucial real-world challenges demanding successful cooperation.",1
"Vision-and-Language Navigation (VLN) tasks such as Room-to-Room (R2R) require machine agents to interpret natural language instructions and learn to act in visually realistic environments to achieve navigation goals. The overall task requires competence in several perception problems: successful agents combine spatio-temporal, vision and language understanding to produce appropriate action sequences. Our approach adapts pre-trained vision and language representations to relevant in-domain tasks making them more effective for VLN. Specifically, the representations are adapted to solve both a cross-modal sequence alignment and sequence coherence task. In the sequence alignment task, the model determines whether an instruction corresponds to a sequence of visual frames. In the sequence coherence task, the model determines whether the perceptual sequences are predictive sequentially in the instruction-conditioned latent space. By transferring the domain-adapted representations, we improve competitive agents in R2R as measured by the success rate weighted by path length (SPL) metric.",0
"In recent years, there has been significant progress towards developing artificial intelligence (AI) agents that can interact with the world through natural language instructions. One important aspect of these systems is their ability to learn representations that transfer across different tasks and environments. However, current approaches still have limited performance on some benchmarks and are often outperformed by human baselines.  In this work, we propose a new method for learning transferable representation",1
"Data collection is a major bottleneck in machine learning and an active research topic in multiple communities. There are largely two reasons data collection has recently become a critical issue. First, as machine learning is becoming more widely-used, we are seeing new applications that do not necessarily have enough labeled data. Second, unlike traditional machine learning, deep learning techniques automatically generate features, which saves feature engineering costs, but in return may require larger amounts of labeled data. Interestingly, recent research in data collection comes not only from the machine learning, natural language, and computer vision communities, but also from the data management community due to the importance of handling large amounts of data. In this survey, we perform a comprehensive study of data collection from a data management point of view. Data collection largely consists of data acquisition, data labeling, and improvement of existing data or models. We provide a research landscape of these operations, provide guidelines on which technique to use when, and identify interesting research challenges. The integration of machine learning and data management for data collection is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research.",0
"Title: ""Big Data Analytics Meets Artificial Intelligence: An Analysis of Data Collection Methods""  Article Summary: This research study aimed to explore how data collection methods can impact machine learning models built using big data analytics techniques. We analyzed existing literature and surveyed experts from academia and industry to identify trends, challenges, and opportunities in integrating these two fields. Our findings indicate that there is significant potential for improving performance by leveraging AI algorithms and advanced analysis tools within big data frameworks. However, we identified several key areas where further development is necessary, including adaptive modeling strategies, scalability issues, and privacy concerns surrounding personal data usage. Ultimately, our results highlight the need for continued collaboration between big data specialists, AI practitioners, and domain experts to ensure efficient and effective use of large-scale datasets. Overall, our work contributes new insights into the interplay between artificial intelligence and big data technologies, providing guidance for future research directions.",1
"Classification-as-a-Service (CaaS) is widely deployed today in machine intelligence stacks for a vastly diverse set of applications including anything from medical prognosis to computer vision tasks to natural language processing to identity fraud detection. The computing power required for training complex models on large datasets to perform inference to solve these problems can be very resource-intensive. A CaaS provider may cheat a customer by fraudulently bypassing expensive training procedures in favor of weaker, less computationally-intensive algorithms which yield results of reduced quality. Given a classification service supplier $S$, intermediary CaaS provider $P$ claiming to use $S$ as a classification backend, and customer $C$, our work addresses the following questions: (i) how can $P$'s claim to be using $S$ be verified by $C$? (ii) how might $S$ make performance guarantees that may be verified by $C$? and (iii) how might one design a decentralized system that incentivizes service proofing and accountability? To this end, we propose a variety of methods for $C$ to evaluate the service claims made by $P$ using probabilistic performance metrics, instance seeding, and steganography. We also propose a method of measuring the robustness of a model using a blackbox adversarial procedure, which may then be used as a benchmark or comparison to a claim made by $S$. Finally, we propose the design of a smart contract-based decentralized system that incentivizes service accountability to serve as a trusted Quality of Service (QoS) auditor.",0
"Machine learning has revolutionized many fields by providing algorithms that can automatically learn from data without explicit programming. However, machine learning models may give incorrect predictions if they have been trained on biased or noisy data, or if they have overfit the training set. While these problems have received significant attention, there remains a large gap in our understanding of how to verify the correctness of machine learning inferences in real-world applications. This work presents techniques for performing provably uncheatable inference using machine learning models under strong adversarial assumptions. Our contributions include new methods for bounding the error of machine learning predictions, as well as novel algorithms for generating certified explanations of model behavior. We evaluate our approach through extensive experiments on several benchmark datasets, demonstrating substantial improvements in accuracy and runtime compared to state-of-the-art baselines. Our results show that provably uncheatable machine learning inference is feasible in practice and can significantly improve trustworthiness and reliability in critical applications such as healthcare, finance, and robotics.",1
"We present ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language. We extend the popular BERT architecture to a multi-modal two-stream model, pro-cessing both visual and textual inputs in separate streams that interact through co-attentional transformer layers. We pretrain our model through two proxy tasks on the large, automatically collected Conceptual Captions dataset and then transfer it to multiple established vision-and-language tasks -- visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval -- by making only minor additions to the base architecture. We observe significant improvements across tasks compared to existing task-specific models -- achieving state-of-the-art on all four tasks. Our work represents a shift away from learning groundings between vision and language only as part of task training and towards treating visual grounding as a pretrainable and transferable capability.",0
"In recent years, advances in natural language processing have been driven by large amounts of data and powerful neural network architectures. However, most of these models require task-specific training, which can be time-consuming and computationally expensive. To address this limitation, we introduce ViLBERT, a pretrained model that learns visiolinguistic representations that transfer well across multiple vision-and-language tasks without task-specific fine tuning. Our approach differs from previous methods as it leverages unsupervised contrastive learning and jointly optimizes multi-modal representations along with their textual counterparts using masked token detection on image features. We evaluate our method on several benchmark datasets and demonstrate significantly improved performance compared to prior state-of-the-art approaches while requiring fewer parameters and less computational resources during inference. Overall, our work shows promising results towards building more efficient, generalizable, and versatile models for real-world applications in vision-and-language processing.",1
"Fusing multi-modality information is known to be able to effectively bring significant improvement in video classification. However, the most popular method up to now is still simply fusing each stream's prediction scores at the last stage. A valid question is whether there exists a more effective method to fuse information cross modality. With the development of attention mechanism in natural language processing, there emerge many successful applications of attention in the field of computer vision. In this paper, we propose a cross-modality attention operation, which can obtain information from other modality in a more effective way than two-stream. Correspondingly we implement a compatible block named CMA block, which is a wrapper of our proposed attention operation. CMA can be plugged into many existing architectures. In the experiments, we comprehensively compare our method with two-stream and non-local models widely used in video classification. All experiments clearly demonstrate strong performance superiority by our proposed method. We also analyze the advantages of the CMA block by visualizing the attention map, which intuitively shows how the block helps the final prediction.",0
"This paper presents two models that improve upon current state-of-the art methods by using two streams: one from RGB frames, and another from optical flow estimated with a learned network. The RGB stream uses InceptionResNetV2 as the backbone, while the second stream consists of additional layers that process the output of theflow estimator. Our first model adds a late fusion branch based on element-wise addition between RGB features and fused feature maps obtained from stacked hourglass modules. We name this architecture Fusion + Flow (F+F). Our second proposed method introduces cross modality attention mechanisms that explicitly attend both streams towards relevant regions before feeding them into individual heads for classification.",1
"Many common sequential data sources, such as source code and natural language, have a natural tree-structured representation. These trees can be generated by fitting a sequence to a grammar, yielding a hierarchical ordering of the tokens in the sequence. This structure encodes a high degree of syntactic information, making it ideal for problems such as grammar correction. However, little work has been done to develop neural networks that can operate on and exploit tree-structured data. In this paper we present the Tree-Transformer \textemdash{} a novel neural network architecture designed to translate between arbitrary input and output trees. We applied this architecture to correction tasks in both the source code and natural language domains. On source code, our model achieved an improvement of $25\%$ $\text{F}0.5$ over the best sequential method. On natural language, we achieved comparable results to the most complex state of the art systems, obtaining a $10\%$ improvement in recall on the CoNLL 2014 benchmark and the highest to date $\text{F}0.5$ score on the AESW benchmark of $50.43$.",0
"This paper presents Treetransformer, which leverages transformers (e.g., BERT) as the backbone model architecture to provide corrective suggestions for tree structured data such as XML documents and HTML pages. The method first generates candidate corrections via retrieval from large datasets, and then uses a pretrained transformer based on a fine-grained characterization to rank these candidates according to their likelihood of success. We introduce several new ideas that make our approach highly flexible including learning embeddings using self attention mechanisms only from masked tokens; training on both positive and negative examples extracted from user inputs rather than relying solely on gold standard annotations; and incorporating contextual cues by encoding local neighborhoods within trees, and even entire trees if desired, rather than simple token sequences. Our experiments demonstrate state-of-the-art accuracy across multiple domains on multiple evaluation measures compared against prior work on benchmark data sets. With promising early results on real-world deployments, we hope that this method can assist users in reducing errors in tree structured markup while speeding up development cycles.",1
"Image caption generation is a long standing and challenging problem at the intersection of computer vision and natural language processing. A number of recently proposed approaches utilize a fully supervised object recognition model within the captioning approach. Such models, however, tend to generate sentences which only consist of objects predicted by the recognition models, excluding instances of the classes without labelled training examples. In this paper, we propose a new challenging scenario that targets the image captioning problem in a fully zero-shot learning setting, where the goal is to be able to generate captions of test images containing objects that are not seen during training. The proposed approach jointly uses a novel zero-shot object detection model and a template-based sentence generator. Our experiments show promising results on the COCO dataset.",0
"Image caption generation has been a popular research topic in computer vision recently due to its wide range of applications such as image retrieval, object detection, visual question answering, etc. Despite significant progress made by deep learning methods, most existing approaches struggle to describe novel objects that are unseen during training. This work addresses this problem through integrating semantic parsing into traditional convolutional neural networks (CNNs) architectures. We present an end-to-end trainable framework that can accurately localize and identify both seen and unseen objects using natural language queries without any external supervision. Experimental results on standard benchmark datasets show that our approach outperforms state-of-the-art methods by achieving higher F1 scores on multiple metrics including METEOR, CIDEr, SPICE, ROUGE-L, and human evaluations. Our method effectively improves the performance on identifying previously unseen objects by exploiting their similarities to known classes, demonstrating the effectiveness of incorporating textual guidance into CNN models for image understanding tasks.",1
"Learning text-video embeddings usually requires a dataset of video clips with manually provided captions. However, such datasets are expensive and time consuming to create and therefore difficult to obtain on a large scale. In this work, we propose instead to learn such embeddings from video data with readily available natural language annotations in the form of automatically transcribed narrations. The contributions of this work are three-fold. First, we introduce HowTo100M: a large-scale dataset of 136 million video clips sourced from 1.22M narrated instructional web videos depicting humans performing and describing over 23k different visual tasks. Our data collection procedure is fast, scalable and does not require any additional manual annotation. Second, we demonstrate that a text-video embedding trained on this data leads to state-of-the-art results for text-to-video retrieval and action localization on instructional video datasets such as YouCook2 or CrossTask. Finally, we show that this embedding transfers well to other domains: fine-tuning on generic Youtube videos (MSR-VTT dataset) and movies (LSMDC dataset) outperforms models trained on these datasets alone. Our dataset, code and models will be publicly available at: www.di.ens.fr/willow/research/howto100m/.",0
"Artificial intelligence has become increasingly important as technology continues to advance at a rapid pace. One area that has seen significant development in recent years is computer vision, which involves teaching machines how to interpret visual data such as images and videos. In this paper, we propose a new method for training text-video embeddings using a large dataset of narrated video clips. These embeddings can then be used to improve a variety of applications in areas like content understanding, automation, and retrieval. Our approach involves using a deep learning model called Time-Convolutional Networks (TCN) that takes into account both audio transcripts and corresponding frames from each clip in order to create the embedding space. We evaluate our method on several benchmark datasets and show that it significantly outperforms previous state-of-the-art methods while requiring less computational power during inference. This research represents a major step forward in improving computers ability to analyze and make sense of visual media, opening up exciting possibilities for future applications in fields ranging from entertainment to healthcare.",1
"In this paper, we introduce the task of retrieving relevant video moments from a large corpus of untrimmed, unsegmented videos given a natural language query. Our task poses unique challenges as a system must efficiently identify both the relevant videos and localize the relevant moments in the videos. This task is in contrast to prior work that localizes relevant moments in a single video or searches a large collection of already-segmented videos. For our task, we introduce Clip Alignment with Language (CAL), a model that aligns features for a natural language query to a sequence of short video clips that compose a candidate moment in a video. Our approach goes beyond prior work that aggregates video features over a candidate moment by allowing for finer clip alignment. Moreover, our approach is amenable to efficient indexing of the resulting clip-level representations, which makes it suitable for moment localization in large video collections. We evaluate our approach on three recently proposed datasets for temporal localization of moments in video with natural language extended to our video corpus moment retrieval setting: DiDeMo, Charades-STA, and ActivityNet-captions. We show that our CAL model outperforms the recently proposed Moment Context Network (MCN) on all criteria across all datasets on our proposed task, obtaining an 8%-85% and 11%-47% boost for average recall and median rank, respectively, and achieves 5x faster retrieval and 8x smaller index size with a 500K video corpus.",0
"In this paper we propose a method for temporally localizing moments within video collections using natural language descriptions. We use deep learning techniques such as object detection and temporal grounding, along with semantic embeddings from large pre-trained models like BERT, to model relationships between visual cues and textual prompts. Our approach builds on existing work by extending state-of-the-art methods and incorporating new data sources and modalities. Experiments show that our system can effectively identify relevant clips even under noisy conditions, providing high accuracy at both frame level and moment level granularity. This has implications for applications such as video retrieval, caption generation, and summarization.",1
"Pricing a rental property on Airbnb is a challenging task for the owner as it determines the number of customers for the place. On the other hand, customers have to evaluate an offered price with minimal knowledge of an optimal value for the property. This paper aims to develop a reliable price prediction model using machine learning, deep learning, and natural language processing techniques to aid both the property owners and the customers with price evaluation given minimal available information about the property. Features of the rentals, owner characteristics, and the customer reviews will comprise the predictors, and a range of methods from linear regression to tree-based models, support-vector regression (SVR), K-means Clustering (KMC), and neural networks (NNs) will be used for creating the prediction model.",0
"This paper presents a novel approach to predicting Airbnb rental prices using machine learning techniques combined with sentiment analysis. In recent years, there has been growing interest in understanding how online sharing platforms like Airbnb impact housing markets and tourism industries. Accurate price prediction models can inform hosts on optimal pricing strategies, while guests could benefit from finding better deals. However, few studies have investigated the factors influencing Airbnb rental rates, particularly those involving textual data extracted from online listings. We propose to address these limitations by integrating two streams of research: (i) analyzing user reviews as a source of unstructured data that captures the host’s reputation; (ii) extracting structured features based on property characteristics such as location, amenities, number of bedrooms/bathrooms. By developing a comprehensive model of rental prices through training on the entire dataset and evaluating predictions with standard metrics, we aim at providing insights into the price drivers that influence consumer behavior on the platform. Ultimately, our findings contribute to improved efficiency and accessibility in the short-term rental industry.",1
"The sample inefficiency of standard deep reinforcement learning methods precludes their application to many real-world problems. Methods which leverage human demonstrations require fewer samples but have been researched less. As demonstrated in the computer vision and natural language processing communities, large-scale datasets have the capacity to facilitate research by serving as an experimental and benchmarking platform for new methods. However, existing datasets compatible with reinforcement learning simulators do not have sufficient scale, structure, and quality to enable the further development and evaluation of methods focused on using human examples. Therefore, we introduce a comprehensive, large-scale, simulator-paired dataset of human demonstrations: MineRL. The dataset consists of over 60 million automatically annotated state-action pairs across a variety of related tasks in Minecraft, a dynamic, 3D, open-world environment. We present a novel data collection scheme which allows for the ongoing introduction of new tasks and the gathering of complete state information suitable for a variety of methods. We demonstrate the hierarchality, diversity, and scale of the MineRL dataset. Further, we show the difficulty of the Minecraft domain along with the potential of MineRL in developing techniques to solve key research challenges within it.",0
"In this paper we introduce MineRL (pronounced mine-rel), a new large-scale dataset of demonstrations from expert human players that can serve as supervised examples to train agents within the popular sandbox game Minecraft. We designed the collection process such that only high quality, informative, and diverse demonstrations were collected. These demonstrations cover 48 different tasks spread across five categories. Furthermore, we provide baseline results trained on our dataset outperforming those previously available. Our hope is that researchers utilize the MineRL dataset to further advance reinforcement learning algorithms.",1
"The automatic generation of radiology reports given medical radiographs has significant potential to operationally and improve clinical patient care. A number of prior works have focused on this problem, employing advanced methods from computer vision and natural language generation to produce readable reports. However, these works often fail to account for the particular nuances of the radiology domain, and, in particular, the critical importance of clinical accuracy in the resulting generated reports. In this work, we present a domain-aware automatic chest X-ray radiology report generation system which first predicts what topics will be discussed in the report, then conditionally generates sentences corresponding to these topics. The resulting system is fine-tuned using reinforcement learning, considering both readability and clinical accuracy, as assessed by the proposed Clinically Coherent Reward. We verify this system on two datasets, Open-I and MIMIC-CXR, and demonstrate that our model offers marked improvements on both language generation metrics and CheXpert assessed accuracy over a variety of competitive baselines.",0
"Automatically generated chest x-ray reports using deep learning have recently achieved high levels of accuracy. In fact, several studies indicate that these automatically generated reports can rival those produced by human radiologists. However, research has revealed several limitations related to their clinical use such as lack of standardized reporting templates, variability in annotation protocols, and inter-observer disagreements among annotators. We aimed to address some of these challenges by developing a system called Caesar which generates accurate and informative chest x-ray reports based on fine-tuned pre-trained models in the setting of diagnostic imaging at a large academic institution. Our system underwent validation including comparing against expert radiologist reports (N=42), and demonstrated good agreement rates ranging from 86% to 94%. Our work contributes to the broader goal of providing efficient automation tools which improve healthcare outcomes while minimizing human error and increasing efficiency.",1
"When describing images with natural language, the descriptions can be made more informative if tuned using downstream tasks. This is often achieved by training two networks: a ""speaker network"" that generates sentences given an image, and a ""listener network"" that uses them to perform a task. Unfortunately, training multiple networks jointly to communicate to achieve a joint task, faces two major challenges. First, the descriptions generated by a speaker network are discrete and stochastic, making optimization very hard and inefficient. Second, joint training usually causes the vocabulary used during communication to drift and diverge from natural language.   We describe an approach that addresses both challenges. We first develop a new effective optimization based on partial-sampling from a multinomial distribution combined with straight-through gradient updates, which we name PSST for Partial-Sampling Straight-Through. Second, we show that the generated descriptions can be kept close to natural by constraining them to be similar to human descriptions. Together, this approach creates descriptions that are both more discriminative and more natural than previous approaches. Evaluations on the standard COCO benchmark show that PSST Multinomial dramatically improve the recall@10 from 60% to 86% maintaining comparable language naturalness, and human evaluations show that it also increases naturalness while keeping the discriminative power of generated captions.",0
"This paper presents a cooperative approach to image captioning that leverages human feedback to improve the quality and relevance of generated descriptions. Traditional approaches to automatic image captioning have relied solely on computer vision algorithms, which can often generate incomplete or irrelevant descriptions. By incorporating human input into the caption generation process, we are able to address these limitations and produce more accurate and engaging descriptions. Our method involves two stages: first, an initial set of candidate captions is automatically generated using a convolutional neural network; then, humans evaluate these candidates and provide feedback on their quality and relevance. This feedback is used to update the model and refine the selection of candidate captions until a satisfactory description has been reached. We demonstrate the effectiveness of our approach through experiments on several benchmark datasets, showing that our cooperative system significantly outperforms traditional methods in terms of both accuracy and user satisfaction. Overall, this work represents a step forward towards building intelligent systems that effectively integrate human expertise and machine learning algorithms.",1
"For any financial organization, computing accurate quarterly forecasts for various products is one of the most critical operations. As the granularity at which forecasts are needed increases, traditional statistical time series models may not scale well. We apply deep neural networks in the forecasting domain by experimenting with techniques from Natural Language Processing (Encoder-Decoder LSTMs) and Computer Vision (Dilated CNNs), as well as incorporating transfer learning. A novel contribution of this paper is the application of curriculum learning to neural network models built for time series forecasting. We illustrate the performance of our models using Microsoft's revenue data corresponding to Enterprise, and Small, Medium & Corporate products, spanning approximately 60 regions across the globe for 8 different business segments, and totaling in the order of tens of billions of USD. We compare our models' performance to the ensemble model of traditional statistics and machine learning techniques currently used by Microsoft Finance. With this in-production model as a baseline, our experiments yield an approximately 30% improvement in overall accuracy on test data. We find that our curriculum learning LSTM-based model performs best, showing that it is reasonable to implement our proposed methods without overfitting on medium-sized data.",0
"This work presents a study on using deep neural networks (DNN) for financial forecasting by employing curriculum learning techniques. We aim to improve upon traditional methods used in DNN based forecasting models by utilizing the curriculum learning framework which allows the model to learn more effectively through a predefined sequence of tasks. Our approach focuses on constructing task sequences that allow the network to learn important features incrementally rather than attempting to capture all aspects at once. By doing so, we hypothesize that our proposed method can achieve better results compared to standard training approaches commonly used today. We evaluate our algorithm against benchmark datasets and other state-of-the-art algorithms to demonstrate its effectiveness. Overall, the use of curriculum learning provides an efficient means to enhance DNN performance without increasing computational complexity or adding additional parameters, making it appealing for real world applications in finance.",1
"Deep neural networks have achieved state-of-art performance in many domains including computer vision, natural language processing and self-driving cars. However, they are very computationally expensive and memory intensive which raises significant challenges when it comes to deploy or train them on strict latency applications or resource-limited environments. As a result, many attempts have been introduced to accelerate and compress deep learning models, however the majority were not able to maintain the same accuracy of the baseline models. In this paper, we describe EnSyth, a deep learning ensemble approach to enhance the predictability of compact neural network's models. First, we generate a set of diverse compressed deep learning models using different hyperparameters for a pruning method, after that we utilise ensemble learning to synthesise the outputs of the compressed models to compose a new pool of classifiers. Finally, we apply backward elimination on the generated pool to explore the best performing combinations of models. On CIFAR-10, CIFAR-5 data-sets with LeNet-5, EnSyth outperforms the predictability of the baseline model.",0
"In recent years deep learning has emerged as one of the most effective methods for machine learning tasks such as image classification, speech recognition, object detection, etc [4][2]. In practice, different models often perform differently on different datasets, which makes ensemble techniques that combine multiple models very popular [9],[8]. Ensemble methods can improve accuracy over individual models by reducing variance, which leads to improved generalization performance of models. While state-of-the-art ensembling techniques have demonstrated competitive results across many applications, they usually require complex hyperparameter tuning, extensive computational resources, or specific knowledge of individual model strengths and weaknesses. This work proposes EnSyth, an alternative approach to synthesizing a new deep neural network (DNN) from the output predictions of pre-trained DNNs using pruning [6],[7]. Using the CIFAR10 dataset as our benchmark, we compare two variants of EnSyth (random selection versus selecting based on confidence), against other state-of-the-art synthesis approaches like voting schemes, averaging, stacking, boosting, bagging, and gradient boosting. We find that both versions of EnSyth achieve significantly higher validation accuracy than these traditional methods while outperforming baseline single models trained under similar conditions. Moreover, EnSyth only selects top-performing networks whereas Random Selection always adds all available nets into its final result ensemble. Finally, compared to random selection EnSyth shows better parameter efficiency since it requires fewer parameters per unit area to reach comparable test accuracies. These results indicate that EnSyth could serve as a fast, simple-to-implement alternative method capable of matching existing high performance synthesi",1
"With the rapid growth of video data and the increasing demands of various applications such as intelligent video search and assistance toward visually-impaired people, video captioning task has received a lot of attention recently in computer vision and natural language processing fields. The state-of-the-art video captioning methods focus more on encoding the temporal information, while lack of effective ways to remove irrelevant temporal information and also neglecting the spatial details. However, the current RNN encoding module in single time order can be influenced by the irrelevant temporal information, especially the irrelevant temporal information is at the beginning of the encoding. In addition, neglecting spatial information will lead to the relationship confusion of the words and detailed loss. Therefore, in this paper, we propose a novel recurrent video encoding method and a novel visual spatial feature for the video captioning task. The recurrent encoding module encodes the video twice with the predicted key frame to avoid the irrelevant temporal information often occurring at the beginning and the end of a video. The novel spatial features represent the spatial information in different regions of a video and enrich the details of a caption. Experiments on two benchmark datasets show superior performance of the proposed method.",0
"""Caption generation for videos has been shown to be challenging due to the high complexity of visual content. Previous methods have relied on pre-training large language models on extensive amounts of text data or fine-tuning architectures designed for image recognition tasks. In our work, we propose using a novel approach that utilizes refocus attention modules within a video encoder network. Our method allows for better alignment between textual representations of objects found in videos and their corresponding visual contexts. We demonstrate that our model outperforms other state-of-the-art techniques through evaluation metrics such as BLEU score, METEOR, and CIDEr. Overall, our findings suggest that incorporating refocused attention into video encoders can lead to improved accuracy and coherency of generated captions.""",1
"Deep learning has been shown to achieve impressive results in several domains like computer vision and natural language processing. A key element of this success has been the development of new loss functions, like the popular cross-entropy loss, which has been shown to provide faster convergence and to reduce the vanishing gradient problem in very deep structures. While the cross-entropy loss is usually justified from a probabilistic perspective, this paper shows an alternative and more direct interpretation of this loss in terms of t-norms and their associated generator functions, and derives a general relation between loss functions and t-norms. In particular, the presented work shows intriguing results leading to the development of a novel class of loss functions. These losses can be exploited in any supervised learning task and which could lead to faster convergence rates that the commonly employed cross-entropy loss.",0
"In this paper we study the relations between t-norms (also called multiplicative negations) and loss functions from intuitionistic fuzzy set theory (IFS). Intuitionistic fuzzy sets have shown themselves very successful as tool in uncertainty management: they allow capturing more detailed and realistic descriptions compared to their crisp counterparts by means of two membership degrees, a nonmembership degree, and an accuracy (or possibility) coefficient. T-norms generalize Boolean conjunction and result in normalized values that model the ""degree of truth"" of the input IFS values; they are used to aggregate the latter into one value expressing some collective behavior based on different similarity measures. Loss functions were introduced to incorporate qualitative evaluations into the decision making process since they can reflect personal preferences among possible outcomes according to several criteria like accuracy, novelty etc. They assign real numbers serving as importance factors which lead then by multiplication either to overall results or rankings of alternatives. In our work we aim at identifying relations between both kinds of operations because so far little work has been done in order to provide links between these concepts even though a deep relationship appears plausible due to fundamental reasons originating from the mathematical theory of fuzzy sets and IFS extensions. Our approach starts considering a specific class of uninorms as case study whose representing functional expressions can be reformulated via corresponding Choquet integrals over certain types of t-conorms - the dual operation with respect to t-norms involving infima instead of suprema. This leads us into investigating suitable mappings between t-conorms and quantile functions appearing within definition of loss functions. We show that a wide variety of reasonable conditions can indeed generate such interdependencies guaranteei",1
"One of the questions that arises when designing models that learn to solve multiple tasks simultaneously is how much of the available training budget should be devoted to each individual task. We refer to any formalized approach to addressing this problem (learned or otherwise) as a task selection policy. In this work we provide an empirical evaluation of the performance of some common task selection policies in a synthetic bandit-style setting, as well as on the GLUE benchmark for natural language understanding. We connect task selection policy learning to existing work on automated curriculum learning and off-policy evaluation, and suggest a method based on counterfactual estimation that leads to improved model performance in our experimental settings.",0
"""Multitask learning (MTL) is a machine learning approach where a single model is trained on multiple tasks simultaneously. One key aspect of MTL is task selection: which subset of available tasks should the model be trained on? In this paper, we explore several task selection policies for MTL and evaluate their performance using real world datasets. We find that certain policies outperform others significantly, particularly those that use measures of redundancy among tasks and balance the tradeoff between exploiting diverse and related tasks. Our results provide insights into how different types of data characteristics affect MTL performance and suggest promising directions for future research.""",1
"As a fundamental problem of natural language processing, it is important to measure the distance between different documents. Among the existing methods, the Word Mover's Distance (WMD) has shown remarkable success in document semantic matching for its clear physical insight as a parameter-free model. However, WMD is essentially based on the classical Wasserstein metric, thus it often fails to robustly represent the semantic similarity between texts of different lengths. In this paper, we apply the newly developed Wasserstein-Fisher-Rao (WFR) metric from unbalanced optimal transport theory to measure the distance between different documents. The proposed WFR document distance maintains the great interpretability and simplicity as WMD. We demonstrate that the WFR document distance has significant advantages when comparing the texts of different lengths. In addition, an accelerated Sinkhorn based algorithm with GPU implementation has been developed for the fast computation of WFR distances. The KNN classification results on eight datasets have shown its clear improvement over WMD.",0
"In recent years, deep learning has revolutionized many fields by providing powerful algorithms capable of solving complex problems that were previously thought unsolvable. One such problem is document distance computation, which plays a crucial role in areas like natural language processing (NLP) and computer vision. Despite advances in NLP and computer vision, accurate document distance measures remain elusive due to the inherent complexity and subtleties involved in textual data analysis. This study proposes a new approach based on the Wasserstein-Fisher-Rao metric, which is a variant of the classical Earth Mover's Distance used commonly in transportation theory, econometrics, and machine learning applications. We evaluate our method using a range of benchmark datasets across diverse domains and compare it against several state-of-the-art approaches. Our results demonstrate the superiority of the proposed method in terms of accuracy and efficiency while offering more insight into how the algorithm works, making it easier for practitioners to fine-tune their models. Moreover, we provide code snippets and extensive experiments to facilitate research reproducibility and enable further exploration into related topics. Overall, this work represents an important step towards addressing one of the most challenging aspects of modern document analytics and opens up promising directions for future investigation.",1
"Bayesian nonparametric approaches, in particular the Pitman-Yor process and the associated two-parameter Chinese Restaurant process, have been successfully used in applications where the data exhibit a power-law behavior. Examples include natural language processing, natural images or networks. There is also growing empirical evidence that some datasets exhibit a two-regime power-law behavior: one regime for small frequencies, and a second regime, with a different exponent, for high frequencies. In this paper, we introduce a class of completely random measures which are doubly regularly-varying. Contrary to the Pitman-Yor process, we show that when completely random measures in this class are normalized to obtain random probability measures and associated random partitions, such partitions exhibit a double power-law behavior. We discuss in particular three models within this class: the beta prime process (Broderick et al. (2015, 2018), a novel process called generalized BFRY process, and a mixture construction. We derive efficient Markov chain Monte Carlo algorithms to estimate the parameters of these models. Finally, we show that the proposed models provide a better fit than the Pitman-Yor process on various datasets.",0
"This paper presents a new methodology for modeling data sets exhibiting double power law behavior, extending beyond traditional approaches such as the Chinese restaurant process (CRP) and the Pitman-Yor process. Our approach utilizes a novel class of statistical models capable of capturing both global and local features of the data, resulting in improved accuracy and flexibility over existing methods. We demonstrate the effectiveness of our models on real-world datasets, showing superior performance in terms of fitting and predictive ability compared to state-of-the-art techniques. Our work represents a significant advance in the field of uncovering hidden structure in complex data sets, with broad applications across diverse areas including machine learning, natural language processing, computer vision, and network science.",1
"Attention operators have been widely applied in various fields, including computer vision, natural language processing, and network embedding learning. Attention operators on graph data enables learnable weights when aggregating information from neighboring nodes. However, graph attention operators (GAOs) consume excessive computational resources, preventing their applications on large graphs. In addition, GAOs belong to the family of soft attention, instead of hard attention, which has been shown to yield better performance. In this work, we propose novel hard graph attention operator (hGAO) and channel-wise graph attention operator (cGAO). hGAO uses the hard attention mechanism by attending to only important nodes. Compared to GAO, hGAO improves performance and saves computational cost by only attending to important nodes. To further reduce the requirements on computational resources, we propose the cGAO that performs attention operations along channels. cGAO avoids the dependency on the adjacency matrix, leading to dramatic reductions in computational resource requirements. Experimental results demonstrate that our proposed deep models with the new operators achieve consistently better performance. Comparison results also indicates that hGAO achieves significantly better performance than GAO on both node and graph embedding tasks. Efficiency comparison shows that our cGAO leads to dramatic savings in computational resources, making them applicable to large graphs.",0
"Recently, graph representation learning has emerged as a promising field that exploits graphs to capture complex relationships among objects. Existing methods have mainly focused on designing powerful convolutional neural networks (CNN) architectures on graphs. Motivated by these works, we propose a novel hard and channel-wise attention network (HCA) which can adaptively weigh the importance of nodes and channels under resource constraints. We demonstrate that our HCA method outperforms state-of-the-art GCN baselines across three benchmark datasets: Citeseer, Cora, and Pubmed, in terms of accuracy metrics. Our ablation studies show that both hard attention and channel-wise softmax attention mechanisms contribute significantly to performance gains. Furthermore, we study convergence analysis of our approach over different training sizes and epochs to confirm its stability. Finally, we visualize node embeddings learned by HCA to verify its interpretability. Overall, our work highlights the effectiveness of incorporating hard and channel-wise attention into graph representation learning, laying the foundation for future research in this area.",1
"Deep Neural Networks have shown tremendous success in the area of object recognition, image classification and natural language processing. However, designing optimal Neural Network architectures that can learn and output arbitrary graphs is an ongoing research problem. The objective of this survey is to summarize and discuss the latest advances in methods to Learn Representations of Graph Data. We start by identifying commonly used types of graph data and review basics of graph theory. This is followed by a discussion of the relationships between graph kernel methods and neural networks. Next we identify the major approaches used for learning representations of graph data namely: Kernel approaches, Convolutional approaches, Graph neural networks approaches, Graph embedding approaches and Probabilistic approaches. A variety of methods under each of the approaches are discussed and the survey is concluded with a brief discussion of the future of learning representation of graph data.",0
"This paper presents a survey of recent advances in learning representations of graph data. With the increasing importance of large scale network analysis, representing complex relationships among data points has become crucial. In this work we analyze existing methods for extracting meaningful features from graph data. Our results highlight the strengths and weaknesses of different approaches, allowing us to develop new insights into how these techniques can be improved. We find that while some methods perform well on certain types of graphs, no single approach works consistently across all datasets. Therefore, developing adaptive representations which can effectively capture the underlying structure of diverse graph data remains an open challenge. Finally, we discuss promising directions for future research aimed at addressing this problem.",1
"Machine learning plays an increasing role in intelligent tutoring systems as both the amount of data available and specialization among students grow. Nowadays, these systems are frequently deployed on mobile applications. Users on such mobile education platforms are dynamic, frequently being added, accessing the application with varying levels of focus, and changing while using the service. The education material itself, on the other hand, is often static and is an exhaustible resource whose use in tasks such as problem recommendation must be optimized. The ability to update user models with respect to educational material in real-time is thus essential; however, existing approaches require time-consuming re-training of user features whenever new data is added. In this paper, we introduce a neural pedagogical agent for real-time user modeling in the task of predicting user response correctness, a central task for mobile education applications. Our model, inspired by work in natural language processing on sequence modeling and machine translation, updates user features in real-time via bidirectional recurrent neural networks with an attention mechanism over embedded question-response pairs. We experiment on the mobile education application SantaTOEIC, which has 559k users, 66M response data points as well as a set of 10k study problems each expert-annotated with topic tags and gathered since 2016. Our model outperforms existing approaches over several metrics in predicting user response correctness, notably out-performing other methods on new users without large question-response histories. Additionally, our attention mechanism and annotated tag set allow us to create an interpretable education platform, with a smart review system that addresses the aforementioned issue of varied user attention and problem exhaustion.",0
"Abstract: In recent years, there has been growing interest in developing artificial intelligence (AI) systems that can support teaching and learning. One promising approach involves creating neural pedagogical agents (NPAs), which are intelligent software programs designed to interact with learners and provide personalized feedback on their performance. However, building effective NPAs remains a challenge due to the complexity of human learning dynamics and the lack of datasets suitable for training these models.  This paper presents a novel method for creating NPAs using multi-task deep learning techniques to jointly address two key tasks: assessment prediction and review analysis. By leveraging rich linguistic cue structures present in reviews and learner responses during interaction, our model can capture subtle relationships and dependencies between different aspects of cognition and metacognition relevant to pedagogy, such as knowledge vs. skills. Experiments conducted on real data demonstrate that our proposed algorithm significantly outperforms state-of-the-art baselines across several benchmarks and provides meaningful insights into how learners engage with content. Our work showscases the potential of integrating multiple sources of fine-grained data with advanced machine learning methods to create highly capable NPAs that may positively impact education practices worldwide.  Note: This is a fictional study but please contact me if you have any questions about writing scientific papers or would like assistance with your research. I am here to help!",1
"Open-ended video question answering aims to automatically generate the natural-language answer from referenced video contents according to the given question. Currently, most existing approaches focus on short-form video question answering with multi-modal recurrent encoder-decoder networks. Although these works have achieved promising performance, they may still be ineffectively applied to long-form video question answering due to the lack of long-range dependency modeling and the suffering from the heavy computational cost. To tackle these problems, we propose a fast Hierarchical Convolutional Self-Attention encoder-decoder network(HCSA). Concretely, we first develop a hierarchical convolutional self-attention encoder to efficiently model long-form video contents, which builds the hierarchical structure for video sequences and captures question-aware long-range dependencies from video context. We then devise a multi-scale attentive decoder to incorporate multi-layer video representations for answer generation, which avoids the information missing of the top encoder layer. The extensive experiments show the effectiveness and efficiency of our method.",0
"In recent years, there has been increasing interest in developing algorithms that can effectively handle complex natural language tasks such as question answering, where the goal is to generate a concise and accurate response given a text input. However, most existing approaches have focused on specific types of questions (e.g., factoid queries) and often require extensive hand engineering of features or training data. In contrast, our work addresses open-ended long-form video question answering, which involves generating answers to questions asked by users watching videos. To achieve this, we propose a novel deep learning model called Hierarchical Convolutional Self-Attention Networks (HCSAN), which combines convolutional neural networks with self-attention mechanisms to enable efficient handling of sequential inputs like video frames, while capturing hierarchical structure across different levels of abstraction. Our experimental results demonstrate that HCSAN outperforms several strong baseline models on a challenging dataset of instructional cooking videos and questions, achieving state-of-the-art performance in terms of both automatic metrics (nRouge) and human evaluation (Likert scale). These findings suggest that our approach holds promise for enabling more advanced conversational agents in domains ranging from customer support and education to entertainment and gaming. Overall, we hope that this research paves the way towards creating intelligent systems capable of engaging in richer forms of dialogue with humans, beyond simple question-and-answer interactions.",1
"When translating natural language questions into SQL queries to answer questions from a database, we would like our methods to generalize to domains and database schemas outside of the training set. To handle complex questions and database schemas with a neural encoder-decoder paradigm, it is critical to properly encode the schema as part of the input with the question. In this paper, we use relation-aware self-attention within the encoder so that it can reason about how the tables and columns in the provided schema relate to each other and use this information in interpreting the question. We achieve significant gains on the recently-released Spider dataset with 42.94% exact match accuracy, compared to the 18.96% reported in published work.",0
"This research paper presents a novel approach for encoding database schemas during the training process of text-to-sql parsers using relation-aware self-attention mechanisms. We introduce two attention methods that leverage structured knowledge within relational databases to provide better guidance to these models on how to generate SQL queries from natural language statements. Our proposed techniques can be applied as drop-in replacements for existing attention layers without changing the overall architecture. Experiments conducted on three benchmark datasets demonstrate the effectiveness of our approaches, outperforming previous state-of-the-art methods by significant margins while maintaining competitive computational efficiency. In addition to improving text-to-sql performance, we conduct an ablation study to analyze the impact of each individual component within our attention frameworks, providing insights into their contributions towards improved query generation accuracy. Overall, our findings highlight the importance of incorporating external domain knowledge into NLP systems to enhance problem-solving capabilities in complex application domains such as semantic parsing.",1
"Due to the surprisingly good representation power of complex distributions, neural network (NN) classifiers are widely used in many tasks which include natural language processing, computer vision and cyber security. In recent works, people noticed the existence of adversarial examples. These adversarial examples break the NN classifiers' underlying assumption that the environment is attack free and can easily mislead fully trained NN classifier without noticeable changes. Among defensive methods, adversarial training is a popular choice. However, original adversarial training with single-step adversarial examples (Single-Adv) can not defend against iterative adversarial examples. Although adversarial training with iterative adversarial examples (Iter-Adv) can defend against iterative adversarial examples, it consumes too much computational power and hence is not scalable. In this paper, we analyze Iter-Adv techniques and identify two of their empirical properties. Based on these properties, we propose modifications which enhance Single-Adv to perform competitively as Iter-Adv. Through preliminary evaluation, we show that the proposed method enhances the test accuracy of state-of-the-art (SOTA) Single-Adv defensive method against iterative adversarial examples by up to 16.93% while reducing its training cost by 28.75%.",0
"Artificial intelligence has seen rapid advances over recent years thanks to deep learning methods which utilize data-driven models to train their weights through backpropagation. These models have been trained using large amounts of data and can achieve remarkable performance on tasks such as image classification and speech recognition. However, these models may become brittle against attacks that exploit input perturbations resulting in incorrect predictions, causing safety concerns across fields employing machine learning models. In particular, adversaries can use gradient sign to craft adversarial examples. To address security issues and guarantee robustness of machine learning systems, defenses like adversarial training were introduced by applying random noise to inputs during model training in order to increase resilience towards possible threats. As for most approaches, stronger guarantees come at higher computational cost, making them hard to scale up in real world applications. This research suggests improving efficiency in terms of computation time can be achieved while preserving reliability of the defense with simple techniques built upon intuitions gathered from empirical studies. The methodology developed here sets out new evidence about how to leverage properties from training loss curves during adversarial training without requiring complex analysis nor extra hyperparameter tuning. Practitioners would then obtain better trade-offs when deploying their own models to production stage. Overall, the results contribute to providing more efficient solutions that retain integrity and offer insights into enhancing our understanding of adversarial training defenses.",1
"Generating textual descriptions for images has been an attractive problem for the computer vision and natural language processing researchers in recent years. Dozens of models based on deep learning have been proposed to solve this problem. The existing approaches are based on neural encoder-decoder structures equipped with the attention mechanism. These methods strive to train decoders to minimize the log likelihood of the next word in a sentence given the previous ones, which results in the sparsity of the output space. In this work, we propose a new approach to train decoders to regress the word embedding of the next word with respect to the previous ones instead of minimizing the log likelihood. The proposed method is able to learn and extract long-term information and can generate longer fine-grained captions without introducing any external memory cell. Furthermore, decoders trained by the proposed technique can take the importance of the generated words into consideration while generating captions. In addition, a novel semantic attention mechanism is proposed that guides attention points through the image, taking the meaning of the previously generated word into account. We evaluate the proposed approach with the MS-COCO dataset. The proposed model outperformed the state of the art models especially in generating longer captions. It achieved a CIDEr score equal to 125.0 and a BLEU-4 score equal to 50.5, while the best scores of the state of the art models are 117.1 and 48.0, respectively.",0
"In recent years, encoder-decoder models have shown promising results in image caption generation tasks due to their ability to capture visual features from images and generate natural language descriptions accordingly. However, these models still face challenges such as poor decoding performance and difficulty capturing high-level semantic information. To address these issues, we propose a novel deep decoder structure based on word embedding regression that leverages pretrained word embeddings and a carefully designed network architecture to enhance the decoding process. Our approach consists of two components: a global context module and local content modules. The global context module uses pretrained word embeddings to encode global contextual information about previous tokens in the generated sentence, while the local content module processes spatial information extracted from each image region to create more detailed captions. We evaluate our model on three benchmark datasets (MSCOCO, Flickr8K, and SBU) and show significant improvements over state-of-the-art methods. Our proposed approach achieves new state-of-the-art performances across all metrics, demonstrating the effectiveness of our solution in generating accurate and informative image captions. Overall, our work provides valuable insights into enhancing the quality of image captioning outputs using advanced deep learning techniques.",1
"One of the most exciting technology breakthroughs in the last few years has been the rise of deep learning. State-of-the-art deep learning models are being widely deployed in academia and industry, across a variety of areas, from image analysis to natural language processing. These models have grown from fledgling research subjects to mature techniques in real-world use. The increasing scale of data, computational power and the associated algorithmic innovations are the main drivers for the progress we see in this field. These developments also have a huge potential for the automotive industry and therefore the interest in deep learning-based technology is growing. A lot of the product innovations, such as self-driving cars, parking and lane-change assist or safety functions, such as autonomous emergency braking, are powered by deep learning algorithms. Deep learning is poised to offer gains in performance and functionality for most ADAS (Advanced Driver Assistance System) solutions. Virtual sensing for vehicle dynamics application, vehicle inspection/heath monitoring, automated driving and data-driven product development are key areas that are expected to get the most attention. This article provides an overview of the recent advances and some associated challenges in deep learning techniques in the context of automotive applications.",0
"Abstract:  The automotive industry has seen significant advancements in recent years due to deep learning techniques that have been applied to several domains within this field. From enhancing vehicle performance through autonomous driving systems and driver assistance technologies to improving manufacturing processes and customer experience, the use of artificial intelligence (AI) algorithms such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), and reinforcement learning models has provided valuable insights and innovations. This article discusses some of these recent developments and their impact on different areas of the automotive sector. We present case studies demonstrating how machine learning methods can be employed to solve real-world problems encountered by both OEMs and suppliers operating in this space. Our findings show that while challenges remain in terms of data quality and availability, scalability, interpretability, and explainability, there is vast potential for further growth and adoption of deep learning approaches across the entire value chain. Ultimately, this research seeks to provide a comprehensive review of current trends in the application of AI to the automobile business and inspire new ideas and collaborations that push the boundaries of knowledge creation.",1
"We propose a sign language translation system based on human keypoint estimation. It is well-known that many problems in the field of computer vision require a massive amount of dataset to train deep neural network models. The situation is even worse when it comes to the sign language translation problem as it is far more difficult to collect high-quality training data. In this paper, we introduce the KETI (short for Korea Electronics Technology Institute) sign language dataset which consists of 14,672 videos of high resolution and quality. Considering the fact that each country has a different and unique sign language, the KETI sign language dataset can be the starting line for further research on the Korean sign language translation. Using the KETI sign language dataset, we develop a neural network model for translating sign videos into natural language sentences by utilizing the human keypoints extracted from a face, hands, and body parts. The obtained human keypoint vector is normalized by the mean and standard deviation of the keypoints and used as input to our translation model based on the sequence-to-sequence architecture. As a result, we show that our approach is robust even when the size of the training data is not sufficient. Our translation model achieves 93.28% (55.28%, respectively) translation accuracy on the validation set (test set, respectively) for 105 sentences that can be used in emergency situations. We compare several types of our neural sign translation models based on different attention mechanisms in terms of classical metrics for measuring the translation performance.",0
"In recent years, there has been growing interest in developing automatic sign language translation systems that can facilitate communication between deaf individuals and hearing individuals who do not know sign language. One promising approach to this problem is to use neural networks to learn the mapping from images of signed gestures to their corresponding textual translations. However, acquiring large datasets of paired sign gesture and transcription data is challenging due to the time-consuming process involved in collecting manually labeled sign gestures from real-world scenes. To address this issue, we propose using human keypoint estimation as a means of automatically generating high-quality annotated training data for sign language translation models. Our method leverages state-of-the-art computer vision algorithms to detect and track human body joints in video sequences of sign gestures, and uses these estimates to extract feature representations that capture the unique spatial relationships among key points in each gesture. We then train a deep convolutional network model to predict signs directly from these features without requiring explicit annotations at the level of individual pixels. Experimental results demonstrate the effectiveness of our approach compared to several baseline methods across both publicly available benchmark datasets and newly collected data captured by a user-friendly mobile application designed specifically for our study. This work represents an important step towards enabling robust sign language translation tools through efficient annotation techniques that can scale to real-world usage scenarios.",1
"Any given classification problem can be modeled using multi-class or One-vs-All (OVA) architecture. An OVA system consists of as many OVA models as the number of classes, providing the advantage of asynchrony, where each OVA model can be re-trained independent of other models. This is particularly advantageous in settings where scalable model training is a consideration (for instance in an industrial environment where multiple and frequent updates need to be made to the classification system). In this paper, we conduct empirical analysis on realizing independent updates to OVA models and its impact on the accuracy of the overall OVA system. Given that asynchronous updates lead to differences in training datasets for OVA models, we first define a metric to quantify the differences in datasets. Thereafter, using Natural Language Understanding as a task of interest, we estimate the impact of three factors: (i) number of classes, (ii) number of data points and, (iii) divergences in training datasets across OVA models; on the OVA system accuracy. Finally, we observe the accuracy impact of increased asynchrony in a Spoken Language Understanding system. We analyze the results and establish that the proposed metric correlates strongly with the model performances in both the experimental settings.",0
"In this paper, we evaluate the effectiveness of one-versus-all (OvA) models for asynchronous training using empirical analysis. We conduct our experiments on several benchmark datasets, including MNIST, CIFAR-10, and ImageNet. Our results demonstrate that OvA models achieve competitive accuracy compared to other state-of-the-art methods such as batch normalization and mixup regularization techniques. Furthermore, we show that OvA models can significantly reduce model size while maintaining high performance levels. These findings suggest that OvA models could potentially be used to improve the efficiency of deep learning systems without sacrificing accuracy. Additionally, we propose two new variations of the standard OvA technique - one based on confidence intervals and another based on temperature scaling - which further enhance the overall performance of these models. Overall, our work highlights the potential of OvA models for improving the robustness and scalability of modern machine learning systems.",1
"Neural machine translation models are used to automatically generate a document from given source code since this can be regarded as a machine translation task. Source code summarization is one of the components for automatic document generation, which generates a summary in natural language from given source code. This suggests that techniques used in neural machine translation, such as Long Short-Term Memory (LSTM), can be used for source code summarization. However, there is a considerable difference between source code and natural language: Source code is essentially {\em structured}, having loops and conditional branching, etc. Therefore, there is some obstacle to apply known machine translation models to source code.   Abstract syntax trees (ASTs) capture these structural properties and play an important role in recent machine learning studies on source code. Tree-LSTM is proposed as a generalization of LSTMs for tree-structured data. However, there is a critical issue when applying it to ASTs: It cannot handle a tree that contains nodes having an arbitrary number of children and their order simultaneously, which ASTs generally have such nodes. To address this issue, we propose an extension of Tree-LSTM, which we call \emph{Multi-way Tree-LSTM} and apply it for source code summarization. As a result of computational experiments, our proposal achieved better results when compared with several state-of-the-art techniques.",0
"Automatic source code summarization has been a challenging task in software engineering, as it requires understanding complex code structures and identifying important information within large amounts of data. In recent years, deep learning techniques have shown promise in tackling this problem due to their ability to process vast amounts of data and capture patterns at multiple levels of abstraction. Our research introduces a novel approach that utilizes extended tree-Long Short Term Memory (Tree-LSTM) networks, which can effectively model hierarchical representations of programming constructs and generate more accurate summaries than existing methods. Through extensive experimentation on four real-world datasets representing different program domains, we demonstrate that our approach achieves state-of-the-art performance across various evaluation metrics, including Fowlkes Mallow distance, ROUGE score, and average token similarity. This work offers a promising solution for automating the generation of high-quality code summaries, which could significantly improve developer productivity by providing concise and easily digestible summaries of codebases.",1
"Whether it is computer vision, natural language processing or speech recognition, the essence of these applications is to obtain powerful feature representations that make downstream applications completion more efficient. Taking image recognition as an example, whether it is hand-crafted low-level feature representation or feature representation extracted by a convolutional neural networks(CNNs), the goal is to extract features that better represent image features, thereby improving classification accuracy. However, we observed that image feature representations share a large common vector and a few top dominating directions. To address this problems, we propose a simple but effective postprocessing method to render off-the-shelf feature representations even stronger by eliminating the common mean vector from off-the-shelf feature representations. The postprocessing is empirically validated on a variety of datasets and feature extraction methods.such as VGG, LBP, and HOG. Some experiments show that the features that have been post-processed by postprocessing algorithm can get better results than original ones.",0
"This paper presents a new postprocessing method for improving image classification accuracy. Our approach utilizes channel-wise Gaussian blurring followed by thresholding to generate more robust binary masks, which can then be applied to the original RGB images during inference. We demonstrate through extensive experiments that our method significantly outperforms existing state-of-the-art methods across a variety of benchmark datasets while maintaining competitive model size requirements. Additionally, we provide ablation studies to explore the contribution of each component in our proposed pipeline. Overall, our findings indicate that our simple yet effective method holds great potential for enhancing real-world applications in computer vision.",1
"In this work, we describe practical lessons we have learned from successfully using contextual bandits (CBs) to improve key business metrics of the Microsoft Virtual Agent for customer support. While our current use cases focus on single step einforcement learning (RL) and mostly in the domain of natural language processing and information retrieval we believe many of our findings are generally applicable. Through this article, we highlight certain issues that RL practitioners may encounter in similar types of applications as well as offer practical solutions to these challenges.",0
"In recent years, contextual bandit learning has emerged as a powerful technique for designing personalized user experiences on web platforms. By leveraging machine learning algorithms to learn which content items lead to desired outcomes (such as clicks), companies have been able to boost key metrics like clickthrough rates and conversion rates. Despite these successes, relatively little research has focused on how contextual bandits can improve customer support bots - software agents designed to interact with users online using natural language. This study seeks to address that gap by presenting empirical results showing how contextual bandits can greatly enhance customer satisfaction with bot interaction while reducing human intervention costs. We demonstrate how our approach provides significant gains over other state-of-the-art methods, including deep reinforcement learning techniques, through experiments conducted at a large technology company. Our work opens up new opportunities for researchers looking to create more effective and engaging conversational software agents.",1
"City Logistics is characterized by multiple stakeholders that often have different views of such a complex system. From a public policy perspective, identifying stakeholders, issues and trends is a daunting challenge, only partially addressed by traditional observation systems. Nowadays, social media is one of the biggest channels of public expression and is often used to communicate opinions and content related to City Logistics. The idea of this research is that analysing social media content could help in understanding the public perception of City logistics. This paper offers a methodology for collecting content from Twitter and implementing Machine Learning techniques (Unsupervised Learning and Natural Language Processing), to perform content and sentiment analysis. The proposed methodology is applied to more than 110 000 tweets containing City Logistics key-terms. Results allowed the building of an Interest Map of concepts and a Sentiment Analysis to determine if City Logistics entries are positive, negative or neutral.",0
"Logistics, both commercial and private, have been transformed by social media and online services; indeed, today you can order almost anything online. Social networks, such as Facebook and Twitter provide insights into daily life like never before – including how we travel around our cities. This analysis takes a look at logistical tweets from across Birmingham (one of England’s largest cities) using an unsupervised approach called LDA (Latent Dirichlet Allocation). Our findings showed that there was clear spatial segregation of tweeting patterns during rush hour; these were further analysed based on mode of transport: train, car, bike & walking. Moreover, weekend tweeting differed greatly compared to the working week - in particular, bus usage spiked over Friday/Saturday and remained high throughout Saturday. Finally, we found evidence of ""Twitter taxis"" based on pickup location hotspots near late night venues and taxi ranks. These findings suggest that even within just one city, logistical activity is far more complex than simple origin/destination matrices often used in transport modelling. They show that using novel data sources like Twitter can provide new insights and complement official statistics - thus widening our understanding of human movement beyond traditional sources.",1
"In this work\footnote {This work was supported in part by the National Science Foundation under grant IIS-1212948.}, we present a method to represent a video with a sequence of words, and learn the temporal sequencing of such words as the key information for predicting and recognizing human actions. We leverage core concepts from the Natural Language Processing (NLP) literature used in sentence classification to solve the problems of action prediction and action recognition. Each frame is converted into a word that is represented as a vector using the Bag of Visual Words (BoW) encoding method. The words are then combined into a sentence to represent the video, as a sentence. The sequence of words in different actions are learned with a simple but effective Temporal Convolutional Neural Network (T-CNN) that captures the temporal sequencing of information in a video sentence. We demonstrate that a key characteristic of the proposed method is its low-latency, i.e. its ability to predict an action accurately with a partial sequence (sentence). Experiments on two datasets, \textit{UCF101} and \textit{HMDB51} show that the method on average reaches 95\% of its accuracy within half the video frames. Results, also demonstrate that our method achieves compatible state-of-the-art performance in action recognition (i.e. at the completion of the sentence) in addition to action prediction.",0
"Title: A Temporal Sequence Learning Approach for Effective Action Recognition and Prediction  This research proposes a novel approach based on temporal sequence learning that significantly improves the performance of action recognition and prediction tasks. We present a new model architecture called Temporal Convolutional Network (TCN) which explicitly models spatial and temporal dependencies between video frames using dilated convolutions. Our proposed method achieves state-of-the-art results on two challenging datasets, namely UCF-101 and HMDB-51, outperforming several popular methods by large margins.  Our TCN architecture exploits the spatio-temporal hierarchy inherent in action recognition by employing multi-scale feature extraction and deep supervision through multiple stages of processing. By leveraging both local and global features simultaneously, our approach captures complex patterns in human actions more effectively than traditional approaches relying solely on handcrafted feature representations. Furthermore, we introduce an online adaptation mechanism that enables real-time fine-tuning during deployment, enabling our model to adapt rapidly to unseen videos and improve prediction accuracy over time.  In summary, this work demonstrates the effectiveness of a temporal sequence learning approach for action recognition and prediction tasks, setting a new benchmark for future research in this field. With its strong performance and versatility across different domains, our TCN framework has wide applicability in areas such as video surveillance, robotics, and virtual reality.",1
"Recent advances in multi-modal vision and language tasks enable a new set of applications. In this paper, we consider the task of generating natural language fashion feedback on outfit images. We collect a unique dataset, which contains outfit images and corresponding positive and constructive fashion feedback. We treat each feedback type separately, and train deep generative encoder-decoder models with visual attention, similar to the standard image captioning pipeline. Following this approach, the generated sentences tend to be too general and non-informative. We propose an alternative decoding technique based on the Maximum Mutual Information objective function, which leads to more diverse and detailed responses. We evaluate our model with common language metrics, and also show human evaluation results. This technology is applied within the ``Alexa, how do I look?'' feature, publicly available in Echo Look devices.",0
"Here we present two tasks that allow a generative model trained on large amounts of user generated content such as reviews, comments and articles (such as from Reddit and Yelp) to generate diverse and informative feedback based upon prompts such as images and/or textual descriptions (e.g. ""this dress looks great!""). This allows users to browse items by generating natural language feedback tailored towards their preferences without actually having to view or interact with the underlying system responsible. As well as allowing greater ease of use, this approach has significant privacy benefits: users who wish to remain anonymous can still obtain personalized recommendations without exposing themselves to the system they are using to access them. We further discuss technical details necessary to make such systems usable for end-users.",1
"Video captioning aims to automatically generate natural language descriptions of video content, which has drawn a lot of attention recent years. Generating accurate and fine-grained captions needs to not only understand the global content of video, but also capture the detailed object information. Meanwhile, video representations have great impact on the quality of generated captions. Thus, it is important for video captioning to capture salient objects with their detailed temporal dynamics, and represent them using discriminative spatio-temporal representations. In this paper, we propose a new video captioning approach based on object-aware aggregation with bidirectional temporal graph (OA-BTG), which captures detailed temporal dynamics for salient objects in video, and learns discriminative spatio-temporal representations by performing object-aware local feature aggregation on detected object regions. The main novelties and advantages are: (1) Bidirectional temporal graph: A bidirectional temporal graph is constructed along and reversely along the temporal order, which provides complementary ways to capture the temporal trajectories for each salient object. (2) Object-aware aggregation: Learnable VLAD (Vector of Locally Aggregated Descriptors) models are constructed on object temporal trajectories and global frame sequence, which performs object-aware aggregation to learn discriminative representations. A hierarchical attention mechanism is also developed to distinguish different contributions of multiple objects. Experiments on two widely-used datasets demonstrate our OA-BTG achieves state-of-the-art performance in terms of BLEU@4, METEOR and CIDEr metrics.",0
"This paper presents a novel approach to video captioning that leverages object detection and bidirectional temporal graph reasoning. The proposed method uses state-of-the-art object detection algorithms to identify objects in each frame of the input video sequence. These detected objects are then used as nodes in a temporally connected graph representation, where edges represent interactions and relationships between objects over time. By employing a bidirectional temporal graph architecture, our approach can capture both short-term and long-term dependencies among the objects across different frames, enabling more accurate and contextually rich captions. To evaluate our method, we conducted extensive experiments on several benchmark datasets and achieved competitive results compared to existing approaches in the literature. Our work demonstrates the effectiveness of incorporating temporal object awareness into video captioning models for improved performance and better capturing complex visual narratives in videos.",1
"Variational autoencoders (VAEs) with an auto-regressive decoder have been applied for many natural language processing (NLP) tasks. The VAE objective consists of two terms, (i) reconstruction and (ii) KL regularization, balanced by a weighting hyper-parameter \beta. One notorious training difficulty is that the KL term tends to vanish. In this paper we study scheduling schemes for \beta, and show that KL vanishing is caused by the lack of good latent codes in training the decoder at the beginning of optimization. To remedy this, we propose a cyclical annealing schedule, which repeats the process of increasing \beta multiple times. This new procedure allows the progressive learning of more meaningful latent codes, by leveraging the informative representations of previous cycles as warm re-starts. The effectiveness of cyclical annealing is validated on a broad range of NLP tasks, including language modeling, dialog response generation and unsupervised language pre-training.",0
"A common challenge faced in training machine learning models is the problem of ""KL vanishing,"" where the Kullback-Leibler (KL) divergence between the model distribution and the target distribution becomes very small during optimization, leading to slow convergence and poor generalization performance. In this work, we propose a simple approach called cyclical annealing schedule that helps mitigate KL vanishing. This method involves periodically reducing the temperature parameter used in the softmax function during training. We show through experiments on several benchmark datasets that using our proposed approach leads to significant improvements in model accuracy compared to traditional methods. Our findings demonstrate the effectiveness of cyclical annealing schedule as a straightforward technique for addressing the issue of KL vanishing.",1
"To be successful in real-world tasks, Reinforcement Learning (RL) needs to exploit the compositional, relational, and hierarchical structure of the world, and learn to transfer it to the task at hand. Recent advances in representation learning for language make it possible to build models that acquire world knowledge from text corpora and integrate this knowledge into downstream decision making problems. We thus argue that the time is right to investigate a tight integration of natural language understanding into RL in particular. We survey the state of the field, including work on instruction following, text games, and learning from textual domain knowledge. Finally, we call for the development of new environments as well as further investigation into the potential uses of recent Natural Language Processing (NLP) techniques for such tasks.",0
"In recent years, there has been significant interest in using natural language processing techniques to enhance reinforcement learning algorithms. This survey paper provides an overview of the state-of-the-art in this field, examining how natural language can be used to inform and improve reinforcement learning algorithms. We discuss key challenges and opportunities presented by incorporating natural language into reinforcement learning, as well as promising approaches that have emerged in this area. Our aim is to provide researchers and practitioners with a comprehensive understanding of the current landscape and future directions for research in this exciting field.",1
"We propose a new approach to address the text classification problems when learning with partial labels is beneficial. Instead of offering each training sample a set of candidate labels, we assign negative-oriented labels to the ambiguous training examples if they are unlikely fall into certain classes. We construct our new maximum likelihood estimators with self-correction property, and prove that under some conditions, our estimators converge faster. Also we discuss the advantages of applying one of our estimator to a fully supervised learning problem. The proposed method has potential applicability in many areas, such as crowdsourcing, natural language processing and medical image analysis.",0
"A new approach has been proposed for reducing the computational costs associated with training machine learning models for text classification tasks while maintaining high levels of accuracy. This approach involves a novel technique called ""partial labeling"", which reduces the number of labeled examples required during training by identifying instances where full labeling may not be necessary. By leveraging the power of natural language processing techniques, the method can accurately estimate missing labels for partially labeled data points. Experiments show that the resulting model achieves comparable performance to those trained on fully-labeled datasets, but with significant reductions in both time and labor costs. These findings have important implications for real-world applications of text classification, including fields such as customer service, social media analysis, and business intelligence. Overall, this work presents a promising direction for researchers looking to balance efficiency with accuracy in large-scale machine learning problems.",1
"Machine Learning (ML) and Deep Learning (DL) models have achieved state-of-the-art performance on multiple learning tasks, from vision to natural language modelling. With the growing adoption of ML and DL to many areas of computer science, recent research has also started focusing on the security properties of these models. There has been a lot of work undertaken to understand if (deep) neural network architectures are resilient to black-box adversarial attacks which craft perturbed input samples that fool the classifier without knowing the architecture used. Recent work has also focused on the transferability of adversarial attacks and found that adversarial attacks are generally easily transferable between models, datasets, and techniques. However, such attacks and their analysis have not been covered from the perspective of unsupervised machine learning algorithms. In this paper, we seek to bridge this gap through multiple contributions. We first provide a strong (iterative) black-box adversarial attack that can craft adversarial samples which will be incorrectly clustered irrespective of the choice of clustering algorithm. We choose 4 prominent clustering algorithms, and a real-world dataset to show the working of the proposed adversarial algorithm. Using these clustering algorithms we also carry out a simple study of cross-technique adversarial attack transferability.",0
"Title: Advancing Unsupervised ML Model Vulnerability through Black-Box Adversarial Attack Techniques  This paper focuses on evaluating how vulnerable unsupervised machine learning (ML) models can become if subjected to powerful black-box adversaries. We introduce innovative attack methods capable of generating perturbations against ML classifiers that produce highly accurate results within the unsupervised domain. Our techniques effectively bypass model defenses and evade detection while ensuring high success rates across multiple datasets and architectures. By conducting extensive experiments, we demonstrate the ease at which even state-of-the-art unsupervised models succumb to advanced attacks, emphasizing the importance of continued research into ML security measures. Additionally, we provide insights into potential directions for future work aimed towards enhancing the robustness of these models under real-world threat scenarios. Overall, our study highlights the current limitations of relying solely on unlabeled data for training unsupervised ML systems and underscores the need for further development of defense mechanisms in this area.",1
"In this paper, we propose Text2Scene, a model that generates various forms of compositional scene representations from natural language descriptions. Unlike recent works, our method does NOT use Generative Adversarial Networks (GANs). Text2Scene instead learns to sequentially generate objects and their attributes (location, size, appearance, etc) at every time step by attending to different parts of the input text and the current status of the generated scene. We show that under minor modifications, the proposed framework can handle the generation of different forms of scene representations, including cartoon-like scenes, object layouts corresponding to real images, and synthetic images. Our method is not only competitive when compared with state-of-the-art GAN-based methods using automatic metrics and superior based on human judgments but also has the advantage of producing interpretable results.",0
"This study presents a novel approach called ""Text2Scene"" that generates compositional scenes directly from text descriptions. Our method leverages recent advances in generative models and scene understanding techniques to enable the creation of complex environments from simple written prompts. We demonstrate the versatility of our system by generating scenarios across different domains and tasks, including indoor layout design, outdoor urban environments, and even hypothetical sci-fi settings. Evaluation results show significant improvements over baseline methods on both objective metrics such as user study ratings. In conclusion, we believe Text2Scene has wide applicability for applications ranging from architectural visualization and computer animation to virtual reality content creation and gaming.",1
"Deep Learning (DL) has become a crucial technology for Artificial Intelligence (AI). It is a powerful technique to automatically extract high-level features from complex data which can be exploited for applications such as computer vision, natural language processing, cybersecurity, communications, and so on. For the particular case of computer vision, several algorithms like object detection in real time videos have been proposed and they work well on Desktop GPUs and distributed computing platforms. However these algorithms are still heavy for mobile and embedded visual applications. The rapid spreading of smart portable devices and the emerging 5G network are introducing new smart multimedia applications in mobile environments. As a consequence, the possibility of implementing deep neural networks to mobile environments has attracted a lot of researchers. This paper presents emerging deep learning acceleration techniques that can enable the delivery of real time visual recognition into the hands of end users, anytime and anywhere.",0
"This work presents deep learning acceleration techniques that enable real-time performance on mobile devices for vision applications. We focus on optimizing convolutional neural networks (CNNs) which are commonly used in computer vision tasks such as image classification, object detection, and semantic segmentation. Our approach uses a combination of model pruning, quantization, and hardware acceleration to achieve efficient inference on resource-constrained platforms. Experimental results show significant speedup and memory savings while maintaining high accuracy compared to full precision models. These improvements make it possible to run demanding vision algorithms directly on mobile phones without relying on cloud computing resources. As such, our work has important implications for enabling new classes of intelligent systems capable of running on edge devices.",1
"Figures, such as bar charts, pie charts, and line plots, are widely used to convey important information in a concise format. They are usually human-friendly but difficult for computers to process automatically. In this work, we investigate the problem of figure captioning where the goal is to automatically generate a natural language description of the figure. While natural image captioning has been studied extensively, figure captioning has received relatively little attention and remains a challenging problem. First, we introduce a new dataset for figure captioning, FigCAP, based on FigureQA. Second, we propose two novel attention mechanisms. To achieve accurate generation of labels in figures, we propose Label Maps Attention. To model the relations between figure labels, we propose Relation Maps Attention. Third, we use sequence-level training with reinforcement learning in order to directly optimizes evaluation metrics, which alleviates the exposure bias issue and further improves the models in generating long captions. Extensive experiments show that the proposed method outperforms the baselines, thus demonstrating a significant potential for the automatic captioning of vast repositories of figures.",0
"In recent years, automating image caption generation has become an active research area in computer vision due to its applications in diverse domains such as web search, content retrieval, and assistive technology. One approach towards achieving this goal is by using deep neural networks that learn from vast amounts of annotated data. However, current methods often lack reasoning capabilities which can lead to incorrect or incomplete descriptions. Furthermore, most approaches only focus on generating global feature representations rather than considering sequence-level training. This work proposes a novel model for figure captioning that utilizes reasoning techniques, specifically those based on logical formalisms like first-order logic, to achieve better accuracy. Additionally, our method employs recurrent units at both the encoder and decoder stages to incorporate temporal dependencies and promote local consistency within each generated caption. Experiments show significant improvements over baseline models across several evaluation metrics.",1
"The Transformer architecture is widely used in natural language processing. Despite its success, the design principle of the Transformer remains elusive. In this paper, we provide a novel perspective towards understanding the architecture: we show that the Transformer can be mathematically interpreted as a numerical Ordinary Differential Equation (ODE) solver for a convection-diffusion equation in a multi-particle dynamic system. In particular, how words in a sentence are abstracted into contexts by passing through the layers of the Transformer can be interpreted as approximating multiple particles' movement in the space using the Lie-Trotter splitting scheme and the Euler's method. Given this ODE's perspective, the rich literature of numerical analysis can be brought to guide us in designing effective structures beyond the Transformer. As an example, we propose to replace the Lie-Trotter splitting scheme by the Strang-Marchuk splitting scheme, a scheme that is more commonly used and with much lower local truncation errors. The Strang-Marchuk splitting scheme suggests that the self-attention and position-wise feed-forward network (FFN) sub-layers should not be treated equally. Instead, in each layer, two position-wise FFN sub-layers should be used, and the self-attention sub-layer is placed in between. This leads to a brand new architecture. Such an FFN-attention-FFN layer is ""Macaron-like"", and thus we call the network with this new architecture the Macaron Net. Through extensive experiments, we show that the Macaron Net is superior to the Transformer on both supervised and unsupervised learning tasks. The reproducible codes and pretrained models can be found at https://github.com/zhuohan123/macaron-net",0
"Abstract: Natural language processing (NLP) models have advanced significantly over the past few years thanks largely to deep learning methods such as transformers. These models can capture complex relationships among words and perform well on many tasks including translation, question answering and summarization. However, despite their successes, these NLP systems suffer from several shortcomings that hinder their wider adoption. In particular, they struggle to handle data sets containing outliers, noise or missing values which limits their real-world applicability. Our aim is therefore to study the limitations of current state-of-the-art natural language models by modelling them using dynamic multi-particle methods. We seek to introduce novel regularizations inspired by physical intuitions so that these models better fit noisy datasets without any loss in accuracy on clean data. Our contributions are threefold; first we develop theoretical foundations connecting NLP problems to dissipative particle dynamics thus allowing us to leverage tools from nonequilibrium statistical mechanics for developing new optimization algorithms tailored to each specific task. Secondly, based on our theory, we propose two general families of variational autoencoders (VAEs), one discrete and the other continuous, whose architecture encourages robustness against noise in both training and testing regimes. Finally, on six benchmark data sets, experiments validate that our VAEs trained via proposed schedules indeed improve upon previous arts while maintaining competitive performance across all metrics. By demonstrating advantages brought by dissipation we hope researchers will revisit existing theories/algorithms thus paving path towards universal artificial intelligence.",1
"With the maturity of visual detection techniques, we are more ambitious in describing visual content with open-vocabulary, fine-grained and free-form language, i.e., the task of image captioning. In particular, we are interested in generating longer, richer and more fine-grained sentences and paragraphs as image descriptions. Image captioning can be translated to the task of sequential language prediction given visual content, where the output sequence forms natural language description with plausible grammar. However, existing image captioning methods focus only on language policy while not visual policy, and thus fail to capture visual context that are crucial for compositional reasoning such as object relationships (e.g., ""man riding horse"") and visual comparisons (e.g., ""small(er) cat""). This issue is especially severe when generating longer sequences such as a paragraph. To fill the gap, we propose a Context-Aware Visual Policy network (CAVP) for fine-grained image-to-language generation: image sentence captioning and image paragraph captioning. During captioning, CAVP explicitly considers the previous visual attentions as context, and decides whether the context is used for the current word/sentence generation given the current visual attention. Compared against traditional visual attention mechanism that only fixes a single visual region at each step, CAVP can attend to complex visual compositions over time. The whole image captioning model -- CAVP and its subsequent language policy network -- can be efficiently optimized end-to-end by using an actor-critic policy gradient method. We have demonstrated the effectiveness of CAVP by state-of-the-art performances on MS-COCO and Stanford captioning datasets, using various metrics and sensible visualizations of qualitative visual context.",0
"In recent years, there has been significant progress in the development of computer vision algorithms that generate descriptive text from images. These algorithms rely on visual representations, such as features extracted by convolutional neural networks (CNNs), combined with natural language processing techniques to generate image descriptions. However, most existing methods treat all parts of an image equally, ignoring the context provided by the surrounding scene. This can lead to incomplete or irrelevant description if a salient object happens to occupy only a small portion of the image.  In order to address these shortcomings, we propose a novel approach called Context-Aware Visual Policy Network (CAVPN) that integrates both spatial and semantic attention mechanisms into a policy network framework. CAVPN dynamically selects informative regions within the given image and generates corresponding descriptions accordingly. Our model captures high-level relationships between image objects using a graph structure which enables effective reasoning over multiple dependencies across regions.  The effectiveness of our method is evaluated through experiments conducted on two benchmark datasets, MSCOCO and Flickr30K Entities, where CAVPN consistently outperforms state-of-the-art methods in terms of both quantitative metrics and human evaluation. Our ablation study further validates the contribution of each component in our model. Finally, we demonstrate that our trained model is capable of generating detailed and coherent descriptions even for highly complex scenes. Overall, our work takes a step towards more accurate fine-grained image captioning systems.",1
"Grounding natural language in images, such as localizing ""the black dog on the left of the tree"", is one of the core problems in artificial intelligence, as it needs to comprehend the fine-grained and compositional language space. However, existing solutions rely on the association between the holistic language features and visual features, while neglect the nature of compositional reasoning implied in the language. In this paper, we propose a natural language grounding model that can automatically compose a binary tree structure for parsing the language and then perform visual reasoning along the tree in a bottom-up fashion. We call our model RVG-TREE: Recursive Grounding Tree, which is inspired by the intuition that any language expression can be recursively decomposed into two constituent parts, and the grounding confidence score can be recursively accumulated by calculating their grounding scores returned by sub-trees. RVG-TREE can be trained end-to-end by using the Straight-Through Gumbel-Softmax estimator that allows the gradients from the continuous score functions passing through the discrete tree construction. Experiments on several benchmarks show that our model achieves the state-of-the-art performance with more explainable reasoning.",0
"Incorporating natural language understanding into artificial intelligence (AI) systems has become increasingly important as they interact with humans more frequently. One key aspect of human communication that current AI models struggle with is reasoning about relationships among multiple entities within a conversation. To address this issue, we propose a novel approach called LT-ReGround which learns to compose and reason with language tree structures for visual grounding. Our method extends the notion of traditional phrase-based representations by modeling interactions between phrases within a hierarchical structure known as a linguistic landscape. By doing so, our model can capture richer semantic connections between concepts and better contextualize them in the environment. We evaluate our approach on several benchmark datasets related to text-to-image retrieval tasks such as COCO and VGQA. Experimental results show significant improvements over state-of-the-art methods across all metrics, demonstrating the effectiveness and robustness of our proposed framework. Overall, our work represents a step forward towards developing AI agents capable of truly understanding and engaging in complex conversations with humans.",1
"In this work, we propose a goal-driven collaborative task that combines language, perception, and action. Specifically, we develop a Collaborative image-Drawing game between two agents, called CoDraw. Our game is grounded in a virtual world that contains movable clip art objects. The game involves two players: a Teller and a Drawer. The Teller sees an abstract scene containing multiple clip art pieces in a semantically meaningful configuration, while the Drawer tries to reconstruct the scene on an empty canvas using available clip art pieces. The two players communicate with each other using natural language. We collect the CoDraw dataset of ~10K dialogs consisting of ~138K messages exchanged between human players. We define protocols and metrics to evaluate learned agents in this testbed, highlighting the need for a novel ""crosstalk"" evaluation condition which pairs agents trained independently on disjoint subsets of the training data. We present models for our task and benchmark them using both fully automated evaluation and by having them play the game live with humans.",0
"As collaborative drawing has become increasingly popular due to advancements in technologies like pen displays, touch screens, and VR/AR devices, the need for efficient communication strategies during such activities has grown substantially. This paper presents CoDraw (Collaborative Drawing), which enables multiple participants to draw together on a shared digital canvas using natural language commands. By integrating both visual output generation and goal identification into one framework, CoDraw serves as a test bed to explore how grounded, task-oriented interaction can enable multi-party collaboration and foster more effective exchanges in real-world settings. Through qualitative analysis, we demonstrate that grounded, collaborative interactions enabled by CoDraw align with cognitive science literature and provide insights on design principles for multimodal human-AI systems.",1
"Exploiting relationships among objects has achieved remarkable progress in interpreting images or videos by natural language. Most existing methods resort to first detecting objects and their relationships, and then generating textual descriptions, which heavily depends on pre-trained detectors and leads to performance drop when facing problems of heavy occlusion, tiny-size objects and long-tail in object detection. In addition, the separate procedure of detecting and captioning results in semantic inconsistency between the pre-defined object/relation categories and the target lexical words. We exploit prior human commonsense knowledge for reasoning relationships between objects without any pre-trained detectors and reaching semantic coherency within one image or video in captioning. The prior knowledge (e.g., in the form of knowledge graph) provides commonsense semantic correlation and constraint between objects that are not explicit in the image and video, serving as useful guidance to build semantic graph for sentence generation. Particularly, we present a joint reasoning method that incorporates 1) commonsense reasoning for embedding image or video regions into semantic space to build semantic graph and 2) relational reasoning for encoding semantic graph to generate sentences. Extensive experiments on the MS-COCO image captioning benchmark and the MSVD video captioning benchmark validate the superiority of our method on leveraging prior commonsense knowledge to enhance relational reasoning for visual captioning.",0
"In this paper, we present a novel approach to visual captioning that utilizes relational reasoning based on prior knowledge. We introduce a framework that combines bottom-up processing of image features with top-down processing of semantic representations from external resources such as WordNet and ConceptNet. Our approach allows us to capture rich relationships between objects and concepts within the scene, enabling our model to generate more accurate and informative captions compared to previous methods that rely solely on local feature extraction. To evaluate the effectiveness of our method, we conduct experiments on two benchmark datasets and demonstrate significant improvements over state-of-the-art approaches across multiple evaluation metrics, including METEOR, CIDEr, SPICE, and ROUGE-L. Overall, our results showcase the potential of integrating prior knowledge into visual representation learning frameworks to enable advanced capabilities like relational reasoning in computer vision tasks.",1
"In recent years, transfer learning gained particular interest in the field of vision and natural language processing. In the research field of vision, e.g., deep neural networks and transfer learning techniques achieve almost perfect classification scores within minutes. Nonetheless, these techniques are not yet widely applied in other domains. Therefore, this article identifies critical challenges and shows potential solutions for power forecasts in the field of renewable energies. It proposes a framework utilizing transfer learning techniques in wind power forecasts with limited or no historical data. On the one hand, this allows evaluating the applicability of transfer learning in the field of renewable energy. On the other hand, by developing automatic procedures, we assure that the proposed methods provide a framework that applies to domains in organic computing as well.",0
"In recent years, transfer learning has emerged as a promising approach in the field of renewable energies, particularly for wind farms that require accurate power forecasts throughout their lifecycle. This framework introduces a novel methodology for providing these forecasts by leveraging existing datasets from connected wind farms, enabling further improvements in accuracy without requiring additional data collection. We present a detailed analysis of the proposed architecture, including extensive evaluation on real-world datasets demonstrating its effectiveness. Ultimately, our results indicate that our transfer learning model can significantly enhance the reliability of power prediction, leading to improved decision making for both operators and grid managers alike.",1
"In this paper, the problem of describing visual contents of a video sequence with natural language is addressed. Unlike previous video captioning work mainly exploiting the cues of video contents to make a language description, we propose a reconstruction network (RecNet) in a novel encoder-decoder-reconstructor architecture, which leverages both forward (video to sentence) and backward (sentence to video) flows for video captioning. Specifically, the encoder-decoder component makes use of the forward flow to produce a sentence description based on the encoded video semantic features. Two types of reconstructors are subsequently proposed to employ the backward flow and reproduce the video features from local and global perspectives, respectively, capitalizing on the hidden state sequence generated by the decoder. Moreover, in order to make a comprehensive reconstruction of the video features, we propose to fuse the two types of reconstructors together. The generation loss yielded by the encoder-decoder component and the reconstruction loss introduced by the reconstructor are jointly cast into training the proposed RecNet in an end-to-end fashion. Furthermore, the RecNet is fine-tuned by CIDEr optimization via reinforcement learning, which significantly boosts the captioning performance. Experimental results on benchmark datasets demonstrate that the proposed reconstructor can boost the performance of video captioning consistently.",0
"Automatic video captioning has been shown to improve accessibility for deaf viewers, but current algorithms rely on pretrained models that lack adaptability to new domains. In order to overcome these limitations, we propose using reinforcement learning (RL) to optimize parameters for domain adaptation while reconstructing raw frames from compressed video data. By doing so, our algorithm learns how to generate high quality representations directly from low resolution inputs. We evaluate our model on a large dataset of diverse videos and compare its performance against both traditional methods and other deep learning based approaches. Our results demonstrate significant improvements in accuracy over baseline methods without requiring any additional labeled training data. These findings have important implications for the development of more accessible and efficient video captioning systems.",1
"Deep neural networks (DNNs) have recently achieved state-of-the-art performance and provide significant progress in many machine learning tasks, such as image classification, speech processing, natural language processing, etc. However, recent studies have shown that DNNs are vulnerable to adversarial attacks. For instance, in the image classification domain, adding small imperceptible perturbations to the input image is sufficient to fool the DNN and to cause misclassification. The perturbed image, called \textit{adversarial example}, should be visually as close as possible to the original image. However, all the works proposed in the literature for generating adversarial examples have used the $L_{p}$ norms ($L_{0}$, $L_{2}$ and $L_{\infty}$) as distance metrics to quantify the similarity between the original image and the adversarial example. Nonetheless, the $L_{p}$ norms do not correlate with human judgment, making them not suitable to reliably assess the perceptual similarity/fidelity of adversarial examples. In this paper, we present a database for visual fidelity assessment of adversarial examples. We describe the creation of the database and evaluate the performance of fifteen state-of-the-art full-reference (FR) image fidelity assessment metrics that could substitute $L_{p}$ norms. The database as well as subjective scores are publicly available to help designing new metrics for adversarial examples and to facilitate future research works.",0
"In recent years, there has been significant progress in developing deep learning models for image classification tasks, such as Convolutional Neural Networks (CNN). However, these models remain vulnerable to adversarial attacks that can cause them to make incorrect predictions even though they appear visually indistinguishable from correctly classified images. This study aimed to evaluate the effectiveness of several popular adversarial attack methods on different versions of the popular AlexNet architecture using perception studies and traditional evaluation metrics. Results showed a high level of success for the attacks across all versions of the model and revealed interesting patterns related to human perception and robustness training. These findings highlight the importance of considering both quantitative measures and human interpretation when evaluating models in the field of computer vision.",1
"We consider the problem of learning from sparse and underspecified rewards, where an agent receives a complex input, such as a natural language instruction, and needs to generate a complex response, such as an action sequence, while only receiving binary success-failure feedback. Such success-failure rewards are often underspecified: they do not distinguish between purposeful and accidental success. Generalization from underspecified rewards hinges on discounting spurious trajectories that attain accidental success, while learning from sparse feedback requires effective exploration. We address exploration by using a mode covering direction of KL divergence to collect a diverse set of successful trajectories, followed by a mode seeking KL divergence to train a robust policy. We propose Meta Reward Learning (MeRL) to construct an auxiliary reward function that provides more refined feedback for learning. The parameters of the auxiliary reward function are optimized with respect to the validation performance of a trained policy. The MeRL approach outperforms our alternative reward learning technique based on Bayesian Optimization, and achieves the state-of-the-art on weakly-supervised semantic parsing. It improves previous work by 1.2% and 2.4% on WikiTableQuestions and WikiSQL datasets respectively.",0
"This paper investigates how machine learning algorithms can learn from sparse and underspecified rewards, which occur frequently in real world applications such as robotics and natural language processing. We present two approaches that tackle this problem: semi-supervised reinforcement learning (SSL) and reward model transfer (RMT). SSL uses unlabeled data collected by a random policy during exploration phase to improve generalization performance using a novel intrinsic curiosity regularizer. RMT learns to predict the expected reward based on previous interactions with similar tasks. Our experiments show that both methods significantly outperform baseline models trained without any additional training signals or information, demonstrating their effectiveness in handling sparsity and underspecification in the environment. Finally, we discuss some limitations of our work and suggest potential directions for future research.",1
"Recent reinforcement learning (RL) approaches have shown strong performance in complex domains such as Atari games, but are often highly sample inefficient. A common approach to reduce interaction time with the environment is to use reward shaping, which involves carefully designing reward functions that provide the agent intermediate rewards for progress towards the goal. However, designing appropriate shaping rewards is known to be difficult as well as time-consuming. In this work, we address this problem by using natural language instructions to perform reward shaping. We propose the LanguagE-Action Reward Network (LEARN), a framework that maps free-form natural language instructions to intermediate rewards based on actions taken by the agent. These intermediate language-based rewards can seamlessly be integrated into any standard reinforcement learning algorithm. We experiment with Montezuma's Revenge from the Atari Learning Environment, a popular benchmark in RL. Our experiments on a diverse set of 15 tasks demonstrate that, for the same number of interactions with the environment, language-based rewards lead to successful completion of the task 60% more often on average, compared to learning without language.",0
"Title: ""Natural Language For Reward Shaping""  Reworded abstract: This research explores the potential benefits of natural language in reward shaping for reinforcement learning agents. By using human feedback through language, we aim to improve the efficiency and performance of these agents. Our approach involves integrating natural language processing techniques into traditional reinforcement learning algorithms to enable agents to better interpret and respond to user instructions. We evaluate our method on a range of tasks and demonstrate that incorporating natural language significantly enhances agent performance while maintaining low sample complexity. This work has important implications for developing more intelligent and adaptive artificial intelligence systems that can communicate effectively with humans. Overall, this study contributes new insights into the intersection of natural language processing and reinforcement learning.",1
"Inspired by the recent successes of deep learning on Computer Vision and Natural Language Processing, we present a deep learning approach for recognizing scanned receipts. The recognition system has two main modules: text detection based on Connectionist Text Proposal Network and text recognition based on Attention-based Encoder-Decoder. We also proposed pre-processing to extract receipt area and OCR verification to ignore handwriting. The experiments on the dataset of the Robust Reading Challenge on Scanned Receipts OCR and Information Extraction 2019 demonstrate that the accuracies were improved by integrating the pre-processing and the OCR verification. Our recognition system achieved 71.9% of the F1 score for detection and recognition task.",0
"In recent years, deep learning has emerged as one of the most promising techniques for image recognition tasks such as receipt processing. This is due to the ability of convolutional neural networks (CNNs) to learn complex representations from large amounts of data without requiring extensive feature engineering. To address the challenges associated with receipt recognition, we propose a novel approach that combines traditional computer vision features with CNNs trained on large datasets of synthetic receipts generated using a wide range of fonts, colors, backgrounds and styles. Our method first applies several preprocessing steps to improve image quality, remove noise and convert grayscale images to color ones before feeding them into the network. We then use a multi-stage architecture comprising multiple CNN layers followed by a fully connected layer to generate the final output – which is either a text classification or bounding box regression depending on the task at hand. Experimental results demonstrate significant improvements over previous state-of-the art methods for both benchmarked and real-world datasets. Our work represents an important step towards enabling accurate automated receipt analysis and demonstrates the potential of deep learning approaches for addressing complex document processing problems.",1
"Vision-to-language tasks aim to integrate computer vision and natural language processing together, which has attracted the attention of many researchers. For typical approaches, they encode image into feature representations and decode it into natural language sentences. While they neglect high-level semantic concepts and subtle relationships between image regions and natural language elements. To make full use of these information, this paper attempt to exploit the text guided attention and semantic-guided attention (SA) to find the more correlated spatial information and reduce the semantic gap between vision and language. Our method includes two level attention networks. One is the text-guided attention network which is used to select the text-related regions. The other is SA network which is used to highlight the concept-related regions and the region-related concepts. At last, all these information are incorporated to generate captions or answers. Practically, image captioning and visual question answering experiments have been carried out, and the experimental results have shown the excellent performance of the proposed approach.",0
"In recent years, there has been significant progress in developing artificial intelligence (AI) systems that can generate human-like responses based on visual input, such as images and videos. These ""vision-to-language"" tasks have a wide range of potential applications, from generating descriptive captions for images to creating narrative summaries of video clips. However, existing approaches often struggle with handling complex scenes and accurately identifying relevant details. To address these challenges, we propose using attributes and attention mechanisms to improve the performance of vision-to-language models. We evaluate our approach through several experiments and show that it leads to more accurate and informative text generation compared to state-of-the-art methods. Our work represents an important step forward towards enabling AI systems to effectively process and interpret visual content.",1
"How can local-search methods such as stochastic gradient descent (SGD) avoid bad local minima in training multi-layer neural networks? Why can they fit random labels even given non-convex and non-smooth architectures? Most existing theory only covers networks with one hidden layer, so can we go deeper?   In this paper, we focus on recurrent neural networks (RNNs) which are multi-layer networks widely used in natural language processing. They are harder to analyze than feedforward neural networks, because the $\textit{same}$ recurrent unit is repeatedly applied across the entire time horizon of length $L$, which is analogous to feedforward networks of depth $L$. We show when the number of neurons is sufficiently large, meaning polynomial in the training data size and in $L$, then SGD is capable of minimizing the regression loss in the linear convergence rate. This gives theoretical evidence of how RNNs can memorize data.   More importantly, in this paper we build general toolkits to analyze multi-layer networks with ReLU activations. For instance, we prove why ReLU activations can prevent exponential gradient explosion or vanishing, and build a perturbation theory to analyze first-order approximation of multi-layer networks.",0
"""The convergence rate of recurrent neural networks (RNNs) has been an area of active research due to their importance in applications such as natural language processing and time series forecasting. In recent years, advancements in training techniques have improved the performance of RNNs on many tasks. However, there still exists a lack of understanding regarding the factors that affect the convergence rate of these models. This paper investigates the relationship between several key parameters and their impact on the convergence rate of RNNs during training. By analyzing different combinations of hyperparameters through extensive experiments, we provide insights into which configurations lead to faster convergence rates and better generalization. Our findings contribute to the field by providing a more comprehensive understanding of how to optimize RNN training and improve their performance.""",1
"Recent breakthroughs in the field of deep learning have led to advancements in a broad spectrum of tasks in computer vision, audio processing, natural language processing and other areas. In most instances where these tasks are deployed in real-world scenarios, the models used in them have been shown to be susceptible to adversarial attacks, making it imperative for us to address the challenge of their adversarial robustness. Existing techniques for adversarial robustness fall into three broad categories: defensive distillation techniques, adversarial training techniques, and randomized or non-deterministic model based techniques. In this paper, we propose a novel neural network paradigm that falls under the category of randomized models for adversarial robustness, but differs from all existing techniques under this category in that it models each parameter of the network as a statistical distribution with learnable parameters. We show experimentally that this framework is highly robust to a variety of white-box and black-box adversarial attacks, while preserving the task-specific performance of the traditional neural network model.",0
"Abstract: ""Neural networks have revolutionized artificial intelligence by providing unprecedented results across numerous tasks such as image recognition, natural language processing and decision making systems. One challenge faced by these models however is their vulnerability to adversarial attacks where slight modifications to inputs can result in incorrect predictions, which can have severe consequences particularly in safety critical applications like autonomous driving.""",1
"Language grounded image understanding tasks have often been proposed as a method for evaluating progress in artificial intelligence. Ideally, these tasks should test a plethora of capabilities that integrate computer vision, reasoning, and natural language understanding. However, rather than behaving as visual Turing tests, recent studies have demonstrated state-of-the-art systems are achieving good performance through flaws in datasets and evaluation procedures. We review the current state of affairs and outline a path forward.",0
"One of the most exciting areas of artificial intelligence (AI) research today involves developing models that can effectively process both visual inputs and language data. This task, known as vision and language research, has gained significant attention due to its potential applications in fields such as computer vision, natural language processing, and human-computer interaction. However, there remain several challenges that must be overcome before these models can achieve widespread adoption and use. In this work, we examine some of the key challenges facing researchers working on vision and language problems and discuss possible solutions to address them. We also explore the prospects of using these models for real-world applications and outline future directions for this rapidly evolving field. By understanding the current limitations and opportunities available in this area of study, researchers can better develop innovative approaches to tackle complex vision and language tasks. Ultimately, our goal is to provide insights into how recent advances in computer science and engineering can be used to enhance existing techniques and create more effective systems for processing visual and linguistic information.",1
"Attention mechanisms and non-local mean operations in general are key ingredients in many state-of-the-art deep learning techniques. In particular, the Transformer model based on multi-head self-attention has recently achieved great success in natural language processing and computer vision. However, the vanilla algorithm computing the Transformer of an image with n pixels has O(n^2) complexity, which is often painfully slow and sometimes prohibitively expensive for large-scale image data. In this paper, we propose a fast randomized algorithm --- SCRAM --- that only requires O(n log(n)) time to produce an image attention map. Such a dramatic acceleration is attributed to our insight that attention maps on real-world images usually exhibit (1) spatial coherence and (2) sparse structure. The central idea of SCRAM is to employ PatchMatch, a randomized correspondence algorithm, to quickly pinpoint the most compatible key (argmax) for each query first, and then exploit that knowledge to design a sparse approximation to non-local mean operations. Using the argmax (mode) to dynamically construct the sparse approximation distinguishes our algorithm from all of the existing sparse approximate methods and makes it very efficient. Moreover, SCRAM is a broadly applicable approximation to any non-local mean layer in contrast to some other sparse approximations that can only approximate self-attention. Our preliminary experimental results suggest that SCRAM is indeed promising for speeding up or scaling up the computation of attention maps in the Transformer.",0
"In recent years, deep learning has revolutionized image understanding tasks by leveraging convolutional neural networks (CNNs). While these models achieve state-of-the-art results on numerous benchmarks, they rely heavily on dense grids of spatially fixed kernels that lack robustness against input transformations such as scaling, translation or rotation. To address this issue, we present a novel approach called ""SCRAM"" that randomizes the attention maps of CNNs at test time. Our method introduces continuous sampling from sets of attentive regions at each layer through a lightweight network. We find that this method leads to better generalization across different scales, orientations and illuminations while still maintaining good accuracy on clean inputs. Additionally, our approach runs efficiently both in terms of computational requirements and inference speed, making it suitable for deployment on mobile devices. We demonstrate the effectiveness of SCRAM on several challenging image classification datasets including ImageNet, PASCAL VOC and COCO, achieving competitive performance compared to other methods for spatial pyramid dilated convolutions. These promising results pave the way towards improving the resilience of modern visual recognition systems to natural variations encountered in real world data distributions.",1
"Standard methods in deep learning for natural language processing fail to capture the compositional structure of human language that allows for systematic generalization outside of the training distribution. However, human learners readily generalize in this way, e.g. by applying known grammatical rules to novel words. Inspired by work in neuroscience suggesting separate brain systems for syntactic and semantic processing, we implement a modification to standard approaches in neural machine translation, imposing an analogous separation. The novel model, which we call Syntactic Attention, substantially outperforms standard methods in deep learning on the SCAN dataset, a compositional generalization task, without any hand-engineered features or additional supervision. Our work suggests that separating syntactic from semantic learning may be a useful heuristic for capturing compositional structure.",0
"This paper presents an approach for enabling compositional generalization in deep sequence-to-sequence models by decoupling the learning of semantic relationships from that of syntactic dependencies. We demonstrate that our method leads to significant improvements on a challenging text generation task. By doing so, we provide insights into how these techniques may be applied more broadly across NLP domains. ---  (Note: This is meant to just give me some ideas of possible ways to organize my thoughts. I am looking for alternative perspectives.)",1
"In recent years, deep learning has achieved remarkable achievements in many fields, including computer vision, natural language processing, speech recognition and others. Adequate training data is the key to ensure the effectiveness of the deep models. However, obtaining valid data requires a lot of time and labor resources. Data augmentation (DA) is an effective alternative approach, which can generate new labeled data based on existing data using label-preserving transformations. Although we can benefit a lot from DA, designing appropriate DA policies requires a lot of expert experience and time consumption, and the evaluation of searching the optimal policies is costly. So we raise a new question in this paper: how to achieve automated data augmentation at as low cost as possible? We propose a method named BO-Aug for automating the process by finding the optimal DA policies using the Bayesian optimization approach. Our method can find the optimal policies at a relatively low search cost, and the searched policies based on a specific dataset are transferable across different neural network architectures or even different datasets. We validate the BO-Aug on three widely used image classification datasets, including CIFAR-10, CIFAR-100 and SVHN. Experimental results show that the proposed method can achieve state-of-the-art or near advanced classification accuracy. Code to reproduce our experiments is available at https://github.com/zhangxiaozao/BO-Aug.",0
"Abstract: In the field of image classification tasks, data augmentation has been shown to improve performance on many benchmark datasets by increasing the size of available training data. Selecting the most effective set of data augmentation operations can vary greatly depending on the task at hand; however, there exists little guidance on how to choose such operations. To solve this problem, we propose using Bayesian optimization (BO) as a means to automate the search over all possible combinations of data augmentations. Our approach first uses BO to learn which data augmentation policies provide optimal improvement compared to random chance and then trains models based on these learned policies. We demonstrate that our method achieves state-of-the-art results across multiple benchmarks including CIFAR-10/100 and SVHN, surpassing even expert human selection of data augmentation operations. These results highlight the potential value of learning optimal data augmentation policies during model training for improving generalization accuracy. -----",1
"In this work we present a new efficient approach to Human Action Recognition called Video Transformer Network (VTN). It leverages the latest advances in Computer Vision and Natural Language Processing and applies them to video understanding. The proposed method allows us to create lightweight CNN models that achieve high accuracy and real-time speed using just an RGB mono camera and general purpose CPU. Furthermore, we explain how to improve accuracy by distilling from multiple models with different modalities into a single model. We conduct a comparison with state-of-the-art methods and show that our approach performs on par with most of them on famous Action Recognition datasets. We benchmark the inference time of the models using the modern inference framework and argue that our approach compares favorably with other methods in terms of speed/accuracy trade-off, running at 56 FPS on CPU. The models and the training code are available.",0
"This study presents a lightweight network architecture designed for real-time action recognition on resource-constrained devices such as smartphones and embedded systems. Our model utilizes efficient techniques such as channel pruning, quantization, and knowledge distillation to achieve high performance while reducing computational requirements, making it suitable for deployment on low-power hardware. We evaluate our model using several benchmark datasets and demonstrate that our approach achieves comparable accuracy to state-of-the-art models at significantly lower inference time and memory usage. In addition, we analyze the impact of each component in our model on performance and provide insights into how these components can be tuned to optimize tradeoffs between accuracy and efficiency. Overall, our work shows that it is possible to design effective action recognition algorithms that can run on a wide range of platforms without sacrificing quality or functionality.",1
"Text generation is an important Natural Language Processing task with various applications. Although several metrics have already been introduced to evaluate the text generation methods, each of them has its own shortcomings. The most widely used metrics such as BLEU only consider the quality of generated sentences and neglect their diversity. For example, repeatedly generation of only one high quality sentence would result in a high BLEU score. On the other hand, the more recent metric introduced to evaluate the diversity of generated texts known as Self-BLEU ignores the quality of generated texts. In this paper, we propose metrics to evaluate both the quality and diversity simultaneously by approximating the distance of the learned generative model and the real data distribution. For this purpose, we first introduce a metric that approximates this distance using n-gram based measures. Then, a feature-based measure which is based on a recent highly deep model trained on a large text corpus called BERT is introduced. Finally, for oracle training mode in which the generator's density can also be calculated, we propose to use the distance measures between the corresponding explicit distributions. Eventually, the most popular and recent text generation models are evaluated using both the existing and the proposed metrics and the preferences of the proposed metrics are determined.",0
"As natural language processing models continue to advance in their abilities, there is a growing need for evaluating text generation systems that produce diverse outputs while still maintaining high quality. In recent years, researchers have proposed several evaluation metrics for measuring diversity in generated text, but these often come at the expense of sacrificing quality. Likewise, traditional measures of quality can overlook important aspects of diversity. To address this gap, we propose a joint metric that combines both diversity and quality into a single score. Our method uses a deep learning model trained on human judgments of diversity and quality, enabling us to accurately evaluate text generators across different domains and tasks. We experiment with our metric on four large-scale datasets spanning three application areas: machine translation, summarization, and question answering. Results show consistent correlation with human judgment and demonstrate how our approach improves upon existing methods for evaluating text generation systems. By providing a more complete picture of system performance, our work offers insights into design choices and future directions for developing improved text generation models. Ultimately, this work has significant implications for many NLP applications where generating diverse, high-quality responses is critical, such as conversational agents and personalized content generation.",1
"Self-attention (SA) mechanisms can capture effectively global dependencies in deep neural networks, and have been applied to natural language processing and image processing successfully. However, SA modules for image reconstruction have high time and space complexity, which restrict their applications to higher-resolution images. In this paper, we refine the SA module in self-attention generative adversarial networks (SAGAN) via adapting a non-local operation, revising the connectivity among the units in SA module and re-implementing its computational pattern, such that its time and space complexity is reduced from $\text{O}(n^2)$ to $\text{O}(n)$, but it is still equivalent to the original SA module. Further, we explore the principles behind the module and discover that our module is a special kind of channel attention mechanisms. Experimental results based on two benchmark datasets of image reconstruction, verify that under the same computational environment, two models can achieve comparable effectiveness for image reconstruction, but the proposed one runs faster and takes up less memory space.",0
"In recent years, self-attention modules have emerged as powerful tools for image reconstruction tasks. These models can effectively capture global dependencies among features and achieve state-of-the-art performance on many benchmarks. However, these models often require large amounts of memory during training and inference, which can make them difficult to deploy on limited hardware resources. Additionally, they may suffer from slow convergence due to the high computational cost of attention operations. To address these limitations, we propose a refined version of the self-attention module that requires less memory while still maintaining high accuracy. Our approach uses a dynamic graph algorithm to selectively attend only to relevant spatial locations at each layer, reducing the computational complexity and memory footprint of traditional self-attention mechanisms. We show experimentally that our method significantly improves upon baseline architectures across several benchmark datasets, achieving faster speed with comparable accuracy. This work demonstrates the potential for more efficient deployment of self-attention networks in real-world applications by balancing model size, computational cost, and performance.",1
"Image captioning aims to automatically generate a natural language description of a given image, and most state-of-the-art models have adopted an encoder-decoder framework. The framework consists of a convolution neural network (CNN)-based image encoder that extracts region-based visual features from the input image, and an recurrent neural network (RNN)-based caption decoder that generates the output caption words based on the visual features with the attention mechanism. Despite the success of existing studies, current methods only model the co-attention that characterizes the inter-modal interactions while neglecting the self-attention that characterizes the intra-modal interactions. Inspired by the success of the Transformer model in machine translation, here we extend it to a Multimodal Transformer (MT) model for image captioning. Compared to existing image captioning approaches, the MT model simultaneously captures intra- and inter-modal interactions in a unified attention block. Due to the in-depth modular composition of such attention blocks, the MT model can perform complex multimodal reasoning and output accurate captions. Moreover, to further improve the image captioning performance, multi-view visual features are seamlessly introduced into the MT model. We quantitatively and qualitatively evaluate our approach using the benchmark MSCOCO image captioning dataset and conduct extensive ablation studies to investigate the reasons behind its effectiveness. The experimental results show that our method significantly outperforms the previous state-of-the-art methods. With an ensemble of seven models, our solution ranks the 1st place on the real-time leaderboard of the MSCOCO image captioning challenge at the time of the writing of this paper.",0
"This paper presents a new approach to image captioning that leverages multimodality and multi-view visual representation using transformer architecture. We propose a novel model named MTMTVR (MultimodalTransformerwithMultipleViewRepresentation) which integrates multiple views of images obtained from different modalities such as textual descriptions, attributes, and relationships along with the visual features extracted by convolutional neural networks (CNNs). Our approach utilizes pre-training on each view separately followed by fusing them into a unified framework. Experimental results show significant improvements over existing approaches on two benchmark datasets: COCO (Common Objects in Context) dataset and MSCOCO (Microsoft Common Objects in Context) dataset. Furthermore, we analyze our method's robustness by testing on various subsets, including instances lacking textual annotations or with high background complexity. In summary, this work demonstrates the effectiveness of incorporating multimodal information into single models to achieve enhanced performance in complex natural language processing tasks like image captioning.",1
"Deep Neural Networks have achieved remarkable success in computer vision, natural language processing, and audio tasks.",0
"In recent years, adversarial attacks have become increasingly prevalent in the field of deep learning. These attacks aim to fool machine learning models by adding small perturbations to inputs, causing incorrect predictions despite high confidence from the model. To address these issues, we propose the use of adaptive gradient methods for generating adversarial perturbations. Our method uses gradient descent to iteratively update input samples until they yield misclassification from the targeted model. We demonstrate that our approach outperforms existing state-of-the-art methods in terms of effectiveness and efficiency on both black-box and white-box attack settings. Furthermore, we provide theoretical analysis on why adaptive gradient techniques can effectively solve the optimization problems underlying adversarial examples generation, and how the choice of step size can affect the process. This work has important implications for improving the robustness of machine learning systems against such threats.",1
"Machine learning models that take computer program source code as input typically use Natural Language Processing (NLP) techniques. However, a major challenge is that code is written using an open, rapidly changing vocabulary due to, e.g., the coinage of new variable and method names. Reasoning over such a vocabulary is not something for which most NLP methods are designed. We introduce a Graph-Structured Cache to address this problem; this cache contains a node for each new word the model encounters with edges connecting each word to its occurrences in the code. We find that combining this graph-structured cache strategy with recent Graph-Neural-Network-based models for supervised learning on code improves the models' performance on a code completion task and a variable naming task --- with over $100\%$ relative improvement on the latter --- at the cost of a moderate increase in computation time.",0
"This paper presents a novel approach to learning open vocabularies from source code using a graph-structured cache. We propose a model that leverages natural language processing techniques and a graph representation to store and retrieve relevant tokens in real time as developers write code. Our system builds upon prior work in code search and completion by expanding the types of content that can be searched and completing queries more accurately. Furthermore, our method employs a ranking algorithm to provide results ordered by relevance rather than simply suggesting all possible matches. Experimental evaluation demonstrates the effectiveness of our approach, outperforming state-of-the-art methods across multiple metrics.",1
"This research strives for natural language moment retrieval in long, untrimmed video streams. The problem is not trivial especially when a video contains multiple moments of interests and the language describes complex temporal dependencies, which often happens in real scenarios. We identify two crucial challenges: semantic misalignment and structural misalignment. However, existing approaches treat different moments separately and do not explicitly model complex moment-wise temporal relations. In this paper, we present Moment Alignment Network (MAN), a novel framework that unifies the candidate moment encoding and temporal structural reasoning in a single-shot feed-forward network. MAN naturally assigns candidate moment representations aligned with language semantics over different temporal locations and scales. Most importantly, we propose to explicitly model moment-wise temporal relations as a structured graph and devise an iterative graph adjustment network to jointly learn the best structure in an end-to-end manner. We evaluate the proposed approach on two challenging public benchmarks DiDeMo and Charades-STA, where our MAN significantly outperforms the state-of-the-art by a large margin.",0
"In this study we present MAN (Moment Alignment Network), a novel approach for natural language moment retrieval that uses an iterative graph adjustment process to align textual inputs to moments in videos. Our method leverages pretrained language models and graph algorithms to learn temporal contexts and model complex relationships among texts, images and moments. We show through extensive experiments on two large scale video benchmark datasets, ActivityNet and Charades, that our method outperforms state-of-the-art approaches by significant margins under both clip level accuracy and NDCG evaluation metrics. Additionally, we perform ablation studies and visualizations to demonstrate the effectiveness of each component in MAN. Overall, MAN offers valuable insights into how language can guide efficient search for specific events within larger volumes of unstructured video data, providing greater efficiency for downstream applications like content management systems, recommendation engines, education platforms and surveillance systems.",1
"Neural networks (NN) are considered as black-boxes due to the lack of explainability and transparency of their decisions. This significantly hampers their deployment in environments where explainability is essential along with the accuracy of the system. Recently, significant efforts have been made for the interpretability of these deep networks with the aim to open up the black-box. However, most of these approaches are specifically developed for visual modalities. In addition, the interpretations provided by these systems require expert knowledge and understanding for intelligibility. This indicates a vital gap between the explainability provided by the systems and the novice user. To bridge this gap, we present a novel framework i.e. Time-Series eXplanation (TSXplain) system which produces a natural language based explanation of the decision taken by a NN. It uses the extracted statistical features to describe the decision of a NN, merging the deep learning world with that of statistics. The two-level explanation provides ample description of the decision made by the network to aid an expert as well as a novice user alike. Our survey and reliability assessment test confirm that the generated explanations are meaningful and correct. We believe that generating natural language based descriptions of the network's decisions is a big step towards opening up the black-box.",0
"Title: Explaining Neural Network Decisions for Time Series Analysis Using Natural Language and Statistical Techniques  The analysis of time series data has become increasingly important as there is a growing need for accurate predictions and decision making based on past trends. With the rise of deep neural networks (DNNs), the accuracy of these models has improved significantly, but their interpretability remains a challenge. In many cases, the decisions made by such models are difficult to explain and understand, leading to concerns about transparency and accountability.  To address this issue, we propose a novel approach called TSXplain that combines natural language processing techniques and statistical features to explain the reasoning behind DNN decisions for time series data. Our method employs an attribution system based on gradient-based methods to identify the most influential parts of the input data. These results are then transformed into human-readable descriptions using NLP techniques, providing insights into why the model made certain predictions. We evaluate our approach on several real-world datasets and demonstrate its effectiveness in explaining complex patterns and relationships present in the data.  Our work contributes to demystifying DNN decision-making processes for time series analysis, which can increase user trust and enable more informed decision making. By bridging the gap between technical representations and human understanding, we hope to create a new generation of interpretable machine learning algorithms that support better-informed decision-making. Overall, our study underscores the importance of explanations in building reliable artificial intelligence systems that integrate seamlessly into society while maintaining transparency and accountability.",1
"Click through rate (CTR) estimation is a fundamental task in personalized advertising and recommender systems. Recent years have witnessed the success of both the deep learning based model and attention mechanism in various tasks in computer vision (CV) and natural language processing (NLP). How to combine the attention mechanism with deep CTR model is a promising direction because it may ensemble the advantages of both sides. Although some CTR model such as Attentional Factorization Machine (AFM) has been proposed to model the weight of second order interaction features, we posit the evaluation of feature importance before explicit feature interaction procedure is also important for CTR prediction tasks because the model can learn to selectively highlight the informative features and suppress less useful ones if the task has many input features. In this paper, we propose a new neural CTR model named Field Attentive Deep Field-aware Factorization Machine (FAT-DeepFFM) by combining the Deep Field-aware Factorization Machine (DeepFFM) with Compose-Excitation network (CENet) field attention mechanism which is proposed by us as an enhanced version of Squeeze-Excitation network (SENet) to highlight the feature importance. We conduct extensive experiments on two real-world datasets and the experiment results show that FAT-DeepFFM achieves the best performance and obtains different improvements over the state-of-the-art methods. We also compare two kinds of attention mechanisms (attention before explicit feature interaction vs. attention after explicit feature interaction) and demonstrate that the former one outperforms the latter one significantly.",0
"""This paper presents a new model called FAT-DeepFFM that improves on traditional field-aware factorization machines by incorporating attention mechanisms to better capture nonlinear relationships between input features and output labels. Our approach outperforms existing methods on several benchmark datasets, demonstrating its effectiveness at handling complex real-world tasks.""",1
"Multi-task learning shares information between related tasks, sometimes reducing the number of parameters required. State-of-the-art results across multiple natural language understanding tasks in the GLUE benchmark have previously used transfer from a single large task: unsupervised pre-training with BERT, where a separate BERT model was fine-tuned for each task. We explore multi-task approaches that share a single BERT model with a small number of additional task-specific parameters. Using new adaptation modules, PALs or `projected attention layers', we match the performance of separately fine-tuned models on the GLUE benchmark with roughly 7 times fewer parameters, and obtain state-of-the-art results on the Recognizing Textual Entailment dataset.",0
"In recent years, pretrained language models like BERT have shown exceptional performance on a wide range of natural language processing tasks. However, fine-tuning these models for specific tasks can be computationally expensive and requires significant amounts of data and computational resources. To address this issue, we propose using projected attention layers (PALs) as an efficient alternative to traditional self-attention mechanisms used in transformer architectures. Our approach projects the input onto a lower-dimensional space, allowing for faster computations while still retaining the expressive power required for multi-task learning. We evaluate our method on several benchmark datasets and show that our proposed model achieves comparable accuracy to state-of-the-art methods while requiring significantly fewer parameters and training steps. This work demonstrates the effectiveness of PALs for efficient adaptation in multi-task learning and highlights their potential as a valuable addition to the NLP toolkit.",1
"We address the problem of graph classification based only on structural information. Inspired by natural language processing techniques (NLP), our model sequentially embeds information to estimate class membership probabilities. Besides, we experiment with NLP-like variational regularization techniques, making the model predict the next node in the sequence as it reads it. We experimentally show that our model achieves state-of-the-art classification results on several standard molecular datasets. Finally, we perform a qualitative analysis and give some insights on whether the node prediction helps the model better classify graphs.",0
"Introduction: Many graph classification tasks involve predicting labels based on structured data represented as graphs. Traditional machine learning models often struggle in capturing spatial dependencies present in such representations. In recent years, several deep learning approaches have emerged that address these shortcomings by leveraging convolutional neural networks (CNN) and recurrent neural networks (RNN). Motivation: This study explores the application of variational RNNs for graph classification tasks, aimed at improving upon state-of-the-art results achieved using traditional CNN-based architectures. Methodology: We propose VARGRN, which integrates variational inference principles within standard RNNs to learn latent embeddings that capture global features from the input graph data while maintaining temporal dependencies. Results & Discussion: Extensive experiments were conducted on four benchmark datasets commonly used for evaluating graph classification techniques, including MUTAG, NCI1, PROTEINS, and D\&D. Our model outperformed competitive baseline methods, demonstrating significant improvements across all datasets evaluated, achieving an average accuracy improvement of ~7%. Conclusion: These findings highlight the promise of variational RNNs as powerful tools for graph classification problems. Further research may explore potential gains from applying our methodology in other domains involving complex structured data, such as natural language processing and computer vision.",1
"Benefiting from the advancement of computer vision, natural language processing and information retrieval techniques, visual question answering (VQA), which aims to answer questions about an image or a video, has received lots of attentions over the past few years. Although some progress has been achieved so far, several studies have pointed out that current VQA models are heavily affected by the \emph{language prior problem}, which means they tend to answer questions based on the co-occurrence patterns of question keywords (e.g., how many) and answers (e.g., 2) instead of understanding images and questions. Existing methods attempt to solve this problem by either balancing the biased datasets or forcing models to better understand images. However, only marginal effects and even performance deterioration are observed for the first and second solution, respectively. In addition, another important issue is the lack of measurement to quantitatively measure the extent of the language prior effect, which severely hinders the advancement of related techniques.   In this paper, we make contributions to solve the above problems from two perspectives. Firstly, we design a metric to quantitatively measure the language prior effect of VQA models. The proposed metric has been demonstrated to be effective in our empirical studies. Secondly, we propose a regularization method (i.e., score regularization module) to enhance current VQA models by alleviating the language prior problem as well as boosting the backbone model performance. The proposed score regularization module adopts a pair-wise learning strategy, which makes the VQA models answer the question based on the reasoning of the image (upon this question) instead of basing on question-answer patterns observed in the biased training set. The score regularization module is flexible to be integrated into various VQA models.",0
"In recent years, there has been significant progress in developing systems that can automatically generate natural language answers based on visual content (i.e., image and video). However, these models often struggle to answer questions that require more complex understanding of the underlying concepts represented in the images. One reason for this difficulty is the ""language prior problem,"" which refers to the fact that many models rely heavily on their training data and may lack the ability to generalize beyond what they have seen before. This study proposes an approach for quantifying and alleviating the language prior problem in visual question answering by utilizing an attention mechanism that allows the model to focus on relevant portions of the input data. We evaluate our method using several standard datasets and show that it significantly improves performance over existing methods. Our findings suggest that addressing the language prior problem is crucial for achieving better results in visual question answering, and we hope that our work will inspire future research in this area.",1
"Sequence models assign probabilities to variable-length sequences such as natural language texts. The ability of sequence models to capture temporal dependence can be characterized by the temporal scaling of correlation and mutual information. In this paper, we study the mutual information of recurrent neural networks (RNNs) including long short-term memories and self-attention networks such as Transformers. Through a combination of theoretical study of linear RNNs and empirical study of nonlinear RNNs, we find their mutual information decays exponentially in temporal distance. On the other hand, Transformers can capture long-range mutual information more efficiently, making them preferable in modeling sequences with slow power-law mutual information, such as natural languages and stock prices. We discuss the connection of these results with statistical mechanics. We also point out the non-uniformity problem in many natural language datasets. We hope this work provides a new perspective in understanding the expressive power of sequence models and shed new light on improving the architecture of them.",0
"This paper investigates two key components that contribute to the success of modern natural language processing (NLP) models: mutual information scaling and expressive power of sequence models. We demonstrate how these elements work together to facilitate the learning of contextually relevant representations in NLP tasks such as machine translation and text generation. By evaluating different model configurations and training objectives, we provide insights into their impact on performance and robustness under domain shifts and changes in task conditions. Our results show that the choice of scaling factor can greatly influence the quality of learned representations, highlighting the importance of appropriate normalization techniques during pretraining and fine-tuning. Additionally, our experiments indicate that incorporating explicit supervision over low-level linguistic structures can enhance the expressive capacity of sequence models without sacrificing generality. Overall, our findings contribute to the understanding of how neural network architectures process sequential data and how they relate to human judgments, paving the way for further advancements in NLP research.",1
"Point cloud registration is a key problem for computer vision applied to robotics, medical imaging, and other applications. This problem involves finding a rigid transformation from one point cloud into another so that they align. Iterative Closest Point (ICP) and its variants provide simple and easily-implemented iterative methods for this task, but these algorithms can converge to spurious local optima. To address local optima and other difficulties in the ICP pipeline, we propose a learning-based method, titled Deep Closest Point (DCP), inspired by recent techniques in computer vision and natural language processing. Our model consists of three parts: a point cloud embedding network, an attention-based module combined with a pointer generation layer, to approximate combinatorial matching, and a differentiable singular value decomposition (SVD) layer to extract the final rigid transformation. We train our model end-to-end on the ModelNet40 dataset and show in several settings that it performs better than ICP, its variants (e.g., Go-ICP, FGR), and the recently-proposed learning-based method PointNetLK. Beyond providing a state-of-the-art registration technique, we evaluate the suitability of our learned features transferred to unseen objects. We also provide preliminary analysis of our learned model to help understand whether domain-specific and/or global features facilitate rigid registration.",0
"In this work we address point cloud registration, the task of aligning two sets of points that represent different views of the same three dimensional object. Current state of the art methods rely on handcrafted features to achieve robustness against noise and varying conditions. We present DeepClosestPoint (DCP), a learning based approach which directly learns to find correspondences without intermediate hand crafted representations. Our method learns a representation that captures both local and global context, enabling it to recover from missing data and clutter. Experiments show that our method outperforms current approaches and achieves state of the art results across multiple datasets including real world scans. To validate our design choices we ablate different components and study the effectiveness of each in comparison to competitive baselines. This research paves the road towards end-to-end deep learning pipelines for computer vision tasks.",1
"In recent years, increasingly augmentation of health data, such as patient Electronic Health Records (EHR), are becoming readily available. This provides an unprecedented opportunity for knowledge discovery and data mining algorithms to dig insights from them, which can, later on, be helpful to the improvement of the quality of care delivery. Predictive modeling of clinical risk, including in-hospital mortality, hospital readmission, chronic disease onset, condition exacerbation, etc., from patient EHR, is one of the health data analytic problems that attract most of the interests. The reason is not only because the problem is important in clinical settings, but also there are challenges working with EHR such as sparsity, irregularity, temporality, etc. Different from applications in other domains such as computer vision and natural language processing, the labeled data samples in medicine (patients) are relatively limited, which creates lots of troubles for effective predictive model learning, especially for complicated models such as deep learning. In this paper, we propose MetaPred, a meta-learning for clinical risk prediction from longitudinal patient EHRs. In particular, in order to predict the target risk where there are limited data samples, we train a meta-learner from a set of related risk prediction tasks which learns how a good predictor is learned. The meta-learned can then be directly used in target risk prediction, and the limited available samples can be used for further fine-tuning the model performance. The effectiveness of MetaPred is tested on a real patient EHR repository from Oregon Health & Science University. We are able to demonstrate that with CNN and RNN as base predictors, MetaPred can achieve much better performance for predicting target risk with low resources comparing with the predictor trained on the limited samples available for this risk.",0
"In recent years, electronic health records (EHRs) have become increasingly important sources of data for medical research and clinical decision making. However, many patients lack complete EHRs, which can limit the accuracy and generalizability of risk prediction models based on these records. To address this challenge, we propose MetaPred, a meta-learning approach that leverages limited patient EHR data along with additional external datasets and expert knowledge to improve clinical risk prediction. Our method involves several key steps, including transfer learning from relevant domains, fine-tuning using task-specific regularization, and integration of diverse evidence via Bayesian model averaging. We demonstrate the effectiveness of our framework through comprehensive experiments on three real-world datasets, showing significant improvements over state-of-the-art baseline methods across multiple evaluation metrics. Overall, MetaPred represents a promising solution for enhancing clinical risk prediction under challenging data scarcity conditions, with potential applications in areas such as precision medicine, drug development, and public health monitoring.",1
"Vanilla convolutional neural networks are known to provide superior performance not only in image recognition tasks but also in natural language processing and time series analysis. One of the strengths of convolutional layers is the ability to learn features about spatial relations in the input domain using various parameterized convolutional kernels. However, in time series analysis learning such spatial relations is not necessarily required nor effective. In such cases, kernels which model temporal dependencies or kernels with broader spatial resolutions are recommended for more efficient training as proposed by dilation kernels. However, the dilation has to be fixed a priori which limits the flexibility of the kernels. We propose generalized dilation networks which generalize the initial dilations in two aspects. First we derive an end-to-end learnable architecture for dilation layers where also the dilation rate can be learned. Second we break up the strict dilation structure, in that we develop kernels operating independently in the input space.",0
"Abstract: In this paper we introduce generalized dilation neural networks (GDNN), a new kind of deep learning model that combines multi-scale processing with dilated convolutions. GDNNs allow for high spatial accuracy while still maintaining a low computational cost by leveraging dilations on top of standard dilated conv layers. Our proposed architecture outperforms other state-of-the-art methods across multiple benchmark datasets including MNIST, CIFAR-10, and ImageNet, demonstrating significant improvements in image classification accuracy. Additionally, our work shows how traditional architectures can be modified using these techniques to achieve better results. Overall, GDNNs offer a powerful tool for computer vision tasks and have potential applications in many other fields where images play a critical role.",1
"Automatically captioning images with natural language sentences is an important research topic. State of the art models are able to produce human-like sentences. These models typically describe the depicted scene as a whole and do not target specific objects of interest or emotional relationships between these objects in the image. However, marketing companies require to describe these important attributes of a given scene. In our case, objects of interest are consumer goods, which are usually identifiable by a product logo and are associated with certain brands. From a marketing point of view, it is desirable to also evaluate the emotional context of a trademarked product, i.e., whether it appears in a positive or a negative connotation. We address the problem of finding brands in images and deriving corresponding captions by introducing a modified image captioning network. We also add a third output modality, which simultaneously produces real-valued image ratings. Our network is trained using a classification-aware loss function in order to stimulate the generation of sentences with an emphasis on words identifying the brand of a product. We evaluate our model on a dataset of images depicting interactions between humans and branded products. The introduced network improves mean class accuracy by 24.5 percent. Thanks to adding the third output modality, it also considerably improves the quality of generated captions for images depicting branded products.",0
"Title: ""Multimodal Image Captioning for Marketing Analysis""  Abstract: With the rise of social media platforms like Instagram and Pinterest, marketing professionals have increasingly turned to visual content to promote their products and services. However, analyzing the effectiveness of these images can be challenging due to the large volume of data generated each day, as well as the subjective nature of user engagement metrics such as likes and comments. In this study, we propose a multimodal image captioning approach that combines textual and visual features of images to generate descriptive captions that capture both the content of the image and its intended context. Our approach uses deep learning models trained on large datasets of labeled images and captions to learn how different attributes influence user engagement. By automatically generating captions for new images, our method allows marketers to quickly analyze their campaigns at scale without relying on manual annotations. Experimental results demonstrate the efficacy of our model in capturing key elements of effective product promotion through image description, including color schemes, composition, and objects present in the image. These insights could provide valuable guidance for improving future marketing strategies across industries.",1
"Domain classification is the task of mapping spoken language utterances to one of the natural language understanding domains in intelligent personal digital assistants (IPDAs). This is a major component in mainstream IPDAs in industry. Apart from official domains, thousands of third-party domains are also created by external developers to enhance the capability of IPDAs. As more domains are developed rapidly, the question of how to continuously accommodate the new domains still remains challenging. Moreover, existing continual learning approaches do not address the problem of incorporating personalized information dynamically for better domain classification. In this paper, we propose CoNDA, a neural network based approach for domain classification that supports incremental learning of new classes. Empirical evaluation shows that CoNDA achieves high accuracy and outperforms baselines by a large margin on both incrementally added new domains and existing domains.",0
"Incorporate keywords such as deep learning model, continuous learning system, meta learning method, online learning algorithm and large scale datasets. Describe the problems addressed by the proposed approach. Include relevant state of the art works that inspired you and how your work improves upon them. Mention evaluation methods used.  Deep learning models have shown promising results in personalized domain classification tasks on large scale datasets, but their performance can suffer from overfitting due to limited training data. This study proposes a novel continuous learning system based on a meta learning method utilizing an online learning algorithm. By updating the parameters at each time step, our system learns incrementally and adapts dynamically to new classes while maintaining previously learned knowledge. Our approach addresses issues related to catastrophic forgetting and memory efficiency encountered in traditional methods. We evaluate our proposed solution through experiments using public benchmark datasets, demonstrating significant improvements compared to competitive baselines under both transductive and inductive settings. Overall, we contribute a reliable large-scale personalized domain classification system applicable to real world applications where continuous adaptation is required.",1
"Edit distance, also known as Levenshtein distance, is an essential way to compare two strings that proved to be particularly useful in the analysis of genetic sequences and natural language processing. However, edit distance is a discrete function that is known to be hard to optimize. This fact hampers the use of this metric in Machine Learning. Even as simple algorithm as K-means fails to cluster a set of sequences using edit distance if they are of variable length and abundance. In this paper we propose a novel metric - soft edit distance (SED), which is a smooth approximation of edit distance. It is differentiable and therefore it is possible to optimize it with gradient methods. Similar to original edit distance, SED as well as its derivatives can be calculated with recurrent formulas at polynomial time. We prove usefulness of the proposed metric on synthetic datasets and clustering of biological sequences.",0
"In this work, we present a new method for comparing symbolic sequences that combines the advantages of both hard and soft alignment approaches. Our approach uses a soft edit distance metric that allows for flexible matching of subsequence patterns while still preserving the structural information of the original sequences. We show how our method can be used to compare a wide range of sequence data types including natural language texts, musical scores, and biological sequences. By using a differentiable approximation of the edit distance function, we can train machine learning models that can effectively learn from these comparisons and make accurate predictions on new data. Overall, our method provides a powerful tool for analyzing and modeling complex sequential data.",1
"The abundance of open-source code, coupled with the success of recent advances in deep learning for natural language processing, has given rise to a promising new application of machine learning to source code. In this work, we explore the use of a Siamese recurrent neural network model on Python source code to create vectors which capture the semantics of code. We evaluate the quality of embeddings by identifying which problem from a programming competition the code solves. Our model significantly outperforms a bag-of-tokens embedding, providing promising results for improving code embeddings that can be used in future software engineering tasks.",0
"In this paper we propose a novel neural network model that learns semantic vector representations of source code. Our approach leverages a siamese architecture, which allows us to compare and contrast different pieces of code by measuring their similarity or dissimilarity. We train our model on large datasets of cleaned and normalized software code, using a variety of metrics including tokenization, parsing, type inference, control flow analysis, dataflow analysis, alias analysis, etc. After training, we evaluate the quality of learned embeddings using several downstream tasks such as semantic clustering, search ranking, refactoring recommendation, vulnerability prediction, bug detection, etc. Experiment results demonstrate that our model significantly outperforms state-of-the-art baselines across all these tasks, proving the effectiveness and general applicability of our proposed methodology. This work has important implications for both researchers who study program understanding and developers who build software engineering tools based on machine learning algorithms.",1
"Time Series Classification (TSC) problems are encountered in many real life data mining tasks ranging from medicine and security to human activity recognition and food safety. With the recent success of deep neural networks in various domains such as computer vision and natural language processing, researchers started adopting these techniques for solving time series data mining problems. However, to the best of our knowledge, no previous work has considered the vulnerability of deep learning models to adversarial time series examples, which could potentially make them unreliable in situations where the decision taken by the classifier is crucial such as in medicine and security. For computer vision problems, such attacks have been shown to be very easy to perform by altering the image and adding an imperceptible amount of noise to trick the network into wrongly classifying the input image. Following this line of work, we propose to leverage existing adversarial attack mechanisms to add a special noise to the input time series in order to decrease the network's confidence when classifying instances at test time. Our results reveal that current state-of-the-art deep learning time series classifiers are vulnerable to adversarial attacks which can have major consequences in multiple domains such as food safety and quality assurance.",0
"In recent years, deep neural networks (DNNs) have become increasingly popular in time series classification tasks due to their impressive performance in many applications such as finance, healthcare, and manufacturing. However, these models remain susceptible to adversarial attacks that can drastically degrade their accuracy without being detected by human observers. This research investigates several adversarial attack methods specifically designed for DNN-based time series classifiers. Our study reveals vulnerabilities in commonly used architectures and underlines the need for developing robust countermeasures against such attacks. Furthermore, we propose a new hybrid method combining feature engineering techniques with genetic algorithms to craft more effective attacks. We demonstrate the effectiveness of our approach through extensive experiments using several public datasets from diverse domains. Finally, we provide insights into potential future directions for securing time series classification systems and discuss possible use cases for the proposed attacks. By highlighting these limitations and suggesting mitigation strategies, our work aims to bridge the gap between theory and practice in deploying machine learning-based solutions in real-world scenarios.  Keywords: Adversarial attacks; Deep neural networks; Time series classification; Feature engineering; Genetic algorithms; Secure machine learning",1
"Deep neural networks have revolutionized many fields such as computer vision and natural language processing. Inspired by this recent success, deep learning started to show promising results for Time Series Classification (TSC). However, neural networks are still behind the state-of-the-art TSC algorithms, that are currently composed of ensembles of 37 non deep learning based classifiers. We attribute this gap in performance due to the lack of neural network ensembles for TSC. Therefore in this paper, we show how an ensemble of 60 deep learning models can significantly improve upon the current state-of-the-art performance of neural networks for TSC, when evaluated over the UCR/UEA archive: the largest publicly available benchmark for time series analysis. Finally, we show how our proposed Neural Network Ensemble (NNE) is the first time series classifier to outperform COTE while reaching similar performance to the current state-of-the-art ensemble HIVE-COTE.",0
"This paper presents a new approach to time series classification using deep neural network ensembles (DNN). DNNs have been shown to achieve state-of-the-art results on many image recognition tasks but they can suffer from overfitting and underfitting when applied directly to temporal data due to their fixed input size and strong assumption about stationarity. To overcome these issues, we propose an ensemble method that combines multiple instances of different kinds of DNN architectures trained on transformed versions of the input sequences to improve accuracy and robustness. We demonstrate through experiments that our proposed method significantly outperforms single models and other baseline methods across several benchmark datasets, while maintaining competitive computational efficiency compared to traditional machine learning algorithms like random forest. Finally, we provide visualizations showing how individual models in the ensemble make predictions differently than the final output, providing insight into which parts of each instance contribute to the ensemble's high performance.",1
"Machine learning has been widely applied to various applications, some of which involve training with privacy-sensitive data. A modest number of data breaches have been studied, including credit card information in natural language data and identities from face dataset. However, most of these studies focus on supervised learning models. As deep reinforcement learning (DRL) has been deployed in a number of real-world systems, such as indoor robot navigation, whether trained DRL policies can leak private information requires in-depth study. To explore such privacy breaches in general, we mainly propose two methods: environment dynamics search via genetic algorithm and candidate inference based on shadow policies. We conduct extensive experiments to demonstrate such privacy vulnerabilities in DRL under various settings. We leverage the proposed algorithms to infer floor plans from some trained Grid World navigation DRL agents with LiDAR perception. The proposed algorithm can correctly infer most of the floor plans and reaches an average recovery rate of 95.83% using policy gradient trained agents. In addition, we are able to recover the robot configuration in continuous control environments and an autonomous driving simulator with high accuracy. To the best of our knowledge, this is the first work to investigate privacy leakage in DRL settings and we show that DRL-based agents do potentially leak privacy-sensitive information from the trained policies.",0
Be explicit and clear but concise. Use active voice. Provide at least one specific finding/result that should be included in this section. If there is no such thing then summarize important features of your work instead. Include main contributions at end. |},1
"The task of Natural Language Inference (NLI) is widely modeled as supervised sentence pair classification. While there has been a lot of work recently on generating explanations of the predictions of classifiers on a single piece of text, there have been no attempts to generate explanations of classifiers operating on pairs of sentences. In this paper, we show that it is possible to generate token-level explanations for NLI without the need for training data explicitly annotated for this purpose. We use a simple LSTM architecture and evaluate both LIME and Anchor explanations for this task. We compare these to a Multiple Instance Learning (MIL) method that uses thresholded attention make token-level predictions. The approach we present in this paper is a novel extension of zero-shot single-sentence tagging to sentence pairs for NLI. We conduct our experiments on the well-studied SNLI dataset that was recently augmented with manually annotation of the tokens that explain the entailment relation. We find that our white-box MIL-based method, while orders of magnitude faster, does not reach the same accuracy as the black-box methods.",0
"This paper presents a method for generating token-level explanations for natural language inference (NLI). NLI involves determining whether two sentences entail each other, contradict each other, or are neutral with respect to one another. However, while there has been significant progress in developing models that can perform well on NLI tasks, these models often lack interpretability, making it difficult to understand why certain sentence pairs are classified as they are. To address this issue, we propose a novel approach that generates explicit, interpretable explanations at the level of individual tokens within the input sentences. Our approach first computes sentence embeddings using a pretrained model and then uses these embeddings as inputs to a deep learning model that predicts which tokens contribute most strongly to the overall NLI judgment. We evaluate our method on several benchmark datasets and demonstrate that it yields high accuracy and provides detailed, intuitive explanations for NLI judgments. Furthermore, by examining how our model makes predictions, we shed light on the underlying structure of NLI problems and offer insights into the types of linguistic features and relationships that determine NLI outcomes. Overall, our work represents a step towards more transparent and explainable artificial intelligence, with potential applications in areas such as language generation, question answering, and sentiment analysis.",1
"It is a big challenge of computer vision to make machine automatically describe the content of an image with a natural language sentence. Previous works have made great progress on this task, but they only use the global or local image feature, which may lose some important subtle or global information of an image. In this paper, we propose a model with 3-gated model which fuses the global and local image features together for the task of image caption generation. The model mainly has three gated structures. 1) Gate for the global image feature, which can adaptively decide when and how much the global image feature should be imported into the sentence generator. 2) The gated recurrent neural network (RNN) is used as the sentence generator. 3) The gated feedback method for stacking RNN is employed to increase the capability of nonlinearity fitting. More specially, the global and local image features are combined together in this paper, which makes full use of the image information. The global image feature is controlled by the first gate and the local image feature is selected by the attention mechanism. With the latter two gates, the relationship between image and text can be well explored, which improves the performance of the language part as well as the multi-modal embedding part. Experimental results show that our proposed method outperforms the state-of-the-art for image caption generation.",0
"This research paper presents a novel approach to generating captions for images using a 3G (Generative Graph Network) model architecture. Our method addresses the challenges faced by existing image captioning models by incorporating graph-based reasoning, attention mechanisms, and transfer learning techniques. We demonstrate that our proposed model outperforms state-of-the-art methods on both publicly available benchmark datasets and new challenging datasets. By leveraging recent advances in natural language processing, computer vision, and machine learning, we provide a powerful tool for automatically generating descriptive and accurate image captions. Overall, our work represents an important step towards achieving more human-like image understanding capabilities through deep neural networks.",1
"Using a natural language sentence to describe the content of an image is a challenging but very important task. It is challenging because a description must not only capture objects contained in the image and the relationships among them, but also be relevant and grammatically correct. In this paper a multi-modal embedding model based on gated recurrent units (GRU) which can generate variable-length description for a given image. In the training step, we apply the convolutional neural network (CNN) to extract the image feature. Then the feature is imported into the multi-modal GRU as well as the corresponding sentence representations. The multi-modal GRU learns the inter-modal relations between image and sentence. And in the testing step, when an image is imported to our multi-modal GRU model, a sentence which describes the image content is generated. The experimental results demonstrate that our multi-modal GRU model obtains the state-of-the-art performance on Flickr8K, Flickr30K and MS COCO datasets.",0
"Our paper presents a novel approach to multi-modal learning using Gated Recurrent Units (GRUs) for image description tasks. We demonstrate that our method outperforms state-of-the-art models on several benchmark datasets, including COCO Caption and Flickr30K. We introduce a two-stream architecture where one stream processes visual features from an image and another process textual representations extracted from an image caption. Both streams then feed into a joint gate mechanism which controls the flow of information, enabling better fusion of information across modalities. Furthermore, we use adversarial training to improve the quality of generated descriptions by encouraging the model to generate descriptions that are more difficult to distinguish from human written ones. Through extensive experiments, we show that our method achieves significant improvements over previous approaches and provides new insights into the challenges of multi-modal learning. Overall, our work advances the field of computer vision and natural language processing by providing a powerful tool for generating high-quality image descriptions.",1
"Distributed representations of sentences have become ubiquitous in natural language processing tasks. In this paper, we consider a continual learning scenario for sentence representations: Given a sequence of corpora, we aim to optimize the sentence encoder with respect to the new corpus while maintaining its accuracy on the old corpora. To address this problem, we propose to initialize sentence encoders with the help of corpus-independent features, and then sequentially update sentence encoders using Boolean operations of conceptor matrices to learn corpus-dependent features. We evaluate our approach on semantic textual similarity tasks and show that our proposed sentence encoder can continually learn features from new corpora while retaining its competence on previously encountered corpora.",0
"In summary: This paper proposes using ""Conceptors"" as building blocks for language models that continually learn sentence representations over time, by updating those concepts through attention mechanisms. The authors demonstrate state-of-the art performance on multiple text classification tasks.  ---  This paper presents a novel approach to constructing language models capable of learning sentence representations continually over time. The key idea lies in utilizing conceptors - atomic units of meaning that can be combined to form complex semantic structures. These conceptor modules are equipped with self-attention mechanisms to dynamically update their knowledge bases during inference. To achieve state-of-the-art results across several benchmark text classification problems, we train our model end-to-end under weak supervision from human annotators and showcase its ability to generalize well beyond the scope of pretraining data. Our method has promise for natural language processing applications that require continuously evolving semantic understanding without requiring explicit fine-tuning.",1
"Literary critics often attempt to uncover meaning in a single work of literature through careful reading and analysis. Applying natural language processing methods to aid in such literary analyses remains a challenge in digital humanities. While most previous work focuses on ""distant reading"" by algorithmically discovering high-level patterns from large collections of literary works, here we sharpen the focus of our methods to a single literary theory about Italo Calvino's postmodern novel Invisible Cities, which consists of 55 short descriptions of imaginary cities. Calvino has provided a classification of these cities into eleven thematic groups, but literary scholars disagree as to how trustworthy his categorization is. Due to the unique structure of this novel, we can computationally weigh in on this debate: we leverage pretrained contextualized representations to embed each city's description and use unsupervised methods to cluster these embeddings. Additionally, we compare results of our computational approach to similarity judgments generated by human readers. Our work is a first step towards incorporating natural language processing into literary criticism.",0
"Throughout history, cities have been depicted in literature through vivid descriptions that evoke sensory images and emotional responses from readers. However, these literary portrayals can often obscure the realities of urban life, presenting idealized versions of cities rather than their complex physical and social infrastructures. This study seeks to computationally engage with literary criticism by using data visualization techniques to illuminate hidden aspects of literary representations of cities and provide new insights into how they relate to actual urban spaces. By analyzing literary texts as datasets and mapping them against geospatial maps, we can uncover patterns in the distribution of literary elements across space and time. These findings challenge traditional assumptions about literature’s role in shaping our understanding of urban environments, and highlight the need for more nuanced approaches to studying literature’s relationship to place. Our research shows that computational methods offer powerful tools for interrogating familiar literary texts and revealing new perspectives on urban life.",1
"Describing what has changed in a scene can be useful to a user, but only if generated text focuses on what is semantically relevant. It is thus important to distinguish distractors (e.g. a viewpoint change) from relevant changes (e.g. an object has moved). We present a novel Dual Dynamic Attention Model (DUDA) to perform robust Change Captioning. Our model learns to distinguish distractors from semantic changes, localize the changes via Dual Attention over ""before"" and ""after"" images, and accurately describe them in natural language via Dynamic Speaker, by adaptively focusing on the necessary visual inputs (e.g. ""before"" or ""after"" image). To study the problem in depth, we collect a CLEVR-Change dataset, built off the CLEVR engine, with 5 types of scene changes. We benchmark a number of baselines on our dataset, and systematically study different change types and robustness to distractors. We show the superiority of our DUDA model in terms of both change captioning and localization. We also show that our approach is general, obtaining state-of-the-art results on the recent realistic Spot-the-Diff dataset which has no distractors.",0
"Automatic change detection in satellite imagery is crucial for monitoring dynamic phenomena on earth. However, current methods often fail due to occlusions, illumination changes, weather effects, and other factors that cause significant appearance differences before and after events. In this work, we present Robust Change Captioning (RCC), a novel deep learning approach for capturing spatial patterns across these difficulties. By explicitly modeling the uncertainty caused by occlusions and appearance changes through attention mechanisms, our method achieves state-of-the-art results without requiring expert postprocessing. To evaluate RCC’s effectiveness at handling real-world challenges such as missing pixels, clouds, and varying light conditions, we create SynthEye ––a new synthetic dataset based on Google Earth images with diverse terrain types, scene configurations, and sensor resolutions. Our comprehensive experiments demonstrate that RCC outperforms other approaches on six benchmark datasets, including four recently proposed ones, while ensembling the top three models further boosts performance. We plan to release both RCC and SynthEye upon acceptance so that researchers can easily compare and develop their own methods for robust change captioning.",1
"To solve the problem that convolutional neural networks (CNNs) are difficult to process non-grid type relational data like graphs, Kipf et al. proposed a graph convolutional neural network (GCN). The core idea of the GCN is to perform two-fold informational fusion for each node in a given graph during each iteration: the fusion of graph structure information and the fusion of node feature dimensions. Because of the characteristic of the combinatorial generalizations, GCN has been widely used in the fields of scene semantic relationship analysis, natural language processing and few-shot learning etc. However, due to its two-fold informational fusion involves mathematical irreversible calculations, it is hard to explain the decision reason for the prediction of the each node classification. Unfortunately, most of the existing attribution analysis methods concentrate on the models like CNNs, which are utilized to process grid-like data. It is difficult to apply those analysis methods to the GCN directly. It is because compared with the independence among CNNs input data, there is correlation between the GCN input data. This resulting in the existing attribution analysis methods can only obtain the partial model contribution from the central node features to the final decision of the GCN, but ignores the other model contribution from central node features and its neighbor nodes features to that decision. To this end, we propose a gradient attribution analysis method for the GCN called Node Attribution Method (NAM), which can get the model contribution from not only the central node but also its neighbor nodes to the GCN output. We also propose the Node Importance Visualization (NIV) method to visualize the central node and its neighbor nodes based on the value of the contribution...",0
"This research presents a novel approach to interpreting and understanding graph convolutional neural networks (GCNNs) through the use of gradient-based attribution methods. The proposed method enables us to quantify the contribution of each node in the input graph to the output predictions made by the GCNN. We demonstrate that our method can provide valuable insights into how these models work and make predictions, which could aid in developing more effective architectures and improving model interpretability. Our results show that the attribution values obtained from our method correlate well with human expert knowledge in some cases and can reveal interesting patterns and relationships within the data. Overall, we believe that our approach provides a powerful tool for analyzing and understanding the behavior of GCNNs, paving the way for future advancements in this area.",1
"Deep learning has arguably achieved tremendous success in recent years. In simple words, deep learning uses the composition of many nonlinear functions to model the complex dependency between input features and labels. While neural networks have a long history, recent advances have greatly improved their performance in computer vision, natural language processing, etc. From the statistical and scientific perspective, it is natural to ask: What is deep learning? What are the new characteristics of deep learning, compared with classical methods? What are the theoretical foundations of deep learning? To answer these questions, we introduce common neural network models (e.g., convolutional neural nets, recurrent neural nets, generative adversarial nets) and training techniques (e.g., stochastic gradient descent, dropout, batch normalization) from a statistical point of view. Along the way, we highlight new characteristics of deep learning (including depth and over-parametrization) and explain their practical and theoretical benefits. We also sample recent results on theories of deep learning, many of which are only suggestive. While a complete understanding of deep learning remains elusive, we hope that our perspectives and discussions serve as a stimulus for new statistical research.",0
"Deep learning has become one of the most popular research areas in computer science over the last decade. In particular, deep neural networks have proven highly effective in solving complex problems ranging from image classification and speech recognition to natural language processing and game playing. With their ability to learn representations that can generalize across multiple tasks and domains, these models hold great promise for a wide range of applications. However, there remains much work to be done to better understand how they perform and why they work so well on some tasks but fail on others. This selective survey provides a high level introduction to key concepts and recent developments in the field, with an emphasis on techniques that are particularly relevant for real world systems. We focus on discussing current state-of-the art methods as well as identifying promising future directions for research.",1
"In this paper, we propose a novel multi-task learning method based on the deep convolutional network. The proposed deep network has four convolutional layers, three max-pooling layers, and two parallel fully connected layers. To adjust the deep network to multi-task learning problem, we propose to learn a low-rank deep network so that the relation among different tasks can be explored. We proposed to minimize the number of independent parameter rows of one fully connected layer to explore the relations among different tasks, which is measured by the nuclear norm of the parameter of one fully connected layer, and seek a low-rank parameter matrix. Meanwhile, we also propose to regularize another fully connected layer by sparsity penalty, so that the useful features learned by the lower layers can be selected. The learning problem is solved by an iterative algorithm based on gradient descent and back-propagation algorithms. The proposed algorithm is evaluated over benchmark data sets of multiple face attribute prediction, multi-task natural language processing, and joint economics index predictions. The evaluation results show the advantage of the low-rank deep CNN model over multi-task problems.",0
"Deep learning has revolutionized many fields by solving complex problems that were previously solved using rule-based systems. In particular, deep convolutional neural networks (CNNs) have been used successfully on high resolution data such as images and videos. However, designing CNN architectures can be computationally expensive due to their parameter count which increases exponentially with the size of input images. Furthermore, training these models requires large amounts of labeled data which may not always be available. We propose a low rank CNN architecture called LRCNN which uses Tensor decomposition techniques to reduce computational requirements during inference and training. Our model achieves competitive performance on benchmark datasets while reducing the number of parameters required compared to traditional CNNs. Finally we showcase our method on real world multi task problem.",1
"We propose Visual Query Detection (VQD), a new visual grounding task. In VQD, a system is guided by natural language to localize a variable number of objects in an image. VQD is related to visual referring expression recognition, where the task is to localize only one object. We describe the first dataset for VQD and we propose baseline algorithms that demonstrate the difficulty of the task compared to referring expression recognition.",0
"In recent years, there has been growing interest in developing computer vision systems that can effectively identify objects within natural scenes. However, traditional object detection algorithms often struggle to accurately detect complex visual queries in images due to limitations such as small scale, cluttered backgrounds, partial occlusions, or changes in lighting conditions. To address these challenges, we propose a novel approach called Visual Query Detection (VQD), which combines advanced deep learning techniques with classical feature extraction methods to improve query detection accuracy.  Our proposed method consists of two main components: a semantic segmentation network and a query detection module. The semantic segmentation network generates high-quality pixel-level masks for each image by predicting the class probabilities of all pixels belonging to the desired query category. These masks serve as strong contextual priors to guide the subsequent query detection step. We then use a fully convolutional neural network based on HED (Histogram of Oriented Gradients Network) features to predict the bounding box coordinates of the query object in the original image space. Our algorithm leverages both local texture patterns and global context from the scene, allowing it to accurately handle variations in lighting and pose. Additionally, our model uses adaptive thresholding strategies based on estimated uncertainty, ensuring robustness to different scales and illumination conditions.  We evaluate our VQD system using several benchmark datasets consisting of diverse real-world images with varying levels of complexity. Experimental results demonstrate substantial improvements over state-of-the-art approaches, achieving higher precision and recall rates while maintaining efficiency. Our contributions provide a powerful new tool for researchers working in areas such as object recognition, robotics, or autonomous driving, where accurate query identification is critical for successful task completion. Overall, our work paves the way f",1
"This paper presents a novel method to manipulate the visual appearance (pose and attribute) of a person image according to natural language descriptions. Our method can be boiled down to two stages: 1) text guided pose generation and 2) visual appearance transferred image synthesis. In the first stage, our method infers a reasonable target human pose based on the text. In the second stage, our method synthesizes a realistic and appearance transferred person image according to the text in conjunction with the target pose. Our method extracts sufficient information from the text and establishes a mapping between the image space and the language space, making generating and editing images corresponding to the description possible. We conduct extensive experiments to reveal the effectiveness of our method, as well as using the VQA Perceptual Score as a metric for evaluating the method. It shows for the first time that we can automatically edit the person image from the natural language descriptions.",0
"This study presents a novel approach to generating synthetic images of people guided by textual descriptions. We propose a deep neural network architecture that takes as input a natural language description and produces a corresponding image output. Our model utilizes convolutional and transformer networks to encode both visual and textual features, enabling the generation of realistic and detailed person images that closely match the given text description. To evaluate the performance of our method, we conduct extensive experiments on several benchmark datasets and demonstrate improved results compared to state-of-the-art methods in terms of image quality, attention alignment, and diversity metrics. Furthermore, we present qualitative analysis and user studies which showcase the effectiveness of our method in generating high-quality and coherent person images based on descriptive text inputs. Overall, our work opens up new possibilities for exploring text-to-image synthesis and advancing applications such as virtual reality environments, gaming, and art creation.",1
"We consider the problem of referring image segmentation. Given an input image and a natural language expression, the goal is to segment the object referred by the language expression in the image. Existing works in this area treat the language expression and the input image separately in their representations. They do not sufficiently capture long-range correlations between these two modalities. In this paper, we propose a cross-modal self-attention (CMSA) module that effectively captures the long-range dependencies between linguistic and visual features. Our model can adaptively focus on informative words in the referring expression and important regions in the input image. In addition, we propose a gated multi-level fusion module to selectively integrate self-attentive cross-modal features corresponding to different levels in the image. This module controls the information flow of features at different levels. We validate the proposed approach on four evaluation datasets. Our proposed approach consistently outperforms existing state-of-the-art methods.",0
"In recent years, referring expression segmentation has emerged as an important task in computer vision that involves identifying regions in images referred to by natural language expressions such as ""the cat on the couch."" Existing methods primarily focus on localizing objects from textual queries without considering visual contexts. In this work, we propose a novel cross-modal self-attention network (CSANet) that can effectively attend to both modalities. Our CSANet employs two branches: one processes image features using a convolutional neural network (CNN), while the other uses a transformer architecture to process textual queries. We introduce a multi-scale feature fusion module that aligns the image and query representations at multiple levels. This allows our model to identify spatially more accurate object boundaries through fine-grained attention across different scales. Extensive experiments demonstrate significant improvements over state-of-the-art approaches on several benchmark datasets. Overall, our method shows great potential in advancing the field of cross-modal referring expression segmentation.",1
"Over the past few years machine learning has seen a renewed explosion of interest, following a number of studies showing the effectiveness of neural networks in a range of tasks which had previously been considered incredibly hard. Neural networks' effectiveness in the fields of image recognition and natural language processing stems primarily from the vast amounts of data available to companies and researchers, coupled with the huge amounts of compute power available in modern accelerators such as GPUs, FPGAs and ASICs. There are a number of approaches available to developers for utilizing GPGPU technologies such as SYCL, OpenCL and CUDA, however many applications require the same low level mathematical routines. Libraries dedicated to accelerating these common routines allow developers to easily make full use of the available hardware without requiring low level knowledge of the hardware themselves, however such libraries are often provided by hardware manufacturers for specific hardware such as cuDNN for Nvidia hardware or MIOpen for AMD hardware.   SYCL-DNN is a new open-source library dedicated to providing accelerated routines for neural network operations which are hardware and vendor agnostic. Built on top of the SYCL open standard and written entirely in standard C++, SYCL-DNN allows a user to easily accelerate neural network code for a wide range of hardware using a modern C++ interface. The library is tested on AMD's OpenCL for GPU, Intel's OpenCL for CPU and GPU, ARM's OpenCL for Mali GPUs as well as ComputeAorta's OpenCL for R-Car CV engine and host CPU. In this talk we will present performance figures for SYCL-DNN on this range of hardware, and discuss how high performance was achieved on such a varied set of accelerators with such different hardware features.",0
"This paper presents a methodology for accelerating neural network computations using OpenCL devices with the SYCL-DNN library. We describe how we used SYCL-DNN to optimize performance by reducing memory transfer overhead and improving parallelization. Our results show that our approach can lead to significant speedups compared to traditional CPU-based implementations. Additionally, we demonstrate that our method can scale well across different device architectures, making it a flexible solution for implementing high-performance neural networks. Overall, this work contributes new insights into the use of SYCL-DNN for accelerating machine learning applications.",1
"Referring object detection and referring image segmentation are important tasks that require joint understanding of visual information and natural language. Yet there has been evidence that current benchmark datasets suffer from bias, and current state-of-the-art models cannot be easily evaluated on their intermediate reasoning process. To address these issues and complement similar efforts in visual question answering, we build CLEVR-Ref+, a synthetic diagnostic dataset for referring expression comprehension. The precise locations and attributes of the objects are readily available, and the referring expressions are automatically associated with functional programs. The synthetic nature allows control over dataset bias (through sampling strategy), and the modular programs enable intermediate reasoning ground truth without human annotators.   In addition to evaluating several state-of-the-art models on CLEVR-Ref+, we also propose IEP-Ref, a module network approach that significantly outperforms other models on our dataset. In particular, we present two interesting and important findings using IEP-Ref: (1) the module trained to transform feature maps into segmentation masks can be attached to any intermediate module to reveal the entire reasoning process step-by-step; (2) even if all training data has at least one object referred, IEP-Ref can correctly predict no-foreground when presented with false-premise referring expressions. To the best of our knowledge, this is the first direct and quantitative proof that neural modules behave in the way they are intended.",0
"This should summarize all the important details found within your research without any biases. Include why referring expressions are hard to create but may lead to more advanced systems. Abstract: Recently there have been a number of breakthroughs involving artificial intelligence in natural language processing tasks such as question answering and code generation. However, these models often lack the ability to incorporate real world knowledge into their reasoning processes. To address this deficiency, we present a new dataset called CLEVR-Ref+ which adds referring expressions (REFs) to existing VQA datasets like CLEVR. We find that REFs increase the complexity of multi-step reasoning problems by requiring additional context to determine which objects are being referred to. Our experiments demonstrate that current state-of-the-art models struggle on this task, indicating opportunities for future improvements in visual reasoning capabilities. Despite this challenge, successful integration of REFs has potential to allow for richer, more dynamic interactions with humans through natural language instructions.  Note - Please use the provided paper title in the final product. Also if possible please provide me the pdf version of the same.",1
"The techniques of deep learning have become the state of the art methodology for executing complicated tasks from various domains of computer vision, natural language processing, and several other areas. Due to its rapid development and promising benchmarks in those fields, researchers started experimenting with this technique to perform in the area of, especially in intrusion detection related tasks. Deep learning is a subset and a natural extension of classical Machine learning and an evolved model of neural networks. This paper contemplates and discusses all the methodologies related to the leading edge Deep learning and Neural network models purposing to the arena of Intrusion Detection Systems.",0
"Title: ""A Comprehensive Overview of IDS"" Abstract: An intrusion detection system (IDS) is crucial in modern cybersecurity. There are two main types of IDSs: network-based and host-based systems. Network IDS focuses on monitoring network traffic and detecting anomalies while host-based monitors individual hosts. Both have their advantages and disadvantages. This comprehensive review article provides insights into both network and host-based IDS techniques by presenting current literature and highlighting existing gaps that need further research attention. Keywords: intrusion detection system; IDS; network-based IDS; host-based IDS; anomaly detection I certify that this submission is my original work and has not been submitted elsewhere. Please use the following reference format. <https://www.ncbi.nlm.nih.gov/pmc/article-lookup/?uid=PMC6947857>",1
"Deep learning models have lately shown great performance in various fields such as computer vision, speech recognition, speech translation, and natural language processing. However, alongside their state-of-the-art performance, it is still generally unclear what is the source of their generalization ability. Thus, an important question is what makes deep neural networks able to generalize well from the training set to new data. In this article, we provide an overview of the existing theory and bounds for the characterization of the generalization error of deep neural networks, combining both classical and more recent theoretical and empirical results.",0
"This paper investigates generalization error in deep learning models. We begin by discussing the problem statement and motivation behind understanding how these errors arise. Next, we survey existing work on the topic from both theoretical and experimental perspectives. Our analysis reveals that there are several factors contributing to the high incidence of overfitting in deep neural networks, including complexity mismatch, insufficient training data, lack of regularization, and vanishing gradients. To address these issues, we propose a novel approach using model pruning techniques combined with gradient descent optimization. Our experiments show that our method significantly reduces the risk of overfitting while maintaining good test performance across multiple datasets. In conclusion, our research provides valuable insights into the sources of generalization error in deep learning and offers promising strategies for improving the robustness and reliability of these models.",1
"Vision-language navigation (VLN) is the task of navigating an embodied agent to carry out natural language instructions inside real 3D environments. In this paper, we study how to address three critical challenges for this task: the cross-modal grounding, the ill-posed feedback, and the generalization problems. First, we propose a novel Reinforced Cross-Modal Matching (RCM) approach that enforces cross-modal grounding both locally and globally via reinforcement learning (RL). Particularly, a matching critic is used to provide an intrinsic reward to encourage global matching between instructions and trajectories, and a reasoning navigator is employed to perform cross-modal grounding in the local visual scene. Evaluation on a VLN benchmark dataset shows that our RCM model significantly outperforms previous methods by 10% on SPL and achieves the new state-of-the-art performance. To improve the generalizability of the learned policy, we further introduce a Self-Supervised Imitation Learning (SIL) method to explore unseen environments by imitating its own past, good decisions. We demonstrate that SIL can approximate a better and more efficient policy, which tremendously minimizes the success rate performance gap between seen and unseen environments (from 30.7% to 11.7%).",0
"In this work we present an approach that combines reinforced cross modal matching and self supervised imitation learning to solve vision-language navigation tasks where an agent needs to generate natural language instructions to navigate through an environment while solving complex tasks such as object manipulation and reasoning. Our method builds on recent advances in pretraining large transformer models on image-text pairs using contrastive losses. We then use these pretrained models as encoders for visual features and tokenize instruction texts into discrete actions, which serve as the decoder input. To improve performance on downstream tasks, we train two separate agents: one focused on generating high quality textual descriptions, and another optimizing for successful task completion measured by metrics like success rate, accuracy, or time efficiency. During training, both agents share the same latent representation learned from visual and textual inputs but optimize different reward signals. Furthermore, we demonstrate how to transfer knowledge across environments and improve performance over previous state-of-the-art methods on challenging benchmark datasets.",1
"Natural language processing has improved tremendously after the success of word embedding techniques such as word2vec. Recently, the same idea has been applied on source code with encouraging results. In this survey, we aim to collect and discuss the usage of word embedding techniques on programs and source code. The articles in this survey have been collected by asking authors of related work and with an extensive search on Google Scholar. Each article is categorized into five categories: 1. embedding of tokens 2. embedding of functions or methods 3. embedding of sequences or sets of method calls 4. embedding of binary code 5. other embeddings. We also provide links to experimental data and show some remarkable visualization of code embeddings. In summary, word embedding has been successfully applied on different granularities of source code. With access to countless open-source repositories, we see a great potential of applying other data-driven natural language processing techniques on source code in the future.",0
"This literature study aims to provide a comprehensive analysis of embeddings as they relate to source code. The main objectives of this study were to identify common techniques used in embedding generation, evaluate their effectiveness, and discuss challenges faced by researchers in this field.  The results of our study indicate that most current approaches use some form of deep learning algorithm, such as neural networks, combined with other natural language processing methods. These models have shown significant improvement over traditional keyword search methods. However, there remain several limitations, including difficulty modeling complex relationships between concepts, handling code constructs like loops and conditionals, and generalizing across different domains and languages. Additionally, evaluation metrics often rely solely on benchmark datasets without considering real-world scenarios.  This literature study concludes with recommendations for future work in the area of embeddings for source code. Improved evaluation methodologies that account for application domain and task type would significantly benefit the community. Moreover, exploring unsupervised pretraining using large scale data corpora could potentially improve performance and reduce reliance on labeled datasets. Overall, while substantial progress has been made, further advancements are necessary before embeddings can fully replace existing solutions.",1
"Adversarial examples are important for understanding the behavior of neural models, and can improve their robustness through adversarial training. Recent work in natural language processing generated adversarial examples by assuming white-box access to the attacked model, and optimizing the input directly against it (Ebrahimi et al., 2018). In this work, we show that the knowledge implicit in the optimization procedure can be distilled into another more efficient neural network. We train a model to emulate the behavior of a white-box attack and show that it generalizes well across examples. Moreover, it reduces adversarial example generation time by 19x-39x. We also show that our approach transfers to a black-box setting, by attacking The Google Perspective API and exposing its vulnerability. Our attack flips the API-predicted label in 42\% of the generated examples, while humans maintain high-accuracy in predicting the gold label.",0
"Abstract: In recent years, adversarial attacks have become increasingly prevalent, posing significant threats to modern machine learning systems. As a result, research has focused on developing methods that can generate robust models capable of resisting such attacks. One approach involves distilling the knowledge from a pre-trained model into a smaller, more efficient model. However, existing techniques suffer from issues related to computation complexity, scalability, and transferability across different architectures. This work proposes a new method called ""White-to-Black"" which addresses these challenges by leveraging ensemble distillation for efficiently generating black-box attack-resistant models. Experimental results demonstrate that our proposed technique outperforms state-of-the-art approaches while maintaining competitive accuracies. Our findings highlight the potential utility of ensembling as a powerful tool for generating efficient adversarially trained models with wide applicability. Overall, our contributions provide insights towards improving the stability and security of machine learning systems against malicious inputs.",1
"Graph convolutional networks (GCNs) have recently become one of the most powerful tools for graph analytics tasks in numerous applications, ranging from social networks and natural language processing to bioinformatics and chemoinformatics, thanks to their ability to capture the complex relationships between concepts. At present, the vast majority of GCNs use a neighborhood aggregation framework to learn a continuous and compact vector, then performing a pooling operation to generalize graph embedding for the classification task. These approaches have two disadvantages in the graph classification task: (1)when only the largest sub-graph structure ($k$-hop neighbor) is used for neighborhood aggregation, a large amount of early-stage information is lost during the graph convolution step; (2) simple average/sum pooling or max pooling utilized, which loses the characteristics of each node and the topology between nodes. In this paper, we propose a novel framework called, dual attention graph convolutional networks (DAGCN) to address these problems. DAGCN automatically learns the importance of neighbors at different hops using a novel attention graph convolution layer, and then employs a second attention component, a self-attention pooling layer, to generalize the graph representation from the various aspects of a matrix graph embedding. The dual attention network is trained in an end-to-end manner for the graph classification task. We compare our model with state-of-the-art graph kernels and other deep learning methods. The experimental results show that our framework not only outperforms other baselines but also achieves a better rate of convergence.",0
"Abstract  DAGCN (Dual Attention Graph Convolutional Networks) addresses the problem of graph representation learning by introducing a novel neural network architecture that leverages dual attention mechanisms within a GCN framework. Current state-of-the art techniques focus on either node features or edge attributes and overlook their mutually beneficial relationship. Our model learns to attend simultaneously to both local graph context as well as non-local semantic information that can enhance discriminative power without increasing computational cost. We demonstrate improved performance across several benchmark datasets compared to other methods, such as those from baseline GCN models and variants like JKNet and DGNN.",1
"Recently, deep learning based natural language processing techniques are being extensively used to deal with spam mail, censorship evaluation in social networks, among others. However, there is only a couple of works evaluating the vulnerabilities of such deep neural networks. Here, we go beyond attacks to investigate, for the first time, universal rules, i.e., rules that are sample agnostic and therefore could turn any text sample in an adversarial one. In fact, the universal rules do not use any information from the method itself (no information from the method, gradient information or training dataset information is used), making them black-box universal attacks. In other words, the universal rules are sample and method agnostic. By proposing a coevolutionary optimization algorithm we show that it is possible to create universal rules that can automatically craft imperceptible adversarial samples (only less than five perturbations which are close to misspelling are inserted in the text sample). A comparison with a random search algorithm further justifies the strength of the method. Thus, universal rules for fooling networks are here shown to exist. Hopefully, the results from this work will impact the development of yet more sample and model agnostic attacks as well as their defenses, culminating in perhaps a new age for artificial intelligence.",0
"Artificial intelligence (AI) has made significant progress over recent years on tasks like computer vision, natural language understanding, and game playing. Despite these breakthroughs, deep neural networks (DNNs), which underlie many modern AIs, are still vulnerable to attack by adversarial examples. Adversarial examples are small perturbations that cause DNNs to make predictions that are wildly incorrect from their perspective point of view. In this work we present universal rules that can generate adversarial text for any input into an AI system, thus showing how vulnerable modern AI systems are. We use these rules as part of our algorithm to optimize crafted examples that fool state-of-the art DNN models across multiple datasets and architectures. Our results show that such attacks are feasible even with high confidence intervals set for the classifier, i.e., adversarial text can reliably be generated with low perplexity and human readability as judged by Amazon Mechanical Turkers. Moreover, our universal method outperforms existing heuristics by achieving better accuracies against state-of-theart DNN classifiers while maintaining both low perplexity and high human readability.",1
"Dense captioning aims at simultaneously localizing semantic regions and describing these regions-of-interest (ROIs) with short phrases or sentences in natural language. Previous studies have shown remarkable progresses, but they are often vulnerable to the aperture problem that a caption generated by the features inside one ROI lacks contextual coherence with its surrounding context in the input image. In this work, we investigate contextual reasoning based on multi-scale message propagations from the neighboring contents to the target ROIs. To this end, we design a novel end-to-end context and attribute grounded dense captioning framework consisting of 1) a contextual visual mining module and 2) a multi-level attribute grounded description generation module. Knowing that captions often co-occur with the linguistic attributes (such as who, what and where), we also incorporate an auxiliary supervision from hierarchical linguistic attributes to augment the distinctiveness of the learned captions. Extensive experiments and ablation studies on Visual Genome dataset demonstrate the superiority of the proposed model in comparison to state-of-the-art methods.",0
"Abstract:  Image understanding has been one of the most active research areas in computer vision over the past few decades, as visual data plays a significant role in many applications such as autonomous driving, medical diagnosis, robotics, etc. In recent years, we have seen great advancements in image classification, object detection, semantic segmentation, and other pixel-level tasks using convolutional neural networks (CNNs). However, obtaining descriptive textual information from images remains challenging due to the limited capacity of existing methods. In this work, we propose a novel approach called Context and Attribute Grounded Dense Captioning (CAGDC) that can generate natural language descriptions of complex scenes efficiently and accurately. Our method builds upon previous approaches by introducing several new components designed to address specific shortcomings of current caption generation techniques. By leveraging context and attributes effectively, CAGDC generates more coherent, accurate, and informative captions compared to state-of-the-art systems. We evaluate our model on standard benchmark datasets and demonstrate its superior performance through extensive experiments and analysis. This study paves the way for further development of high-quality image captioning models that capture more nuanced aspects of scene understanding, benefiting numerous real-world applications that rely on accurate image interpretation.",1
"In this paper, we present LaSOT, a high-quality benchmark for Large-scale Single Object Tracking. LaSOT consists of 1,400 sequences with more than 3.5M frames in total. Each frame in these sequences is carefully and manually annotated with a bounding box, making LaSOT the largest, to the best of our knowledge, densely annotated tracking benchmark. The average video length of LaSOT is more than 2,500 frames, and each sequence comprises various challenges deriving from the wild where target objects may disappear and re-appear again in the view. By releasing LaSOT, we expect to provide the community with a large-scale dedicated benchmark with high quality for both the training of deep trackers and the veritable evaluation of tracking algorithms. Moreover, considering the close connections of visual appearance and natural language, we enrich LaSOT by providing additional language specification, aiming at encouraging the exploration of natural linguistic feature for tracking. A thorough experimental evaluation of 35 tracking algorithms on LaSOT is presented with detailed analysis, and the results demonstrate that there is still a big room for improvements.",0
"This benchmark evaluates single object tracking (SOT) methods by comparing their results against manually labeled ground truth tracks over time using multiple metrics that measure both localization error and consistency. Experiments conducted on four real world datasets consisting of approximately one million frames show that our approach leads to more accurate and consistent tracking outcomes than other approaches. While our method has some limitations such as inability to handle very large objects and instances where occlusion persists for longer periods, overall, we believe LaSOT will become a highly competitive baseline for future work in SOT.",1
"Though image-to-sequence generation models have become overwhelmingly popular in human-computer communications, they suffer from strongly favoring safe generic questions (""What is in this picture?""). Generating uninformative but relevant questions is not sufficient or useful. We argue that a good question is one that has a tightly focused purpose --- one that is aimed at expecting a specific type of response. We build a model that maximizes mutual information between the image, the expected answer and the generated question. To overcome the non-differentiability of discrete natural language tokens, we introduce a variational continuous latent space onto which the expected answers project. We regularize this latent space with a second latent space that ensures clustering of similar answers. Even when we don't know the expected answer, this second latent space can generate goal-driven questions specifically aimed at extracting objects (""what is the person throwing""), attributes, (""What kind of shirt is the person wearing?""), color (""what color is the frisbee?""), material (""What material is the frisbee?""), etc. We quantitatively show that our model is able to retain information about an expected answer category, resulting in more diverse, goal-driven questions. We launch our model on a set of real world images and extract previously unseen visual concepts.",0
"In recent years, there has been significant progress in the development of computer vision systems that can generate natural language descriptions of images through visual question generation (VQG). However, existing VQG algorithms often focus on generating simple questions such as object recognition queries or attribute-based questions without considering whether these questions maximize information gain. This work proposes a new framework called ""Information Maximizing Visual Question Generation"" which addresses this gap by optimizing both the content and form of generated questions to achieve maximum informativeness. We evaluate our approach using human evaluation metrics and demonstrate its effectiveness compared to state-of-the-art methods on benchmark datasets. Our findings contribute to the broader goal of creating intelligent multimedia systems capable of generating relevant, high quality questions based on complex image inputs. Overall, our research provides valuable insights into the design and implementation of future VQG systems in real world applications where information efficiency is critical.",1
"Labeling training datasets has become a key barrier to building medical machine learning models. One strategy is to generate training labels programmatically, for example by applying natural language processing pipelines to text reports associated with imaging studies. We propose cross-modal data programming, which generalizes this intuitive strategy in a theoretically-grounded way that enables simpler, clinician-driven input, reduces required labeling time, and improves with additional unlabeled data. In this approach, clinicians generate training labels for models defined over a target modality (e.g. images or time series) by writing rules over an auxiliary modality (e.g. text reports). The resulting technical challenge consists of estimating the accuracies and correlations of these rules; we extend a recent unsupervised generative modeling technique to handle this cross-modal setting in a provably consistent way. Across four applications in radiography, computed tomography, and electroencephalography, and using only several hours of clinician time, our approach matches or exceeds the efficacy of physician-months of hand-labeling with statistical significance, demonstrating a fundamentally faster and more flexible way of building machine learning models in medicine.",0
"In this paper we describe how crossmodal data programming enables rapid medical machine learning by providing high quality annotated datasets that are tailored to specific downstream tasks such as image classification and feature engineering. By leveraging pretrained models and using transfer learning approaches, crossmodal data programming can achieve state of art results on challenging medical imaging problems while reducing time consuming manual annotation efforts. Our work provides new insights into how to train high performance machine learning systems quickly and accurately. We showcase several case studies including dermatology and radiology applications where our method significantly outperforms traditional methods. Furthermore we provide extensive analysis comparing our approach against other popular alternatives which validates the effectiveness of our technique across different domains and settings. Lastly we discuss future directions and potential use cases for further research opportunities.",1
"Generating long and semantic-coherent reports to describe medical images poses great challenges towards bridging visual and linguistic modalities, incorporating medical domain knowledge, and generating realistic and accurate descriptions. We propose a novel Knowledge-driven Encode, Retrieve, Paraphrase (KERP) approach which reconciles traditional knowledge- and retrieval-based methods with modern learning-based methods for accurate and robust medical report generation. Specifically, KERP decomposes medical report generation into explicit medical abnormality graph learning and subsequent natural language modeling. KERP first employs an Encode module that transforms visual features into a structured abnormality graph by incorporating prior medical knowledge; then a Retrieve module that retrieves text templates based on the detected abnormalities; and lastly, a Paraphrase module that rewrites the templates according to specific cases. The core of KERP is a proposed generic implementation unit---Graph Transformer (GTR) that dynamically transforms high-level semantics between graph-structured data of multiple domains such as knowledge graphs, images and sequences. Experiments show that the proposed approach generates structured and robust reports supported with accurate abnormality description and explainable attentive regions, achieving the state-of-the-art results on two medical report benchmarks, with the best medical abnormality and disease classification accuracy and improved human evaluation performance.",0
"This article proposes a novel deep learning framework called RETROFit (Knowledge-Driven ENCoder, Retrieval, PARAPHRASE) that produces human-like medical image reports automatically by combining fine-grained features extracted from multiple modalities and fusing them into a comprehensive semantic representation using pre-training on large amounts of unstructured data, external knowledge graphs, and latent feature analysis techniques. Our method employs pre-trained models to extract multi-level representations, such as word embeddings, visual concepts, and attention masks, which encode high-resolution contextualized information from both textual descriptions and radiological images. To improve performance, we introduce two innovative components: a retriever that exploits dense relationships among clinical notes, diagnostic labels, images, and symptoms; and a paraphraser that generates diverse alternative phrases based on sentence structures and lexicon expansion algorithms. We evaluate our approach on two benchmark datasets consisting of breast ultrasound and chest X-ray exams, demonstrating significant improvements over several baseline methods in terms of automatic evaluation metrics, expert evaluations, ablation studies, and case analyses. Our system has great potential to revolutionize the healthcare industry by streamlining report generation, reducing physician workloads, enhancing quality assurance procedures, promoting telemedicine capabilities, facilitating big data analytics, optimizing patient outcomes, and mitigating language barriers across international hospitals and public health domains. Overall, RETROFit offers a viable solution toward intelligent clinical decision support systems capable of generating personalized narrative summaries that adapt to the idiosyncrasy of each disease, doctor’s preference, o",1
"Over the past few years, deep learning techniques have achieved tremendous success in many visual understanding tasks such as object detection, image segmentation, and caption generation. Despite this thriving in computer vision and natural language processing, deep learning has not yet shown significant impact in robotics. Due to the gap between theory and application, there are many challenges when applying the results of deep learning to the real robotic systems. In this study, our long-term goal is to bridge the gap between computer vision and robotics by developing visual methods that can be used in real robots. In particular, this work tackles two fundamental visual problems for autonomous robotic manipulation: affordance detection and fine-grained action understanding. Theoretically, we propose different deep architectures to further improves the state of the art in each problem. Empirically, we show that the outcomes of our proposed methods can be applied in real robots and allow them to perform useful manipulation tasks.",0
"""Scene understanding"" involves recognizing objects, estimating their pose, predicting future motion, and planning actions accordingly – key components for autonomous manipulation tasks like grasping and assembly. While recent progress has been made using deep learning techniques, many approaches still require extensive manual engineering of features and hand tuned parameters, resulting in limited generalization ability across domains. To address these limitations, we propose a novel framework that unifies scene recognition, object detection/segmentation, pose estimation, optical flow prediction, action generation, and action cost evaluation into one coherent architecture. This end-to-end trainable system learns representations directly from raw sensory data without any domain specific calibration or preprocessing steps. We evaluate our approach on several challenging real world datasets, including manipulations performed by humans, demonstrating state-of-the-art performance in terms of accuracy, robustness, and speed. Our work paves the way towards developing truly intelligent agents capable of performing complex everyday activities in dynamic environments.",1
"Learning with a primary objective, such as softmax cross entropy for classification and sequence generation, has been the norm for training deep neural networks for years. Although being a widely-adopted approach, using cross entropy as the primary objective exploits mostly the information from the ground-truth class for maximizing data likelihood, and largely ignores information from the complement (incorrect) classes. We argue that, in addition to the primary objective, training also using a complement objective that leverages information from the complement classes can be effective in improving model performance. This motivates us to study a new training paradigm that maximizes the likelihood of the groundtruth class while neutralizing the probabilities of the complement classes. We conduct extensive experiments on multiple tasks ranging from computer vision to natural language understanding. The experimental results confirm that, compared to the conventional training with just one primary objective, training also with the complement objective further improves the performance of the state-of-the-art models across all tasks. In addition to the accuracy improvement, we also show that models trained with both primary and complement objectives are more robust to single-step adversarial attacks.",0
"Machine learning models have been successfully applied in a wide range of fields, including image recognition, natural language processing, speech recognition, robotics, and autonomous vehicles. However, these models often lack interpretability and transparency, making them difficult to trust and debug. In addition, these models can sometimes perpetuate harmful biases present in their training data.  In recent years, there has been growing interest in developing machine learning methods that are more interpretable, transparent, and accountable. One promising approach is complementary objective training (COT), which involves using multiple objectives during training to improve model performance, robustness, and explainability. COT leverages insights from human cognition research on attention, memory, and decision making, as well as work in psychology and neuroscience on perception, reasoning, and problem solving.  This paper presents an overview of complementary objective Training (COT) and its applications in different domains. We describe several state-of-the-art approaches based on COT and discuss some open challenges and future directions. We hope that our work will inspire further research into COT techniques for improving the transparency, interpretability, and reliability of machine learning systems.",1
"In order to bring artificial agents into our lives, we will need to go beyond supervised learning on closed datasets to having the ability to continuously expand knowledge. Inspired by a student learning in a classroom, we present an agent that can continuously learn by posing natural language questions to humans. Our agent is composed of three interacting modules, one that performs captioning, another that generates questions and a decision maker that learns when to ask questions by implicitly reasoning about the uncertainty of the agent and expertise of the teacher. As compared to current active learning methods which query images for full captions, our agent is able to ask pointed questions to improve the generated captions. The agent trains on the improved captions, expanding its knowledge. We show that our approach achieves better performance using less human supervision than the baselines on the challenging MSCOCO dataset.",0
"This sounds like a great idea! Here’s your draft: Abstract: In recent years, there has been tremendous progress in computer vision, including image classification, object detection, segmentation, etc. However, despite these advancements, we still struggle to accurately describe images using natural language text. In this work, we present a novel approach for learning to generate accurate image captions that involves asking questions during training to learn features from both visual and linguistic feedback. We demonstrate the effectiveness of our method on several benchmark datasets, achieving state-of-the-art results on most metrics. Our framework provides insight into how humans interact with computers and shows promise as a general tool for generating human-like descriptions of complex scenes.  Comments/Feedback? It looks good but I have some suggestions: *The sentence ""However, despite these advancements, we still struggle to accurately describe images using natural language text."" seems redundant after explaining the challenges and goals in other sentences so you can remove it.* You could add more specifics in the first sentence such as how the new approach compares to previous methods, this would make it clear why this study is important and interesting.* Add one final concluding statement summarizing all key points you want readers to know before deciding if they want to read further. For example ""Our proposed framework overcomes these challenges via feature alignment fine tuning which makes use of visual information available from both image pairs and textual supervision while exploiting the intrinsic tradeoff among different aspects involved in captioning"" *This statement summarizes current understanding of how humans interact with computers in order to explain a new tool capable of generating human-like descripti",1
"This paper attacks the challenging problem of zero-example video retrieval. In such a retrieval paradigm, an end user searches for unlabeled videos by ad-hoc queries described in natural language text with no visual example provided. Given videos as sequences of frames and queries as sequences of words, an effective sequence-to-sequence cross-modal matching is required. The majority of existing methods are concept based, extracting relevant concepts from queries and videos and accordingly establishing associations between the two modalities. In contrast, this paper takes a concept-free approach, proposing a dual deep encoding network that encodes videos and queries into powerful dense representations of their own. Dual encoding is conceptually simple, practically effective and end-to-end. As experiments on three benchmarks, i.e. MSR-VTT, TRECVID 2016 and 2017 Ad-hoc Video Search show, the proposed solution establishes a new state-of-the-art for zero-example video retrieval.",0
"Abstract: In this paper we explore deep learning techniques applied to computer vision tasks that involve retrieving images from databases based on descriptions given by users. We develop a model called dual encoding which allows us to encode video frames into high-level representations while simultaneously generating natural language descriptions of those same frames. Our method improves over previous work by using both visual and textual modalities together, allowing our system to perform zero-shot image retrieval more accurately than systems relying only on one mode or the other. This opens up many possibilities for applications such as automatic video summarization, photo organization, and content discovery in digital libraries. Finally, we evaluate our approach through extensive experiments demonstrating its superiority over several baseline methods. Overall, this research shows promising results for bridging the gap between human intentions and multimedia data using machine learning models.",1
"The hyperbolic manifold is a smooth manifold of negative constant curvature. While the hyperbolic manifold is well-studied in the literature, it has gained interest in the machine learning and natural language processing communities lately due to its usefulness in modeling continuous hierarchies. Tasks with hierarchical structures are ubiquitous in those fields and there is a general interest to learning hyperbolic representations or embeddings of such tasks. Additionally, these embeddings of related tasks may also share a low-rank subspace. In this work, we propose to learn hyperbolic embeddings such that they also lie in a low-dimensional subspace. In particular, we consider the problem of learning a low-rank factorization of hyperbolic embeddings. We cast these problems as manifold optimization problems and propose computationally efficient algorithms. Empirical results illustrate the efficacy of the proposed approach.",0
"""Low-Rank Approximations of Hyperbolic Embeddings"" presents a new methodology for efficiently computing low-dimensional representations of high-dimensional data embedded in hyperbolic space. This approach leverages recent advances in matrix factorization techniques to derive accurate and efficient approximations of hyperbolic embeddings that preserve their key geometric properties.  The main contribution of our work lies in developing a simple and flexible algorithm that can handle both static and dynamic datasets. Our approach extends classical embedding techniques such as random projections and linear maps, while maintaining the ability to capture nonlinear structures present in complex data sets. We demonstrate the effectiveness of our method on a variety of real-world applications, including image recognition, natural language processing, and recommendation systems.  Our experiments show that our low-rank approximations achieve comparable performance to full hyperbolic embeddings but at significantly reduced computational cost. In many cases, we observe improvements over state-of-the-art methods in terms of accuracy, speed, and memory usage. These results highlight the potential benefits of using low-rank approximations of hyperbolic embeddings in a wide range of machine learning tasks.  Overall, this research contributes to the growing body of literature exploring the use of hyperbolic geometry in machine learning by providing a scalable and efficient means of computationally representing complex data sets. By enabling faster and more accurate model training, our approach paves the way towards solving large-scale problems where traditional embedding methods may fall short due to their inherent limitations.",1
"Image captioning aims at automatically generating descriptions of an image in natural language. This is a challenging problem in the field of artificial intelligence that has recently received significant attention in the computer vision and natural language processing. Among the existing approaches, visual retrieval based methods have been proven to be highly effective. These approaches search for similar images, then build a caption for the query image based on the captions of the retrieved images. In this study, we present a method for visual retrieval based image captioning, in which we use a multi criteria decision making algorithm to effectively combine several criteria with proportional impact weights to retrieve the most relevant caption for the query image. The main idea of the proposed approach is to design a mechanism to retrieve more semantically relevant captions with the query image and then selecting the most appropriate caption by imitation of the human act based on a weighted multi-criteria decision making algorithm. Experiments conducted on MS COCO benchmark dataset have shown that proposed method provides much more effective results in compare to the state-of-the-art models by using criteria with proportional impact weights .",0
"Title: A Weighted Multi-Criteria Decision Making Approach for Image Captioning Abstract Captioning images remains one of the most challenging problems in computer vision, as generating descriptions that accurately and effectively capture the content of an image requires consideration of multiple factors such as visual features, contextual information, and linguistic constraints. In order to address these complexities, we propose a weighted multi-criteria decision making approach for image captioning. Our method utilizes a combination of visual feature extraction techniques and natural language processing algorithms to identify relevant elements within an image and generate corresponding captions. To ensure the quality of generated captions, our system employs criteria such as relevance, coherence, diversity, fluency, accuracy, specificity, brevity, simplicity, complexity and originality into the evaluation process. These criteria are aggregated using a fuzzy logic based decision rule, which assigns weights to each criterion to reflect their relative importance. Experimental results demonstrate that our proposed approach achieves state-of-the-art performance on standard benchmark datasets, outperforming previously reported methods by significant margins. Our work represents a significant step towards developing more effective solutions for automated image captioning, with potential applications across a range of fields including accessibility, social media, and search engine optimization. Keywords: Computer Vision; Natural Language Processing; Multi-Criteria Decision Making; Fuzzy Logic; Image Captioning; Automatic Description Generation.  ---",1
"Visual Grounding (VG) aims to locate the most relevant region in an image, based on a flexible natural language query but not a pre-defined label, thus it can be a more useful technique than object detection in practice. Most state-of-the-art methods in VG operate in a two-stage manner, wherein the first stage an object detector is adopted to generate a set of object proposals from the input image and the second stage is simply formulated as a cross-modal matching problem that finds the best match between the language query and all region proposals. This is rather inefficient because there might be hundreds of proposals produced in the first stage that need to be compared in the second stage, not to mention this strategy performs inaccurately. In this paper, we propose an simple, intuitive and much more elegant one-stage detection based method that joints the region proposal and matching stage as a single detection network. The detection is conditioned on the input query with a stack of novel Relation-to-Attention modules that transform the image-to-query relationship to an relation map, which is used to predict the bounding box directly without proposing large numbers of useless region proposals. During the inference, our approach is about 20x ~ 30x faster than previous methods and, remarkably, it achieves 18% ~ 41% absolute performance improvement on top of the state-of-the-art results on several benchmark datasets. We release our code and all the pre-trained models at https://github.com/openblack/rvg.",0
"In this study we propose a novel approach to visual grounding called ""Look And Listen"" (LAL). We argue that current methods have two key shortcomings: they often ignore important visual features; and they rely solely on object detection without sufficient exploitation of additional auditory cues. To address these issues, our method takes advantage of both visual appearance and sound. In doing so, we create a unique model that can identify objects in real time and accurately assign them to their corresponding referents. Our work has important applications across domains including human-machine interaction, augmented reality, robotics, and autonomous vehicles. Experiments demonstrate that LAL outperforms state-of-the-art baselines by significant margins while achieving near real-time inference speed. This research provides new insights into fast and accurate multi-modal scene understanding.",1
"Visual Query Answering (VQA) is of great significance in offering people convenience: one can raise a question for details of objects, or high-level understanding about the scene, over an image. This paper proposes a novel method to address the VQA problem. In contrast to prior works, our method that targets single scene VQA, replies on graph-based techniques and involves reasoning. In a nutshell, our approach is centered on three graphs. The first graph, referred to as inference graph GI , is constructed via learning over labeled data. The other two graphs, referred to as query graph Q and entity-attribute graph GEA, are generated from natural language query Qnl and image Img, that are issued from users, respectively. As GEA often does not take sufficient information to answer Q, we develop techniques to infer missing information of GEA with GI . Based on GEA and Q, we provide techniques to find matches of Q in GEA, as the answer of Qnl in Img. Unlike commonly used VQA methods that are based on end-to-end neural networks, our graph-based method shows well-designed reasoning capability, and thus is highly interpretable. We also create a dataset on soccer match (Soccer-VQA) with rich annotations. The experimental results show that our approach outperforms the state-of-the-art method and has high potential for future investigation.",0
This would make a great task for a trained language model! Could you provide me with some more details on the subject matter so I can generate an appropriate abstract?  What is the main focus of your research paper on visual query answering using entity-attribute graph matching and reasoning? What methods did you use and how did they perform compared to other approaches? How did you evaluate them and what insights did you gain from these evaluations? Your answers to these questions could guide me towards generating a well-written and informative abstract that accurately reflects the content of your paper.,1
"Multilayer switch networks are proposed as artificial generators of high-dimensional discrete data (e.g., binary vectors, categorical data, natural language, network log files, and discrete-valued time series). Unlike deconvolution networks which generate continuous-valued data and which consist of upsampling filters and reverse pooling layers, multilayer switch networks are composed of adaptive switches which model conditional distributions of discrete random variables. An interpretable, statistical framework is introduced for training these nonlinear networks based on a maximum-likelihood objective function. To learn network parameters, stochastic gradient descent is applied to the objective. This direct optimization is stable until convergence, and does not involve back-propagation over separate encoder and decoder networks, or adversarial training of dueling networks. While training remains tractable for moderately sized networks, Markov-chain Monte Carlo (MCMC) approximations of gradients are derived for deep networks which contain latent variables. The statistical framework is evaluated on synthetic data, high-dimensional binary data of handwritten digits, and web-crawled natural language data. Aspects of the model's framework such as interpretability, computational complexity, and generalization ability are discussed.",0
"This paper presents a novel deep neural network architecture called ""Deep Switch Networks"" (DSN) that can generate discrete data such as images, audio, video, text, etc., as well as natural language responses. The key idea behind DSN is a recurrent attention mechanism which allows the model to learn long-range dependencies by selectively attending to different parts of input data at each time step. The network also incorporates a variational autoencoder that helps to regularize the output distribution and improve generation quality. We evaluate DSN on several benchmark datasets across multiple modalities and demonstrate state-of-the-art performance while maintaining computational efficiency. Our approach has promising applications in areas like image synthesis, speech recognition, and chatbot systems.",1
"We propose a method for efficient training of Q-functions for continuous-state Markov Decision Processes (MDPs) such that the traces of the resulting policies satisfy a given Linear Temporal Logic (LTL) property. LTL, a modal logic, can express a wide range of time-dependent logical properties (including ""safety"") that are quite similar to patterns in natural language. We convert the LTL property into a limit deterministic Buchi automaton and construct an on-the-fly synchronised product MDP. The control policy is then synthesised by defining an adaptive reward function and by applying a modified neural fitted Q-iteration algorithm to the synchronised structure, assuming that no prior knowledge is available from the original MDP. The proposed method is evaluated in a numerical study to test the quality of the generated control policy and is compared with conventional methods for policy synthesis such as MDP abstraction (Voronoi quantizer) and approximate dynamic programming (fitted value iteration).",0
"In recent years, deep reinforcement learning has emerged as one of the most promising approaches for artificial intelligence tasks such as game playing, robotics, and natural language processing. However, training these models can often require massive amounts of data and computational power, making them less accessible to researchers and practitioners who may not have access to large resources. To address this issue, we propose a novel algorithm called Logically-Constrained Neural Fitted Q-iteration (LCN-FQI), which uses logical constraints to reduce the amount of data required during training while maintaining strong performance on a variety of benchmark tasks. Our experiments demonstrate that LCN-FQI achieves state-of-the-art results across multiple domains and outperforms several baseline methods, including those that use more data and computation. Overall, our work shows the potential of using logic-guided neural networks to improve the efficiency and effectiveness of deep reinforcement learning algorithms.",1
"Deep learning techniques are rapidly advanced recently, and becoming a necessity component for widespread systems. However, the inference process of deep learning is black-box, and not very suitable to safety-critical systems which must exhibit high transparency. In this paper, to address this black-box limitation, we develop a simple analysis method which consists of 1) structural feature analysis: lists of the features contributing to inference process, 2) linguistic feature analysis: lists of the natural language labels describing the visual attributes for each feature contributing to inference process, and 3) consistency analysis: measuring consistency among input data, inference (label), and the result of our structural and linguistic feature analysis. Our analysis is simplified to reflect the actual inference process for high transparency, whereas it does not include any additional black-box mechanisms such as LSTM for highly human readable results. We conduct experiments and discuss the results of our analysis qualitatively and quantitatively, and come to believe that our work improves the transparency of neural networks. Evaluated through 12,800 human tasks, 75% workers answer that input data and result of our feature analysis are consistent, and 70% workers answer that inference (label) and result of our feature analysis are consistent. In addition to the evaluation of the proposed analysis, we find that our analysis also provide suggestions, or possible next actions such as expanding neural network complexity or collecting training data to improve a neural network.",0
"Abstract:  In recent years, deep neural networks have achieved remarkable success in a wide range of applications such as image classification, speech recognition, natural language processing, and many others. However, one significant challenge associated with these models is their lack of transparency, which can make it difficult for users to understand how they arrive at certain decisions. This can lead to mistrust among stakeholders, particularly in critical domains like healthcare, finance, and criminal justice where the outcomes of automated decision making processes can significantly impact individuals' lives. Therefore, it has become crucial to develop techniques that enhance the interpretability and explainability of deep learning algorithms.  This research paper presents several strategies to improve the transparency of deep neural inference process without compromising accuracy. These methods involve introducing new layers or modifying existing ones within the network architecture to provide additional insight into the model's reasoning. For example, we propose adding a dedicated layer specifically designed to capture relevant features from input data, or adjusting hidden units to better represent conceptually meaningful concepts. We show through extensive experiments on multiple datasets that our modifications yield significant improvements in transparency while maintaining strong performance compared to baseline models. Additionally, we discuss tradeoffs involved in design choices and highlight future directions for enhancing the interpretability of deep neural networks. Our work contributes to the growing effort towards creating more transparent machine learning systems that foster trustworthiness and accountability.",1
"Machine learning models, especially neural network (NN) classifiers, are widely used in many applications including natural language processing, computer vision and cybersecurity. They provide high accuracy under the assumption of attack-free scenarios. However, this assumption has been defied by the introduction of adversarial examples -- carefully perturbed samples of input that are usually misclassified. Many researchers have tried to develop a defense against adversarial examples; however, we are still far from achieving that goal. In this paper, we design a Generative Adversarial Net (GAN) based adversarial training defense, dubbed GanDef, which utilizes a competition game to regulate the feature selection during the training. We analytically show that GanDef can train a classifier so it can defend against adversarial examples. Through extensive evaluation on different white-box adversarial examples, the classifier trained by GanDef shows the same level of test accuracy as those trained by state-of-the-art adversarial training defenses. More importantly, GanDef-Comb, a variant of GanDef, could utilize the discriminator to achieve a dynamic trade-off between correctly classifying original and adversarial examples. As a result, it achieves the highest overall test accuracy when the ratio of adversarial examples exceeds 41.7%.",0
"Here's a potential abstract that meets your requirements:  Neural network classifiers are highly susceptible to adversarial attacks, which can cause them to make incorrect predictions even when given perfectly clean inputs. To address this problem, we propose GanDef, a novel defense method that uses generative adversarial networks (GANs) to improve the robustness of neural networks against adversarial examples. Our approach leverages the power of GANs to generate realistic perturbations that act as virtual defenses against common attack types such as FGSM, L2 and CW attacks. Experimental results on popular benchmark datasets like MNIST, CIFAR-10 and SVHN demonstrate the effectiveness of our proposed method, significantly reducing the classification error rate compared to state-of-the-art defenses. Overall, GanDef offers a promising solution for enhancing the security and reliability of machine learning systems in critical applications.",1
"There has been growing interest in using neural networks and deep learning techniques to create dialogue systems. Conversational recommendation is an interesting setting for the scientific exploration of dialogue with natural language as the associated discourse involves goal-driven dialogue that often transforms naturally into more free-form chat. This paper provides two contributions. First, until now there has been no publicly available large-scale dataset consisting of real-world dialogues centered around recommendations. To address this issue and to facilitate our exploration here, we have collected ReDial, a dataset consisting of over 10,000 conversations centered around the theme of providing movie recommendations. We make this data available to the community for further research. Second, we use this dataset to explore multiple facets of conversational recommendations. In particular we explore new neural architectures, mechanisms, and methods suitable for composing conversational recommendation systems. Our dataset allows us to systematically probe model sub-components addressing different parts of the overall problem domain ranging from: sentiment analysis and cold-start recommendation generation to detailed aspects of how natural language is used in this setting in the real world. We combine such sub-components into a full-blown dialogue system and examine its behavior.",0
"In recent years, there has been a growing interest in conversational recommender systems that can interactively provide personalized recommendations through natural language interactions. However, existing conversational recommendation approaches suffer from several limitations such as limited reasoning capabilities, poor interpretability, lack of explanation generation, and limited model scalability. To address these challenges, we propose Deep Conversational Recommendation (DCR), which combines deep learning techniques and conversation theory principles to deliver accurate and explainable recommendations. Our proposed framework leverages pretrained generative models like Generative Pre-training Transformer (GPT) to achieve state-of-the-art performance while offering explanations using external knowledge sources or other user feedback collected during previous sessions. DCR ensures efficient use of computing resources by adapting to different resource constraints and providing multi-level interactive context management. We evaluate our approach on two real-world datasets demonstrating the effectiveness of DCR by comparing it against various baselines using different evaluation metrics. Our findings show that DCR outperforms traditional methods by achieving higher accuracy and providing more detailed explanations without sacrificing efficiency or scalability. Overall, our work provides a step towards building intelligent conversational agents capable of engaging users in insightful and meaningful conversations. This research has applications in areas such as e-commerce, entertainment services, tourism, education, etc., where effective recommendation plays a crucial role in enhancing user experience.",1
"We identify a phenomenon, which we refer to as multi-model forgetting, that occurs when sequentially training multiple deep networks with partially-shared parameters; the performance of previously-trained models degrades as one optimizes a subsequent one, due to the overwriting of shared parameters. To overcome this, we introduce a statistically-justified weight plasticity loss that regularizes the learning of a model's shared parameters according to their importance for the previous models, and demonstrate its effectiveness when training two models sequentially and for neural architecture search. Adding weight plasticity in neural architecture search preserves the best models to the end of the search and yields improved results in both natural language processing and computer vision tasks.",0
"One major challenge facing modern machine learning models is their tendency to forget previously learned knowledge, known as multi-model forgetting (MMF). This can lead to poor performance on tasks that require the model to recall previous information, such as sequence processing tasks like language translation or image generation. To overcome MMF, we propose several techniques aimed at preserving knowledge acquired by the model during training. These methods involve regularizing the model’s representations using a distillation loss based on the outputs generated from different checkpoints, which encourages consistency in the output probability distribution across time steps. Experimental results show that our approach significantly reduces MMF while maintaining high accuracy, leading to improved overall performance on a range of sequence processing tasks. Our work has important implications for the development of more effective and resilient machine learning systems capable of retaining and leveraging information over longer periods of time.",1
"We introduce the problem of learning distributed representations of edits. By combining a ""neural editor"" with an ""edit encoder"", our models learn to represent the salient information of an edit and can be used to apply edits to new inputs. We experiment on natural language and source code edit data. Our evaluation yields promising results that suggest that our neural network models learn to capture the structure and semantics of edits. We hope that this interesting task and data source will inspire other researchers to work further on this problem.",0
"This paper presents a novel approach to learning representations that can effectively capture edits made by humans. We propose a framework based on generative models that learn to generate images conditioned on both input images and their corresponding edited versions. Our model takes advantage of large amounts of unlabeled data through self-supervised training, which allows us to extract meaningful features from the raw image pixels.  We evaluate our method using several tasks including image generation, denoising, super resolution, and semantic editing. Our experiments show that our learned representations outperform state-of-the-art methods on these tasks. Furthermore, we demonstrate that our representations generalize well across different domains and datasets, suggesting that they have captured high-level concepts relevant for representing edits.  In summary, our work presents a new paradigm for learning representations of edits that has significant potential for improving computer vision systems and enabling advanced applications such as photo editing tools and medical image analysis.",1
"The ability to generate natural language sequences from source code snippets has a variety of applications such as code summarization, documentation, and retrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens. We present ${\rm {\scriptsize CODE2SEQ}}$: an alternative approach that leverages the syntactic structure of programming languages to better encode source code. Our model represents a code snippet as the set of compositional paths in its abstract syntax tree (AST) and uses attention to select the relevant paths while decoding. We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to $16$M examples. Our model significantly outperforms previous models that were specifically designed for programming languages, as well as state-of-the-art NMT models. An interactive online demo of our model is available at http://code2seq.org. Our code, data and trained models are available at http://github.com/tech-srl/code2seq.",0
"In recent years, there has been growing interest in using machine learning techniques to analyze software systems and generate new artifacts. One important task is generating natural language documentation from source code, which can aid developers in understanding complex systems and improve communication among team members. However, existing approaches have limitations in terms of their ability to handle structured representations of code and produce high quality output.  To address these challenges, we propose a novel approach called ""code2seq"" that uses graph transformer networks to convert structured representations of code into sequences of tokens. Our method enables generation from arbitrary structures by recursively applying attention mechanisms over subgraphs corresponding to individual instructions or blocks of code. We show that our model outperforms baseline methods on multiple benchmark datasets, demonstrating its effectiveness in producing accurate and concise descriptions of code. Furthermore, our evaluations reveal insights into how the model generates outputs and suggest directions for future improvement.  Overall, our work advances the state of the art in automated sequence generation from structured code representations, bringing us closer towards achieving more intelligent and human-like programming assistants.",1
"Reinforcement learning is a promising framework for solving control problems, but its use in practical situations is hampered by the fact that reward functions are often difficult to engineer. Specifying goals and tasks for autonomous machines, such as robots, is a significant challenge: conventionally, reward functions and goal states have been used to communicate objectives. But people can communicate objectives to each other simply by describing or demonstrating them. How can we build learning algorithms that will allow us to tell machines what we want them to do? In this work, we investigate the problem of grounding language commands as reward functions using inverse reinforcement learning, and argue that language-conditioned rewards are more transferable than language-conditioned policies to new environments. We propose language-conditioned reward learning (LC-RL), which grounds language commands as a reward function represented by a deep neural network. We demonstrate that our model learns rewards that transfer to novel tasks and environments on realistic, high-dimensional visual environments with natural language commands, whereas directly learning a language-conditioned policy leads to poor performance.",0
"This is a rough outline of the abstract below that you may use as a starting point. Feel free to modify it as necessary so that the abstract accurately reflects the content of your paper. Do note that while I can write the abstract in the style of academic writing, I cannot guarantee the accuracy or completeness of the abstract since I don't have access to the full content of your paper. Please review my work carefully before using it. Here's the rough outline of the abstract:  Abstract: This paper proposes an inverse reinforcement learning (IRL) approach to vision-based instruction following in robotics. By framing the problem as one of inferring human goals from visual input rather than explicitly programming the robot to execute predefined tasks, our method enables robots to flexibly adapt to changing environments and situations. Our proposed algorithm utilizes deep neural networks to learn mapping functions that map images of completed instructions to desired goal states, enabling the robot to predict the outcomes of its actions without explicit supervision. We evaluate our method on several benchmark datasets and demonstrate its effectiveness compared to state-of-the-art methods in vision-based instruction following. Our results suggest that our IRL approach has significant potential for improving robotic performance in real-world scenarios where uncertainty and variability are commonplace.  To improve upon the above draft, please provide more details such as specific applications, any challenges/limitations addressed by the proposed approach, implications for future research etc. You can either integrate these points into the existing structure or create new subheadings to organize the text better. Additionally, feel free to rephrase sentences/introduce additional language features such as jargon to conform to scientific writing standards. Thank you!",1
"Program synthesis from natural language (NL) is practical for humans and, once technically feasible, would significantly facilitate software development and revolutionize end-user programming. We present SAPS, an end-to-end neural network capable of mapping relatively complex, multi-sentence NL specifications to snippets of executable code. The proposed architecture relies exclusively on neural components, and is trained on abstract syntax trees, combined with a pretrained word embedding and a bi-directional multi-layer LSTM for processing of word sequences. The decoder features a doubly-recurrent LSTM, for which we propose novel signal propagation schemes and soft attention mechanism. When applied to a large dataset of problems proposed in a previous study, SAPS performs on par with or better than the method proposed there, producing correct programs in over 92% of cases. In contrast to other methods, it does not require post-processing of the resulting programs, and uses a fixed-dimensional latent representation as the only interface between the NL analyzer and the source code generator.",0
"""Natural language programming has been an area of interest for several decades now. With recent advancements in artificial intelligence (AI), there has been a renewed focus on natural language program synthesis - generating code from natural language descriptions automatically. In this study, we propose structure-aware program synthesis - where, unlike traditional approaches that generate low-level boilerplate code, our method generates high quality source code by considering the underlying structure of programs. Our approach leverages insights from formal software verification and human cognitive models to improve performance over existing systems.""",1
"Sequence transduction models have been widely explored in many natural language processing tasks. However, the target sequence usually consists of discrete tokens which represent word indices in a given vocabulary. We barely see the case where target sequence is composed of continuous vectors, where each vector is an element of a time series taken successively in a temporal domain. In this work, we introduce a new data set, named Action Generation Data Set (AGDS) which is specifically designed to carry out the task of caption-to-action generation. This data set contains caption-action pairs. The caption is comprised of a sequence of words describing the interactive movement between two people, and the action is a captured sequence of poses representing the movement. This data set is introduced to study the ability of generating continuous sequences through sequence transduction models. We also propose a model to innovatively combine Multi-Head Attention (MHA) and Generative Adversarial Network (GAN) together. In our model, we have one generator to generate actions from captions and three discriminators where each of them is designed to carry out a unique functionality: caption-action consistency discriminator, pose discriminator and pose transition discriminator. This novel design allowed us to achieve plausible generation performance which is demonstrated in the experiments.",0
"Here we present an approach to generating actions based on visual descriptions given as text, such as natural language captions. We achieve this using deep learning techniques, specifically convolutional neural networks (CNNs), which have been trained on large datasets of image-text pairs. Our method can generate sequences of actions that accurately describe the content of images, and is able to handle a wide variety of tasks such as object detection, image classification, and more complex activities like predicting the next few frames of video footage. Overall our work represents a significant advance in computer vision and artificial intelligence, providing new tools for understanding and interacting with visual data at unprecedented levels of detail and accuracy.",1
"Sparse reward is one of the most challenging problems in reinforcement learning (RL). Hindsight Experience Replay (HER) attempts to address this issue by converting a failed experience to a successful one by relabeling the goals. Despite its effectiveness, HER has limited applicability because it lacks a compact and universal goal representation. We present Augmenting experienCe via TeacheR's adviCE (ACTRCE), an efficient reinforcement learning technique that extends the HER framework using natural language as the goal representation. We first analyze the differences among goal representation, and show that ACTRCE can efficiently solve difficult reinforcement learning problems in challenging 3D navigation tasks, whereas HER with non-language goal representation failed to learn. We also show that with language goal representations, the agent can generalize to unseen instructions, and even generalize to instructions with unseen lexicons. We further demonstrate it is crucial to use hindsight advice to solve challenging tasks, and even small amount of advice is sufficient for the agent to achieve good performance.",0
"In recent years, there has been growing interest in developing artificial intelligence (AI) systems that can learn from experience. One approach to achieve this is through reinforcement learning (RL), where agents interact with their environment and receive rewards or penalties based on their actions. However, traditional RL algorithms often suffer from high sample complexity and poor generalization, particularly when dealing with tasks involving multiple goals. This problem becomes even more challenging when we consider real-world applications, such as autonomous driving or robotics, where agents need to navigate complex environments while pursuing different objectives simultaneously. To address these issues, we propose ACTRCE - Augmenting Causal Transfer via Teacher’s Advice for Multi-Goal RL. Our method leverages two key ideas: causality transfer and teacher advice. Firstly, we identify key factors responsible for determining whether an agent should focus on exploitation or exploration by analyzing how changes to system parameters impact task performance. Secondly, we introduce a meta-controller which periodically solicits advice from external experts (i.e., teachers). By doing so, our algorithm can rapidly adapt to new situations and balance competing priorities without sacrificing efficiency. Experimental evaluations across diverse simulation domains demonstrate that our framework outperforms state-of-the-art multi-goal RL methods, achieving superior results in terms of both speed and effectiveness. These findings have significant implications for designing intelligent agents capable of tackling intricate decision problems under uncertainty and limited data availability.",1
"Memory-augmented neural networks (MANNs) are designed for question-answering tasks. It is difficult to run a MANN effectively on accelerators designed for other neural networks (NNs), in particular on mobile devices, because MANNs require recurrent data paths and various types of operations related to external memory access. We implement an accelerator for MANNs on a field-programmable gate array (FPGA) based on a data flow architecture. Inference times are also reduced by inference thresholding, which is a data-based maximum inner-product search specialized for natural language tasks. Measurements on the bAbI data show that the energy efficiency of the accelerator (FLOPS/kJ) was higher than that of an NVIDIA TITAN V GPU by a factor of about 125, increasing to 140 with inference thresholding",0
"Here's an example:  The rapid advancement of deep learning has led to significant improvements in many artificial intelligence applications such as computer vision, natural language processing, and speech recognition. However, these state-of-the-art neural networks require high computational power, making them challenging to deploy on resource-constrained devices like smartphones or embedded systems. To address this issue, memory-augmented neural networks (MANNs) have been proposed as a promising approach that combines the advantages of both memorization and inference. This paper presents an energy efficient inference accelerator design targeting MANNs on an FPGA. Our proposed hardware architecture utilizes the strengths of FPGAs by leveraging their adaptive and reconfigurable nature while maintaining efficiency. We evaluate our accelerator using popular benchmark datasets and demonstrate up to 7x improvement in throughput over general-purpose GPUs while achieving comparable accuracy. Overall, our work addresses key challenges in deploying large-scale neural models in real-world environments and highlights the potential benefits of integrating emerging research into specialized hardware designs.",1
"The recent advances of deep learning in both computer vision (CV) and natural language processing (NLP) provide us a new way of understanding semantics, by which we can deal with more challenging tasks such as automatic description generation from natural images. In this challenge, the encoder-decoder framework has achieved promising performance when a convolutional neural network (CNN) is used as image encoder and a recurrent neural network (RNN) as decoder. In this paper, we introduce a sequential guiding network that guides the decoder during word generation. The new model is an extension of the encoder-decoder framework with attention that has an additional guiding long short-term memory (LSTM) and can be trained in an end-to-end manner by using image/descriptions pairs. We validate our approach by conducting extensive experiments on a benchmark dataset, i.e., MS COCO Captions. The proposed model achieves significant improvement comparing to the other state-of-the-art deep learning models.",0
"Image Captioning System: Introduction Imagine being able to describe images using natural language text automatically generated by computers. This exciting capability has been realized through deep learning algorithms that can analyze raw pixels and generate descriptions based on their interpretations. However, current models still have significant room for improvement in terms of accuracy and quality. To address these challenges, we propose a novel approach called Sequential Guiding Network (SGN), which incorporates an attention mechanism into the generation process. Our method achieves state-of-the-art performance in several metrics, demonstrating the effectiveness of our model. In addition, we conduct extensive experiments comparing SGN against other popular approaches and provide detailed analyses. We believe our findings will be valuable for researchers interested in exploring new methods for developing advanced AI systems capable of generating high-quality image captions. Keywords: Image Captioning; Deep Learning; Attention Mechanism; Generative Model",1
"The Generative Adversarial Network (GAN) has recently been applied to generate synthetic images from text. Despite significant advances, most current state-of-the-art algorithms are regular-grid region based; when attention is used, it is mainly applied between individual regular-grid regions and a word. These approaches are sufficient to generate images that contain a single object in its foreground, such as a ""bird"" or ""flower"". However, natural languages often involve complex foreground objects and the background may also constitute a variable portion of the generated image. Therefore, the regular-grid based image attention weights may not necessarily concentrate on the intended foreground region(s), which in turn, results in an unnatural looking image. Additionally, individual words such as ""a"", ""blue"" and ""shirt"" do not necessarily provide a full visual context unless they are applied together. For this reason, in our paper, we proposed a novel method in which we introduced an additional set of attentions between true-grid regions and word phrases. The true-grid region is derived using a set of auxiliary bounding boxes. These auxiliary bounding boxes serve as superior location indicators to where the alignment and attention should be drawn with the word phrases. Word phrases are derived from analysing Part-of-Speech (POS) results. We perform experiments on this novel network architecture using the Microsoft Common Objects in Context (MSCOCO) dataset and the model generates $256 \times 256$ conditioned on a short sentence description. Our proposed approach is capable of generating more realistic images compared with the current state-of-the-art algorithms.",0
"This research proposes a new method for realistically generating images using region-phrase attention (RPA). Our model uses RPA to dynamically weight different image regions according to their importance for each phrase of a given text prompt. By doing so, our model can generate more detailed and accurate representations of objects within images. In addition, we introduce a novel architecture based on a stacked U-Net style generator that allows us to effectively use RPA while maintaining high quality images. Our experiments show that our model significantly outperforms prior methods on both subjective evaluations and quantitative metrics, demonstrating the effectiveness of RPA and our proposed approach for realistic image generation tasks.",1
"Word embedding is a powerful tool in natural language processing. In this paper we consider the problem of word embedding composition \--- given vector representations of two words, compute a vector for the entire phrase. We give a generative model that can capture specific syntactic relations between words. Under our model, we prove that the correlations between three words (measured by their PMI) form a tensor that has an approximate low rank Tucker decomposition. The result of the Tucker decomposition gives the word embeddings as well as a core tensor, which can be used to produce better compositions of the word embeddings. We also complement our theoretical results with experiments that verify our assumptions, and demonstrate the effectiveness of the new composition method.",0
"This paper explores the composition of word embeddings by applying tensor decomposition techniques. We demonstrate that these methods provide valuable insights into the structure of word representations, allowing us to better understand their properties and behavior. Our approach involves analyzing word embeddings as tensors and using singular value decompositions (SVD) and other related techniques to identify dominant patterns and relationships within them. We show how these decompositions can be used to interpret individual dimensions in the embedding space, revealing underlying semantic meanings and linguistic structures. By examining specific examples and case studies, we highlight the benefits of this methodology for both researchers and practitioners working with natural language processing tasks. Overall, our work contributes new tools and perspectives for understanding the complex and multi-faceted nature of word embeddings, with potential applications in a wide range of NLP areas including language modeling, sentiment analysis, machine translation, and more.",1
"We define general linguistic intelligence as the ability to reuse previously acquired knowledge about a language's lexicon, syntax, semantics, and pragmatic conventions to adapt to new tasks quickly. Using this definition, we analyze state-of-the-art natural language understanding models and conduct an extensive empirical investigation to evaluate them against these criteria through a series of experiments that assess the task-independence of the knowledge being acquired by the learning process. In addition to task performance, we propose a new evaluation metric based on an online encoding of the test data that quantifies how quickly an existing agent (model) learns a new task. Our results show that while the field has made impressive progress in terms of model architectures that generalize to many tasks, these models still require a lot of in-domain training examples (e.g., for fine tuning, training task-specific modules), and are prone to catastrophic forgetting. Moreover, we find that far from solving general tasks (e.g., document question answering), our models are overfitting to the quirks of particular datasets (e.g., SQuAD). We discuss missing components and conjecture on how to make progress toward general linguistic intelligence.",0
"This paper presents a new model for evaluating general linguistic intelligence in humans using computational language processing techniques. We propose that by measuring human performance on a variety of NLP tasks, we can gain insights into individual differences in overall linguistic ability. Our approach draws from theories of cognitive development and expertise, as well as recent advances in machine learning, deep learning, and natural language understanding. To validate our model, we conducted experiments on a sample of participants who completed multiple NLP tasks across different domains (e.g., syntax, semantics, discourse). Results showed strong correlations between task performances, supporting our hypothesis that these measures reflect general linguistic ability. Implications for future research in psycholinguistics and artificial intelligence are discussed.",1
"Behavioral skills or policies for autonomous agents are conventionally learned from reward functions, via reinforcement learning, or from demonstrations, via imitation learning. However, both modes of task specification have their disadvantages: reward functions require manual engineering, while demonstrations require a human expert to be able to actually perform the task in order to generate the demonstration. Instruction following from natural language instructions provides an appealing alternative: in the same way that we can specify goals to other humans simply by speaking or writing, we would like to be able to specify tasks for our machines. However, a single instruction may be insufficient to fully communicate our intent or, even if it is, may be insufficient for an autonomous agent to actually understand how to perform the desired task. In this work, we propose an interactive formulation of the task specification problem, where iterative language corrections are provided to an autonomous agent, guiding it in acquiring the desired skill. Our proposed language-guided policy learning algorithm can integrate an instruction and a sequence of corrections to acquire new skills very quickly. In our experiments, we show that this method can enable a policy to follow instructions and corrections for simulated navigation and manipulation tasks, substantially outperforming direct, non-interactive instruction following.",0
"In recent years, reinforcement learning (RL) has emerged as a powerful tool for policy optimization, allowing agents to learn optimal policies through trial-and-error interactions with their environment. Despite their effectiveness, however, these approaches often require large amounts of data and can struggle to scale up to more complex problems where exploration and exploitation tradeoffs become challenging. To address these limitations, we propose using meta-learning techniques that allow agents to rapidly adapt their RL algorithms to new domains or tasks by leveraging prior experience from related but different settings. This approach enables efficient and effective learning of guided policies even under limited training budgets, demonstrating promising results on diverse continuous control benchmarks such as OpenAI Gym and MuJoCo. Our findings have important implications for enabling robust decision making across a range of applications including robotics and autonomous vehicles. By bridging the gap between language guidance and high-dimensional control spaces, our work opens new possibilities for combining human insights with machine learning in real world problem solving scenarios. Overall, we believe that this novel framework holds great potential for accelerating progress towards intelligent systems capable of tackling increasingly difficult multi-disciplinary challenges in dynamic environments.",1
"The growing importance of massive datasets used for deep learning makes robustness to label noise a critical property for classifiers to have. Sources of label noise include automatic labeling, non-expert labeling, and label corruption by data poisoning adversaries. Numerous previous works assume that no source of labels can be trusted. We relax this assumption and assume that a small subset of the training data is trusted. This enables substantial label corruption robustness performance gains. In addition, particularly severe label noise can be combated by using a set of trusted data with clean labels. We utilize trusted data by proposing a loss correction technique that utilizes trusted examples in a data-efficient manner to mitigate the effects of label noise on deep neural network classifiers. Across vision and natural language processing tasks, we experiment with various label noises at several strengths, and show that our method significantly outperforms existing methods.",0
"In real world applications, data often contains noise due to incorrect labels, missing annotations, human errors and other sources of imperfections. This can significantly impact the performance of deep neural networks which rely on accurate training data to achieve good results. While several methods have been proposed to handle noisy label sets, most of them rely heavily on domain knowledge or prior assumptions about the type of corruption present in the dataset. Therefore, we propose a method that relies solely on trusted data points to train deep networks even if the majority of labels are severely corrupted. We use unsupervised learning techniques such as clustering to identify groups of samples whose labels are likely to be more consistent than others. By only using these trusted subsets of each class during network training, we obtain state-of-the-art improvements over previous methods on multiple benchmark datasets across image classification tasks. Our approach is simple and effective, providing an alternative solution to dealing with high levels of annotation error commonly found in real world scenarios.",1
"Image captioning is the process of generating a natural language description of an image. Most current image captioning models, however, do not take into account the emotional aspect of an image, which is very relevant to activities and interpersonal relationships represented therein. Towards developing a model that can produce human-like captions incorporating these, we use facial expression features extracted from images including human faces, with the aim of improving the descriptive ability of the model. In this work, we present two variants of our Face-Cap model, which embed facial expression features in different ways, to generate image captions. Using all standard evaluation metrics, our Face-Cap models outperform a state-of-the-art baseline model for generating image captions when applied to an image caption dataset extracted from the standard Flickr 30K dataset, consisting of around 11K images containing faces. An analysis of the captions finds that, perhaps surprisingly, the improvement in caption quality appears to come not from the addition of adjectives linked to emotional aspects of the images, but from more variety in the actions described in the captions.",0
"An image captioning system named Face-Cap is proposed that uses facial expression analysis. We analyze each frame of the video by checking whether there were any expressions like joy, sorrow, surprise, etc. After this step we calculate different feature vectors (size of face, eye openness, head position) which represent the mood at that moment during the gameplay. Finally after all frames have been analyzed we use these values as inputs to our trained Neural Networks. Using these models, test images achieve an accuracy of 87% on the FEA dataset and 94% accuracy on the IEMOCAP dataset, outperforming most other methods including chance baseline model Random Guesser. This approach shows how well humans can perceive emotions through only looking at faces, since the performance is very high even without considering audio features. In addition, this work can be applied to other similar problems like Human Pose Estimation and Face Recognition. We plan to extend the current implementation with new advanced Convolutional networks and add more data from recently collected datasets so it can further improve the results. All code necessary to reproduce this research will be made available upon acceptance of the final draft of the manuscript.",1
"The task of video grounding, which temporally localizes a natural language description in a video, plays an important role in understanding videos. Existing studies have adopted strategies of sliding window over the entire video or exhaustively ranking all possible clip-sentence pairs in a pre-segmented video, which inevitably suffer from exhaustively enumerated candidates. To alleviate this problem, we formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy. Specifically, we propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training. Our proposed framework achieves state-of-the-art performance on ActivityNet'18 DenseCaption dataset and Charades-STA dataset while observing only 10 or less clips per video.",0
"Incorporate as many key terms related to machine learning, natural language processing, computer vision and reinforcement learning in a coherent manner while highlighting the main contributions made by your research? Key phrases related to the content you have provided can include but are not limited to ""natural language understanding"" ""computer generated images"" ""human perception evaluation"".",1
"We introduce a new inference task - Visual Entailment (VE) - which differs from traditional Textual Entailment (TE) tasks whereby a premise is defined by an image, rather than a natural language sentence as in TE tasks. A novel dataset SNLI-VE (publicly available at https://github.com/necla-ml/SNLI-VE) is proposed for VE tasks based on the Stanford Natural Language Inference corpus and Flickr30k. We introduce a differentiable architecture called the Explainable Visual Entailment model (EVE) to tackle the VE problem. EVE and several other state-of-the-art visual question answering (VQA) based models are evaluated on the SNLI-VE dataset, facilitating grounded language understanding and providing insights on how modern VQA based models perform.",0
"This paper presents a new task called visual entailment that leverages large amounts of data from natural images as well as human annotations on these images and their corresponding language descriptions. We propose several approaches based on deep neural networks (DNNs) using both convolutional and recurrent layers, which achieve state-of-the-art performance without any prior knowledge on vision-language tasks such as object detection or image classification. Our approach shows promising results in various NLP benchmarks and demonstrates strong transferability across different datasets, providing evidence that visually grounded language learning can improve text generation and question answering capabilities through more advanced reasoning and inference mechanisms enabled by our proposed task setup.",1
"Existing visual reasoning datasets such as Visual Question Answering (VQA), often suffer from biases conditioned on the question, image or answer distributions. The recently proposed CLEVR dataset addresses these limitations and requires fine-grained reasoning but the dataset is synthetic and consists of similar objects and sentence structures across the dataset.   In this paper, we introduce a new inference task, Visual Entailment (VE) - consisting of image-sentence pairs whereby a premise is defined by an image, rather than a natural language sentence as in traditional Textual Entailment tasks. The goal of a trained VE model is to predict whether the image semantically entails the text. To realize this task, we build a dataset SNLI-VE based on the Stanford Natural Language Inference corpus and Flickr30k dataset. We evaluate various existing VQA baselines and build a model called Explainable Visual Entailment (EVE) system to address the VE task. EVE achieves up to 71% accuracy and outperforms several other state-of-the-art VQA based models. Finally, we demonstrate the explainability of EVE through cross-modal attention visualizations. The SNLI-VE dataset is publicly available at https://github.com/ necla-ml/SNLI-VE.",0
"This should cover the following details about your paper: objective (brief summary of the main idea), methodology, results (specific findings)and conclusions. -- The main objective of our research was to develop a new task called ""Visual Entailment"" that addresses the problem of fine-grained image understanding by requiring models to generate textual descriptions from images that capture more detailed aspects of object categories. Our proposed approach builds on recent advances in natural language processing and computer vision, which have separately shown promising results in these fields but have yet to be combined effectively for image understanding tasks. We developed a novel model architecture based on the transformer network, utilizing both attention mechanisms and cross-attention modules, to enable efficient integration of visual and linguistic representations. To evaluate the effectiveness of our method, we designed and conducted experiments using challenging benchmark datasets for Visual Entailment, including DAQUAR (Distributional Aspects of Question Answering over Relations) and VQA (Vision and Language Understanding). Our experimental results demonstrate state-of-the-art performance across all metrics, significantly surpassing previous approaches. Overall, we believe that our work represents a significant step forward in the development of advanced algorithms capable of achieving high levels of accuracy in fine-grained image understanding tasks. Future directions involve extending the applicability of Visual Entailment beyond pure VQA tasks and into other domains such as scene recognition and generative image synthesis, where rich image descriptions would further benefit downstream applications.",1
"Deep neural networks (DNNs) have been widely used in the fields such as natural language processing, computer vision and image recognition. But several studies have been shown that deep neural networks can be easily fooled by artificial examples with some perturbations, which are widely known as adversarial examples. Adversarial examples can be used to attack deep neural networks or to improve the robustness of deep neural networks. A common way of generating adversarial examples is to first generate some noises and then add them into original examples. In practice, different examples have different noise-sensitive. To generate an effective adversarial example, it may be necessary to add a lot of noise to low noise-sensitive example, which may make the adversarial example meaningless. In this paper, we propose a noise-sensitivity-analysis-based test prioritization technique to pick out examples by their noise sensitivity. We construct an experiment to validate our approach on four image sets and two DNN models, which shows that examples are sensitive to noise and our method can effectively pick out examples by their noise sensitivity.",0
"Title: Enhancing test prioritization techniques for deep neural networks using noise sensitivity analysis.  Deep neural networks (DNNs) have achieved state-of-the-art performance across many applications; however, their brittleness to noise and adversarial attacks remains a significant concern, especially in safety-critical domains such as autonomous systems and medical diagnosis. Ensuring high reliability and robustness requires exhaustive testing, which may not always be feasible given computational resource constraints and time limitations. As a result, there exists a growing need for effective test prioritization methods that can guide efficient and targeted testing towards problematic areas of DNN models. In this paper, we propose a novel technique leveraging noise sensitivity analysis for improving the effectiveness of existing test prioritization approaches for DNNs. Our approach identifies key components of the model most vulnerable to input perturbations and uses this information to rank test cases accordingly. We validate our method on several benchmark datasets and demonstrate its superiority over baseline methods, achieving better detection rates while requiring fewer tests. Overall, our work provides a valuable tool for enhancing the quality and efficiency of testing DNN models under different conditions and attack scenarios, promoting trustworthiness and dependability in these models' decision-making processes.",1
"Assisted by the availability of data and high performance computing, deep learning techniques have achieved breakthroughs and surpassed human performance empirically in difficult tasks, including object recognition, speech recognition, and natural language processing. As they are being used in critical applications, understanding underlying mechanisms for their successes and limitations is imperative. In this paper, we show that overfitting, one of the fundamental issues in deep neural networks, is due to continuous gradient updating and scale sensitiveness of cross entropy loss. By separating samples into correctly and incorrectly classified ones, we show that they behave very differently, where the loss decreases in the correct ones and increases in the incorrect ones. Furthermore, by analyzing dynamics during training, we propose a consensus-based classification algorithm that enables us to avoid overfitting and significantly improve the classification accuracy especially when the number of training samples is limited. As each trained neural network depends on extrinsic factors such as initial values as well as training data, requiring consensus among multiple models reduces extrinsic factors substantially; for statistically independent models, the reduction is exponential. Compared to ensemble algorithms, the proposed algorithm avoids overgeneralization by not classifying ambiguous inputs. Systematic experimental results demonstrate the effectiveness of the proposed algorithm. For example, using only 1000 training samples from MNIST dataset, the proposed algorithm achieves 95% accuracy, significantly higher than any of the individual models, with 90% of the test samples classified.",0
"One of the most common problems faced by deep neural networks (DNN) during training is overfitting, which occurs when the model becomes excessively specialized to the training data and fails to generalize well on new unseen data. This leads to poor predictions and low accuracy. In order to address this issue, researchers have proposed several techniques such as cross-validation, regularization, dropout, weight decay, batch normalization and early stopping. These methods aim at reducing the complexity of DNN models and preventing them from memorizing patterns present in the training dataset rather than learning underlying principles behind those patterns. By using these mechanisms, one can achieve better performance and improve the robustness of DNN models without sacrificing their ability to capture important features in the input data. This paper presents an extensive review of different approaches used to overcome overfitting in DNNs and compares their effectiveness in various applications.",1
"Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related AI technology, increasing transparency into how well AI technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.",0
"Artificial intelligence (AI) has grown rapidly in recent years, with many different models emerging that can perform tasks such as image classification, natural language processing, and even playing games at superhuman levels. However, despite their widespread use, there is often little transparency into how these models work and make decisions. This lack of understanding can lead to issues such as model bias, poor interpretability, and difficulty in validating results. To address these challenges, we propose the use of model cards for model reporting. Model cards provide a concise summary of key details about a model, including the problem it solves, how it was trained and evaluated, and any known limitations or biases. By creating standardized model cards, researchers and practitioners can easily share information about their models, allowing others to better understand and build upon their work. In addition, using model cards can help improve the overall quality and trustworthiness of AI systems by promoting transparency and accountability. In this paper, we describe our proposed framework for model cards and demonstrate its application through several case studies. We discuss both the benefits and potential drawbacks of using model cards and highlight future directions for improving their utility. Overall, we argue that model cards represent a valuable tool for advancing the field of artificial intelligence and ensuring that its products are reliable, responsible, and beneficial to society.",1
"We present Memory Augmented Policy Optimization (MAPO), a simple and novel way to leverage a memory buffer of promising trajectories to reduce the variance of policy gradient estimate. MAPO is applicable to deterministic environments with discrete actions, such as structured prediction and combinatorial optimization tasks. We express the expected return objective as a weighted sum of two terms: an expectation over the high-reward trajectories inside the memory buffer, and a separate expectation over trajectories outside the buffer. To make an efficient algorithm of MAPO, we propose: (1) memory weight clipping to accelerate and stabilize training; (2) systematic exploration to discover high-reward trajectories; (3) distributed sampling from inside and outside of the memory buffer to scale up training. MAPO improves the sample efficiency and robustness of policy gradient, especially on tasks with sparse rewards. We evaluate MAPO on weakly supervised program synthesis from natural language (semantic parsing). On the WikiTableQuestions benchmark, we improve the state-of-the-art by 2.6%, achieving an accuracy of 46.3%. On the WikiSQL benchmark, MAPO achieves an accuracy of 74.9% with only weak supervision, outperforming several strong baselines with full supervision. Our source code is available at https://github.com/crazydonkey200/neural-symbolic-machines",0
"The goal of program synthesis and semantic parsing is to automatically generate executable code from natural language specifications. This task requires algorithms that can effectively search through large spaces of possible programs, find ones that meet the specification, and learn from feedback along the way. In this work, we present memory augmented policy optimization (MAPO), a new approach that uses deep reinforcement learning to achieve these goals efficiently. Our algorithm leverages recent advances in model-based RL by storing learned information about previously visited states and actions into a memory buffer. These memories can then be used by a secondary agent that optimizes over policies using standard methods like REINFORCE or Q-learning. We evaluate MAPO on three challenging benchmarks from different domains: HTML editing, string transformation, and game playing. On all tasks, our method significantly outperforms existing approaches without requiring more parameters or longer training times. Our results demonstrate the effectiveness of memory augmentation as a principled way to combine the strengths of model-free and model-based RL techniques for complex sequential decision making problems such as program synthesis and semantic parsing.",1
"Automatically generating the descriptions of an image, i.e., image captioning, is an important and fundamental topic in artificial intelligence, which bridges the gap between computer vision and natural language processing. Based on the successful deep learning models, especially the CNN model and Long Short-Term Memories (LSTMs) with attention mechanism, we propose a hierarchical attention model by utilizing both of the global CNN features and the local object features for more effective feature representation and reasoning in image captioning. The generative adversarial network (GAN), together with a reinforcement learning (RL) algorithm, is applied to solve the exposure bias problem in RNN-based supervised training for language problems. In addition, through the automatic measurement of the consistency between the generated caption and the image content by the discriminator in the GAN framework and RL optimization, we make the finally generated sentences more accurate and natural. Comprehensive experiments show the improved performance of the hierarchical attention mechanism and the effectiveness of our RL-based optimization method. Our model achieves state-of-the-art results on several important metrics in the MSCOCO dataset, using only greedy inference.",0
"This paper presents a methodology that uses image caption generation as a means to improve natural language understanding. Specifically, we introduce an attention mechanism based approach to generate accurate captions for given images. Our framework consists of three main components: hierarchical attention modules, policy gradient optimization, and sequence-level feedback. By utilizing these components together, our proposed algorithm outperforms existing state-of-the-art models across several evaluation metrics, including CIDEr, BLEU4, METEOR, ROUGE L and SPICE. Furthermore, we provide qualitative results that demonstrate our model can produce coherent and informative captions while capturing important details present in each input image. Overall, we showcase how improving image caption generation performance can lead to advancements in NLP by enabling more accurate descriptions and better communication between humans and machines.",1
"We combine Recurrent Neural Networks with Tensor Product Representations to learn combinatorial representations of sequential data. This improves symbolic interpretation and systematic generalisation. Our architecture is trained end-to-end through gradient descent on a variety of simple natural language reasoning tasks, significantly outperforming the latest state-of-the-art models in single-task and all-tasks settings. We also augment a subset of the data such that training and test data exhibit large systematic differences and show that our approach generalises better than the previous state-of-the-art.",0
"In recent years, third-order tensor products have emerged as a powerful tool for modeling complex dependencies in high-dimensional data. However, reasoning with these tensors can be challenging due to their inherent structure, which requires new mathematical frameworks that account for both multilinearity and nonlinearities across different modes. This paper presents a novel approach that extends standard tensor algebra operations by introducing higher-order latent variables (HOV), which capture essential geometric structures present in the data but lost during canonical decompositions such as CANDECOMP/PARAFAC (CP) and Tucker3. Our methodology allows for efficient computation of rank-one approximations and provides superior performance over existing methods on several benchmark datasets commonly used in applications ranging from signal processing to neuroscience. We showcase promising results in supervised learning tasks where HOV-based models achieve significantly better accuracy than state-of-the-art competitors and demonstrate scalability to larger datasets. By bridging the gap between linear algebraic manipulations and deep learning architectures, our framework offers a flexible interface between machine learning and applied mathematics communities interested in exploring and exploiting the properties of third-order tensor products.",1
"Recent improvements to Generative Adversarial Networks (GANs) have made it possible to generate realistic images in high resolution based on natural language descriptions such as image captions. Furthermore, conditional GANs allow us to control the image generation process through labels or even natural language descriptions. However, fine-grained control of the image layout, i.e. where in the image specific objects should be located, is still difficult to achieve. This is especially true for images that should contain multiple distinct objects at different spatial locations. We introduce a new approach which allows us to control the location of arbitrarily many objects within an image by adding an object pathway to both the generator and the discriminator. Our approach does not need a detailed semantic layout but only bounding boxes and the respective labels of the desired objects are needed. The object pathway focuses solely on the individual objects and is iteratively applied at the locations specified by the bounding boxes. The global pathway focuses on the image background and the general image layout. We perform experiments on the Multi-MNIST, CLEVR, and the more complex MS-COCO data set. Our experiments show that through the use of the object pathway we can control object locations within images and can model complex scenes with multiple objects at various locations. We further show that the object pathway focuses on the individual objects and learns features relevant for these, while the global pathway focuses on global image characteristics and the image background.",0
"In this work we describe how to create multiple objects simultaneously from raw materials such as stone, clay or metal at spatially distinct locations using simple techniques that can be carried out by individuals without specialized training. We demonstrate the effectiveness of our method through experiments involving the creation of several small sculptures each located at a different site within an urban environment. The resulting art pieces were evaluated using subjective measures including visual inspection and touch tests, as well as objective metrics such as density measurements. Our results indicate that our approach is capable of producing high quality works which meet commonly accepted standards for craftsmanship. Overall our work highlights the potential utility of low cost decentralized fabrication tools and methods, particularly in situations where resources are limited or access to centralized production facilities is challenging.",1
"Recent progress has been made in using attention based encoder-decoder framework for image and video captioning. Most existing decoders apply the attention mechanism to every generated word including both visual words (e.g., ""gun"" and ""shooting"") and non-visual words (e.g. ""the"", ""a""). However, these non-visual words can be easily predicted using natural language model without considering visual signals or attention. Imposing attention mechanism on non-visual words could mislead and decrease the overall performance of visual captioning. Furthermore, the hierarchy of LSTMs enables more complex representation of visual data, capturing information at different scales. To address these issues, we propose a hierarchical LSTM with adaptive attention (hLSTMat) approach for image and video captioning. Specifically, the proposed framework utilizes the spatial or temporal attention for selecting specific regions or frames to predict the related words, while the adaptive attention is for deciding whether to depend on the visual information or the language context information. Also, a hierarchical LSTMs is designed to simultaneously consider both low-level visual information and high-level language context information to support the caption generation. We initially design our hLSTMat for video captioning task. Then, we further refine it and apply it to image captioning task. To demonstrate the effectiveness of our proposed framework, we test our method on both video and image captioning tasks. Experimental results show that our approach achieves the state-of-the-art performance for most of the evaluation metrics on both tasks. The effect of important components is also well exploited in the ablation study.",0
"This should just give me an idea of the key concepts, contributions, methods/approach, findings etc so I can write a draft for a fuller version later.  ---  Our work proposes a novel approach for visual captioning using hierarchical Long Short Term Memory (LSTM) networks with adaptive attention mechanisms. We introduce a multi-level LSTM architecture that captures both short and long term dependencies between image features and language representations. Additionally, we propose an attention mechanism that weights different parts of the image based on their relevance to each part of the generated description, allowing our model to focus on relevant image regions. Our method achieves state-of-the-art results on two widely used benchmark datasets: MSCOCO Image Captions and Flickr8K. In conclusion, our work represents an important step towards more advanced artificial intelligence systems that can generate human-like descriptions of complex scenes and images.  This study develops a novel framework utilizing hierarchical Long Short Term Memory (hierarchical LSTM) networks in conjunction with adaptive attention to enhance visual captioning performance. By leveraging this multilevel architecture which accounts for short as well as prolonged correlations amongst picture elements and dialect depictions, plus a customized attentiveness system emphasizing vital aspects of the graphic, the suggested technique yields exceptional outcomes on standard data sets – MSCOCO Image Descriptions along with Flickr8k. These advances contribute to enhancing AI proficiency at creating descriptive text resembling genuine person’s writing about intricate snapshots.",1
"In several natural language tasks, labeled sequences are available in separate domains (say, languages), but the goal is to label sequences with mixed domain (such as code-switched text). Or, we may have available models for labeling whole passages (say, with sentiments), which we would like to exploit toward better position-specific label inference (say, target-dependent sentiment annotation). A key characteristic shared across such tasks is that different positions in a primary instance can benefit from different `experts' trained from auxiliary data, but labeled primary instances are scarce, and labeling the best expert for each position entails unacceptable cognitive burden. We propose GITNet, a unified position-sensitive multi-task recurrent neural network (RNN) architecture for such applications. Auxiliary and primary tasks need not share training instances. Auxiliary RNNs are trained over auxiliary instances. A primary instance is also submitted to each auxiliary RNN, but their state sequences are gated and merged into a novel composite state sequence tailored to the primary inference task. Our approach is in sharp contrast to recent multi-task networks like the cross-stitch and sluice network, which do not control state transfer at such fine granularity. We demonstrate the superiority of GIRNet using three applications: sentiment classification of code-switched passages, part-of-speech tagging of code-switched text, and target position-sensitive annotation of sentiment in monolingual passages. In all cases, we establish new state-of-the-art performance beyond recent competitive baselines.",0
"""GIRNet: Interleaved Multi-Task Recurrent State Sequence Models"" proposes a novel deep learning model for natural language understanding tasks that involves combining recurrent neural networks (RNNs) with multi-task learning techniques. RNNs have traditionally been used individually for each task, but our approach interleaves multiple tasks within a single network architecture. By doing so, we can exploit the structure shared across tasks and improve overall performance. Our framework is based on the idea that different subtasks can be represented using gating mechanisms and then combined into one sequence. We evaluate our proposed method on several benchmark datasets and demonstrate significant improvements over baseline models. In summary, our work provides a promising new direction for natural language processing research, allowing for more efficient use of computational resources while still achieving strong results.",1
"The goal of this work is to recognise phrases and sentences being spoken by a talking face, with or without the audio. Unlike previous works that have focussed on recognising a limited number of words or phrases, we tackle lip reading as an open-world problem - unconstrained natural language sentences, and in the wild videos. Our key contributions are: (1) we compare two models for lip reading, one using a CTC loss, and the other using a sequence-to-sequence loss. Both models are built on top of the transformer self-attention architecture; (2) we investigate to what extent lip reading is complementary to audio speech recognition, especially when the audio signal is noisy; (3) we introduce and publicly release a new dataset for audio-visual speech recognition, LRS2-BBC, consisting of thousands of natural sentences from British television. The models that we train surpass the performance of all previous work on a lip reading benchmark dataset by a significant margin.",0
"This paper presents a new approach to speech recognition that combines audio and visual cues to improve accuracy and robustness. Traditional speech recognition systems rely solely on acoustic features extracted from the audio signal, which can be limited by background noise, speaker variability, and other environmental factors. By integrating visual information such as lip movements and facial expressions, our method significantly enhances the representation of spoken language, resulting in more accurate transcriptions. We describe the technical details of our system, including feature extraction, fusion, and decoding algorithms, and evaluate its performance on several benchmark datasets. Our experiments demonstrate significant improvements over state-of-the-art methods, particularly in noisy environments and situations where speakers have accents or speech impairments. Overall, deep audio-visual speech recognition has the potential to revolutionize human-computer interaction and enable new applications in areas such as accessibility, education, and entertainment.",1
"Learning in environments with large state and action spaces, and sparse rewards, can hinder a Reinforcement Learning (RL) agent's learning through trial-and-error. For instance, following natural language instructions on the Web (such as booking a flight ticket) leads to RL settings where input vocabulary and number of actionable elements on a page can grow very large. Even though recent approaches improve the success rate on relatively simple environments with the help of human demonstrations to guide the exploration, they still fail in environments where the set of possible instructions can reach millions. We approach the aforementioned problems from a different perspective and propose guided RL approaches that can generate unbounded amount of experience for an agent to learn from. Instead of learning from a complicated instruction with a large vocabulary, we decompose it into multiple sub-instructions and schedule a curriculum in which an agent is tasked with a gradually increasing subset of these relatively easier sub-instructions. In addition, when the expert demonstrations are not available, we propose a novel meta-learning framework that generates new instruction following tasks and trains the agent more effectively. We train DQN, deep reinforcement learning agent, with Q-value function approximated with a novel QWeb neural network architecture on these smaller, synthetic instructions. We evaluate the ability of our agent to generalize to new instructions on World of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions. The QWeb agent outperforms the baseline without using any human demonstration achieving 100% success rate on several difficult environments.",0
"The world wide web (www) has become one of our primary sources of information today. Many aspects of life including business, education and socialising depend on information available on www. Learning how to navigate effectively through websites can enable us to gather important data from all over the world within seconds . As well as this learning to code allows you to create your own personalised pages using html, css and many other languages. These skills allow programmers to develop new software that makes lives easier such as google docs which is used by millions daily. Although coding seems like hard work, there is no denying the satisfaction you feel after creating something yourself that you previously had to pay someone else for. This skill set opens up limitless opportunities, providing you’re willing to put in some practice first.",1
"Existing methods for interactive image retrieval have demonstrated the merit of integrating user feedback, improving retrieval results. However, most current systems rely on restricted forms of user feedback, such as binary relevance responses, or feedback based on a fixed set of relative attributes, which limits their impact. In this paper, we introduce a new approach to interactive image search that enables users to provide feedback via natural language, allowing for more natural and effective interaction. We formulate the task of dialog-based interactive image retrieval as a reinforcement learning problem, and reward the dialog system for improving the rank of the target image during each dialog turn. To mitigate the cumbersome and costly process of collecting human-machine conversations as the dialog system learns, we train our system with a user simulator, which is itself trained to describe the differences between target and candidate images. The efficacy of our approach is demonstrated in a footwear retrieval application. Experiments on both simulated and real-world data show that 1) our proposed learning framework achieves better accuracy than other supervised and reinforcement learning baselines and 2) user feedback based on natural language rather than pre-specified attributes leads to more effective retrieval results, and a more natural and expressive communication interface.",0
"This paper presents a study on Dialog-based Interactive Image Retrieval (DIIR), which allows users to communicate their needs using natural language queries and receive images that match those descriptions. Our approach uses deep learning algorithms and a large dataset of annotated images to rank and retrieve relevant results. The system can then refine its understanding by asking clarifying questions before providing the final set of images. We evaluate DIIR against several state-of-the-art methods and show significant improvements in both effectiveness and efficiency. We conclude that our approach has potential applications in areas such as e-commerce, entertainment, and education where image search is critical.",1
"Image Captioning is a task that requires models to acquire a multi-modal understanding of the world and to express this understanding in natural language text. While the state-of-the-art for this task has rapidly improved in terms of n-gram metrics, these models tend to output the same generic captions for similar images. In this work, we address this limitation and train a model that generates more diverse and specific captions through an unsupervised training approach that incorporates a learning signal from an Image Retrieval model. We summarize previous results and improve the state-of-the-art on caption diversity and novelty. We make our source code publicly available online.",0
"Automatically generating accurate and relevant captions for images has been a challenging problem that remains unsolved by current technology. Previous approaches have focused on using textual descriptions of images as inputs for generating captions but often struggle with capturing diverse meaning and contexts of the images accurately. This work addresses these limitations by proposing a novel framework that generates diverse and meaningful image captions by integrating natural language processing (NLP) techniques such as sentiment analysis, entity recognition and named entity linking, with deep learning models like transformers to generate informative and diverse caption candidates. We evaluate our approach through extensive experiments on benchmark datasets and demonstrate significant improvements over existing state-of-the-art methods in terms of caption quality, diversity, and faithfulness to the input images. Our contributions provide new insights into addressing multimodal generation tasks that require both semantic understanding and linguistic expressiveness, paving the way for future research towards more human-like artificial intelligence systems.",1
"Deep Neural networks are efficient and flexible models that perform well for a variety of tasks such as image, speech recognition and natural language understanding. In particular, convolutional neural networks (CNN) generate a keen interest among researchers in computer vision and more specifically in classification tasks. CNN architecture and related hyperparameters are generally correlated to the nature of the processed task as the network extracts complex and relevant characteristics allowing the optimal convergence. Designing such architectures requires significant human expertise, substantial computation time and doesn't always lead to the optimal network. Model configuration topic has been extensively studied in machine learning without leading to a standard automatic method. This survey focuses on reviewing and discussing the current progress in automating CNN architecture search.",0
"This paper presents a comprehensive review of meta-reinforcement learning (RL) approaches that have been used for neural architecture search (NAS). We begin by providing background on NAS and RL, including their respective goals, methods, and challenges. Next, we discuss recent developments in the field of meta-RL, which has focused on accelerating model training through adaptive exploration. These techniques enable agents to learn across multiple tasks, improving generalization and reducing sample complexity. Applying these ideas to NAS leads to significant advances in architecture discovery, resulting in compact and powerful models that rival those found using state-of-the-art methods. Finally, we summarize open questions and future research directions within meta-RL NAS, highlighting potential paths towards broader adoption of these promising techniques.",1
Progress in image captioning is gradually getting complex as researchers try to generalized the model and define the representation between visual features and natural language processing. This work tried to define such kind of relationship in the form of representation called Tensor Product Representation (TPR) which generalized the scheme of language modeling and structuring the linguistic attributes (related to grammar and parts of speech of language) which will provide a much better structure and grammatically correct sentence. TPR enables better and unique representation and structuring of the feature space and will enable better sentence composition from these representations. A large part of the different ways of defining and improving these TPR are discussed and their performance with respect to the traditional procedures and feature representations are evaluated for image captioning application. The new models achieved considerable improvement than the corresponding previous architectures.,0
"Title: ""Feature Fusion Effects of Tensor Product Representation on Compositional Neural Networks for Image Captioning""  Abstract:  Image caption generation has emerged as a popular task in computer vision research due to its potential applications such as image description for visually impaired individuals, automatic image annotation, and content retrieval systems. In recent years, compositional neural networks have been introduced for generating natural language descriptions based on their constituent parts. Despite the promising results achieved by these models, there remains room for improvement, particularly in handling more complex scenes that involve multiple objects and relationships among them.  In this work, we investigate the effects of tensor product representation and feature fusion on improving caption generation performance. Specifically, our proposed approach uses a decompositional network followed by a recompositional architecture to capture both local and global context within images. We introduce a new method called Attention Guided Modality Adaptive Factorization Machines (AGMAFM), which combines multimodal features extracted from pretrained convolutional neural networks along with generated text embeddings using a tensordot operation. This allows us to jointly fuse visual representations while attending to the most relevant regions in each image. Experimental evaluation shows that AGMAFM significantly outperforms state-of-the-art techniques across several benchmark datasets, demonstrating improved accuracy and diversity in generated captions. Our findings suggest that incorporating tensor product representation and feature fusion can enhance the ability of compositional neural networks to handle diverse image settings and generate more coherent descriptions.",1
"What has happened in machine learning lately, and what does it mean for the future of medical image analysis? Machine learning has witnessed a tremendous amount of attention over the last few years. The current boom started around 2009 when so-called deep artificial neural networks began outperforming other established models on a number of important benchmarks. Deep neural networks are now the state-of-the-art machine learning models across a variety of areas, from image analysis to natural language processing, and widely deployed in academia and industry. These developments have a huge potential for medical imaging technology, medical data analysis, medical diagnostics and healthcare in general, slowly being realized. We provide a short overview of recent advances and some associated challenges in machine learning applied to medical image processing and image analysis. As this has become a very broad and fast expanding field we will not survey the entire landscape of applications, but put particular focus on deep learning in MRI.   Our aim is threefold: (i) give a brief introduction to deep learning with pointers to core references; (ii) indicate how deep learning has been applied to the entire MRI processing chain, from acquisition to image retrieval, from segmentation to disease prediction; (iii) provide a starting point for people interested in experimenting and perhaps contributing to the field of machine learning for medical imaging by pointing out good educational resources, state-of-the-art open-source code, and interesting sources of data and problems related medical imaging.",0
"This paper provides an overview of how Artificial Intelligence has been applied to magnetic resonance (MRI) images specifically through computer vision methods that can learn from vast amounts of data. Recently advancements have significantly improved automatic interpretation of disease presence, severity, and localisation into radiologist reports with human level performance and accuracy. As well as providing high quality interpretations artificial intelligence systems may assist clinical experts by pre-processing image sets to make them easier to visualise for human interpretation; provide automated patient triage that prioritises those needing urgent care; identify new patterns present across large image datasets, allowing discovery of novel associations between diseases potentially relevant for therapeutic development. We discuss applications to brain tumour segmentation but comment upon many other areas where deep learning approaches have been explored in the literature.",1
"Deep neural networks are the state-of-the-art methods for many real-world tasks, such as computer vision, natural language processing and speech recognition. For all its popularity, deep neural networks are also criticized for consuming a lot of memory and draining battery life of devices during training and inference. This makes it hard to deploy these models on mobile or embedded devices which have tight resource constraints. Quantization is recognized as one of the most effective approaches to satisfy the extreme memory requirements that deep neural network models demand. Instead of adopting 32-bit floating point format to represent weights, quantized representations store weights using more compact formats such as integers or even binary numbers. Despite a possible degradation in predictive performance, quantization provides a potential solution to greatly reduce the model size and the energy consumption. In this survey, we give a thorough review of different aspects of quantized neural networks. Current challenges and trends of quantized neural networks are also discussed.",0
"In this paper we present an overview of methods and theories related to quantized neural networks (QNN). We survey research from both academia and industry which aimed at improving performance, reducing memory usage, lowering computational requirements and extending deployment possibilities of QNNs. This includes approaches based on pruning weights, weight quantization methods like product-form or fixed point representations, mixed precision training and inference techniques as well as novel hardware accelerators such as those utilizing approximate computing paradigms. The focus lies on providing deep insights into how these advances can significantly enhance the capabilities of modern Artificial Intelligence systems operating in challenging conditions characterised by resource constraints, e.g., battery operated autonomous vehicles, edge devices in IoT infrastructures, and mobile handheld electronics, but potentially apply in other domains as well. To achieve these goals, state-of-the-art methodologies including quantitative evaluation across multiple metrics, comparison against relevant baseline models and architectures, detailed qualitative analysis grounded in theoretical foundations and comprehensive literature surveys were used. An additional aspect covered throughout the work was identifying current gaps in knowledge and proposing future directions of research that may lead to further improvements and broaden adoption of QNN in real-world scenarios. Our findings should provide practitioners and academics interested in deploying advanced machine learning algorithms under tight restrictions with valuable recommendations.",1
"Recent advancements in the area of Computer Vision with state-of-art Neural Networks has given a boost to Optical Character Recognition (OCR) accuracies. However, extracting characters/text alone is often insufficient for relevant information extraction as documents also have a visual structure that is not captured by OCR. Extracting information from tables, charts, footnotes, boxes, headings and retrieving the corresponding structured representation for the document remains a challenge and finds application in a large number of real-world use cases. In this paper, we propose a novel enterprise based end-to-end framework called DeepReader which facilitates information extraction from document images via identification of visual entities and populating a meta relational model across different entities in the document image. The model schema allows for an easy to understand abstraction of the entities detected by the deep vision models and the relationships between them. DeepReader has a suite of state-of-the-art vision algorithms which are applied to recognize handwritten and printed text, eliminate noisy effects, identify the type of documents and detect visual entities like tables, lines and boxes. Deep Reader maps the extracted entities into a rich relational schema so as to capture all the relevant relationships between entities (words, textboxes, lines etc) detected in the document. Relevant information and fields can then be extracted from the document by writing SQL queries on top of the relationship tables. A natural language based interface is added on top of the relationship schema so that a non-technical user, specifying the queries in natural language, can fetch the information with minimal effort. In this paper, we also demonstrate many different capabilities of Deep Reader and report results on a real-world use case.",0
"Title: Automatic Text Extraction using Convolutional Neural Networks and Recurrent Neural Networks  This research focuses on improving text extraction methods for document images by incorporating both convolutional neural networks (CNN) and recurrent neural networks (RNN). Typical OCR approaches rely heavily on CNNs to identify characters and tokens within documents but neglect RNNs which are capable of capturing important contextual information such as sentence structure and relationships between entities mentioned in the text. To address these limitations, we propose using a combination of CNN and RNN architectures to perform text extraction and entity recognition tasks simultaneously, thereby increasing accuracy in both areas. We evaluate our approach on several datasets and show that our method achieves state-of-the-art results compared to traditional OCR systems. Our work has significant implications in many fields where large amounts of unstructured data must be processed, including business intelligence, historical research, and natural language processing.",1
"The task in referring expression comprehension is to localise the object instance in an image described by a referring expression phrased in natural language. As a language-to-vision matching task, the key to this problem is to learn a discriminative object feature that can adapt to the expression used. To avoid ambiguity, the expression normally tends to describe not only the properties of the referent itself, but also its relationships to its neighbourhood. To capture and exploit this important information we propose a graph-based, language-guided attention mechanism. Being composed of node attention component and edge attention component, the proposed graph attention mechanism explicitly represents inter-object relationships, and properties with a flexibility and power impossible with competing approaches. Furthermore, the proposed graph attention mechanism enables the comprehension decision to be visualisable and explainable. Experiments on three referring expression comprehension datasets show the advantage of the proposed approach.",0
"This paper proposes an approach that combines natural language processing techniques with graph attention networks in order to improve the comprehension of referring expressions. Our method leverages recent advances in neural machine translation (NMT) to generate synthetic training data from scratch, which allows us to train our model without requiring any labeled data on target expressions. In contrast to previous work that relies heavily on pre-trained models, we show that our proposed solution outperforms state-of-the-art methods by a significant margin across multiple evaluation metrics. Furthermore, through extensive experiments we demonstrate that our model achieves better generalization performance and robustness under challenging conditions. Overall, our results highlight the importance of jointly considering both the textual content and the surrounding context to improve understanding of referring expressions. We believe that our work paves the way for future research in NLP to address more complex real-world problems involving referential uncertainty.",1
"Deep neural networks (DNNs) have witnessed as a powerful approach in this year by solving long-standing Artificial intelligence (AI) supervised and unsupervised tasks exists in natural language processing, speech processing, computer vision and others. In this paper, we attempt to apply DNNs on three different cyber security use cases: Android malware classification, incident detection and fraud detection. The data set of each use case contains real known benign and malicious activities samples. The efficient network architecture for DNN is chosen by conducting various trails of experiments for network parameters and network structures. The experiments of such chosen efficient configurations of DNNs are run up to 1000 epochs with learning rate set in the range [0.01-0.5]. Experiments of DNN performed well in comparison to the classical machine learning algorithms in all cases of experiments of cyber security use cases. This is due to the fact that DNNs implicitly extract and build better features, identifies the characteristics of the data that lead to better accuracy. The best accuracy obtained by DNN and XGBoost on Android malware classification 0.940 and 0.741, incident detection 1.00 and 0.997 fraud detection 0.972 and 0.916 respectively.",0
"Abstract: With the rise of cyber attacks and security breaches, there is an urgent need for advanced tools that can detect, prevent, and mitigate these threats. In this work, we present a novel deep neural network (DNN) architecture called ""Deep-Net"" specifically designed for cybersecurity use cases. Our approach leverages recent advances in DNNs, including transfer learning and attention mechanisms, to achieve state-of-the-art performance on several common benchmark datasets. We demonstrate the effectiveness of our model by evaluating it against multiple attack types, including malware detection, intrusion detection, and phishing website classification. Through extensive experimentation, we show that our method achieves better results compared to traditional machine learning algorithms and other recently proposed DNN models. Finally, we discuss potential future directions, such as integrating additional features like natural language processing techniques, which could further improve the accuracy of our system. Overall, our work represents a significant step forward towards automating the task of identifying and responding to cyber threats.",1
"Phrase Grounding aims to detect and localize objects in images that are referred to and are queried by natural language phrases. Phrase grounding finds applications in tasks such as Visual Dialog, Visual Search and Image-text co-reference resolution. In this paper, we present a framework that leverages information such as phrase category, relationships among neighboring phrases in a sentence and context to improve the performance of phrase grounding systems. We propose three modules: Proposal Indexing Network(PIN); Inter-phrase Regression Network(IRN) and Proposal Ranking Network(PRN) each of which analyze the region proposals of an image at increasing levels of detail by incorporating the above information. Also, in the absence of ground-truth spatial locations of the phrases(weakly-supervised), we propose knowledge transfer mechanisms that leverages the framework of PIN module. We demonstrate the effectiveness of our approach on the Flickr 30k Entities and ReferItGame datasets, for which we achieve improvements over state-of-the-art approaches in both supervised and weakly-supervised variants.",0
"This research presents a novel approach to phrase grounding using proposal indexing, relationships, and context (PIRC). Traditional methods rely on manually curated datasets and handcrafted features, which can result in limited performance and scalability. Our method addresses these challenges by leveraging semantic parsing techniques to extract implicit relationships between phrases and propositions in natural language texts. These extracted relationships form the basis for building a proposal index that enables efficient retrieval and ranking of relevant proposals for any given phrase. We evaluate our model on two benchmark datasets and demonstrate significant improvements over state-of-the-art models, including higher accuracy, robustness to noisy inputs, and better scalability. Additionally, we analyze the effects of different components of our model on its performance and provide insights into potential future directions for phrase grounding research. Overall, our work provides a promising new direction for natural language understanding tasks requiring accurate phrase interpretation.",1
"Automatically generating a natural language sentence to describe the content of an input video is a very challenging problem. It is an essential multimodal task in which auditory and visual contents are equally important. Although audio information has been exploited to improve video captioning in previous works, it is usually regarded as an additional feature fed into a black box fusion machine. How are the words in the generated sentences associated with the auditory and visual modalities? The problem is still not investigated. In this paper, we make the first attempt to design an interpretable audio-visual video captioning network to discover the association between words in sentences and audio-visual sequences. To achieve this, we propose a multimodal convolutional neural network-based audio-visual video captioning framework and introduce a modality-aware module for exploring modality selection during sentence generation. Besides, we collect new audio captioning and visual captioning datasets for further exploring the interactions between auditory and visual modalities for high-level video understanding. Extensive experiments demonstrate that the modality-aware module makes our model interpretable on modality selection during sentence generation. Even with the added interpretability, our video captioning network can still achieve comparable performance with recent state-of-the-art methods.",0
"This paper presents a novel approach to audio-visual video captioning that focuses on interpretability. We propose an end-to-end framework that generates both spoken language captions and visual representations in the form of semantic maps, which provide interpretable explanations of how the model processes input data to generate output descriptions. Our framework integrates state-of-the-art models from different fields into a unified architecture, allowing us to leverage advanced techniques in image processing, natural language generation, and multimodal fusion. By doing so, we aim to achieve superior performance compared to traditional approaches while providing insightful explanations of the underlying decision process. Evaluation results demonstrate significant improvements over strong baseline methods in terms of both objective metrics and human judgments. Additionally, qualitative analysis confirms that our method indeed produces meaningful and informative visualizations as intended. Overall, this work takes a step toward making audio-visual video captioning more transparent and comprehensible, ultimately paving the way for future research on explainable artificial intelligence systems.",1
"Optimal parameter initialization remains a crucial problem for neural network training. A poor weight initialization may take longer to train and/or converge to sub-optimal solutions. Here, we propose a method of weight re-initialization by repeated annealing and injection of noise in the training process. We implement this through a cyclical batch size schedule motivated by a Bayesian perspective of neural network training. We evaluate our methods through extensive experiments on tasks in language modeling, natural language inference, and image classification. We demonstrate the ability of our method to improve language modeling performance by up to 7.91 perplexity and reduce training iterations by up to $61\%$, in addition to its flexibility in enabling snapshot ensembling and use with adversarial training.",0
"This research presents a new approach to parameter re-initialization in deep learning algorithms called ""Parameter Re-Initialization through Cyclical Batch Size Schedules"" (PRBSS). PRBSS involves gradually reducing the batch size used during training over time, then slowly increasing it again before repeating the process from scratch. Our method is designed to improve the performance of deep neural networks by mitigating the effects of local minima and other optimization challenges that can arise during training. We demonstrate the effectiveness of our method on several benchmark datasets across different domains, including image classification, speech recognition, and natural language processing tasks. Results show significant improvements in accuracy compared to traditional methods, particularly as the model becomes larger and more complex. Overall, PRBSS offers a simple yet effective strategy for improving the generalizability and robustness of deep learning models.",1
"For the task of generating complex outputs such as source code, editing existing outputs can be easier than generating complex outputs from scratch. With this motivation, we propose an approach that first retrieves a training example based on the input (e.g., natural language description) and then edits it to the desired output (e.g., code). Our contribution is a computationally efficient method for learning a retrieval model that embeds the input in a task-dependent way without relying on a hand-crafted metric or incurring the expense of jointly training the retriever with the editor. Our retrieve-and-edit framework can be applied on top of any base model. We show that on a new autocomplete task for GitHub Python code and the Hearthstone cards benchmark, retrieve-and-edit significantly boosts the performance of a vanilla sequence-to-sequence model on both tasks.",0
"""Predictive models trained on structured data such as tables or trees have recently shown impressive performance across domains from image generation to machine translation. However, these models often suffer from poor interpretability due to their complex deep learning components, making them less desirable than simpler models that explicitly capture domain knowledge but lack capacity. In contrast, we propose a novel framework called 'Retrieval-Augmented Editing Network (RAEN)' which combines the strengths of both worlds by enabling edit operations in latent space via retrieval. RAEN maintains multiple copies of an inference network guided by different objectives, leading to specialized submodels that can focus on certain tasks while sharing parameters through knowledge distillation. To improve sample diversity, a noising schedule further varies training objectives adaptively based on the level of ambiguity in input text. Experiments demonstrate RAEN significantly outperforms several strong baselines on six challenging benchmark datasets.""",1
"Although neural networks can achieve very high predictive performance on various different tasks such as image recognition or natural language processing, they are often considered as opaque ""black boxes"". The difficulty of interpreting the predictions of a neural network often prevents its use in fields where explainability is important, such as the financial industry where regulators and auditors often insist on this aspect. In this paper, we present a way to assess the relative input features importance of a neural network based on the sensitivity of the model output with respect to its input. This method has the advantage of being fast to compute, it can provide both global and local levels of explanations and is applicable for many types of neural network architectures. We illustrate the performance of this method on both synthetic and real data and compare it with other interpretation techniques. This method is implemented into an open-source Python package that allows its users to easily generate and visualize explanations for their neural networks.",0
"In recent years, there has been significant interest in developing neural networks that can explain their decision-making process. Explainability is becoming increasingly important as these models are used in critical domains such as healthcare, finance, and criminal justice, where transparency and accountability are crucial. One approach to improving model interpretability is through sensitivity analysis, which involves studying how different input features affect the output of the network. In this work, we present a methodology for performing sensitivity-based explanations of neural networks. Our approach utilizes first-order Taylor expansions to quantify the impact of individual inputs on the final prediction. We evaluate our method using several case studies across different application areas and show that our explanations provide valuable insights into the behavior of the network. Additionally, we demonstrate that our approach is more accurate and efficient than existing methods for interpreting neural networks. Overall, our work advances the state of the art in explainable artificial intelligence (XAI) by providing a novel framework for understanding and explaining complex machine learning models.",1
"Markov networks are widely used in many Machine Learning applications including natural language processing, computer vision, and bioinformatics . Learning Markov networks have many complications ranging from intractable computations involved to the possibility of learning a model with a huge number of parameters. In this report, we provide a computationally tractable greedy heuristic for learning Markov networks structure. The proposed heuristic results in a model with a limited predefined number of parameters. We ran our method on 3 fully-observed real datasets, and we observed that our method is doing comparably good to the state of the art methods.",0
"Machine learning algorithms have been successfully applied in numerous domains such as image recognition, natural language processing, speech synthesis and so on. These models usually involve deep neural networks that learn from large amounts of data during training. However, these models can suffer from several limitations such as the curse of dimensionality and overfitting, which lead to high computational costs and poor generalization performance. One popular method to address these issues is pruning techniques. In this work we propose forced pruning as a new technique which encourages model parsimony by removing all connections whose importance scores fall below a predefined threshold. Our experiments show that our method significantly improves accuracy while reducing parameter counts by up to two orders of magnitude compared to the state of the art methods. Additionally, our algorithm allows us to analyze the structure of the network itself, providing insights into why some components are more important than others. With our novel framework, we believe researchers in the field of machine learning will be able to design more efficient models without sacrificing performance.",1
"In this paper, we propose an end-to-end capsule network for pixel level localization of actors and actions present in a video. The localization is performed based on a natural language query through which an actor and action are specified. We propose to encode both the video as well as textual input in the form of capsules, which provide more effective representation in comparison with standard convolution based features. We introduce a novel capsule based attention mechanism for fusion of video and text capsules for text selected video segmentation. The attention mechanism is performed via joint EM routing over video and text capsules for text selected actor and action localization. The existing works on actor-action localization are mainly focused on localization in a single frame instead of the full video. Different from existing works, we propose to perform the localization on all frames of the video. To validate the potential of the proposed network for actor and action localization on all the frames of a video, we extend an existing actor-action dataset (A2D) with annotations for all the frames. The experimental evaluation demonstrates the effectiveness of the proposed capsule network for text selective actor and action localization in videos, and it also improves upon the performance of the existing state-of-the art works on single frame-based localization.",0
"This paper presents a multi-modal capsule routing algorithm which utilizes both image features and natural language queries for actor and action video segmentation. Our method leverages capsules for feature extraction from images and natural languages, allowing the two modalities to interact with each other via dynamic routing operations. We introduce a novel routing algorithm based on semantic attention mechanisms that jointly models the spatial layout and temporal dynamics of actors and actions. Experimental results demonstrate significant improvement over previous state-of-the-art methods, achieving better performance in challenging scenarios involving occlusions and varied poses. Furthermore, our approach is shown to generalize well across different datasets while maintaining competitive speed compared to real-time systems. Overall, we believe this work represents an important step towards bridging the gap between deep learning and human-level understanding, paving the path towards more advanced applications such as conversational agents, personalized recommendations, and autonomous robots that can interact seamlessly with humans under complex environments and situations.",1
"Nowadays, deep learning can be employed to a wide ranges of fields including medicine, engineering, etc. In deep learning, Convolutional Neural Network (CNN) is extensively used in the pattern and sequence recognition, video analysis, natural language processing, spam detection, topic categorization, regression analysis, speech recognition, image classification, object detection, segmentation, face recognition, robotics, and control. The benefits associated with its near human level accuracies in large applications lead to the growing acceptance of CNN in recent years. The primary contribution of this paper is to analyze the impact of the pattern of the hidden layers of a CNN over the overall performance of the network. To demonstrate this influence, we applied neural network with different layers on the Modified National Institute of Standards and Technology (MNIST) dataset. Also, is to observe the variations of accuracies of the network for various numbers of hidden layers and epochs and to make comparison and contrast among them. The system is trained utilizing stochastic gradient and backpropagation algorithm and tested with feedforward algorithm.",0
"Title: An Empirical Analysis of Handwritten Digit Recognition Techniques Using Deep Learning Models  Abstract: The field of deep learning has been revolutionized by the development and application of convolutional neural networks (CNNs), which have achieved remarkable results in areas such as image recognition, natural language processing, and speech recognition. In recent years, there has been growing interest in exploring the potential uses of CNNs in handwritten digit recognition tasks. This study aimed to analyze the impact of different hidden layers and epochs on the accuracy of these models in classifying MNIST dataset.  To achieve this goal, several experiments were conducted using a variety of CNN architectures with varying numbers of hidden layers and epochs. Each model was trained, tested and evaluated according to standard protocols, and their accuracies compared against each other and well established baselines. Our analysis demonstrated that increasing the number of hidden layers leads to improved classification performance up until a certain point where overfitting occurs. Furthermore, our findings showed that the optimal number of epochs required for each layer may vary depending on the specific architecture. Finally, we provide insights into how to select the most appropriate network architecture based on performance metrics to increase overall accuracy. These results contribute to our understanding of CNNs in handwritten digit recognition tasks and offer valuable guidance for future research efforts in this area.",1
"With the recent success of embeddings in natural language processing, research has been conducted into applying similar methods to code analysis. Most works attempt to process the code directly or use a syntactic tree representation, treating it like sentences written in a natural language. However, none of the existing methods are sufficient to comprehend program semantics robustly, due to structural features such as function calls, branching, and interchangeable order of statements. In this paper, we propose a novel processing technique to learn code semantics, and apply it to a variety of program analysis tasks. In particular, we stipulate that a robust distributional hypothesis of code applies to both human- and machine-generated programs. Following this hypothesis, we define an embedding space, inst2vec, based on an Intermediate Representation (IR) of the code that is independent of the source programming language. We provide a novel definition of contextual flow for this IR, leveraging both the underlying data- and control-flow of the program. We then analyze the embeddings qualitatively using analogies and clustering, and evaluate the learned representation on three different high-level tasks. We show that even without fine-tuning, a single RNN architecture and fixed inst2vec embeddings outperform specialized approaches for performance prediction (compute device mapping, optimal thread coarsening); and algorithm classification from raw code (104 classes), where we set a new state-of-the-art.",0
"Recent advances in deep learning have enabled significant progress towards automating code understanding tasks such as bug detection, refactoring, and program synthesis. However, existing neural approaches often struggle to accurately capture the semantic meaning of code snippets, leading to suboptimal performance on these tasks. In order to address this shortcoming, we propose a novel framework called ""Neural Code Comprehender"" which learns a latent representation of code semantics that can effectively model how source code relates to its behavior. Our approach leverages powerful convolutional layers along with a custom attention mechanism to capture localized dependencies within code and produces representations that generalize across different programming languages. We evaluate our method on several benchmarks, including code classification, variable identification, and method-level summary generation, demonstrating state-of-the-art results across all metrics. Overall, our work shows promising steps towards achieving human-level comprehension of complex software systems by machines.",1
"Human beings understand natural language description and could able to imagine a corresponding visual for the same. For example, given a description of the interior of a house, we could imagine its structure and arrangements of furniture. Automatic synthesis of real-world images from text descriptions has been explored in the computer vision community. However, there is no such attempt in the area of document images, like floor plans. Floor plan synthesis from sketches, as well as data-driven models, were proposed earlier. Ours is the first attempt to render building floor plan images from textual description automatically. Here, the input is a natural language description of the internal structure and furniture arrangements within a house, and the output is the 2D floor plan image of the same. We have experimented on publicly available benchmark floor plan datasets. We were able to render realistic synthesized floor plan images from the description written in English.",0
"This study aimed to develop a method for automatically generating images of building floor plans based on textual descriptions written in English. To achieve this goal, we proposed a novel deep learning model that uses recurrent neural networks (RNNs) to encode the input text into a visual representation space where architectural elements can be synthesized to form the final image output. We evaluated our approach by testing its performance on two datasets: one containing real estate listings and another comprising descriptions of fictional buildings. Our results showed that the proposed method could generate high-quality renderings that were consistent with the provided texts while capturing relevant details such as room shapes, dimensions, and wall colors. In addition, we conducted user studies which demonstrated that human raters found the generated images to be highly similar to ground truth drawings prepared by professional designers. These findings suggest that our approach has the potential to significantly streamline the process of creating detailed blueprints for residential and commercial properties, with applications in urban planning, architecture, construction, and virtual reality.",1
"Visual question answering (VQA) demands simultaneous comprehension of both the image visual content and natural language questions. In some cases, the reasoning needs the help of common sense or general knowledge which usually appear in the form of text. Current methods jointly embed both the visual information and the textual feature into the same space. However, how to model the complex interactions between the two different modalities is not an easy task. In contrast to struggling on multimodal feature fusion, in this paper, we propose to unify all the input information by natural language so as to convert VQA into a machine reading comprehension problem. With this transformation, our method not only can tackle VQA datasets that focus on observation based questions, but can also be naturally extended to handle knowledge-based VQA which requires to explore large-scale external knowledge base. It is a step towards being able to exploit large volumes of text and natural language processing techniques to address VQA problem. Two types of models are proposed to deal with open-ended VQA and multiple-choice VQA respectively. We evaluate our models on three VQA benchmarks. The comparable performance with the state-of-the-art demonstrates the effectiveness of the proposed method.",0
"Title: ""Visual Question Answering as Reading Comprehension"" (VQA) is a rapidly growing field that involves building artificial intelligence systems capable of understanding and answering questions based on visual input. These systems must be able to analyze complex images and videos, identify relevant information, and generate accurate responses to natural language queries. While significant progress has been made in recent years, there remains a significant gap between human-level performance and state-of-the-art VQA models. In this paper, we present a comprehensive review of the current literature on VQA and propose several directions for future research aimed at closing this gap. We discuss the challenges facing VQA systems, including scene understanding, question understanding, multimodal fusion, and evaluation metrics. We also examine promising approaches such as attention mechanisms, memory networks, and transfer learning. Our goal is to provide a solid foundation for advancing the development of VQA technology and ultimately achieving human-like performance in visual reasoning tasks.",1
"This paper addresses the problem of manipulating images using natural language description. Our task aims to semantically modify visual attributes of an object in an image according to the text describing the new visual appearance. Although existing methods synthesize images having new attributes, they do not fully preserve text-irrelevant contents of the original image. In this paper, we propose the text-adaptive generative adversarial network (TAGAN) to generate semantically manipulated images while preserving text-irrelevant contents. The key to our method is the text-adaptive discriminator that creates word-level local discriminators according to input text to classify fine-grained attributes independently. With this discriminator, the generator learns to generate images where only regions that correspond to the given text are modified. Experimental results show that our method outperforms existing methods on CUB and Oxford-102 datasets, and our results were mostly preferred on a user study. Extensive analysis shows that our method is able to effectively disentangle visual attributes and produce pleasing outputs.",0
"Title: ""Text-Driven Image Generation using GANs""  In recent years, generative adversarial networks (GANs) have been widely used to generate realistic images from text descriptions. However, existing methods often struggle with generating high-quality images that accurately capture the intended meaning of the input text. In this work, we present a novel approach to image generation using GANs that can dynamically adapt to the content of the text description. Our method enables fine-grained control over the generated image by allowing users to manipulate individual features within the image through natural language commands. We evaluate our model on several benchmark datasets and demonstrate that it consistently outperforms state-of-the-art models in terms of visual quality and semantic fidelity. Additionally, we provide qualitative and quantitative analysis to showcase the effectiveness of our approach. This research has important implications for computer vision, graphics, and NLP communities, opening up new possibilities for interactive image manipulation and content creation using natural language.",1
"The increase in the number of Internet users and the strong interaction brought by Web 2.0 made the Opinion Mining an important task in the area of natural language processing. Although several methods are capable of performing this task, few use multi-label classification, where there is a group of true labels for each example. This type of classification is useful for situations where the opinions are analyzed from the perspective of the reader, this happens because each person can have different interpretations and opinions on the same subject. This paper discuss the efficiency of problem transformation methods combined with different classification algorithms for the task of multi-label classification of reactions in news texts. To do that, extensive tests were carried out on two news corpora written in Brazilian Portuguese annotated with reactions. A new corpus called BFRC-PT is presented. In the tests performed, the highest number of correct predictions was obtained with the Classifier Chains method combined with the Random Forest algorithm. When considering the class distribution, the best results were obtained with the Binary Relevance method combined with the LSTM and Random Forest algorithms.",0
"This is just an example model abstract that I came up with: ""Online news platforms have become increasingly popular over recent years. With many different outlets providing users with vast amounts of information, understanding user reactions has become vital for news websites to tailor their content and increase engagement. In this work we present a multi-label classification approach based on text features extracted from online news articles and user comments. Our system achieved state-of-the-art results on several benchmark datasets, demonstrating its effectiveness in accurately predicting multiple labels for each article/comment pair.""",1
"Convolutional Neural Networks (CNN) has become more popular choice for various tasks such as computer vision, speech recognition and natural language processing. Thanks to their large computational capability and throughput, GPUs ,which are not power efficient and therefore does not suit low power systems such as mobile devices, are the most common platform for both training and inferencing tasks. Recent studies has shown that FPGAs can provide a good alternative to GPUs as a CNN accelerator, due to their re-configurable nature, low power and small latency. In order for FPGA-based accelerators outperform GPUs in inference task, both the parameters of the network and the activations must be quantized. While most works use uniform quantizers for both parameters and activations, it is not always the optimal one, and a non-uniform quantizer need to be considered. In this work we introduce a custom hardware-friendly approach to implement non-uniform quantizers. In addition, we use a single scale integer representation of both parameters and activations, for both training and inference. The combined method yields a hardware efficient non-uniform quantizer, fit for real-time applications. We have tested our method on CIFAR-10 and CIFAR-100 image classification datasets with ResNet-18 and VGG-like architectures, and saw little degradation in accuracy.",0
"Quantization has emerged as an essential technique to deploy deep learning models on edge devices or other platforms that lack sufficient computing power, such as smartphones or embedded systems. A recent trend towards accelerating the inference speed further through utilizing reconfigurable hardware such as FPGAs or custom ASICs brings new challenges due to their limited memory resources. Most existing quantizers focus on balancing model size reduction against model accuracy degradation and assume uniform bit precision. However, under tight constraints on resource usage like area multipliers or available dynamic logic gates required by modern FPGA architectures, more aggressive compression may lead to severe dropouts. We propose an efficient non-uniform quantizer (NuQ) to enable flexible compression rates within each layer while guaranteeing the smallest feasible weight representation without losing significant quantization error. Our evaluation using several state-of-the-art benchmark models demonstrates that our approach achieves higher quantization efficiency than previous methods, reducing bit requirements up to 29%, while preserving acceptable inference accuracy relative to full-precision models in most cases. In addition, we design a lightweight architecture of our NuQ suitable for deployment on reconfigurable hardware such as Xilinx Adaptive Compute Accelerator (ACA), which significantly outperforms prior solutions regarding latency and energy consumption. Lastly, we provide insight into how different design choices impact the tradeoff between compression and accuracy to guide future research. This work bridges the gap between theory and practice, paving the path toward deployment of complex deep learning models onto edge platforms while maintaining adequate performance.",1
"The tracking-by-detection framework requires a set of positive and negative training samples to learn robust tracking models for precise localization of target objects. However, existing tracking models mostly treat different samples independently while ignores the relationship information among them. In this paper, we propose a novel structure-aware deep neural network to overcome such limitations. In particular, we construct a graph to represent the pairwise relationships among training samples, and additionally take the natural language as the supervised information to learn both feature representations and classifiers robustly. To refine the states of the target and re-track the target when it is back to view from heavy occlusion and out of view, we elaborately design a novel subnetwork to learn the target-driven visual attentions from the guidance of both visual and natural language cues. Extensive experiments on five tracking benchmark datasets validated the effectiveness of our proposed method.",0
"This paper proposes two approaches that jointly learn natural language (NL) guided structural representation for object tracking. We first present our NL track model (NLTM), which learns to attend to informative regions of a frame conditioned on NL prompts. Our proposed approach achieves state-of-the-art performance on standard benchmark datasets while reducing error rates by up to 19%. Second, we introduce a novel framework called Learn2Track, which can take raw image frames as input and outputs tracking results without any additional annotations. By utilizing large amounts of unlabeled data and fine-tuning an existing tracker architecture, Learn2Track outperforms all previous weakly supervised methods. In addition, integrating NLTM into Learn2Track leads to more significant improvements than applying other attention mechanisms alone. Overall, these models demonstrate competitive performance compared to fully supervised methods trained on densely annotated sequences while operating under realistic deployment scenarios that require only sparsely labeled data. They further enable interpretable attentive visualization and highlight novel connections between NLP and computer vision tasks beyond simple text localization.",1
"With more than 300 million people depressed worldwide, depression is a global problem. Due to access barriers such as social stigma, cost, and treatment availability, 60% of mentally-ill adults do not receive any mental health services. Effective and efficient diagnosis relies on detecting clinical symptoms of depression. Automatic detection of depressive symptoms would potentially improve diagnostic accuracy and availability, leading to faster intervention. In this work, we present a machine learning method for measuring the severity of depressive symptoms. Our multi-modal method uses 3D facial expressions and spoken language, commonly available from modern cell phones. It demonstrates an average error of 3.67 points (15.3% relative) on the clinically-validated Patient Health Questionnaire (PHQ) scale. For detecting major depressive disorder, our model demonstrates 83.3% sensitivity and 82.6% specificity. Overall, this paper shows how speech recognition, computer vision, and natural language processing can be combined to assist mental health patients and practitioners. This technology could be deployed to cell phones worldwide and facilitate low-cost universal access to mental health care.",0
"In this study we investigated whether depression symptom severity could be effectively measured using both spoken language analysis (SLA) and automated 3D facial expression analysis (A3DFE). Both SLA and A3DFE have been shown as effective ways to assess emotional states, however, little research has examined how these two methods can work together to measure mental health indicators such as depression symptom severity. We collected data from participants diagnosed with Major Depressive Disorder who provided audio recordings and were filmed speaking about their mood over a period of time. To analyze the recorded speech, we used Automatic Speech Recognition software and extracted features related to prosody and lexical content. For the facial expressions analysis, we used computer vision algorithms that tracked and analyzed 68 landmarks on the face, producing a dynamic 3D model of each participant’s face during their recording session. Using regression models, our findings showed strong correlations between measures of depression symptom severity derived from SLA and A3DFE and established clinician ratings of depression symptom severity. These results suggest that the combined use of SLA and A3DFE could potentially provide a more comprehensive understanding of depression symptom severity compared to either method alone. This research provides important new insights into how technology can be leveraged to improve the accuracy of measuring depression symptoms remotely in real-time which may lead to better outcomes for patients.",1
"Coronary Artery Disease (CAD) is one of the leading causes of death worldwide, and so it is very important to correctly diagnose patients with the disease. For medical diagnosis, machine learning is a useful tool, however features and algorithms must be carefully selected to get accurate classification. To this effect, three feature selection methods have been used on 13 input features from the Cleveland dataset with 297 entries, and 7 were selected. The selected features were used to train three different classifiers, which are SVM, Na\""ive Bayes and KNN using 10-fold cross-validation. The resulting models evaluated using Accuracy, Recall, Specificity and Precision. It is found that the Na\""ive Bayes classifier performs the best on this dataset and features, outperforming or matching SVM and KNN in all the four evaluation parameters used and achieving an accuracy of 84%.",0
"Coronary artery disease (CAD) affects millions worldwide, making prompt diagnosis critical to patient outcomes. Traditionally, CAD risk assessment relies on clinical markers such as blood pressure, cholesterol levels, smoking history, and family medical histories. Despite these efforts, current diagnostic tests fail to achieve high sensitivity and specificity, resulting in both false positive and false negative rates. In recent years, machine learning approaches have emerged as potential solutions for improving CAD classification accuracy by leveraging large datasets comprising diverse patient demographics, traditional risk factors, and imaging modalities including coronary CT angiography (CTA). This article reviews the state-of-the art applications of supervised/unsupervised learning techniques towards accurate prediction models addressing issues pertaining to data quality and preprocessing. We analyze their strengths, weaknesses, and future prospects using evidence from multiple studies performed across different cohorts. Our work highlights that while these methods demonstrate promising results compared to conventional strategies, significant challenges must still be overcome before incorporating them into routine cardiac care. The goal is to provide guidance for researchers seeking to design efficient CAD classification models and foster collaboration among stakeholders for improved healthcare practices. Keywords: coronary artery disease, machine learning classifications, supervised/ unsupervised learning techniques, coronary computed tomographic angiography (CCTA), predictive modelling",1
"We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions.   Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant update.",0
"This paper provides an overview of deep reinforcement learning (DRL), a subfield of machine learning that focuses on training agents to make decisions in complex environments by using neural networks to approximate state-action values. DRL has been shown to achieve impressive results across a wide range of domains, from games to robotics, but it remains a challenging problem due to the difficulties associated with exploring large, high-dimensional spaces and balancing exploration versus exploitation. We survey existing work in the field, highlight common themes and approaches, and discuss open research questions and future directions. Throughout our discussion, we emphasize the importance of understanding the connections between deep learning and traditional RL algorithms in order to design effective models capable of solving real-world problems. Our ultimate goal is to provide readers with a broad view of the current landscape and encourage further investigation into this exciting area of study.",1
"We show that correspondence analysis (CA) is equivalent to defining a Gini index with appropriately scaled one-hot encoding. Using this relation, we introduce a nonlinear kernel extension to CA. This extended CA gives a known analysis for natural language via specialized kernels that use an appropriate contingency table. We propose a semi-supervised CA, which is a special case of the kernel extension to CA. Because CA requires excessive memory if applied to numerous categories, CA has not been used for natural language processing. We address this problem by introducing delayed evaluation to randomized singular value decomposition. The memory-efficient CA is then applied to a word-vector representation task. We propose a tail-cut kernel, which is an extension to the skip-gram within the kernel extension to CA. Our tail-cut kernel outperforms existing word-vector representation methods.",0
"This paper argues that Word2Vec, one of the most widely used techniques in natural language processing (NLP), can be seen as a specific instance of kernel correspondence analysis (KCA) - a technique from machine learning and data mining. We demonstrate how to perform KCA on textual datasets using linear algebra operations similar to those found in popular Word2Vec implementations. This allows us to interpret Word2Vec models in terms of kernel functions commonly applied in NLP tasks such as classification, clustering, regression, feature extraction, etc., enabling direct comparison with other kernel methods from these areas. Experimental evaluation confirms effectiveness and applicability of our approach, which we believe has important implications both for better understanding existing NLP systems like Word2Vec and developing new ones based on related kernels. Our work represents a bridge between classic ideas from statistics/applied mathematics and modern developments in deep learning and artificial intelligence. For example, a comprehensive study evaluating properties of different kernels on benchmark NLP datasets is presented. These results open up exciting prospects for more general uses of kernels beyond traditional settings where they have been applied previously but could potentially bring benefits even in less structured domains or problem types. Finally, some speculative thoughts concerning future development of NLP in light of recent successes with deep neural networks conclude our research.  Keywords: Kernel Methods; Correlation and Dependence Measures; Word Embeddings; Text Mining; Applied Machine Learning; Data Science.",1
"There are two major approaches for sequence labeling. One is the probabilistic gradient-based methods such as conditional random fields (CRF) and neural networks (e.g., RNN), which have high accuracy but drawbacks: slow training, and no support of search-based optimization (which is important in many cases). The other is the search-based learning methods such as structured perceptron and margin infused relaxed algorithm (MIRA), which have fast training but also drawbacks: low accuracy, no probabilistic information, and non-convergence in real-world tasks. We propose a novel and ""easy"" solution, a search-based probabilistic online learning method, to address most of those issues. The method is ""easy"", because the optimization algorithm at the training stage is as simple as the decoding algorithm at the test stage. This method searches the output candidates, derives probabilities, and conducts efficient online learning. We show that this method with fast training and theoretical guarantee of convergence, which is easy to implement, can support search-based optimization and obtain top accuracy. Experiments on well-known tasks show that our method has better accuracy than CRF and BiLSTM\footnote{The SAPO code is released at \url{https://github.com/lancopku/SAPO}.}.",0
"This paper presents a search based probabilistic online learning framework called SAPO designed specifically for sequence labeling problems in natural language processing. The proposed model utilizes state of art optimization techniques such as gradient boosted decision trees and stochastic rounding that enables efficient use of labeled data, computational time and makes it easy to handle complex features like one hot encoding without requiring preprocessing. Our experimental results showcase improved performance compared to previously existing methods on both publicly available benchmark datasets and on real world tasks we have been working on at Amazon.",1
"There has been a drastic growth of research in Generative Adversarial Nets (GANs) in the past few years. Proposed in 2014, GAN has been applied to various applications such as computer vision and natural language processing, and achieves impressive performance. Among the many applications of GAN, image synthesis is the most well-studied one, and research in this area has already demonstrated the great potential of using GAN in image synthesis. In this paper, we provide a taxonomy of methods used in image synthesis, review different models for text-to-image synthesis and image-to-image translation, and discuss some evaluation metrics as well as possible future research directions in image synthesis with GAN.",0
"Title: Image Synthesis Using Generative Adversarial Networks - State of the Art Overview  In recent years, generative adversarial networks (GANs) have emerged as one of the most powerful techniques for generating high quality images, videos, and other types of data that resemble real world examples. This overview paper provides an introduction to image synthesis using GANs by presenting a survey of some of the current state of art research on the subject. We review key applications such as text-to-image synthesis, image completion, superresolution, etc., along with their corresponding models and evaluation metrics. In addition, we highlight some of the challenges and limitations that still exist, and provide insights into possible future directions. By providing a comprehensive analysis of the latest advancements in image synthesis using GANs, our hope is that this paper can serve both researchers new to the field and experienced practitioners alike.",1
"Localizing natural language phrases in images is a challenging problem that requires joint understanding of both the textual and visual modalities. In the unsupervised setting, lack of supervisory signals exacerbate this difficulty. In this paper, we propose a novel framework for unsupervised visual grounding which uses concept learning as a proxy task to obtain self-supervision. The simple intuition behind this idea is to encourage the model to localize to regions which can explain some semantic property in the data, in our case, the property being the presence of a concept in a set of images. We present thorough quantitative and qualitative experiments to demonstrate the efficacy of our approach and show a 5.6% improvement over the current state of the art on Visual Genome dataset, a 5.8% improvement on the ReferItGame dataset and comparable to state-of-art performance on the Flickr30k dataset.",0
"We present a novel unsupervised approach for learning visual grounding that leverages semantic self-supervision. Our method learns to associate image regions with human text descriptions by predicting missing words in natural language sentences. This task provides fine-grained supervision at the level of individual words, without requiring explicit annotation of object bounding boxes or region proposals.  Our model learns a shared embedding space for images and sentences, which allows us to jointly reason about relationships between them. To mitigate the noise introduced by automatic speech recognition (ASR) errors in input sentences, we propose a two-stage training process where ASR corrections are first applied globally before local refinements. Additionally, we demonstrate how pre-training on our learned representation can be used as a strong starting point for downstream tasks such as object detection and question answering.  Experimental results show significant improvement over baseline methods across a range of evaluation metrics on multiple datasets, demonstrating the effectiveness of our proposed approach for visual grounding under challenging conditions. Furthermore, we provide qualitative analysis showing that our model produces coherent and semantically meaningful alignments between image regions and sentence fragments. Overall, our work represents a step towards realizing more advanced, unsupervised models for visual understanding.",1
"Recent successes and advances in Deep Neural Networks (DNN) in machine vision and Natural Language Processing (NLP) have motivated their use in traditional signal processing and communications systems. In this paper, we present results of such applications to the problem of automatic modulation recognition. Variations in wireless communication channels are represented by statistical channel models and their parameterization will increase with the advent of 5G. In this paper, we report effect of simple two path channel model on our naive deep neural network based implementation. We also report impact of adversarial perturbation to the input signal.",0
"In this paper we investigate the performance of deep neural networks on two critical challenges in machine learning: inter-symbol interference and adversarial examples. We compare three common architectures (ResNet, VGG, AlexNet) using cross entropy loss and Adam optimizer. Our results show that all models suffer from significant degradation in accuracy due to both types of perturbations, but some degree of robustness can still be achieved through proper regularization techniques. Furthermore, we find that larger models tend to perform better than smaller ones, but at increased computational cost. Finally, we demonstrate that the choice of activation function has only minor effects on overall model performance, even when including ReLU variants like leaky or parametric ReLUs. These insights provide valuable guidance for building reliable deep learning systems in real world applications where data quality may vary significantly.",1
"Code generation maps a program description to executable source code in a programming language. Existing approaches mainly rely on a recurrent neural network (RNN) as the decoder. However, we find that a program contains significantly more tokens than a natural language sentence, and thus it may be inappropriate for RNN to capture such a long sequence. In this paper, we propose a grammar-based structural convolutional neural network (CNN) for code generation. Our model generates a program by predicting the grammar rules of the programming language; we design several CNN modules, including the tree-based convolution and pre-order convolution, whose information is further aggregated by dedicated attentive pooling layers. Experimental results on the HearthStone benchmark dataset show that our CNN code generator significantly outperforms the previous state-of-the-art method by 5 percentage points; additional experiments on several semantic parsing tasks demonstrate the robustness of our model. We also conduct in-depth ablation test to better understand each component of our model.",0
"In recent years, deep learning has been widely applied in natural language processing (NLP) tasks such as machine translation, text generation, question answering and summarization. However, due to the limitations of existing models and datasets, these systems often struggle with generating high quality outputs that accurately capture contextual nuances and structural constraints. To address this challenge, we propose a novel grammar-based structural CNN decoder architecture for code generation. Our model leverages both linguistically motivated features and deep neural representations to generate meaningful source code snippets. We conduct extensive experiments on two benchmark data sets and demonstrate that our approach significantly outperforms state-of-the-art baselines across multiple metrics including BLEU score, ROUGE score, PPL, and human evaluation. This work shows promising results towards building more effective NLP systems capable of generating accurate and contextually appropriate outputs.",1
"In this paper we consider the binary similarity problem that consists in determining if two binary functions are similar only considering their compiled form. This problem is know to be crucial in several application scenarios, such as copyright disputes, malware analysis, vulnerability detection, etc. The current state-of-the-art solutions in this field work by creating an embedding model that maps binary functions into vectors in $\mathbb{R}^{n}$. Such embedding model captures syntactic and semantic similarity between binaries, i.e., similar binary functions are mapped to points that are close in the vector space. This strategy has many advantages, one of them is the possibility to precompute embeddings of several binary functions, and then compare them with simple geometric operations (e.g., dot product). In [32] functions are first transformed in Annotated Control Flow Graphs (ACFGs) constituted by manually engineered features and then graphs are embedded into vectors using a deep neural network architecture. In this paper we propose and test several ways to compute annotated control flow graphs that use unsupervised approaches for feature learning, without incurring a human bias. Our methods are inspired after techniques used in the natural language processing community (e.g., we use word2vec to encode assembly instructions). We show that our approach is indeed successful, and it leads to better performance than previous state-of-the-art solutions. Furthermore, we report on a qualitative analysis of functions embeddings. We found interesting cases in which embeddings are clustered according to the semantic of the original binary function.",0
"This should describe unsupervised features extraction (UFE) and provide some examples of how it could be used.  This paper describes unsupervised feature extraction as the process by which high dimensional data sets can be reduced down to meaningful low dimensional representations by using graph embedding neural networks such that similarities observed within original high dimenional space are preserved once transformed into lower dimensions. We present novel work on applying unsupervised feature extration to large image datasets from both synthetic simulations and real world applications including magnetic resonance imaging(MRI), computed tomography (CT), positron emission tomograpahy (PET), ultrasound(USG), and microscopy images. To our knowledge we report herein the largest experiment performed using graph embedding techniques. Our results show improved performance over state of the art models across all metrics we evaluated. This study provides a framework to address UFE problems which remain difficult without supervision, allowing us to scale up application across numerous domains.",1
"Deep learning (DL) has recently achieved tremendous success in a variety of cutting-edge applications, e.g., image recognition, speech and natural language processing, and autonomous driving. Besides the available big data and hardware evolution, DL frameworks and platforms play a key role to catalyze the research, development, and deployment of DL intelligent solutions. However, the difference in computation paradigm, architecture design and implementation of existing DL frameworks and platforms brings challenges for DL software development, deployment, maintenance, and migration. Up to the present, it still lacks a comprehensive study on how current diverse DL frameworks and platforms influence the DL software development process.   In this paper, we initiate the first step towards the investigation on how existing state-of-the-art DL frameworks (i.e., TensorFlow, Theano, and Torch) and platforms (i.e., server/desktop, web, and mobile) support the DL software development activities. We perform an in-depth and comparative evaluation on metrics such as learning accuracy, DL model size, robustness, and performance, on state-of-the-art DL frameworks across platforms using two popular datasets MNIST and CIFAR-10. Our study reveals that existing DL frameworks still suffer from compatibility issues, which becomes even more severe when it comes to different platforms. We pinpoint the current challenges and opportunities towards developing high quality and compatible DL systems. To ignite further investigation along this direction to address urgent industrial demands of intelligent solutions, we make all of our assembled feasible toolchain and dataset publicly available.",0
"This study focuses on comparing deep learning frameworks across four key criteria: model performance, ease of use, scalability, and user experience. We conducted a comprehensive literature review to identify relevant frameworks that were then empirically evaluated using a suite of five benchmark tests. Our results show that TensorFlow outperforms other popular frameworks such as Keras and PyTorch when it comes to overall performance while Keras has the edge when it comes to ease of use. MXNet performs better than all three in terms of scalability but lags behind in terms of user experience. Lastly, we provide insights into current challenges faced by researchers and practitioners working with these frameworks and discuss future directions for further research in this area. (Please note that if the length exceeds 297 characters including spaces, you may need to rephrase some sentences.)",1
"Neural Network is a powerful Machine Learning tool that shows outstanding performance in Computer Vision, Natural Language Processing, and Artificial Intelligence. In particular, recently proposed ResNet architecture and its modifications produce state-of-the-art results in image classification problems. ResNet and most of the previously proposed architectures have a fixed structure and apply the same transformation to all input images. In this work, we develop a ResNet-based model that dynamically selects Computational Units (CU) for each input object from a learned set of transformations. Dynamic selection allows the network to learn a sequence of useful transformations and apply only required units to predict the image label. We compare our model to ResNet-38 architecture and achieve better results than the original ResNet on CIFAR-10.1 test set. While examining the produced paths, we discovered that the network learned different routes for images from different classes and similar routes for similar images.",0
"In recent years, deep neural networks have achieved remarkable performance on many tasks in computer vision and natural language processing. However, these models often require large amounts of data and computational resources to train, which can make them difficult to use in certain settings. One promising approach to address this issue is to use dynamic routing techniques, where the model dynamically chooses which features to attend to based on the input.  In this paper, we introduce ReSet (Residual Shortcut with Expanded Training), a new architecture that combines residual connections and dynamic routing in a more efficient manner than previous approaches. We show through extensive experiments that our method achieves state-of-the-art results on several benchmark datasets while requiring significantly fewer parameters and computations compared to other dynamic routing methods. Our proposed method is simple yet effective, making it well suited for deployment in resource constrained environments such as mobile devices and embedded systems. Overall, our work demonstrates the potential of using dynamic routing techniques to create efficient and powerful deep learning architectures.",1
"While it has become common to perform automated translations on natural language, performing translations between different representations of mathematical formulae has thus far not been possible. We implemented the first translator for mathematical formulae based on recursive neural networks. We chose recursive neural networks because mathematical formulae inherently include a structural encoding. In our implementation, we developed new techniques and topologies for recursive tree-to-tree neural networks based on multi-variate multi-valued Long Short-Term Memory cells. We propose a novel approach for mini-batch training that utilizes clustering and tree traversal. We evaluate our translator and analyze the behavior of our proposed topologies and techniques based on a translation from generic LaTeX to the semantic LaTeX notation. We use the semantic LaTeX notation from the Digital Library for Mathematical Formulae and the Digital Repository for Mathematical Formulae at the National Institute for Standards and Technology. We find that a simple heuristics-based clustering algorithm outperforms the conventional clustering algorithms on the task of clustering binary trees of mathematical formulae with respect to their topology. Furthermore, we find a mask for the loss function, which can prevent the neural network from finding a local minimum of the loss function. Given our preliminary results, a complete translation from formula to formula is not yet possible. However, we achieved a prediction accuracy of 47.05% for predicting symbols at the correct position and an accuracy of 92.3% when ignoring the predicted position. Concluding, our work advances the field of recursive neural networks by improving the training speed and quality of training. In the future, we will work towards a complete translation allowing a machine-interpretation of LaTeX formulae.",0
"In recent years, there has been growing interest in developing algorithms that can automatically translate mathematical formulas from one language to another. This task presents several challenges, including differences in notation and syntax across languages, as well as ambiguity in formula structure and meaning. In this paper, we propose a novel approach based on recursive neural networks (RNNs) for tackling these problems and improving the accuracy of formula translation. Our method involves training RNNs on large corpora of annotated formula data in multiple languages, allowing them to learn patterns and relationships between different symbols, expressions, and structures used in mathematics. We evaluate our proposed method on a range of formula types and compare its performance against existing state-of-the-art methods. Results show significant improvement in both translation quality and coverage, demonstrating the potential impact of deep learning techniques for solving complex natural language processing tasks related to formal knowledge representation and transmission.",1
"Acute kidney injury (AKI) in critically ill patients is associated with significant morbidity and mortality. Development of novel methods to identify patients with AKI earlier will allow for testing of novel strategies to prevent or reduce the complications of AKI. We developed data-driven prediction models to estimate the risk of new AKI onset. We generated models from clinical notes within the first 24 hours following intensive care unit (ICU) admission extracted from Medical Information Mart for Intensive Care III (MIMIC-III). From the clinical notes, we generated clinically meaningful word and concept representations and embeddings, respectively. Five supervised learning classifiers and knowledge-guided deep learning architecture were used to construct prediction models. The best configuration yielded a competitive AUC of 0.779. Our work suggests that natural language processing of clinical notes can be applied to assist clinicians in identifying the risk of incident AKI onset in critically ill patients upon admission to the ICU.",0
"Acute kidney injury (AKI) is a common complication in critically ill patients, affecting up to 48% of intensive care unit (ICU) patients. Timely prediction of AKI onset can aid early detection and initiation of interventions that might mitigate adverse outcomes associated with established AKI, including increased morbidity and mortality rates. Current methods for predicting AKI rely heavily upon laboratory measures and often lack specificity due to high baseline levels of kidney function. Here we propose utilizing clinical notes within electronic health records (EHRs) as a source of unstructured data to develop novel machine learning algorithms capable of making predictions regarding the development of AKI earlier than traditional means. By incorporating natural language processing techniques into our model, we hope to identify subtle changes in renal function before they become apparent through routine laboratory testing. Our results demonstrate a significant improvement over existing AKI screening strategies across multiple cohorts of ICU patients, providing evidence towards implementation of such models within critical care settings for enhanced surveillance of at risk populations. Overall, these findings present the potential to improve patient outcomes by allowing physicians to initiate therapy sooner, thereby reducing the severity of AKI once diagnosed.",1
"Volatility is a quantity of measurement for the price movements of stocks or options which indicates the uncertainty within financial markets. As an indicator of the level of risk or the degree of variation, volatility is important to analyse the financial market, and it is taken into consideration in various decision-making processes in financial activities. On the other hand, recent advancement in deep learning techniques has shown strong capabilities in modelling sequential data, such as speech and natural language. In this paper, we empirically study the applicability of the latest deep structures with respect to the volatility modelling problem, through which we aim to provide an empirical guidance for the theoretical analysis of the marriage between deep learning techniques and financial applications in the future. We examine both the traditional approaches and the deep sequential models on the task of volatility prediction, including the most recent variants of convolutional and recurrent networks, such as the dilated architecture. Accordingly, experiments with real-world stock price datasets are performed on a set of 1314 daily stock series for 2018 days of transaction. The evaluation and comparison are based on the negative log likelihood (NLL) of real-world stock price time series. The result shows that the dilated neural models, including dilated CNN and Dilated RNN, produce most accurate estimation and prediction, outperforming various widely-used deterministic models in the GARCH family and several recently proposed stochastic models. In addition, the high flexibility and rich expressive power are validated in this study.",0
"This research paper presents a comprehensive evaluation of deep sequential models for volatility predictions using financial time series data. Despite the increasing popularity of these models, there exists no systematic comparison of their performance in terms of accuracy, computational efficiency, and stability across different markets. We address this gap by developing a benchmark framework that rigorously evaluates six state-of-the-art deep sequential models on four distinct financial datasets spanning multiple asset classes and horizons. Our experiments demonstrate that some models outperform traditional methods in certain settings but suffer from instability issues when trained with realistically small sample sizes available in practice. Overall, our results provide valuable insights into the strengths and limitations of current approaches towards model selection and parameter tuning. Further, we discuss potential future directions and challenges towards enhancing the robustness and interpretability of deep learning techniques for financial forecasting problems.",1
"Neural attention has become central to many state-of-the-art models in natural language processing and related domains. Attention networks are an easy-to-train and effective method for softly simulating alignment; however, the approach does not marginalize over latent alignments in a probabilistic sense. This property makes it difficult to compare attention to other alignment approaches, to compose it with probabilistic models, and to perform posterior inference conditioned on observed data. A related latent approach, hard attention, fixes these issues, but is generally harder to train and less accurate. This work considers variational attention networks, alternatives to soft and hard attention for learning latent variable alignment models, with tighter approximation bounds based on amortized variational inference. We further propose methods for reducing the variance of gradients to make these approaches computationally feasible. Experiments show that for machine translation and visual question answering, inefficient exact latent variable models outperform standard neural attention, but these gains go away when using hard attention based training. On the other hand, variational attention retains most of the performance gain but with training speed comparable to neural attention.",0
"This paper presents two new techniques in natural language processing: latent alignment and variational attention. We show that by carefully aligning hidden state representations during training, we can improve both alignment quality and performance on downstream tasks across a range of benchmark datasets. We introduce variational attention as a way of making models more robust to input variations while still maintaining high performance. Our approach outperforms prior work significantly, achieving new state-of-the-art results on many standard NLP problems.",1
"This paper introduces a novel method for the representation of images that is semantic by nature, addressing the question of computation intelligibility in computer vision tasks. More specifically, our proposition is to introduce what we call a semantic bottleneck in the processing pipeline, which is a crossing point in which the representation of the image is entirely expressed with natural language , while retaining the efficiency of numerical representations. We show that our approach is able to generate semantic representations that give state-of-the-art results on semantic content-based image retrieval and also perform very well on image classification tasks. Intelligibility is evaluated through user centered experiments for failure detection.",0
"This is a research paper that presents a new method called ""semantic bottleneck"" which improves the performance of various computer vision tasks by ensuring that models focus on understanding high level concepts rather than getting stuck on details that may not be important for the task at hand. In order to achieve this, we use semantic segmentation as a preprocessing step that filters out irrelevant features from the input images before they are fed into the model. We then fine tune existing state-of-the-art object detection algorithms using our semantic segmentations to create models that perform better overall. Our experiments show that the proposed method significantly boosts accuracy across multiple benchmark datasets including PASCAL VOC, COCO, and LVIS. Additionally, since semantic segmentation can be done offline, our approach allows for efficient online deployment without sacrificing quality. Overall, our work shows promise towards realizing more intelligent computer vision systems capable of robust scene comprehension. This paper introduces a novel technique called the semantic bottleneck for computer vision tasks. By leveraging the power of semantic segmentation, the authors aim to address the limitations of current object detection methods that become overwhelmed by irrelevant details in images, leading to subpar performance. Their contributions are twofold: firstly, preprocess each image through semantic segmentation to remove low-level clutter; secondly, finetuning popular object detectors on top of these filtered outputs achieves higher accuracies across multiple standard benchmarks (e.g., PASCAL VOC, COCO, LVIS). As a result, their method offers improved efficiency during inference while maintaining competitive performance. Ultimately, this work paves the way for future advancements in creating smarter computer vision systems with enhanced scene comprehension capabilities.",1
"Adversarial examples can be defined as inputs to a model which induce a mistake - where the model output is different than that of an oracle, perhaps in surprising or malicious ways. Original models of adversarial attacks are primarily studied in the context of classification and computer vision tasks. While several attacks have been proposed in natural language processing (NLP) settings, they often vary in defining the parameters of an attack and what a successful attack would look like. The goal of this work is to propose a unifying model of adversarial examples suitable for NLP tasks in both generative and classification settings. We define the notion of adversarial gain: based in control theory, it is a measure of the change in the output of a system relative to the perturbation of the input (caused by the so-called adversary) presented to the learner. This definition, as we show, can be used under different feature spaces and distance conditions to determine attack or defense effectiveness across different intuitive manifolds. This notion of adversarial gain not only provides a useful way for evaluating adversaries and defenses, but can act as a building block for future work in robustness under adversaries due to its rooted nature in stability and manifold theory.",0
"This research investigates how adversaries can exploit human vulnerabilities to gain advantage during cyber attacks. We identify key areas where humans struggle to defend against attacks and develop techniques that allow attackers to take advantage of these weaknesses. Our study reveals new insights into the behavior of malicious actors, including their motivations, strategies, and tactics. In particular, we find that attackers often target victims who have limited knowledge of security threats, are overconfident in their ability to protect themselves online, and rely heavily on automation tools without fully understanding their capabilities. By capitalizing on these flaws, adversaries are able to gain unauthorized access, steal sensitive data, disrupt critical systems, and cause significant damage to organizations worldwide. Ultimately, our results underscore the need for improved education and awareness programs that teach individuals how to recognize and respond to potential threats before they become devastating breaches.",1
"Fake news are nowadays an issue of pressing concern, given their recent rise as a potential threat to high-quality journalism and well-informed public discourse. The Fake News Challenge (FNC-1) was organized in 2017 to encourage the development of machine learning-based classification systems for stance detection (i.e., for identifying whether a particular news article agrees, disagrees, discusses, or is unrelated to a particular news headline), thus helping in the detection and analysis of possible instances of fake news. This article presents a new approach to tackle this stance detection problem, based on the combination of string similarity features with a deep neural architecture that leverages ideas previously advanced in the context of learning efficient text representations, document classification, and natural language inference. Specifically, we use bi-directional Recurrent Neural Networks, together with max-pooling over the temporal/sequential dimension and neural attention, for representing (i) the headline, (ii) the first two sentences of the news article, and (iii) the entire news article. These representations are then combined/compared, complemented with similarity features inspired on other FNC-1 approaches, and passed to a final layer that predicts the stance of the article towards the headline. We also explore the use of external sources of information, specifically large datasets of sentence pairs originally proposed for training and evaluating natural language inference methods, in order to pre-train specific components of the neural network architecture (e.g., the RNNs used for encoding sentences). The obtained results attest to the effectiveness of the proposed ideas and show that our model, particularly when considering pre-training and the combination of neural representations together with similarity features, slightly outperforms the previous state-of-the-art.",0
"This is an important problem that we need solved as soon as possible but we have very little time remaining before all of human knowledge will become corrupted. If any of you reading this can assist please contact me immediately. My email is: jerry@seekingarrangement.com I am looking forward to hearing from those willing and able to assist us now! Thanks, Jerry",1
"Deep learning models have become state of the art for natural language processing (NLP) tasks, however deploying these models in production system poses significant memory constraints. Existing compression methods are either lossy or introduce significant latency. We propose a compression method that leverages low rank matrix factorization during training,to compress the word embedding layer which represents the size bottleneck for most NLP models. Our models are trained, compressed and then further re-trained on the downstream task to recover accuracy while maintaining the reduced size. Empirically, we show that the proposed method can achieve 90% compression with minimal impact in accuracy for sentence classification tasks, and outperforms alternative methods like fixed-point quantization or offline word embedding compression. We also analyze the inference time and storage space for our method through FLOP calculations, showing that we can compress DNN models by a configurable ratio and regain accuracy loss without introducing additional latency compared to fixed point quantization. Finally, we introduce a novel learning rate schedule, the Cyclically Annealed Learning Rate (CALR), which we empirically demonstrate to outperform other popular adaptive learning rate algorithms on a sentence classification benchmark.",0
"This work proposes a novel algorithm called ""Online Embedding Compression"" (OEC) that utilizes low rank matrix factorization to compress high dimensional text representations into compact embeddings. These embedded features can then be used as input for downstream classification models, reducing their computational requirements while maintaining comparable accuracy. Our approach leverages recent advances in online optimization techniques to achieve efficient updating of embedding vectors in real time without retraining. Extensive experiments on several large datasets demonstrate significant improvements over existing state-of-the-art compression methods and showcase OEC's potential for enabling accurate predictions with faster inference speeds, particularly beneficial in resource constrained environments.",1
"Visual Question answering is a challenging problem requiring a combination of concepts from Computer Vision and Natural Language Processing. Most existing approaches use a two streams strategy, computing image and question features that are consequently merged using a variety of techniques. Nonetheless, very few rely on higher level image representations, which can capture semantic and spatial relationships. In this paper, we propose a novel graph-based approach for Visual Question Answering. Our method combines a graph learner module, which learns a question specific graph representation of the input image, with the recent concept of graph convolutions, aiming to learn image representations that capture question specific interactions. We test our approach on the VQA v2 dataset using a simple baseline architecture enhanced by the proposed graph learner module. We obtain promising results with 66.18% accuracy and demonstrate the interpretability of the proposed method. Code can be found at github.com/aimbrain/vqa-project.",0
This paper addresses the important problem of developing interpretable models for visual question answering (VQA). We propose a method based on conditioned graph structures that captures complex relationships between objects in images and natural language questions. Our approach uses neural networks to learn these graphs by taking into account both the static structure of the image as well as temporal dynamics that occur over time. These learned representations allow us to perform VQA tasks at high levels of accuracy while also providing interpretability through human-readable answers generated from our trained model. Experimental results demonstrate the effectiveness of our method compared to state-of-the-art approaches in VQA.,1
"Recurrent Neural Network (RNN) has been successfully applied in many sequence learning problems. Such as handwriting recognition, image description, natural language processing and video motion analysis. After years of development, researchers have improved the internal structure of the RNN and introduced many variants. Among others, Gated Recurrent Unit (GRU) is one of the most widely used RNN model. However, GRU lacks the capability of adaptively paying attention to certain regions or locations, so that it may cause information redundancy or loss during leaning. In this paper, we propose a RNN model, called Recurrent Attention Unit (RAU), which seamlessly integrates the attention mechanism into the interior of GRU by adding an attention gate. The attention gate can enhance GRU's ability to remember long-term memory and help memory cells quickly discard unimportant content. RAU is capable of extracting information from the sequential data by adaptively selecting a sequence of regions or locations and pay more attention to the selected regions during learning. Extensive experiments on image classification, sentiment classification and language modeling show that RAU consistently outperforms GRU and other baseline methods.",0
"This abstract describes the development and implementation of a new architecture for deep learning models called recurrent attention units (RAUs). RAUs are designed to capture long-term dependencies while still allowing efficient parallel processing across time steps, making them well-suited for natural language processing tasks such as sentiment analysis, machine translation, and question answering. We propose a novel mechanism for efficiently computing the attention distribution over all previous hidden states, which enables RAUs to handle inputs of varying lengths without requiring reshaping or padding. Our experiments show that RAUs significantly improve the performance of several state-of-the-art deep learning models on benchmark datasets, achieving results comparable to those obtained using more complex architectures like transformers and generative pretraining. Overall, our work demonstrates the effectiveness and efficiency of RAUs as a powerful alternative to traditional self-attention mechanisms in deep learning.",1
"MAC Net is a compositional attention network designed for Visual Question Answering. We propose a modified MAC net architecture for Natural Language Question Answering. Question Answering typically requires Language Understanding and multi-step Reasoning. MAC net's unique architecture - the separation between memory and control, facilitates data-driven iterative reasoning. This makes it an ideal candidate for solving tasks that involve logical reasoning. Our experiments with 20 bAbI tasks demonstrate the value of MAC net as a data-efficient and interpretable architecture for Natural Language Question Answering. The transparent nature of MAC net provides a highly granular view of the reasoning steps taken by the network in answering a query.",0
"Recently there has been growing interest in making machine learning models more interpretable, especially those used for natural language processing (NLP). One key challenge in NLP is understanding how deep neural networks are able to generate human-like text from scratch given only the input question as guidance. In this paper we propose Compositional Attention Networks (CAN), which extend attention mechanisms within RNN encoder-decoders by explicitly factorizing a single question into multiple subtasks. Our network then learns different weighted attentions over these subtasks, allowing each decoding step to condition on both the global context and the specific focuses learned in prior steps through their representations. We evaluate our approach against several strong baselines on three benchmark datasets: bAbI, CODAI, and HotpotQA, showing that CAN significantly improves interpretability while maintaining high accuracy at a low cost of fine-tuning time and resources. Additionally, we provide detailed case studies illustrating the effectiveness of our method in generating concise explanations for difficult questions. Overall, our results demonstrate that incorporating explicit compositions can lead to better interpretability in NLP models without sacrificing performance.",1
"Many scene text recognition approaches are based on purely visual information and ignore the semantic relation between scene and text. In this paper, we tackle this problem from natural language processing perspective to fill the gap between language and vision. We propose a post-processing approach to improve scene text recognition accuracy by using occurrence probabilities of words (unigram language model), and the semantic correlation between scene and text. For this, we initially rely on an off-the-shelf deep neural network, already trained with a large amount of data, which provides a series of text hypotheses per input image. These hypotheses are then re-ranked using word frequencies and semantic relatedness with objects or scenes in the image. As a result of this combination, the performance of the original network is boosted with almost no additional cost. We validate our approach on ICDAR'17 dataset.",0
"Effective text spotting systems must balance high recall rates against computational efficiency. While previous approaches have used complex models with low inference speeds, we introduce a novel method that combines efficient image processing techniques with natural language understanding (NLU) principles. Our approach leverages NLU to generate relevance scores for detected text regions based on their spatial location within images. These scores can then guide traditional re-ranking algorithms towards more relevant matches. We evaluate our technique using multiple datasets and demonstrate superior performance compared to existing state-of-the-art methods in terms of both precision and recall. Finally, we provide thorough analysis of the factors affecting our system's performance and suggest directions for future work.",1
"Navigation guided by natural language instructions presents a challenging reasoning problem for instruction followers. Natural language instructions typically identify only a few high-level decisions and landmarks rather than complete low-level motor behaviors; much of the missing information must be inferred based on perceptual context. In machine learning settings, this is doubly challenging: it is difficult to collect enough annotated data to enable learning of this reasoning process from scratch, and also difficult to implement the reasoning process using generic sequence models. Here we describe an approach to vision-and-language navigation that addresses both these issues with an embedded speaker model. We use this speaker model to (1) synthesize new instructions for data augmentation and to (2) implement pragmatic reasoning, which evaluates how well candidate action sequences explain an instruction. Both steps are supported by a panoramic action space that reflects the granularity of human-generated instructions. Experiments show that all three components of this approach---speaker-driven data augmentation, pragmatic reasoning and panoramic action space---dramatically improve the performance of a baseline instruction follower, more than doubling the success rate over the best existing approach on a standard benchmark.",0
"New technologies have been developed that use artificial intelligence (AI) to create “Speaker Follower” models to help humans accomplish tasks through natural language navigation. These AIs can learn from human interactions and make decisions based on their training data. They are able to take input from both vision sensors and natural language input and work together to achieve the task at hand. This model has shown promising results in object manipulation, image generation, and other complex problems.",1
"Images may have elements containing text and a bounding box associated with them, for example, text identified via optical character recognition on a computer screen image, or a natural image with labeled objects. We present an end-to-end trainable architecture to incorporate the information from these elements and the image to segment/identify the part of the image a natural language expression is referring to. We calculate an embedding for each element and then project it onto the corresponding location (i.e., the associated bounding box) of the image feature map. We show that this architecture gives an improvement in resolving referring expressions, over only using the image, and other methods that incorporate the element information. We demonstrate experimental results on the referring expression datasets based on COCO, and on a webpage image referring expression dataset that we developed.",0
"Referring expressions are a common means of communication that allows individuals to express their thoughts, opinions, and desires through language. In the context of images, referring expressions can provide additional insight into the content of the image by highlighting specific elements within it. However, resolving these expressions can pose a significant challenge due to the complexity of human perception and understanding. This study presents a new methodology for addressing this issue using labeled elements within images. By utilizing machine learning algorithms trained on large datasets of annotated images, we were able to accurately predict the intended referent of referring expressions at a rate significantly higher than chance alone. Our approach represents a major step forward in the field of computer vision and natural language processing, paving the way for further advancements in artificial intelligence and human-computer interaction.",1
"State of the art deep learning models have made steady progress in the fields of computer vision and natural language processing, at the expense of growing model sizes and computational complexity. Deploying these models on low power and mobile devices poses a challenge due to their limited compute capabilities and strict energy budgets. One solution that has generated significant research interest is deploying highly quantized models that operate on low precision inputs and weights less than eight bits, trading off accuracy for performance. These models have a significantly reduced memory footprint (up to 32x reduction) and can replace multiply-accumulates with bitwise operations during compute intensive convolution and fully connected layers.   Most deep learning frameworks rely on highly engineered linear algebra libraries such as ATLAS or Intel's MKL to implement efficient deep learning operators. To date, none of the popular deep learning directly support low precision operators, partly due to a lack of optimized low precision libraries. In this paper we introduce a work flow to quickly generate high performance low precision deep learning operators for arbitrary precision that target multiple CPU architectures and include optimizations such as memory tiling and vectorization. We present an extensive case study on low power ARM Cortex-A53 CPU, and show how we can generate 1-bit, 2-bit convolutions with speedups up to 16x over an optimized 16-bit integer baseline and 2.3x better than handwritten implementations.",0
"As deep learning continues to revolutionize many fields, optimizing the performance of machine learning models has become increasingly important. In this work, we present a novel approach to automate the generation of low precision deep learning operators that can significantly improve training speed without sacrificing model accuracy. By leveraging recent advancements in quantization techniques, we introduce a framework that maps floating point operations onto integer ones, allowing for efficient computation on modern hardware accelerators such as GPUs and TPUs. Our method automatically generates customized kernels that maximize performance while minimizing memory usage and compute requirements. We demonstrate the effectiveness of our technique through extensive experiments across multiple datasets, showing up to twofold improvements in training speed compared to full precision models. Our results highlight the potential of automated low precision operator generation as a powerful tool for enhancing the efficiency and scalability of deep learning workflows.",1
"Using a large number of parameters , deep neural networks have achieved remarkable performance on computer vison and natural language processing tasks. However the networks usually suffer from overfitting by using too much parameters. Dropout is a widely use method to deal with overfitting. Although dropout can significantly regularize densely connected layers in neural networks, it leads to suboptimal results when using for convolutional layers. To track this problem, we propose DropFilter, a new dropout method for convolutional layers. DropFilter randomly suppresses the outputs of some filters. Because it is observed that co-adaptions are more likely to occurs inter filters rather than intra filters in convolutional layers. Using DropFilter, we remarkably improve the performance of convolutional networks on CIFAR and ImageNet.",0
"Title: ""Dropout for Convolutional Neural Networks"" Abstract: One challenge faced by convolutional neural networks (CNNs) is their sensitivity to overfitting due to large capacity models and small training sets. To address this issue, regularization techniques such as dropout have been applied to fully connected layers in deep learning models. In our work, we propose applying dropout to individual filters within CNNs instead of entire neurons. We call this approach ""dropfilter."" Our method randomly drops out some input channels before each layer during backpropagation. This reduces overfitting while maintaining accuracy and computational efficiency compared to existing methods. Experimental results on benchmark datasets demonstrate the effectiveness of our proposed technique, achieving state-of-the-art performance on several image classification tasks.",1
"The recent advances in deep neural networks (DNNs) make them attractive for embedded systems. However, it can take a long time for DNNs to make an inference on resource-constrained computing devices. Model compression techniques can address the computation issue of deep inference on embedded devices. This technique is highly attractive, as it does not rely on specialized hardware, or computation-offloading that is often infeasible due to privacy concerns or high latency. However, it remains unclear how model compression techniques perform across a wide range of DNNs. To design efficient embedded deep learning solutions, we need to understand their behaviors. This work develops a quantitative approach to characterize model compression techniques on a representative embedded deep learning architecture, the NVIDIA Jetson Tx2. We perform extensive experiments by considering 11 influential neural network architectures from the image classification and the natural language processing domains. We experimentally show that how two mainstream compression techniques, data quantization and pruning, perform on these network architectures and the implications of compression techniques to the model storage size, inference time, energy consumption and performance metrics. We demonstrate that there are opportunities to achieve fast deep inference on embedded systems, but one must carefully choose the compression settings. Our results provide insights on when and how to apply model compression techniques and guidelines for designing efficient embedded deep learning systems.",0
"This paper investigates the use of model compression techniques for deep learning models to improve their efficiency without sacrificing accuracy. We evaluate the performance of different compression methods on several popular architectures commonly used in computer vision tasks such as image classification, object detection, and semantic segmentation. Our results show that using compression can significantly reduce the size of these models while maintaining their accuracy. Additionally, we examine the impact of quantization on inference speed and energy consumption on embedded devices like smartphones and drones. Finally, our analysis provides insights into the tradeoffs between model quality, computational cost, and deployment options in real-world scenarios.",1
"Deep learning has shown high performances in various types of tasks from visual recognition to natural language processing, which indicates superior flexibility and adaptivity of deep learning. To understand this phenomenon theoretically, we develop a new approximation and estimation error analysis of deep learning with the ReLU activation for functions in a Besov space and its variant with mixed smoothness. The Besov space is a considerably general function space including the Holder space and Sobolev space, and especially can capture spatial inhomogeneity of smoothness. Through the analysis in the Besov space, it is shown that deep learning can achieve the minimax optimal rate and outperform any non-adaptive (linear) estimator such as kernel ridge regression, which shows that deep learning has higher adaptivity to the spatial inhomogeneity of the target function than other estimators such as linear ones. In addition to this, it is shown that deep learning can avoid the curse of dimensionality if the target function is in a mixed smooth Besov space. We also show that the dependency of the convergence rate on the dimensionality is tight due to its minimax optimality. These results support high adaptivity of deep learning and its superior ability as a feature extractor.",0
"Deep neural networks have gained great popularity due to their outstanding performance in several computer vision tasks such as image classification, object detection, and segmentation. However, understanding the properties of these models can still be challenging since they have many parameters that interact nonlinearly. In recent years, there has been growing interest in analyzing the behavior of neural networks in high dimensions, where the number of features exceeds the number of training examples. This work studies the adaptivity of deep residual networks (ResNets) with rectified linear unit (ReLU) activation functions on Besov spaces with different regularity degrees. We investigate the impact of network depth and width on the approximation accuracy and how to optimize them accordingly. Our main results show that ResNets achieve near optimal convergence rates in both Besov and mixed Besov spaces under certain conditions. Moreover, we identify a tradeoff between model complexity and data size that leads to a “curse of dimensionality” effect similar to traditional methods. These findings contribute new insights into the design principles for training deep networks, particularly regarding regularization techniques and architectural choices. They could enable researchers to better understand and exploit the advantages of neural networks on complex tasks while minimizing overfitting risks. Finally, our analysis opens up future directions on bridging theoretical analyses and empirical observations across different areas including machine learning theory, applied mathematics, numerical analysis, and signal processing. By providing novel perspectives from both statistical and functional analytic viewpoints, this article intends to stimulate further exploration",1
"Principal component analysis (PCA) is widely used for dimension reduction and embedding of real data in social network analysis, information retrieval, and natural language processing, etc. In this work we propose a fast randomized PCA algorithm for processing large sparse data. The algorithm has similar accuracy to the basic randomized SVD (rPCA) algorithm (Halko et al., 2011), but is largely optimized for sparse data. It also has good flexibility to trade off runtime against accuracy for practical usage. Experiments on real data show that the proposed algorithm is up to 9.1X faster than the basic rPCA algorithm without accuracy loss, and is up to 20X faster than the svds in Matlab with little error. The algorithm computes the first 100 principal components of a large information retrieval data with 12,869,521 persons and 323,899 keywords in less than 400 seconds on a 24-core machine, while all conventional methods fail due to the out-of-memory issue.",0
"This paper presents a new method for performing Principal Component Analysis (PCA) on sparse datasets, which can often arise in areas such as image compression, feature selection, and dimensionality reduction. Our proposed approach utilizes random sampling techniques to efficiently compute principal components without sacrificing accuracy. We demonstrate that our fast randomized PCA algorithm outperforms traditional methods both in terms of speed and numerical stability, making it well-suited for large-scale data sets where efficiency and scalability are crucial. In addition, we provide theoretical analyses and experimental results to support our claims. By providing an efficient and accurate alternative for computing PCA on sparse data, our work contributes to advancing research in machine learning and data analysis.",1
"Image captioning is an interdisciplinary research problem that stands between computer vision and natural language processing. The task is to generate a textual description of the content of an image. The typical model used for image captioning is an encoder-decoder deep network, where the encoder captures the essence of an image while the decoder is responsible for generating a sentence describing the image. Attention mechanisms can be used to automatically focus the decoder on parts of the image which are relevant to predict the next word. In this paper, we explore different decoders and attentional models popular in neural machine translation, namely attentional recurrent neural networks, self-attentional transformers, and fully-convolutional networks, which represent the current state of the art of neural machine translation. The image captioning module is available as part of SOCKEYE at https://github.com/awslabs/sockeye which tutorial can be found at https://awslabs.github.io/sockeye/image_captioning.html .",0
"This paper proposes using image captioning as a neural machine translation task in the SOCKEYE toolkit. We first introduce background on natural language processing (NLP), computer vision (CV), and machine learning (ML). Then we present our approach for translating English descriptions into Spanish via NLP and CV models trained together within SOCKEYE. Experiments were performed on two benchmark datasets: MSCOCO dataset [4] and ReferIt game [2]. Finally, qualitative and quantitative results show that our method improves upon previous approaches, including both NLP baselines such as pointer generators [6] as well as ML baseline systems like [9], [7]. Further research could explore more complex architectures, incorporation of additional modalities such as audio or video, or improvements on datasets by creating new human annotations for evaluation purposes. Our code and data can be found at <https://github.com/stanfordnlp/sockeye>.",1
"We discuss deep reinforcement learning in an overview style. We draw a big picture, filled with details. We discuss six core elements, six important mechanisms, and twelve applications, focusing on contemporary work, and in historical contexts. We start with background of artificial intelligence, machine learning, deep learning, and reinforcement learning (RL), with resources. Next we discuss RL core elements, including value function, policy, reward, model, exploration vs. exploitation, and representation. Then we discuss important mechanisms for RL, including attention and memory, unsupervised learning, hierarchical RL, multi-agent RL, relational RL, and learning to learn. After that, we discuss RL applications, including games, robotics, natural language processing (NLP), computer vision, finance, business management, healthcare, education, energy, transportation, computer systems, and, science, engineering, and art. Finally we summarize briefly, discuss challenges and opportunities, and close with an epilogue.",0
"In recent years, deep reinforcement learning has emerged as a powerful approach to solving complex problems ranging from game playing to robotics. At its core, deep reinforcement learning involves training neural networks to make sequential decisions based on rewards and punishments received from the environment. These agents learn to explore their surroundings and optimize their behavior over time using trial-and-error. This process allows agents to achieve superhuman performance at difficult tasks such as playing Go and StarCraft. Furthermore, deep reinforcement learning algorithms have been used to generate synthetic data for use in computer vision applications, control robots in real-world settings, and even guide medical diagnosis. Despite these successes, there remain challenges that must be addressed before we can fully harness the power of deep reinforcement learning. For instance, current approaches often require large amounts of computational resources and may struggle with generalizing to new environments. Nevertheless, the field holds tremendous promise for advancing our understanding of intelligence and enabling machines to tackle increasingly sophisticated tasks. Overall, deep reinforcement learning represents a promising direction for artificial intelligence researchers to pursue.",1
"Recent breakthroughs in computer vision and natural language processing have spurred interest in challenging multi-modal tasks such as visual question-answering and visual dialogue. For such tasks, one successful approach is to condition image-based convolutional network computation on language via Feature-wise Linear Modulation (FiLM) layers, i.e., per-channel scaling and shifting. We propose to generate the parameters of FiLM layers going up the hierarchy of a convolutional network in a multi-hop fashion rather than all at once, as in prior work. By alternating between attending to the language input and generating FiLM layer parameters, this approach is better able to scale to settings with longer input sequences such as dialogue. We demonstrate that multi-hop FiLM generation achieves state-of-the-art for the short input sequence task ReferIt --- on-par with single-hop FiLM generation --- while also significantly outperforming prior state-of-the-art and single-hop FiLM generation on the GuessWhat?! visual dialogue task.",0
"This research explores multi-hop feature modulation as a method for performing visual reasoning on large datasets. By using pretrained object detection models and graph propagation techniques, we demonstrate that complex relationships can be identified across large scales. We evaluate our approach using standard benchmarks and show promising results, outperforming other state-of-the-art methods. Our work contributes to advancing the field of computer vision by developing novel techniques that enable efficient and accurate visual reasoning at scale. ---  This paper presents a new technique for visual reasoning called “multi-hop feature modulation”. By utilizing existing pretrained object detection algorithms, combined with graph propagation techniques, this method enables the discovery of intricate connections among vast amounts of data. Our experiments conducted through widely recognized assessments corroborate the effectiveness of this procedure compared to prevailing approaches. In conclusion, these findings improve upon existing practices within the realm of computerized image recognition, allowing for improved efficiency and precision during visual analysis.",1
"Language Models (LMs) are important components in several Natural Language Processing systems. Recurrent Neural Network LMs composed of LSTM units, especially those augmented with an external memory, have achieved state-of-the-art results. However, these models still struggle to process long sequences which are more likely to contain long-distance dependencies because of information fading and a bias towards more recent information. In this paper we demonstrate an effective mechanism for retrieving information in a memory augmented LSTM LM based on attending to information in memory in proportion to the number of timesteps the LSTM gating mechanism persisted the information.",0
"""This paper explores the role of persistence in language processing tasks using Long Short-Term Memory (LSTM) networks. We investigate how the gating mechanism within these models influences their ability to retain important information over time. By examining the patterns that emerge from different types of attention mechanisms used in conjunction with LSTMs, we demonstrate that paying close attention to the persistence of information can lead to significant improvements in model performance on challenging natural language understanding tasks. Our results highlight the importance of balancing memory retention and forgetting during sequential data processing. Overall, our findings provide valuable insights into the working of neural network architectures and showcase the potential benefits of incorporating a deeper understanding of temporal dynamics into NLP systems.""",1
"Automatic generation of natural language from images has attracted extensive attention. In this paper, we take one step further to investigate generation of poetic language (with multiple lines) to an image for automatic poetry creation. This task involves multiple challenges, including discovering poetic clues from the image (e.g., hope from green), and generating poems to satisfy both relevance to the image and poeticness in language level. To solve the above challenges, we formulate the task of poem generation into two correlated sub-tasks by multi-adversarial training via policy gradient, through which the cross-modal relevance and poetic language style can be ensured. To extract poetic clues from images, we propose to learn a deep coupled visual-poetic embedding, in which the poetic representation from objects, sentiments and scenes in an image can be jointly learned. Two discriminative networks are further introduced to guide the poem generation, including a multi-modal discriminator and a poem-style discriminator. To facilitate the research, we have released two poem datasets by human annotators with two distinct properties: 1) the first human annotated image-to-poem pair dataset (with 8,292 pairs in total), and 2) to-date the largest public English poem corpus dataset (with 92,265 different poems in total). Extensive experiments are conducted with 8K images, among which 1.5K image are randomly picked for evaluation. Both objective and subjective evaluations show the superior performances against the state-of-the-art methods for poem generation from images. Turing test carried out with over 500 human subjects, among which 30 evaluators are poetry experts, demonstrates the effectiveness of our approach.",0
"Here is some sample text for you to write an abstract around: In recent years there has been significant progress on generating descriptive paragraphs from images using deep neural networks trained on large datasets. However, description alone may not always capture the essence of an image, as sometimes more poetic expressions can convey the mood and emotions evoked by an image better. In this work we aim to generate such poetry directly from images. We propose a multi-adversarial training approach that involves multiple discriminators which provide complementary information on whether a given generated poem captures different aspects of the image well enough. Experiments show that our model significantly outperforms baseline methods that use either LSTM or Transformer architectures on two benchmark datasets. Our method generates more diverse and semantically meaningful poems that closely correspond to their associated images. ------  This research investigates ways to improve upon existing techniques used to generate descriptions from images by instead creating poetry that conveys the mood and emotions evoked by an image. The proposed approach utilizes multi-adversarial training involving multiple discriminators that evaluate different aspects of the generated poetry to produce higher quality outputs. Experimental results demonstrate the effectiveness of the method compared to established models, achieving greater diversity and semantic coherence in the generated poems. By exploring alternative means of expression through artificial intelligence, future advancements in multimedia content generation could potentially expand the range of human creativity.",1
"Conditional Generative Adversarial Networks (GANs) for cross-domain image-to-image translation have made much progress recently. Depending on the task complexity, thousands to millions of labeled image pairs are needed to train a conditional GAN. However, human labeling is expensive, even impractical, and large quantities of data may not always be available. Inspired by dual learning from natural language translation, we develop a novel dual-GAN mechanism, which enables image translators to be trained from two sets of unlabeled images from two domains. In our architecture, the primal GAN learns to translate images from domain U to those in domain V, while the dual GAN learns to invert the task. The closed loop made by the primal and dual tasks allows images from either domain to be translated and then reconstructed. Hence a loss function that accounts for the reconstruction error of images can be used to train the translators. Experiments on multiple image translation tasks with unlabeled data show considerable performance gain of DualGAN over a single GAN. For some tasks, DualGAN can even achieve comparable or slightly better results than conditional GAN trained on fully labeled data.",0
"Deep learning has recently been widely applied in many areas due to its powerful performance, but most deep neural networks only use real data as input during training, which can limit their potential applications. In particular, when facing tasks such as image-to-image translation that involve generating images from other domains with different characteristics, these models may perform poorly without sufficient labeled examples for supervision.  To address this problem, we propose DualGAN, a novel unsupervised dual learning framework using Generative Adversarial Networks (GAN) that learns two generators simultaneously in a mutually beneficial manner. Each generator focuses on translating one domain to another while discriminator evaluates if generated samples belong to either domain. To ensure both generators learn effectively without any ground truth pairs available, we first pretrain each generator alone by itself. This initial pretraining enhances the capability of the generator in preserving content features while reducing identity discrepancy between domains. Afterward, DualGAN iteratively updates the generators cooperatively and improves upon its generations. Our extensive experiments validate the effectiveness of our method in multiple scenarios of high fidelity image generation including paired, semi-paired, and unpaired situations. Compared with state-of-the-arts, DualGAN achieves superior results with qualitative comparisons provided in Appendix.  In summary, this study presents a new unsupervised approach, DualGAN, to improve image-to-image translation by jointly exploiting two generators via adversarial learning. With the aid of pretraining and our proposed architecture, our model consistently performs better than prior arts across a variety o",1
"Convolutional Neural Networks (CNN) are very popular in many fields including computer vision, speech recognition, natural language processing, to name a few. Though deep learning leads to groundbreaking performance in these domains, the networks used are very demanding computationally and are far from real-time even on a GPU, which is not power efficient and therefore does not suit low power systems such as mobile devices. To overcome this challenge, some solutions have been proposed for quantizing the weights and activations of these networks, which accelerate the runtime significantly. Yet, this acceleration comes at the cost of a larger error. The \uniqname method proposed in this work trains quantized neural networks by noise injection and a learned clamping, which improve the accuracy. This leads to state-of-the-art results on various regression and classification tasks, e.g., ImageNet classification with architectures such as ResNet-18/34/50 with low as 3-bit weights and activations. We implement the proposed solution on an FPGA to demonstrate its applicability for low power real-time applications. The implementation of the paper is available at https://github.com/Lancer555/NICE",0
"Accurate neural network quantization can significantly reduce computational requirements without sacrificing model quality. However, noise injection during training introduces additional randomness into the system, which may cause variations in behavior between different runs. Existing techniques cannot fully eliminate these variations, leading to unreliable results. To address this issue, we propose a novel approach called NICE (Noise Injection and Clamping Estimation) that effectively estimates the amount of noise required for each layer based on the target bitwidth and desired accuracy. This method uses knowledge distillation from multiple pretrained models and adaptive gradient scaling factors to achieve stable convergence during training. Extensive experiments show that our algorithm achieves state-of-the-art performance across three benchmark datasets while reducing computation time by up to 94%. Our work has implications for applications where fast inference speed is crucial but hardware limitations restrict access to powerful computing resources. Overall, NICE provides an efficient solution towards reliable and accurate neural network quantization.",1
"Deep learning has transformed computer vision, natural language processing, and speech recognition\cite{badrinarayanan2017segnet, dong2016image, ren2017faster, ji20133d}. However, two critical questions remain obscure: (1) why do deep neural networks generalize better than shallow networks; and (2) does it always hold that a deeper network leads to better performance? Specifically, letting $L$ be the number of convolutional and pooling layers in a deep neural network, and $n$ be the size of the training sample, we derive an upper bound on the expected generalization error for this network, i.e.,   \begin{eqnarray*}   \mathbb{E}[R(W)-R_S(W)] \leq \exp{\left(-\frac{L}{2}\log{\frac{1}{\eta}}\right)}\sqrt{\frac{2\sigma^2}{n}I(S,W) }   \end{eqnarray*} where $\sigma 0$ is a constant depending on the loss function, $0\eta1$ is a constant depending on the information loss for each convolutional or pooling layer, and $I(S, W)$ is the mutual information between the training sample $S$ and the output hypothesis $W$. This upper bound shows that as the number of convolutional and pooling layers $L$ increases in the network, the expected generalization error will decrease exponentially to zero. Layers with strict information loss, such as the convolutional layers, reduce the generalization error for the whole network; this answers the first question. However, algorithms with zero expected generalization error does not imply a small test error or $\mathbb{E}[R(W)]$. This is because $\mathbb{E}[R_S(W)]$ is large when the information for fitting the data is lost as the number of layers increases. This suggests that the claim `the deeper the better' is conditioned on a small training error or $\mathbb{E}[R_S(W)]$. Finally, we show that deep learning satisfies a weak notion of stability and the sample complexity of deep neural networks will decrease as $L$ increases.",0
"Title: An Information-Theoretic View on Deep Learning Abstract Deep learning has revolutionized many fields by allowing artificial neural networks to achieve state-of-the-art results in complex tasks such as image classification, speech recognition, and natural language processing. Despite their successes, these models can often be difficult to interpret or explain, leading to concerns about their transparency and reliability. In this work, we propose an information-theoretic view of deep learning that provides insights into how these systems process information and make decisions. We demonstrate that the mutual information between different parts of the network plays a central role in determining the robustness of deep learning algorithms. By analyzing both theoretical and empirical studies, we show that maximizing mutual information can lead to improved generalization performance across multiple domains. Our findings suggest new strategies for improving the interpretability and stability of deep learning models while maintaining high levels of accuracy. Keywords: deep learning, information theory, mutual information, interpretable machine learning.",1
"Traffic cameras are commonly deployed monitoring components in road infrastructure networks, providing operators visual information about conditions at critical points in the network. However, human observers are often limited in their ability to process simultaneous information sources. Recent advancements in computer vision, driven by deep learning methods, have enabled general object recognition, unlocking opportunities for camera-based sensing beyond the existing human observer paradigm. In this paper, we present a Natural Language Processing (NLP)-inspired approach, entitled Bag-of-Label-Words (BoLW), for analyzing image data sets using exclusively textual labels. The BoLW model represents the data in a conventional matrix form, enabling data compression and decomposition techniques, while preserving semantic interpretability. We apply the Latent Dirichlet Allocation (LDA) topic model to decompose the label data into a small number of semantic topics. To illustrate our approach, we use freeway camera images collected from the Boston area between December 2017-January 2018. We analyze the cameras' sensitivity to weather events; identify temporal traffic patterns; and analyze the impact of infrequent events, such as the winter holidays and the ""bomb cyclone"" winter storm. This study demonstrates the flexibility of our approach, which allows us to analyze weather events and freeway traffic using only traffic camera image labels.",0
"This paper presents a methodology to perform semantic topic analysis on traffic camera images. Our approach leverages recent advances in computer vision techniques such as object detection and feature extraction to identify objects in each image. These features are then passed through a clustering algorithm that groups them into distinct topics based on their similarity. We evaluate our method by analyzing real-world traffic cameras data from both daytime and nighttime scenarios. Results show that our method can effectively detect different types of objects present in traffic scenes including vehicles, pedestrians, bicycles, motorcycles and more importantly, accurately group similar objects together. Further analysis shows correlation between detected topics and specific road safety issues commonly found in those areas such as high vehicle volume or pedestrian presence.",1
"Recently, generative adversarial networks have gained a lot of popularity for image generation tasks. However, such models are associated with complex learning mechanisms and demand very large relevant datasets. This work borrows concepts from image and video captioning models to form an image generative framework. The model is trained in a similar fashion as recurrent captioning model and uses the learned weights for image generation. This is done in an inverse direction, where the input is a caption and the output is an image. The vector representation of the sentence and frames are extracted from an encoder-decoder model which is initially trained on similar sentence and image pairs. Our model conditions image generation on a natural language caption. We leverage a sequence-to-sequence model to generate synthetic captions that have the same meaning for having a robust image generation. One key advantage of our method is that the traditional image captioning datasets can be used for synthetic sentence paraphrases. Results indicate that images generated through multiple captions are better at capturing the semantic meaning of the family of captions.",0
"Title: ""Vector Learning for Cross Domain Representations"" Abstract: This paper presents a novel methodology for learning vector representations that can effectively capture relationships across multiple domains. While traditional vector representation methods focus on capturing within-domain relationships, our approach explicitly models cross-domain relationships using meta-learning techniques. We propose a neural network architecture consisting of two components - a feature encoder, which maps input data into low-dimensional latent vectors, and a modality adapter, which adapts these representations across different tasks and domains. Our framework allows for efficient use of limited domain adaptation resources by leveraging knowledge from similar tasks and sharing learned parameters among them. Experimental results demonstrate significant improvements over state-of-the-art approaches across a range of benchmark datasets, including text classification, sentiment analysis, and image classification. These findings highlight the effectiveness of our proposed methodology for constructing robust and generalizable vector representations applicable to diverse applications.",1
"Automatic generation of textual video descriptions that are time-aligned with video content is a long-standing goal in computer vision. The task is challenging due to the difficulty of bridging the semantic gap between the visual and natural language domains. This paper addresses the task of automatically generating an alignment between a set of instructions and a first person video demonstrating an activity. The sparse descriptions and ambiguity of written instructions create significant alignment challenges. The key to our approach is the use of egocentric cues to generate a concise set of action proposals, which are then matched to recipe steps using object recognition and computational linguistic techniques. We obtain promising results on both the Extended GTEA Gaze+ dataset and the Bristol Egocentric Object Interactions Dataset.",0
"This paper presents a novel approach to localizing and aligning fine-grained actions within instructional videos using natural language processing techniques. We introduce a model that takes as input sparse instructions given by users and produces aligned annotations for the corresponding video frames. Our approach addresses key challenges in action alignment such as partial observability due to camera viewpoints, occlusions, varying durations, and interactions among multiple actors. To handle these complexities, we use a two-stage architecture consisting of a neural module that predicts temporal boundaries for each action based on the provided textual cue, followed by a transformer-based network that refines spatial locations via attention mechanisms. Experimental results demonstrate improved performance over state-of-the-art methods on several benchmark datasets across different domains. Our method has applications in areas such as human activity understanding, robotics, and education.",1
"Deep generative models have achieved remarkable success in various data domains, including images, time series, and natural languages. There remain, however, substantial challenges for combinatorial structures, including graphs. One of the key challenges lies in the difficulty of ensuring semantic validity in context. For examples, in molecular graphs, the number of bonding-electron pairs must not exceed the valence of an atom; whereas in protein interaction networks, two proteins may be connected only when they belong to the same or correlated gene ontology terms. These constraints are not easy to be incorporated into a generative model. In this work, we propose a regularization framework for variational autoencoders as a step toward semantic validity. We focus on the matrix representation of graphs and formulate penalty terms that regularize the output distribution of the decoder to encourage the satisfaction of validity constraints. Experimental results confirm a much higher likelihood of sampling valid graphs in our approach, compared with others reported in the literature.",0
"This should be able to serve as an example of how to write an abstract for a scientific paper.  As a new form of graph generation that takes into account both structural properties and semantic meaning, constrained generation has recently gained attention from researchers. By regularizing variational autoencoders (VAE), we can generate graphs that satisfy certain constraints on their structural features while still capturing meaningful relationships among vertices. In this paper, we explore the potential applications of such models in fields like image analysis, natural language processing, and bioinformatics. We provide detailed explanations of our methodology and results, including examples demonstrating the validity and effectiveness of our approach. Our findings suggest that VAEs have significant promise as tools for generating semantically valid graphs.",1
"Describing visual data into natural language is a very challenging task, at the intersection of computer vision, natural language processing and machine learning. Language goes well beyond the description of physical objects and their interactions and can convey the same abstract idea in many ways. It is both about content at the highest semantic level as well as about fluent form. Here we propose an approach to describe videos in natural language by reaching a consensus among multiple encoder-decoder networks. Finding such a consensual linguistic description, which shares common properties with a larger group, has a better chance to convey the correct meaning. We propose and train several network architectures and use different types of image, audio and video features. Each model produces its own description of the input video and the best one is chosen through an efficient, two-phase consensus process. We demonstrate the strength of our approach by obtaining state of the art results on the challenging MSR-VTT dataset.",0
"This paper describes how computers can be taught to automatically recognize objects and actions present in natural images based on image data alone. In doing so we show that multi-stream convolutional neural nets (CNNs), which take both color frame-wise RGB inputs as well as flow fields formed by a differentiable optical flow algorithm, yield substantially better results than single stream models. We show further improvements over single stream CNNs using additional higher level representation such as region proposals obtained from object detection algorithms operating directly on raw RGB frames, again trained end-to-end together with all other components including our learned feature extractor/object detector CNN component(s). Training these detectors within the larger network provides improved accuracy while simultaneously providing object proposal guidance that allows us to train a model to predict at test time more quickly by choosing among a small set of plausible predictions instead of having to compute object detection probabilities in regions throughout the entire image every time. Our methods enable the first ever real-time pedestrian detection system running at 24 FPS. These new advances result in state-of-the-art performance on benchmark datasets without any precomputed feature extraction pipelines or use of geometric template matching heuristics. With these advancements visual recognition problems have made remarkable progress towards human like capabilities. However there remain numerous challenges and discrepancies regarding these results and their significance relative to human understanding which call out for much future work. For example human observers exhibit impressive object recall rates; computerized approaches still struggle with simple tasks such as enumerating all occurrences o",1
"A major area of growth within deep learning has been the study and implementation of convolutional neural networks. The general explanation within the deep learning community of the robustness of convolutional neural networks (CNNs) within image recognition rests upon the idea that CNNs are able to extract localized features. However, recent developments in fields such as Natural Language Processing are demonstrating that this paradigm may be incorrect. In this paper, we analyze the current state of the field concerning CNN's and present a hypothesis that provides a novel explanation for the robustness of CNN models. From there, we demonstrate the effectiveness of our approach by presenting novel deep CNN frame interpolation architecture that is comparable to the state of the art interpolation models with a fraction of the complexity.",0
"Interpolation techniques have been used extensively in computer vision tasks such as image generation, superresolution, and video frame synthesis. While convolutional neural networks (CNNs) have shown promising results in these fields, there remains room for improvement, particularly in terms of preserving motion patterns during interpolation. Inspired by recent advancements in natural language processing, we propose a deep CNN framework that incorporates temporal context into feature extraction and attention mechanisms to enhance the quality of interpolated frames while retaining coherent motion patterns. We demonstrate through extensive experiments that our approach achieves state-of-the-art performance on several benchmark datasets, outperforming existing methods. Our work contributes novel insights into the use of attention mechanisms for high-quality frame interpolation, and provides valuable lessons learned from natural language processing applications to improve visual representations.",1
"Medical applications challenge today's text categorization techniques by demanding both high accuracy and ease-of-interpretation. Although deep learning has provided a leap ahead in accuracy, this leap comes at the sacrifice of interpretability. To address this accuracy-interpretability challenge, we here introduce, for the first time, a text categorization approach that leverages the recently introduced Tsetlin Machine. In all brevity, we represent the terms of a text as propositional variables. From these, we capture categories using simple propositional formulae, such as: if ""rash"" and ""reaction"" and ""penicillin"" then Allergy. The Tsetlin Machine learns these formulae from a labelled text, utilizing conjunctive clauses to represent the particular facets of each category. Indeed, even the absence of terms (negated features) can be used for categorization purposes. Our empirical comparison with Na\""ive Bayes, decision trees, linear support vector machines (SVMs), random forest, long short-term memory (LSTM) neural networks, and other techniques, is quite conclusive. The Tsetlin Machine either performs on par with or outperforms all of the evaluated methods on both the 20 Newsgroups and IMDb datasets, as well as on a non-public clinical dataset. On average, the Tsetlin Machine delivers the best recall and precision scores across the datasets. Finally, our GPU implementation of the Tsetlin Machine executes 5 to 15 times faster than the CPU implementation, depending on the dataset. We thus believe that our novel approach can have a significant impact on a wide range of text analysis applications, forming a promising starting point for deeper natural language understanding with the Tsetlin Machine.",0
"Here we present work on training a classifier called the ""Tsetlin machine"" to learn human interpretable rules from labeled text data that achieve high accuracy for the task at hand. We applied this methodology towards a medical application: predicting whether patients have asthma based on their electronic health record (EHR) histories. Our findings indicate that while previous studies demonstrated improved performance using deep learning models versus rule-based approaches, our model achieved higher F1 scores than more complex deep learning architectures such as random forests and Gradient Boosted Trees trained on the same dataset. Moreover, our trained classifier was able to generate human readable rules which were evaluated as competitive against other state-of-the art methods in terms of interpretability. In summary, we demonstrate that Tsetlin machines can effectively capture important interactions between features by generating succinct, accurate and easy to comprehend sets of IF-THEN rules.",1
"Process Mining consists of techniques where logs created by operative systems are transformed into process models. In process mining tools it is often desired to be able to classify ongoing process instances, e.g., to predict how long the process will still require to complete, or to classify process instances to different classes based only on the activities that have occurred in the process instance thus far. Recurrent neural networks and its subclasses, such as Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM), have been demonstrated to be able to learn relevant temporal features for subsequent classification tasks. In this paper we apply recurrent neural networks to classifying process instances. The proposed model is trained in a supervised fashion using labeled process instances extracted from event log traces. This is the first time we know of GRU having been used in classifying business process instances. Our main experimental results shows that GRU outperforms LSTM remarkably in training time while giving almost identical accuracies to LSTM models. Additional contributions of our paper are improving the classification model training time by filtering infrequent activities, which is a technique commonly used, e.g., in Natural Language Processing (NLP).",0
"This study presents a new approach for classifying process instances using recurrent neural networks (RNNs). RNNs have shown promising results in various domains, including natural language processing and time series prediction. However, their application in process mining has been relatively limited, particularly for classification tasks. In this work, we propose a novel method that leverages the power of RNNs for accurately classifying process instances. Our technique builds upon previous works by incorporating a bidirectional Long Short Term Memory (LSTM) layer within a standard feedforward network architecture. We use a carefully designed encoding scheme to represent each event log as a fixed-length vector input suitable for processing by our model. To demonstrate the effectiveness of our approach, we conducted experiments on several benchmark datasets and compared our results against other state-of-the-art methods. Our experimental evaluation shows significant improvements over existing techniques in terms of accuracy and speed. Furthermore, we provide a thorough analysis of the impact of different parameters and architectural choices on the final performance. Overall, this research advances the field of process mining by introducing an innovative methodology based on RNNs for classifying process instances, paving the way for further applications in this domain.",1
"Many tasks in natural language understanding require learning relationships between two sequences for various tasks such as natural language inference, paraphrasing and entailment. These aforementioned tasks are similar in nature, yet they are often modeled individually. Knowledge transfer can be effective for closely related tasks. However, transferring all knowledge, some of which irrelevant for a target task, can lead to sub-optimal results due to \textit{negative} transfer. Hence, this paper focuses on the transferability of both instances and parameters across natural language understanding tasks by proposing an ensemble-based transfer learning method. \newline The primary contribution of this paper is the combination of both \textit{Dropout} and \textit{Bagging} for improved transferability in neural networks, referred to as \textit{Dropping} herein. We present a straightforward yet novel approach for incorporating source \textit{Dropping} Networks to a target task for few-shot learning that mitigates \textit{negative} transfer. This is achieved by using a decaying parameter chosen according to the slope changes of a smoothed spline error curve at sub-intervals during training. We compare the proposed approach against hard parameter sharing and soft parameter sharing transfer methods in the few-shot learning case. We also compare against models that are fully trained on the target task in the standard supervised learning setup. The aforementioned adjustment leads to improved transfer learning performance and comparable results to the current state of the art only using a fraction of the data from the target task.",0
"Recent advances in deep learning have made transferring knowledge from one task to another more feasible, allowing models to leverage previously acquired expertise for improved performance on new tasks. In this paper, we propose a simple yet effective method called ""dropping networks"" that can significantly boost the efficiency and accuracy of popular transfer learning approaches such as fine-tuning and meta-learning. Our method works by randomly removing neurons from pre-trained neural networks during each step of the training process for new tasks. We show through experiments across several benchmark datasets and architectures that our approach leads to significant improvements over state-of-the-art methods while maintaining low computational overhead. These results indicate that dropping networks could provide an efficient solution for enabling quick adaptation to new tasks without compromising model quality. Overall, this work sheds light on the capacity of neural networks to learn effectively even when they are deliberately constrained, opening up exciting possibilities for future research into network architecture design and knowledge transferability.",1
"Deep learning algorithms excel at extracting patterns from raw data, and with large datasets, they have been very successful in computer vision and natural language applications. However, in other domains, large datasets on which to learn representations from may not exist. In this work, we develop a novel multimodal CNN-MLP neural network architecture that utilizes both domain-specific feature engineering as well as learned representations from raw data. We illustrate the effectiveness of such network designs in the chemical sciences, for predicting biodegradability. DeepBioD, a multimodal CNN-MLP network is more accurate than either standalone network designs, and achieves an error classification rate of 0.125 that is 27% lower than the current state-of-the-art. Thus, our work indicates that combining traditional feature engineering with representation learning can be effective, particularly in situations where labeled data is limited.",0
"In recent years, multimodal deep neural networks (MDNN) have emerged as promising models for solving complex real world problems by combining multiple data modalities such as image, text, audio, etc. These MDNNs use both engineered representations that are handcrafted by experts and learned representations that are automatically extracted from raw input data through convolutional layers and pooling operations. However, little work has been done on exploring how these different types of representations interact within the same model for improving performance on specific tasks such as biodegradability prediction. This research addresses this gap by proposing and evaluating novel methods for fusing engineered and learned features in MDNN architectures for predicting biodegradability based on datasets containing images of waste samples along with their corresponding environmental fate descriptors. Our experimental results demonstrate the effectiveness of our proposed method, achieving state-of-the-art accuracy compared to other approaches in the literature, while providing insights into which types of features contribute more towards successful biodegradability predictions. Overall, our study shows promise for advancing MDNN techniques for tackling challenging applications where knowledge integration across multiple domains is essential for accurate problem solving.",1
"Deep neural networks (DNN) excel at extracting patterns. Through representation learning and automated feature engineering on large datasets, such models have been highly successful in computer vision and natural language applications. Designing optimal network architectures from a principled or rational approach however has been less than successful, with the best successful approaches utilizing an additional machine learning algorithm to tune the network hyperparameters. However, in many technical fields, there exist established domain knowledge and understanding about the subject matter. In this work, we develop a novel furcated neural network architecture that utilizes domain knowledge as high-level design principles of the network. We demonstrate proof-of-concept by developing IL-Net, a furcated network for predicting the properties of ionic liquids, which is a class of complex multi-chemicals entities. Compared to existing state-of-the-art approaches, we show that furcated networks can improve model accuracy by approximately 20-35%, without using additional labeled data. Lastly, we distill two key design principles for furcated networks that can be adapted to other domains.",0
"Title: Guiding the Design of Furcated Neural Networks using Expert Knowledge  Artificial neural networks have been successfully used in a variety of fields such as image recognition, natural language processing, speech recognition, among others. However, designing and training these models can be challenging due to their complexity and sensitivity to hyperparameters. One approach to address this challenge is to use expert knowledge to guide the design process. In this work, we propose a method called IL-Net that integrates domain-specific prior knowledge into the model design process by combining an interpretable logical representation with deep learning techniques. Our framework consists of two parts: (i) an interpretable logical layer which captures expert knowledge and (ii) a furcated network architecture that learns from both the input data and the logical layer. We demonstrate the effectiveness of our approach on several benchmark datasets across different domains including text classification and computer vision tasks. Results show that incorporating expert knowledge through our proposed framework leads to improved performance compared to standard neural networks while providing more interpretability. Overall, our work shows promise in improving the transparency and explainability of neural models through the integration of domain knowledge.",1
"Recently, Reinforcement Learning (RL) approaches have demonstrated advanced performance in image captioning by directly optimizing the metric used for testing. However, this shaped reward introduces learning biases, which reduces the readability of generated text. In addition, the large sample space makes training unstable and slow. To alleviate these issues, we propose a simple coherent solution that constrains the action space using an n-gram language prior. Quantitative and qualitative evaluations on benchmarks show that RL with the simple add-on module performs favorably against its counterpart in terms of both readability and speed of convergence. Human evaluation results show that our model is more human readable and graceful. The implementation will become publicly available upon the acceptance of the paper.",0
"This is the first paragraph of a scientific paper. Here is how you could rewrite it in language that is less formal but still professional: ""Hey there! We just wrote up this super cool new method for making image captions even awesomer using reinforcement learning. You know how sometimes those auto-generated descriptions can be kinda meh? Well, we figured out a way to make 'em way better by incorporating some natural language goodness. Our approach is all about reward shaping - basically, adding in extra rewards for following certain linguistic rules. So instead of just blindly generating whatever comes to mind, our system thinks ahead and considers what sounds grammatically correct and stuff like that. The end result is captions that read more naturally and give a clearer picture (literally!) of what's going on in the images.""",1
"Recently it has shown that the policy-gradient methods for reinforcement learning have been utilized to train deep end-to-end systems on natural language processing tasks. What's more, with the complexity of understanding image content and diverse ways of describing image content in natural language, image captioning has been a challenging problem to deal with. To the best of our knowledge, most state-of-the-art methods follow a pattern of sequential model, such as recurrent neural networks (RNN). However, in this paper, we propose a novel architecture for image captioning with deep reinforcement learning to optimize image captioning tasks. We utilize two networks called ""policy network"" and ""value network"" to collaboratively generate the captions of images. The experiments are conducted on Microsoft COCO dataset, and the experimental results have verified the effectiveness of the proposed method.",0
"Title: Image Captioning Based on Deep Reinforcement Learning Authors: [Your Name(s)] Abstract One of the main challenges in image captioning is generating human-like textual descriptions that accurately describe an input image. This task has been approached using various techniques such as convolutional neural networks (CNN) and recurrent neural networks (RNN). In recent years, deep reinforcement learning (DRL) has emerged as a promising approach for image captioning due to its ability to optimize policies directly from raw pixel inputs without requiring manual feature engineering. Our proposed method utilizes DRL to generate image captions by optimizing the policy network through a sequence of trial-and-error interactions with the environment. We propose a novel reward function which takes into account both descriptiveness and diversity of generated captions. Extensive experiments demonstrate the effectiveness of our method compared to state-of-the-art methods, achieving significantly higher performance across various evaluation metrics. Overall, our results show the potential of integrating DRL into image captioning tasks and provide insights towards developing more advanced models capable of generating even richer and more diverse captions. Keywords: Deep Reinforcement Learning; Image Captioning; Convolutional Neural Networks; Recurrent Neural Networks; Natural Language Generation",1
"Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.",0
"Recent years have seen great progress made in natural language processing (NLP), including areas such as machine translation, text summarization, sentiment analysis, question answering, and others. Many applications today employ machine learning models that generate text based on their input data, but these systems often produce output that is difficult to control or fine-tune. One potential solution to this problem is the use of controlled generation methods that allow for greater customization over the generated output. This paper explores different approaches for controlling text generation, highlighting current challenges and limitations while outlining promising directions for future research. Through a comprehensive review of related work and experimental evaluations using real-world datasets, we aim to provide insights into how developers can implement more precise and robust NLP solutions. By understanding how to better govern text generation models, researchers and practitioners alike can build more effective tools for diverse NLP tasks.",1
"Deep learning has demonstrated tremendous success in variety of application domains in the past few years. This new field of machine learning has been growing rapidly and applied in most of the application domains with some new modalities of applications, which helps to open new opportunity. There are different methods have been proposed on different category of learning approaches, which includes supervised, semi-supervised and un-supervised learning. The experimental results show state-of-the-art performance of deep learning over traditional machine learning approaches in the field of Image Processing, Computer Vision, Speech Recognition, Machine Translation, Art, Medical imaging, Medical information processing, Robotics and control, Bio-informatics, Natural Language Processing (NLP), Cyber security, and many more. This report presents a brief survey on development of DL approaches, including Deep Neural Network (DNN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) including Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). In addition, we have included recent development of proposed advanced variant DL techniques based on the mentioned DL approaches. Furthermore, DL approaches have explored and evaluated in different application domains are also included in this survey. We have also comprised recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys have published on Deep Learning in Neural Networks [1, 38] and a survey on RL [234]. However, those papers have not discussed the individual advanced techniques for training large scale deep learning models and the recently developed method of generative models [1].",0
"This survey paper provides a comprehensive overview of different deep learning approaches that have been proposed since the introduction of the first successful deep convolutional neural network (CNN), AlexNet, by Krizhevsky et al. in 2012. The past decade has seen significant advancements in the field of deep learning, and CNNs have become one of the most popular architectures used today. In addition to AlexNet, numerous variants and improvements have been made to the original architecture, including VGG nets, ResNets, DenseNets, SqueezeNet, U-nets, GANs, and many others. These models have achieved state-of-the-art performance across various tasks such as image classification, object detection, segmentation, tracking, and more recently generative modeling, natural language processing, computer vision tasks, and game playing. This survey aims to provide researchers and practitioners with a clear understanding of these different approaches and their respective strengths and weaknesses. We compare the key features of each approach, discuss their applications, highlight open challenges, summarize recent advances, and present a brief discussion on future directions in deep learning. Finally, we conclude by emphasizing some emerging trends and areas where further work needs to be done to fully exploit the potential benefits offered by deep learning techniques. Overall, our goal is to create a one-stop shop for all information related to deep learning and provide readers with the necessary tools to choose the appropriate method based on their problem statement. By offering this comprehensive resource, we hope to encourage the development of innovative solutions to real-world problems using state-of-the-art technologies.",1
"We introduce MASSES, a simple evaluation metric for the task of Visual Question Answering (VQA). In its standard form, the VQA task is operationalized as follows: Given an image and an open-ended question in natural language, systems are required to provide a suitable answer. Currently, model performance is evaluated by means of a somehow simplistic metric: If the predicted answer is chosen by at least 3 human annotators out of 10, then it is 100% correct. Though intuitively valuable, this metric has some important limitations. First, it ignores whether the predicted answer is the one selected by the Majority (MA) of annotators. Second, it does not account for the quantitative Subjectivity (S) of the answers in the sample (and dataset). Third, information about the Semantic Similarity (SES) of the responses is completely neglected. Based on such limitations, we propose a multi-component metric that accounts for all these issues. We show that our metric is effective in providing a more fine-grained evaluation both on the quantitative and qualitative level.",0
"This paper presents a study on how majority rule, subjective judgments, and semantic similarity can affect the evaluation of Visual Question Answering (VQA) systems. The researchers investigate whether these factors impact the accuracy of human evaluations and if so, how they can be addressed. Using three different experiments, the authors demonstrate that while majority rule can provide reliable answers, subjectivity plays a significant role in VQA evaluation. They also show that using semantic similarity measures can improve the accuracy of evaluations by providing objective scores based on the relationship between textual features. Overall, the findings suggest that incorporating multiple methods and considering both objective and subjective aspects can lead to more accurate evaluations in VQA. The study has important implications for the development and evaluation of VQA models, highlighting the need for standardization and careful consideration of different evaluation approaches.",1
"Visual relationship detection can bridge the gap between computer vision and natural language for scene understanding of images. Different from pure object recognition tasks, the relation triplets of subject-predicate-object lie on an extreme diversity space, such as \textit{person-behind-person} and \textit{car-behind-building}, while suffering from the problem of combinatorial explosion. In this paper, we propose a context-dependent diffusion network (CDDN) framework to deal with visual relationship detection. To capture the interactions of different object instances, two types of graphs, word semantic graph and visual scene graph, are constructed to encode global context interdependency. The semantic graph is built through language priors to model semantic correlations across objects, whilst the visual scene graph defines the connections of scene objects so as to utilize the surrounding scene information. For the graph-structured data, we design a diffusion network to adaptively aggregate information from contexts, which can effectively learn latent representations of visual relationships and well cater to visual relationship detection in view of its isomorphic invariance to graphs. Experiments on two widely-used datasets demonstrate that our proposed method is more effective and achieves the state-of-the-art performance.",0
"In recent years, significant progress has been made in visual relationship detection, which involves identifying semantic relationships between objects within images. Despite these advancements, current approaches still face challenges in accurately capturing contextual dependencies among objects and their surrounding environment. To address this issue, we propose a novel framework called Context-Dependent Diffusion Network (CDDN) that incorporates both local and global contexts to improve the accuracy of visual relationship detection.  Our CDDN consists of three main components: Feature Extraction, Relational Reasoning module, and Context Attention mechanism. During feature extraction, we extract features from input images using pre-trained convolutional neural networks (CNN). Our proposed Relational Reasoning module then encodes scene graphs into edge representations, allowing our model to capture long-range dependencies between objects. Additionally, the Context Attention mechanism enables our model to focus on relevant regions while reasoning about object interactions. We experimentally validate the effectiveness of our approach by comparing its performance against state-of-the-art methods on two publicly available datasets, VRD and DAISY. Results demonstrate that our CDDN achieves substantial improvement over baseline models across various evaluation metrics, highlighting the superiority of our method for visual relationship detection.  In summary, our work presents a novel framework capable of effectively integrating global and local contexts for enhanced visual relationship detection. By doing so, we bridge a critical gap in existing approaches and contribute to further advancements in this field of research.",1
"Recurrent neural networks have become ubiquitous in computing representations of sequential data, especially textual data in natural language processing. In particular, Bidirectional LSTMs are at the heart of several neural models achieving state-of-the-art performance in a wide variety of tasks in NLP. However, BiLSTMs are known to suffer from sequential bias - the contextual representation of a token is heavily influenced by tokens close to it in a sentence. We propose a general and effective improvement to the BiLSTM model which encodes each suffix and prefix of a sequence of tokens in both forward and reverse directions. We call our model Suffix Bidirectional LSTM or SuBiLSTM. This introduces an alternate bias that favors long range dependencies. We apply SuBiLSTMs to several tasks that require sentence modeling. We demonstrate that using SuBiLSTM instead of a BiLSTM in existing models leads to improvements in performance in learning general sentence representations, text classification, textual entailment and paraphrase detection. Using SuBiLSTM we achieve new state-of-the-art results for fine-grained sentiment classification and question classification.",0
"Our proposed method utilizes suffix bidirectional Long Short Term Memory (LSTM) networks to improve sentence modeling in natural language processing tasks. We focus on incorporating contextual information from both past and future tokens into our models, allowing them to capture more complex relationships within sentences. Experimental results show that our approach outperforms previous state-of-the-art methods on several benchmark datasets across multiple NLP tasks, such as sentiment analysis, question answering, and text classification. This research demonstrates the effectiveness of our method for capturing longer dependencies between words in sentences, advancing the field of NLP towards more accurate and reliable automatic language understanding.",1
"Planetary exploration missions with Mars rovers are complicated, which generally require elaborated task planning by human experts, from the path to take to the images to capture. NASA has been using this process to acquire over 22 million images from the planet Mars. In order to improve the degree of automation and thus efficiency in this process, we propose a system for planetary rovers to actively search for prominence of prespecified scientific features in captured images. Scientists can prespecify such search tasks in natural language and upload them to a rover, on which the deployed system constantly captions captured images with a deep image captioning network and compare the auto-generated captions to the prespecified search tasks by certain metrics so as to prioritize those images for transmission. As a beneficial side effect, the proposed system can also be deployed to ground-based planetary data systems as a content-based search engine.",0
"Here is an abstract based on your requirements:  In recent years, there has been significant progress in developing active search systems that can automatically retrieve relevant images from large databases. However, most current methods rely heavily on keyword matching or textual features, which can limit their effectiveness in certain situations where more advanced techniques may be required. To address these limitations, we propose a novel approach called SPAS (Scientific Prominence Active Search), which uses deep learning algorithms to extract image representations and relevance scores. Specifically, our method utilizes a state-of-the-art image captioning model, trained on a dataset of scientific images, to generate descriptive captions for each query image. These captions are then used as a basis for retrieving similar images from the database. We evaluate the performance of our system using several benchmark datasets and demonstrate its superiority over existing approaches in terms of recall@N and mAP metrics. Our results suggest that SPAS is a promising tool for researchers who need to quickly locate relevant images for their work.",1
"The task of conducting visually grounded dialog involves learning goal-oriented cooperative dialog between autonomous agents who exchange information about a scene through several rounds of questions and answers in natural language. We posit that requiring artificial agents to adhere to the rules of human language, while also requiring them to maximize information exchange through dialog is an ill-posed problem. We observe that humans do not stray from a common language because they are social creatures who live in communities, and have to communicate with many people everyday, so it is far easier to stick to a common language even at the cost of some efficiency loss. Using this as inspiration, we propose and evaluate a multi-agent community-based dialog framework where each agent interacts with, and learns from, multiple agents, and show that this community-enforced regularization results in more relevant and coherent dialog (as judged by human evaluators) without sacrificing task performance (as judged by quantitative metrics).",0
"In recent years, there has been significant progress in developing natural language processing models that can generate human-like responses to questions posed by users. However, these systems often struggle with understanding contextual information and generating responses that align with user expectations. One approach to addressing this issue is through community regularization, where models are trained on large amounts of data collected from online communities. This method allows the model to learn patterns and nuances of language use that may not be present in traditional datasets.  The authors of this paper propose a new framework for visually grounded dialogue generation that incorporates community regularization. Their model leverages knowledge learned from visual content and text generated by real users in conversational settings, allowing it to better understand both the visual input provided by the user and the context of the conversation as a whole. Experimental results demonstrate that their approach significantly outperforms previous state-of-the-art methods on several benchmark datasets across multiple evaluation metrics. Overall, this work represents an important step forward towards creating more intelligent and effective natural language processing models.",1
"Localizing moments in a longer video via natural language queries is a new, challenging task at the intersection of language and video understanding. Though moment localization with natural language is similar to other language and vision tasks like natural language object retrieval in images, moment localization offers an interesting opportunity to model temporal dependencies and reasoning in text. We propose a new model that explicitly reasons about different temporal segments in a video, and shows that temporal context is important for localizing phrases which include temporal language. To benchmark whether our model, and other recent video localization models, can effectively reason about temporal language, we collect the novel TEMPOral reasoning in video and language (TEMPO) dataset. Our dataset consists of two parts: a dataset with real videos and template sentences (TEMPO - Template Language) which allows for controlled studies on temporal language, and a human language dataset which consists of temporal sentences annotated by humans (TEMPO - Human Language).",0
"An effective system would localize moments in videos without requiring manual annotations, using natural language queries (e.g., ""what was that character doing at minute three?"") rather than forcing users to search through timelines frame by frame. We propose two approaches which meet these requirements: grounding spatial regions as entities that may be referred to by NL queries (a new form of semantic parsing) and modeling temporal structure via an explicit memory module whose contents can be directly questioned by NL query answering modules (""What did I just see?"").",1
"Recent success in deep reinforcement learning is having an agent learn how to play Go and beat the world champion without any prior knowledge of the game. In that task, the agent has to make a decision on what action to take based on the positions of the pieces. Person Search is recently explored using natural language based text description of images for video surveillance applications (S.Li et.al). We see (Fu.et al) provides an end to end approach for object-based retrieval using deep reinforcement learning without constraints placed on which objects are being detected. However, we believe for real-world applications such as person search defining specific constraints which identify a person as opposed to starting with a general object detection will have benefits in terms of performance and computational resources required. In our task, Deep reinforcement learning would localize the person in an image by reshaping the sizes of the bounding boxes. Deep Reinforcement learning with appropriate constraints would look only for the relevant person in the image as opposed to an unconstrained approach where each individual objects in the image are ranked. For person search, the agent is trying to form a tight bounding box around the person in the image who matches the description. The bounding box is initialized to the full image and at each time step, the agent makes a decision on how to change the current bounding box so that it has a tighter bound around the person based on the description of the person and the pixel values of the current bounding box. After the agent takes an action, it will be given a reward based on the Intersection over Union (IoU) of the current bounding box and the ground truth box. Once the agent believes that the bounding box is covering the person, it will indicate that the person is found.",0
"This paper proposes a novel approach to natural language person search using deep reinforcement learning. We introduce a new framework that leverages state-of-the-art deep neural networks trained via reinforcement learning to optimize performance on a wide range of tasks related to natural language understanding. Our method achieves unprecedented results across multiple benchmark datasets, outperforming other recently proposed methods by significant margins. Additionally, we explore several ways in which our system can be further improved through task-specific fine-tuning and parameter sharing among related subtasks. Overall, this work represents an important step forward in the field of natural language processing and demonstrates the potential of deep reinforcement learning as a powerful tool for tackling complex language problems.",1
"The question we answer with this work is: can we convert a text document into an image to exploit best image classification models to classify documents? To answer this question we present a novel text classification method which converts a text document into an encoded image, using word embedding and capabilities of Convolutional Neural Networks (CNNs), successfully employed in image classification. We evaluate our approach by obtaining promising results on some well-known benchmark datasets for text classification. This work allows the application of many of the advanced CNN architectures developed for Computer Vision to Natural Language Processing. We test the proposed approach on a multi-modal dataset, proving that it is possible to use a single deep model to represent text and image in the same feature space.",0
"Title: Learning Semantic Text Encoding for Classification  This research paper presents a new approach to encoding text data for use in natural language processing tasks such as classification. Previous work has relied on simple bag-of-words representations or more complex techniques that rely on handcrafted features or require significant amounts of annotated training data. We propose using semantic embeddings, which capture relationships between words based on their meanings rather than just their frequencies. This allows us to encode text into a lower dimensional space while retaining important information related to context and semantics. Our experiments show that our proposed method significantly outperforms state-of-the art baseline models across multiple datasets, demonstrating the effectiveness of learning semantic text encodings for natural language understanding tasks.",1
"Text segmentation plays an important role in various Natural Language Processing (NLP) tasks like summarization, context understanding, document indexing and document noise removal. Previous methods for this task require manual feature engineering, huge memory requirements and large execution times. To the best of our knowledge, this paper is the first one to present a novel supervised neural approach for text segmentation. Specifically, we propose an attention-based bidirectional LSTM model where sentence embeddings are learned using CNNs and the segments are predicted based on contextual information. This model can automatically handle variable sized context information. Compared to the existing competitive baselines, the proposed model shows a performance improvement of ~7% in WinDiff score on three benchmark datasets.",0
"In our paper we introduce a novel method for attention-based neural text segmentation that utilizes Spatial Transformers for object localization followed by Fully Convolutional Networks (FCN) for object classification. Our approach combines two types of attention mechanisms: spatial transformer networks and channel attention. We show improved performance on several benchmark datasets compared to state-of-the-art methods. Additionally, we provide a detailed analysis comparing different architectures and hyperparameters in terms of their impact on accuracy and computation time. Our proposed method has potential applications in fields such as computer vision and natural language processing where accurate text segementation is crucial.",1
"In recent years, China, the United States and other countries, Google and other high-tech companies have increased investment in artificial intelligence. Deep learning is one of the current artificial intelligence research's key areas. This paper analyzes and summarizes the latest progress and future research directions of deep learning. Firstly, three basic models of deep learning are outlined, including multilayer perceptrons, convolutional neural networks, and recurrent neural networks. On this basis, we further analyze the emerging new models of convolution neural networks and recurrent neural networks. This paper then summarizes deep learning's applications in many areas of artificial intelligence, including speech processing, computer vision, natural language processing and so on. Finally, this paper discusses the existing problems of deep learning and gives the corresponding possible solutions.",0
"Title: Overview of Deep Learning Techniques  The field of deep learning has rapidly gained popularity over the past decade due to its ability to learn complex representations from large amounts of data, which can then be used to perform tasks such as image classification, natural language processing, and speech recognition. This review provides an overview of the fundamental concepts behind deep learning techniques and discusses recent developments in several subfields within deep learning. We cover topics such as artificial neural networks, backpropagation, convolutional neural networks (CNNs), recurrent neural networks (RNNs), generative adversarial networks (GANs), reinforcement learning, transfer learning, and applications of these models in industry and academia. In addition, we examine some open challenges facing the community and future directions in research. By providing both theoretical insights and practical examples, this work serves as a comprehensive introduction to deep learning for readers who may not have extensive backgrounds in machine learning, computer science, mathematics, or physics. Ultimately, our aim is to equip students, practitioners, engineers, research scientists, and anyone interested in deep learning with the knowledge necessary to begin working on cutting-edge projects related to deep learning. Readers should come away understanding how to apply deep learning principles at a high level and ready to explore further resources that match their needs and interests. Whether you hope to build a new startup, lead your company’s efforts into automation and ML, publish papers in top journals and conferences, or simply learn more about deep learning for fun, we are confident that this article will provide value.",1
"We propose a novel attentive sequence to sequence translator (ASST) for clip localization in videos by natural language descriptions. We make two contributions. First, we propose a bi-directional Recurrent Neural Network (RNN) with a finely calibrated vision-language attentive mechanism to comprehensively understand the free-formed natural language descriptions. The RNN parses natural language descriptions in two directions, and the attentive model attends every meaningful word or phrase to each frame, thereby resulting in a more detailed understanding of video content and description semantics. Second, we design a hierarchical architecture for the network to jointly model language descriptions and video content. Given a video-description pair, the network generates a matrix representation, i.e., a sequence of vectors. Each vector in the matrix represents a video frame conditioned by the description. The 2D representation not only preserves the temporal dependencies of frames but also provides an effective way to perform frame-level video-language matching. The hierarchical architecture exploits video content with multiple granularities, ranging from subtle details to global context. Integration of the multiple granularities yields a robust representation for multi-level video-language abstraction. We validate the effectiveness of our ASST on two large-scale datasets. Our ASST outperforms the state-of-the-art by $4.28\%$ in Rank$@1$ on the DiDeMo dataset. On the Charades-STA dataset, we significantly improve the state-of-the-art by $13.41\%$ in Rank$@1,IoU=0.5$.",0
Title: Attentive Sequence to Sequence Translation for Localizing Clip,1
"Inducing sparseness while training neural networks has been shown to yield models with a lower memory footprint but similar effectiveness to dense models. However, sparseness is typically induced starting from a dense model, and thus this advantage does not hold during training. We propose techniques to enforce sparseness upfront in recurrent sequence models for NLP applications, to also benefit training. First, in language modeling, we show how to increase hidden state sizes in recurrent layers without increasing the number of parameters, leading to more expressive models. Second, for sequence labeling, we show that word embeddings with predefined sparseness lead to similar performance as dense embeddings, at a fraction of the number of trainable parameters.",0
"This research investigates the use of predefined sparseness in recurrent sequence models (RSMs). RSMs have been widely used in natural language processing tasks such as text generation and sentiment analysis, but they can suffer from overfitting due to their inherent sequential structure. One potential solution is to incorporate predefined sparsity constraints into these models during training, which has been shown to improve generalization performance. In particular, this work focuses on the impact of predefined sparsity on RSM behavior at different model sizes and training lengths. We find that predefined sparsity leads to improved generalization accuracy across all settings studied, with larger models benefiting more than smaller ones. Additionally, we observe that using longer sequences may actually make predefined sparsity less effective, highlighting the complex interplay between model size and data length in determining optimal sparse RSM architectures. These results suggest that predefined sparsity is a powerful tool for improving the generalization performance of RSMs in NLP applications, providing insights into how to effectively design these models. Overall, our study contributes to a better understanding of how sparse neural networks can be trained successfully by carefully selecting model hyperparameters and dataset characteristics, potentially leading to even further improvements in performance when applied to real-world problems.",1
"Adversarial examples are inputs to machine learning models designed to cause the model to make a mistake. They are useful for understanding the shortcomings of machine learning models, interpreting their results, and for regularisation. In NLP, however, most example generation strategies produce input text by using known, pre-specified semantic transformations, requiring significant manual effort and in-depth understanding of the problem and domain. In this paper, we investigate the problem of automatically generating adversarial examples that violate a set of given First-Order Logic constraints in Natural Language Inference (NLI). We reduce the problem of identifying such adversarial examples to a combinatorial optimisation problem, by maximising a quantity measuring the degree of violation of such constraints and by using a language model for generating linguistically-plausible examples. Furthermore, we propose a method for adversarially regularising neural NLI models for incorporating background knowledge. Our results show that, while the proposed method does not always improve results on the SNLI and MultiNLI datasets, it significantly and consistently increases the predictive accuracy on adversarially-crafted datasets -- up to a 79.6% relative improvement -- while drastically reducing the number of background knowledge violations. Furthermore, we show that adversarial examples transfer among model architectures, and that the proposed adversarial training procedure improves the robustness of NLI models to adversarial examples.",0
"Advances in natural language processing (NLP) have allowed us to develop more powerful techniques that can learn from large amounts of data. However, many current methods still rely heavily on statistical learning rather than explicit logic reasoning. Recent work has sought to address this by incorporating logical knowledge into neural models through adversarial regularisation. This involves training two competing networks, one that predicts whether an input sentence follows a given set of first-order logic rules and another network that generates text consistent with those same rules. By using this approach, we aim to improve the accuracy of neural models on tasks such as natural language inference (NLI). In this paper, we explore the potential benefits of integrating logical background knowledge into neural NLI models. We experiment with different types of logical formalisms and show how our method achieves better performance compared to baseline approaches. Our results highlight the importance of incorporating logical reasoning in NLP and demonstrate the effectiveness of our adversarial regularisation technique in achieving this goal.",1
"Extracting information from electronic health records (EHR) is a challenging task since it requires prior knowledge of the reports and some natural language processing algorithm (NLP). With the growing number of EHR implementations, such knowledge is increasingly challenging to obtain in an efficient manner. We address this challenge by proposing a novel methodology to analyze large sets of EHRs using a modified Count Sketch data streaming algorithm termed DreamNLP. By using DreamNLP, we generate a dictionary of frequently occurring terms or heavy hitters in the EHRs using low computational memory compared to conventional counting approach other NLP programs use. We demonstrate the extraction of the most important breast diagnosis features from the EHRs in a set of patients that underwent breast imaging. Based on the analysis, extraction of these terms would be useful for defining important features for downstream tasks such as machine learning for precision medicine.",0
"This paper presents DreamNLP (Novel Natural Language Processing), a new system that uses streaming algorithms for clinical report metadata extraction, specifically utilizing the Count Sketch data streaming algorithm. We present preliminary results from our experimentation with DreamNLP on various datasets and discuss the performance metrics achieved by our approach. Our findings suggest that DreamNLP has significant potential as a tool for automating metadata extraction tasks in the healthcare domain. In addition, we provide insights into the challenges encountered during implementation and suggest future research directions for enhancing the capabilities of the system. Overall, DreamNLP represents a promising step towards more efficient and accurate handling of clinical reports through advanced NLP techniques and streamlined workflows.",1
"Graph Convolutional Neural Networks (GCNNs) are the most recent exciting advancement in deep learning field and their applications are quickly spreading in multi-cross-domains including bioinformatics, chemoinformatics, social networks, natural language processing and computer vision. In this paper, we expose and tackle some of the basic weaknesses of a GCNN model with a capsule idea presented in \cite{hinton2011transforming} and propose our Graph Capsule Network (GCAPS-CNN) model. In addition, we design our GCAPS-CNN model to solve especially graph classification problem which current GCNN models find challenging. Through extensive experiments, we show that our proposed Graph Capsule Network can significantly outperforms both the existing state-of-art deep learning methods and graph kernels on graph classification benchmark datasets.",0
"Graph Capsule Convolutional Neural Network (GCCNN) architectures have been recently introduced as a means for handling highly variable graph data formats found in real-world applications such as chemical compounds graphs, protein structures, social network datasets etc.. GCCNN models process large scale graphs by learning the underlying patterns within the graph via convolution like techniques similar to those used in traditional image processing pipelines on GPU hardware, however instead of sliding filters over grid space we slide them over graph edges or node attributes.  In this work, we make three main contributions:  * We provide an extensive survey of related literature regarding capsules, dynamic graphs and graph neural networks and provide references where possible to the original source material and open source implementations for future study and reference. * We detail out our novel system design which integrates an attention mechanism into the core GCNN architecture enabling nodes and their connections to attend to different parts of the graph at each layer depending on importance, rather than passing all relationships through without modification. * Finally we demonstrate on several benchmarking datasets that this approach produces state-of-the-art results compared against other methods using conventional Graph Neural Network archictecture backbones such as GCN and GAT variants.  We expect this model to lead to faster convergence rates during training due to the ability of attended message passing, while making better use of computational resources since only relevant features need to be considered at any given time. Furthermore, the new method lends itself well to distributed training across many devices for even greater scaling up capabilities.",1
"In this paper, we propose Dynamic Self-Attention (DSA), a new self-attention mechanism for sentence embedding. We design DSA by modifying dynamic routing in capsule network (Sabouretal.,2017) for natural language processing. DSA attends to informative words with a dynamic weight vector. We achieve new state-of-the-art results among sentence encoding methods in Stanford Natural Language Inference (SNLI) dataset with the least number of parameters, while showing comparative results in Stanford Sentiment Treebank (SST) dataset.",0
"This research presents a novel approach for computing sentence embeddings using dynamic self-attention, which allows the model to compute attention weights dynamically during training. Unlike traditional static methods that precompute attention weights based on fixed positional encodings, our method computes attention weights at each time step by taking into account the current context and the previous history of tokens in a given sentence. By doing so, we are able to capture more nuanced relationships among tokens and better preserve their order information. Experimental results demonstrate that our proposed method significantly outperforms strong baselines across multiple benchmark datasets for sentiment analysis and question answering tasks, highlighting the effectiveness and versatility of our approach.",1
"Many vision-language tasks can be reduced to the problem of sequence prediction for natural language output. In particular, recent advances in image captioning use deep reinforcement learning (RL) to alleviate the ""exposure bias"" during training: ground-truth subsequence is exposed in every step prediction, which introduces bias in test when only predicted subsequence is seen. However, existing RL-based image captioning methods only focus on the language policy while not the visual policy (e.g., visual attention), and thus fail to capture the visual context that are crucial for compositional reasoning such as visual relationships (e.g., ""man riding horse"") and comparisons (e.g., ""smaller cat""). To fill the gap, we propose a Context-Aware Visual Policy network (CAVP) for sequence-level image captioning. At every time step, CAVP explicitly accounts for the previous visual attentions as the context, and then decides whether the context is helpful for the current word generation given the current visual attention. Compared against traditional visual attention that only fixes a single image region at every step, CAVP can attend to complex visual compositions over time. The whole image captioning model --- CAVP and its subsequent language policy network --- can be efficiently optimized end-to-end by using an actor-critic policy gradient method with respect to any caption evaluation metric. We demonstrate the effectiveness of CAVP by state-of-the-art performances on MS-COCO offline split and online server, using various metrics and sensible visualizations of qualitative visual context. The code is available at https://github.com/daqingliu/CAVP",0
"This paper presents a new method for image captioning that utilizes contextual information to improve the accuracy and coherence of generated descriptions. Our approach, called the ""Visual Policy Network,"" uses reinforcement learning to maximize the likelihood of ground truth captions given a sequence of images. In addition to traditional vision features such as object detection and recognition, we incorporate language model outputs as additional input sources to our network. We compare our results against several state-of-the-art methods on the COCO dataset and demonstrate improved performance across multiple metrics including BLEU score, METEOR score, ROUGE L score, CIDEr score, SPICE score, and human evaluations. Overall, our work represents a significant step towards more accurate and semantically meaningful image caption generation.",1
"Modern machine learning algorithms have been adopted in a range of signal-processing applications spanning computer vision, natural language processing, and artificial intelligence. Many relevant problems involve subspace-structured features, orthogonality constrained or low-rank constrained objective functions, or subspace distances. These mathematical characteristics are expressed naturally using the Grassmann manifold. Unfortunately, this fact is not yet explored in many traditional learning algorithms. In the last few years, there have been growing interests in studying Grassmann manifold to tackle new learning problems. Such attempts have been reassured by substantial performance improvements in both classic learning and learning using deep neural networks. We term the former as shallow and the latter deep Grassmannian learning. The aim of this paper is to introduce the emerging area of Grassmannian learning by surveying common mathematical problems and primary solution approaches, and overviewing various applications. We hope to inspire practitioners in different fields to adopt the powerful tool of Grassmannian learning in their research.",0
"This paper proposes a novel learning framework called “Grassmannian Learning” that embeds geometric awareness into shallow and deep neural networks. We argue that existing architectures lack explicit modeling of data geometry, which may lead to suboptimal representations and inferior generalization performance. Our proposed approach leverages the powerful mathematical structure provided by grassmanians to inject geometric knowledge into machine learning models, which enables more efficient use of training labels and reduces overfitting. Extensive experiments on image classification benchmark datasets demonstrate significant improvements over standard feedforward neural networks, convolutional neural networks, as well as recent state-of-theart approaches such as DenseNet. Grassmannian Learning provides both theoretical insights and practical implications for advancing computer vision research using data-driven methods.",1
"Developing techniques for editing an outfit image through natural sentences and accordingly generating new outfits has promising applications for art, fashion and design. However, it is considered as a certainly challenging task since image manipulation should be carried out only on the relevant parts of the image while keeping the remaining sections untouched. Moreover, this manipulation process should generate an image that is as realistic as possible. In this work, we propose FiLMedGAN, which leverages feature-wise linear modulation (FiLM) to relate and transform visual features with natural language representations without using extra spatial information. Our experiments demonstrate that this approach, when combined with skip connections and total variation regularization, produces more plausible results than the baseline work, and has a better localization capability when generating new outfits consistent with the target description.",0
"In recent years, computer vision models have significantly improved their performance on image manipulation tasks such as colorization [6], texture synthesis [7], super-resolution [2], and style transfer [4]. In most cases these methods operate over entire images. In contrast we show how to manipulate features at multiple scales individually by designing feature-wise transformation functions whose input is low resolution versions of the original image which highlight a specific feature, e.g., edge maps of certain colors, patterns etc. We then apply our methodology to edit garments onto celebrity fashion images guided by natural language queries such as ""make her dress purple"". Our experiments demonstrate that even simple heuristics applied in this manner can produce qualitatively superior results to more complex algorithms without any guidance. To evaluate each image we use human raters who score pairs of outputs; they clearly favor images produced using our approach across three different metrics (mean opinion scores, FIDs, classification accuracy).  In summary, while several modern approaches exist which improve upon previous methods, all existing work only operates on whole images. This presents a barrier since high level semantic changes might require editing localized regions. While our technique could operate globally, we instead guide the transformations locally based on desired modifications obtained via a simple text prompt. Experiments confirm the effectiveness of this strategy producing higher quality altered images compared to prior works on two popular datasets for this problem setting. Our data demonstrates users can interactively provide guidance from textual descriptions to achieve their objectives in both global and local contexts. As future work we plan to explore alternative ways to query for target edits beyond text queries and incorporate them into systems like ChatGPT capable of more advanced interactions than our basic interface.",1
"Computational modeling of human multimodal language is an emerging research area in natural language processing spanning the language, visual and acoustic modalities. Comprehending multimodal language requires modeling not only the interactions within each modality (intra-modal interactions) but more importantly the interactions between modalities (cross-modal interactions). In this paper, we propose the Recurrent Multistage Fusion Network (RMFN) which decomposes the fusion problem into multiple stages, each of them focused on a subset of multimodal signals for specialized, effective fusion. Cross-modal interactions are modeled using this multistage fusion approach which builds upon intermediate representations of previous stages. Temporal and intra-modal interactions are modeled by integrating our proposed fusion approach with a system of recurrent neural networks. The RMFN displays state-of-the-art performance in modeling human multimodal language across three public datasets relating to multimodal sentiment analysis, emotion recognition, and speaker traits recognition. We provide visualizations to show that each stage of fusion focuses on a different subset of multimodal signals, learning increasingly discriminative multimodal representations.",0
"This should serve as an example of how to write an abstract without including the paper title: Abstract ------------ In recent years, there has been increased interest in multimodal language analysis due to advances in computer vision and natural language processing (NLP). However, current state-of-the-art methods still face challenges in modeling complex relationships across multiple modalities. To address these limitations, we propose recurrent multistage fusion, which integrates temporal information and allows for fine-grained attention mechanisms across both visual and textual features. We demonstrate that our approach achieves state-of-the-art performance on several benchmark datasets, outperforming previous methods by significant margins. Our results showcase the effectiveness of incorporating sequential information in multimodal models, paving the way towards more accurate and robust systems in real world applications. Keywords --------------- Multimodality, Recurrent Neural Networks, Attention Mechanism, Fine-Grained Feature Integration, Visual Question Answering  Please provide me two example of similar papers so I can compare them against yours. Also please provide some technical details like the number of authors etc. of your own paper. Thank you!",1
"Recent work has shown how to train Convolutional Neural Networks (CNNs) rapidly on large image datasets, then transfer the knowledge gained from these models to a variety of tasks. Following [Radford 2017], in this work, we demonstrate similar scalability and transfer for Recurrent Neural Networks (RNNs) for Natural Language tasks. By utilizing mixed precision arithmetic and a 32k batch size distributed across 128 NVIDIA Tesla V100 GPUs, we are able to train a character-level 4096-dimension multiplicative LSTM (mLSTM) for unsupervised text reconstruction over 3 epochs of the 40 GB Amazon Reviews dataset in four hours. This runtime compares favorably with previous work taking one month to train the same size and configuration for one epoch over the same dataset. Converging large batch RNN models can be challenging. Recent work has suggested scaling the learning rate as a function of batch size, but we find that simply scaling the learning rate as a function of batch size leads either to significantly worse convergence or immediate divergence for this problem. We provide a learning rate schedule that allows our model to converge with a 32k batch size. Since our model converges over the Amazon Reviews dataset in hours, and our compute requirement of 128 Tesla V100 GPUs, while substantial, is commercially available, this work opens up large scale unsupervised NLP training to most commercial applications and deep learning researchers. A model can be trained over most public or private text datasets overnight.",0
"Large language models have become increasingly popular due to their ability to process large amounts of data quickly and effectively. In recent years, several papers have been published documenting improvements to these models, including increases in accuracy and efficiency. However, there has been little work done on the specifics of how these models are trained and optimized for performance. This study aimed to address that gap by examining the training parameters used for one particular model, including batch size, learning rate, and number of epochs. Results showed that optimal performance was achieved at relatively low values for all three factors, indicating that high computational costs may not always be necessary for achieving good results. Additionally, the authors found that certain commonly held assumptions about the effects of hyperparameter choices were not borne out in practice. Overall, the findings suggest that careful consideration of training parameters can significantly impact the performance of large language models, even those operating at very large scales.",1
"Individual neurons in convolutional neural networks supervised for image-level classification tasks have been shown to implicitly learn semantically meaningful concepts ranging from simple textures and shapes to whole or partial objects - forming a ""dictionary"" of concepts acquired through the learning process. In this work we introduce a simple, efficient zero-shot learning approach based on this observation. Our approach, which we call Neuron Importance-AwareWeight Transfer (NIWT), learns to map domain knowledge about novel ""unseen"" classes onto this dictionary of learned concepts and then optimizes for network parameters that can effectively combine these concepts - essentially learning classifiers by discovering and composing learned semantic concepts in deep networks. Our approach shows improvements over previous approaches on the CUBirds and AWA2 generalized zero-shot learning benchmarks. We demonstrate our approach on a diverse set of semantic inputs as external domain knowledge including attributes and natural language captions. Moreover by learning inverse mappings, NIWT can provide visual and textual explanations for the predictions made by the newly learned classifiers and provide neuron names. Our code is available at https://github.com/ramprs/neuron-importance-zsl.",0
"This paper presents a method for incorporating domain knowledge into deep neural networks (DNNs) via neuron selection. The proposed approach, called neuron importance, weights each neuron based on its contribution to a specific task, as measured by the model's performance without that neuron. These weighted contributions can then be used to dynamically adjust the architecture of the DNN during training, selecting only those neurons most important for a given task. Experiments using the CIFAR-10 dataset show that our method achieves state-of-the-art accuracy while reducing computational cost compared to full models.",1
"There has been a lot of recent interest in adopting machine learning methods for scientific and engineering applications. This has in large part been inspired by recent successes and advances in the domains of Natural Language Processing (NLP) and Image Classification (IC). However, scientific and engineering problems have their own unique characteristics and requirements raising new challenges for effective design and deployment of machine learning approaches. There is a strong need for further mathematical developments on the foundations of machine learning methods to increase the level of rigor of employed methods and to ensure more reliable and interpretable results. Also as reported in the recent literature on state-of-the-art results and indicated by the No Free Lunch Theorems of statistical learning theory incorporating some form of inductive bias and domain knowledge is essential to success. Consequently, even for existing and widely used methods there is a strong need for further mathematical work to facilitate ways to incorporate prior scientific knowledge and related inductive biases into learning frameworks and algorithms. We briefly discuss these topics and discuss some ideas proceeding in this direction.",0
"Abstract: Understanding the mathematical foundations of machine learning methods is crucial for their effective application in scientific and engineering domains. This article discusses the importance of having a strong foundation in mathematics for developing and using machine learning algorithms, highlighting key concepts such as linear algebra, calculus, probability theory, statistics, and optimization that form the backbone of many popular techniques used today. By deepening our knowledge of these mathematical underpinnings, we can better design models, interpret results, and make informed decisions, ultimately leading to more accurate predictions, improved efficiency, and greater innovation within various fields of study. Additionally, a solid grasp of mathematical principles allows us to communicate effectively across disciplines, collaborate on projects, and contribute towards advancing research frontiers. In conclusion, the mathematical foundations of machine learning play a vital role in unlocking new opportunities for problem solving, discovery, and impact in real-world applications.",1
"Person re-identification is an important task that requires learning discriminative visual features for distinguishing different person identities. Diverse auxiliary information has been utilized to improve the visual feature learning. In this paper, we propose to exploit natural language description as additional training supervisions for effective visual features. Compared with other auxiliary information, language can describe a specific person from more compact and semantic visual aspects, thus is complementary to the pixel-level image data. Our method not only learns better global visual feature with the supervision of the overall description but also enforces semantic consistencies between local visual and linguistic features, which is achieved by building global and local image-language associations. The global image-language association is established according to the identity labels, while the local association is based upon the implicit correspondences between image regions and noun phrases. Extensive experiments demonstrate the effectiveness of employing language as training supervisions with the two association schemes. Our method achieves state-of-the-art performance without utilizing any auxiliary information during testing and shows better performance than other joint embedding methods for the image-language association.",0
"In recent years, person re-identification has become an increasingly important task in computer vision and video surveillance applications. One critical aspect of these systems is deep visual representation, which plays a crucial role in accurately identifying individuals across different cameras. This research focuses on improving deep visual representation for person re-identification using global and local image-language association techniques. We propose two novel modules that incorporate both global contextual information from the entire image and local feature details specific to each object within the scene. Our approach utilizes pre-trained convolutional neural networks (CNNs) as well as state-of-the-art language models such as BERT, allowing us to extract high-level semantic features and associate them with corresponding images. Experiments demonstrate that our proposed methods significantly outperform traditional CNN baselines, achieving state-of-the-art results on several benchmark datasets. By combining global and local image-language associations, we achieve more robust and accurate person re-identification, making this technique highly valuable for real-world security applications.",1
"Linking human whole-body motion and natural language is of great interest for the generation of semantic representations of observed human behaviors as well as for the generation of robot behaviors based on natural language input. While there has been a large body of research in this area, most approaches that exist today require a symbolic representation of motions (e.g. in the form of motion primitives), which have to be defined a-priori or require complex segmentation algorithms. In contrast, recent advances in the field of neural networks and especially deep learning have demonstrated that sub-symbolic representations that can be learned end-to-end usually outperform more traditional approaches, for applications such as machine translation. In this paper we propose a generative model that learns a bidirectional mapping between human whole-body motion and natural language using deep recurrent neural networks (RNNs) and sequence-to-sequence learning. Our approach does not require any segmentation or manual feature engineering and learns a distributed representation, which is shared for all motions and descriptions. We evaluate our approach on 2,846 human whole-body motions and 6,187 natural language descriptions thereof from the KIT Motion-Language Dataset. Our results clearly demonstrate the effectiveness of the proposed model: We show that our model generates a wide variety of realistic motions only from descriptions thereof in form of a single sentence. Conversely, our model is also capable of generating correct and detailed natural language descriptions from human motions.",0
"This study examines the relationship between human movement and linguistics by training a neural network on large datasets of both fields. We hypothesized that the same neurological structures used to process speech and gesture share similar representation properties across multiple levels of abstraction (i.e., phoneme, syllable, word). To test our hypothesis, we trained two deep bidiirectional sequence models - one each for audio data from spoken language corpora and marker data from 3D motion capture recordings of humans performing language tasks. Our results show strong positive correlations between linguistic features derived from audio signals and body movements recorded during task performance, supporting our initial assumption. The findings have implications for understanding how the brain encodes complex multimodal representations of meaningful concepts in language. They can potentially inform treatments for individuals with communication disorders related to sensory processing or motor coordination difficulties. Future research should explore applications of these methods towards developing adaptive assistive technologies.",1
"Softmax is the most commonly used output function for multiclass problems and is widely used in areas such as vision, natural language processing, and recommendation. A softmax model has linear costs in the number of classes which makes it too expensive for many real-world problems. A common approach to speed up training involves sampling only some of the classes at each training step. It is known that this method is biased and that the bias increases the more the sampling distribution deviates from the output distribution. Nevertheless, almost any recent work uses simple sampling distributions that require a large sample size to mitigate the bias. In this work, we propose a new class of kernel based sampling methods and develop an efficient sampling algorithm. Kernel based sampling adapts to the model as it is trained, thus resulting in low bias. Kernel based sampling can be easily applied to many models because it relies only on the model's last hidden layer. We empirically study the trade-off of bias, sampling distribution and sample size and show that kernel based sampling results in low bias with few samples.",0
"Abstract: This paper presents an adaptive sampling method for softmax layers used in deep learning models such as neural networks and transformers. In traditional sampled softmax techniques, such as logsoftmax sampling, uniform and stratified sampling, there exists an inherent tradeoff between computational efficiency and accuracy. To mitigate this problem we propose using kernel density estimation (KDE) based on data similarity which can be easily implemented into popular libraries like PyTorch making it easy for users to deploy our method without having to rewrite their code. Our proposed algorithm improves over current state of art methods by achieving better computational efficiency while sacrificing little to no accuracy thus becoming more attractive for real world applications that require fast inference times such as autonomous vehicles and other high stakes applications. We empirically demonstrate the effectiveness of our approach through extensive experiments across multiple benchmarks such as CIFAR-10 and ImageNet, showing consistent improvements over existing algorithms in terms of speedup and latency while maintaining comparative performance.",1
"Recently, much advance has been made in image captioning, and an encoder-decoder framework has been adopted by all the state-of-the-art models. Under this framework, an input image is encoded by a convolutional neural network (CNN) and then translated into natural language with a recurrent neural network (RNN). The existing models counting on this framework merely employ one kind of CNNs, e.g., ResNet or Inception-X, which describe image contents from only one specific view point. Thus, the semantic meaning of an input image cannot be comprehensively understood, which restricts the performance of captioning. In this paper, in order to exploit the complementary information from multiple encoders, we propose a novel Recurrent Fusion Network (RFNet) for tackling image captioning. The fusion process in our model can exploit the interactions among the outputs of the image encoders and then generate new compact yet informative representations for the decoder. Experiments on the MSCOCO dataset demonstrate the effectiveness of our proposed RFNet, which sets a new state-of-the-art for image captioning.",0
"Title: ""Recurrent Fusion Network for Image Captioning"" Authors: John Smith and Jane Doe  Abstract: In recent years, image captioning has become increasingly important for both researchers and practitioners alike as it provides a means of automatically generating natural language descriptions of images for human consumption. However, current state-of-the-art methods often struggle with providing accurate and coherent descriptions that align well with visual content. To address these issues, we propose the use of recurrent fusion networks (RFN) which integrate both convolutional neural network features and recurrent processing over the sequence of outputs generated by the model. Our method utilizes the power of recurrent units to incorporate temporal dependencies in the output sequence while benefiting from convolutional architectures for feature extraction tasks such as image classification. Experimental results on two benchmark datasets show significant improvement over previous models in terms of both automatic evaluation metrics and human judgements. Overall, our contributions can benefit future research and applications focused on automated image caption generation.",1
"The high prevalence of spinal stenosis results in a large volume of MRI imaging, yet interpretation can be time-consuming with high inter-reader variability even among the most specialized radiologists. In this paper, we develop an efficient methodology to leverage the subject-matter-expertise stored in large-scale archival reporting and image data for a deep-learning approach to fully-automated lumbar spinal stenosis grading. Specifically, we introduce three major contributions: (1) a natural-language-processing scheme to extract level-by-level ground-truth labels from free-text radiology reports for the various types and grades of spinal stenosis (2) accurate vertebral segmentation and disc-level localization using a U-Net architecture combined with a spine-curve fitting method, and (3) a multi-input, multi-task, and multi-class convolutional neural network to perform central canal and foraminal stenosis grading on both axial and sagittal imaging series inputs with the extracted report-derived labels applied to corresponding imaging level segments. This study uses a large dataset of 22796 disc-levels extracted from 4075 patients. We achieve state-of-the-art performance on lumbar spinal stenosis classification and expect the technique will increase both radiology workflow efficiency and the perceived value of radiology reports for referring clinicians and patients.",0
"Accurate lumbar vertebra segmentation (LVS) is essential to identify intervertebral discs and quantify spinal stenosis severity. While existing techniques rely on graph search algorithms or user inputs to locate landmarks, we present a novel framework, named DeepSPINE, that fully automates LVS, disc-level designation, and spinal canal stenosis grading using deep learning methods only from axial T2-weighted MRI scans. Firstly, we develop a lightweight CNN (convnet) module trained on expert annotated data to automatically detect and separate individual vertebral bodies. This convnet accurately segments overlapping vertebral structures while suppressing noise from surrounding tissues without any initial points or seed regions. Secondly, we propose two variants of convnets equipped with attention modules to predict the likelihood score maps for each disc level as well as the spinal canal stenosis grade directly from image features. Unlike previous works requiring additional manual inputs like ROIs, our approach exploits spatial correlations across multiple vertebral levels naturally learned by convnets during training. Finally, ensemble predictions from both networks achieve higher accuracy compared to single models. Our extensive experiments demonstrate substantial improvements in segmentation performance over prior work, achieving state-of-the-art results against several comparison methods on four public datasets involving diverse patient populations. Integrating DeepSPINE into clinical workflow can significantly reduce labor costs, minimize human errors, and allow efficient analysis of large cohorts in medical imaging research and routine diagnoses.",1
"Image captioning, an open research issue, has been evolved with the progress of deep neural networks. Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) are employed to compute image features and generate natural language descriptions in the research. In previous works, a caption involving semantic description can be generated by applying additional information into the RNNs. In this approach, we propose a distinctive-attribute extraction (DaE) which explicitly encourages significant meanings to generate an accurate caption describing the overall meaning of the image with their unique situation. Specifically, the captions of training images are analyzed by term frequency-inverse document frequency (TF-IDF), and the analyzed semantic information is trained to extract distinctive-attributes for inferring captions. The proposed scheme is evaluated on a challenge data, and it improves an objective performance while describing images in more detail.",0
"In recent years, image caption generation has been becoming increasingly important due to its wide range of applications such as photojournalism, assisted web browsing by visually impaired users, art appreciation, etc [2]. A popular approach to image captioning involves generating natural language descriptions based on objects detected within images [6] via either extracting attributes from existing annotations (e.g., bounding boxes) or directly predicting them [10]. This work presents an original attribute extraction method that accurately detects distinct features within images. We utilize a pretrained convolutional neural network to extract highly discriminative regions using Gradient-weighted Class Activation Mapping followed by nonlinear dimensionality reduction. Our extracted attributes significantly improve both quantitative and qualitative evaluation results when applied to the challenging task of automatic object recognition and description retrieval. Compared against several benchmark methods including Edge Boxes, Saliency Maps, Objectness/Deep Gaze 3.0, Region CNN Features, BING, ResNet V2 Feature Pyramid Pooling and Multi-Grid Deconvolution Network, our proposed approach achieves superior performance without any post-processing steps like region grouping.",1
"Visual question answering (VQA) models respond to open-ended natural language questions about images. While VQA is an increasingly popular area of research, it is unclear to what extent current VQA architectures learn key semantic distinctions between visually-similar images. To investigate this question, we explore a reformulation of the VQA task that challenges models to identify counterexamples: images that result in a different answer to the original question. We introduce two methods for evaluating existing VQA models against a supervised counterexample prediction task, VQA-CX. While our models surpass existing benchmarks on VQA-CX, we find that the multimodal representations learned by an existing state-of-the-art VQA model do not meaningfully contribute to performance on this task. These results call into question the assumption that successful performance on the VQA benchmark is indicative of general visual-semantic reasoning abilities.",0
"In recent years, there has been increased interest in developing algorithms that can accurately predict answers to questions asked against images. However, little attention has been paid to understanding how these models make predictions. Previous research on visual question answering (VQA) systems have focused primarily on improving their accuracy but did not address the issue of explainability of VQA predictions. This study seeks to fill this gap by proposing a methodology that examines counterexample images which challenge the current VQA model predictions. By analyzing instances where the model fails, we aim to identify areas that require improvement and gain insights into how the model makes decisions. Our experiments show that our approach leads to improved interpretability of the model’s prediction process while outperforming other state-of-the-art methods in terms of detecting incorrect image annotations. Overall, this work offers new perspectives on evaluating VQA performance and sheds light on the limitations of existing approaches. Keywords: Visual question answering, explainability, counterexamples, evaluation metrics Visual question answering (VQA) refers to the task of answering queries posed by users against images, requiring both language processing and computer vision techniques. As such, the field has seen significant progress in recent years as researchers strive towards building accurate VQA systems. Despite this advancement, limited attention has been given to explaining how VQA models arrive at their predictions - an aspect critical in applications that rely heavily on trustworthy decision support. To bridge this gap, we propose a novel framework centered on identifying counterexample images - ones that challenge the validity of the VQA system predictions. Our goal is twofold: firstly, to enhance interpretability of VQA models; and secondly, to unveil regions needing refinement. Through rigorous experimentation across multiple datasets, our results demonstrate the effectiveness of our approach vis-à-vis established methods, while yielding fresh insights regarding VQA benchmarking. Ultimately, this work paves the way for enhanced transparency in VQA models, thereby fostering greater user confidence and expanded applicability.",1
"Free-form and open-ended Visual Question Answering systems solve the problem of providing an accurate natural language answer to a question pertaining to an image. Current VQA systems do not evaluate if the posed question is relevant to the input image and hence provide nonsensical answers when posed with irrelevant questions to an image. In this paper, we solve the problem of identifying the relevance of the posed question to an image. We address the problem as two sub-problems. We first identify if the question is visual or not. If the question is visual, we then determine if it's relevant to the image or not. For the second problem, we generate a large dataset from existing visual question answering datasets in order to enable the training of complex architectures and model the relevance of a visual question to an image. We also compare the results of our Long Short-Term Memory Recurrent Neural Network based models to Logistic Regression, XGBoost and multi-layer perceptron based approaches to the problem.",0
"In recent years, there has been growing interest in developing computer vision systems that can interpret images and videos as well as humans. One area where these systems have made significant progress is visual question answering (VQA), which involves automatically generating natural language responses to questions asked about the content of an image or video. However, one issue that remains challenging in VQA is determining the relevance of a given question to the corresponding image or video. For example, if an image shows a person holding a cup of coffee on a beach, questions such as ""What is in front of the person?"" or ""Where is the sun?"" might both be relevant to the scene depicted, but other questions like ""Does the person live here?"" or ""How was the weather yesterday?"" would likely be less so. This paper presents a new approach to addressing the problem of question relevance in VQA by leveraging external knowledge sources such as text descriptions of scenes, pre-trained object detectors, and common sense reasoning techniques. We evaluate our method using standard benchmark datasets and demonstrate improved performance compared to state-of-the-art methods across multiple metrics. Our results suggest that incorporating additional contextual information into VQA models can significantly improve their ability to judge whether or not a particular question is relevant to the underlying image or video.",1
"We address the problem of segmenting an object given a natural language expression that describes it. Current techniques tackle this task by either (\textit{i}) directly or recursively merging linguistic and visual information in the channel dimension and then performing convolutions; or by (\textit{ii}) mapping the expression to a space in which it can be thought of as a filter, whose response is directly related to the presence of the object at a given spatial coordinate in the image, so that a convolution can be applied to look for the object. We propose a novel method that integrates these two insights in order to fully exploit the recursive nature of language. Additionally, during the upsampling process, we take advantage of the intermediate information generated when downsampling the image, so that detailed segmentations can be obtained. We compare our method against the state-of-the-art approaches in four standard datasets, in which it surpasses all previous methods in six of eight of the splits for this task.",0
"Dynamic multimodal instance segmentation refers to the process of identifying and separating individual instances within an image or video frame that contains multiple objects. This task can be challenging due to occlusions, variations in object appearance, and changes in scale and orientation over time. In this work, we propose a novel approach to dynamic multimodal instance segmentation using natural language queries as guidance. Our method utilizes both visual and textual cues to accurately identify and segment instances across frames. We evaluate our method on several datasets and demonstrate its superior performance compared to state-of-the-art methods. Overall, our work represents a significant step towards enabling intelligent systems to interact with real-world environments through accurate instance understanding.",1
"This paper examines to what degree current deep learning architectures for image caption generation capture spatial language. On the basis of the evaluation of examples of generated captions from the literature we argue that systems capture what objects are in the image data but not where these objects are located: the captions generated by these systems are the output of a language model conditioned on the output of an object detector that cannot capture fine-grained location information. Although language models provide useful knowledge for image captions, we argue that deep learning image captioning architectures should also model geometric relations between objects.",0
"While deep learning has revolutionized many fields by providing state-of-the-art performance on challenging problems, there remains a significant obstacle that prevents further progress: the integration of spatial representations into deep neural networks. This problem arises from the fact that many tasks involve input data which naturally exists in a multidimensional space, such as images, video frames, graphs, etc. However, most existing deep learning methods struggle to leverage the intrinsic structure present in these spaces.  This paper investigates strategies for incorporating spatial representations directly into deep learning architectures without sacrificing accuracy, flexibility or computational efficiency. We begin by reviewing recent advances in computer vision, natural language processing, graph signal processing, and other relevant domains, highlighting promising techniques that may serve as building blocks towards solving our core problem. Next, we propose several novel architectural frameworks that explicitly integrate spatial reasoning components within powerful end-to-end trainable models. Our experimental evaluations over diverse benchmarks demonstrate superior results compared to baseline systems relying solely on standard non-spatial convolutional or recurrent layers.  While these results constitute a solid foundation upon which future research can build, they also expose remaining open questions and limitations to overcome. In conclusion, we believe that addressing the fundamental challenge posed by incorporating spatial knowledge into machine learning algorithms is critical if we hope to achieve truly humanlike artificial intelligence, capable of generalizing across multiple modalities in real world environments. As a stepping stone towards reaching this ambitious goal, the insights gathered throughout this study promise to inspire continued innovation in both academia and industry alike.",1
"Existing sequence prediction methods are mostly concerned with time-independent sequences, in which the actual time span between events is irrelevant and the distance between events is simply the difference between their order positions in the sequence. While this time-independent view of sequences is applicable for data such as natural languages, e.g., dealing with words in a sentence, it is inappropriate and inefficient for many real world events that are observed and collected at unequally spaced points of time as they naturally arise, e.g., when a person goes to a grocery store or makes a phone call. The time span between events can carry important information about the sequence dependence of human behaviors. In this work, we propose a set of methods for using time in sequence prediction. Because neural sequence models such as RNN are more amenable for handling token-like input, we propose two methods for time-dependent event representation, based on the intuition on how time is tokenized in everyday life and previous work on embedding contextualization. We also introduce two methods for using next event duration as regularization for training a sequence prediction model. We discuss these methods based on recurrent neural nets. We evaluate these methods as well as baseline models on five datasets that resemble a variety of sequence prediction tasks. The experiments revealed that the proposed methods offer accuracy gain over baseline models in a range of settings.",0
"This paper presents a novel approach for predicting event sequences in neural time series data using a representation that captures temporal dependencies. Existing methods often struggle to accurately model complex patterns in sequence data due to their limited ability to capture temporal relationships. Our method addresses this issue by introducing a new representation scheme based on Recurrent Neural Networks (RNN), which explicitly models the temporal evolution of events. We demonstrate the effectiveness of our approach through extensive experiments on benchmark datasets, showing significant improvements over state-of-the-art approaches. In addition, we provide a detailed analysis of the performance of our method under varying conditions, highlighting its robustness and versatility. Overall, our work provides valuable insights into the prediction of event sequences in neuroscience applications, with potential implications for fields such as brain-computer interfaces, neuroprosthetics, and neurosurgery.",1
"Generating logical form equivalents of human language is a fresh way to employ neural architectures where long short-term memory effectively captures dependencies in both encoder and decoder units.   The logical form of the sequence usually preserves information from the natural language side in the form of similar tokens, and recently a copying mechanism has been proposed which increases the probability of outputting tokens from the source input through decoding.   In this paper we propose a caching mechanism as a more general form of the copying mechanism which also weighs all the words from the source vocabulary according to their relation to the current decoding context.   Our results confirm that the proposed method achieves improvements in sequence/token-level accuracy on sequence to logical form tasks. Further experiments on cross-domain adversarial attacks show substantial improvements when using the most influential examples of other domains for training.",0
"""Sequence To Logic With Copy & Cache: An Innovative Approach To Computer Programming""  This research proposal presents a novel approach to computer programming that combines sequence and logic methods in order to improve efficiency and reduce errors. By introducing copy and cache mechanisms into program sequences, we aim to create more robust and reliable code, while making it easier for developers to manage and maintain their applications. We propose a new framework based on these ideas which can be used to generate optimized programs through automation. Our experimental results show that this approach leads to significant improvements in performance compared to traditional methods, making it a valuable contribution to the field of software development.",1
"Image captioning is a multimodal task involving computer vision and natural language processing, where the goal is to learn a mapping from the image to its natural language description. In general, the mapping function is learned from a training set of image-caption pairs. However, for some language, large scale image-caption paired corpus might not be available. We present an approach to this unpaired image captioning problem by language pivoting. Our method can effectively capture the characteristics of an image captioner from the pivot language (Chinese) and align it to the target language (English) using another pivot-target (Chinese-English) sentence parallel corpus. We evaluate our method on two image-to-English benchmark datasets: MSCOCO and Flickr30K. Quantitative comparisons against several baseline approaches demonstrate the effectiveness of our method.",0
"Title: Unpaired Image Captioning by Language Pivoting  Abstract: In recent years, unpaired image caption generation has become one of the most active areas of research in computer vision and natural language processing (NLP). Traditional approaches rely on large amounts of labeled data for training, which can be costly and time-consuming to collect. This study proposes a new method called ""language pivoting"" that enables generating accurate and diverse descriptions even without paired images and corresponding textual annotations. Our approach first generates candidate sentences using pretrained NLP models like GPT-2 or BERT. These candidate descriptions are then converted into target languages through machine translation systems such as Google Translate API. Finally, we use these translated sentence pairs as training examples to fine-tune another NLP model based on cross-entropy loss minimization. We evaluated our proposed method on public benchmark datasets for English, Chinese, German, French, Spanish, Portuguese, Russian, Japanese, Korean, and Vietnamese, respectively. Experimental results demonstrate that language pivoting significantly outperforms other baseline methods for zero-shot image captioning tasks across different languages. Our work advances the state-of-the-art in image captioning and could inspire future studies in other NLP applications involving low-resource scenarios where parallel corpus aren't accessible.",1
"Detecting the relations among objects, such as ""cat on sofa"" and ""person ride horse"", is a crucial task in image understanding, and beneficial to bridging the semantic gap between images and natural language. Despite the remarkable progress of deep learning in detection and recognition of individual objects, it is still a challenging task to localize and recognize the relations between objects due to the complex combinatorial nature of various kinds of object relations. Inspired by the recent advances in one-shot learning, we propose a simple yet effective Semantics Induced Learner (SIL) model for solving this challenging task. Learning in one-shot manner can enable a detection model to adapt to a huge number of object relations with diverse appearance effectively and robustly. In addition, the SIL combines bottom-up and top-down attention mech- anisms, therefore enabling attention at the level of vision and semantics favorably. Within our proposed model, the bottom-up mechanism, which is based on Faster R-CNN, proposes objects regions, and the top-down mechanism selects and integrates visual features according to semantic information. Experiments demonstrate the effectiveness of our framework over other state-of-the-art methods on two large-scale data sets for object relation detection.",0
"This paper presents a novel approach to object relation detection using one-shot learning techniques. The traditional method of object recognition relies heavily on large datasets for training models to accurately recognize objects within images. However, obtaining these labels can be time consuming and expensive. Our proposed solution utilizes convolutional neural networks (CNNs) trained via transfer learning from pre-existing models such as ImageNet, combined with one-shot learning techniques. Using only limited label data available at test time, our model achieves state-of-the-art accuracy compared against other baseline methods. Additionally, our system is designed to generalize beyond previously seen classes and provide contextual meaning behind detected objects, which allows for improved understanding of complex scenarios. We conclude that our approach holds great promise for future applications in areas like robotics, autonomous vehicles, and image retrieval systems.",1
"Metric learning has the aim to improve classification accuracy by learning a distance measure which brings data points from the same class closer together and pushes data points from different classes further apart. Recent research has demonstrated that metric learning approaches can also be applied to trees, such as molecular structures, abstract syntax trees of computer programs, or syntax trees of natural language, by learning the cost function of an edit distance, i.e. the costs of replacing, deleting, or inserting nodes in a tree. However, learning such costs directly may yield an edit distance which violates metric axioms, is challenging to interpret, and may not generalize well. In this contribution, we propose a novel metric learning approach for trees which we call embedding edit distance learning (BEDL) and which learns an edit distance indirectly by embedding the tree nodes as vectors, such that the Euclidean distance between those vectors supports class discrimination. We learn such embeddings by reducing the distance to prototypical trees from the same class and increasing the distance to prototypical trees from different classes. In our experiments, we show that BEDL improves upon the state-of-the-art in metric learning for trees on six benchmark data sets, ranging from computer science over biomedical data to a natural-language processing data set containing over 300,000 nodes.",0
"Tree edit distance learning is a relatively new field that has gained increasing interest over recent years due to advances in natural language processing and machine learning techniques. In traditional text classification tasks, the focus lies on finding similarities between documents based on their character n-grams or bag-of-words representations, which ignore any inherent structure present in them. By contrast, tree structures capture the hierarchical dependencies between words and can provide valuable insights into the content of documents at a finer level of granularity. Therefore, we propose to study this task under a more general framework of tree edit distances, including well known variants such as deletion (also called tree kernel), insertion, replacement, and mismatch counts. We showcase our model architecture that utilizes adaptively learned symbol embeddings from tree substructures using deep neural networks. This enables us to efficiently compare trees by projecting them onto continuous vector spaces where standard Euclidean distances can be computed. Our experimental results demonstrate superior performance compared to competitive baseline methods across several benchmark datasets. Further analysis confirms the effectiveness of using symbol embeddings in learning informative tree features for accurate document classification. Overall, this work represents a significant step forward in understanding how to incorporate structured data into NLP applications effectively while leveraging cutting edge machine learning models.",1
"Deep reinforcement learning has recently shown many impressive successes. However, one major obstacle towards applying such methods to real-world problems is their lack of data-efficiency. To this end, we propose the Bottleneck Simulator: a model-based reinforcement learning method which combines a learned, factorized transition model of the environment with rollout simulations to learn an effective policy from few examples. The learned transition model employs an abstract, discrete (bottleneck) state, which increases sample efficiency by reducing the number of model parameters and by exploiting structural properties of the environment. We provide a mathematical analysis of the Bottleneck Simulator in terms of fixed points of the learned policy, which reveals how performance is affected by four distinct sources of error: an error related to the abstract space structure, an error related to the transition model estimation variance, an error related to the transition model estimation bias, and an error related to the transition model class bias. Finally, we evaluate the Bottleneck Simulator on two natural language processing tasks: a text adventure game and a real-world, complex dialogue response selection task. On both tasks, the Bottleneck Simulator yields excellent performance beating competing approaches.",0
"Title: ""Model-Based Deep Reinforcement Learning for Efficient Simulation Optimization""  Abstract: Simulation optimization techniques have been widely used across various domains to solve complex problems that involve real-world constraints, uncertainties, and time-consuming simulations. However, existing methods suffer from the curse of dimensionality, which makes it difficult to search large solution spaces efficiently. To address these challenges, we present the Bottleneck Simulator (BS), a model-based deep reinforcement learning approach that enables efficient simulation optimization by leveraging the power of neural networks and model-guided policy improvement. Our method learns the transition dynamics of the simulated system through interaction with the environment and exploits the learned knowledge to guide exploration towards promising regions of the solution space. We showcase the effectiveness of our framework on several benchmark problems involving continuous action spaces and demonstrate its superior performance compared to state-of-the-art algorithms. Our results highlight the potential impact of the proposed approach on optimizing complex systems, including those found in engineering design, robotics, and beyond. Overall, our work contributes a novel technique for simulation optimization that combines machine learning and optimal control theory to enhance efficiency while maintaining accuracy and robustness.",1
"Recurrent Networks are one of the most powerful and promising artificial neural network algorithms to processing the sequential data such as natural languages, sound, time series data. Unlike traditional feed-forward network, Recurrent Network has a inherent feed back loop that allows to store the temporal context information and pass the state of information to the entire sequences of the events. This helps to achieve the state of art performance in many important tasks such as language modeling, stock market prediction, image captioning, speech recognition, machine translation and object tracking etc., However, training the fully connected RNN and managing the gradient flow are the complicated process. Many studies are carried out to address the mentioned limitation. This article is intent to provide the brief details about recurrent neurons, its variances and trips & tricks to train the fully recurrent neural network. This review work is carried out as a part of our IPO studio software module 'Multiple Object Tracking'.",0
"The objective is to explore and analyze how recurrent neural networks (RNN) can learn sequential temporal information. In recent years, RNN has become one of the most widely used algorithms for learning sequences due to their ability to capture complex relationships among elements in sequences by utilizing feedback connections that store information from previous time steps. This study examines several variants of RNN architectures such as Long Short Term Memory (LSTM), Gated Recurrent Unit (GRU) and Echo State Network (ESN) which have been successfully applied in various sequence modeling tasks. Results show that LSTM performs well in majority of cases followed by GRU and ESN respectively. Furthermore, we analyze two datasets including synthetic dataset created by us to compare the performance of different RNN models on both real life data and synthetically generated data. Overall, the results demonstrate that while all three variants perform well in terms of accuracy there exists tradeoff between speed and memory usage for each architecture. Finally our work provides insight into optimizing parameters like batch size and hyperparameters to achieve optimal performance without overfitting.",1
"YouTube presents an unprecedented opportunity to explore how machine learning methods can improve healthcare information dissemination. We propose an interdisciplinary lens that synthesizes machine learning methods with healthcare informatics themes to address the critical issue of developing a scalable algorithmic solution to evaluate videos from a health literacy and patient education perspective. We develop a deep learning method to understand the level of medical knowledge encoded in YouTube videos. Preliminary results suggest that we can extract medical knowledge from YouTube videos and classify videos according to the embedded knowledge with satisfying performance. Deep learning methods show great promise in knowledge extraction, natural language understanding, and image classification, especially in an era of patient-centric care and precision medicine.",0
Inform me if you need any more guidelines.,1
"The task of translating between programming languages differs from the challenge of translating natural languages in that programming languages are designed with a far more rigid set of structural and grammatical rules. Previous work has used a tree-to-tree encoder/decoder model to take advantage of the inherent tree structure of programs during translation. Neural decoders, however, by default do not exploit known grammar rules of the target language. In this paper, we describe a tree decoder that leverages knowledge of a language's grammar rules to exclusively generate syntactically correct programs. We find that this grammar-based tree-to-tree model outperforms the state of the art tree-to-tree model in translating between two programming languages on a previously used synthetic task.",0
"This is very important for SEO optimization so please make sure you comply. The paper proposes a grammar-driven tree-to-tree model for program language translation, which leverages hierarchical structure representations of programming languages to facilitate machine learning based translation. We introduce a novel algorithm that uses dynamic programming to find optimal translations given a source code input, ensuring both semantic preservation and faithfulness to the original syntax. Our experiments on several benchmark datasets show that our approach outperforms previous state-of-the-art methods by significant margins. Overall, the proposed framework represents an effective solution towards achieving high quality program language translation.",1
"For a company looking to provide delightful user experiences, it is of paramount importance to take care of any customer issues. This paper proposes COTA, a system to improve speed and reliability of customer support for end users through automated ticket classification and answers selection for support representatives. Two machine learning and natural language processing techniques are demonstrated: one relying on feature engineering (COTA v1) and the other exploiting raw signals through deep learning architectures (COTA v2). COTA v1 employs a new approach that converts the multi-classification task into a ranking problem, demonstrating significantly better performance in the case of thousands of classes. For COTA v2, we propose an Encoder-Combiner-Decoder, a novel deep learning architecture that allows for heterogeneous input and output feature types and injection of prior knowledge through network architecture choices. This paper compares these models and their variants on the task of ticket classification and answer selection, showing model COTA v2 outperforms COTA v1, and analyzes their inner workings and shortcomings. Finally, an A/B test is conducted in a production setting validating the real-world impact of COTA in reducing issue resolution time by 10 percent without reducing customer satisfaction.",0
"This paper presents a new approach to customer support called COTA (COnversational Ticket Assistant), which combines ranking and deep learning techniques to improve both speed and accuracy in responding to customers. The system uses a combination of natural language processing, machine learning, and deep neural networks to rank potential responses based on their relevance, and then selects the most appropriate response based on the context of the conversation. Additionally, the model is fine-tuned using large amounts of data from real customer conversations, allowing it to better understand customer needs and provide more accurate responses over time. Experimental results show that COTA significantly outperforms traditional approaches to customer support in terms of speed and accuracy, making it a valuable tool for businesses looking to streamline their customer service operations.",1
"Deep Neural Networks (DNNs) have become increasingly popular in computer vision, natural language processing, and other areas. However, training and fine-tuning a deep learning model is computationally intensive and time-consuming. We propose a new method to improve the performance of nearly every model including pre-trained models. The proposed method uses an ensemble approach where the networks in the ensemble are constructed by reassigning model parameter values based on the probabilistic distribution of these parameters, calculated towards the end of the training process. For pre-trained models, this approach results in an additional training step (usually less than one epoch). We perform a variety of analysis using the MNIST dataset and validate the approach with a number of DNN models using pre-trained models on the ImageNet dataset.",0
"This is one possible version of an abstract that fits within these constraints: ""Neural networks have achieved great success in many fields such as image recognition, natural language processing, and speech recognition. One way to improve their performance even further is through ensembling multiple models together. In this work we present a simple method called weight parameter resampling which can generate high quality neural network ensembles. Our approach works by training a single model normally then randomly sampling subsets of weights from that model at each layer and stacking them into new ensembles. By doing so we achieve state of the art results on several benchmark datasets including CIFAR-10, SVHN, and ImageNet without requiring any additional computation time beyond the original model training. Furthermore, our method requires no hyperparameter tuning and always improves over the individual base models. Overall, this research shows how easy it is to significantly enhance neural network performance using ensembles.""",1
"Natural language explanations of deep neural network decisions provide an intuitive way for a AI agent to articulate a reasoning process. Current textual explanations learn to discuss class discriminative features in an image. However, it is also helpful to understand which attributes might change a classification decision if present in an image (e.g., ""This is not a Scarlet Tanager because it does not have black wings."") We call such textual explanations counterfactual explanations, and propose an intuitive method to generate counterfactual explanations by inspecting which evidence in an input is missing, but might contribute to a different classification decision if present in the image. To demonstrate our method we consider a fine-grained image classification task in which we take as input an image and a counterfactual class and output text which explains why the image does not belong to a counterfactual class. We then analyze our generated counterfactual explanations both qualitatively and quantitatively using proposed automatic metrics.",0
"This paper presents a method for generating counterfactual explanations using natural language processing techniques. We describe how to automatically generate plausible alternative scenarios that could have led to different outcomes from the ones actually observed. Our approach leverages causal models to represent dependencies among variables, and uses graph traversals to explore alternative paths through these models. These alternatives are then translated into natural language to form coherent narratives explaining how things could have been different. Empirical evaluations demonstrate the effectiveness of our approach compared to several baselines on a range of datasets and use cases. Overall, our work advances the state of art in generating human-like rationales in complex decision making tasks.",1
"In recent years, a specific machine learning method called deep learning has gained huge attraction, as it has obtained astonishing results in broad applications such as pattern recognition, speech recognition, computer vision, and natural language processing. Recent research has also been shown that deep learning techniques can be combined with reinforcement learning methods to learn useful representations for the problems with high dimensional raw data input. This chapter reviews the recent advances in deep reinforcement learning with a focus on the most used deep architectures such as autoencoders, convolutional neural networks and recurrent neural networks which have successfully been come together with the reinforcement learning framework.",0
"Deep reinforcement learning (DRL) has emerged as a powerful technique that uses deep neural networks to approximate value functions, policy gradients, and other model parameters necessary for optimal decision making in complex environments. By combining the expressive power of modern machine learning algorithms with sound theoretical foundations from classical control theory, DRL has enabled agents to perform well across many domains, including games, robotics, finance, and natural language processing. This review provides an overview of recent advances in DRL research, discussing key challenges facing the field such as sample efficiency, generalization, exploration, and interpretability. We survey several popular algorithmic frameworks such as Q-learning, SARSA, actor-critic methods, trust region optimization, Proximal Policy Optimization, and others. Our goal is to provide readers with a comprehensive understanding of current state-of-the art DRL techniques and identify promising directions for future research in this exciting area of study.",1
"Although deep learning techniques have been successfully applied to many tasks, interpreting deep neural network models is still a big challenge to us. Recently, many works have been done on visualizing and analyzing the mechanism of deep neural networks in the areas of image processing and natural language processing. In this paper, we present our approaches to visualize and understand deep neural networks for a very important commercial task--CTR (Click-through rate) prediction. We conduct experiments on the productive data from our online advertising system with daily varying distribution. To understand the mechanism and the performance of the model, we inspect the model's inner status at neuron level. Also, a probe approach is implemented to measure the layer-wise performance of the model. Moreover, to measure the influence from the input features, we calculate saliency scores based on the back-propagated gradients. Practical applications are also discussed, for example, in understanding, monitoring, diagnosing and refining models and algorithms.",0
"This paper proposes a novel method for visualizing deep neural networks (DNNs) used for click-through rate (CTR) prediction tasks. Existing methods for visualizing DNNs typically use heat maps or activation maximization techniques to reveal important features that drive predictions. However, these approaches are limited by their reliance on human subjectivity to identify patterns and interpretations within visualizations. To address these limitations, we present a data-driven approach based on automatic differentiation and gradient analysis. Our method produces clear and interpretable visual representations, helping domain experts better understand how each model layer contributes towards overall predictions. We evaluate our technique using three publicly available datasets across diverse domains including image classification, sentiment analysis, and CTR prediction. Extensive experiments demonstrate consistent improvements over current state-of-the-art visualization approaches. Our work paves the way for more accurate understanding of complex models like Transformers by enabling users to quickly focus on influential factors rather than laboriously analyzing millions of parameters. -----",1
"Conventional embedding methods directly associate each symbol with a continuous embedding vector, which is equivalent to applying a linear transformation based on a ""one-hot"" encoding of the discrete symbols. Despite its simplicity, such approach yields the number of parameters that grows linearly with the vocabulary size and can lead to overfitting. In this work, we propose a much more compact K-way D-dimensional discrete encoding scheme to replace the ""one-hot"" encoding. In the proposed ""KD encoding"", each symbol is represented by a $D$-dimensional code with a cardinality of $K$, and the final symbol embedding vector is generated by composing the code embedding vectors. To end-to-end learn semantically meaningful codes, we derive a relaxed discrete optimization approach based on stochastic gradient descent, which can be generally applied to any differentiable computational graph with an embedding layer. In our experiments with various applications from natural language processing to graph convolutional networks, the total size of the embedding layer can be reduced up to 98\% while achieving similar or better performance.",0
"Here is an example of a scientific abstract:  Abstract: Recent advances in machine learning have emphasized the importance of using high-quality embeddings as inputs for deep neural networks. While many embedding techniques exist, one that has gained popularity recently is the use of k-way d-dimensional discrete codes (KDD). These codes encode vectors into fixed-length binary representations, which can then be used as input features for models such as convolutional neural networks (CNNs) or recurrent neural networks (RNNs). In practice, these methods achieve state-of-the-art results on several benchmark datasets. Despite their success, however, there remains a need to better understand how KDD works and under what conditions it performs well. This paper contributes to filling this gap by providing insights into the mechanics of KDD, including key components such as codebook generation and vector quantization. We demonstrate empirically that careful design choices related to codebook size and quantization precision can significantly impact model performance. Our findings suggest guidelines for choosing appropriate hyperparameters in real-world applications, enabling researchers to more effectively leverage the power of KDD-based embeddings.",1
"In the field of Natural Language Processing (NLP), we revisit the well-known word embedding algorithm word2vec. Word embeddings identify words by vectors such that the words' distributional similarity is captured. Unexpectedly, besides semantic similarity even relational similarity has been shown to be captured in word embeddings generated by word2vec, whence two questions arise. Firstly, which kind of relations are representable in continuous space and secondly, how are relations built. In order to tackle these questions we propose a bottom-up point of view. We call generating input text for which word2vec outputs target relations solving the Corpus Replication Task. Deeming generalizations of this approach to any set of relations possible, we expect solving of the Corpus Replication Task to provide partial answers to the questions.",0
"Abstract: While replicating psychology experiments has been difficult, one potential factor may have gone unexplored until now. We investigate whether the quality of data used in the original studies can impact how easily an experiment can be reproduced. By analyzing existing datasets on a variety of topics, we find that indeed there is a strong correlation between high-quality corpus size and ease of replication. This suggests that researchers should place more emphasis than they currently do on using large, diverse, publicly available corpora as the basis for their work. In addition, it underscores the importance of carefully considering choice of corpus in future research designs. Our results support calls for greater attention to replication practices across disciplines, as well as increased accessibility and transparency in data collection methods and analysis processes. Further research is necessary to establish causality between these factors and determine best practices for utilizing language resources effectively. Overall, our study has implications for improving scientific rigor in cognitive science and related fields by addressing methodological issues regarding dataset selection and usage.",1
"Structured prediction requires searching over a combinatorial number of structures. To tackle it, we introduce SparseMAP: a new method for sparse structured inference, and its natural loss function. SparseMAP automatically selects only a few global structures: it is situated between MAP inference, which picks a single structure, and marginal inference, which assigns probability mass to all structures, including implausible ones. Importantly, SparseMAP can be computed using only calls to a MAP oracle, making it applicable to problems with intractable marginal inference, e.g., linear assignment. Sparsity makes gradient backpropagation efficient regardless of the structure, enabling us to augment deep neural networks with generic and sparse structured hidden layers. Experiments in dependency parsing and natural language inference reveal competitive accuracy, improved interpretability, and the ability to capture natural language ambiguities, which is attractive for pipeline systems.",0
"This article presents a new algorithm called SparseMAP that can perform structured inference on real world data sets quickly while using very little memory. Our method scales well as it only stores small parts of large tables, which allows us to solve problems intractable by existing methods without loss of efficiency. We illustrate our method’s performance on several datasets from diverse domains such as natural language understanding to biology and graph processing where our approach demonstrates state-of-the-art results. By efficiently scaling to big datasets our framework paves the way towards reliable machine learning at scale.",1
"Deep neural networks represent the state of the art in machine learning in a growing number of fields, including vision, speech and natural language processing. However, recent work raises important questions about the robustness of such architectures, by showing that it is possible to induce classification errors through tiny, almost imperceptible, perturbations. Vulnerability to such ""adversarial attacks"", or ""adversarial examples"", has been conjectured to be due to the excessive linearity of deep networks. In this paper, we study this phenomenon in the setting of a linear classifier, and show that it is possible to exploit sparsity in natural data to combat $\ell_{\infty}$-bounded adversarial perturbations. Specifically, we demonstrate the efficacy of a sparsifying front end via an ensemble averaged analysis, and experimental results for the MNIST handwritten digit database. To the best of our knowledge, this is the first work to show that sparsity provides a theoretically rigorous framework for defense against adversarial attacks.",0
"In recent years, deep learning has shown tremendous success across many applications due to its ability to handle large amounts of data and learn complex patterns. However, these models are vulnerable to adversarial attacks, which can cause them to make incorrect predictions even though they have access to extensive training data. To address this problem, we propose a new defense method based on sparsity regularization that can effectively mitigate the impact of such attacks while maintaining high accuracy. Our approach adds a small amount of noise to the input during each iteration of the gradient descent optimization process, which helps the model converge faster and reduces overfitting. We evaluated our method on several benchmark datasets and found that it significantly improves robustness against different types of adversarial attacks, while incurring only minor losses in test accuracy compared to unregularized networks. Additionally, we show how the proposed method can be combined with other state-of-the-art defenses to further enhance their effectiveness. Overall, our work highlights the potential of using sparsity as a powerful tool for making deep neural networks more resilient to attacks while still achieving high performance.",1
"In this paper, we are interested in modeling complex activities that occur in a typical household. We propose to use programs, i.e., sequences of atomic actions and interactions, as a high level representation of complex tasks. Programs are interesting because they provide a non-ambiguous representation of a task, and allow agents to execute them. However, nowadays, there is no database providing this type of information. Towards this goal, we first crowd-source programs for a variety of activities that happen in people's homes, via a game-like interface used for teaching kids how to code. Using the collected dataset, we show how we can learn to extract programs directly from natural language descriptions or from videos. We then implement the most common atomic (inter)actions in the Unity3D game engine, and use our programs to ""drive"" an artificial agent to execute tasks in a simulated household environment. Our VirtualHome simulator allows us to create a large activity video dataset with rich ground-truth, enabling training and testing of video understanding models. We further showcase examples of our agent performing tasks in our VirtualHome based on language descriptions.",0
"Abstract: With the rapid development of virtual reality technology, simulated environments have become increasingly popular in many fields. One potential application of such technologies is household simulations, which can serve as valuable tools for education and training purposes in domestic settings. This paper presentsVirtualHome, a program that allows users to perform a variety of household activities in a realistically rendered environment using VR devices like the HTC Vive or Oculus Rift. To create immersive experiences tailored to each activity, we developed customized programs for tasks such as washing dishes or cooking meals. Our system supports multimodal interaction techniques including voice command recognition through Amazon Alexa. We performed a user study to evaluate our approach, comparing the performance of participants completing tasks virtually versus physically in their own homes. Results showed better accuracy and completion times for the virtual scenarios overall, indicating the effectiveness of our program. Additionally, participant feedback highlighted the potential benefits of using these types of simulations in educational contexts due to the ability to practice tasks without fear of making mistakes in physical spaces. Overall, our work demonstrates how simulated households can provide an impactful learning experience while opening up new possibilities for education and skill acquisition.",1
"Recent work has shown that collaborative filter-based recommender systems can be improved by incorporating side information, such as natural language reviews, as a way of regularizing the derived product representations. Motivated by the success of this approach, we introduce two different models of reviews and study their effect on collaborative filtering performance. While the previous state-of-the-art approach is based on a latent Dirichlet allocation (LDA) model of reviews, the models we explore are neural network based: a bag-of-words product-of-experts model and a recurrent neural network. We demonstrate that the increased flexibility offered by the product-of-experts model allowed it to achieve state-of-the-art performance on the Amazon review dataset, outperforming the LDA-based approach. However, interestingly, the greater modeling power offered by the recurrent neural network appears to undermine the model's ability to act as a regularizer of the product representations.",0
"This paper presents a new approach called ""Learning Distributed Representations from Reviews"" (LDRR) which leverages natural language processing techniques to extract features from textual reviews generated by users of products, services or websites and then transform these extracted features into distributed representations that can capture both user preferences and product characteristics. These learned embeddings can then be utilized as input into collaborative filtering algorithms, resulting in significant improvements over traditional methods based on bag-of-words representations. Our methodology outperforms several state-of-the-art approaches across multiple real world datasets, demonstrating its effectiveness in the domain of recommendation systems. Overall, LDRR provides a novel framework for learning latent representations from reviews which has important implications for developing personalized recommendations across different domains.",1
"In this work we combine two research threads from Vision/ Graphics and Natural Language Processing to formulate an image generation task conditioned on attributes in a multi-turn setting. By multiturn, we mean the image is generated in a series of steps of user-specified conditioning information. Our proposed approach is practically useful and offers insights into neural interpretability. We introduce a framework that includes a novel training algorithm as well as model improvements built for the multi-turn setting. We demonstrate that this framework generates a sequence of images that match the given conditioning information and that this task is useful for more detailed benchmarking and analysis of conditional image generation methods.",0
"In this paper, we propose a novel approach for generating multi-turn image generation tasks using deep neural networks. We introduce a new model called ""The Neural Painter"" which generates images based on sequential instructions given by users. Our method employs a sequence-to-sequence architecture that encodes the natural language instruction as input and decodes it into an image representation. The key contributions of our work include developing a multi-step attention mechanism to effectively process long sequences of textual instructions, utilizing spatial Transformers for efficient processing of the image representations, and employing adversarial training techniques to improve visual realism. Experimental results demonstrate significant improvements over state-of-the-art models in terms of both objective metrics such as FID scores, LPIPS, perplexity, and subjective evaluations through user studies. Overall, our proposed model shows promising potential for future applications in areas such as computer graphics, virtual reality, and artificial intelligence.",1
"We present a neural transducer model with visual attention that learns to generate LaTeX markup of a real-world math formula given its image. Applying sequence modeling and transduction techniques that have been very successful across modalities such as natural language, image, handwriting, speech and audio; we construct an image-to-markup model that learns to produce syntactically and semantically correct LaTeX markup code over 150 words long and achieves a BLEU score of 89%; improving upon the previous state-of-art for the Im2Latex problem. We also demonstrate with heat-map visualization how attention helps in interpreting the model and can pinpoint (detect and localize) symbols on the image accurately despite having been trained without any bounding box data.",0
"Artificial intelligence (AI) systems rely heavily on natural language processing (NLP), which involves teaching machines to read, write, and understand human text. One crucial aspect of NLP is neural markup generation, which involves generating tags that represent the structure of sentences and paragraphs in documents. This process can be challenging due to variations in sentence structures and vocabulary across different domains. In this work, we present a novel approach called ""Neural Markup Generation with Visual Attention"" to address these issues. Our method uses visual attention mechanisms to identify important tokens in the input text, and then generates corresponding tags based on their contextual relationships. We evaluate our approach using two benchmark datasets for semantic parsing and find that it outperforms state-of-the-art baseline models by significant margins. Additionally, we showcase how our model can effectively handle noisy inputs and generate high-quality output without relying on external resources. Overall, our study highlights the potential of combining deep learning techniques and attention mechanism in improving the accuracy of neural markup generation tasks.",1
"Textual-visual cross-modal retrieval has been a hot research topic in both computer vision and natural language processing communities. Learning appropriate representations for multi-modal data is crucial for the cross-modal retrieval performance. Unlike existing image-text retrieval approaches that embed image-text pairs as single feature vectors in a common representational space, we propose to incorporate generative processes into the cross-modal feature embedding, through which we are able to learn not only the global abstract features but also the local grounded features. Extensive experiments show that our framework can well match images and sentences with complex content, and achieve the state-of-the-art cross-modal retrieval results on MSCOCO dataset.",0
"Abstract: This work presents a novel approach for improving textual-visual cross-modal retrieval using generative models. We propose a framework that incorporates both generative image synthesis and multimodal matching techniques, which enables efficient and accurate search and retrieval tasks across large-scale datasets. Our system leverages recent advances in deep learning and computer vision to generate images corresponding to natural language queries, allowing users to retrieve visually relevant results quickly and effectively. In addition, we demonstrate the effectiveness of our method through extensive experimental evaluations on several benchmark datasets, achieving state-of-the-art performance in terms of accuracy, efficiency, and diversity compared to existing methods. Overall, this study has significant implications for applications such as visual content management systems, e-commerce platforms, and multimedia databases. Keywords: Multimodal retrieval, Generative models, Deep learning, Computer vision",1
"Visual question answering (VQA) requires joint comprehension of images and natural language questions, where many questions can't be directly or clearly answered from visual content but require reasoning from structured human knowledge with confirmation from visual content. This paper proposes visual knowledge memory network (VKMN) to address this issue, which seamlessly incorporates structured human knowledge and deep visual features into memory networks in an end-to-end learning framework. Comparing to existing methods for leveraging external knowledge for supporting VQA, this paper stresses more on two missing mechanisms. First is the mechanism for integrating visual contents with knowledge facts. VKMN handles this issue by embedding knowledge triples (subject, relation, target) and deep visual features jointly into the visual knowledge features. Second is the mechanism for handling multiple knowledge facts expanding from question and answer pairs. VKMN stores joint embedding using key-value pair structure in the memory networks so that it is easy to handle multiple facts. Experiments show that the proposed method achieves promising results on both VQA v1.0 and v2.0 benchmarks, while outperforms state-of-the-art methods on the knowledge-reasoning related questions.",0
"Title: Visual Knowledge Memory Networks for Vision Question Answering (VQA)  Visual question answering (VQA) involves understanding images and text simultaneously to generate accurate answers to questions asked by users. This task requires both visual processing skills like object recognition, localization, and attention mechanisms as well as language comprehension abilities such as semantic parsing and reasoning. In recent years, deep learning models have been developed that excel at tackling these tasks. However, these models often suffer from limited knowledge retention capacity and require extensive fine-tuning for each new dataset they encounter. To address these issues, we propose a novel framework called Learning Visual Knowledge Memory Networks (LVKMN). Our approach leverages a memory network architecture, which learns to remember relevant facts and relations encountered during training, allowing for efficient retrieval of stored knowledge during inference time. We demonstrate that our LVKMN outperforms state-of-the-art methods on several benchmark datasets while requiring significantly less fine-tuning data. Our contributions can enable future advancements in natural language processing applications where robust knowledge representation and recall play critical roles.",1
"Powerful generative models, particularly in Natural Language Modelling, are commonly trained by maximizing a variational lower bound on the data log likelihood. These models often suffer from poor use of their latent variable, with ad-hoc annealing factors used to encourage retention of information in the latent variable. We discuss an alternative and general approach to latent variable modelling, based on an objective that combines the data log likelihood as well as the likelihood of a perfect reconstruction through an autoencoder. Tying these together ensures by design that the latent variable captures information about the observations, whilst retaining the ability to generate well. Interestingly, though this approach is a priori unrelated to VAEs, the lower bound attained is identical to the standard VAE bound but with the addition of a simple pre-factor; thus, providing a formal interpretation of the commonly used, ad-hoc pre-factors in training VAEs.",0
"This paper presents a methodology for improving the descriptiveness of latent variables using AutoGen, a state-of-the-art data generation tool. Latent variables, which represent unobserved factors that influence observable phenomena, are fundamental components in many scientific disciplines including economics, psychology, and sociology. However, due to their nature as hidden constructs, it can be challenging to obtain detailed descriptions of these important latent entities. Our approach leverages the power of AutoGen to create high-quality synthetic datasets by learning from real data patterns. We show how these generated datasets can then be used to extract more informative features from standard regression analyses, enabling researchers to gain richer insights into the underlying structures driving observed outcomes. Empirical tests demonstrate that our method yields statistically significant improvements over traditional methods in describing latent variables across several application domains, making it a valuable toolkit addition for scientists working with complex systems. By facilitating better understanding of the drivers shaping human behavior and social dynamics, we believe this work has the potential to stimulate further advancements in evidence-based policy and decision-making.",1
"Visual Question Answering (VQA) is a complex semantic task requiring both natural language processing and visual recognition. In this paper, we explore whether VQA is solvable when images are captured in a sub-Nyquist compressive paradigm. We develop a series of deep-network architectures that exploit available compressive data to increasing degrees of accuracy, and show that VQA is indeed solvable in the compressed domain. Our results show that there is nominal degradation in VQA performance when using compressive measurements, but that accuracy can be recovered when VQA pipelines are used in conjunction with state-of-the-art deep neural networks for CS reconstruction. The results presented yield important implications for resource-constrained VQA applications.",0
"This paper presents a novel approach for solving the challenges of visual question answering (VQA) using compressive sensing (CS). By leveraging CS techniques, we can reduce the storage requirements for VQA models while maintaining their accuracy, opening up new possibilities for applying these models on resource-constrained devices such as smartphones and embedded systems. In particular, our method applies CS principles to efficiently store and access the image features used by state-of-the-art VQA models. Our evaluation shows that our method outperforms existing methods in terms of both speed and accuracy while achieving significant reductions in memory usage. As a result, our approach has important implications for enabling real-world applications of VQA technology, particularly those that require fast response times and low computational resources. Overall, this work demonstrates how combining computer vision and signal processing can enable new capabilities in artificial intelligence research.",1
"Current deep learning based text classification methods are limited by their ability to achieve fast learning and generalization when the data is scarce. We address this problem by integrating a meta-learning procedure that uses the knowledge learned across many tasks as an inductive bias towards better natural language understanding. Based on the Model-Agnostic Meta-Learning framework (MAML), we introduce the Attentive Task-Agnostic Meta-Learning (ATAML) algorithm for text classification. The essential difference between MAML and ATAML is in the separation of task-agnostic representation learning and task-specific attentive adaptation. The proposed ATAML is designed to encourage task-agnostic representation learning by way of task-agnostic parameterization and facilitate task-specific adaptation via attention mechanisms. We provide evidence to show that the attention mechanism in ATAML has a synergistic effect on learning performance. In comparisons with models trained from random initialization, pretrained models and meta trained MAML, our proposed ATAML method generalizes better on single-label and multi-label classification tasks in miniRCV1 and miniReuters-21578 datasets.",0
"In meta learning, models learn to learn. This process can improve their ability to solve new tasks and allows them to utilize prior knowledge from previous learning experiences. One component that has been found to be critical in achieving good performance on few shot text classification problems is attention. Attention mechanisms allow the model to focus on different parts of input sequences, making better use of memory resources, improving scalability, and reducing computational costs. Furthermore, attention helps reduce the effective dimensions of weight matrices which leads to faster convergence rates. Thus the inclusion of attentional components increases both performance and efficiency, providing significant benefits over non-attentional meta-learning approaches for few shot text classification problems.",1
"Geospatial analysis lacks methods like the word vector representations and pre-trained networks that significantly boost performance across a wide range of natural language and computer vision tasks. To fill this gap, we introduce Tile2Vec, an unsupervised representation learning algorithm that extends the distributional hypothesis from natural language -- words appearing in similar contexts tend to have similar meanings -- to spatially distributed data. We demonstrate empirically that Tile2Vec learns semantically meaningful representations on three datasets. Our learned representations significantly improve performance in downstream classification tasks and, similar to word vectors, visual analogies can be obtained via simple arithmetic in the latent space.",0
"In this work we present Tile2Vec: Unsupervised representation learning for spatially distributed data. We introduce two novel components that have been shown to effectively learn meaningful representations on a wide variety of datasets without any labeled training examples. First, by defining vector tile regions as basic building blocks of continuous space, we enable the neural network to explore relationships within and between spatial localities independently from one another. Second, a simple self attention mechanism encourages each location in the tile to attend selectively to relevant aspects of surrounding tiles and thus encode locally relevant information globally in the learned vectors. Our approach provides a general methodology for unsupervised pretraining of high capacity models across multiple domains while improving on existing state-of-the art methods both qualitatively and quantitatively. To demonstrate its effectiveness we evaluate our model on diverse tasks including image generation, object detection, and classification problems. Overall our results show that the proposed framework significantly outperforms competing alternatives.",1
"We consider the task of aligning two sets of points in high dimension, which has many applications in natural language processing and computer vision. As an example, it was recently shown that it is possible to infer a bilingual lexicon, without supervised data, by aligning word embeddings trained on monolingual data. These recent advances are based on adversarial training to learn the mapping between the two embeddings. In this paper, we propose to use an alternative formulation, based on the joint estimation of an orthogonal matrix and a permutation matrix. While this problem is not convex, we propose to initialize our optimization algorithm by using a convex relaxation, traditionally considered for the graph isomorphism problem. We propose a stochastic algorithm to minimize our cost function on large scale problems. Finally, we evaluate our method on the problem of unsupervised word translation, by aligning word embeddings trained on monolingual data. On this task, our method obtains state of the art results, while requiring less computational resources than competing approaches.",0
"Title: ""Unsupervised Alignment of Word Vectors""  Word embedding models like word2vec and GloVe have been successfully used to capture semantic relationships between words. However, these embeddings are often trained without any explicit alignment method, which leads to inconsistent and sometimes even conflicting interpretations. In order to address this problem, we propose an unsupervised alignment approach based on the Wasserstein Procrustes distance (WAD). We show that our proposed algorithm effectively aligns word vectors while preserving their original meaning, resulting in improved performance on downstream tasks such as analogy completion and sentence similarity comparison. Our results demonstrate the importance of proper alignment methods for effective use of pre-trained embeddings.",1
"We introduce GroundNet, a neural network for referring expression recognition -- the task of localizing (or grounding) in an image the object referred to by a natural language expression. Our approach to this task is the first to rely on a syntactic analysis of the input referring expression in order to inform the structure of the computation graph. Given a parse tree for an input expression, we explicitly map the syntactic constituents and relationships present in the tree to a composed graph of neural modules that defines our architecture for performing localization. This syntax-based approach aids localization of \textit{both} the target object and auxiliary supporting objects mentioned in the expression. As a result, GroundNet is more interpretable than previous methods: we can (1) determine which phrase of the referring expression points to which object in the image and (2) track how the localization of the target object is determined by the network. We study this property empirically by introducing a new set of annotations on the GoogleRef dataset to evaluate localization of supporting objects. Our experiments show that GroundNet achieves state-of-the-art accuracy in identifying supporting objects, while maintaining comparable performance in the localization of target objects.",0
"In natural image understanding tasks, one crucial challenge lies in the disambiguating referential expressions that point at objects. Recent advancements have shown great progress on object detection using deep learning algorithms. These approaches, however, often fail to accurately interpret referring expressions due to their reliance on global features extracted from images. We propose a novel approach that utilizes syntax together with visual representations as a means to ground referring expressions more accurately in natural images. Our approach leverages state-of-the-art language processing models pre-trained on large scale linguistic corpora, along with recently introduced visual representation models trained on massive datasets. Experimental results demonstrate substantially improved accuracy over competing baseline methods across various benchmarks including VRD, RefCOCOG, and CLEVR, indicating the effectiveness of our proposed approach.",1
"Visual Question Answering (VQA) models should have both high robustness and accuracy. Unfortunately, most of the current VQA research only focuses on accuracy because there is a lack of proper methods to measure the robustness of VQA models. There are two main modules in our algorithm. Given a natural language question about an image, the first module takes the question as input and then outputs the ranked basic questions, with similarity scores, of the main given question. The second module takes the main question, image and these basic questions as input and then outputs the text-based answer of the main question about the given image. We claim that a robust VQA model is one, whose performance is not changed much when related basic questions as also made available to it as input. We formulate the basic questions generation problem as a LASSO optimization, and also propose a large scale Basic Question Dataset (BQD) and Rscore (novel robustness measure), for analyzing the robustness of VQA models. We hope our BQD will be used as a benchmark for to evaluate the robustness of VQA models, so as to help the community build more robust and accurate VQA models.",0
"This paper presents a study on the robustness of visual question answering (VQA) models against basic questions, which are simple questions that can easily be answered using common sense knowledge. We investigate the ability of VQA models to answer these types of questions accurately and reliably, as well as their performance under different conditions such as varying complexity levels and formats of the questions. Our results show that while some VQA models perform well overall, there is still room for improvement, particularly when dealing with more complex questions. Additionally, we find that models trained specifically for the VQA task tend to outperform general image understanding systems. Overall, our research contributes to a better understanding of the strengths and limitations of current VQA models, which may inform future developments in this area.",1
"We propose a novel approach to multimodal sentiment analysis using deep neural networks combining visual analysis and natural language processing. Our goal is different than the standard sentiment analysis goal of predicting whether a sentence expresses positive or negative sentiment; instead, we aim to infer the latent emotional state of the user. Thus, we focus on predicting the emotion word tags attached by users to their Tumblr posts, treating these as ""self-reported emotions."" We demonstrate that our multimodal model combining both text and image features outperforms separate models based solely on either images or text. Our model's results are interpretable, automatically yielding sensible word lists associated with emotions. We explore the structure of emotions implied by our model and compare it to what has been posited in the psychology literature, and validate our model on a set of images that have been used in psychology studies. Finally, our work also provides a useful tool for the growing academic study of images - both photographs and memes - on social networks.",0
"The paper Multimodal Sentiment Analysis To Explore the Structure of Emotions seeks to investigate how sentiment analysis can be used to better understand the structure of emotions. By examining the relationships between different types of emotional data, such as facial expressions, text, voice tone, and body language, we can gain insights into the complex nature of human emotions. This research represents an important step towards building more accurate emotion recognition systems that can support tasks like customer service, healthcare, and education. Overall, our findings demonstrate the potential value of multimodal sentiment analysis for exploring the intricate connections between emotions, which could have far-reaching implications for psychology, neuroscience, sociology, anthropology, and other fields that study human behavior and experience.",1
"Training deep neural networks requires many training samples, but in practice training labels are expensive to obtain and may be of varying quality, as some may be from trusted expert labelers while others might be from heuristics or other sources of weak supervision such as crowd-sourcing. This creates a fundamental quality versus-quantity trade-off in the learning process. Do we learn from the small amount of high-quality data or the potentially large amount of weakly-labeled data? We argue that if the learner could somehow know and take the label-quality into account when learning the data representation, we could get the best of both worlds. To this end, we propose ""fidelity-weighted learning"" (FWL), a semi-supervised student-teacher approach for training deep neural networks using weakly-labeled data. FWL modulates the parameter updates to a student network (trained on the task we care about) on a per-sample basis according to the posterior confidence of its label-quality estimated by a teacher (who has access to the high-quality labels). Both student and teacher are learned from the data. We evaluate FWL on two tasks in information retrieval and natural language processing where we outperform state-of-the-art alternative semi-supervised methods, indicating that our approach makes better use of strong and weak labels, and leads to better task-dependent data representations.",0
"A new machine learning technique called fidelity-weighted learning has been proposed as a method for improving the accuracy and robustness of neural networks. This approach involves assigning weights to different parts of the network based on their importance and reliability. By doing so, the algorithm can learn more quickly from high-fidelity data while ignoring low-quality examples that may confuse it. The idea behind fidelity-weighted learning is that some pieces of input data may be of higher quality than others. For example, images taken under certain lighting conditions or with certain cameras might be less reliable than those taken under better circumstances. Similarly, audio recordings made in noisy environments might contain important details but have lower overall fidelity compared to pristine studio recordings. To account for these differences, the authors of the paper propose adding explicit weights to each sample during training. These weights reflect both the intrinsic value of individual samples (such as the clarity of an image) and how well they agree with other inputs. In essence, this ensures that the model focuses more on accurate examples and disregards ones with low confidence. The effectiveness of the method was tested using several benchmark datasets for object recognition, speech processing, and other tasks. Compared to standard models trained without weighting, the authors found significant improvements in performance across all metrics. For instance, average precision increased by up to six percent when classifying objects in photos; similar gains were observed elsewhere. Overall, the results demonstrate that fidelity-weighted learning is a promising direction for enhancing artificial intelligence systems. By incorporating quality awareness into neural nets, we can achieve greater stability and generalization abilities — essential properties fo",1
"Image captioning is a challenging task that combines the field of computer vision and natural language processing. A variety of approaches have been proposed to achieve the goal of automatically describing an image, and recurrent neural network (RNN) or long-short term memory (LSTM) based models dominate this field. However, RNNs or LSTMs cannot be calculated in parallel and ignore the underlying hierarchical structure of a sentence. In this paper, we propose a framework that only employs convolutional neural networks (CNNs) to generate captions. Owing to parallel computing, our basic model is around 3 times faster than NIC (an LSTM-based model) during training time, while also providing better results. We conduct extensive experiments on MSCOCO and investigate the influence of the model width and depth. Compared with LSTM-based models that apply similar attention mechanisms, our proposed models achieves comparable scores of BLEU-1,2,3,4 and METEOR, and higher scores of CIDEr. We also test our model on the paragraph annotation dataset, and get higher CIDEr score compared with hierarchical LSTMs",0
"In recent years, image captioning has emerged as an important task in computer vision research. This task involves generating natural language descriptions of images that accurately reflect their content. Many approaches have been proposed to address this challenge, including those based on convolutional neural networks (CNNs). However, these methods often struggle to generate accurate and coherent captions due to the complexity of the problem domain. To improve upon existing techniques, we propose a novel approach called ""Convolutional Decoder"" (CNNDecoder) that leverages the power of multi-modal fusion and spatial attention mechanisms to better capture semantic relationships between image features and corresponding text tokens. Our method achieves state-of-the-art performance on several benchmark datasets, demonstrating its effectiveness in capturing both global context and local details to produce more descriptive and accurate captions. Overall, our work represents a significant advance in the field of image captioning and provides valuable insights into how deep learning can be applied to solve complex problems at the intersection of computer vision and natural language processing.",1
"Vector embedding is a foundational building block of many deep learning models, especially in natural language processing. In this paper, we present a theoretical framework for understanding the effect of dimensionality on vector embeddings. We observe that the distributional hypothesis, a governing principle of statistical semantics, requires a natural unitary-invariance for vector embeddings. Motivated by the unitary-invariance observation, we propose the Pairwise Inner Product (PIP) loss, a unitary-invariant metric on the similarity between two embeddings. We demonstrate that the PIP loss captures the difference in functionality between embeddings, and that the PIP loss is tightly connect with two basic properties of vector embeddings, namely similarity and compositionality. By formulating the embedding training process as matrix factorization with noise, we reveal a fundamental bias-variance trade-off between the signal spectrum and noise power in the dimensionality selection process. This bias-variance trade-off sheds light on many empirical observations which have not been thoroughly explained, for example the existence of an optimal dimensionality. Moreover, we discover two new results about vector embeddings, namely their robustness against over-parametrization and their forward stability. The bias-variance trade-off of the PIP loss explicitly answers the fundamental open problem of dimensionality selection for vector embeddings.",0
"This should be short but descriptive, so please make sure that you cover the most important details but don't go into specific examples or figures from within the paper! Here is the general structure I recommend using (Feel free to edit as necessary): ""The purpose of this paper is [brief overview of topic area]. We investigate [key question/research problem], focusing on [specific methodology, approach, data set used]. Our findings support/contradict existing literature by showing[what was found; be specific and clear here]."" Abstract: In recent years, vector embeddings have become increasingly popular in natural language processing tasks due to their ability to capture semantic relationships among words. The distributional hypothesis posits that words close together in space correspond to semantically similar words in context. To learn these representations, models often rely on the pairwise inner product loss which minimizes the difference between predicted and true distances between pairs of instances. However, this loss function has been shown to suffer from a bias-variance trade-off where underfitting occurs at low regularization strengths while overfitting happens at high ones. In this work, we aim to gain a deeper understanding of the functionality and dimensionality of vector embeddings. Specifically, we examine the effects of different architectures and training settings on embeddings obtained via the pairwise inner product loss. Our results provide insights into how hyperparameters such as learning rate and batch size impact model performance, confirming previous findings in the literature. Additionally, our analysis shows that pretraining significantly reduces the effect of regularization strength on embeddings, allowing fo",1
"Fueled by massive amounts of data, models produced by machine-learning (ML) algorithms, especially deep neural networks, are being used in diverse domains where trustworthiness is a concern, including automotive systems, finance, health care, natural language processing, and malware detection. Of particular concern is the use of ML algorithms in cyber-physical systems (CPS), such as self-driving cars and aviation, where an adversary can cause serious consequences. However, existing approaches to generating adversarial examples and devising robust ML algorithms mostly ignore the semantics and context of the overall system containing the ML component. For example, in an autonomous vehicle using deep learning for perception, not every adversarial example for the neural network might lead to a harmful consequence. Moreover, one may want to prioritize the search for adversarial examples towards those that significantly modify the desired semantics of the overall system. Along the same lines, existing algorithms for constructing robust ML algorithms ignore the specification of the overall system. In this paper, we argue that the semantics and specification of the overall system has a crucial role to play in this line of research. We present preliminary research results that support this claim.",0
"Semantic adversarial deep learning refers to the field of study where artificial intelligence (AI) models trained on large amounts of data use natural language processing techniques to generate humanlike responses. In recent years, there has been significant growth in research related to these AI systems, which have shown promising results in areas such as customer service chatbots, content generation, and language translation. This review provides an overview of semantic adversarial deep learning, discussing its applications, advantages, challenges, and future directions. We analyze existing literature to identify key trends and highlight relevant case studies that demonstrate the potential impact of semantic adversarial deep learning on real-world problems. Our findings suggest that while there are still limitations to current approaches, continued development of semantic adversarial deep learning could lead to even more advanced capabilities that enable AI to perform complex tasks requiring both creativity and understanding. Ultimately, we hope that by providing insights into the state of the art, our work can inspire further innovation in the rapidly evolving field of AI.",1
"Metric learning has the aim to improve classification accuracy by learning a distance measure which brings data points from the same class closer together and pushes data points from different classes further apart. Recent research has demonstrated that metric learning approaches can also be applied to trees, such as molecular structures, abstract syntax trees of computer programs, or syntax trees of natural language, by learning the cost function of an edit distance, i.e. the costs of replacing, deleting, or inserting nodes in a tree. However, learning such costs directly may yield an edit distance which violates metric axioms, is challenging to interpret, and may not generalize well. In this contribution, we propose a novel metric learning approach for trees which learns an edit distance indirectly by embedding the tree nodes as vectors, such that the Euclidean distance between those vectors supports class discrimination. We learn such embeddings by reducing the distance to prototypical trees from the same class and increasing the distance to prototypical trees from different classes. In our experiments, we show that our proposed metric learning approach improves upon the state-of-the-art in metric learning for trees on six benchmark data sets, ranging from computer science over biomedical data to a natural-language processing data set containing over 300,000 nodes.",0
"This paper presents a new method for learning tree edit distance (TED) using adaptive symbol embeddings. TED is a measure of similarity between two trees that takes into account the insertion, deletion, and substitution of nodes. The proposed approach uses adaptive symbol embeddings to capture local patterns in trees and learns an efficient representation for TED computation. We evaluate our method on several benchmark datasets and show that it outperforms state-of-the-art methods in terms of accuracy and efficiency. Our supplementary materials provide detailed explanations of the methodology used in the paper as well as additional experimental results and analysis. Overall, our work demonstrates the effectiveness of using adaptive symbol embeddings for TED learning and provides insights into improving tree comparison algorithms.",1
"Linguistic style is an essential part of written communication, with the power to affect both clarity and attractiveness. With recent advances in vision and language, we can start to tackle the problem of generating image captions that are both visually grounded and appropriately styled. Existing approaches either require styled training captions aligned to images or generate captions with low relevance. We develop a model that learns to generate visually relevant styled captions from a large corpus of styled text without aligned images. The core idea of this model, called SemStyle, is to separate semantics and style. One key component is a novel and concise semantic term representation generated using natural language processing techniques and frame semantics. In addition, we develop a unified language model that decodes sentences with diverse word choices and syntax for different styles. Evaluations, both automatic and manual, show captions from SemStyle preserve image semantics, are descriptive, and are style shifted. More broadly, this work provides possibilities to learn richer image descriptions from the plethora of linguistic data available on the web.",0
"Title: Generating Styled Image Descriptions without Alignment Data  Abstract: This work proposes a novel approach to generating styled image descriptions that are both visually appealing and semantically accurate. Our method relies on existing image generation models that produce images conditioned on textual prompts, but instead of directly conditioning on scene content, we rely on pretrained object detectors and visual features from these generated images to create high quality descriptions. We use an LSTM based language model to generate descriptive paragraphs in a particular style, given only the detection bounding boxes and no alignment data. Experiments demonstrate that our method produces detailed, accurate, and varied results across multiple styles of writing, outperforming baseline methods by significant margins while requiring substantially less training data and computational resources. Additionally, we show how our system can generate styled captions for real-world photos to evaluate their coherence and relevance to actual scenes. This research contributes new insights into computer vision applications, providing practitioners with tools for generating human-like, creative, and diverse descriptions for their systems.",1
"Following great success in the image processing field, the idea of adversarial training has been applied to tasks in the natural language processing (NLP) field. One promising approach directly applies adversarial training developed in the image processing field to the input word embedding space instead of the discrete input space of texts. However, this approach abandons such interpretability as generating adversarial texts to significantly improve the performance of NLP tasks. This paper restores interpretability to such methods by restricting the directions of perturbations toward the existing words in the input embedding space. As a result, we can straightforwardly reconstruct each input with perturbations to an actual text by considering the perturbations to be the replacement of words in the sentence while maintaining or even improving the task performance.",0
"In recent years, deep learning has emerged as one of the most promising approaches for natural language processing tasks, such as sentiment analysis, machine translation, and question answering. However, despite their successes, these models still struggle to explain why they make certain predictions, which hinders their trustworthiness and use in critical applications. To address this challenge, researchers have explored interpretability techniques that aim to shed light on how neural networks arrive at their decisions. One popular approach involves adding interpretabi",1
"Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code's known syntax. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures.   In this work, we present how to construct graphs from source code and how to scale Gated Graph Neural Networks training to such large graphs. We evaluate our method on two tasks: VarNaming, in which a network attempts to predict the name of a variable given its usage, and VarMisuse, in which the network learns to reason about selecting the correct variable that should be used at a given program location. Our comparison to methods that use less structured program representations shows the advantages of modeling known structure, and suggests that our models learn to infer meaningful names and to solve the VarMisuse task in many cases. Additionally, our testing showed that VarMisuse identifies a number of bugs in mature open-source projects.",0
"Title: ""A Study on Graph-Based Representations of Computer Programs""  This research investigates the potential benefits of using graph representations to model computer programs. Traditional approaches have relied heavily on textual descriptions, which can become cumbersome as programs grow more complex. In contrast, graphs offer powerful visual tools that allow for easy manipulation and understanding of program structure. The goal of our study was to determine if graph-based representations could enhance programming productivity by improving comprehension, maintainability, and collaboration among developers. We conducted experiments using real-world software projects and evaluated their effectiveness through user surveys and code analysis metrics. Our results showed that graph-based representations were well received by participants and led to significant improvements in several areas of concern to software engineering professionals. This work has important implications for future developments in programming languages, IDEs, and other software development tools. By exploring alternative forms of representation, we can push the boundaries of software development and empower developers to build even better applications.",1
"Generating images from natural language is one of the primary applications of recent conditional generative models. Besides testing our ability to model conditional, highly dimensional distributions, text to image synthesis has many exciting and practical applications such as photo editing or computer-aided content creation. Recent progress has been made using Generative Adversarial Networks (GANs). This material starts with a gentle introduction to these topics and discusses the existent state of the art models. Moreover, I propose Wasserstein GAN-CLS, a new model for conditional image generation based on the Wasserstein distance which offers guarantees of stability. Then, I show how the novel loss function of Wasserstein GAN-CLS can be used in a Conditional Progressive Growing GAN. In combination with the proposed loss, the model boosts by 7.07% the best Inception Score (on the Caltech birds dataset) of the models which use only the sentence-level visual semantics. The only model which performs better than the Conditional Wasserstein Progressive Growing GAN is the recently proposed AttnGAN which uses word-level visual semantics as well.",0
"This research work explores the use of generative adversarial networks (GANs) for text to image synthesis. GANs have been used successfully in several computer vision tasks such as image generation, super resolution, image translation, and more recently in text to image synthesis. In these systems, two neural network architectures, generator and discriminator, play against each other in a zero-sum game-like framework that alternately trains both models until convergence. While early attempts at using GANs for text to image synthesis were promising, they suffered from instability during training due to nonlinear mappings between input vectors and output images. Recent advancements in GAN architecture design have mitigated these issues, allowing for high-quality text to image synthesis models to be developed. Our contribution is in extending these recent advances to enable fast and efficient large scale text-to-image modeling with superior image quality compared to previous methods. Furthermore, we introduce two new regularization techniques: MixUp based on linear interpolations between training samples and DropConnect which randomly drops weights in the convolutional layers during training. We show that our proposed improvements allow for significant enhancement over state-of-the art text-to-image synthesis approaches. Finally, experimental evaluation demonstrate the effectiveness of our approach by comparing it to prior works via quantitative measures and human study.",1
"Grounding textual phrases in visual content is a meaningful yet challenging problem with various potential applications such as image-text inference or text-driven multimedia interaction. Most of the current existing methods adopt the supervised learning mechanism which requires ground-truth at pixel level during training. However, fine-grained level ground-truth annotation is quite time-consuming and severely narrows the scope for more general applications. In this extended abstract, we explore methods to localize flexibly image regions from the top-down signal (in a form of one-hot label or natural languages) with a weakly supervised attention learning mechanism. In our model, two types of modules are utilized: a backbone module for visual feature capturing, and an attentive module generating maps based on regularized bilinear pooling. We construct the model in an end-to-end fashion which is trained by encouraging the spatial attentive map to shift and focus on the region that consists of the best matched visual features with the top-down signal. We demonstrate the preliminary yet promising results on a testbed that is synthesized with multi-label MNIST data.",0
"Text grounding refers to linking natural language text with relevant images or videos. Textual phrases correspond to meaningful parts of text such as objects, attributes, relationships, events, etc., which can ground to corresponding image regions. In recent years, weak supervision has become increasingly popular due to its scalability and flexibility. However, most previous works focus on learning semantic representations from fully labeled data or limited human annotations. Here we propose a novel approach called Weakly Supervised Attention Learning (WSAL) that integrates multiple weak supervisory signals including phrase-level labels, web search results, and knowledge base facts, to learn attention models for efficient and effective textual phrase grounding under weak supervision. Our framework first predicts bounding boxes associated with each phrase, then refines them by fusing multiple signals through learned attention mechanisms. Extensive experiments show WSAL outperforms state-of-the-art methods under weak label settings while providing superior tradeoffs between performance and efficiency. As future work, we plan to extend our method to handle more complex scenarios like interactive grounding tasks.",1
"In this paper, a taxonomy for memory networks is proposed based on their memory organization. The taxonomy includes all the popular memory networks: vanilla recurrent neural network (RNN), long short term memory (LSTM ), neural stack and neural Turing machine and their variants. The taxonomy puts all these networks under a single umbrella and shows their relative expressive power , i.e. vanilla RNN =LSTM=neural stack=neural RAM. The differences and commonality between these networks are analyzed. These differences are also connected to the requirements of different tasks which can give the user instructions of how to choose or design an appropriate memory network for a specific task. As a conceptual simplified class of problems, four tasks of synthetic symbol sequences: counting, counting with interference, reversing and repeat counting are developed and tested to verify our arguments. And we use two natural language processing problems to discuss how this taxonomy helps choosing the appropriate neural memory networks for real world problem.",0
"Title: A Comprehensive Analysis of Neural Memory Network Architectures  Abstract:  Memory networks have emerged as powerful neural network architectures that leverage memory mechanisms to solve complex tasks such as question answering, machine translation, and image classification. These models are based on different principles, including attention, recurrent layers, convolutional neural networks (CNNs), transformers, generative adversarial networks (GANs), variational autoencoders (VAEs), reinforcement learning, meta-learning, transfer learning, continuous training, hierarchical processing, hybridization, multi-tasking, and ensembling. In addition, memory networks can vary according to their capacity, depth, width, sparsity, regularization techniques, weight initialization methods, optimization algorithms, loss functions, evaluation metrics, hyperparameter tuning procedures, computational complexity, parallelism, distributed computing environments, scalability, explainability, interpretability, robustness, generalizability, stability, and performance under uncertainty conditions. Therefore, developing a systematic taxonomy to classify, compare, contrast, evaluate, analyze, extend, and improve existing neural memory networks represents a formidable challenge but offers immense opportunities for further advancing state-of-the-art artificial intelligence research. This study aims to fill that gap by providing a comprehensive analysis of current neural memory networks and proposes a unified framework for future developments. Our work has implications across multiple domains where memory-augmented neural networks play crucial roles, paving the way towards more human-like intelligent systems capable of addressing societal grand challenges while minimizing negative impacts on the environment.",1
"Humans effortlessly ""program"" one another by communicating goals and desires in natural language. In contrast, humans program robotic behaviours by indicating desired object locations and poses to be achieved, by providing RGB images of goal configurations, or supplying a demonstration to be imitated. None of these methods generalize across environment variations, and they convey the goal in awkward technical terms. This work proposes joint learning of natural language grounding and instructable behavioural policies reinforced by perceptual detectors of natural language expressions, grounded to the sensory inputs of the robotic agent. Our supervision is narrated visual demonstrations(NVD), which are visual demonstrations paired with verbal narration (as opposed to being silent). We introduce a dataset of NVD where teachers perform activities while describing them in detail. We map the teachers' descriptions to perceptual reward detectors, and use them to train corresponding behavioural policies in simulation.We empirically show that our instructable agents (i) learn visual reward detectors using a small number of examples by exploiting hard negative mined configurations from demonstration dynamics, (ii) develop pick-and place policies using learned visual reward detectors, (iii) benefit from object-factorized state representations that mimic the syntactic structure of natural language goal expressions, and (iv) can execute behaviours that involve novel objects in novel locations at test time, instructed by natural language.",0
"Abstract Rewarding agents for tasks that require demonstration can pose unique challenges given the high-dimensional nature of such problems. Here we present work on reward learning from narrated demonstrations (RLfND) which addresses these difficulties by modeling the demonstration as text and applying natural language processing techniques to convert the demonstrated steps into low-level actions that can then form the basis of reward shaping. We compare three approaches: using only expert annotations as rewards, incorporating both annotator guidance and program states for better alignment, and exploiting additional contextual feedback provided via natural language queries posed during demo playback. Empirically evaluating across a range of complex programming domains shows clear advantages over all baseline methods. While still preliminary, our findings indicate promising directions towards enabling more effective reinforcement learning through interactive human teaching intervention.",1
"Recent works on zero-shot learning make use of side information such as visual attributes or natural language semantics to define the relations between output visual classes and then use these relationships to draw inference on new unseen classes at test time. In a novel extension to this idea, we propose the use of visual prototypical concepts as side information. For most real-world visual object categories, it may be difficult to establish a unique prototype. However, in cases such as traffic signs, brand logos, flags, and even natural language characters, these prototypical templates are available and can be leveraged for an improved recognition performance. The present work proposes a way to incorporate this prototypical information in a deep learning framework. Using prototypes as prior information, the deepnet pipeline learns the input image projections into the prototypical embedding space subject to minimization of the final classification loss. Based on our experiments with two different datasets of traffic signs and brand logos, prototypical embeddings incorporated in a conventional convolutional neural network improve the recognition performance. Recognition accuracy on the Belga logo dataset is especially noteworthy and establishes a new state-of-the-art. In zero-shot learning scenarios, the same system can be directly deployed to draw inference on unseen classes by simply adding the prototypical information for these new classes at test time. Thus, unlike earlier approaches, testing on seen and unseen classes is handled using the same pipeline, and the system can be tuned for a trade-off of seen and unseen class performance as per task requirement. Comparison with one of the latest works in the zero-shot learning domain yields top results on the two datasets mentioned above.",0
"In our latest research we explored how prototypes can improve deep learning models by imposing smoothness constraints on them that lead to more discriminative features. Our work was motivated by recent results showing how soft probabilities learned from these priors can significantly improve the accuracy of classifiers trained on small amounts of data without explicit fine-tuning. Specifically, our contributions show that the same kinds of regularization terms used in prior art on CNNs leads to improvements in performance for both ResNet56 and MobileNets using a linear classifier (CNN outputs fed into an L2 regression). We observe state-of-the-art accuracies over all three benchmark datasets and demonstrate new state-of-the-art results on CIFAR10, SVHN, and even tiny ImageNet. Not only were improved accuracies achieved but furthermore they were achieved at faster convergence speeds than those reported before across different model architectures and data sizes. Lastly, we conducted ablation studies to better understand why the smoothness priors led to improvements in accuracy as well as hyperparameter sensitivity analysis. These ablations showed strong evidence of the robustness of our approach against common corruptions and noise as seen through increased accuracy under such conditions. In summary, this work presents a framework using prototypical smoothed posteriors which enables zero shot learning by pretraining classifiers on small labeled dataset with good accuracy and few label queries required; thus providing promise for real world deployment with high efficiency.",1
"Automatically describing a video with natural language is regarded as a fundamental challenge in computer vision. The problem nevertheless is not trivial especially when a video contains multiple events to be worthy of mention, which often happens in real videos. A valid question is how to temporally localize and then describe events, which is known as ""dense video captioning."" In this paper, we present a novel framework for dense video captioning that unifies the localization of temporal event proposals and sentence generation of each proposal, by jointly training them in an end-to-end manner. To combine these two worlds, we integrate a new design, namely descriptiveness regression, into a single shot detection structure to infer the descriptive complexity of each detected proposal via sentence generation. This in turn adjusts the temporal locations of each event proposal. Our model differs from existing dense video captioning methods since we propose a joint and global optimization of detection and captioning, and the framework uniquely capitalizes on an attribute-augmented video captioning architecture. Extensive experiments are conducted on ActivityNet Captions dataset and our framework shows clear improvements when compared to the state-of-the-art techniques. More remarkably, we obtain a new record: METEOR of 12.96% on ActivityNet Captions official test set.",0
"This paper proposes a novel framework that jointly localizes and describes events in video frames to generate dense natural language descriptions. Previous event recognition algorithms have focused on either object detection or classifying high-level labels without precise location annotations. Our method combines object detectors with temporal reasoning to link multiple object instances across time and space, resulting in more accurate localization and description of events. We achieve state-of-the-art performance on two challenging benchmarks, demonstrating our approach’s effectiveness at dense video captioning. By accurately identifying objects and their interactions in scenes over time, we enable more intelligent applications such as video summarization, question answering, and autonomous systems.",1
"Visual question answering is a recently proposed artificial intelligence task that requires a deep understanding of both images and texts. In deep learning, images are typically modeled through convolutional neural networks, and texts are typically modeled through recurrent neural networks. While the requirement for modeling images is similar to traditional computer vision tasks, such as object recognition and image classification, visual question answering raises a different need for textual representation as compared to other natural language processing tasks. In this work, we perform a detailed analysis on natural language questions in visual question answering. Based on the analysis, we propose to rely on convolutional neural networks for learning textual representations. By exploring the various properties of convolutional neural networks specialized for text data, such as width and depth, we present our ""CNN Inception + Gate"" model. We show that our model improves question representations and thus the overall accuracy of visual question answering models. We also show that the text representation requirement in visual question answering is more complicated and comprehensive than that in conventional natural language processing tasks, making it a better task to evaluate textual representation methods. Shallow models like fastText, which can obtain comparable results with deep learning models in tasks like text classification, are not suitable in visual question answering.",0
"Here is my attempt at writing an abstract for the given research paper:  Visual question answering (VQA) is an active area of research that aims to develop systems capable of answering questions about images by providing accurate and relevant responses from vast amounts of text data. In recent years, there has been significant progress in VQA thanks to advancements in deep learning techniques such as convolutional neural networks (CNNs). These models have shown impressive performance on benchmark datasets but still face several challenges, including understanding complex relationships among objects and reasoning across multiple modalities.  This paper proposes a novel approach to learn convolutional representations for visual question answering using an attention mechanism that selects important features from the image and applies them directly to the question. Our model is trained end-to-end on both vision and language tasks without any fine-tuning or pre-training of separate components. Extensive experiments conducted on popular VQA benchmarks demonstrate the effectiveness of our method, achieving state-of-the-art results. Moreover, we analyze the behavior of our model under different scenarios and discuss how it can generalize to new domains and settings. Finally, we provide insights into potential future directions for VQA research.  The proposed methodology presents a promising step toward developing more intelligent computer vision systems capable of human-level comprehension and reasoning about visual scenes. With further improvements and refinement, VQA could find numerous applications in fields such as education, entertainment, healthcare, and robotics.",1
"Inspired by previous work on emergent communication in referential games, we propose a novel multi-modal, multi-step referential game, where the sender and receiver have access to distinct modalities of an object, and their information exchange is bidirectional and of arbitrary duration. The multi-modal multi-step setting allows agents to develop an internal communication significantly closer to natural language, in that they share a single set of messages, and that the length of the conversation may vary according to the difficulty of the task. We examine these properties empirically using a dataset consisting of images and textual descriptions of mammals, where the agents are tasked with identifying the correct object. Our experiments indicate that a robust and efficient communication protocol emerges, where gradual information exchange informs better predictions and higher communication bandwidth improves generalization.",0
"This paper presents an analysis of emergent communication strategies in a multi-modal, multi-step referential game played by human participants and artificial intelligence agents. Using a combination of computational linguistics methods and qualitative case studies, we examine how individuals negotiate meaning in complex, open-ended interactions involving multiple modes of expression (e.g., speech, gesture, writing) and layers of reference (e.g., shared memory, context, social cues). Our results show that successful collaboration across modalities requires flexible interpretation and coordination mechanisms tailored to different interactional needs, as well as creative adaptation of existing communicative resources such as figurative language and pragmatic inference. We discuss implications for developing more expressive and adaptive AI systems capable of interacting naturally within rich multimodal environments, including those requiring extended joint reasoning and decision-making across multiple time scales and levels of abstraction. By highlighting key challenges and opportunities at the intersection of cognitive science, computer science, and design, our work contributes to ongoing efforts towards building next-generation human-AI interfaces bridging the gap between formal symbolic processing and embodied experience.",1
"Imagining a scene described in natural language with realistic layout and appearance of entities is the ultimate test of spatial, visual, and semantic world knowledge. Towards this goal, we present the Composition, Retrieval, and Fusion Network (CRAFT), a model capable of learning this knowledge from video-caption data and applying it while generating videos from novel captions. CRAFT explicitly predicts a temporal-layout of mentioned entities (characters and objects), retrieves spatio-temporal entity segments from a video database and fuses them to generate scene videos. Our contributions include sequential training of components of CRAFT while jointly modeling layout and appearances, and losses that encourage learning compositional representations for retrieval. We evaluate CRAFT on semantic fidelity to caption, composition consistency, and visual quality. CRAFT outperforms direct pixel generation approaches and generalizes well to unseen captions and to unseen video databases with no text annotations. We demonstrate CRAFT on FLINTSTONES, a new richly annotated video-caption dataset with over 25000 videos. For a glimpse of videos generated by CRAFT, see https://youtu.be/688Vv86n0z8.",0
"Scriptwriting has become an essential part of digital media production due to its ability to organize thoughts into visually appealing stories that captivate audiences. However, translating scripts to compositions remains challenging as video producers need clear guidelines on how to adapt narrations into effective videos. This paper offers insightful advice by delving into script analysis techniques used in various film genres from short films to feature films. By analyzing successful movies across different eras, readers can acquire knowledge on constructive ways of developing engaging visual storylines that connect with viewers on both cognitive and emotional levels. To create dynamic videos with impactful messages, scriptwriters should identify their target audience, comprehend their desires and needs, and then tailor their content accordingly. With these tips, novice video creators can begin crafting exceptional compositions that leave lasting impressions on their audiences. This publication is intended for professionals seeking guidance on producing outstanding videos and aspiring scriptwriters looking to hone their skills.",1
"A robot that can carry out a natural-language instruction has been a dream since before the Jetsons cartoon series imagined a life of leisure mediated by a fleet of attentive robot helpers. It is a dream that remains stubbornly distant. However, recent advances in vision and language methods have made incredible progress in closely related areas. This is significant because a robot interpreting a natural-language navigation instruction on the basis of what it sees is carrying out a vision and language process that is similar to Visual Question Answering. Both tasks can be interpreted as visually grounded sequence-to-sequence translation problems, and many of the same methods are applicable. To enable and encourage the application of vision and language methods to the problem of interpreting visually-grounded navigation instructions, we present the Matterport3D Simulator -- a large-scale reinforcement learning environment based on real imagery. Using this simulator, which can in future support a range of embodied vision and language tasks, we provide the first benchmark dataset for visually-grounded natural language navigation in real buildings -- the Room-to-Room (R2R) dataset.",0
"This paper explores the intersection of vision and language as they relate to human navigation within complex environments. The authors present a comprehensive analysis of how individuals use natural language and visual cues to navigate through novel spaces. Specifically, the research focuses on understanding the interpretive processes involved in following visual grounding instructions (e.g., turn left at the red bookcase) versus more traditional linguistically based instructions (e.g., follow signs for room 206). They utilize experiments involving simulated indoor navigation tasks alongside eye tracking technology and other behavioral measures to uncover patterns in participants’ strategies for processing and acting upon different types of instruction. Ultimately, these findings have important implications for the design of future assistive technologies that seek to support individuals during travel or explore new environments. Keywords: Visual Grounding, Language &amp; Cognition, Human Factors Psychology, Spatial Cognition",1
"To truly understand the visual world our models should be able not only to recognize images but also generate them. To this end, there has been exciting recent progress on generating images from natural language descriptions. These methods give stunning results on limited domains such as descriptions of birds or flowers, but struggle to faithfully reproduce complex sentences with many objects and relationships. To overcome this limitation we propose a method for generating images from scene graphs, enabling explicitly reasoning about objects and their relationships. Our model uses graph convolution to process input graphs, computes a scene layout by predicting bounding boxes and segmentation masks for objects, and converts the layout to an image with a cascaded refinement network. The network is trained adversarially against a pair of discriminators to ensure realistic outputs. We validate our approach on Visual Genome and COCO-Stuff, where qualitative results, ablations, and user studies demonstrate our method's ability to generate complex images with multiple objects.",0
"Scene graphs provide a flexible representation format that enables artists to describe complex scenes composed of multiple objects with their geometry, texture coordinates, materials properties, lights, and more importantly, spatial relationships between these entities. This abstraction allows non-programmers such as level designers or content creators to author graphics assets at scale without writing any code or scripting logic using high level data structure definition tools like node editors or blend files commonly used in offline rendering or realtime engines alike. With the rise of artificial intelligence (AI), new techniques have emerged to allow machines to synthesize novel images based on learned statistical models derived from vast collections of training examples. These methods often rely either on convolutional neural networks (CNN) architecture pretrained on large image datasets which can classify or reconstruct specific target viewpoints given some initial input, or generate completely novel contents from scratch by learning latent codes representations that map to desired outputs through generative adversarial network architectures (GAN). In this work we present a method enabling scene graph based image generation by combining these two ideas together. We train our model using a carefully designed dataset of ground truth renderings from different viewpoints, lighting conditions, and object arrangements extracted directly from professional video games or artist created digital assets. Our generator takes as input a simple text prompt describing a scenario along with a set of parameters quantifying camera position or rotation, time of day, fog density, and other relevant variables controllable via a state of art authoring interface. Given the success of existing game engine technology leveraging ray tracing, path tracing algorithms, and screen space reflections capable of delivering high quality visual fidelity approaching photo realism, we believe tha",1
"Despite the availability of a huge amount of video data accompanied by descriptive texts, it is not always easy to exploit the information contained in natural language in order to automatically recognize video concepts. Towards this goal, in this paper we use textual cues as means of supervision, introducing two weakly supervised techniques that extend the Multiple Instance Learning (MIL) framework: the Fuzzy Sets Multiple Instance Learning (FSMIL) and the Probabilistic Labels Multiple Instance Learning (PLMIL). The former encodes the spatio-temporal imprecision of the linguistic descriptions with Fuzzy Sets, while the latter models different interpretations of each description's semantics with Probabilistic Labels, both formulated through a convex optimization algorithm. In addition, we provide a novel technique to extract weak labels in the presence of complex semantics, that consists of semantic similarity computations. We evaluate our methods on two distinct problems, namely face and action recognition, in the challenging and realistic setting of movies accompanied by their screenplays, contained in the COGNIMUSE database. We show that, on both tasks, our method considerably outperforms a state-of-the-art weakly supervised approach, as well as other baselines.",0
"In recent years, there has been significant interest in developing algorithms that can learn from large amounts of visual data without requiring dense annotation. Many approaches have focused on using weak supervision techniques to bootstrap learning from limited labeled data, while others have explored unsupervised pretraining as a means of initializing models before fine-tuning them with limited annotations. However, most existing methods have only considered single modalities such as images or videos, which ignores the rich multimodality present in many real-world applications like human activity recognition from both RGB cameras and depth sensors. This work presents a novel method for leveraging multiple modalities within a weakly supervised framework for the task of concept classification across different tasks including image classification, action classification and object detection. Our approach uses transfer learning along two axes: first by distilling knowledge from a generative model trained with paired video frames, and then distillating that knowledge again into a classifier for each individual modality. By jointly training these classifiers along with a shared backbone network, we enable them to leverage complementary cues from other modalities in order to improve overall performance. Experiments on five challenging benchmark datasets demonstrate the effectiveness of our approach outperforming state-of-the-art methods. Overall, our results highlight the importance of considering multimodal inputs for visual concept learning under weak supervision settings and suggest several promising directions for future research.",1
"Interaction and collaboration between humans and intelligent machines has become increasingly important as machine learning methods move into real-world applications that involve end users. While much prior work lies at the intersection of natural language and vision, such as image captioning or image generation from text descriptions, less focus has been placed on the use of language to guide or improve the performance of a learned visual processing algorithm. In this paper, we explore methods to flexibly guide a trained convolutional neural network through user input to improve its performance during inference. We do so by inserting a layer that acts as a spatio-semantic guide into the network. This guide is trained to modify the network's activations, either directly via an energy minimization scheme or indirectly through a recurrent model that translates human language queries to interaction weights. Learning the verbal interaction is fully automatic and does not require manual text annotations. We evaluate the method on two datasets, showing that guiding a pre-trained network can improve performance, and provide extensive insights into the interaction between the guide and the CNN.",0
"Title: ""Deep Networks: Understanding User Preferences and Providing Relevant Responses""  In recent years, deep networks have become increasingly popular due to their ability to process large amounts of data and generate accurate predictions. However, despite these advances, there remains a significant challenge in understanding user preferences and providing relevant responses. In this work, we present an analysis of several key aspects of interacting with deep networks, including user interaction design, contextualization, and personalization. We discuss the importance of considering user needs and preferences during system development and highlight ways that users can communicate more effectively with deep networks to obtain better results. Our findings provide insights into how to improve interactions with deep networks and enhance the overall user experience. This research has implications for both developers and end-users who want to make the most out of using these powerful tools.",1
"In this paper, the problem of describing visual contents of a video sequence with natural language is addressed. Unlike previous video captioning work mainly exploiting the cues of video contents to make a language description, we propose a reconstruction network (RecNet) with a novel encoder-decoder-reconstructor architecture, which leverages both the forward (video to sentence) and backward (sentence to video) flows for video captioning. Specifically, the encoder-decoder makes use of the forward flow to produce the sentence description based on the encoded video semantic features. Two types of reconstructors are customized to employ the backward flow and reproduce the video features based on the hidden state sequence generated by the decoder. The generation loss yielded by the encoder-decoder and the reconstruction loss introduced by the reconstructor are jointly drawn into training the proposed RecNet in an end-to-end fashion. Experimental results on benchmark datasets demonstrate that the proposed reconstructor can boost the encoder-decoder models and leads to significant gains in video caption accuracy.",0
"Deep learning has revolutionized many fields, including computer vision, natural language processing (NLP), speech recognition, and autonomous vehicles. One area that has benefited greatly from deep learning is video captioning, which involves generating textual descriptions of the content within videos. In recent years, several works have proposed methods using convolutional neural networks (CNNs) pretrained on large datasets such as ImageNet to generate static image captions. However, these approaches suffer from two main limitations: they cannot fully capture the dynamics present in video data and they require heavy computing resources. We propose a novel approach called the Reconstruction Network for Video Captioning (RViCap). Our method leverages temporal context by incorporating a reconstruction loss term into the objective function, allowing our model to learn spatiotemporal representations better suited for describing dynamic scenes found in videos. RViCap generates more coherent and semantically meaningful captions than state-of-the-art image generation techniques, while also requiring fewer computational resources due to its compact architecture. Evaluation results demonstrate the effectiveness of our approach compared to competitive baselines, achieving superior performance both qualitatively and quantitatively across three different datasets: MSVD, MSR-VTT, and DiDeMo. Overall, RViCap establishes itself as a capable solution for real-world applications that require efficient and accurate video captioning capabilities.",1
"We propose a new learning paradigm called Deep Memory. It has the potential to completely revolutionize the Machine Learning field. Surprisingly, this paradigm has not been reinvented yet, unlike Deep Learning. At the core of this approach is the \textit{Learning By Heart} principle, well studied in primary schools all over the world.   Inspired by poem recitation, or by $\pi$ decimal memorization, we propose a concrete algorithm that mimics human behavior. We implement this paradigm on the task of generative modeling, and apply to images, natural language and even the $\pi$ decimals as long as one can print them as text. The proposed algorithm even generated this paper, in a one-shot learning setting. In carefully designed experiments, we show that the generated samples are indistinguishable from the training examples, as measured by any statistical tests or metrics.",0
"In this paper, we argue that memory is all you need in order to process tasks. We begin by discussing the limitations of traditional models trained on massive amounts of data that rely heavily on slow inference algorithms like backpropagation, which can take hours to run. Instead, we propose using generative memory networks (MemNETs), a type of recurrent neural network architecture where memories can be shared across time steps as well as between different parts of the model. By doing so, we show that our approach outperforms state-of-the-art models on a variety of benchmark datasets. Additionally, our method allows us to achieve better generalization performance even with fewer parameters, making it more efficient than existing methods. Our work has important implications for the development of intelligent systems that are able to learn quickly and make decisions based on limited information. Overall, we believe that memory is key to understanding intelligence and should play a central role in future research efforts aimed at building artificially intelligent agents. In this paper, we present a novel approach to processing tasks through the use of generative memory networks (MemNETs). These networks utilize memory sharing across both time steps and within different components of the model. This allows them to significantly outperform current state-of-the-art techniques when evaluated on several benchmark datasets. Further, our results demonstrate that our proposed approach achieves excellent generalizability while requiring fewer parameters than other methods currently available. As such, these findings have potential impact on the development of high performance learning systems capable of adaptive decision making with limited exposure to training inputs. Ultimately, we believe that memory plays a crucial role in understanding cognition, and thus, holds great importance in the pursuit of creating intelligent artificial entities capable of complex reasoning and problem solving. Accordingly, further exploration into the mechanisms underlying human memory processes may prove highly valuable for advancing research in artificial intelligence.",1
"During the last years, a remarkable breakthrough has been made in AI domain thanks to artificial deep neural networks that achieved a great success in many machine learning tasks in computer vision, natural language processing, speech recognition, malware detection and so on. However, they are highly vulnerable to easily crafted adversarial examples. Many investigations have pointed out this fact and different approaches have been proposed to generate attacks while adding a limited perturbation to the original data. The most robust known method so far is the so called C&W attack [1]. Nonetheless, a countermeasure known as feature squeezing coupled with ensemble defense showed that most of these attacks can be destroyed [6]. In this paper, we present a new method we call Centered Initial Attack (CIA) whose advantage is twofold : first, it insures by construction the maximum perturbation to be smaller than a threshold fixed beforehand, without the clipping process that degrades the quality of attacks. Second, it is robust against recently introduced defenses such as feature squeezing, JPEG encoding and even against a voting ensemble of defenses. While its application is not limited to images, we illustrate this using five of the current best classifiers on ImageNet dataset among which two are adversarialy retrained on purpose to be robust against attacks. With a fixed maximum perturbation of only 1.5% on any pixel, around 80% of attacks (targeted) fool the voting ensemble defense and nearly 100% when the perturbation is only 6%. While this shows how it is difficult to defend against CIA attacks, the last section of the paper gives some guidelines to limit their impact.",0
"Neural networks have proven themselves effective at solving complex problems that were previously deemed unsolvable by traditional algorithms. However, these models remain vulnerable to adversarial attacks which exploit their weaknesses. Adversaries can generate input samples crafted to fool classifiers into making incorrect predictions even though they are visually indistinguishable from real inputs. Existing methods to defend against such attacks tend to reduce model accuracy on benign inputs due to their reliance on additive noise, blurring filters, or image transformations. We present a new method for generating robust adversarial examples designed to evade detection while preserving the visual similarity between original and perturbed images. Our approach relies on solving optimization problems directly in image space rather than adding regularization terms to the loss function. As a result, we achieve better performance on multiple benchmark datasets compared to previous state-of-the-art defenses without sacrificing much accuracy on clean data. In summary, our contributions are twofold: (i) a novel method for constructing robust adversarial examples that can escape detection while maintaining high fidelity; (ii) evidence that this strategy performs better than existing alternatives across several datasets on different architectures.",1
"In this paper, we address referring expression comprehension: localizing an image region described by a natural language expression. While most recent work treats expressions as a single unit, we propose to decompose them into three modular components related to subject appearance, location, and relationship to other objects. This allows us to flexibly adapt to expressions containing different types of information in an end-to-end framework. In our model, which we call the Modular Attention Network (MAttNet), two types of attention are utilized: language-based attention that learns the module weights as well as the word/phrase attention that each module should focus on; and visual attention that allows the subject and relationship modules to focus on relevant image components. Module weights combine scores from all three modules dynamically to output an overall score. Experiments show that MAttNet outperforms previous state-of-art methods by a large margin on both bounding-box-level and pixel-level comprehension tasks. Demo and code are provided.",0
"This work presents a new architecture, called MAttNet (Modular Attention Network), which addresses challenges associated with referring expression comprehension tasks such as finding objects in images, referred to as Visual Genome benchmarks. Our approach uses modular attention to efficiently focus on relevant regions at different granularities, enabling it to achieve state-of-the-art performance while being significantly faster than previous methods. Experimental results demonstrate that our model can effectively solve these complex problems while providing significant computational gains compared to existing architectures. Furthermore, we show how adding external modules to the network increases performance even further. Finally, extensive ablation studies confirm the effectiveness of each component of our proposed methodology. Overall, MAttNet represents an important advance in deep learning approaches to referential reasoning. We hope that it will serve as a foundation for future research in this rapidly evolving field.",1
"We introduce a novel framework for image captioning that can produce natural language explicitly grounded in entities that object detectors find in the image. Our approach reconciles classical slot filling approaches (that are generally better grounded in images) with modern neural captioning approaches (that are generally more natural sounding and accurate). Our approach first generates a sentence `template' with slot locations explicitly tied to specific image regions. These slots are then filled in by visual concepts identified in the regions by object detectors. The entire architecture (sentence template generation and slot filling with object detectors) is end-to-end differentiable. We verify the effectiveness of our proposed model on different image captioning tasks. On standard image captioning and novel object captioning, our model reaches state-of-the-art on both COCO and Flickr30k datasets. We also demonstrate that our model has unique advantages when the train and test distributions of scene compositions -- and hence language priors of associated captions -- are different. Code has been made available at: https://github.com/jiasenlu/NeuralBabyTalk",0
"In recent years, there has been significant interest in developing artificial intelligence systems that can interact effectively with humans, particularly in domains such as customer service, healthcare, and education. One key challenge facing these efforts is designing interfaces that can communicate with users at their level of understanding without resorting to simplified language or patronizing tone. This study proposes a novel approach called ""Neural Baby Talk,"" which leverages advances in natural language processing and machine learning to generate conversational responses tailored to individual users based on their cognitive abilities and communication style. Our method utilizes deep neural networks trained on large corpora of text data to learn patterns associated with different levels of linguistic proficiency and comprehension. We demonstrate the effectiveness of our technique through experiments involving human subjects across varying age groups and ability levels, showing improved engagement, comprehension, and overall satisfaction compared to standard adult-directed conversation. Ultimately, we believe that our work represents an important step towards more personalized and inclusive forms of interaction between AI and human populations.",1
"Many vision and language tasks require commonsense reasoning beyond data-driven image and natural language processing. Here we adopt Visual Question Answering (VQA) as an example task, where a system is expected to answer a question in natural language about an image. Current state-of-the-art systems attempted to solve the task using deep neural architectures and achieved promising performance. However, the resulting systems are generally opaque and they struggle in understanding questions for which extra knowledge is required. In this paper, we present an explicit reasoning layer on top of a set of penultimate neural network based systems. The reasoning layer enables reasoning and answering questions where additional knowledge is required, and at the same time provides an interpretable interface to the end users. Specifically, the reasoning layer adopts a Probabilistic Soft Logic (PSL) based engine to reason over a basket of inputs: visual relations, the semantic parse of the question, and background ontological knowledge from word2vec and ConceptNet. Experimental analysis of the answers and the key evidential predicates generated on the VQA dataset validate our approach.",0
"Deep neural networks have been very successful at visual question answering (VQA), but they suffer from opaqueness since their internal representation cannot be directly interpreted by humans. Recently, several works attempt to incorporate explicit reasoning into deep learning models by either imposing external supervision such as rules and logical constraints or designing special architectures that allow gradient backpropagation through complex computations like symbolic operations. However, these methods usually trade off the performance of the model due to limited expressiveness compared to end-to-end trained models. In this work, we present a novel approach that explicitly reasons over intermediate representations generated by pretrained VQA models without changing the architecture itself, hence maintaining competitive performance. Our approach consists of introducing logic programming constructs to guide attention mechanism and refine answers via rule inference, making the inner decision flow more transparent for better interpretability. Extensive experiments on challenging benchmark datasets demonstrate significant improvements over baseline models as well as previous reasoning enhanced methods, validating the effectiveness of our framework. Additionally, ablation studies verify the contribution of each module, providing insights into how human-like reasoning can benefit VQA tasks. This research has potential applications in other natural language processing areas involving reasoning over knowledge graphs, and serves as a step towards interpretable artificial intelligence.",1
"We present a method for generating colored 3D shapes from natural language. To this end, we first learn joint embeddings of freeform text descriptions and colored 3D shapes. Our model combines and extends learning by association and metric learning approaches to learn implicit cross-modal connections, and produces a joint representation that captures the many-to-many relations between language and physical properties of 3D shapes such as color and shape. To evaluate our approach, we collect a large dataset of natural language descriptions for physical 3D objects in the ShapeNet dataset. With this learned joint embedding we demonstrate text-to-shape retrieval that outperforms baseline approaches. Using our embeddings with a novel conditional Wasserstein GAN framework, we generate colored 3D shapes from text. Our method is the first to connect natural language text with realistic 3D objects exhibiting rich variations in color, texture, and shape detail. See video at https://youtu.be/zraPvRdl13Q",0
"Abstract: In recent years, there has been significant progress on using natural language instructions to generate images [8], animations [4], shapes [9] and more recently videos [7]. However, generating complex 2D shapes remains challenging, as existing methods can only generate simple strokes or lines without any coherence or structure. We present a novel method called Text2Shape that generates detailed and structured shapes from natural language descriptions alone. Our model learns joint embeddings of textual descriptions and geometric representations, which enables it to predict valid shape features given a description, even if those features weren't seen during training. Moreover, our method utilizes a hybrid approach combining both image generation techniques like DALL-E and generative models based on transformers like Stable Diffusion [6]. With this combination we show superior results over prior state of the art methods [1][2][5]. Additionally, we introduce several new evaluation metrics to properly measure how well our generated shapes match the desired output. These evaluations show that our method outperforms all prior works across multiple benchmark datasets. Overall, Text2Shape provides the foundation for future research into NLP driven computer graphics, allowing for greater interactivity and control for creators. To access full papers please visit *www.arxiv.org* ​https://arxiv.org/abs/quantumphysics"">**<https://arxiv.org/abs/>www.arxiv.org</a> </abstract>",1
"A group of transition probability functions form a Shannon's channel whereas a group of truth functions form a semantic channel. By the third kind of Bayes' theorem, we can directly convert a Shannon's channel into an optimized semantic channel. When a sample is not big enough, we can use a truth function with parameters to produce the likelihood function, then train the truth function by the conditional sampling distribution. The third kind of Bayes' theorem is proved. A semantic information theory is simply introduced. The semantic information measure reflects Popper's hypothesis-testing thought. The Semantic Information Method (SIM) adheres to maximum semantic information criterion which is compatible with maximum likelihood criterion and Regularized Least Squares criterion. It supports Wittgenstein's view: the meaning of a word lies in its use. Letting the two channels mutually match, we obtain the Channels' Matching (CM) algorithm for machine learning. The CM algorithm is used to explain the evolution of the semantic meaning of natural language, such as ""Old age"". The semantic channel for medical tests and the confirmation measures of test-positive and test-negative are discussed. The applications of the CM algorithm to semi-supervised learning and non-supervised learning are simply introduced. As a predictive model, the semantic channel fits variable sources and hence can overcome class-imbalance problem. The SIM strictly distinguishes statistical probability and logical probability and uses both at the same time. This method is compatible with the thoughts of Bayes, Fisher, Shannon, Zadeh, Tarski, Davidson, Wittgenstein, and Popper.It is a competitive alternative to Bayesian inference.",0
"Title: ""From Shannon's Channel to Semantic Channel via New Bayes' Formula"" Abstract For decades, researchers have been trying to develop methods for transmitting messages over channels that suffer from noise, interference, and other forms of corruption. In recent years, machine learning has emerged as a powerful tool for solving these problems by allowing systems to automatically learn how to encode and decode signals in ways that minimize errors. However, traditional approaches based on maximum likelihood estimation (MLE) can be difficult to implement and may not always produce optimal results. To address these limitations, we propose using new formulas based on the theory of Bayesian statistics. Our approach involves modeling the channel as a probabilistic graphical model, where each possible message corresponds to a different configuration of random variables. By applying Bayes' rule, we can calculate probabilities associated with each message given the observed signal, which allows us to find the most probable message with high confidence. We show through extensive simulations that our method outperforms MLE-based techniques in terms of error rate and robustness to changing channel conditions. Furthermore, by incorporating semantic information into the message space, our system can capture meaningful relationships among concepts and enable more efficient communication. Overall, our work demonstrates the promise of Bayesian approaches for tackling challenges in signal processing and communications, opening up exciting possibilities for future research in machine learning and artificial intelligence. Keywords: channel coding, Bayesian inference, machine learning, error correction, probability theory, Markov Chain Monte Carlo (MCMC), variational inference, natural language understanding, semantic similarity.",1
"This paper strives for pixel-level segmentation of actors and their actions in video content. Different from existing works, which all learn to segment from a fixed vocabulary of actor and action pairs, we infer the segmentation from a natural language input sentence. This allows to distinguish between fine-grained actors in the same super-category, identify actor and action instances, and segment pairs that are outside of the actor and action vocabulary. We propose a fully-convolutional model for pixel-level actor and action segmentation using an encoder-decoder architecture optimized for video. To show the potential of actor and action video segmentation from a sentence, we extend two popular actor and action datasets with more than 7,500 natural language descriptions. Experiments demonstrate the quality of the sentence-guided segmentations, the generalization ability of our model, and its advantage for traditional actor and action segmentation compared to the state-of-the-art.",0
"An important task in video analysis is segmenting actors and actions from raw video footage. This can be challenging due to occlusions, similar appearances among actors, varying camera perspectives, and other factors that make accurate identification difficult. In our approach, we use convolutional neural networks (CNNs) to perform actor and action video segmentation directly from textual descriptions of videos. Our model takes as input a sentence describing the content of a video frame and outputs pixel-level masks corresponding to actors and their associated actions within the frame. We propose several novel techniques, including adversarial training and attention mechanisms, which significantly improve performance compared to prior approaches. Experimental results demonstrate the effectiveness of our method across multiple datasets and settings, making it a promising tool for real-world applications such as surveillance and entertainment systems.",1
"Research in human action recognition has accelerated significantly since the introduction of powerful machine learning tools such as Convolutional Neural Networks (CNNs). However, effective and efficient methods for incorporation of temporal information into CNNs are still being actively explored in the recent literature. Motivated by the popular recurrent attention models in the research area of natural language processing, we propose the Attention-based Temporal Weighted CNN (ATW), which embeds a visual attention model into a temporal weighted multi-stream CNN. This attention model is simply implemented as temporal weighting yet it effectively boosts the recognition performance of video representations. Besides, each stream in the proposed ATW framework is capable of end-to-end training, with both network parameters and temporal weights optimized by stochastic gradient descent (SGD) with backpropagation. Our experiments show that the proposed attention mechanism contributes substantially to the performance gains with the more discriminative snippets by focusing on more relevant video segments.",0
"This paper presents an attention-based temporal weighted convolutional neural network (ATWCNN) architecture that significantly improves action recognition accuracy by focusing on key moments within video data. Our ATWCNN model takes into account the importance of different frames as well as their order relative to one another, allowing for more precise spatial-temporal feature learning than previous methods. Additionally, we introduce the concept of ""attentional pooling"" which enables the network to effectively summarize the most informative regions from the input video sequence during training and inference. By incorporating these advanced techniques, our model achieves state-of-the-art performance across multiple benchmark datasets.",1
"We address the problem of jointly learning vision and language to understand the object in a fine-grained manner. The key idea of our approach is the use of object descriptions to provide the detailed understanding of an object. Based on this idea, we propose two new architectures to solve two related problems: object captioning and natural language-based object retrieval. The goal of the object captioning task is to simultaneously detect the object and generate its associated description, while in the object retrieval task, the goal is to localize an object given an input query. We demonstrate that both problems can be solved effectively using hybrid end-to-end CNN-LSTM networks. The experimental results on our new challenging dataset show that our methods outperform recent methods by a fair margin, while providing a detailed understanding of the object and having fast inference time. The source code will be made available.",0
"Recent years have seen significant advances in artificial intelligence (AI), including natural language processing techniques such as machine learning algorithms that learn representations from raw text. In recent work, we describe the application of these methods to computer vision tasks such as image retrieval and description generation based on natural language queries. We present experiments demonstrating improved accuracy over state-of-the-art models using novel object detectors derived from pretrained classifiers with fine-grained feature analysis, allowing us to retrieve relevant images more accurately by identifying objects within them. Our approach relies on jointly learning visual features and attention mechanisms across multiple levels of abstraction, producing diverse yet accurate descriptions of complex scenes at high resolution. We believe our results set a new benchmark for object captioning and retrieval and hold promise for further applications in areas such as robotics and human-computer interaction.",1
"Given a natural language query, a phrase grounding system aims to localize mentioned objects in an image. In weakly supervised scenario, mapping between image regions (i.e., proposals) and language is not available in the training set. Previous methods address this deficiency by training a grounding system via learning to reconstruct language information contained in input queries from predicted proposals. However, the optimization is solely guided by the reconstruction loss from the language modality, and ignores rich visual information contained in proposals and useful cues from external knowledge. In this paper, we explore the consistency contained in both visual and language modalities, and leverage complementary external knowledge to facilitate weakly supervised grounding. We propose a novel Knowledge Aided Consistency Network (KAC Net) which is optimized by reconstructing input query and proposal's information. To leverage complementary knowledge contained in the visual features, we introduce a Knowledge Based Pooling (KBP) gate to focus on query-related proposals. Experiments show that KAC Net provides a significant improvement on two popular datasets.",0
"Here's an example of how you could structure your abstract:  Recent advances in natural language processing have shown that incorporating external knowledge sources can significantly improve task performance on weakly supervised phrase grounding. However, existing methods often rely on heuristics or predefined rules to select relevant entities from the knowledge base, which can lead to inconsistent and suboptimal results. In our work, we propose a novel approach called ""Knowledge Aided Consistency"" (KAC), which leverages both world knowledge and semantic constraints to ensure consistent entity selection across multiple instances of the same query. Our experiments on three benchmark datasets show significant improvements over state-of-the-art baselines, demonstrating the effectiveness of KAC in enhancing consistency and accuracy in weakly supervised phrase grounding tasks.",1
"The subject of deep learning has recently attracted users of machine learning from various disciplines, including: medical diagnosis and bioinformatics, financial market analysis and online advertisement, speech and handwriting recognition, computer vision and natural language processing, time series forecasting, and search engines. However, theoretical development of deep learning is still at its infancy. The objective of this paper is to introduce a deep neural network (also called deep-net) approach to localized manifold learning, with each hidden layer endowed with a specific learning task. For the purpose of illustrations, we only focus on deep-nets with three hidden layers, with the first layer for dimensionality reduction, the second layer for bias reduction, and the third layer for variance reduction. A feedback component also designed to eliminate outliers. The main theoretical result in this paper is the order $\mathcal O\left(m^{-2s/(2s+d)}\right)$ of approximation of the regression function with regularity $s$, in terms of the number $m$ of sample points, where the (unknown) manifold dimension $d$ replaces the dimension $D$ of the sampling (Euclidean) space for shallow nets.",0
"This paper presents a detailed overview of methods for constructing artificial intelligence systems using local deep learning techniques. Specifically, we explore several approaches for building neural network architectures that can perform complex tasks without requiring access to large amounts of data from distant sources. By focusing on localized training processes, our models are able to learn from limited datasets while still achieving high levels of accuracy. We discuss both supervised and unsupervised learning frameworks, highlighting their strengths and limitations. Furthermore, we provide concrete examples of how these models have been applied to real-world applications, including image recognition, natural language processing, and control systems. Our findings indicate that localized deep learning represents a promising direction for future research in artificial intelligence, enabling more efficient use of resources and increased autonomy for edge devices. Overall, this work serves as a comprehensive guide for researchers and practitioners looking to leverage AI in constrained environments.",1
"Words in natural language follow a Zipfian distribution whereby some words are frequent but most are rare. Learning representations for words in the ""long tail"" of this distribution requires enormous amounts of data. Representations of rare words trained directly on end tasks are usually poor, requiring us to pre-train embeddings on external data, or treat all rare words as out-of-vocabulary words with a unique representation. We provide a method for predicting embeddings of rare words on the fly from small amounts of auxiliary data with a network trained end-to-end for the downstream task. We show that this improves results against baselines where embeddings are trained on the end task for reading comprehension, recognizing textual entailment and language modeling.",0
"In recent years, pre-trained deep learning models have become increasingly popular for natural language processing tasks. One such model that has gained widespread adoption is GPT-2 (Generative Pre-training Transformer 2) which provides state-of-the-art results on several benchmarks while requiring minimal fine-tuning data compared to previous methods. An important part of GPT-2's success lies in its ability to efficiently compute continuous representations of words known as word embeddings. These word embeddings capture semantic relationships between words and improve downstream task performance by providing a richer contextual understanding of each token.  However, computing these word embeddings requires significant computational resources during both training and inference, especially for very large datasets like bookCorpus (over 8 billion tokens). This makes deploying GPT-2 models at scale challenging, particularly for organizations that may not possess sufficient GPU hardware infrastructure.  This paper proposes a novel method called ""Online Word Encoding"" (OWE), designed to drastically reduce the time required to generate high quality word embeddings for specific NLP tasks. OWE leverages existing pre-trained embeddings (either BERT or GPT-2) and adaptively updates their weights by minimizing the cosine similarity between relevant pairs of text inputs. Experimental evaluations demonstrate that our approach achieves comparable or better performance than strong baselines across five distinct NLP applications, including sentiment analysis, question answering, sequence labeling, syntax parsing, and machine translation. Additionally, we provide empirical evidence showing that OWE can substantially accelerate the generation of effective word embeddings even without prior access to expensive hardware. Our findings indicate that OWE effectively decouples the cost and complexity of obtaining top-notch word embeddings from deployment scalability concerns, paving the way towards wider adoption of advanced NLP solutions.",1
"Classes in natural images tend to follow long tail distributions. This is problematic when there are insufficient training examples for rare classes. This effect is emphasized in compound classes, involving the conjunction of several concepts, such as those appearing in action-recognition datasets. In this paper, we propose to address this issue by learning how to utilize common visual concepts which are readily available. We detect the presence of prominent concepts in images and use them to infer the target labels instead of using visual features directly, combining tools from vision and natural-language processing. We validate our method on the recently introduced HICO dataset reaching a mAP of 31.54\% and on the Stanford-40 Actions dataset, where the proposed method outperforms that obtained by direct visual features, obtaining an accuracy 83.12\%. Moreover, the method provides for each class a semantically meaningful list of keywords and relevant image regions relating it to its constituent concepts.",0
"In recent years, there has been a growing interest in developing computational models that can accurately classify actions in videos and images. This task, known as action classification, is challenging due to the large variability in human behaviors and the complex nature of the visual data. Many approaches have been proposed to tackle this problem, but they often rely on handcrafted features or suffer from limited generalization capabilities. To address these issues, we propose a novel framework based on concepts and attributes learning. Our method learns meaningful representations of action classes by explicitly modeling high-level semantic concepts (such as objects involved) and low-level visual cues (such as color histograms). These learned concepts are then used to define discriminative action-specific attribute templates which capture distinctive characteristics across different instances and variations. We evaluate our approach on three popular benchmark datasets and demonstrate state-of-the art results while achieving better interpretability and more flexible zero-shot generalization capabilities. Overall, our work advances the current understanding of concept learning and representation design for real-world applications in computer vision and multimedia analysis.",1
"The explosion in workload complexity and the recent slow-down in Moore's law scaling call for new approaches towards efficient computing. Researchers are now beginning to use recent advances in machine learning in software optimizations, augmenting or replacing traditional heuristics and data structures. However, the space of machine learning for computer hardware architecture is only lightly explored. In this paper, we demonstrate the potential of deep learning to address the von Neumann bottleneck of memory performance. We focus on the critical problem of learning memory access patterns, with the goal of constructing accurate and efficient memory prefetchers. We relate contemporary prefetching strategies to n-gram models in natural language processing, and show how recurrent neural networks can serve as a drop-in replacement. On a suite of challenging benchmark datasets, we find that neural networks consistently demonstrate superior performance in terms of precision and recall. This work represents the first step towards practical neural-network based prefetching, and opens a wide range of exciting directions for machine learning in computer architecture research.",0
"Memory access patterns can reveal valuable insights into how programmers work with data structures, such as which fields are frequently accessed together or which operations are most common. This knowledge can inform future design decisions by helping developers better anticipate usage scenarios and optimize for performance. In this paper we present a technique that automates the process of learning memory access patterns during system runtime without requiring any changes to application code or instrumentation. Our method captures interleaved traces containing both user instructions and corresponding cache events that occur on the same core. We use these traces to construct accurate profiles indicating how individual loads lead to subsequent unpredictable branches based solely on their memory address operands. These profiles guide prefetching strategies that exploit locality, demonstrating significant improvements over traditional approaches across several benchmark suites and realworld applications.",1
"Artificial neural networks have recently shown great results in many disciplines and a variety of applications, including natural language understanding, speech processing, games and image data generation. One particular application in which the strong performance of artificial neural networks was demonstrated is the recognition of objects in images, where deep convolutional neural networks are commonly applied. In this survey, we give a comprehensive introduction to this topic (object recognition with deep convolutional neural networks), with a strong focus on the evolution of network architectures. Therefore, we aim to compress the most important concepts in this field in a simple and non-technical manner to allow for future researchers to have a quick general understanding.   This work is structured as follows:   1. We will explain the basic ideas of (convolutional) neural networks and deep learning and examine their usage for three object recognition tasks: image classification, object localization and object detection.   2. We give a review on the evolution of deep convolutional neural networks by providing an extensive overview of the most important network architectures presented in chronological order of their appearances.",0
"This paper provides a non-technical survey on deep convolutional neural network architectures (CNNs) by examining their most common applications across industries such as healthcare, finance, retail, manufacturing, logistics, security and entertainment. The study starts with introducing CNNs and providing an overview of popular open source libraries that can be used for model development, training, validation, prediction and visualization purposes, which makes the technology more accessible to practitioners. Next, the authors discuss a range of current commercial and research applications where these models have provided meaningful benefits and business results. Finally, a summary of future trends in the field concludes the survey, focusing primarily on recent advancements in Generative Adversarial Networks (GANs), Transfer Learning (TL) and AutoML frameworks, which aim at streamlining the design, training and deployment of CNN systems through automation techniques and algorithmic innovations. Overall, this study serves as a comprehensive introduction into applied artificial intelligence and machine learning using deep neural networks, specifically targeted at managers, professionals, entrepreneurs and students from technical backgrounds who wish to benefit from modern data driven technologies without necessarily getting hands dirty coding or tinkering with infrastructure tools.",1
"The best summary of a long video differs among different people due to its highly subjective nature. Even for the same person, the best summary may change with time or mood. In this paper, we introduce the task of generating customized video summaries through simple text. First, we train a deep architecture to effectively learn semantic embeddings of video frames by leveraging the abundance of image-caption data via a progressive and residual manner. Given a user-specific text description, our algorithm is able to select semantically relevant video segments and produce a temporally aligned video summary. In order to evaluate our textually customized video summaries, we conduct experimental comparison with baseline methods that utilize ground-truth information. Despite the challenging baselines, our method still manages to show comparable or even exceeding performance. We also show that our method is able to generate semantically diverse video summaries by only utilizing the learned visual embeddings.",0
"This paper proposes a novel approach to generating summaries of videos based on natural language input from users. Our method extracts important elements such as actors, objects, actions, and scenes from each frame in the video using computer vision techniques, then uses these elements along with user requests to generate personalized summaries tailored to individual preferences and needs. To achieve this, we utilize advanced machine learning algorithms that can learn and adapt to different contexts and nuances in human language queries. In addition, our system offers fine-grained control over summary generation parameters, allowing users to specify preferred content selection strategies and levels of abstraction. We evaluate our method against several existing approaches for video summarization and demonstrate improved performance in terms of accuracy and efficiency. Our work has wide-ranging applications in areas such as entertainment, education, security, and healthcare, enabling new forms of interactive media consumption and knowledge dissemination.",1
"In this paper we introduce a novel method of gradient normalization and decay with respect to depth. Our method leverages the simple concept of normalizing all gradients in a deep neural network, and then decaying said gradients with respect to their depth in the network. Our proposed normalization and decay techniques can be used in conjunction with most current state of the art optimizers and are a very simple addition to any network. This method, although simple, showed improvements in convergence time on state of the art networks such as DenseNet and ResNet on image classification tasks, as well as on an LSTM for natural language processing tasks.",0
"Abstract: This paper presents two novel techniques to improve training stability and generalizability for deep learning models: gradient normalization and depth based decay. These methods aim to address issues related to vanishing/exploding gradients during backpropagation and overfitting due to high capacity models by carefully controlling the magnitude of model updates and introducing additional regularization. We first introduce gradient normalization, which scales gradients by the running product of their magnitudes along the backward path through the network. This ensures that each layer receives gradients with a consistent magnitude, regardless of depth, resulting in more stable convergence and improved performance on CIFAR and ImageNet datasets. Secondly, we present depth based decay, which exponentially decays the impact of higher layers' contributions to the loss function as they become further away from the input data. This reduces reliance on unreliable higher-level features and encourages the use of lower-level representations for better generalization. Our experiments show that these methods significantly boost accuracy while reducing computational requirements compared to other state-of-the-art regularizers. Overall, our work advances understanding of deep neural networks and provides new tools for improving the stability and robustness of such models.",1
"Due to recent technical and scientific advances, we have a wealth of information hidden in unstructured text data such as offline/online narratives, research articles, and clinical reports. To mine these data properly, attributable to their innate ambiguity, a Word Sense Disambiguation (WSD) algorithm can avoid numbers of difficulties in Natural Language Processing (NLP) pipeline. However, considering a large number of ambiguous words in one language or technical domain, we may encounter limiting constraints for proper deployment of existing WSD models. This paper attempts to address the problem of one-classifier-per-one-word WSD algorithms by proposing a single Bidirectional Long Short-Term Memory (BLSTM) network which by considering senses and context sequences works on all ambiguous words collectively. Evaluated on SensEval-3 benchmark, we show the result of our model is comparable with top-performing WSD algorithms. We also discuss how applying additional modifications alleviates the model fault and the need for more training data.",0
"Title: ""Deep bidirectional LSTM networks for text data word sense disambi...",1
"We propose an interactive image-manipulation system with natural language instruction, which can generate a target image from a source image and an instruction that describes the difference between the source and the target image. The system makes it possible to modify a generated image interactively and make natural language conditioned image generation more controllable. We construct a neural network that handles image vectors in latent space to transform the source vector to the target vector by using the vector of instruction. The experimental results indicate that the proposed framework successfully generates the target image by using a source image and an instruction on manipulation in our dataset.",0
"Here's your output: ----- The natural language processing community has seen great advancements in recent years due to the availability of large datasets and advanced deep learning algorithms that can process them. One application area where these advancements have been particularly beneficial is image manipulation using natural language instruction commands. This work focuses on developing an interactive system capable of generating new images from scratch based solely on user text inputs, which could enable users who lack technical expertise to create highly realistic renders without requiring specialized software or tools. Our approach employs state-of-the-art generative models such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), coupled with attention mechanisms trained specifically for NLP tasks to allow the model to selectively attend to relevant parts of the input command. We demonstrate our method by generating high-quality images corresponding to diverse textual descriptions across several domains such as landscapes, portraits, still life scenes, and others. Extensive experiments conducted on benchmark datasets showcase the effectiveness of our framework in terms of both visual quality and semantic coherence compared against strong baselines. In summary, we believe that our contributions will serve as valuable stepping stones towards enabling truly intelligent conversational systems capable of interacting seamlessly within complex problem spaces involving multimedia data.",1
"Generative adversarial networks (GANs) are a learning framework that rely on training a discriminator to estimate a measure of difference between a target and generated distributions. GANs, as normally formulated, rely on the generated samples being completely differentiable w.r.t. the generative parameters, and thus do not work for discrete data. We introduce a method for training GANs with discrete data that uses the estimated difference measure from the discriminator to compute importance weights for generated samples, thus providing a policy gradient for training the generator. The importance weights have a strong connection to the decision boundary of the discriminator, and we call our method boundary-seeking GANs (BGANs). We demonstrate the effectiveness of the proposed algorithm with discrete image and character-based natural language generation. In addition, the boundary-seeking objective extends to continuous data, which can be used to improve stability of training, and we demonstrate this on Celeba, Large-scale Scene Understanding (LSUN) bedrooms, and Imagenet without conditioning.",0
"Recent advances in machine learning have led to significant improvements in image generation using generative adversarial networks (GANs). However, current GAN architectures suffer from several limitations such as mode collapse, instability, and lack of controllability. In this work, we propose boundary-seeking GANs (BSGAN), which address these issues by incorporating novel constraints that guide the training process towards more meaningful and interpretable results.  Our approach involves formulating a constrained optimization problem where two neural networks compete against each other: a generator network that synthesizes images, and a discriminator network that distinguishes real data from generated ones. We introduce three types of constraints - statistical regularization, latent space alignment, and classification consistency - to encourage both networks to explore different regions of their respective spaces while minimizing their competitive objectives.  We evaluate our method on multiple datasets including ImageNet, CelebA-HQ, and LSUN, comparing it with state-of-the-art GAN models. Our experimental results demonstrate that BSGAN outperforms prior art across diverse evaluation metrics measuring visual fidelity, diversity, coherence, and interpretability.  Furthermore, we perform extensive ablation studies investigating the effectiveness of individual constraints and their combinations. These analyses reveal insights into how each constraint affects the behavior of GANs during training, shaping the distribution of synthesized samples and improving their quality.  Overall, our contributions focus on designing new GAN variants with improved performance through guided exploration of the solution landscape, enabling applications in computer vision, graphics, and beyond. While previous works primarily focused on developing increasingly powerful generators to push forward the boundaries of generative modeling, future research should investigate methods allowing fine control over generated outputs. Such capabilities might enable",1
"Embedding graph nodes into a vector space can allow the use of machine learning to e.g. predict node classes, but the study of node embedding algorithms is immature compared to the natural language processing field because of a diverse nature of graphs. We examine the performance of node embedding algorithms with respect to graph centrality measures that characterize diverse graphs, through systematic experiments with four node embedding algorithms, four or five graph centralities, and six datasets. Experimental results give insights into the properties of node embedding algorithms, which can be a basis for further research on this topic.",0
"Recent work has focused on evaluating node embedding algorithms using different metrics such as accuracy, precision, recall, F1 score, mean squared error (MSE), etc., but little attention has been given to understanding how these embeddings capture structural properties of graphs. In particular, centrality measures have received less attention in relation to graph embeddings, despite their importance in characterizing nodes within networks. Here we analyze node embeddings from popular unsupervised machine learning algorithms GAE, VGAE, Graph U-Net, Deepwalk,Node2Vec and LINE under the lens of multiple centrality metrics like degree centrality, closeness centrality, betweenness centrality, eigenvector centrality, PageRank centrality etc. We further evaluate the effectiveness of these embeddings by training several supervised classification models and report their performance against state of art benchmarks for a number of datasets. Our experiments show that while some embedders tend to outperform others overall across all datasets and quality metrics, no single algorithm uniformly performs better than the rest of the ensemble for any one metric of interest which could pose challenges for practitioners seeking optimal results without careful consideration of trade offs and dataset characteristics.",1
"We consider the problem of accurately recovering a matrix B of size M by M , which represents a probability distribution over M2 outcomes, given access to an observed matrix of ""counts"" generated by taking independent samples from the distribution B. How can structural properties of the underlying matrix B be leveraged to yield computationally efficient and information theoretically optimal reconstruction algorithms? When can accurate reconstruction be accomplished in the sparse data regime? This basic problem lies at the core of a number of questions that are currently being considered by different communities, including building recommendation systems and collaborative filtering in the sparse data regime, community detection in sparse random graphs, learning structured models such as topic models or hidden Markov models, and the efforts from the natural language processing community to compute ""word embeddings"".   Our results apply to the setting where B has a low rank structure. For this setting, we propose an efficient algorithm that accurately recovers the underlying M by M matrix using Theta(M) samples. This result easily translates to Theta(M) sample algorithms for learning topic models and learning hidden Markov Models. These linear sample complexities are optimal, up to constant factors, in an extremely strong sense: even testing basic properties of the underlying matrix (such as whether it has rank 1 or 2) requires Omega(M) samples. We provide an even stronger lower bound where distinguishing whether a sequence of observations were drawn from the uniform distribution over M observations versus being generated by an HMM with two hidden states requires Omega(M) observations. This precludes sublinear-sample hypothesis tests for basic properties, such as identity or uniformity, as well as sublinear sample estimators for quantities such as the entropy rate of HMMs.",0
"In many applications of probability theory, there exists uncertainty about the true values that certain variables may take on. To model these situations, we can use probabilistic models such as Bayesian networks or Hidden Markov models which assign probabilities to different variable states. These models typically have the form of directed graphs where each vertex corresponds to a random variable and edges represent conditional dependencies between them. The probabilities associated with these variables are represented by matrices known as structured probability matrices (SPMs).  However, recovering SPMs from data is often challenging since the data alone cannot uniquely determine all entries in these matrices, particularly if they contain unobserved variables. This has motivated researchers to develop methods that incorporate additional assumptions to aid in their recovery.  This paper presents a novel approach for recovering SPMs using matrix factorization techniques. Our method, called Matrix Factorization for Structure Discovery and Estimation (MaFUSE), leverages the low rank property of SPMs and factorizes the matrix into two low-rank components: one representing the underlying structure and the other capturing the variability patterns in the data. MaFUSE then uses an alternating optimization framework to iteratively estimate the structure and the latent features until convergence.  Experiments conducted on both synthetic and real datasets show that our method significantly outperforms state-of-the-art alternatives, achieving high accuracy in estimating both the structure and the missing probabilities. Furthermore, MaFUSE provides interpretable results allowing us to gain insights into the relationships among variables modeled in the graphical representation. Overall, our work advances the current understanding of how to efficiently recover SPMs, providing valuable tools for practitioners working with probabilistic models across domains.",1
"Deep residual networks (ResNets) and their variants are widely used in many computer vision applications and natural language processing tasks. However, the theoretical principles for designing and training ResNets are still not fully understood. Recently, several points of view have emerged to try to interpret ResNet theoretically, such as unraveled view, unrolled iterative estimation and dynamical systems view. In this paper, we adopt the dynamical systems point of view, and analyze the lesioning properties of ResNet both theoretically and experimentally. Based on these analyses, we additionally propose a novel method for accelerating ResNet training. We apply the proposed method to train ResNets and Wide ResNets for three image classification benchmarks, reducing training time by more than 40% with superior or on-par accuracy.",0
"Multi-level residual networks have recently emerged as a powerful tool for processing high-dimensional data and achieving state-of-the-art performance on a wide range of tasks. In this work, we present a new framework that analyzes these networks from a dynamical systems perspective, allowing us to gain deeper insights into their behavior and optimize their design.  Our approach draws connections between multi-level residual networks and existing theories of nonlinear dynamics and chaos theory, enabling us to analyze their complex interactions across multiple levels of abstraction. We demonstrate how this framework can improve our understanding of these networks’ ability to capture hierarchical structure in data, enhance robustness against noise and perturbations, and facilitate more effective training and optimization.  We validate our findings through experiments on several benchmark datasets, including image classification, natural language processing, and time series forecasting. Our results show consistent improvements over baseline models, providing evidence of the effectiveness and potential impact of our proposed methodology.  In summary, this work represents a significant contribution to the field of deep learning by introducing a novel, dynamically informed perspective on multi-level residual networks. By integrating principles from nonlinear dynamics and chaos theory with cutting-edge machine learning techniques, we provide researchers and practitioners with valuable insights and tools for advancing the development of advanced artificial intelligence systems.",1
"Automatic image captioning has recently approached human-level performance due to the latest advances in computer vision and natural language understanding. However, most of the current models can only generate plain factual descriptions about the content of a given image. However, for human beings, image caption writing is quite flexible and diverse, where additional language dimensions, such as emotion, humor and language styles, are often incorporated to produce diverse, emotional, or appealing captions. In particular, we are interested in generating sentiment-conveying image descriptions, which has received little attention. The main challenge is how to effectively inject sentiments into the generated captions without altering the semantic matching between the visual content and the generated descriptions. In this work, we propose two different models, which employ different schemes for injecting sentiments into image captions. Compared with the few existing approaches, the proposed models are much simpler and yet more effective. The experimental results show that our model outperform the state-of-the-art models in generating sentimental (i.e., sentiment-bearing) image captions. In addition, we can also easily manipulate the model by assigning different sentiments to the testing image to generate captions with the corresponding sentiments.",0
"This paper presents a new image caption generation method that injects various types of sentiments into generated descriptions. To achieve this, we propose a versatile scheme called ImageCaptioningAtWill (ICAW). First, ICAW predicts sentiment scores from given images by applying both pretrained deep models and carefully designed feature extractors. Second, based on inputted sentiment labels, our model modifies initial outputs through a novel gate mechanism to generate emotion-rich captions guided by specific emotional preferences. We evaluate ICAW using standard metrics and demonstrate that it significantly outperforms several state-of-the-art methods across multiple datasets while generating diverse and expressive descriptions with desired sentiments. Our work pushes forward the field of automatic image captioning towards more creative applications involving human emotions and beyond merely factual content understanding.",1
"Visual Question Answering (VQA) is a novel problem domain where multi-modal inputs must be processed in order to solve the task given in the form of a natural language. As the solutions inherently require to combine visual and natural language processing with abstract reasoning, the problem is considered as AI-complete. Recent advances indicate that using high-level, abstract facts extracted from the inputs might facilitate reasoning. Following that direction we decided to develop a solution combining state-of-the-art object detection and reasoning modules. The results, achieved on the well-balanced CLEVR dataset, confirm the promises and show significant, few percent improvements of accuracy on the complex ""counting"" task.",0
"Visual Question Answering (VQA) is a challenging task that requires understanding of both images and natural language questions. Traditional approaches have relied on feature extraction and machine learning models such as convolutional neural networks (CNNs), but these methods often struggle with capturing the contextual relationships between objects in an image and the question posed by the user. In our research, we propose an object-based reasoning approach for VQA that leverages computer vision techniques to identify individual objects within an image and use their properties to answer questions. Our method is based on three key components: 1) object detection using state-of-the-art algorithms; 2) construction of semantic representations using WordNet and Conceptnet data structures; and 3) graph-based inference over hypotheses represented as Bayesian networks. We evaluate our proposed model on several benchmark datasets for VQA and demonstrate improved performance compared to existing approaches, especially for questions involving multiple objects or requiring deeper levels of comprehension. This work represents an important step towards more human-like visual understanding and offers potential applications in areas such as education, customer service, and even social robots.",1
"Zero shot learning in Image Classification refers to the setting where images from some novel classes are absent in the training data but other information such as natural language descriptions or attribute vectors of the classes are available. This setting is important in the real world since one may not be able to obtain images of all the possible classes at training. While previous approaches have tried to model the relationship between the class attribute space and the image space via some kind of a transfer function in order to model the image space correspondingly to an unseen class, we take a different approach and try to generate the samples from the given attributes, using a conditional variational autoencoder, and use the generated samples for classification of the unseen classes. By extensive testing on four benchmark datasets, we show that our model outperforms the state of the art, particularly in the more realistic generalized setting, where the training classes can also appear at the test time along with the novel classes.",0
"In recent years, generative models have proven to be very effective at generating new data that are similar to existing data. This has enabled researchers to use these models in many different fields such as computer vision, natural language processing, and speech recognition. However, one limitation of most current generative models is their lack of ability to generalize well across tasks without large amounts of task specific training data. This can lead to poor performance on tasks where little labeled data is available. To address this issue, we propose using conditional variational autoencoders (CVAEs) in conjunction with a zero shot learning (ZSL) method to create a generative model capable of performing zero shot learning. Our approach involves first pretraining a CVAEs by fine tuning a VAE with additional conditions to learn more complex dependencies within each task. Next, we train our ZSL algorithm alongside the pretrained VAE model, which enables us to take advantage of both methods strengths. Our experiments show that our proposed approach outperforms other state-of-the-art baselines in both quantitative evaluations and qualitative analysis. Furthermore, due to the flexibility of CVAE architectures, our method can accommodate arbitrary condition types making it widely applicable to diverse applications. Overall, our work represents an important step towards achieving zero shot learning capabilities for novel tasks without requiring large quantities of task specific training data.",1
"Visual Question Answering (VQA) has attracted attention from both computer vision and natural language processing communities. Most existing approaches adopt the pipeline of representing an image via pre-trained CNNs, and then using the uninterpretable CNN features in conjunction with the question to predict the answer. Although such end-to-end models might report promising performance, they rarely provide any insight, apart from the answer, into the VQA process. In this work, we propose to break up the end-to-end VQA into two steps: explaining and reasoning, in an attempt towards a more explainable VQA by shedding light on the intermediate results between these two steps. To that end, we first extract attributes and generate descriptions as explanations for an image using pre-trained attribute detectors and image captioning models, respectively. Next, a reasoning module utilizes these explanations in place of the image to infer an answer to the question. The advantages of such a breakdown include: (1) the attributes and captions can reflect what the system extracts from the image, thus can provide some explanations for the predicted answer; (2) these intermediate results can help us identify the inabilities of both the image understanding part and the answer inference part when the predicted answer is wrong. We conduct extensive experiments on a popular VQA dataset and dissect all results according to several measurements of the explanation quality. Our system achieves comparable performance with the state-of-the-art, yet with added benefits of explainability and the inherent ability to further improve with higher quality explanations.",0
"This paper presents an approach towards explainable visual question answering that uses attributes and captions. We propose an architecture called ""Tell-and-Answer"", which is based on the idea of representing facts as attributes of objects in images, such as shape, color, size, texture etc., along with their corresponding descriptions - captions. Using these two components, our model is able to accurately predict answers for complex questions like 'why', 'how' ,etc. on image data. Our experiments show that our method outperforms existing state-of-the art models while providing a simple yet effective framework that can generate explanations from predicted results. In conclusion, our work shows promise towards building more transparent AI systems that provide clear reasoning behind decisions made by machine learning algorithms.",1
"This paper describes the WiLI-2018 benchmark dataset for monolingual written natural language identification. WiLI-2018 is a publicly available, free of charge dataset of short text extracts from Wikipedia. It contains 1000 paragraphs of 235 languages, totaling in 23500 paragraphs. WiLI is a classification dataset: Given an unknown paragraph written in one dominant language, it has to be decided which language it is.",0
"Title: A Dataset for Written Language Identification  Written language has many forms and manifestations, from online text to handwriting samples, all unique in their own ways. Accurately identifying the specific writing system used can have important applications in fields such as digital humanities, education, and cultural heritage preservation. To address the challenge of written language identification, we propose the WiLD (short for ""written language identification"") benchmark dataset. This article presents the details of the dataset including data collection procedures, quality control measures, and evaluation metrics. We also provide baseline results using state-of-the-art machine learning algorithms to demonstrate the potential utility of our approach. With over 8700 texts spanning nearly 240 languages and scripts, WiLDA offers a rich resource that promises new opportunities for researchers working on natural language processing problems related to written texts. In summary, our goal here is to encourage innovative solutions to this understudied problem. By making our datasets publicly available, we hope to stimulate greater interest and progress in understanding how computers can recognize different types of writing systems.",1
"Quantitative modeling of human brain activity based on language representations has been actively studied in systems neuroscience. However, previous studies examined word-level representation, and little is known about whether we could recover structured sentences from brain activity. This study attempts to generate natural language descriptions of semantic contents from human brain activity evoked by visual stimuli. To effectively use a small amount of available brain activity data, our proposed method employs a pre-trained image-captioning network model using a deep learning framework. To apply brain activity to the image-captioning network, we train regression models that learn the relationship between brain activity and deep-layer image features. The results demonstrate that the proposed model can decode brain activity and generate descriptions using natural language sentences. We also conducted several experiments with data from different subsets of brain regions known to process visual stimuli. The results suggest that semantic information for sentence generations is widespread across the entire cortex.",0
What is your question? I am unable to respond without some prompt.,1
"Correctly estimating the discrepancy between two data distributions has always been an important task in Machine Learning. Recently, Cuturi proposed the Sinkhorn distance which makes use of an approximate Optimal Transport cost between two distributions as a distance to describe distribution discrepancy. Although it has been successfully adopted in various machine learning applications (e.g. in Natural Language Processing and Computer Vision) since then, the Sinkhorn distance also suffers from two unnegligible limitations. The first one is that the Sinkhorn distance only gives an approximation of the real Wasserstein distance, the second one is the `divide by zero' problem which often occurs during matrix scaling when setting the entropy regularization coefficient to a small value. In this paper, we introduce a new Brenier approach for calculating a more accurate Wasserstein distance between two discrete distributions, this approach successfully avoids the two limitations shown above for Sinkhorn distance and gives an alternative way for estimating distribution discrepancy.",0
"In many real world scenarios, data is often collected in the form of discrete measures which cannot capture fine details in certain regions. Hence, these discrete measures need to be transformed into continuous measures that can accurately represent the underlying probability density function (PDF) over a region. This conversion from a quasi-discrete measure to a continuous measure has several applications including image processing, signal processing, machine learning etc. To address this problem, we consider two different metrics - Joint Entropy and KL divergence between two multivariate Gaussians and apply them on the given data set in order to evaluate their performance in estimating the PDF as well as reconstructing the original images. We found that both metrics performed equally well and could effectively estimate and reconstruct the PDF's and correspondingly, the original images. Our results demonstrate that using joint entropy alongwith Kullback Leibler divergence provides efficient models for solving problems related to Quasi-Discrete Measures.",1
"Automatically creating the description of an image using any natural languages sentence like English is a very challenging task. It requires expertise of both image processing as well as natural language processing. This paper discuss about different available models for image captioning task. We have also discussed about how the advancement in the task of object recognition and machine translation has greatly improved the performance of image captioning model in recent years. In addition to that we have discussed how this model can be implemented. In the end, we have also evaluated the performance of model using standard evaluation matrices.",0
"In recent years, deep neural architectures have proven to be highly effective at generating image descriptions, known as ""image captioning"". These models leverage both visual features and textual context to generate natural language explanations of images. This work presents an approach that outperforms previous methods by leveraging powerful convolutional neural networks (CNNs) for feature extraction from the input image, coupled with a novel recurrent neural network architecture designed specifically for generating natural language sentences. We evaluate our model on several benchmark datasets and demonstrate significant improvements over state-of-the-art systems. Our contributions include: 1) a new CNN-RNN architecture for image captioning; 2) empirical analysis demonstrating the effectiveness of combining both visual features and textual context for image description generation; and 3) extensive evaluation on multiple benchmark datasets. Our results suggest that deep neural architectures can effectively tackle complex image understanding tasks and offer promising directions for future research in computer vision and natural language processing.",1
"To perform tasks specified by natural language instructions, autonomous agents need to extract semantically meaningful representations of language and map it to visual elements and actions in the environment. This problem is called task-oriented language grounding. We propose an end-to-end trainable neural architecture for task-oriented language grounding in 3D environments which assumes no prior linguistic or perceptual knowledge and requires only raw pixels from the environment and the natural language instruction as input. The proposed model combines the image and text representations using a Gated-Attention mechanism and learns a policy to execute the natural language instruction using standard reinforcement and imitation learning methods. We show the effectiveness of the proposed model on unseen instructions as well as unseen maps, both quantitatively and qualitatively. We also introduce a novel environment based on a 3D game engine to simulate the challenges of task-oriented language grounding over a rich set of instructions and environment states.",0
"In this paper we explore how gated attention architectures can improve task oriented language grounding by focusing on specific parts of the input sentence and allowing our model to attend only to relevant tokens. We propose three variants based on standard attention mechanisms which apply different gates at each level: softmax weighting at each position; self attention and feedforward neural networks applied locally; and linear transformations followed by elementwise multiplication. Our models show consistent improvement over baseline transformer models on four tasks across two datasets, improving performance significantly on one dataset for one task. Aside from simple accuracy improvements these modifications allow us to gain insights into the behavior of attention during training, including that less than half of all positions attended are required for good results and that attending to distant context is far more common in practice than previously believed. These findings have important implications for architecture design as well as understanding how humans process language. While we focus primarily on the core grounding problem here, we believe the improved performance demonstrated along with the greater understanding of attention gained via these experiments has broad applicability to many downstream natural language processing problems where precision recall tradeoffs are critical.",1
"We develop a model using deep learning techniques and natural language processing on unstructured text from medical records to predict hospital-wide $30$-day unplanned readmission, with c-statistic $.70$. Our model is constructed to allow physicians to interpret the significant features for prediction.",0
"Certainly! Here's a possible abstract:  Many hospitals struggle with predicting which patients are at high risk for readmission after discharge. Traditional methods have relied on data such as demographic factors and medical history, but these can often miss important details that could affect a patient's health outcomes. In our study, we explore whether it is possible to use doctors' notes to identify patterns that could indicate heightened readmission risk.  We analyzed over a thousand patient records, looking specifically at the physician progress notes written during hospitalization. We used natural language processing techniques to extract key themes and topics mentioned by the physicians, and then examined how these might relate to readmissions within 30 days post-discharge. Our findings show that certain types of content in physician notes - particularly those related to mental health concerns, social determinants of health, and communication issues - were strong predictors of increased readmission risk. Overall, our results suggest that incorporating doctors' notes into readmission prediction models may improve their accuracy and provide valuable insights for targeted interventions to prevent unnecessary hospitalizations.",1
"With the exponential increase in the amount of digital information over the internet, online shops, online music, video and image libraries, search engines and recommendation system have become the most convenient ways to find relevant information within a short time. In the recent times, deep learning's advances have gained significant attention in the field of speech recognition, image processing and natural language processing. Meanwhile, several recent studies have shown the utility of deep learning in the area of recommendation systems and information retrieval as well. In this short review, we cover the recent advances made in the field of recommendation using various variants of deep learning technology. We organize the review in three parts: Collaborative system, Content based system and Hybrid system. The review also discusses the contribution of deep learning integrated recommendation systems into several application domains. The review concludes by discussion of the impact of deep learning in recommendation system in various domain and whether deep learning has shown any significant improvement over the conventional systems for recommendation. Finally, we also provide future directions of research which are possible based on the current state of use of deep learning in recommendation systems.",0
"This review provides an overview of recent developments in deep learning approaches used in recommendation systems. We summarize key advancements from both academia and industry, including popular architectures like Collaborative Denoising Autoencoders (CDA), Variational Autoencoders (VAE) and Generative Adversarial Networks (GAN). These models have achieved state-of-the-art results on benchmark datasets through their ability to capture latent relationships among items and users in large scale recommendation tasks, which has become increasingly critical as data grows larger and more complex. Furthermore, we discuss important insights into training these types of models, such as optimizing objectives that balance reconstruction error against latent variable uncertainty, alleviating issues of model collapse during optimization, and ensuring diversity in generated recommendations. Finally, we highlight open challenges and future directions related to evaluating and interpreting models that learn deep representations. By distilling current research trends, our work seeks to provide guidance for practitioners looking to implement cutting-edge techniques within real world applications while encouraging further progress in developing innovative solutions towards addressing problems associated with big data.",1
"We have witnessed rapid evolution of deep neural network architecture design in the past years. These latest progresses greatly facilitate the developments in various areas such as computer vision and natural language processing. However, along with the extraordinary performance, these state-of-the-art models also bring in expensive computational cost. Directly deploying these models into applications with real-time requirement is still infeasible. Recently, Hinton etal. have shown that the dark knowledge within a powerful teacher model can significantly help the training of a smaller and faster student network. These knowledge are vastly beneficial to improve the generalization ability of the student model. Inspired by their work, we introduce a new type of knowledge -- cross sample similarities for model compression and acceleration. This knowledge can be naturally derived from deep metric learning model. To transfer them, we bring the ""learning to rank"" technique into deep metric learning formulation. We test our proposed DarkRank method on various metric learning tasks including pedestrian re-identification, image retrieval and image clustering. The results are quite encouraging. Our method can improve over the baseline method by a large margin. Moreover, it is fully compatible with other existing methods. When combined, the performance can be further boosted.",0
"This work presents a new method for accelerating deep metric learning through cross sample similarities transfer. We show that by leveraging existing knowledge from other samples, we can improve accuracy and efficiency of training models such as face verification networks. Our approach transfers the similarity relationships learned among previously seen classes to novel unseen ones which have only few labeled data. By minimizing the distance between intra-class features and maximizing inter-class separations, our proposed method achieves superior performance compared to baselines on standard benchmark datasets such as CelebA, LFW, and IJB-C. In addition, we present thorough ablation studies demonstrating the importance of each component of our method. Overall, our results indicate great promise for applying our approach to a wide range of applications beyond face verification tasks where accurate pairwise comparisons are important.",1
"Generating novel pairs of image and text is a problem that combines computer vision and natural language processing. In this paper, we present strategies for generating novel image and caption pairs based on existing captioning datasets. The model takes advantage of recent advances in generative adversarial networks and sequence-to-sequence modeling. We make generalizations to generate paired samples from multiple domains. Furthermore, we study cycles -- generating from image to text then back to image and vise versa, as well as its connection with autoencoders.",0
"This project focuses on generating novel pairs of images and text that show interesting relationships and convey compelling messages. By leveraging advances in artificial intelligence (AI) and natural language processing (NLP), we can create powerful tools that enable creative expression beyond the capabilities of traditional image generation systems. We describe how these models can generate novel associations by exploring the space of possibilities rather than simply relying on fixed patterns learned from large datasets. Our work represents a step forward towards intelligent creation systems capable of producing high quality content across multiple modalities. We discuss future directions for research into more advanced multimodal generators able to capture complex contextual relationships and achieve even greater creativity.",1
"Optimal Transport has recently gained interest in machine learning for applications ranging from domain adaptation, sentence similarities to deep learning. Yet, its ability to capture frequently occurring structure beyond the ""ground metric"" is limited. In this work, we develop a nonlinear generalization of (discrete) optimal transport that is able to reflect much additional structure. We demonstrate how to leverage the geometry of this new model for fast algorithms, and explore connections and properties. Illustrative experiments highlight the benefit of the induced structured couplings for tasks in domain adaptation and natural language processing.",0
"In this work we study how optimal transport plans can be structured by leveraging novel techniques from convex optimization, specifically exploiting first-order methods that enjoy strong convergence guarantees under rather mild assumptions on both the data and the objective function. This provides an efficient alternative to standard Sinkhorn-Knopp iterations, which suffer from slow convergence and scaling issues as the problem size grows larger: we showcase that our approach leads to better numerical performance across several scenarios of interest, while retaining provably exact solutions even for highly degenerate problems where entropic regularization would fail catastrophically. We believe these findings hold significant promise towards enabling broader use of OT tools beyond those academic settings able to afford expensive software packages running on clusters/supercomputers. Code associated to this paper is publicly available online at [URL].",1
"We present a new approach to the design of deep networks for natural language processing (NLP), based on the general technique of Tensor Product Representations (TPRs) for encoding and processing symbol structures in distributed neural networks. A network architecture --- the Tensor Product Generation Network (TPGN) --- is proposed which is capable in principle of carrying out TPR computation, but which uses unconstrained deep learning to design its internal representations. Instantiated in a model for image-caption generation, TPGN outperforms LSTM baselines when evaluated on the COCO dataset. The TPR-capable structure enables interpretation of internal representations and operations, which prove to contain considerable grammatical content. Our caption-generation model can be interpreted as generating sequences of grammatical categories and retrieving words by their categories from a plan encoded as a distributed representation.",0
"In this paper we present tensor product generation networks (TPGN) as a novel model class for deep natural language processing (NLP). We describe our approach in detail, including training procedures, hyperparameter selection, and evaluation metrics used. Our results indicate that TPGN models achieve strong performance on a wide range of tasks across several benchmark datasets, rivaling state-of-the-art methods. Furthermore, we demonstrate the benefits of using TPGN over traditional architectures in terms of improved efficiency and scalability. Finally, we conclude by discussing future directions for research in this area.",1
"The chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals' Picture Archiving and Communication Systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high precision computer-aided diagnosis (CAD) systems.   In this paper, we present a new chest X-ray database, namely ""ChestX-ray8"", which comprises 108,948 frontal-view X-ray images of 32,717 unique patients with the text-mined eight disease image labels (where each image can have multi-labels), from the associated radiological reports using natural language processing. Importantly, we demonstrate that these commonly occurring thoracic diseases can be detected and even spatially-located via a unified weakly-supervised multi-label image classification and disease localization framework, which is validated using our proposed dataset. Although the initial quantitative results are promising as reported, deep convolutional neural network based ""reading chest X-rays"" (i.e., recognizing and locating the common disease patterns trained with only image-level labels) remains a strenuous task for fully-automated high precision CAD systems. Data download link: https://nihcc.app.box.com/v/ChestXray-NIHCC",0
"This paper presents the ChestX-ray8 database, which includes over 257K hospital chest x-ray images from nearly 60k unique patients across two hospitals. To build such a large dataset without direct medical supervision, we developed web-based user interfaces where expert radiologists could localize regions of interest (ROIs) for up to three thorax diseases per image. Annotations were iteratively collected until agreement was reached among three annotators. Using these ROIs as weak labels, we conduct benchmark studies to evaluate advances in computer vision algorithms for automating classification and disease location on chest x-rays. Our findings demonstrate significant progress is possible, particularly on classifying five common thoracic diseases, but that challenges remain for locating precise lesion boundaries in complex backgrounds. Code and data available at https://github.com/NYUML/chexdb .",1
"The goal of counterfactual learning for statistical machine translation (SMT) is to optimize a target SMT system from logged data that consist of user feedback to translations that were predicted by another, historic SMT system. A challenge arises by the fact that risk-averse commercial SMT systems deterministically log the most probable translation. The lack of sufficient exploration of the SMT output space seemingly contradicts the theoretical requirements for counterfactual learning. We show that counterfactual learning from deterministic bandit logs is possible nevertheless by smoothing out deterministic components in learning. This can be achieved by additive and multiplicative control variates that avoid degenerate behavior in empirical risk minimization. Our simulation experiments show improvements of up to 2 BLEU points by counterfactual learning from deterministic bandit feedback.",0
"In this study we examine counterfactual learning in bandits under deterministic logging scenarios where uncertainty over user feedback can make it difficult to identify the cause of rewards or regrets received by the agent. Our approach utilizes statistical machine translation models which provide us with more fine-grained representations of text that we use as features in our learner model allowing for greater exploration space. We show experimentally how using these features leads to improvements in regret compared to traditional methods used in counterfactual learning from bandit feedback. Finally, we conclude by discussing future directions for work on refining and improving our methodology further.",1
"Many radiological studies can reveal the presence of several co-existing abnormalities, each one represented by a distinct visual pattern. In this article we address the problem of learning a distance metric for plain radiographs that captures a notion of ""radiological similarity"": two chest radiographs are considered to be similar if they share similar abnormalities. Deep convolutional neural networks (DCNs) are used to learn a low-dimensional embedding for the radiographs that is equipped with the desired metric. Two loss functions are proposed to deal with multi-labelled images and potentially noisy labels. We report on a large-scale study involving over 745,000 chest radiographs whose labels were automatically extracted from free-text radiological reports through a natural language processing system. Using 4,500 validated exams, we demonstrate that the methodology performs satisfactorily on clustering and image retrieval tasks. Remarkably, the learned metric separates normal exams from those having radiological abnormalities.",0
"Deep metric learning is a rapidly developing field that has shown promising results in image classification tasks. In medical imaging domains such as mammography and dental X-ray analysis, multi-label annotation can present a challenge due to ambiguity arising from overlapping structures and varying interpretations among experts. However, using deep metric learning techniques to learn feature representations tailored specifically for each label improves performance while reducing reliance on large amounts of annotated data. This work presents an approach to deep metric learning for multi-labeled chest radiographs where the learnt features yield state-of-the-art performance against other methods relying solely on raw pixel intensities, handcrafted features, and traditional machine learning models. Our method achieves improved accuracy by optimizing inter- and intra-class distances through triplet loss and mining hard negative examples based on class-wise semantic similarity. The model generalizes well across datasets with limited fine-tuning, demonstrating potential real-world applications in healthcare. These findings suggest that applying deep metric learning frameworks adapted for multi-labels could significantly improve automation within diagnostic decision support systems. By leveraging these advances, we take one step closer towards enabling accurate diagnosis at an early stage without sacrificing patient safety. We hope our research inspires further exploration into harnessing both radiologist intuition and cutting-edge technology in shaping the future of medicine.",1
"Fine-grained object recognition that aims to identify the type of an object among a large number of subcategories is an emerging application with the increasing resolution that exposes new details in image data. Traditional fully supervised algorithms fail to handle this problem where there is low between-class variance and high within-class variance for the classes of interest with small sample sizes. We study an even more extreme scenario named zero-shot learning (ZSL) in which no training example exists for some of the classes. ZSL aims to build a recognition model for new unseen categories by relating them to seen classes that were previously learned. We establish this relation by learning a compatibility function between image features extracted via a convolutional neural network and auxiliary information that describes the semantics of the classes of interest by using training samples from the seen classes. Then, we show how knowledge transfer can be performed for the unseen classes by maximizing this function during inference. We introduce a new data set that contains 40 different types of street trees in 1-ft spatial resolution aerial data, and evaluate the performance of this model with manually annotated attributes, a natural language model, and a scientific taxonomy as auxiliary information. The experiments show that the proposed model achieves 14.3% recognition accuracy for the classes with no training examples, which is significantly better than a random guess accuracy of 6.3% for 16 test classes, and three other ZSL algorithms.",0
"Abstract: Fine-grained object recognition and zero-shot learning have become increasingly important topics in remote sensing imagery due to their ability to accurately classify objects at subcategory levels and handle new classes without retraining, respectively. In recent years, deep convolutional neural networks (CNNs) have been widely used in fine-grained object recognition, but they often require large amounts of labeled data that may not always be available. In contrast, transfer learning has emerged as a promising approach for leveraging pre-trained models and limited amounts of target dataset data to achieve high accuracy. Despite these advances, there remain several challenges in applying these techniques to real-world applications, including handling incomplete datasets, ensuring generalization across different environments and illumination conditions, and addressing issues related to scalability and interpretability. To tackle these problems, this paper presents novel approaches based on adversarial training, domain adaptation, and attention mechanisms, among others, to enhance performance in fine-grained object recognition and zero-shot learning tasks using CNNs in remote sensing imagery. Extensive experiments demonstrate significant improvements over state-of-the-art methods across multiple benchmarks under different settings. This work paves the way towards more accurate analysis of complex urban scenarios and natural phenomena by providing effective means to analyze remote sensing imagery captured from various platforms and acquisition systems.",1
"The digitalization of stored information in hospitals now allows for the exploitation of medical data in text format, as electronic health records (EHRs), initially gathered for other purposes than epidemiology. Manual search and analysis operations on such data become tedious. In recent years, the use of natural language processing (NLP) tools was highlighted to automatize the extraction of information contained in EHRs, structure it and perform statistical analysis on this structured information. The main difficulties with the existing approaches is the requirement of synonyms or ontology dictionaries, that are mostly available in English only and do not include local or custom notations. In this work, a team composed of oncologists as domain experts and data scientists develop a custom NLP-based system to process and structure textual clinical reports of patients suffering from breast cancer. The tool relies on the combination of standard text mining techniques and an advanced synonym detection method. It allows for a global analysis by retrieval of indicators such as medical history, tumor characteristics, therapeutic responses, recurrences and prognosis. The versatility of the method allows to obtain easily new indicators, thus opening up the way for retrospective studies with a substantial reduction of the amount of manual work. With no need for biomedical annotators or pre-defined ontologies, this language-agnostic method reached an good extraction accuracy for several concepts of interest, according to a comparison with a manually structured file, without requiring any existing corpus with local or new notations.",0
"Breast Cancer Research: Innovations in Text Analytics and Data Mining Techniques With the widespread availability of digital medical records, large volumes of data have become available for research on breast cancer diagnosis, prognosis, treatment planning, outcomes evaluation, personalized medicine and more. This has led to increased interest in advanced computational techniques that can effectively process unstructured clinical narratives and integrate them into structured datasets. However, traditional natural language processing (NLP) and machine learning approaches often require extensive manual annotation and feature engineering which makes scaling these efforts challenging. To address this gap, we propose the use of deep learning methods such as Recurrent Neural Networks (RNN), Long Short Term Memory (LSTM), Gated Recurrent Units (GRU) and Transformer architecture based models like BERT and ELMo. These models capture intricate relationships among different features without explicit supervision. Our contribution lies in using cutting edge deep learning architectures and transfer learning techniques to accelerate the task at hand while improving performance over conventional NLP pipelines. We evaluate our approach on multiple benchmarked datasets popularly used in Natural Language Processing (NLP). Results show significant improvement in performance compared to established baseline systems. Additionally, through integration with visualization tools and domain expert feedback, insights from the proposed system could aid radiologists in making accurate screening decisions. Overall, our work lays groundwork towards realizing scalable next generation systems aimed at assisting stakeholders across the entire breast health continuum through evidence driven data analytics.",1
"Visual Question Answering (VQA) has attracted much attention since it offers insight into the relationships between the multi-modal analysis of images and natural language. Most of the current algorithms are incapable of answering open-domain questions that require to perform reasoning beyond the image contents. To address this issue, we propose a novel framework which endows the model capabilities in answering more complex questions by leveraging massive external knowledge with dynamic memory networks. Specifically, the questions along with the corresponding images trigger a process to retrieve the relevant information in external knowledge bases, which are embedded into a continuous vector space by preserving the entity-relation structures. Afterwards, we employ dynamic memory networks to attend to the large body of facts in the knowledge graph and images, and then perform reasoning over these facts to generate corresponding answers. Extensive experiments demonstrate that our model not only achieves the state-of-the-art performance in the visual question answering task, but can also answer open-domain questions effectively by leveraging the external knowledge.",0
"How does your model outperform existing models on VQA2.0? We use two types of comparison: human baseline (which we denote as HB) and an automatic evaluation metric called Accuracy@1 (denoted by AA@1). Against these metrics, we show that our model outperforms all previously published submissions across both test sets! For example, against AA@1, our approach achieves +4.8% absolute improvement over the second place method.",1
"Vision and language understanding has emerged as a subject undergoing intense study in Artificial Intelligence. Among many tasks in this line of research, visual question answering (VQA) has been one of the most successful ones, where the goal is to learn a model that understands visual content at region-level details and finds their associations with pairs of questions and answers in the natural language form. Despite the rapid progress in the past few years, most existing work in VQA have focused primarily on images. In this paper, we focus on extending VQA to the video domain and contribute to the literature in three important ways. First, we propose three new tasks designed specifically for video VQA, which require spatio-temporal reasoning from videos to answer questions correctly. Next, we introduce a new large-scale dataset for video VQA named TGIF-QA that extends existing VQA work with our new tasks. Finally, we propose a dual-LSTM based approach with both spatial and temporal attention, and show its effectiveness over conventional VQA techniques through empirical evaluations.",0
"Recent years have seen significant progress on visual question answering (VQA), where systems must localize relevant regions in images before natural language understanding can take place. Here we study two key pieces of evidence used by humans that VQAs lack -- spatial context reasoning over objects and their relationships, and temporal reasoning to track change across frames. By incorporating both into our model, we find substantial improvements on the challenging CLEVR dataset. Our success here sets the stage for future work exploring how these kinds of common sense reasoning abilities might apply more broadly to human intelligence tasks involving vision and language. Keywords: Vision Question Answering, Spatial Temporal Reasoning",1
"In this paper, we propose an Attentional Generative Adversarial Network (AttnGAN) that allows attention-driven, multi-stage refinement for fine-grained text-to-image generation. With a novel attentional generative network, the AttnGAN can synthesize fine-grained details at different subregions of the image by paying attentions to the relevant words in the natural language description. In addition, a deep attentional multimodal similarity model is proposed to compute a fine-grained image-text matching loss for training the generator. The proposed AttnGAN significantly outperforms the previous state of the art, boosting the best reported inception score by 14.14% on the CUB dataset and 170.25% on the more challenging COCO dataset. A detailed analysis is also performed by visualizing the attention layers of the AttnGAN. It for the first time shows that the layered attentional GAN is able to automatically select the condition at the word level for generating different parts of the image.",0
"Our work focuses on text-to-image generation using attentional generative adversarial networks (AttnGAN). Existing approaches rely on recurrent neural networks (RNNs) that map paragraphs of text into a fixed-size vector space, which becomes impractical as we increase the size of text inputs. In contrast, our approach uses GAN architectures specifically designed for sequence data processing. We introduce two new components to enhance fine-grained control over generated images—an attention mechanism that allows for local feature extraction from input texts and a conditional discriminator that enables accurate mapping of small changes in image features to their corresponding textual descriptions. This leads to improved visual coherence and better alignment between generated images and their descriptive texts. Additionally, AttnGAN demonstrates significantly lower reconstruction errors compared to existing methods while maintaining comparable Frechet inception distance scores, indicating more realistic outputs. Experiments on four benchmark datasets confirm the effectiveness of our approach. Overall, AttnGAN sets a strong baseline for future advancements in the field of fine-grained text-to-image synthesis.",1
"Generating natural language descriptions of images is an important capability for a robot or other visual-intelligence driven AI agent that may need to communicate with human users about what it is seeing. Such image captioning methods are typically trained by maximising the likelihood of ground-truth annotated caption given the image. While simple and easy to implement, this approach does not directly maximise the language quality metrics we care about such as CIDEr. In this paper we investigate training image captioning methods based on actor-critic reinforcement learning in order to directly optimise non-differentiable quality metrics of interest. By formulating a per-token advantage and value computation strategy in this novel reinforcement learning based captioning model, we show that it is possible to achieve the state of the art performance on the widely used MSCOCO benchmark.",0
"This paper presents a new approach to image captioning using deep reinforcement learning. We propose an actor-critic sequence training (ACST) method that combines actor-based policy optimization with critic-based value estimation. Our model learns to generate natural language descriptions of images by maximizing a reward function that measures both the quality of the generated text and its similarity to human annotations. Experimental results show that our ACST approach outperforms state-of-the-art methods on standard benchmark datasets, achieving significant improvements in accuracy and coherence. Our work demonstrates the effectiveness of combining deep reinforcement learning with natural language processing techniques for generating high-quality image captions.",1
"Recent systems on structured prediction focus on increasing the level of structural dependencies within the model. However, our study suggests that complex structures entail high overfitting risks. To control the structure-based overfitting, we propose to conduct structure regularization decoding (SR decoding). The decoding of the complex structure model is regularized by the additionally trained simple structure model. We theoretically analyze the quantitative relations between the structural complexity and the overfitting risk. The analysis shows that complex structure models are prone to the structure-based overfitting. Empirical evaluations show that the proposed method improves the performance of the complex structure models by reducing the structure-based overfitting. On the sequence labeling tasks, the proposed method substantially improves the performance of the complex neural network models. The maximum F1 error rate reduction is 36.4% for the third-order model. The proposed method also works for the parsing task. The maximum UAS improvement is 5.5% for the tri-sibling model. The results are competitive with or better than the state-of-the-art results.",0
"This study investigates how complex structure can lead to overfitting in natural language processing (NLP) models, particularly those using transformer architecture. We propose a new method called ""structure regularization decoding"" that utilizes a simplified search process and incorporation of linguistically motivated constraints to reduce overfitting while still maintaining high performance on NLP tasks. Our approach leverages structural bias during training by introducing simple dependencies based on grammatical rules, resulting in improved generalization ability compared to baseline models trained without regularization. Experiments show promising results across several benchmark datasets and demonstrate the effectiveness of our proposed method in preventing overfitting while improving overall model performance. By introducing linguistically informed constraints into decoding, we provide a novel direction towards addressing overfitting problems commonly found in large pre-trained NLP models.",1
"Reasoning about the relationships between object pairs in images is a crucial task for holistic scene understanding. Most of the existing works treat this task as a pure visual classification task: each type of relationship or phrase is classified as a relation category based on the extracted visual features. However, each kind of relationships has a wide variety of object combination and each pair of objects has diverse interactions. Obtaining sufficient training samples for all possible relationship categories is difficult and expensive. In this work, we propose a natural language guided framework to tackle this problem. We propose to use a generic bi-directional recurrent neural network to predict the semantic connection between the participating objects in the relationship from the aspect of natural language. The proposed simple method achieves the state-of-the-art on the Visual Relationship Detection (VRD) and Visual Genome datasets, especially when predicting unseen relationships (e.g. recall improved from 76.42% to 89.79% on VRD zero-shot testing set).",0
"Abstract: The task of detecting visual relationships has recently gained attention as a means to improve scene understanding by computing high-level descriptions of image content such as object interactions and dependencies. In this work, we propose a novel method that utilizes natural language guidance during the relationship detection process. Our approach leverages the advantages of both vision and linguistics while mitigating their respective limitations. We demonstrate how our proposed model can accurately predict visual relationships even in challenging scenarios where traditional computer vision methods may fail. Additionally, we provide extensive evaluations on two publicly available datasets showcasing the effectiveness of our method. By incorporating natural language knowledge into the visual relationship detection pipeline, our work paves the way towards more advanced scene understanding applications.",1
"We present a novel approach for learning to predict sets using deep learning. In recent years, deep neural networks have shown remarkable results in computer vision, natural language processing and other related problems. Despite their success, traditional architectures suffer from a serious limitation in that they are built to deal with structured input and output data, i.e. vectors or matrices. Many real-world problems, however, are naturally described as sets, rather than vectors. Existing techniques that allow for sequential data, such as recurrent neural networks, typically heavily depend on the input and output order and do not guarantee a valid solution. Here, we derive in a principled way, a mathematical formulation for set prediction where the output is permutation invariant. In particular, our approach jointly learns both the cardinality and the state distribution of the target set. We demonstrate the validity of our method on the task of multi-label image classification and achieve a new state of the art on the PASCAL VOC and MS COCO datasets.",0
"Recent advancements in machine learning have led to increased interest in multiagent reinforcement learning (MARL), which involves multiple agents interacting with each other in complex environments to achieve common goals. One key challenge in MARL is determining how to model the interaction between agents and their environment. In particular, estimating the number of objects or elements in the state of the system, known as set cardinality estimation, can improve the accuracy of predictions and decision making in MARL.  This work proposes a novel method for jointly learning set cardinality and state distribution in MARL systems. By combining deep neural networks and Bayesian inference techniques, our approach enables efficient and accurate estimation of both variables. Our framework allows each agent to communicate its beliefs on the set cardinality to a centralized entity that aggregates them into a single estimate. This provides a more reliable prediction of the set cardinality than individual estimates alone.  To evaluate our proposed approach, we perform experiments using two benchmark MARL problems: the Bridge Game and the MG-II Gridworld. Our results show significant improvement over existing methods for set cardinality estimation, resulting in improved performance in terms of coordination among agents and overall task completion rate. Moreover, we demonstrate that our method effectively reduces uncertainty about set cardinality during the course of learning, leading to better decision making and stronger coordination among agents.  Overall, our work contributes to the field of MARL by addressing a fundamental problem in multiagent planning and control - estimating the number of elements in the state space. Our approach has the potential to benefit a wide range of applications including smart grids, transportation systems, and team robots.",1
"Recently, deep residual networks have been successfully applied in many computer vision and natural language processing tasks, pushing the state-of-the-art performance with deeper and wider architectures. In this work, we interpret deep residual networks as ordinary differential equations (ODEs), which have long been studied in mathematics and physics with rich theoretical and empirical success. From this interpretation, we develop a theoretical framework on stability and reversibility of deep neural networks, and derive three reversible neural network architectures that can go arbitrarily deep in theory. The reversibility property allows a memory-efficient implementation, which does not need to store the activations for most hidden layers. Together with the stability of our architectures, this enables training deeper networks using only modest computational resources. We provide both theoretical analyses and empirical results. Experimental results demonstrate the efficacy of our architectures against several strong baselines on CIFAR-10, CIFAR-100 and STL-10 with superior or on-par state-of-the-art performance. Furthermore, we show our architectures yield superior results when trained using fewer training data.",0
"In recent years, deep neural networks have shown remarkable performance on many tasks ranging from image classification to natural language processing. However, their training can be computationally expensive, resulting in extended training times and increased risk of overfitting. One approach to address these challenges is to use reversible architectures, which allow for efficient gradient propagation during backpropagation without losing representational power. This work presents new reversible building blocks that enable the construction of arbitrarily deep residual neural networks. Our contributions include analytic expressions for both forward and reverse passes through our proposed blocks and experiments demonstrating the effectiveness of our methods on several benchmark datasets. We find that models built using our reversible components achieve comparable accuracy to traditional non-reversible approaches while significantly reducing computational cost and memory usage. Overall, our results demonstrate the promise of reversible architecture designs as a means to improve efficiency and scalability in deep learning applications.",1
"Recognising objects according to a pre-defined fixed set of class labels has been well studied in the Computer Vision. There are a great many practical applications where the subjects that may be of interest are not known beforehand, or so easily delineated, however. In many of these cases natural language dialog is a natural way to specify the subject of interest, and the task achieving this capability (a.k.a, Referring Expression Comprehension) has recently attracted attention. To this end we propose a unified framework, the ParalleL AttentioN (PLAN) network, to discover the object in an image that is being referred to in variable length natural expression descriptions, from short phrases query to long multi-round dialogs. The PLAN network has two attention mechanisms that relate parts of the expressions to both the global visual content and also directly to object candidates. Furthermore, the attention mechanisms are recurrent, making the referring process visualizable and explainable. The attended information from these dual sources are combined to reason about the referred object. These two attention mechanisms can be trained in parallel and we find the combined system outperforms the state-of-art on several benchmarked datasets with different length language input, such as RefCOCO, RefCOCO+ and GuessWhat?!.",0
"In recent years, there has been growing interest in developing visual object discovery methods that can effectively leverage natural language inputs such as queries and dialogue interactions to identify objects within images. This paper presents a novel framework called ""Parallel Attention"" that unifies these two approaches into a single model. Our method leverages both bottom-up and top-down attention mechanisms to facilitate interaction between textual input and image data, enabling more accurate detection of relevant regions and objects across multiple modalities. We demonstrate the effectiveness of our approach on several benchmark datasets and show significant improvements over state-of-the-art methods. Furthermore, we provide detailed analysis and ablation studies to validate key components of the proposed architecture. Overall, this work represents an important step towards realizing robust and efficient systems for object discovery using natural language interactions.",1
"A fundamental challenge in developing high-impact machine learning technologies is balancing the need to model rich, structured domains with the ability to scale to big data. Many important problem areas are both richly structured and large scale, from social and biological networks, to knowledge graphs and the Web, to images, video, and natural language. In this paper, we introduce two new formalisms for modeling structured data, and show that they can both capture rich structure and scale to big data. The first, hinge-loss Markov random fields (HL-MRFs), is a new kind of probabilistic graphical model that generalizes different approaches to convex inference. We unite three approaches from the randomized algorithms, probabilistic graphical models, and fuzzy logic communities, showing that all three lead to the same inference objective. We then define HL-MRFs by generalizing this unified objective. The second new formalism, probabilistic soft logic (PSL), is a probabilistic programming language that makes HL-MRFs easy to define using a syntax based on first-order logic. We introduce an algorithm for inferring most-probable variable assignments (MAP inference) that is much more scalable than general-purpose convex optimization methods, because it uses message passing to take advantage of sparse dependency structures. We then show how to learn the parameters of HL-MRFs. The learned HL-MRFs are as accurate as analogous discrete models, but much more scalable. Together, these algorithms enable HL-MRFs and PSL to model rich, structured data at scales not previously possible.",0
"Title: Advancing Artificial Intelligence through Hinge-Loss Markov Random Fields and Probabilistic Soft Logic  Artificial intelligence (AI) has made significant strides over the past few decades, but there still exist challenges in areas such as natural language processing and computer vision that limit our ability to develop truly intelligent systems. In recent years, researchers have turned to probabilistic soft logic and hinge-loss markov random fields to address these issues.  Probabilistic soft logic provides a powerful framework for modeling complex relationships and reasoning under uncertainty by combining probability theory with propositional logic. This approach enables machines to make more accurate predictions and better-informed decisions based on available evidence. Meanwhile, markov random fields provide a mathematical structure for capturing spatial dependencies within data sets, allowing for improved understanding of patterns and trends. Combining probabilistic soft logic with markov random fields results in even greater capability in machine learning tasks.  In this work, we explore the use of hinge-loss markov random fields and probabilistic soft logic together to advance artificial intelligence. Our analysis shows promising results across several domains, including image classification, text generation, and decision making. We evaluate the performance of our models using standard benchmark datasets and demonstrate their effectiveness compared to existing approaches.  Our contributions lay the foundation for future research in integrating probabilistic soft logic with markov random fields, leading to enhanced AI capabilities. By leveraging these techniques, we can create smarter machines that excel at humanlike tasks and further enhance society’s reliance on AI technology.  As such, the presented work makes valuable insights into advancing artificial intelligence, offering implications far beyond traditional computing and engineering applications. As AI continues to evolve and play increasingly important roles in our lives, novel solutions like those proposed herein become ever mor",1
"Humans are able to explain their reasoning. On the contrary, deep neural networks are not. This paper attempts to bridge this gap by introducing a new way to design interpretable neural networks for classification, inspired by physiological evidence of the human visual system's inner-workings. This paper proposes a neural network design paradigm, termed InterpNET, which can be combined with any existing classification architecture to generate natural language explanations of the classifications. The success of the module relies on the assumption that the network's computation and reasoning is represented in its internal layer activations. While in principle InterpNET could be applied to any existing classification architecture, it is evaluated via an image classification and explanation task. Experiments on a CUB bird classification and explanation dataset show qualitatively and quantitatively that the model is able to generate high-quality explanations. While the current state-of-the-art METEOR score on this dataset is 29.2, InterpNET achieves a much higher METEOR score of 37.9.",0
"""InterpNET is an innovative deep learning model that incorporates introspection techniques into the neural network training process. This method enables more accurate predictions by identifying the most important features used by the network to make decisions. Unlike previous interpretability methods, which analyze the network after training has been completed, InterpNET provides continuous feedback during training, allowing for better fine-tuning and optimization. Additionally, our approach uses transfer learning from pre-trained models to further improve accuracy and speed up convergence. We evaluate InterpNET on several challenging datasets and demonstrate that it outperforms existing interpretability methods while maintaining strong performance on benchmark tasks.""",1
"Automatic generation of caption to describe the content of an image has been gaining a lot of research interests recently, where most of the existing works treat the image caption as pure sequential data. Natural language, however possess a temporal hierarchy structure, with complex dependencies between each subsequence. In this paper, we propose a phrase-based hierarchical Long Short-Term Memory (phi-LSTM) model to generate image description. In contrast to the conventional solutions that generate caption in a pure sequential manner, our proposed model decodes image caption from phrase to sentence. It consists of a phrase decoder at the bottom hierarchy to decode noun phrases of variable length, and an abbreviated sentence decoder at the upper hierarchy to decode an abbreviated form of the image description. A complete image caption is formed by combining the generated phrases with sentence during the inference stage. Empirically, our proposed model shows a better or competitive result on the Flickr8k, Flickr30k and MS-COCO datasets in comparison to the state-of-the art models. We also show that our proposed model is able to generate more novel captions (not seen in the training data) which are richer in word contents in all these three datasets.",0
"This article presents a new system and methodology for generating image descriptions using natural language processing techniques. Our approach combines state-of-the-art sequence modeling tools with innovative heuristics designed to capture and exploit high-level structures present in descriptive text. We demonstrate that our model significantly outperforms existing methods on standard benchmarks, achieving unprecedented accuracy and coherence in generated outputs. Our work represents an important step towards more intelligent systems capable of understanding complex visual content and communicating insights effectively through language. Further research directions are discussed, including potential applications in automated reporting, assistive technologies, and open-domain question answering.",1
"We present a novel and scalable label embedding framework for large-scale multi-label learning a.k.a ExMLDS (Extreme Multi-Label Learning using Distributional Semantics). Our approach draws inspiration from ideas rooted in distributional semantics, specifically the Skip Gram Negative Sampling (SGNS) approach, widely used to learn word embeddings for natural language processing tasks. Learning such embeddings can be reduced to a certain matrix factorization. Our approach is novel in that it highlights interesting connections between label embedding methods used for multi-label learning and paragraph/document embedding methods commonly used for learning representations of text data. The framework can also be easily extended to incorporate auxiliary information such as label-label correlations; this is crucial especially when there are a lot of missing labels in the training data. We demonstrate the effectiveness of our approach through an extensive set of experiments on a variety of benchmark datasets, and show that the proposed learning methods perform favorably compared to several baselines and state-of-the-art methods for large-scale multi-label learning. To facilitate end-to-end learning, we develop a joint learning algorithm that can learn the embeddings as well as a regression model that predicts these embeddings given input features, via efficient gradient-based methods.",0
"Advances in natural language processing (NLP) have led to the development of distributional semantics, which involves modeling the meaning of words based on their co-occurrences with other words in large corpora. This approach has shown promising results in tasks such as sentiment analysis and text classification. In this work, we explore how distributional semantics can be leveraged for multi-label learning, where each instance belongs to multiple labels simultaneously. We propose a novel method that utilizes contextualized embeddings generated from BERT, a state-of-the-art NLP model, to represent instances and labels. Our method then learns a low-dimensional subspace using singular value decomposition, capturing the most relevant information from the high-dimensional embedding space while reducing computation time and memory usage during training and inference. Experimental evaluations demonstrate the effectiveness of our approach against several baselines across diverse datasets, illustrating the potential benefits of incorporating distributional semantics into multi-label learning frameworks.",1
"Deep learning has achieved a great success in many areas, from computer vision to natural language processing, to game playing, and much more. Yet, what deep learning is really doing is still an open question. There are a lot of works in this direction. For example, [5] tried to explain deep learning by group renormalization, and [6] tried to explain deep learning from the view of functional approximation. In order to address this very crucial question, here we see deep learning from perspective of mechanical learning and learning machine (see [1], [2]). From this particular angle, we can see deep learning much better and answer with confidence: What deep learning is really doing? why it works well, how it works, and how much data is necessary for learning. We also will discuss advantages and disadvantages of deep learning at the end of this work.",0
"The Abstract:  Recent advances in deep learning have allowed artificial intelligence (AI) algorithms to perform tasks that were previously thought only possible by humans. However, there remains ongoing debate as to how these models achieve their impressive results, particularly given their lack of transparency compared to traditional machine learning methods. In this review, we take an in-depth look at the current state of deep learning research to provide insights into what is actually happening beneath the surface. We examine several popular applications of deep learning across different domains, from image classification to natural language processing, and highlight key techniques used in each case. Our analysis reveals that while deep learning may appear to operate like magic, it ultimately relies on foundational principles such as optimization theory, signal processing, and statistical inference. Ultimately, our goal is to contribute towards a deeper understanding of the field that can inform future development of more explainable and reliable AI systems. --- In summary: A new paradigm called “deep learning” has emerged within the artificial intelligence (AI) community and has proven capable of achieving human-like accuracy in complex data-driven tasks such as image recognition or speech synthesis. Despite significant progress and commercial adoption, questions remain regarding the interpretability and reliability of these powerful but opaque mathematical models. This document examines several modern deep learning algorithms – and related application scenarios like computer vision – through lenses including optimization, statistics, control theory, and distributed computing. Its main contribution is to identify simple mental constructs underlying what seem to be highly counterintuitive processes; thus opening up perspectives toward developing even smarter machines while retaining safety guarantees essential in critical use cases such as medical diagnosis, self-driving cars, etc. An informed public discourse on the actual capabilities – and limits – of intelligent software is crucial both scientifically and ethically. The present work provides fresh, accessible insight to advance general audience appreciation of AI, bridge perceived gaps betw",1
"Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. Unlike previous work, we require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). We compare a reinforcement learning approach and one using a differentiable relaxation (straight-through Gumbel-softmax estimator) and observe that the latter is much faster to converge and it results in more effective protocols. Interestingly, we also observe that the protocol we induce by optimizing the communication success exhibits a degree of compositionality and variability (i.e. the same information can be phrased in different ways), both properties characteristic of natural languages. As the ultimate goal is to ensure that communication is accomplished in natural language, we also perform experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.",0
"In this paper we investigate how groups of intelligent agents can develop shared symbolic systems without direct instruction or guidance from supervisors. Our focus is on emergent languages that allow humans and computers to communicate more effectively. To accomplish this goal, we introduce a novel multi-agent reinforcement learning algorithm called EchoGame, which encourages agents to learn and use sequences of symbols as part of their interactions. We demonstrate through experiments using both synthetic and real-world datasets that EchoGame is able to facilitate the emergence of meaningful language structures. These results suggest new approaches for training machine learning models to generate fluent natural language text as well as enabling human-AI collaboration via more effective communication channels. Keywords: multi-agent games, language evolution, emergent language, machine learning, deep learning, generative modelling.",1
"While the visualization of statistical data tends to a mature technology, the visualization of textual data is still in its infancy, especially for the artistic text. Due to the fact that visualization of artistic text is valuable and attractive in both art and information science, we attempt to realize this tentative idea in this article. We propose the Generative Adversarial Network based Artistic Textual Visualization (GAN-ATV) which can create paintings after analyzing the semantic content of existing poems. Our GAN-ATV consists of two main sections: natural language analysis section and visual information synthesis section. In natural language analysis section, we use Bag-of-Word (BoW) feature descriptors and a two-layer network to mine and analyze the high-level semantic information from poems. In visual information synthesis section, we design a cross-modal semantic understanding module and integrate it with Generative Adversarial Network (GAN) to create paintings, whose content are corresponding to the original poems. Moreover, in order to train our GAN-ATV and verify its performance, we establish a cross-modal artistic dataset named ""Cross-Art"". In the Cross-Art dataset, there are six topics and each topic has their corresponding paintings and poems. The experimental results on Cross-Art dataset are shown in this article.",0
"This paper presents a novel approach to artistic textual visualization using Generative Adversarial Networks (GAN). By leveraging the power of deep learning and advanced computer vision techniques, we propose a system that can generate visually striking images from raw text inputs. Our method utilizes a two-stage pipeline consisting of image generation followed by fine-grained enhancement, enabling us to create images that more closely align with the underlying text input. We extensively evaluate our model on a diverse range of datasets and demonstrate its ability to produce high-quality results across various domains such as natural scenes, objects, characters, and abstractions. Our approach has important implications for artistic applications, including computer graphics, animation, video games, virtual reality, and interactive media installations. Overall, this work represents an exciting new direction for research at the intersection of art and technology.",1
"Probabilistic methods for classifying text form a rich tradition in machine learning and natural language processing. For many important problems, however, class prediction is uninteresting because the class is known, and instead the focus shifts to estimating latent quantities related to the text, such as affect or ideology. We focus on one such problem of interest, estimating the ideological positions of 55 Irish legislators in the 1991 D\'ail confidence vote. To solve the D\'ail scaling problem and others like it, we develop a text modeling framework that allows actors to take latent positions on a ""gray"" spectrum between ""black"" and ""white"" polar opposites. We are able to validate results from this model by measuring the influences exhibited by individual words, and we are able to quantify the uncertainty in the scaling estimates by using a sentence-level block bootstrap. Applying our method to the D\'ail debate, we are able to scale the legislators between extreme pro-government and pro-opposition in a way that reveals nuances in their speeches not captured by their votes or party affiliations.",0
"This work proposes a new method for scaling up text generation models. The problem we address here is that traditional methods for generating large language models have required massive computational resources, meaning they cannot scale to meet demand from end users without significant centralization at server farms. Our proposed solution is based on training smaller language models locally on user devices themselves using very little data, then combining these predictions using class affinity scores to generate human-quality responses. We demonstrate experimentally that our approach achieves state-of-the-art results both quantitatively and qualitatively while reducing computational requirements by several orders of magnitude compared to existing approaches. In addition, since users retain control over their own local model weights, we can achieve privacy benefits as well. By dramatically reducing the amount of compute and storage necessary for high quality language generation, we hope to make AI more accessible to all users across different platforms such as smartphones, laptops, virtual assistants etc..",1
"Deep learning models (aka Deep Neural Networks) have revolutionized many fields including computer vision, natural language processing, speech recognition, and is being increasingly used in clinical healthcare applications. However, few works exist which have benchmarked the performance of the deep learning models with respect to the state-of-the-art machine learning models and prognostic scoring systems on publicly available healthcare datasets. In this paper, we present the benchmarking results for several clinical prediction tasks such as mortality prediction, length of stay prediction, and ICD-9 code group prediction using Deep Learning models, ensemble of machine learning models (Super Learner algorithm), SAPS II and SOFA scores. We used the Medical Information Mart for Intensive Care III (MIMIC-III) (v1.4) publicly available dataset, which includes all patients admitted to an ICU at the Beth Israel Deaconess Medical Center from 2001 to 2012, for the benchmarking tasks. Our results show that deep learning models consistently outperform all the other approaches especially when the `raw' clinical time series data is used as input features to the models.",0
"This paper presents benchmarks of state-of-the art deep learning models on large healthcare datasets from the Massachusetts Institute of Technology's Medical Information Mart for Intensive Care (MIMIC) III database. These models were tested on several clinical tasks such as mortality prediction, diagnosis classification, and medication dosage recommendation using physiological parameters, laboratory values, and vital signs collected from ICU patients. Results showed that the use of transfer learning techniques improved model performance compared to traditional training methods. Additionally, ensemble methods combining different architectures further increased predictive accuracy. These results demonstrate the potential value of deep learning approaches for improving patient care by analyzing complex medical data.",1
"The ability to compare two degenerate probability distributions (i.e. two probability distributions supported on two distinct low-dimensional manifolds living in a much higher-dimensional space) is a crucial problem arising in the estimation of generative models for high-dimensional observations such as those arising in computer vision or natural language. It is known that optimal transport metrics can represent a cure for this problem, since they were specifically designed as an alternative to information divergences to handle such problematic scenarios. Unfortunately, training generative machines using OT raises formidable computational and statistical challenges, because of (i) the computational burden of evaluating OT losses, (ii) the instability and lack of smoothness of these losses, (iii) the difficulty to estimate robustly these losses and their gradients in high dimension. This paper presents the first tractable computational method to train large scale generative models using an optimal transport loss, and tackles these three issues by relying on two key ideas: (a) entropic smoothing, which turns the original OT loss into one that can be computed using Sinkhorn fixed point iterations; (b) algorithmic (automatic) differentiation of these iterations. These two approximations result in a robust and differentiable approximation of the OT loss with streamlined GPU execution. Entropic smoothing generates a family of losses interpolating between Wasserstein (OT) and Maximum Mean Discrepancy (MMD), thus allowing to find a sweet spot leveraging the geometry of OT and the favorable high-dimensional sample complexity of MMD which comes with unbiased gradient estimates. The resulting computational architecture complements nicely standard deep network generative models by a stack of extra layers implementing the loss function.",0
"This paper presents a novel approach for learning generative models using sinkhorn divergences. We propose a new method that utilizes these divergences to regularize training procedures and improve model performance. Our experiments demonstrate improved results over current state-of-the-art methods across multiple tasks and datasets. Additionally, we provide theoretical analysis on why our approach works. In summary, this work provides an important contribution to the field by introducing a powerful tool for learning generative models.",1
"In the last few years, deep learning has led to very good performance on a variety of problems, such as visual recognition, speech recognition and natural language processing. Among different types of deep neural networks, convolutional neural networks have been most extensively studied. Leveraging on the rapid growth in the amount of the annotated data and the great improvements in the strengths of graphics processor units, the research on convolutional neural networks has been emerged swiftly and achieved state-of-the-art results on various tasks. In this paper, we provide a broad survey of the recent advances in convolutional neural networks. We detailize the improvements of CNN on different aspects, including layer design, activation function, loss function, regularization, optimization and fast computation. Besides, we also introduce various applications of convolutional neural networks in computer vision, speech and natural language processing.",0
"Deep learning has revolutionized fields such as computer vision, natural language processing, speech recognition, and robotics over recent years. At the core of many deep learning models lie convolutional neural networks (CNNs), which have emerged as one of the most popular architectures due to their strong performance on challenging tasks. Despite their successes, there are still open questions and limitations regarding CNNs that need addressing. This survey aims at providing both researchers and practitioners with insights into advancements made towards alleviating these issues and improving their effectiveness. We present a comprehensive literature review on novel techniques and innovations in different components of the traditional CNN architecture: filters, layers, activation functions, normalization, pooling operations, cost functions, loss functions, optimization methods, data augmentation strategies, regularization methods, ensemble methods, evaluation metrics, transfer/few shot learning methods, new applications, hardware acceleration, software frameworks and toolkits. By highlighting current trends, we aim to stimulate further progress in developing more efficient and advanced CNN architectures applicable to real-world problems. Our findings can serve as a reference guide helping stakeholders identify relevant papers in their area of interest and focus efforts accordingly to bring even stronger breakthroughs in the field. As a result, this work bridges the gap between state-of-the art research findings and practical use cases.",1
"In the present paper, we propose a source camera identification method for mobile devices based on deep learning. Recently, convolutional neural networks (CNNs) have shown a remarkable performance on several tasks such as image recognition, video analysis or natural language processing. A CNN consists on a set of layers where each layer is composed by a set of high pass filters which are applied all over the input image. This convolution process provides the unique ability to extract features automatically from data and to learn from those features. Our proposal describes a CNN architecture which is able to infer the noise pattern of mobile camera sensors (also known as camera fingerprint) with the aim at detecting and identifying not only the mobile device used to capture an image (with a 98\% of accuracy), but also from which embedded camera the image was captured. More specifically, we provide an extensive analysis on the proposed architecture considering different configurations. The experiment has been carried out using the images captured from different mobile devices cameras (MICHE-I Dataset was used) and the obtained results have proved the robustness of the proposed method.",0
"In recent years there has been growing interest in using deep neural networks (DNNs) for tasks related to media authentication [4]. These networks have achieved impressive results in various image analysis applications such as object detection, segmentation and classification [6][8], but their use for source camera identification remains largely unexplored. This work presents a deep learning method which utilizes convolutional neural network architecture based on VGGNet16 [7] and Siamese Network Architecture [9] to achieve efficient and accurate source camera identification from images taken by mobile phone cameras. Our approach was trained and validated against several benchmark datasets commonly used in digital forensics research: RECONSPECT [2],[3],[1]. We experimented with varying network architectures and data augmentations, eventually arriving at a model that achieves an accuracy rate of 85% across all benchmark sets. Overall, our findings demonstrate the feasibility and potential effectiveness of DNNs in solving real world problems associated with media authentication [5],[4]. Although further investigation is required to fully explore these models capabilities, we believe our approach represents an important step forward in advancing this area of study.",1
"NLP tasks are often limited by scarcity of manually annotated data. In social media sentiment analysis and related tasks, researchers have therefore used binarized emoticons and specific hashtags as forms of distant supervision. Our paper shows that by extending the distant supervision to a more diverse set of noisy labels, the models can learn richer representations. Through emoji prediction on a dataset of 1246 million tweets containing one of 64 common emojis we obtain state-of-the-art performance on 8 benchmark datasets within sentiment, emotion and sarcasm detection using a single pretrained model. Our analyses confirm that the diversity of our emotional labels yield a performance improvement over previous distant supervision approaches.",0
"This paper describes how large language models (LLMs) can be fine-tuned on hundreds of thousands of text examples with corresponding labels for sentiment and other attributes like emotions and sarcasm expressed via language and/or accompanying emojis. LLMs allow us to learn distributed representations that map texts, contextually coherent groups of tokens such as words, to numerical vectors which then serve as input features for predictive model architectures. Training over vast amounts of data results in rich feature representations that generalize well across domains and languages making them good candidates for tasks such as detecting sentiment, emotions, and sarcasm in short messages from social media platforms where users prefer using graphical symbols such as emoticons instead of textual content. Emoji representation techniques differ between tokenization methods: sequence or one hot encoding, but also embeddings which capture their meaning through dense vectorial representation. We show experimental validation through cross-validation on four different datasets; two publicly available - Twitter dataset and Instagram dataset as well as two new created ones: Twitter SMS and Twitter DMs - demonstrating outperformance compared to several baseline models as well as competitive performance against human annotators. With proper hyperparameter tuning and preprocessing our approach achieves state of the art results on three out of four datasets reaching near perfect scores on binary classification problems while surpassing human accuracy by 4% points on the more challenging multiclass problem (8 classes). Our findings show that transfer learning based on massive amounts of labeled data leads to high quality solutions applicable on various use cases involving online communication data",1
"Layered neural networks have greatly improved the performance of various applications including image processing, speech recognition, natural language processing, and bioinformatics. However, it is still difficult to discover or interpret knowledge from the inference provided by a layered neural network, since its internal representation has many nonlinear and complex parameters embedded in hierarchical layers. Therefore, it becomes important to establish a new methodology by which layered neural networks can be understood.   In this paper, we propose a new method for extracting a global and simplified structure from a layered neural network. Based on network analysis, the proposed method detects communities or clusters of units with similar connection patterns. We show its effectiveness by applying it to three use cases. (1) Network decomposition: it can decompose a trained neural network into multiple small independent networks thus dividing the problem and reducing the computation time. (2) Training assessment: the appropriateness of a trained result with a given hyperparameter or randomly chosen initial parameters can be evaluated by using a modularity index. And (3) data analysis: in practical data it reveals the community structure in the input, hidden, and output layers, which serves as a clue for discovering knowledge from a trained neural network.",0
"The ability of deep neural networks (DNN) to achieve state-of-the-art performance across many domains has sparked significant interest in recent years. However, understanding how these models work remains challenging due to their complex nature and opaque representations. To address this issue, we propose a modular representation of layered DNNs that facilitates interpretable decomposition of the model into smaller units. This representation allows us to break down a dense neural network into several simple modules, making it easier to analyze and understand each component's role in the overall system. We evaluate our method on several benchmark datasets, demonstrating its effectiveness in improving interpretability while maintaining high accuracy. Our findings have important implications for the development of more transparent machine learning systems, enabling better decision support and reducing reliance on black box predictions.",1
"Deep learning (DL) training-as-a-service (TaaS) is an important emerging industrial workload. The unique challenge of TaaS is that it must satisfy a wide range of customers who have no experience and resources to tune DL hyper-parameters, and meticulous tuning for each user's dataset is prohibitively expensive. Therefore, TaaS hyper-parameters must be fixed with values that are applicable to all users. IBM Watson Natural Language Classifier (NLC) service, the most popular IBM cognitive service used by thousands of enterprise-level clients around the globe, is a typical TaaS service. By evaluating the NLC workloads, we show that only the conservative hyper-parameter setup (e.g., small mini-batch size and small learning rate) can guarantee acceptable model accuracy for a wide range of customers. We further justify theoretically why such a setup guarantees better model convergence in general. Unfortunately, the small mini-batch size causes a high volume of communication traffic in a parameter-server based system. We characterize the high communication bandwidth requirement of TaaS using representative industrial deep learning workloads and demonstrate that none of the state-of-the-art scale-up or scale-out solutions can satisfy such a requirement. We then present GaDei, an optimized shared-memory based scale-up parameter server design. We prove that the designed protocol is deadlock-free and it processes each gradient exactly once. Our implementation is evaluated on both commercial benchmarks and public benchmarks to demonstrate that it significantly outperforms the state-of-the-art parameter-server based implementation while maintaining the required accuracy and our implementation reaches near the best possible runtime performance, constrained only by the hardware limitation. Furthermore, to the best of our knowledge, GaDei is the only scale-up DL system that provides fault-tolerance.",0
"Deep learning has revolutionized many fields by providing state-of-the-art results, but the complexity involved in building such models can make it difficult for non-experts to take advantage of these breakthroughs. This paper presents Gadei (Globally Aligned DAta Engineering Interface), a web service that enables easy scale-up training of deep learning models across multiple GPUs, without requiring any changes to your existing model codebase. By leveraging standard APIs like TensorFlow and PyTorch, Gadei provides a simple yet powerful interface for managing distributed training on commodity hardware or cloud platforms like Amazon Web Services (AWS) or Google Cloud Platform (GCP). In addition, Gadei offers several benefits over competing solutions, including automatic hyperparameter tuning via Monte Carlo Tree Search optimization methods, enabling faster convergence times at reduced costs. With Gadei, users can now seamlessly access advanced machine learning capabilities previously available only to those with extensive experience in data engineering and high performance computing.",1
"In this paper we propose a new approach to person re-identification using images and natural language descriptions. We propose a joint vision and language model based on CCA and CNN architectures to match across the two modalities as well as to enrich visual examples for which there are no language descriptions. We also introduce new annotations in the form of natural language descriptions for two standard Re-ID benchmarks, namely CUHK03 and VIPeR. We perform experiments on these two datasets with techniques based on CNN, hand-crafted features as well as LSTM for analysing visual and natural description data. We investigate and demonstrate the advantages of using natural language descriptions compared to attributes as well as CNN compared to LSTM in the context of Re-ID. We show that the joint use of language and vision can significantly improve the state-of-the-art performance on standard Re-ID benchmarks.",0
"""Person re-identification involves matching instances of the same person across different cameras in a network. Recent progresses have been made on using deep learning based methods for vision tasks such as object detection, image classification and tracking. In this work, we study how to use these approaches to tackle person re-identification, with particular emphasis on language."" Abstract: In recent years, there has been significant interest in developing algorithms that can identify and match individuals across multiple camera networks. This task, known as person re-identification (ReID), presents unique challenges due to variations in lighting conditions, occlusions, viewpoints, and other factors. Traditionally, researchers have approached ReID from a computer vision perspective, developing complex models that extract features from images and videos. However, there is growing evidence that incorporating natural language processing techniques may improve performance, particularly if we can learn representations that jointly capture visual and textual aspects of the data. In this work, we explore how state-of-the-art convolutional neural networks (CNNs) trained for image classification and object detection perform on benchmark datasets commonly used for evaluating person ReID systems. Our experiments show that while simple baseline CNNs achieve high accuracy on some datasets, fine-grained model selection is essential since improvements seen in other domains don't always carry over to ReID tasks. We further investigate ways to boost feature robustness by generating adversarial examples through small perturbations, which simulate noise introduced during testing scenarios like transmission errors in wireless sensor networks. Overall, our findings demonstrate the potential benefits of combining advances in both vision and language to advance real-world applications such as surveillance and security monitoring systems. Future work should focus on designing more sophisticated language-aware models that directly address common failure modes seen i",1
"Visual Question Answering (VQA) presents a unique challenge as it requires the ability to understand and encode the multi-modal inputs - in terms of image processing and natural language processing. The algorithm further needs to learn how to perform reasoning over this multi-modal representation so it can answer the questions correctly. This paper presents a survey of different approaches proposed to solve the problem of Visual Question Answering. We also describe the current state of the art model in later part of paper. In particular, the paper describes the approaches taken by various algorithms to extract image features, text features and the way these are employed to predict answers. We also briefly discuss the experiments performed to evaluate the VQA models and report their performances on diverse datasets including newly released VQA2.0[8].",0
"In recent years, there has been significant progress in the field of computer vision, particularly in the area of visual question answering (VQA). VQA involves understanding natural language questions posed by users and generating answers based on the content of images or videos. This survey paper provides an overview of some of the most recent advances made in VQA. These advances have focused on improving performance through new architectures and model designs as well as the development of more challenging benchmarks. Additionally, attention mechanisms, memory networks, and pretraining techniques have played important roles in these improvements. The paper presents state-of-the-art models that achieve high accuracy on popular VQA datasets while highlighting future research directions and open problems in the field. Overall, this survey provides valuable insights into current trends and developments in VQA, which may inspire further innovations in this exciting subfield of artificial intelligence.",1
"In recent years, deep learning (DL), a re-branding of neural networks (NNs), has risen to the top in numerous areas, namely computer vision (CV), speech recognition, natural language processing, etc. Whereas remote sensing (RS) possesses a number of unique challenges, primarily related to sensors and applications, inevitably RS draws from many of the same theories as CV; e.g., statistics, fusion, and machine learning, to name a few. This means that the RS community should be aware of, if not at the leading edge of, of advancements like DL. Herein, we provide the most comprehensive survey of state-of-the-art RS DL research. We also review recent new developments in the DL field that can be used in DL for RS. Namely, we focus on theories, tools and challenges for the RS community. Specifically, we focus on unsolved challenges and opportunities as it relates to (i) inadequate data sets, (ii) human-understandable solutions for modelling physical phenomena, (iii) Big Data, (iv) non-traditional heterogeneous data sources, (v) DL architectures and learning algorithms for spectral, spatial and temporal data, (vi) transfer learning, (vii) an improved theoretical understanding of DL systems, (viii) high barriers to entry, and (ix) training and optimizing the DL.",0
"This survey paper provides an overview of deep learning techniques that have been applied for remote sensing analysis tasks like image classification, object detection, segmentation, feature extraction etc. In recent years, deep learning has gained immense popularity among researchers due to their state-of-the-art performance on complex computer vision problems. However, applying these methods directly to satellite/airborne imagery often leads to poor results due to lack of labeled data, high dimensionality of features and the complexity of natural scenes represented by remotely sensed images. Therefore, this article discusses several open source tools, frameworks and libraries used in literature which have facilitated the use of DL models in remote sensing community. Additionally, challenges and future directions related to application of DL models in earth observation field including limitations related to large size of datasets, computational complexity, interpretability, uncertainty quantification, domain adaptation are discussed at length. Lastly, we highlight some of the real world applications where DL have shown great promises like precision agriculture, urban planning, climate change studies, geology, disaster management etc. To summarize, we believe that this work serves as a comprehensive resource for beginners as well as experienced practitioners interested in harnessing power of DNNs for solving Earth Observation problems.",1
"Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.",0
"An Introduction to Gated Graph Sequence Neural Networks =======================================================  In recent years, graph sequence neural networks (GSNN) have become increasingly popular as they offer an efficient solution to modeling complex interactions in dynamic environments. However, traditional GSNN methods often suffer from difficulties in handling memory constraints and learning long-term dependencies. To address these challenges, gated graph sequence neural networks (GGSNN) were introduced, which combine the power of gate mechanisms and the expressiveness of graph sequence models. This article presents an overview of GGSNNs, including their architecture, training process, advantages, and applications. We highlight how GGSNNs enable more effective representation learning by allowing selective attention to important features while forgetting irrelevant ones. Our experimental results demonstrate that GGSNNs achieve state-of-the-art performance on several benchmark datasets, validating the efficacy of our approach. By providing an accessible introduction to this burgeoning field, we aim to encourage further research in the development of GGSNN techniques for real-world problems.  ---  Please note: I am unable to provide suggestions regarding any specific papers related to ""gated graph sequence neural network"". If you can provide additional details such as your target audience, purpose of publication, etc., I may be able to help better. Please advise if this would be possible. Thank you!",1
"Natural language questions are inherently compositional, and many are most easily answered by reasoning about their decomposition into modular sub-problems. For example, to answer ""is there an equal number of balls and boxes?"" we can look for balls, look for boxes, count them, and compare the results. The recently proposed Neural Module Network (NMN) architecture implements this approach to question answering by parsing questions into linguistic substructures and assembling question-specific deep networks from smaller modules that each solve one subtask. However, existing NMN implementations rely on brittle off-the-shelf parsers, and are restricted to the module configurations proposed by these parsers rather than learning them from data. In this paper, we propose End-to-End Module Networks (N2NMNs), which learn to reason by directly predicting instance-specific network layouts without the aid of a parser. Our model learns to generate network structures (by imitating expert demonstrations) while simultaneously learning network parameters (using the downstream task loss). Experimental results on the new CLEVR dataset targeted at compositional question answering show that N2NMNs achieve an error reduction of nearly 50% relative to state-of-the-art attentional approaches, while discovering interpretable network architectures specialized for each question.",0
"This paper presents an approach to visual question answering using end-to-end module networks (E2EMN), which learn to reason through continuous interaction with humans in a lifelong learning paradigm. E2EMN consists of multiple reasoning modules that communicate seamlessly with each other, enabling them to exchange representations at different levels of abstraction. We demonstrate the effectiveness of our approach on several challenging benchmark datasets and show that our model outperforms previous state-of-the-art methods by significant margins. Our results suggest that the proposed methodology can enable agents to engage in more sophisticated forms of question-answering and problem-solving tasks, paving the way towards intelligent systems capable of human-like understanding and reasoning.",1
"Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are ""scaled exponential linear units"" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.",0
"Abstract: Deep learning has revolutionized many areas of artificial intelligence (AI), but traditional neural networks suffer from several challenges such as requiring large amounts of data and computational resources for training, overfitting, vanishing/exploding gradients, and poor generalization performance on unseen datasets. In recent years, self-normalizing neural networks have emerged as an alternative architecture that addresses these issues by incorporating built-in normalization mechanisms into each layer. This type of network learns a scaling parameter for each weight instead of relying solely on batch normalization, which can mitigate the need for oversampling/undersampling techniques commonly used to address class imbalance problems. We present an overview of current research in the field of self-normalizing neural networks, including their design principles, advantages, and potential applications across multiple domains. Our findings show that self-normalizing networks outperform conventional neural networks in terms of accuracy and stability while requiring fewer parameters and computational resources during both training and inference stages.",1
"Generating video descriptions in natural language (a.k.a. video captioning) is a more challenging task than image captioning as the videos are intrinsically more complicated than images in two aspects. First, videos cover a broader range of topics, such as news, music, sports and so on. Second, multiple topics could coexist in the same video. In this paper, we propose a novel caption model, topic-guided model (TGM), to generate topic-oriented descriptions for videos in the wild via exploiting topic information. In addition to predefined topics, i.e., category tags crawled from the web, we also mine topics in a data-driven way based on training captions by an unsupervised topic mining model. We show that data-driven topics reflect a better topic schema than the predefined topics. As for testing video topic prediction, we treat the topic mining model as teacher to train the student, the topic prediction model, by utilizing the full multi-modalities in the video especially the speech modality. We propose a series of caption models to exploit topic guidance, including implicitly using the topics as input features to generate words related to the topic and explicitly modifying the weights in the decoder with topics to function as an ensemble of topic-aware language decoders. Our comprehensive experimental results on the current largest video caption dataset MSR-VTT prove the effectiveness of our topic-guided model, which significantly surpasses the winning performance in the 2016 MSR video to language challenge.",0
"Here's my attempt at drafting an abstract without using ""this"" or referencing the paper title:  The task of automatically generating natural language descriptions of video content remains a significant challenge. In order to improve the quality of generated descriptions, we propose utilizing topic guidance to direct attention towards specific aspects of the scene that may otherwise go unmentioned. Our approach involves fine-tuning a pre-trained transformer model on a dataset consisting of pairs of videos and corresponding textual descriptions. The trained model takes as input both the visual features extracted from the video and a specified set of topics, which serve as constraints during generation. Experimental evaluations demonstrate that our method outperforms baseline models in terms of both objective metrics (such as BLEU score) and subjective user ratings. Overall, our work presents a promising step forward in creating more accurate and informative automatic video summarization systems.",1
"Taking an image and question as the input of our method, it can output the text-based answer of the query question about the given image, so called Visual Question Answering (VQA). There are two main modules in our algorithm. Given a natural language question about an image, the first module takes the question as input and then outputs the basic questions of the main given question. The second module takes the main question, image and these basic questions as input and then outputs the text-based answer of the main question. We formulate the basic questions generation problem as a LASSO optimization problem, and also propose a criterion about how to exploit these basic questions to help answer main question. Our method is evaluated on the challenging VQA dataset and yields state-of-the-art accuracy, 60.34% in open-ended task.",0
"Visual question answering (VQA) has emerged as one of the most challenging tasks in computer vision and natural language processing research. Its goal is to accurately generate descriptive answers to natural language questions posed on images, such as ""What is the woman wearing?"" or ""How many trees are there?"". Many current approaches use deep neural networks that require large amounts of training data, complex architectures, and computational resources to achieve state-of-the-art performance.  This paper proposes a new approach called Visual Question Answering by Basic Questions (VQABQ), which aims to simplify the task of visual question answering without sacrificing accuracy. Our method leverages basic questions related to each image in order to facilitate the learning process of both the image representation and the question answering model. Specifically, instead of directly predicting the final answer given an image and a question pair, our system first generates candidate solutions from the input sentence using a rule-based parser and then confirms whether these candidates match the ground truth answer through another round of reasoning. We evaluate our proposed method on several benchmark datasets and demonstrate that it achieves comparable results to other state-of-the-art methods while requiring significantly less computational resources and fine-tuning. Additionally, we perform an extensive analysis on the impact of different components within our framework and highlight some promising future directions for further improvement. Overall, VQABQ presents a more efficient and effective alternative to conventional VQA techniques, opening up opportunities for real-world applications where scalability and simplicity are essential requirements.",1
"Recurrent Neural Networks (RNNs) have been widely used in natural language processing and computer vision. Among them, the Hierarchical Multi-scale RNN (HM-RNN), a kind of multi-scale hierarchical RNN proposed recently, can learn the hierarchical temporal structure from data automatically. In this paper, we extend the work to solve the computer vision task of action recognition. However, in sequence-to-sequence models like RNN, it is normally very hard to discover the relationships between inputs and outputs given static inputs. As a solution, attention mechanism could be applied to extract the relevant information from input thus facilitating the modeling of input-output relationships. Based on these considerations, we propose a novel attention network, namely Hierarchical Multi-scale Attention Network (HM-AN), by combining the HM-RNN and the attention mechanism and apply it to action recognition. A newly proposed gradient estimation method for stochastic neurons, namely Gumbel-softmax, is exploited to implement the temporal boundary detectors and the stochastic hard attention mechanism. To amealiate the negative effect of sensitive temperature of the Gumbel-softmax, an adaptive temperature training method is applied to better the system performance. The experimental results demonstrate the improved effect of HM-AN over LSTM with attention on the vision task. Through visualization of what have been learnt by the networks, it can be observed that both the attention regions of images and the hierarchical temporal structure can be captured by HM-AN.",0
"In recent years, deep learning has emerged as one of the most powerful tools for solving complex problems in computer vision. Among these problems is action recognition, which involves identifying human actions from video data. Despite the progress made in this field, challenges remain due to variations in scale, pose, and viewpoint among other factors. To address these challenges, we propose a novel approach based on hierarchical multi-scale attention networks (HMMA). HMMA captures features at different scales using multiple parallel branches, followed by a global glimpse module that integrates contextual information from all branches. Our experiments show significant improvements over state-of-the-art methods across several benchmark datasets, demonstrating the effectiveness of our proposed method. Overall, our work highlights the importance of capturing spatiotemporal relationships in action recognition tasks, and offers new insights into the design of effective models for this purpose.",1
"Object recognition has become a crucial part of machine learning and computer vision recently. The current approach to object recognition involves Deep Learning and uses Convolutional Neural Networks to learn the pixel patterns of the objects implicitly through backpropagation. However, CNNs require thousands of examples in order to generalize successfully and often require heavy computing resources for training. This is considered rather sluggish when compared to the human ability to generalize and learn new categories given just a single example. Additionally, CNNs make it difficult to explicitly programmatically modify or intuitively interpret their learned representations.   We propose a computational model that can successfully learn an object category from as few as one example and allows its learning style to be tailored explicitly to a scenario. Our model decomposes each image into two attributes: shape and color distribution. We then use a Bayesian criterion to probabilistically determine the likelihood of each category. The model takes each factor into account based on importance and calculates the conditional probability of the object belonging to each learned category. Our model is not only applicable to visual scenarios, but can also be implemented in a broader and more practical scope of situations such as Natural Language Processing as well as other places where it is possible to retrieve and construct individual attributes. Because the only condition our model presents is the ability to retrieve and construct individual attributes such as shape and color, it can be applied to essentially any class of visual objects.",0
"Abstract: This paper proposes a novel approach to one-shot concept learning using evolutionary instinct development simulation. Inspired by the natural process of instinct formation in living organisms, we develop an algorithm that allows artificial agents to learn new concepts quickly and efficiently. Our method involves simulated genetic mutation and environmental selection pressures to evolve the agent's cognitive abilities, enabling them to generalize knowledge from limited data. We evaluate our proposed method on several benchmark tasks and demonstrate significant improvement over traditional approaches in terms of accuracy and efficiency. By leveraging insights from biological processes, our work contributes to the field of artificial intelligence and highlights the potential of evolutionary algorithms in solving complex problems.",1
"In this paper, we address the problem of spatio-temporal person retrieval from multiple videos using a natural language query, in which we output a tube (i.e., a sequence of bounding boxes) which encloses the person described by the query. For this problem, we introduce a novel dataset consisting of videos containing people annotated with bounding boxes for each second and with five natural language descriptions. To retrieve the tube of the person described by a given natural language query, we design a model that combines methods for spatio-temporal human detection and multimodal retrieval. We conduct comprehensive experiments to compare a variety of tube and text representations and multimodal retrieval methods, and present a strong baseline in this task as well as demonstrate the efficacy of our tube representation and multimodal feature embedding technique. Finally, we demonstrate the versatility of our model by applying it to two other important tasks.",0
"A spatial person retrieval system allows users to search through videos by specifying where and when they would like to see results. To develop such a system, we must first address two technical challenges: capturing high quality dense trajectories from videos that cover the entire time span of interest; and handling natural language queries that may refer to specific objects without providing their precise location or appearance characteristics. We build upon recent advances in video processing techniques such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), along with traditional computer vision approaches such as object detection, tracking, and matching algorithms. Our system consists of three components: a novel CNN architecture capable of accurate multi-object detection at multiple scales, which can localize instances using region proposal networks (RPNs); a RNN framework able to track objects across frames while associating them with unique identities even if they briefly disappear due to occlusions; and lastly, a query interface able to translate natural language descriptions into spatio-temporal requests. Experimental evaluation on benchmark datasets shows our method achieves state-of-the-art performance compared against competing systems. Overall, our work takes a significant step towards developing real-world applications of visual understanding technologies for searching large repositories of video data based on freeform textual prompts.",1
"When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that state of art deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.",0
"This study presents a novel approach to learning physics experiments using deep reinforcement learning algorithms. We propose a new method that utilizes a simulated environment to train agents to perform physical tasks such as rolling objects down inclined planes or bouncing balls off walls. Our method allows agents to learn through trial and error without explicit guidance from human experts, enabling them to discover important scientific principles underlying these experiments. By leveraging state-of-the-art computer vision techniques to measure the outcome of each experiment, we can evaluate the effectiveness of different strategies employed by our agents during learning. Finally, we demonstrate how our trained agents generalize well across diverse experimental settings and initial conditions, showcasing their robustness and versatility. Overall, our research offers a promising direction for developing intelligent systems capable of tackling complex science inquiry problems autonomously.",1
"Visual Question Answering (VQA) has attracted a lot of attention in both Computer Vision and Natural Language Processing communities, not least because it offers insight into the relationships between two important sources of information. Current datasets, and the models built upon them, have focused on questions which are answerable by direct analysis of the question and image alone. The set of such questions that require no external information to answer is interesting, but very limited. It excludes questions which require common sense, or basic factual knowledge to answer, for example. Here we introduce FVQA, a VQA dataset which requires, and supports, much deeper reasoning. FVQA only contains questions which require external information to answer.   We thus extend a conventional visual question answering dataset, which contains image-question-answerg triplets, through additional image-question-answer-supporting fact tuples. The supporting fact is represented as a structural triplet, such as Cat,CapableOf,ClimbingTrees.   We evaluate several baseline models on the FVQA dataset, and describe a novel model which is capable of reasoning about an image on the basis of supporting facts.",0
"This work presents a novel approach to visual question answering (VQA) that utilizes external fact sources to improve accuracy and diversity in answers. Unlike traditional VQA methods that only rely on image and textual evidence, our method integrates knowledge from external fact databases to provide more accurate and informative answers to questions posed over images. We introduce a new benchmark dataset called FactVQA consisting of real-world questions with factually verifiable answers. Extensive experimental results demonstrate that our model outperforms existing state-of-the-art VQA models by significantly reducing incorrect answers and improving overall accuracy while maintaining competitive performance on subjective metrics such as coherence and simplicity. Our approach opens up new possibilities for developing large-scale VQA systems that can access relevant background knowledge, making them better suited for practical applications like education and information retrieval.",1
"In this paper we present a new dataset and user simulator e-QRAQ (explainable Query, Reason, and Answer Question) which tests an Agent's ability to read an ambiguous text; ask questions until it can answer a challenge question; and explain the reasoning behind its questions and answer. The User simulator provides the Agent with a short, ambiguous story and a challenge question about the story. The story is ambiguous because some of the entities have been replaced by variables. At each turn the Agent may ask for the value of a variable or try to answer the challenge question. In response the User simulator provides a natural language explanation of why the Agent's query or answer was useful in narrowing down the set of possible answers, or not. To demonstrate one potential application of the e-QRAQ dataset, we train a new neural architecture based on End-to-End Memory Networks to successfully generate both predictions and partial explanations of its current understanding of the problem. We observe a strong correlation between the quality of the prediction and explanation.",0
"This paper presents e-QRAQ (Electronic Queries Real Answers Questions), a comprehensive dataset that contains multi-turn interactions between humans and machines. Our aim was to create a resource that could facilitate research on natural language processing tasks like question answering, text generation, and dialogue systems by providing diverse user inputs, correct answers, and detailed explanations. We believe that this dataset has the potential to foster advancements in human-machine interaction, particularly as artificial intelligence becomes more prevalent in our daily lives. In addition to discussing the dataset creation process and characteristics, we introduce a simulated environment designed to simplify data augmentation techniques and further enhance the capabilities of future applications built upon e-QRAQ. By making these resources freely available, we hope to encourage collaboration within the scientific community and accelerate the development of intelligent virtual assistants.",1
"In this paper we are interested in the problem of image segmentation given natural language descriptions, i.e. referring expressions. Existing works tackle this problem by first modeling images and sentences independently and then segment images by combining these two types of representations. We argue that learning word-to-image interaction is more native in the sense of jointly modeling two modalities for the image segmentation task, and we propose convolutional multimodal LSTM to encode the sequential interactions between individual words, visual information, and spatial information. We show that our proposed model outperforms the baseline model on benchmark datasets. In addition, we analyze the intermediate output of the proposed multimodal LSTM approach and empirically explain how this approach enforces a more effective word-to-image interaction.",0
"In recent years, there has been growing interest in developing algorithms that can automatically segment images into multiple regions based on their content. However, many existing methods rely heavily on manual annotations, which can be time consuming and expensive. To address this challenge, we propose a new approach that leverages recurrent multimodal interactions to learn representations of object shapes without explicit supervision. Our method uses a deep neural network to predict masks for objects in an image, and then refines these predictions by interacting with a user who provides feedback through natural language commands. We show that our approach significantly improves upon state-of-the-art methods for referring image segmentation tasks, achieving more accurate and detailed segmentations with fewer iterations. Additionally, we demonstrate the robustness of our system by evaluating it on a variety of challenging datasets and use cases. Overall, our work represents an important step towards enabling computers to interpret visual scenes at human levels of understanding and communication.",1
"We consider retrieving a specific temporal segment, or moment, from a video given a natural language text description. Methods designed to retrieve whole video clips with natural language determine what occurs in a video but not when. To address this issue, we propose the Moment Context Network (MCN) which effectively localizes natural language queries in videos by integrating local and global video features over time. A key obstacle to training our MCN model is that current video datasets do not include pairs of localized video segments and referring expressions, or text descriptions which uniquely identify a corresponding moment. Therefore, we collect the Distinct Describable Moments (DiDeMo) dataset which consists of over 10,000 unedited, personal videos in diverse visual settings with pairs of localized video segments and referring expressions. We demonstrate that MCN outperforms several baseline methods and believe that our initial results together with the release of DiDeMo will inspire further research on localizing video moments with natural language.",0
"Video moments can provide valuable insights into content and user engagement, but locating relevant segments within a longer video remains challenging. This work presents a novel approach that utilizes natural language queries to precisely localize moments in video. By bridging the gap between textual descriptions and visual media, our method offers greater precision than existing systems and enables users to access specific parts of videos quickly and efficiently. We demonstrate the effectiveness of our technique through extensive experiments on diverse datasets and showcase its potential applications in areas such as education, entertainment, and customer support. Our work contributes to the broader field of multimedia search by advancing the state-of-the-art in moment retrieval from video.",1
"This paper focuses on temporal localization of actions in untrimmed videos. Existing methods typically train classifiers for a pre-defined list of actions and apply them in a sliding window fashion. However, activities in the wild consist of a wide combination of actors, actions and objects; it is difficult to design a proper activity list that meets users' needs. We propose to localize activities by natural language queries. Temporal Activity Localization via Language (TALL) is challenging as it requires: (1) suitable design of text and video representations to allow cross-modal matching of actions and language queries; (2) ability to locate actions accurately given features from sliding windows of limited granularity. We propose a novel Cross-modal Temporal Regression Localizer (CTRL) to jointly model text query and video clips, output alignment scores and action boundary regression results for candidate clips. For evaluation, we adopt TaCoS dataset, and build a new dataset for this task on top of Charades by adding sentence temporal annotations, called Charades-STA. We also build complex sentence queries in Charades-STA for test. Experimental results show that CTRL outperforms previous methods significantly on both datasets.",0
"This paper presents a novel approach for temporally localizing activities using natural language queries. Our method, called TALL (Temporal Activity Localization via Language), combines language processing techniques with temporal reasoning methods to accurately identify relevant activity segments within a video sequence. We first use a pretrained language model to generate descriptive questions that summarize a given video segment. These questions serve as input to our system, which then utilizes a graph-based algorithm to efficiently explore possible answers by analyzing both the visual content of the video and the semantics encoded in the query. Through extensive experiments on several benchmark datasets, we demonstrate that TALL outperforms existing state-of-the-art approaches while providing interpretable outputs that can aid human annotators in their task. Additionally, we showcase two real-world applications where TALL could greatly benefit society: educational learning from instructional videos and improved accessibility for visually impaired individuals through audio descriptions of multimedia content.",1
"Deep learning has enabled major advances in the fields of computer vision, natural language processing, and multimedia among many others. Developing a deep learning system is arduous and complex, as it involves constructing neural network architectures, managing training/trained models, tuning optimization process, preprocessing and organizing data, etc. TensorLayer is a versatile Python library that aims at helping researchers and engineers efficiently develop deep learning systems. It offers rich abstractions for neural networks, model and data management, and parallel workflow mechanism. While boosting efficiency, TensorLayer maintains both performance and scalability. TensorLayer was released in September 2016 on GitHub, and has helped people from academia and industry develop real-world applications of deep learning.",0
"Title: TensorLayer: A Versatile Library for Efficient Deep Learning Development  Abstract: This paper introduces ""TensorLayer,"" an open source deep learning library designed for efficient development of machine learning models. As deep learning research progresses, there is an increasing need for tools that make prototyping and experimentation fast and easy, while providing flexibility and efficiency.  We describe the motivations behind creating TensorLayer as well as its key features. We demonstrate how users can quickly create neural network architectures from scratch using simple Python code and use built-in functionalities such as automatic differentiation, data augmentation, model saving/loading, and hyperparameter tuning. Additionally, we showcase several examples of advanced usages, including multi-GPU support, distributed training on clusters, dynamic architectures, and generative adversarial networks (GANs).  Our experiments compare TensorLayer against popular libraries such as PyTorch and TensorFlow. Results indicate that TensorLayer consistently performs better than these two packages in terms of speed and memory usage due to its optimized implementation of fundamental operations. Furthermore, our evaluation shows that TensorLayer maintains compatibility with existing deep learning frameworks, enabling seamless integration into existing workflows.  Overall, we believe that TensorLayer provides a valuable contribution to the machine learning community by offering a flexible and efficient framework for quick prototyping, scientific computing, and high performance computing workloads. With its focus on performance and simplicity, TensorLayer offers a versatile toolkit for both practitioners and researchers working in diverse domains within the umbrella field of artificial intelligence and beyond.",1
Recurrent neural networks show state-of-the-art results in many text analysis tasks but often require a lot of memory to store their weights. Recently proposed Sparse Variational Dropout eliminates the majority of the weights in a feed-forward neural network without significant loss of quality. We apply this technique to sparsify recurrent neural networks. To account for recurrent specifics we also rely on Binary Variational Dropout for RNN. We report 99.5% sparsity level on sentiment analysis task without a quality drop and up to 87% sparsity level on language modeling task with slight loss of accuracy.,0
"Bayesian sparsification of recurrent neural networks (RNN) presents a powerful methodology for compressing these models while maintaining their accuracy on prediction tasks. By utilizing Bayesian techniques such as variational inference and probabilistic modeling, we can learn which weights within RNNs contribute most significantly to the network output, allowing us to prune away insignificant connections. This approach can result in significant reduction in computational requirements without sacrificing predictive performance, making it particularly well suited for resource constrained environments where large scale RNN models cannot be practically deployed. In addition, by quantifying uncertainty about the pruning process, our framework provides valuable insight into the reliability of predictions produced by the compressed RNNs. We evaluate the effectiveness of our method using several benchmark datasets across different domains, demonstrating that we achieve better predictive performance than alternative approaches in many cases, while reducing model size substantially. Overall, our work represents an important contribution to the field of machine learning, offering a novel way of achieving more efficient use of resources while still delivering accurate predictions from complex systems such as RNNs.",1
"Deep models are the defacto standard in visual decision models due to their impressive performance on a wide array of visual tasks. However, they are frequently seen as opaque and are unable to explain their decisions. In contrast, humans can justify their decisions with natural language and point to the evidence in the visual world which led to their decisions. We postulate that deep models can do this as well and propose our Pointing and Justification (PJ-X) model which can justify its decision with a sentence and point to the evidence by introspecting its decision and explanation process using an attention mechanism. Unfortunately there is no dataset available with reference explanations for visual decision making. We thus collect two datasets in two domains where it is interesting and challenging to explain decisions. First, we extend the visual question answering task to not only provide an answer but also a natural language explanation for the answer. Second, we focus on explaining human activities which is traditionally more challenging than object classification. We extensively evaluate our PJ-X model, both on the justification and pointing tasks, by comparing it to prior models and ablations using both automatic and human evaluations.",0
"This short paper concerns how attentive explanations support decisions by pointing out relevant evidence (e.g., what was considered) in ways that provide transparency for audiences interested in understanding why these conclusions were reached. In so doing, decision processes can be traced through these justifications which helps stakeholders evaluate if those choices make sense given available data. Additionally, by highlighting relevant facts used as inputs into a choice process, others may consider or notice items they had initially missed but might have implications for those judgments. Ultimately, attention to these aspects makes evidentiary justifications less likely to overlook important details while making them more accountable. As such, explaining how certain deductions came about improves their reliability. To illustrate these points, several examples from different domains are discussed along with brief comments on related issues like scope restrictions. Overall, this analysis argues that attentiveness during explanation generation leads to enhanced transparency in reasoning—a key tenet of many professional practices—and ultimately heightened trustworthiness in those declarations. By examining instances where clear presentations successfully elucidate reasons underpinning particular actions or assessments plus demonstrating fallible ones which could benefit from greater precision, I contend that there exist tangible benefits to honing our abilities in offering detailed justifications that reveal substantial portions of their respective rationale.",1
"The paper presents a systematic review of state-of-the-art approaches to identify patient cohorts using electronic health records. It gives a comprehensive overview of the most commonly de-tected phenotypes and its underlying data sets. Special attention is given to preprocessing of in-put data and the different modeling approaches. The literature review confirms natural language processing to be a promising approach for electronic phenotyping. However, accessibility and lack of natural language process standards for medical texts remain a challenge. Future research should develop such standards and further investigate which machine learning approaches are best suited to which type of medical data.",0
"Electronic health records (EHR) have emerged as important resources for conducting research on patient populations. One common application of EHR data has been the detection of disease cohorts within large groups of patients. In order to identify these cohorts effectively, several different approaches have been developed and applied. This paper presents a systematic review of current literature in the field of ""Health Analytics"" to identify and characterize some of the most successful techniques for identifying patient cohorts from EHR data. Additionally, we aim to evaluate each approach according to both effectiveness and efficiency. Finally, we propose a novel framework that integrates multiple data sources and machine learning algorithms to improve performance compared to existing methods. Our comprehensive analysis offers valuable insights into the strengths and limitations of current approaches and serves as a starting point for future studies aimed at improving the accuracy and efficiency of EHR-based patient cohort identification. Ultimately, our findings could help accelerate biomedical discovery by facilitating the creation of well-defined cohorts based on diverse criteria.",1
"In this paper, we propose a way of synthesizing realistic images directly with natural language description, which has many useful applications, e.g. intelligent image manipulation. We attempt to accomplish such synthesis: given a source image and a target text description, our model synthesizes images to meet two requirements: 1) being realistic while matching the target text description; 2) maintaining other image features that are irrelevant to the text description. The model should be able to disentangle the semantic information from the two modalities (image and text), and generate new images from the combined semantics. To achieve this, we proposed an end-to-end neural architecture that leverages adversarial learning to automatically learn implicit loss functions, which are optimized to fulfill the aforementioned two requirements. We have evaluated our model by conducting experiments on Caltech-200 bird dataset and Oxford-102 flower dataset, and have demonstrated that our model is capable of synthesizing realistic images that match the given descriptions, while still maintain other features of original images.",0
"This should present the core idea behind our work without requiring any specialized knowledge in computer vision: Title: Semantic Image Synthesis via Adversarial Learning Abstract: Our paper presents a new method for generating synthetic images that capture specific semantic concepts. We use adversarial learning, wherein two deep neural networks compete against each other by predicting either real vs generated image pairings or the corresponding segmentation masks. During training, we progressively refine these generative models through iterative steps until they can reliably produce novel examples depicting targeted scenarios at high resolution. Our approach has applications in areas such as virtual world creation and content augmentation but could extend far beyond into scientific visualization, education, entertainment, and more.",1
"Inference in log-linear models scales linearly in the size of output space in the worst-case. This is often a bottleneck in natural language processing and computer vision tasks when the output space is feasibly enumerable but very large. We propose a method to perform inference in log-linear models with sublinear amortized cost. Our idea hinges on using Gumbel random variable perturbations and a pre-computed Maximum Inner Product Search data structure to access the most-likely elements in sublinear amortized time. Our method yields provable runtime and accuracy guarantees. Further, we present empirical experiments on ImageNet and Word Embeddings showing significant speedups for sampling, inference, and learning in log-linear models.",0
"This paper presents fast amortized inference and learning algorithms for log-linear models using randomly perturbed nearest neighbor search techniques. We introduce new methods that significantly improve upon existing approaches by reducing both computational complexity and memory usage, while maintaining high accuracy. Our experiments demonstrate substantial gains over baseline methods on real-world datasets in both speed and model quality metrics such as negative log likelihood (NLL) and perplexity. These improvements come at little cost: our approach achieves similar accuracy compared to state-of-the-art alternatives, making our method well-suited for large scale problems. Furthermore, we explore ways to learn the parameters directly from the data which can produce competitive results across different settings. By introducing random perturbations into nearest neighbor search, we achieve a balance between stability and noise sensitivity, ultimately leading to improved performance and generalization abilities in our algorithmic framework. Overall, our work offers an efficient and accurate solution to challenges faced in modern natural language processing tasks while paving the way towards more scalable solutions.",1
"The Recurrent Neural Networks and their variants have shown promising performances in sequence modeling tasks such as Natural Language Processing. These models, however, turn out to be impractical and difficult to train when exposed to very high-dimensional inputs due to the large input-to-hidden weight matrix. This may have prevented RNNs' large-scale application in tasks that involve very high input dimensions such as video modeling; current approaches reduce the input dimensions using various feature extractors. To address this challenge, we propose a new, more general and efficient approach by factorizing the input-to-hidden weight matrix using Tensor-Train decomposition which is trained simultaneously with the weights themselves. We test our model on classification tasks using multiple real-world video datasets and achieve competitive performances with state-of-the-art models, even though our model architecture is orders of magnitude less complex. We believe that the proposed approach provides a novel and fundamental building block for modeling high-dimensional sequential data with RNN architectures and opens up many possibilities to transfer the expressive and advanced architectures from other domains such as NLP to modeling high-dimensional sequential data.",0
"""This paper presents an approach for video classification using tensor trains and recurrent neural networks (RNNs). We describe a novel architecture that combines tensor train decompositions with RNN layers to capture temporal dependencies across frames of video data. Our method outperforms state-of-the-art techniques on benchmark datasets such as UCF101 and HMDB51. In addition to achieving superior accuracy, our model requires fewer parameters compared to other deep learning approaches, making it more efficient and suitable for real-world applications.""",1
"In this paper, we propose a new approach for retrieval of video segments using natural language queries. Unlike most previous approaches such as concept-based methods or rule-based structured models, the proposed method uses image captioning model to construct sentential queries for visual information. In detail, our approach exploits multiple captions generated by visual features in each image with `Densecap'. Then, the similarities between captions of adjacent images are calculated, which is used to track semantically similar captions over multiple frames. Besides introducing this novel idea of 'tracking by captioning', the proposed method is one of the first approaches that uses a language generation model learned by neural networks to construct semantic query describing the relations and properties of visual information. To evaluate the effectiveness of our approach, we have created a new evaluation dataset, which contains about 348 segments of scenes in 20 movie-trailers. Through quantitative and qualitative evaluation, we show that our method is effective for retrieval of video segments using natural language queries.",0
This should describe the content of your research without referring back to the title. Remember that the reader may never see the title before reading the abstract!,1
"We propose a new neural sequence model training method in which the objective function is defined by $\alpha$-divergence. We demonstrate that the objective function generalizes the maximum-likelihood (ML)-based and reinforcement learning (RL)-based objective functions as special cases (i.e., ML corresponds to $\alpha \to 0$ and RL to $\alpha \to1$). We also show that the gradient of the objective function can be considered a mixture of ML- and RL-based objective gradients. The experimental results of a machine translation task show that minimizing the objective function with $\alpha  0$ outperforms $\alpha \to 0$, which corresponds to ML-based methods.",0
"Here is an example of an abstract without paper title: This article introduces a new method called neural sequence model training via $α$-divergence minimization (NSM). NSM trains deep learning models to predict sequences by optimizing the log probability of the correct sequence while simultaneously minimizing the Kullback-Leibler divergence between the predicted distribution and the true distribution. This technique overcomes limitations such as incorrect predictions due to optimization problems and underfitting issues caused by limited context window size during inference. Our approach applies to both discrete and continuous distributions and outperforms other state-of-the-art methods on several benchmark datasets across different domains. The implications of our work extend beyond natural language processing applications to fields such as computer vision and speech recognition that rely on sequential data analysis. Overall, we believe that NSM has the potential to significantly improve many areas of artificial intelligence research and application. Please note that I have included keywords from your paper so you can easily identify them when reading further. Let me know if there are any specific requirements regarding length, tone, etc., which may differ compared to standard scientific articles. In case, please share some guidelines or reference papers/abstracts you want to follow in terms of these parameters. If not, the provided sample abstract should suffice to fulfill most general expectations of modern scientific writing style.  Regarding the submission process – at first glance, it seems you need to provide only an abstract for now, so please confirm beforehand after having reviewed my suggestions above. Then send your paper title along with the finalized text for consideration and approval by the conference committee.",1
"Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifications to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness, and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing (""Hacker's Delight"") programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization.",0
"""Learning to Superoptimize Programs"" presents a novel approach to program optimization using machine learning techniques. The paper begins by discussing the importance of efficient code execution in modern computing systems, particularly in high performance domains such as scientific simulations and artificial intelligence applications. While traditional methods for optimizing code have been effective in improving computational efficiency, these approaches can often become overwhelmingly complex due to their reliance on manual tuning, specialized domain knowledge, and trial-and-error testing.  The authors propose a solution that utilizes deep reinforcement learning algorithms to automatically optimize code at runtime. By training an agent to select the most computationally advantageous instructions and data layouts, the system can learn to generate highly optimized code without explicit guidance from human experts. This approach offers several benefits over existing methods: (a) automation of optimization workflows, reducing labor costs associated with manual tuning; (b) adaptability across diverse platforms, making it possible to produce optimal code for different hardware architectures; and (c) scalability, allowing optimization to handle more complex problems than currently feasible through hand engineering.  To demonstrate the effectiveness of their proposed technique, the authors present comprehensive experiments evaluating the learning process against real-world benchmarks written in popular programming languages like C++ and Python. Their results indicate significant improvements in both runtime speedup (ranging up to 4x faster) and code size reduction compared to non-optimized versions. These findings suggest promising potential for applying learned superoptimization to practical scenarios where high-performance computing demands low latency and resource utilization, paving the way for future research on integrating machine learning into software development pipelines.  Overall, ""Learning to S",1
"Recent work in computer vision has yielded impressive results in automatically describing images with natural language. Most of these systems generate captions in a sin- gle language, requiring multiple language-specific models to build a multilingual captioning system. We propose a very simple technique to build a single unified model across languages, using artificial tokens to control the language, making the captioning system more compact. We evaluate our approach on generating English and Japanese captions, and show that a typical neural captioning architecture is capable of learning a single model that can switch between two different languages.",0
"This research presents an approach to using artificial tokens to control languages for multilingual image caption generation. By developing a tokenizer that identifies key concepts within an image, we can generate descriptive and accurate language captions across multiple languages. Our method uses transfer learning techniques from pre-trained models to improve results, which enables us to create high quality, humanlike captions without requiring extensive training data. We demonstrate our model's effectiveness through comprehensive experiments on several datasets and show improved performance over baseline methods. Overall, our work advances the state of art for multilingual image captioning by enabling efficient and flexible cross-linguistic communication.",1
"Building neural networks to query a knowledge base (a table) with natural language is an emerging research topic in deep learning. An executor for table querying typically requires multiple steps of execution because queries may have complicated structures. In previous studies, researchers have developed either fully distributed executors or symbolic executors for table querying. A distributed executor can be trained in an end-to-end fashion, but is weak in terms of execution efficiency and explicit interpretability. A symbolic executor is efficient in execution, but is very difficult to train especially at initial stages. In this paper, we propose to couple distributed and symbolic execution for natural language queries, where the symbolic executor is pretrained with the distributed executor's intermediate execution results in a step-by-step fashion. Experiments show that our approach significantly outperforms both distributed and symbolic executors, exhibiting high accuracy, high learning efficiency, high execution efficiency, and high interpretability.",0
"In recent years, natural language processing has made significant strides due to advances in deep learning techniques such as neural machine translation, sentiment analysis, and text generation. However, there remains a need for more flexible querying systems that can handle complex tasks and generate coherent responses without relying solely on large pre-trained models. This work presents a novel approach that combines distributed and symbolic execution to address these challenges by enabling efficient handling of structured input, dynamic computation over symbolic expressions, and incorporation of external knowledge sources into the response process. Our system couples these components together using reinforcement learning principles to maximize user satisfaction and minimize runtime overhead. We evaluate our method through extensive experiments and demonstrate its effectiveness in generating accurate and concise answers across multiple domains. This work represents a step forward towards creating robust and scalable querying systems capable of handling real-world applications.",1
"Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.",0
"Title: ""An Overview of Multi-Task Learning in Deep Neural Networks""",1
"Neural networks trained on datasets such as ImageNet have led to major advances in visual object classification. One obstacle that prevents networks from reasoning more deeply about complex scenes and situations, and from integrating visual knowledge with natural language, like humans do, is their lack of common sense knowledge about the physical world. Videos, unlike still images, contain a wealth of detailed information about the physical world. However, most labelled video datasets represent high-level concepts rather than detailed physical aspects about actions and scenes. In this work, we describe our ongoing collection of the ""something-something"" database of video prediction tasks whose solutions require a common sense understanding of the depicted situation. The database currently contains more than 100,000 videos across 174 classes, which are defined as caption-templates. We also describe the challenges in crowd-sourcing this data at scale.",0
"Developing effective artificial intelligence systems that can perform tasks requiring everyday human knowledge remains a challenging problem in computer science. One reason for this difficulty is the limited availability of high-quality training data that captures the nuances of real world scenarios. In order to address this issue, we present the Something Something Video Database (SSVD), which provides researchers with a large collection of annotated videos covering a wide range of daily events and activities. By leveraging SSVD as both a source of training data and a benchmark for evaluation, we demonstrate significant improvements over previous state-of-the-art results on several key benchmark datasets, including VQA 2.0, GQA, and OKVQA. Our contributions showcase the importance of using rich multimodal information for advancing AI capabilities in understanding and reasoning about complex scenes in the physical world. The open release of SSVD aims to encourage further progress towards achieving robust visual commonsense by providing researchers access to a valuable resource for future work in this domain.",1
"Visual Question Answering (VQA) is a recent problem in computer vision and natural language processing that has garnered a large amount of interest from the deep learning, computer vision, and natural language processing communities. In VQA, an algorithm needs to answer text-based questions about images. Since the release of the first VQA dataset in 2014, additional datasets have been released and many algorithms have been proposed. In this review, we critically examine the current state of VQA in terms of problem formulation, existing datasets, evaluation metrics, and algorithms. In particular, we discuss the limitations of current datasets with regard to their ability to properly train and assess VQA algorithms. We then exhaustively review existing algorithms for VQA. Finally, we discuss possible future directions for VQA and image understanding research.",0
"Advances in computer vision have allowed machines to process images and videos more efficiently than ever before, leading to the development of visual question answering (VQA) systems that can automatically generate answers based on inputted images. VQA has become increasingly popular due to its potential applications in industries such as healthcare, education, and entertainment. This paper provides a comprehensive overview of existing datasets, algorithms, and future challenges facing the field of VQA. We begin by discussing current approaches to generating training data, including both manually created sets and automatic generation using human feedback. Next, we review state-of-the-art models used for predictive VQA, highlighting their strengths and weaknesses. Finally, we examine possible future directions in which VQA could evolve, exploring promising areas of research such as zero-shot learning and transfer learning. Overall, our goal is to provide insights into the current state of VQA, identify key challenges facing the field, and suggest fruitful paths for further investigation.",1
"Large collections of videos are grouped into clusters by a topic keyword, such as Eiffel Tower or Surfing, with many important visual concepts repeating across them. Such a topically close set of videos have mutual influence on each other, which could be used to summarize one of them by exploiting information from others in the set. We build on this intuition to develop a novel approach to extract a summary that simultaneously captures both important particularities arising in the given video, as well as, generalities identified from the set of videos. The topic-related videos provide visual context to identify the important parts of the video being summarized. We achieve this by developing a collaborative sparse optimization method which can be efficiently solved by a half-quadratic minimization algorithm. Our work builds upon the idea of collaborative techniques from information retrieval and natural language processing, which typically use the attributes of other similar objects to predict the attribute of a given object. Experiments on two challenging and diverse datasets well demonstrate the efficacy of our approach over state-of-the-art methods.",0
"In the age of digital media, video content has become increasingly prevalent as both individuals and organizations share their knowledge and experiences through visual means. As a result, there exists vast amounts of valuable information available online that can benefit from summarization techniques to extract meaningful insights and contextualize key concepts. Our research proposes collaborative summarization methods that leverage user feedback to improve upon automated algorithms in producing high-quality video synopses. We present three different collaborative models and analyze the impact each approach has on summary quality and human agreement ratings when compared against baseline summaries produced solely by machines. Through our experiments, we demonstrate the effectiveness of incorporating human input during the summarization process and showcase how this collaboration significantly enhances overall performance and improves user satisfaction. Furthermore, we discuss future directions and potential applications of our methodology towards supporting informed decision-making processes across diverse fields such as education, marketing, entertainment, etc.",1
"Kiva is an online non-profit crowdsouring microfinance platform that raises funds for the poor in the third world. The borrowers on Kiva are small business owners and individuals in urgent need of money. To raise funds as fast as possible, they have the option to form groups and post loan requests in the name of their groups. While it is generally believed that group loans pose less risk for investors than individual loans do, we study whether this is the case in a philanthropic online marketplace. In particular, we measure the effect of group loans on funding time while controlling for the loan sizes and other factors. Because loan descriptions (in the form of texts) play an important role in lenders' decision process on Kiva, we make use of this information through deep learning in natural language processing. In this aspect, this is the first paper that uses one of the most advanced deep learning techniques to deal with unstructured data in a way that can take advantage of its superior prediction power to answer causal questions. We find that on average, forming group loans speeds up the funding time by about 3.3 days.",0
"This paper presents a deep causal inference approach for measuring the effects of forming group loans on online non-profit microfinance platforms. We use counterfactual regression methods and data from a large online platform serving low-income entrepreneurs globally. Our results suggest that forming group loans has significant positive impacts on both loan repayment rates (14%) and average revenue growth per customer (6%). These findings provide important insights into how non-profits can design effective financial programs for underprivileged communities using digital technologies. By providing empirical evidence on the effectiveness of group lending mechanisms, we hope to inform public policy aimed at reducing poverty and promoting economic development through innovative fintech solutions.",1
"Automatically generating a natural language description of an image is a task close to the heart of image understanding. In this paper, we present a multi-model neural network method closely related to the human visual system that automatically learns to describe the content of images. Our model consists of two sub-models: an object detection and localization model, which extract the information of objects and their spatial relationship in images respectively; Besides, a deep recurrent neural network (RNN) based on long short-term memory (LSTM) units with attention mechanism for sentences generation. Each word of the description will be automatically aligned to different objects of the input image when it is generated. This is similar to the attention mechanism of the human visual system. Experimental results on the COCO dataset showcase the merit of the proposed method, which outperforms previous benchmark models.",0
"Title: Image Captioning with Object Detection and Localization Abstract: Image captioning is a challenging task that involves generating natural language descriptions of images. In recent years, there has been significant interest in developing computational models that can accurately describe images by predicting relevant textual phrases. One approach to image captioning is to use object detection and localization techniques to identify objects within an image and then generate descriptive captions based on these detected objects. This paper presents a novel model for image captioning that utilizes object detection and localization to improve the quality and accuracy of generated captions. Our proposed model leverages state-of-the-art computer vision algorithms to detect and locate objects within an image, and then uses sequence generation techniques to produce descriptive captions based on these identified objects. We evaluate our model using several standard benchmark datasets and demonstrate that it outperforms current state-of-the-art methods in terms of both quantitative metrics and human evaluation measures. Overall, our work shows that integrating object detection and localization into image captioning models can significantly improve their performance and lead to more accurate and detailed descriptions of images.",1
"Recent progress has been made in using attention based encoder-decoder framework for video captioning. However, most existing decoders apply the attention mechanism to every generated word including both visual words (e.g., ""gun"" and ""shooting"") and non-visual words (e.g. ""the"", ""a""). However, these non-visual words can be easily predicted using natural language model without considering visual signals or attention. Imposing attention mechanism on non-visual words could mislead and decrease the overall performance of video captioning. To address this issue, we propose a hierarchical LSTM with adjusted temporal attention (hLSTMat) approach for video captioning. Specifically, the proposed framework utilizes the temporal attention for selecting specific frames to predict the related words, while the adjusted temporal attention is for deciding whether to depend on the visual information or the language context information. Also, a hierarchical LSTMs is designed to simultaneously consider both low-level visual information and high-level language context information to support the video caption generation. To demonstrate the effectiveness of our proposed framework, we test our method on two prevalent datasets: MSVD and MSR-VTT, and experimental results show that our approach outperforms the state-of-the-art methods on both two datasets.",0
"Title: ""A hierarchical recurrent neural network (RNN) architecture with adjusted temporal attention mechanism for video caption generation.""  The task of generating natural language descriptions of videos is challenging due to the inherent complexity and variability present in visual data. In recent years, deep learning models have shown promising results on this task, particularly those based on sequential architectures such as RNNs and their variants like Long Short Term Memory networks (LSTM). Despite these advances, there remains room for improvement in terms of model performance and interpretability.  In this work, we propose a novel approach that leverages both spatial and temporal features extracted from the input video frames using pre-trained CNNs, which are then fed into a hierarchy of LSTM units with adjusted temporal attention mechanisms. Our proposed architecture captures both short-term and long-range dependencies across different levels of abstraction, allowing for better contextual understanding of the video content. Additionally, our method employs two forms of attention mechanisms - temporal and spatial attention - to focus on relevant frames and spatio-temporal regions within the video. These attentions are carefully calibrated to minimize redundancy and maximize informativeness in the generated captions. Finally, we evaluate our approach against state-of-the-art methods on popular benchmark datasets and show consistent improvements in both quantitative metrics and qualitative analyses.  Our contributions can be summarized as follows:  * A new hierarchical LSTM model designed specifically for video captioning, capable of capturing multi-level representations while maintaining computational efficiency. * An adjusted tempo",1
"Translating information between text and image is a fundamental problem in artificial intelligence that connects natural language processing and computer vision. In the past few years, performance in image caption generation has seen significant improvement through the adoption of recurrent neural networks (RNN). Meanwhile, text-to-image generation begun to generate plausible images using datasets of specific categories like birds and flowers. We've even seen image generation from multi-category datasets such as the Microsoft Common Objects in Context (MSCOCO) through the use of generative adversarial networks (GANs). Synthesizing objects with a complex shape, however, is still challenging. For example, animals and humans have many degrees of freedom, which means that they can take on many complex shapes. We propose a new training method called Image-Text-Image (I2T2I) which integrates text-to-image and image-to-text (image captioning) synthesis to improve the performance of text-to-image synthesis. We demonstrate that %the capability of our method to understand the sentence descriptions, so as to I2T2I can generate better multi-categories images using MSCOCO than the state-of-the-art. We also demonstrate that I2T2I can achieve transfer learning by using a pre-trained image captioning module to generate human images on the MPII Human Pose",0
"Incorporating text to image synthesis into traditional computer vision models often requires large amounts of labeled training data which can be costly, time consuming, and sometimes impossible to obtain. However, recent advances in generative adversarial networks have enabled the creation of synthetic images based on written descriptions alone. This method allows for more efficient use of resources as well as facilitates understanding of complex concepts by enabling visualization through natural language queries. Using the Generate Adversarial Networks (GAN) architecture we present I2T2I, an end-to-end system that generates high quality images directly from text prompts without the need for intermediate representations. We evaluate our model on several benchmark datasets including COCO and LSUN and show significant improvement over current state of the art methods. Additionally, we introduce a novel data augmentation technique called TI2I2TI which significantly improves performance. Finally, we provide detailed analysis of qualitative and quantitative results demonstrating the effectiveness of our approach. Overall, our work represents an important step towards making text to image synthesis widely available as a tool to aid researchers and practitioners in fields such as scientific visualization and education.",1
"With the novel and fast advances in the area of deep neural networks, several challenging image-based tasks have been recently approached by researchers in pattern recognition and computer vision. In this paper, we address one of these tasks, which is to match image content with natural language descriptions, sometimes referred as multimodal content retrieval. Such a task is particularly challenging considering that we must find a semantic correspondence between captions and the respective image, a challenge for both computer vision and natural language processing areas. For such, we propose a novel multimodal approach based solely on convolutional neural networks for aligning images with their captions by directly convolving raw characters. Our proposed character-based textual embeddings allow the replacement of both word-embeddings and recurrent neural networks for text understanding, saving processing time and requiring fewer learnable parameters. Our method is based on the idea of projecting both visual and textual information into a common embedding space. For training such embeddings we optimize a contrastive loss function that is computed to minimize order-violations between images and their respective descriptions. We achieve state-of-the-art performance in the largest and most well-known image-text alignment dataset, namely Microsoft COCO, with a method that is conceptually much simpler and that possesses considerably fewer parameters than current approaches.",0
"This should contain keywords from your paper: order, embeddings, character level, convolution, alignment, multimodal.",1
"Neural embeddings have been used with great success in Natural Language Processing (NLP). They provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph based tasks, embeddings have been learned in high-dimensional Euclidean spaces. However, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but negatively curved, hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that embedding graphs in their natural geometry significantly improves performance on downstream tasks for several real-world public datasets.",0
"In recent years, hyperbolic geometry has emerged as a powerful tool for modeling complex networks in a variety of fields including social sciences, computer science, and biology. One key challenge faced by researchers working on these models is how to effectively represent and manipulate large graphs using non-Euclidean mathematical spaces like hyperbolic space. This paper addresses that challenge by introducing neural embeddings of graphs into hyperbolic space (NEGHH). We provide both theoretical analysis and experimental validation of our method, demonstrating its effectiveness at representing high-dimensional data sets faithfully while reducing their dimensionality significantly. Our results show that NEGHH consistently outperforms competing approaches across several benchmark datasets. Given the ubiquity of graph structured data, we believe that our work represents an important step towards unlocking new capabilities for machine learning applications in real-world scenarios where low-rank approximations may greatly benefit from non-flat geometric representations. Additionally, we contribute code and supplemental materials to encourage future exploration and development of NEGHH methods beyond what appears herein.",1
"In this paper, we propose the first model to be able to generate visually grounded questions with diverse types for a single image. Visual question generation is an emerging topic which aims to ask questions in natural language based on visual input. To the best of our knowledge, it lacks automatic methods to generate meaningful questions with various types for the same visual input. To circumvent the problem, we propose a model that automatically generates visually grounded questions with varying types. Our model takes as input both images and the captions generated by a dense caption model, samples the most probable question types, and generates the questions in sequel. The experimental results on two real world datasets show that our model outperforms the strongest baseline in terms of both correctness and diversity with a wide margin.",0
"Effectively asking questions about visual data is a key component of many applications such as image and video search, caption generation, or summarization. In most cases, manually specifying these questions is time consuming, tedious, and error prone. Therefore, automatic question generators that can generate natural language questions given raw data would greatly improve efficiency and reduce errors. However, current methods often struggle with generating meaningful and grounded questions that accurately reflect the content of the input data. This paper proposes a novel approach for automatically generating grounded visual questions by leveraging convolutional neural networks (CNN) pretrained on large amounts of text and images to predict visually relevant regions that correspond to each generated word in a sequence of questions. Through extensive experiments using several benchmark datasets, we demonstrate our method’s effectiveness in generating diverse, accurate, and informative questions compared to previous state-of-the-art techniques. Our results showcase how well human reviewers evaluate our generated questions, indicating high quality and usability of generated queries for downstream vision tasks. Overall, our work represents a significant step towards automating the process of creating meaningful questions based on real-world visual inputs.",1
"Following the recent progress in image classification and captioning using deep learning, we develop a novel natural language person retrieval system based on an attention mechanism. More specifically, given the description of a person, the goal is to localize the person in an image. To this end, we first construct a benchmark dataset for natural language person retrieval. To do so, we generate bounding boxes for persons in a public image dataset from the segmentation masks, which are then annotated with descriptions and attributes using the Amazon Mechanical Turk. We then adopt a region proposal network in Faster R-CNN as a candidate region generator. The cropped images based on the region proposals as well as the whole images with attention weights are fed into Convolutional Neural Networks for visual feature extraction, while the natural language expression and attributes are input to Bidirectional Long Short- Term Memory (BLSTM) models for text feature extraction. The visual and text features are integrated to score region proposals, and the one with the highest score is retrieved as the output of our system. The experimental results show significant improvement over the state-of-the-art method for generic object retrieval and this line of research promises to benefit search in surveillance video footage.",0
"In recent years, natural language processing (NLP) has made significant progress towards achieving human-like understanding and generation abilities through various tasks such as sentiment analysis, machine translation, and question answering. While NLP models have become increasingly proficient at these individual tasks, they lack robustness in real world scenarios that involve multiple modalities, including textual information and images or videos. This shortcoming hinders their utility in applications where contextual information plays a crucial role, like image/video retrieval based on natural language queries, which remains one of the most challenging areas of computer vision research today. Recent advancements in attention-based methods, inspired by human perception mechanisms, show promise for addressing these limitations by allowing deep neural networks to selectively focus on relevant parts of the input data and generate more informed outputs accordingly. By combining state-of-the-art attentional architectures like the Transformer network from sequence-to-sequence modeling with region proposal algorithms commonly used in object detection systems, we present a novel framework called Attention-Based Natural Language Person Retrieval for matching people descriptions in both video frames and semantic space by exploiting crossmodal interactions between features extracted from textual queries and visual regions corresponding to individuals depicted therein. Our approach outperforms competitive baselines across diverse benchmarks, demonstrating the effectiveness of integrating attentional reasoning into multimodal learning settings while opening up exciting new possibilities for future work in this area.",1
"Deep Neural Networks have been shown to succeed at a range of natural language tasks such as machine translation and text summarization. While tasks on source code (ie, formal languages) have been considered recently, most work in this area does not attempt to capitalize on the unique opportunities offered by its known syntax and structure. In this work, we introduce SmartPaste, a first task that requires to use such information. The task is a variant of the program repair problem that requires to adapt a given (pasted) snippet of code to surrounding, existing source code. As first solutions, we design a set of deep neural models that learn to represent the context of each variable location and variable usage in a data flow-sensitive way. Our evaluation suggests that our models can learn to solve the SmartPaste task in many cases, achieving 58.6% accuracy, while learning meaningful representation of variable usages.",0
"""SmartPaste"" automatically adapts source code from one programming language (e.g., Python) into another target language (e.g., C++) without manual intervention. This saves programmers significant time and effort compared to traditional methods that require extensive hand editing, while producing better results than fully automated approaches such as machine translation. By learning to map across languages and incorporate user feedback along the way, SmartPaste achieves high accuracy and versatility through continuous improvement. Applicability extends beyond simple string matching, allowing for complex data structures like trees, graphs, or tables, enabling seamless integration into different programming paradigms and domains. Furthermore, our approach can generalize to multiple programming domains, reducing development time and cost for software development projects. Overall, SmartPaste offers new possibilities for cross-platform collaboration by facilitating efficient communication among developers working on different languages and platforms.",1
"Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.",0
This should look like an ordinary scientific abstract that would appear at the beginning of an academic research paper.,1
"A number of visual question answering approaches have been proposed recently, aiming at understanding the visual scenes by answering the natural language questions. While the image question answering has drawn significant attention, video question answering is largely unexplored.   Video-QA is different from Image-QA since the information and the events are scattered among multiple frames. In order to better utilize the temporal structure of the videos and the phrasal structures of the answers, we propose two mechanisms: the re-watching and the re-reading mechanisms and combine them into the forgettable-watcher model. Then we propose a TGIF-QA dataset for video question answering with the help of automatic question generation. Finally, we evaluate the models on our dataset. The experimental results show the effectiveness of our proposed models.",0
"In recent years, there has been significant progress in developing artificial intelligence (AI) models that can effectively answer questions based on visual inputs such as images and videos. However, many existing methods rely heavily on pretraining data, which may not always be available or might require large amounts of computational resources. To address these limitations, we propose a new model called the ""Forgettable-Watcher"" that combines forgetting mechanisms from memory-augmented neural networks with attention-based architectures commonly used in video question answering tasks. Our proposed model is capable of learning both short-term memories during inference and long-term knowledge stored within external memory. We evaluate our model on several benchmark datasets and demonstrate its effectiveness by achieving state-of-the-art performance while requiring less training time and significantly fewer parameters compared to previous approaches. This work provides important insights into designing efficient AI systems that can adaptively handle complex visual input without relying solely on massive amounts of pretraining data.",1
"Fine-grained image classification is a challenging task due to the large intra-class variance and small inter-class variance, aiming at recognizing hundreds of sub-categories belonging to the same basic-level category. Most existing fine-grained image classification methods generally learn part detection models to obtain the semantic parts for better classification accuracy. Despite achieving promising results, these methods mainly have two limitations: (1) not all the parts which obtained through the part detection models are beneficial and indispensable for classification, and (2) fine-grained image classification requires more detailed visual descriptions which could not be provided by the part locations or attribute annotations. For addressing the above two limitations, this paper proposes the two-stream model combining vision and language (CVL) for learning latent semantic representations. The vision stream learns deep representations from the original visual information via deep convolutional neural network. The language stream utilizes the natural language descriptions which could point out the discriminative parts or characteristics for each image, and provides a flexible and compact way of encoding the salient visual aspects for distinguishing sub-categories. Since the two streams are complementary, combining the two streams can further achieves better classification accuracy. Comparing with 12 state-of-the-art methods on the widely used CUB-200-2011 dataset for fine-grained image classification, the experimental results demonstrate our CVL approach achieves the best performance.",0
"Recent advancements in computer vision have enabled machines to learn visual representations that rival human performance on specific tasks such as object detection, semantic segmentation, and image classification. However, these approaches often rely heavily on large amounts of labeled data, making them difficult to apply to real world scenarios where labeling may be time consuming or even impossible. To address this challenge, we propose a novel framework for fine-grained image classification that combines both visual representation learning and natural language understanding. By incorporating linguistic knowledge into our model, we aim to improve the robustness and generalizability of the learned visual features. Our approach first extracts region proposals from the input image using a pretrained object detector, then models their interactions using convolutional neural networks (CNN). Finally, we use a recurrent neural network to encode the corresponding sequence of regions along with the sentence describing the task at hand into a fixed length vector, which can be used by any off-the-shelf classifier for evaluation. Experiment results show that our method outperforms state-of-the-art baseline methods on several benchmark datasets while requiring significantly less training data. This work demonstrates the feasibility of combining vision and language towards more effective image recognition systems.",1
"Most natural videos contain numerous events. For example, in a video of a ""man playing a piano"", the video might also contain ""another man dancing"" or ""a crowd clapping"". We introduce the task of dense-captioning events, which involves both detecting and describing events in a video. We propose a new model that is able to identify all events in a single pass of the video while simultaneously describing the detected events with natural language. Our model introduces a variant of an existing proposal module that is designed to capture both short as well as long events that span minutes. To capture the dependencies between the events in a video, our model introduces a new captioning module that uses contextual information from past and future events to jointly describe all events. We also introduce ActivityNet Captions, a large-scale benchmark for dense-captioning events. ActivityNet Captions contains 20k videos amounting to 849 video hours with 100k total descriptions, each with it's unique start and end time. Finally, we report performances of our model for dense-captioning events, video retrieval and localization.",0
"This work introduces dense captioning events in videos, a novel approach that involves using deep learning models to automatically generate detailed descriptions of actions and objects present in video frames. Our method leverages recent advances in image captioning and event detection techniques, enabling it to accurately capture both individual instances of objects and complex activities involving multiple actors and interactions. We demonstrate the effectiveness of our approach on several challenging benchmark datasets and show how it outperforms existing methods across a range of evaluation metrics. In addition, we provide insights into the role played by attention mechanisms in dense caption generation and analyze the impact of different model architectures and training strategies on performance. Overall, this work represents an important step towards realizing more accurate and robust scene understanding capabilities in artificial intelligence systems. This research presents a new technique called ""dense-captioning"" which aims at providing richer description of images, including their contextual details such as location, size, shape etc. The authors employ deep learning approaches, particularly Recurrent Neural Networks (RNN) and Transformers coupled with attention mechanism. They focus on improving state-of-art results obtained by competitive baselines like Up-Down, Top Down Captioning and others. Authors evaluate their system on popular datasets like COCO dataset, MSCOCO and Flickr8K. Their primary contribution relies in developing a multi-task framework named DECAFF, where other tasks can also be added to improve its overall performance. Further future works involve designing efficient algorithms for parallelizing the inference process on GPUs for faster computation times on larger datasets.",1
"Referring expressions are natural language constructions used to identify particular objects within a scene. In this paper, we propose a unified framework for the tasks of referring expression comprehension and generation. Our model is composed of three modules: speaker, listener, and reinforcer. The speaker generates referring expressions, the listener comprehends referring expressions, and the reinforcer introduces a reward function to guide sampling of more discriminative expressions. The listener-speaker modules are trained jointly in an end-to-end learning framework, allowing the modules to be aware of one another during learning while also benefiting from the discriminative reinforcer's feedback. We demonstrate that this unified framework and training achieves state-of-the-art results for both comprehension and generation on three referring expression datasets. Project and demo page: https://vision.cs.unc.edu/refer",0
"This research addresses the challenge of accurately identifying referring expressions, which are phrases that refer to specific entities within a conversation, by proposing a new model called the Joint Speaker-Listener-Reinforcer (JSLR) model. The JSLR model leverages deep learning techniques to jointly predict three key elements: speaker intent, listener context, and reinforcement signals, all of which play a crucial role in understanding referencing expressions during conversations. Experimental results demonstrate that the proposed approach significantly improves over existing methods on two benchmark datasets across multiple evaluation metrics, including precision, recall, F1 score, and BLEU scores. Overall, the JSLR model provides a more comprehensive solution for automatic referring expression recognition and has important implications for natural language processing applications such as virtual assistants, chatbots, and social media platforms.",1
"Associating image regions with text queries has been recently explored as a new way to bridge visual and linguistic representations. A few pioneering approaches have been proposed based on recurrent neural language models trained generatively (e.g., generating captions), but achieving somewhat limited localization accuracy. To better address natural-language-based visual entity localization, we propose a discriminative approach. We formulate a discriminative bimodal neural network (DBNet), which can be trained by a classifier with extensive use of negative samples. Our training objective encourages better localization on single images, incorporates text phrases in a broad range, and properly pairs image regions with text phrases into positive and negative examples. Experiments on the Visual Genome dataset demonstrate the proposed DBNet significantly outperforms previous state-of-the-art methods both for localization on single images and for detection on multiple images. We we also establish an evaluation protocol for natural-language visual detection.",0
"This paper presents a novel approach to visual localization and detection using discriminative bimodal networks with natural language queries. We propose a network architecture that combines multiple modalities such as RGB images, depth maps, and semantic segmentation masks to achieve better performance in object detection tasks. Our method utilizes a deep neural network trained on large datasets of labeled images to learn features that capture both high-level contextual information and low-level details of the scene. We then use these learned features to train our model for localizing objects within cluttered environments by posing natural language questions about their location. Experiments show that our method significantly outperforms state-of-the-art methods in terms of accuracy and speed while requiring fewer computational resources. Additionally, we demonstrate the generalizability of our approach across different types of environments, including indoor and outdoor scenes. Overall, this work represents a significant step forward in the field of computer vision and sets a new benchmark for object detection and localization tasks.",1
"We present a general approach to video understanding, inspired by semantic transfer techniques that have been successfully used for 2D image analysis. Our method considers a video to be a 1D sequence of clips, each one associated with its own semantics. The nature of these semantics -- natural language captions or other labels -- depends on the task at hand. A test video is processed by forming correspondences between its clips and the clips of reference videos with known semantics, following which, reference semantics can be transferred to the test video. We describe two matching methods, both designed to ensure that (a) reference clips appear similar to test clips and (b), taken together, the semantics of the selected reference clips is consistent and maintains temporal coherence. We use our method for video captioning on the LSMDC'16 benchmark, video summarization on the SumMe and TVSum benchmarks, Temporal Action Detection on the Thumos2014 benchmark, and sound prediction on the Greatest Hits benchmark. Our method not only surpasses the state of the art, in four out of five benchmarks, but importantly, it is the only single method we know of that was successfully applied to such a diverse range of tasks.",0
"This paper presents a novel approach for video analysis called Temporal Tessellation. Inspired by tessellations found in art and architecture, Temporal Tessellation divides videos into smaller non-overlapping regions that capture important events while preserving temporal continuity. By using a unified framework, we can efficiently analyze complex activities and events within scenes across different modalities such as audio and visual streams. Our method outperforms traditional approaches on benchmark datasets and demonstrates strong performance on challenging scenarios, including tracking objects in cluttered environments, handling occlusions, and capturing fine-grained motions. We believe our work advances the state-of-the-art in video understanding, paving the way towards more intelligent systems capable of reasoning about dynamic visual content.",1
"Accelerometer measurements are the prime type of sensor information most think of when seeking to measure physical activity. On the market, there are many fitness measuring devices which aim to track calories burned and steps counted through the use of accelerometers. These measurements, though good enough for the average consumer, are noisy and unreliable in terms of the precision of measurement needed in a scientific setting. The contribution of this paper is an innovative and highly accurate regression method which uses an intermediary two-stage classification step to better direct the regression of energy expenditure values from accelerometer counts.   We show that through an additional unsupervised layer of intermediate feature construction, we can leverage latent patterns within accelerometer counts to provide better grounds for activity classification than expert-constructed timeseries features. For this, our approach utilizes a mathematical model originating in natural language processing, the bag-of-words model, that has in the past years been appearing in diverse disciplines outside of the natural language processing field such as image processing. Further emphasizing the natural language connection to stochastics, we use a gaussian mixture model to learn the dictionary upon which the bag-of-words model is built. Moreover, we show that with the addition of these features, we're able to improve regression root mean-squared error of energy expenditure by approximately 1.4 units over existing state-of-the-art methods.",0
"This study proposes a novel application of bag-of-words (BoW) methods to accelerometer measurements as a means of classifying and estimating energy expenditure. Previous research has shown that BoW techniques have been successful in image recognition tasks by representing images as a collection of visual words and assigning weights to each word based on their occurrence within an image. Here, we apply these principles to accelerometry data collected from wearable sensors during physical activity. Our approach first extracts features from raw acceleration signals using a time-domain representation, then applies BoW models to cluster similar segments of movement into distinct movements, and finally uses machine learning algorithms to estimate energy expenditure levels for these activities. Results demonstrate that our method outperforms traditional approaches while providing interpretable insights into human movement patterns. These findings provide important implications for future applications such as personalized health monitoring and optimization of exercise regimens.",1
"Neural image/video captioning models can generate accurate descriptions, but their internal process of mapping regions to words is a black box and therefore difficult to explain. Top-down neural saliency methods can find important regions given a high-level semantic task such as object classification, but cannot use a natural language sentence as the top-down input for the task. In this paper, we propose Caption-Guided Visual Saliency to expose the region-to-word mapping in modern encoder-decoder networks and demonstrate that it is learned implicitly from caption training data, without any pixel-level annotations. Our approach can produce spatial or spatiotemporal heatmaps for both predicted captions, and for arbitrary query sentences. It recovers saliency without the overhead of introducing explicit attention layers, and can be used to analyze a variety of existing model architectures and improve their design. Evaluation on large-scale video and image datasets demonstrates that our approach achieves comparable captioning performance with existing methods while providing more accurate saliency heatmaps. Our code is available at visionlearninggroup.github.io/caption-guided-saliency/.",0
"This paper presents a novel approach for salient object detection that leverages top-down knowledge from natural language captions to guide visual attention. We propose a two-stage framework where, firstly, we generate high-quality bounding boxes based on textual descriptions provided in captions. Secondly, we employ these semantic regions to predict human fixation maps over images. Our method effectively utilizes linguistic context to enhance existing bottom-up features and achieves state-of-the-art results across multiple benchmark datasets. Extensive experiments demonstrate the effectiveness of our approach and reveal insights into how linguistic cues can improve computer vision tasks such as image classification and object detection.",1
"Image captioning is a challenging problem owing to the complexity in understanding the image content and diverse ways of describing it in natural language. Recent advances in deep neural networks have substantially improved the performance of this task. Most state-of-the-art approaches follow an encoder-decoder framework, which generates captions using a sequential recurrent prediction model. However, in this paper, we introduce a novel decision-making framework for image captioning. We utilize a ""policy network"" and a ""value network"" to collaboratively generate captions. The policy network serves as a local guidance by providing the confidence of predicting the next word according to the current state. Additionally, the value network serves as a global and lookahead guidance by evaluating all possible extensions of the current state. In essence, it adjusts the goal of predicting the correct words towards the goal of generating captions similar to the ground truth captions. We train both networks using an actor-critic reinforcement learning model, with a novel reward defined by visual-semantic embedding. Extensive experiments and analyses on the Microsoft COCO dataset show that the proposed framework outperforms state-of-the-art approaches across different evaluation metrics.",0
"In recent years, deep reinforcement learning has emerged as a powerful technique for generating natural language descriptions of images. However, most existing methods rely on hand-engineered features or human annotations to guide the learning process. This can lead to limited performance and scalability. To address these limitations, we propose a novel framework that uses embedding rewards to directly optimize the generation of image captions based on their semantic similarity to reference captions. Our approach leverages state-of-the-art convolutional neural networks (CNNs) to encode both images and captions into continuous vector representations that capture high-level semantics. We then use reinforcement learning algorithms to maximize the cosine similarity between the predicted image embeddings and the corresponding reference embeddings. By doing so, our method learns to generate descriptive captions that closely match human judgment. Our experiments on two large benchmark datasets demonstrate significant improvements over baseline models across multiple evaluation metrics, including automatic scores, user studies, and diversity measures. Overall, our work represents an important step toward more effective and generalizable solutions for the challenging task of image captioning.",1
"This paper presents a new baseline for visual question answering task. Given an image and a question in natural language, our model produces accurate answers according to the content of the image. Our model, while being architecturally simple and relatively small in terms of trainable parameters, sets a new state of the art on both unbalanced and balanced VQA benchmark. On VQA 1.0 open ended challenge, our model achieves 64.6% accuracy on the test-standard set without using additional data, an improvement of 0.4% over state of the art, and on newly released VQA 2.0, our model scores 59.7% on validation set outperforming best previously reported results by 0.5%. The results presented in this paper are especially interesting because very similar models have been tried before but significantly lower performance were reported. In light of the new results we hope to see more meaningful research on visual question answering in the future.",0
"Visual question answering (VQA) is a challenging task that requires models to both understand natural language questions and interpret visual content, such as images. In recent years, many approaches have been proposed to tackle VQA, but there remains a need for simple yet effective baselines. This paper introduces Show, Ask, Attend, and Answer (SAA), a strong baseline model for VQA that outperforms existing methods on several benchmark datasets. SAA consists of three main components: a scene graph constructor, a question reader, and a Q&A module. The scene graph constructor extracts a representation of the input image by constructing a scene graph of objects and relationships, while the question reader generates a fixed-length context vector from the textual query. These representations are then used by the Q&A module to generate answers to the given questions. Experimental results demonstrate that SAA achieves state-of-the-art performance across multiple metrics and datasets. Our analysis shows that SAA is robust to noisy questions and able to generalize well to unseen test data. Overall, SAA provides a strong baseline for future research in VQA and demonstrates the effectiveness of combining intuitive modules with deep learning techniques.",1
"This workshop explores the interface between cognitive neuroscience and recent advances in AI fields that aim to reproduce human performance such as natural language processing and computer vision, and specifically deep learning approaches to such problems.   When studying the cognitive capabilities of the brain, scientists follow a system identification approach in which they present different stimuli to the subjects and try to model the response that different brain areas have of that stimulus. The goal is to understand the brain by trying to find the function that expresses the activity of brain areas in terms of different properties of the stimulus. Experimental stimuli are becoming increasingly complex with more and more people being interested in studying real life phenomena such as the perception of natural images or natural sentences. There is therefore a need for a rich and adequate vector representation of the properties of the stimulus, that we can obtain using advances in machine learning.   In parallel, new ML approaches, many of which in deep learning, are inspired to a certain extent by human behavior or biological principles. Neural networks for example were originally inspired by biological neurons. More recently, processes such as attention are being used which have are inspired by human behavior. However, the large bulk of these methods are independent of findings about brain function, and it is unclear whether it is at all beneficial for machine learning to try to emulate brain function in order to achieve the same tasks that the brain achieves.",0
"This work explores the intersection of artificial intelligence and neuroscience through the use of representation learning techniques applied to both biological neural networks in the human brain as well as artificial neural networks used in machine learning algorithms. By comparing the processes by which these different types of systems learn from data, we aim to gain insights into how our brains process and represent information, as well as improve our understanding of how we can create more effective machine learning models. Our findings demonstrate that both biological and artificial neural networks share similar characteristics in their ability to extract high-level representations from raw input data, suggesting there may be universal principles governing representation learning across multiple domains. This research has implications not only for advancing AI technology but also for improving our understanding of cognition and behavior.",1
"Recent progress on image captioning has made it possible to generate novel sentences describing images in natural language, but compressing an image into a single sentence can describe visual content in only coarse detail. While one new captioning approach, dense captioning, can potentially describe images in finer levels of detail by captioning many regions within an image, it in turn is unable to produce a coherent story for an image. In this paper we overcome these limitations by generating entire paragraphs for describing images, which can tell detailed, unified stories. We develop a model that decomposes both images and paragraphs into their constituent parts, detecting semantic regions in images and using a hierarchical recurrent neural network to reason about language. Linguistic analysis confirms the complexity of the paragraph generation task, and thorough experiments on a new dataset of image and paragraph pairs demonstrate the effectiveness of our approach.",0
"Automatic image description generation has been receiving increasing attention recently due to its potential applications such as assisting visually impaired users and content retrieval. In this work we present a hierarchical approach for generating descriptive paragraphs of images. Our proposed method first generates detailed semantic segmentation results which are then used in combination with visual relationships learned from human annotations to generate high quality descriptions of objects and their interactions within the scene. We evaluate our model on two benchmark datasets achieving state-of-the-art performance and demonstrate its generalization capabilities by testing on real-world images. Finally, we conduct qualitative analysis showing that our method can effectively describe both individual objects and their spatial arrangements within a scene. Overall, our work provides a significant contribution towards building large-scale automatic systems capable of understanding and describing complex scenes in natural images.",1
"Learning how to generate descriptions of images or videos received major interest both in the Computer Vision and Natural Language Processing communities. While a few works have proposed to learn a grounding during the generation process in an unsupervised way (via an attention mechanism), it remains unclear how good the quality of the grounding is and whether it benefits the description quality. In this work we propose a movie description model which learns to generate description and jointly ground (localize) the mentioned characters as well as do visual co-reference resolution between pairs of consecutive sentences/clips. We also propose to use weak localization supervision through character mentions provided in movie descriptions to learn the character grounding. At training time, we first learn how to localize characters by relating their visual appearance to mentions in the descriptions via a semi-supervised approach. We then provide this (noisy) supervision into our description model which greatly improves its performance. Our proposed description model improves over prior work w.r.t. generated description quality and additionally provides grounding and local co-reference resolution. We evaluate it on the MPII Movie Description dataset using automatic and human evaluation measures and using our newly collected grounding and co-reference data for characters.",0
"Incorporate relevant key terms from the field (e.g., deep learning, computer vision). The aim of our work is to generate descriptions that accurately convey the presence and location of objects within images, while concurrently identifying individuals present in those scenes and referring to them coherently throughout subsequent text. We draw inspiration from prior successes in object detection using convolutional neural networks, particularly SSD and Faster R-CNN, which have achieved state-of-the-art results on several benchmark datasets. Additionally, we leverage recent advancements in co-reference resolution, which has improved our ability to disambiguate pronouns in natural language. By integrating these disparate elements, our model generates detailed and accurate descriptions of complex visual scenes containing multiple individuals, referenced consistently throughout their corresponding texts. Through extensive experimentation across two challenging datasets, we demonstrate substantial improvements over baseline models, establishing the efficacy and promise of our methodology.",1
"Searching persons in large-scale image databases with the query of natural language description has important applications in video surveillance. Existing methods mainly focused on searching persons with image-based or attribute-based queries, which have major limitations for a practical usage. In this paper, we study the problem of person search with natural language description. Given the textual description of a person, the algorithm of the person search is required to rank all the samples in the person database then retrieve the most relevant sample corresponding to the queried description. Since there is no person dataset or benchmark with textual description available, we collect a large-scale person description dataset with detailed natural language annotations and person samples from various sources, termed as CUHK Person Description Dataset (CUHK-PEDES). A wide range of possible models and baselines have been evaluated and compared on the person search benchmark. An Recurrent Neural Network with Gated Neural Attention mechanism (GNA-RNN) is proposed to establish the state-of-the art performance on person search.",0
"This paper presents a natural language based approach to person search which allows users to specify their query using any combination of name, age, gender, occupation, education, employer, location, nationality, relation and date range constraints. Our system retrieves images from a large dataset containing tens of millions of faces and compares them against those queries. For each candidate retrieved by our search engine we output image similarity scores as well as relevance scores obtained from fusing different modalities such as textual description similarities, face recognition confidences etc. We propose novel deep neural networks for both tasks that achieve state of the art performance on multiple benchmark datasets. Finally, we present experimental results indicating that our system can effectively rank correct matches for most queries among top few candidates, making it extremely useful in applications like finding missing persons, identifying imposters, verifying social media accounts of celebrities etc. Overall, our method offers a simple yet effective solution to a very important problem by leveraging advancements made recently in computer vision research.  Keywords: Face Recognition; Multi-modal Matching; Deep Learning; Ranking and Retrieval (search); Image Databases; Human Identification.",1
"An important, yet largely unstudied, problem in student data analysis is to detect misconceptions from students' responses to open-response questions. Misconception detection enables instructors to deliver more targeted feedback on the misconceptions exhibited by many students in their class, thus improving the quality of instruction. In this paper, we propose a new natural language processing-based framework to detect the common misconceptions among students' textual responses to short-answer questions. We propose a probabilistic model for students' textual responses involving misconceptions and experimentally validate it on a real-world student-response dataset. Experimental results show that our proposed framework excels at classifying whether a response exhibits one or more misconceptions. More importantly, it can also automatically detect the common misconceptions exhibited across responses from multiple students to multiple questions; this property is especially important at large scale, since instructors will no longer need to manually specify all possible misconceptions that students might exhibit.",0
"This paper describes how data mining techniques can be used to extract insights from large corpora of text written by students. It identifies patterns of incorrect reasoning that persist across many examples of student writing on a variety of topics. By analyzing these errors at scale, teachers may develop targeted interventions designed to correct those prevalent issues faced by their learners. Overall, the study shows promising results for using big data approaches to unlock new knowledge in educational settings, as well as refine classroom instruction and curriculum design. The paper titled ""Data-Mining Textual Responses to Uncover Misconception Patterns"" explores how data mining methods can assist educators in identifying common misconceptions among students through analysis of textual responses. Through examination of extensive collections of student work across multiple subjects, researchers were able to detect recurring mistaken assumptions made by learners. These findings indicate that the implementation of data analytics tools has the potential to enhance teaching strategies and improve education outcomes. Ultimately, this study underscores the potential value of applying advanced technologies in education while emphasizing the importance of further investigation into the subject matter.",1
"Integrating visual and linguistic information into a single multimodal representation is an unsolved problem with wide-reaching applications to both natural language processing and computer vision. In this paper, we present a simple method to build multimodal representations by learning a language-to-vision mapping and using its output to build multimodal embeddings. In this sense, our method provides a cognitively plausible way of building representations, consistent with the inherently re-constructive and associative nature of human memory. Using seven benchmark concept similarity tests we show that the mapped vectors not only implicitly encode multimodal information, but also outperform strong unimodal baselines and state-of-the-art multimodal methods, thus exhibiting more ""human-like"" judgments---particularly in zero-shot settings.",0
"""Predicting the outcome has always been a challenging task but can yield significant benefits if done accurately. The use of machine learning algorithms such as deep neural networks can greatly increase our ability to predict outcomes by analyzing large amounts of data. In the field of multimedia analytics, generating multimodal embeddings that capture relationships across different modalities remains a difficult problem due to differences in modal characteristics and variations in data distributions. This work proposes a fast re-construction method (FRM) to generate high dimensional representations from raw input data, allowing us to effectively learn complex patterns in multi-modal datasets. Our proposed FRM approach involves training models using reconstruction losses to map inputs to corresponding outputs while optimizing computational efficiency. We demonstrate the effectiveness of our method through experiments on several benchmark datasets.""",1
"Large-scale deep neural networks (DNN) have been successfully used in a number of tasks from image recognition to natural language processing. They are trained using large training sets on large models, making them computationally and memory intensive. As such, there is much interest in research development for faster training and test time. In this paper, we present a unique approach using lower precision weights for more efficient and faster training phase. We separate imagery into different frequency bands (e.g. with different information content) such that the neural net can better learn using less bits. We present this approach as a complement existing methods such as pruning network connections and encoding learning weights. We show results where this approach supports more stable learning with 2-4X reduction in precision with 17X reduction in DNN parameters.",0
"Low precision neural networks (LPNNs) have been shown to provide comparable results to high precision counterparts while offering significant energy efficiency advantages in training and deployment stages. In order to further enhance their performance in resource constrained environments such as edge computing devices and mobile platforms, we propose a novel LPNN architecture that exploits subband decomposition techniques to effectively learn from low precision data representations. We begin by presenting our methodology which involves partitioning input data into non-overlapping frequency bands using the discrete wavelet transform followed by quantization down to one bit levels. Next, we utilize popular backpropagation based algorithms to train these lightweight LPNN models. Our experimental evaluation on several benchmark datasets demonstrates up to two orders of magnitude reduction in model size, power consumption and execution time compared to high precision models at similar accuracy levels, making them well suited for use in low end hardware systems. Finally, our proposed framework can support flexible tradeoff between accuracy, latency and energy requirements via efficient configuration parameter management and online selection strategies that enable seamless adaptation according to dynamic application context demands. This research offers promising solutions for building future generation applications requiring realtime machine intelligence processing capabilities within tight constraints in terms of memory space, computational resources and battery life cycle.",1
"Predicting business process behaviour is an important aspect of business process management. Motivated by research in natural language processing, this paper describes an application of deep learning with recurrent neural networks to the problem of predicting the next event in a business process. This is both a novel method in process prediction, which has largely relied on explicit process models, and also a novel application of deep learning methods. The approach is evaluated on two real datasets and our results surpass the state-of-the-art in prediction precision.",0
"Artificial intelligence (AI) has become increasingly prevalent in many aspects of modern society. In particular, deep learning, a subfield of machine learning that uses neural networks with multiple layers to learn from large datasets, has shown great promise in improving predictive models across various fields. In this study, we aimed to investigate how well artificial neural networks can accurately predict process behaviours by training them on data collected from various sources such as sensors, computer logs and other forms of system inputs. We evaluated our approach on simulated datasets and real data collected from industrial processes, demonstrating their effectiveness and potential impact in reducing waste and inefficiencies. Our results indicate that these methods have great potential in improving predictive model performance, providing valuable insights for decision making and optimising production workflows in the future.",1
"We propose an end-to-end approach to the natural language object retrieval task, which localizes an object within an image according to a natural language description, i.e., referring expression. Previous works divide this problem into two independent stages: first, compute region proposals from the image without the exploration of the language description; second, score the object proposals with regard to the referring expression and choose the top-ranked proposals. The object proposals are generated independently from the referring expression, which makes the proposal generation redundant and even irrelevant to the referred object. In this work, we train an agent with deep reinforcement learning, which learns to move and reshape a bounding box to localize the object according to the referring expression. We incorporate both the spatial and temporal context information into the training procedure. By simultaneously exploiting local visual information, the spatial and temporal context and the referring language a priori, the agent selects an appropriate action to take at each time. A special action is defined to indicate when the agent finds the referred object, and terminate the procedure. We evaluate our model on various datasets, and our algorithm significantly outperforms the compared algorithms. Notably, the accuracy improvement of our method over the recent method GroundeR and SCRC on the ReferItGame dataset are 7.67% and 18.25%, respectively.",0
"This paper proposes a new approach to natural language object retrieval (NLO) that integrates deep reinforcement learning with contextual awareness at each stage of processing: from text understanding through perception, search, evaluation and ranking, all the way to post-search feedback analysis. Unlike existing methods that rely heavily on task-specific hand engineering, our method learns end-to-end without human intervention other than supervision by high-level demonstrations, thus significantly reducing manual effort. Our experiments on two benchmark datasets demonstrate improved performance over state-of-the-art systems and robustness across different domains and query types. We believe our work takes one more step towards generalizing NLO as a whole. Please click here if you would like to learn more!",1
"We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative 'image guessing' game between two agents -- Qbot and Abot -- who communicate in natural language dialog so that Qbot can select an unseen image from a lineup of images. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end -- from pixels to multi-agent multi-round dialog to game reward.   We demonstrate two experimental results.   First, as a 'sanity check' demonstration of pure RL (from scratch), we show results on a synthetic world, where the agents communicate in ungrounded vocabulary, i.e., symbols with no pre-specified meanings (X, Y, Z). We find that two bots invent their own communication protocol and start using certain symbols to ask/answer about certain visual attributes (shape/color/style). Thus, we demonstrate the emergence of grounded language and communication among 'visual' dialog agents with no human supervision.   Second, we conduct large-scale real-image experiments on the VisDial dataset, where we pretrain with supervised dialog data and show that the RL 'fine-tuned' agents significantly outperform SL agents. Interestingly, the RL Qbot learns to ask questions that Abot is good at, ultimately resulting in more informative dialog and a better team.",0
"In Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning we present a new deep reinforcement learning algorithm that can learn cooperative visual dialog agents by training them on large sets of natural language conversations from multiple domains. Our agent architecture learns effective policies through exploration and imitation, which leads to improved performance compared to previous approaches based solely on supervised learning or policy gradient methods alone. We demonstrate our approach's effectiveness across several different datasets and showcase how our method outperforms existing state-of-the art models in terms of both task completion rate and response quality metrics such as BLEU score and perplexity. With these results, we hope to provide researchers with a new tool for designing more advanced human-like agents capable of interacting naturally using both textual and visual modalities.",1
"The Aduio-visual Speech Recognition (AVSR) which employs both the video and audio information to do Automatic Speech Recognition (ASR) is one of the application of multimodal leaning making ASR system more robust and accuracy. The traditional models usually treated AVSR as inference or projection but strict prior limits its ability. As the revival of deep learning, Deep Neural Networks (DNN) becomes an important toolkit in many traditional classification tasks including ASR, image classification, natural language processing. Some DNN models were used in AVSR like Multimodal Deep Autoencoders (MDAEs), Multimodal Deep Belief Network (MDBN) and Multimodal Deep Boltzmann Machine (MDBM) that actually work better than traditional methods. However, such DNN models have several shortcomings: (1) They don't balance the modal fusion and temporal fusion, or even haven't temporal fusion; (2)The architecture of these models isn't end-to-end, the training and testing getting cumbersome. We propose a DNN model, Auxiliary Multimodal LSTM (am-LSTM), to overcome such weakness. The am-LSTM could be trained and tested once, moreover easy to train and preventing overfitting automatically. The extensibility and flexibility are also take into consideration. The experiments show that am-LSTM is much better than traditional methods and other DNN models in three datasets.",0
"Title: Enhancing Speech Recognition through Audio-Visual Fusion using MultiModal Long Short Term Memory Networks (MM-LSTM)  This research presents an approach that combines audio-visual speech recognition and lip reading through a novel multi modal architecture, named auxiliary multimodal LSTM (Auxiliary MM-LSTM). This system leverages deep learning techniques, particularly Long Short Term Memory networks (LSTM), which have been trained on both audio and visual input data simultaneously to improve accuracy over single modality approaches. By integrating these two complementary sources of information, our model can achieve state-of-the-art performance in audio-visual speech recognition tasks. We evaluate our method using several benchmark datasets, including TCD-TIMIT, OuluVS2, and AVSpeech. Experimental results show significant improvements in both speech recognition accuracy and robustness against noise compared to other current methods. Overall, our work demonstrates the effectiveness of Auxiliary MM-LSTM in enhancing speech understanding by incorporating multiple sensory channels, paving the way for further advancements in human computer interaction.",1
"We extend neural Turing machine (NTM) model into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects of learning to read and write into a memory through experiments on Facebook bAbI tasks using both a feedforward and GRUcontroller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We have done extensive analysis of our model and different variations of NTM on bAbI task. We also provide further experimental results on sequential pMNIST, Stanford Natural Language Inference, associative recall and copy tasks.",0
"This paper presents two novel addressing schemes: soft addresses and hard addresses, which enable dynamic neural Turing machines (dNTMs) to flexibly search through their memory without sacrificing read stability, and can handle real-world problems where external noise degrades memory performance over time. We validate our proposed approach on a challenging suite of experiments that demonstrate improved accuracy in comparison to previous state-of-the-art approaches. Our findings suggest that dNTMs equipped with either type of addressing scheme yield excellent results across diverse task domains, such as few-shot learning and generative tasks. We hope these improvements inspire future research in the direction of developing more powerful cognitive systems capable of efficiently leveraging large amounts of knowledge from vast, noisy datasets.",1
"Several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years. Attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation.   Recently, similar improvements have been obtained using alternative mechanisms that do not focus on a single part of a memory but operate on all of it in parallel, in a uniform way. Such mechanism, which we call active memory, improved over attention in algorithmic tasks, image processing, and in generative modelling.   So far, however, active memory has not improved over attention for most natural language processing tasks, in particular for machine translation. We analyze this shortcoming in this paper and propose an extended model of active memory that matches existing attention models on neural machine translation and generalizes better to longer sentences. We investigate this model and explain why previous active memory models did not succeed. Finally, we discuss when active memory brings most benefits and where attention can be a better choice.",0
This is an abstract of a research paper discussing whether active memory can effectively replace attention mechanisms used by deep learning models. The authors argue that conventional attention mechanisms require heavy computational overheads and may not always lead to better performance compared to non-attentional methods such as linear regression. They propose an alternative approach called active memory which uses simple recurrent networks (SRN) instead of complex transformers. Their results show that active memory achieves comparable or even superior performance on several natural language processing tasks while reducing model size and latency. The findings suggest that active memory could potentially become a promising new direction for developing efficient neural architectures.,1
"We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a \textit{critic} network that is trained to predict the value of an output token, given the policy of an \textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling.",0
"Title: ""An Adaptive Algorithm for Accurate Sequence Prediction""  Abstract: In recent years, there has been significant interest in developing algorithms that can accurately predict sequences in complex data sets. One approach to sequence prediction is through the use of actor-critic algorithms, which rely on feedback from both a predictor (actor) and evaluator (critic) component to improve predictions over time. This paper presents a novel actor-critic algorithm designed specifically for accurate sequence prediction. Our method utilizes adaptive learning techniques that enable the model to adjust parameters in real-time based on new input data, allowing for greater flexibility and accuracy in predictions. Through extensive experiments on simulated data and real-world datasets, we demonstrate that our method outperforms state-of-the-art methods in terms of prediction accuracy while maintaining computational efficiency. These results have important implications for applications such as speech recognition, natural language processing, and recommender systems. Overall, our work represents a promising step forward in the development of powerful models capable of making informed decisions based on past behavior.",1
"Deep networks thrive when trained on large scale data collections. This has given ImageNet a central role in the development of deep architectures for visual object classification. However, ImageNet was created during a specific period in time, and as such it is prone to aging, as well as dataset bias issues. Moving beyond fixed training datasets will lead to more robust visual systems, especially when deployed on robots in new environments which must train on the objects they encounter there. To make this possible, it is important to break free from the need for manual annotators. Recent work has begun to investigate how to use the massive amount of images available on the Web in place of manual image annotations. We contribute to this research thread with two findings: (1) a study correlating a given level of noisily labels to the expected drop in accuracy, for two deep architectures, on two different types of noise, that clearly identifies GoogLeNet as a suitable architecture for learning from Web data; (2) a recipe for the creation of Web datasets with minimal noise and maximum visual variability, based on a visual and natural language processing concept expansion strategy. By combining these two results, we obtain a method for learning powerful deep object models automatically from the Web. We confirm the effectiveness of our approach through object categorization experiments using our Web-derived version of ImageNet on a popular robot vision benchmark database, and on a lifelong object discovery task on a mobile robot.",0
"Title: Learning Deep Visual Object Models from Noisy Web Data: How to Make it Work Abstract This paper presents an approach for learning deep visual object models from large amounts of noisy web data. Despite the popularity of convolutional neural networks (CNNs) for image classification tasks, they still face significant challenges when dealing with high levels of noise present in uncurated web images. To address these issues, we propose several techniques that can effectively improve CNN performance on such datasets. Our approach involves augmenting standard training methods with new strategies tailored specifically towards handling noisy web data. By doing so, we demonstrate state-of-the-art results on benchmark datasets commonly used in computer vision research while significantly outperforming existing methods designed for similar settings. Furthermore, our methodology enables efficient model training without requiring excessive computational resources or complex architectures typically associated with current cutting-edge approaches. Overall, our work showcases the effectiveness of integrating simple yet robust techniques to achieve strong results even under difficult conditions encountered in real-world applications involving web data.",1
"Visual relations, such as ""person ride bike"" and ""bike next to car"", offer a comprehensive scene understanding of an image, and have already shown their great utility in connecting computer vision and natural language. However, due to the challenging combinatorial complexity of modeling subject-predicate-object relation triplets, very little work has been done to localize and predict visual relations. Inspired by the recent advances in relational representation learning of knowledge bases and convolutional object detection networks, we propose a Visual Translation Embedding network (VTransE) for visual relation detection. VTransE places objects in a low-dimensional relation space where a relation can be modeled as a simple vector translation, i.e., subject + predicate $\approx$ object. We propose a novel feature extraction layer that enables object-relation knowledge transfer in a fully-convolutional fashion that supports training and inference in a single forward/backward pass. To the best of our knowledge, VTransE is the first end-to-end relation detection network. We demonstrate the effectiveness of VTransE over other state-of-the-art methods on two large-scale datasets: Visual Relationship and Visual Genome. Note that even though VTransE is a purely visual model, it is still competitive to the Lu's multi-modal model with language priors.",0
"This paper presents a novel neural network architecture called Visual Translation Embedding Network (VTEN) for visual relation detection task. VTEN addresses two main challenges: lack of large scale training data for visual relation detection; and limited success of previous attempts at encoding contextual relationship among objects beyond simple co-occurrences such as location, pose, and appearance. We propose using self attention modules that allow our model to focus on different relationships within input image pairs without explicitly predicting spatial features like box coordinates. Our approach effectively learns from scarce training data by jointly detecting object classes and relational categories. Extensive experiments show state-of-the-art results against all published benchmarks.  Paper Title: ""Visual Translation Embedding Network for Visual Relation Detection"" Authors: [Name(s)]  Keywords: Object detection, relations, embeddings, attention mechanism, fine-grained recognition.",1
"Discovering causal relations is fundamental to reasoning and intelligence. In particular, observational causal discovery algorithms estimate the cause-effect relation between two random entities $X$ and $Y$, given $n$ samples from $P(X,Y)$.   In this paper, we develop a framework to estimate the cause-effect relation between two static entities $x$ and $y$: for instance, an art masterpiece $x$ and its fraudulent copy $y$. To this end, we introduce the notion of proxy variables, which allow the construction of a pair of random entities $(A,B)$ from the pair of static entities $(x,y)$. Then, estimating the cause-effect relation between $A$ and $B$ using an observational causal discovery algorithm leads to an estimation of the cause-effect relation between $x$ and $y$. For example, our framework detects the causal relation between unprocessed photographs and their modifications, and orders in time a set of shuffled frames from a video.   As our main case study, we introduce a human-elicited dataset of 10,000 pairs of casually-linked pairs of words from natural language. Our methods discover 75% of these causal relations. Finally, we discuss the role of proxy variables in machine learning, as a general tool to incorporate static knowledge into prediction tasks.",0
"One promising approach to causal inference is through proxy variables – surrogate endpoints that capture some aspect of the hypothesized unobserved mechanism of interest. These proxies can act as instruments for estimating natural direct and indirect effects using tools from structural equation modeling (SEM) and econometrics. This paper presents two new contributions to improve causal discovery using proxy variables: first, we develop theory for the identification conditions necessary and sufficient to estimate these natural parameters, and secondly, we introduce a fully Bayesian approach for estimating the posterior distributions of all quantities required for causal interpretation under different types of ignorability assumptions. Simulation studies show improved accuracy over existing methods like instrumental variable regressions, propensity scores matching, or SEMs alone. We apply our methodology to data on childhood vaccinations among low income families who participated in the New York City Housing Authority’s public housing lottery admissions waiting list, exploring potential mediation mechanisms affecting both vaccination rates within and across households living together. Our results suggest complex interrelationships involving economic hardship indicators such as employment status, local crime indicators, neighborhood segregation indices, family size, age, ethnic composition, health insurance coverage, and household relationship complexity. Finally, sensitivity analyses demonstrate robustness of causal effect estimates and credible intervals to different levels of latent class mixture distribution misspecification. In summary, we provide innovative solutions by combining theoretical foundations from causal graphical models with flexible statistical modeling frameworks, enabling practitioners to better decompose total causal effect into direct",1
"Many Collaborative Filtering (CF) algorithms are item-based in the sense that they analyze item-item relations in order to produce item similarities. Recently, several works in the field of Natural Language Processing (NLP) suggested to learn a latent representation of words using neural embedding algorithms. Among them, the Skip-gram with Negative Sampling (SGNS), also known as word2vec, was shown to provide state-of-the-art results on various linguistics tasks. In this paper, we show that item-based CF can be cast in the same framework of neural word embedding. Inspired by SGNS, we describe a method we name item2vec for item-based CF that produces embedding for items in a latent space. The method is capable of inferring item-item relations even when user information is not available. We present experimental results that demonstrate the effectiveness of the item2vec method and show it is competitive with SVD.",0
"Title: ""Item2Vec: Neural Item Embedding for Collaborative Filtering""  In recent years, collaborative filtering has emerged as one of the most popular approaches in recommender systems due to its ability to make accurate predictions by leveraging user-item interactions. However, traditional collaborative filtering methods suffer from the sparsity problem where new items often have very few associated interactions, making it difficult to accurately predict their preferences. To address this issue, we propose a novel approach called Item2Vec which utilizes neural item embeddings to learn low-dimensional representations of items that capture their underlying characteristics and relationships. Our method builds upon the skip-gram model architecture and optimizes it specifically for collaborative filtering tasks. Experimental results on several benchmark datasets show that our proposed method significantly outperforms other state-of-the-art techniques in terms of prediction accuracy, scalability, and interpretability. Additionally, we demonstrate how Item2Vec can generate meaningful and interpretable item embeddings that provide insights into the latent structure of the data. Overall, our work advances the field of recommendation algorithms and highlights the potential benefits of using deep learning models for improving collaborative filtering performance.",1
"Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.",0
"Artificial intelligence (AI) has made significant progress due to advances in deep learning, which uses artificial neural networks that can automatically learn from vast amounts of data. However, designing effective artificial neural network architectures remains a challenging task, as current practice relies heavily on manual expert design and trial-and-error experimentation. To address this issue, recent work has explored using reinforcement learning algorithms to search through large spaces of possible neural network architectures to identify high-performing solutions. This paper presents an extensive study evaluating several state-of-the-art methods across different benchmark datasets and tasks. Our results show that, while there is no clear winner among these approaches, certain methods consistently produce strong performers, outperforming some popular human-designed architectures. Our findings highlight the potential of using reinforcement learning to automate the design of powerful artificial neural network architectures, providing valuable insights into future research directions. Overall, our work represents an important step towards realizing truly autonomous machine learning systems.",1
"We study reinforcement learning of chatbots with recurrent neural network architectures when the rewards are noisy and expensive to obtain. For instance, a chatbot used in automated customer service support can be scored by quality assurance agents, but this process can be expensive, time consuming and noisy. Previous reinforcement learning work for natural language processing uses on-policy updates and/or is designed for on-line learning settings. We demonstrate empirically that such strategies are not appropriate for this setting and develop an off-policy batch policy gradient method (BPG). We demonstrate the efficacy of our method via a series of synthetic experiments and an Amazon Mechanical Turk experiment on a restaurant recommendations dataset.",0
"Abstract: Recent advances in natural language processing (NLP) have been achieved through deep learning models trained on large datasets. These models can generate human-like responses, but often struggle to maintain coherence across multiple turns of conversation. To address this challenge, we propose a novel approach that utilizes batch policy gradient methods to improve neural conversation models. We evaluate our method on two benchmark datasets, showing significant improvements over state-of-the art baseline systems. Our results demonstrate the effectiveness of using batch policy gradients to optimize conversation policies based on different metrics such as perplexity, F1 score and human evaluation scores. Additionally, we show how our method allows us to adapt to conversational context by conditioning on external features such as the current turn number, time elapsed since previous message or user feedback during training. Finally, while our focus here has been on single-turn response generation, extensions could apply our methods to more complex multi-step generative tasks in NLP. In summary, our work presents an innovative approach that enhances the quality of generated responses for NLP applications that require engaging conversations.",1
"The goal of this work is to recognise phrases and sentences being spoken by a talking face, with or without the audio. Unlike previous works that have focussed on recognising a limited number of words or phrases, we tackle lip reading as an open-world problem - unconstrained natural language sentences, and in the wild videos.   Our key contributions are: (1) a 'Watch, Listen, Attend and Spell' (WLAS) network that learns to transcribe videos of mouth motion to characters; (2) a curriculum learning strategy to accelerate training and to reduce overfitting; (3) a 'Lip Reading Sentences' (LRS) dataset for visual speech recognition, consisting of over 100,000 natural sentences from British television.   The WLAS model trained on the LRS dataset surpasses the performance of all previous work on standard lip reading benchmark datasets, often by a significant margin. This lip reading performance beats a professional lip reader on videos from BBC television, and we also demonstrate that visual information helps to improve speech recognition performance even when the audio is available.",0
"In recent years there has been growing interest in automated lip reading as a method for improving accessibility for deaf users of mobile devices. Automatic lip synchronization for multimedia content generation is another application that can benefit from improved lip reading technology. We present a deep learning approach using convolutional neural networks (CNNs) that directly predicts phones for sentences spoken by multiple speakers in videos taken from YouTube. Compared to prior work our model achieves higher accuracy despite training on fewer labelled examples. Furthermore we show significant improvement in sentence level lipreading accuracy through multi-tasking with audio modality. Finally we demonstrate that our trained models can effectively generate novel sentences given only textual prompts - further demonstrating their ability at generalising beyond specific instances seen during training. Our contributions here may enable robust lipreading across real world settings - representing a step forward towards enabling more accurate silent speech interfaces. We see our results providing useful insights into ways CNN architectures might be configured for visual recognition tasks like object detection, pose estimation etc.. These applications could lead to new methods of human computer interaction such as sign language based control for the hearing impaired. Additionally we think our findings have potentially important impacts within fields such as psychology where they offer further evidence relating to human auditory perception especially related to sensory memory/attention/fusion - offering avenues for future exploration through interdisciplinary collaboration. As part of an effort to encourage open sharing of research material all code used in generating these results is made publicly available online alongsi",1
"We consider generation and comprehension of natural language referring expression for objects in an image. Unlike generic ""image captioning"" which lacks natural standard evaluation criteria, quality of a referring expression may be measured by the receiver's ability to correctly infer which object is being described. Following this intuition, we propose two approaches to utilize models trained for comprehension task to generate better expressions. First, we use a comprehension module trained on human-generated expressions, as a ""critic"" of referring expression generator. The comprehension module serves as a differentiable proxy of human evaluation, providing training signal to the generation module. Second, we use the comprehension module in a generate-and-rerank pipeline, which chooses from candidate expressions generated by a model according to their performance on the comprehension task. We show that both approaches lead to improved referring expression generation on multiple benchmark datasets.",0
"Understanding referring expressions (RXs) requires understanding who/what they refer to (their reference), but humans often use RXs with vague meaning and need clues from context to clarify. We show that comprehending the intended reference can itself provide such clarifying information: even without explicit cues, listeners can use linguistically-encoded properties (e.g., social roles and semantic classes) of the referred entities to improve their chances of recovering correct references through probabilistic inference, guiding interpretation by exploiting structured knowledge. Our model predicts human responses to novel sentences and interprets ambiguous ones by leveraging these principles (Section 2). To implement them our approach utilizes a neural network that first generates hypotheses about possible interpretations based on world knowledge using structured embeddings (Section 4); then evaluates each hypothesis against the probabilistic constraints encoded in natural language rules for semantics and pragmatics (Section 6); and finally selects one as most likely (based on maximum expected likelihood; Section 7). In experiments (Sections 8 and 9), we demonstrate the effectiveness of integrating multiple sources of structured knowledge compared to previous methods. While others focus on enriching representation capacity (Wu et al., 2019; Xie et al., 2016), we target knowledge integration techniques directly toward supporting natural language reasoning and handling uncertainty—tasks central to referring expression processing, promising significant advancements over prior models. By revealing how comprehension drives referring expression resolution, we contribute new insights into natural language understanding while advancing the state-of-art",1
"We present a memory augmented neural network for natural language understanding: Neural Semantic Encoders. NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves over time and maintains the understanding of input sequences through read}, compose and write operations. NSE can also access multiple and shared memories. In this paper, we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks: natural language inference, question answering, sentence classification, document sentiment analysis and machine translation where NSE achieved state-of-the-art performance when evaluated on publically available benchmarks. For example, our shared-memory model showed an encouraging result on neural machine translation, improving an attention-based baseline by approximately 1.0 BLEU.",0
"This is an abstract for a paper: ""This paper proposes novel neural semantic encoder architectures based on Graph Convolutional Networks (GCN) which capture relational information among entities in knowledge graphs like DBpedia and YAGO using attention mechanisms."" Abstract: In recent years, there has been growing interest in developing methods that can effectively encode semantically meaningful representations from knowledge graphs such as DBpedia and YAGO. Recent advances have employed graph convolutional networks (GCN), but these approaches suffer from limitations related to capturing relational information. This work addresses this limitation by introducing novel neural semantic encoders based on GCN, incorporating attention mechanisms to better capture relationships among entities. Evaluation results demonstrate significant improvements over existing state-of-the-art models for several downstream natural language processing tasks. Our proposed approach provides researchers in NLP a more robust toolset towards understanding real-world phenomena through knowledge graphs, paving the way for future applications across domains.",1
"This paper describes a method for using Generative Adversarial Networks to learn distributed representations of natural language documents. We propose a model that is based on the recently proposed Energy-Based GAN, but instead uses a Denoising Autoencoder as the discriminator network. Document representations are extracted from the hidden layer of the discriminator and evaluated both quantitatively and qualitatively.",0
"In recent years, natural language processing has seen significant advances through the use of deep learning techniques such as neural networks. However, generating text can still present challenges due to the high dimensionality and sequential nature of natural languages. To address these issues, researchers have turned to generative models that can learn latent representations of complex data distributions and generate new instances based on them. One promising approach in this area is using generative adversarial networks (GANs). GANs consist of two competing subnetworks: one that generates data and another that discriminates real from generated data. By training both networks together, GANs can achieve state-of-the-art results in image synthesis tasks. This study examines whether similar success can be achieved by applying GANs to document generation tasks. We show that our proposed model outperforms baseline approaches across several benchmark datasets. Furthermore, we demonstrate how the learned representation captures semantically meaningful features that provide insight into the document corpus. Overall, our findings suggest that GANs have great potential for modeling complex natural language tasks.",1
"This paper introduces Graph Convolutional Recurrent Network (GCRN), a deep learning model able to predict structured sequences of data. Precisely, GCRN is a generalization of classical recurrent neural networks (RNN) to data structured by an arbitrary graph. Such structured sequences can represent series of frames in videos, spatio-temporal measurements on a network of sensors, or random walks on a vocabulary graph for natural language modeling. The proposed model combines convolutional neural networks (CNN) on graphs to identify spatial structures and RNN to find dynamic patterns. We study two possible architectures of GCRN, and apply the models to two practical problems: predicting moving MNIST data, and modeling natural language with the Penn Treebank dataset. Experiments show that exploiting simultaneously graph spatial and dynamic information about data can improve both precision and learning speed.",0
"Abstract: In recent years, there has been significant progress made in developing deep learning models that can effectively learn from structured data such as graphs. One class of these models that have shown promising results are graph convolutional recurrent networks (GCRN). GCRNs are equipped with both feedforward layers and recurrent layers, which enables them to model complex temporal dependencies while simultaneously taking into account spatial context provided by structured data such as graphs. However, current state-of-the-art methods focus primarily on homogeneous graphs, where each node belongs exclusively to one type and all edges within each set represent relationships specific to that type. To overcome this limitation, we propose a novel framework called ""Structured Sequence Modeling with Graph Convolutional Recurrent Networks"" that extends existing methods to allow for more flexibility in the types of nodes present in the graph and the kinds of edges that connect them. Experimental evaluations demonstrate that our approach significantly outperforms baseline methods on several benchmark datasets, showing the effectiveness of incorporating structural variation in graphs. Our method holds potential for application in a wide range of domains such as natural language processing, bioinformatics, social network analysis, and computer vision.",1
"We propose a scalable approach to learn video-based question answering (QA): answer a ""free-form natural language question"" about a video content. Our approach automatically harvests a large number of videos and descriptions freely available online. Then, a large number of candidate QA pairs are automatically generated from descriptions rather than manually annotated. Next, we use these candidate QA pairs to train a number of video-based QA methods extended fromMN (Sukhbaatar et al. 2015), VQA (Antol et al. 2015), SA (Yao et al. 2015), SS (Venugopalan et al. 2015). In order to handle non-perfect candidate QA pairs, we propose a self-paced learning procedure to iteratively identify them and mitigate their effects in training. Finally, we evaluate performance on manually generated video-based QA pairs. The results show that our self-paced learning procedure is effective, and the extended SS model outperforms various baselines.",0
"In recent years, there has been increasing interest in developing methods for automatically answering questions related to video content. One approach that has shown promise is leveraging video descriptions, which can provide additional contextual information that may be difficult to extract from the visual data alone. However, current approaches to using video descriptions often rely on simple heuristics such as matching keywords from the question against text in the description, which can limit their effectiveness.  In this paper, we propose a novel method for learning video question answering that utilizes both the visual and audio components of the video, along with natural language processing techniques applied to the video descriptions. Our proposed model uses recurrent neural networks (RNNs) to encode both the question and the description into vector representations, and then applies attention mechanisms to fuse these representations. This allows our model to focus on relevant parts of the description and integrate them with the visual and audio information. We evaluate our approach on two benchmark datasets and show significant improvements over previous state-of-the-art results.  Our work demonstrates the potential of leveraging video descriptions to enhance the accuracy of video question answering systems. By integrating multiple sources of information in a principled manner, we are able to better understand the complex relationships among different modalities and generate more accurate responses. Future work could explore ways to incorporate other types of metadata, such as tags and labels, to further improve performance. Overall, our contributions highlight the importance of multimodal reasoning in building effective QA systems and have implications for a wide range of applications including education, entertainment, and personal assistance.",1
"User-given tags or labels are valuable resources for semantic understanding of visual media such as images and videos. Recently, a new type of labeling mechanism known as hash-tags have become increasingly popular on social media sites. In this paper, we study the problem of generating relevant and useful hash-tags for short video clips. Traditional data-driven approaches for tag enrichment and recommendation use direct visual similarity for label transfer and propagation. We attempt to learn a direct low-cost mapping from video to hash-tags using a two step training process. We first employ a natural language processing (NLP) technique, skip-gram models with neural network training to learn a low-dimensional vector representation of hash-tags (Tag2Vec) using a corpus of 10 million hash-tags. We then train an embedding function to map video features to the low-dimensional Tag2vec space. We learn this embedding for 29 categories of short video clips with hash-tags. A query video without any tag-information can then be directly mapped to the vector space of tags using the learned embedding and relevant tags can be found by performing a simple nearest-neighbor retrieval in the Tag2Vec space. We validate the relevance of the tags suggested by our system qualitatively and quantitatively with a user study.",0
"In this paper, we present a novel method for learning embeddings of video clips using a modified version of the Tag2Vec algorithm. This approach allows us to represent videos as fixed-size vectors which capture their underlying structure and meaning. We show that these embedding spaces can then be used effectively for a variety of downstream tasks such as text retrieval, recommendation systems, and zero-shot reasoning. Our experiments demonstrate that our method outperforms several state-of-the-art baselines across multiple benchmark datasets, highlighting the effectiveness and versatility of our proposed solution. Overall, this work represents an important step forward towards developing more accurate and efficient methods for analyzing and understanding large collections of multimedia data.",1
"Visual attention plays an important role to understand images and demonstrates its effectiveness in generating natural language descriptions of images. On the other hand, recent studies show that language associated with an image can steer visual attention in the scene during our cognitive process. Inspired by this, we introduce a text-guided attention model for image captioning, which learns to drive visual attention using associated captions. For this model, we propose an exemplar-based learning approach that retrieves from training data associated captions with each image, and use them to learn attention on visual features. Our attention model enables to describe a detailed state of scenes by distinguishing small or confusable objects effectively. We validate our model on MS-COCO Captioning benchmark and achieve the state-of-the-art performance in standard metrics.",0
"Attempting to generate image descriptions (caption) that accurately capture their content remains challenging, as obtaining textual supervision signals can prove difficult or laborious. Here, we present a model that generates attention maps which can be used to selectively focus on regions from an input query to improve its description quality. Our approach utilizes multi-scale features extracted via convolutional neural networks at several stages of processing depth to maximize the robustness of our method during inference. Experiments demonstrate quantitative improvements over previously published methods on standard benchmark datasets such as COCO and Flickr8K, illustrating that our proposed attentive mechanism significantly boosts model performance. Finally, our work explores ways of generating novel training techniques for producing better captions by learning attention mechanisms end-to-end without explicit guidance.",1
"Deep learning has achieved a remarkable performance breakthrough in several fields, most notably in speech recognition, natural language processing, and computer vision. In particular, convolutional neural network (CNN) architectures currently produce state-of-the-art performance on a variety of image analysis tasks such as object detection and recognition. Most of deep learning research has so far focused on dealing with 1D, 2D, or 3D Euclidean-structured data such as acoustic signals, images, or videos. Recently, there has been an increasing interest in geometric deep learning, attempting to generalize deep learning methods to non-Euclidean structured data such as graphs and manifolds, with a variety of applications from the domains of network analysis, computational social science, or computer graphics. In this paper, we propose a unified framework allowing to generalize CNN architectures to non-Euclidean domains (graphs and manifolds) and learn local, stationary, and compositional task-specific features. We show that various non-Euclidean CNN methods previously proposed in the literature can be considered as particular instances of our framework. We test the proposed method on standard tasks from the realms of image-, graph- and 3D shape analysis and show that it consistently outperforms previous approaches.",0
"Deep Learning (DL) has become increasingly popular due to its ability to capture nonlinear patterns from data without relying heavily on domain knowledge. Recently, there has been growing interest in applying DL techniques to graph and manifold data types due to their ubiquitous presence across diverse domains such as computer vision, natural language processing, and bioinformatics. In particular, Mixture Model Convolutional Neural Networks (CNNs) have shown promising results in achieving high performance, while remaining interpretable. This work presents a comprehensive study of geometric deep learning on graphs and manifolds via Mixture Model CNNs by investigating several important aspects related to design, optimization, evaluation, visualization, implementation, and application. Specifically, we explore a variety of architectures and develop advanced training methods that improve their robustness and generalization capability on irregular-shaped data like graphs and manifolds. Our experiments demonstrate state-of-the-art results over multiple benchmark datasets commonly used in different areas, including image classification, object detection, motion segmentation, face recognition, speech recognition, gene expression prediction, biological network analysis, brain parcellation, etc., outperforming many existing algorithms reported in literature. These findings confirm the power and versatility of our approach in tackling complex problems, while opening up new possibilities for further advancements in the rapidly expanding field of geometric DL.",1
"Understanding human actions is a key problem in computer vision. However, recognizing actions is only the first step of understanding what a person is doing. In this paper, we introduce the problem of predicting why a person has performed an action in images. This problem has many applications in human activity understanding, such as anticipating or explaining an action. To study this problem, we introduce a new dataset of people performing actions annotated with likely motivations. However, the information in an image alone may not be sufficient to automatically solve this task. Since humans can rely on their lifetime of experiences to infer motivation, we propose to give computer vision systems access to some of these experiences by using recently developed natural language models to mine knowledge stored in massive amounts of text. While we are still far away from fully understanding motivation, our results suggest that transferring knowledge from language into vision can help machines understand why people in images might be performing an action.",0
"This paper presents a novel approach to predict motivations behind human actions using natural language processing techniques on text data. We address the problem of inferring user intentions from their written statements, which can then be used in a variety of applications such as personal assistants, customer service chatbots, and recommendation systems. Our method leverages recent advances in deep learning models trained on large amounts of text data to model complex relationships between words, phrases, and sentences that reveal underlying semantic meaning. Specifically, we use pre-trained transformer networks to encode and capture contextual dependencies in sentences and paragraphs. To evaluate our approach, we conduct experiments using two publicly available datasets and demonstrate significant improvement over state-of-the-art baselines. Our results show high accuracy in classifying action motivations across different categories, including goal-oriented, affective, social influence, and other dimensions. Overall, this work contributes towards enabling machines to better understand humans through analyzing their written expressions and offers opportunities for future research in developing more sophisticated language understanding algorithms for artificial intelligence systems.",1
"People often refer to entities in an image in terms of their relationships with other entities. For example, ""the black cat sitting under the table"" refers to both a ""black cat"" entity and its relationship with another ""table"" entity. Understanding these relationships is essential for interpreting and grounding such natural language expressions. Most prior work focuses on either grounding entire referential expressions holistically to one region, or localizing relationships based on a fixed set of categories. In this paper we instead present a modular deep architecture capable of analyzing referential expressions into their component parts, identifying entities and relationships mentioned in the input expression and grounding them all in the scene. We call this approach Compositional Modular Networks (CMNs): a novel architecture that learns linguistic analysis and visual inference end-to-end. Our approach is built around two types of neural modules that inspect local regions and pairwise interactions between regions. We evaluate CMNs on multiple referential expression datasets, outperforming state-of-the-art approaches on all tasks.",0
"This paper presents a novel approach to modeling relationships in referential expressions using compositional modular networks (CMN). In natural language processing tasks involving reference resolution, identifying the appropriate antecedent to complete a given sentence requires understanding the relationship between different entities mentioned within that context. Existing approaches have focused on utilizing pretraining techniques to improve performance in these types of tasks; however, they often lack explicit models for representing the underlying relationships between entities. We propose CMN as a solution to overcome this limitation by modeling relational information at multiple levels of granularity. Our approach exploits compositional structure by breaking down complex relationships into smaller components while retaining their global nature through graph convolutional network layers. Experiments conducted on benchmark datasets demonstrate that our proposed method outperforms previous state-of-the-art models for referential expression recognition, achieving significant improvements in both intrinsic and extrinsic evaluation metrics. These results highlight the potential of incorporating compositional structures to enhance the representation learning process and better capture relationships between entities in NLP applications. Overall, our work provides new insights into how to effectively leverage structural knowledge in natural language understanding.",1
"Automated extraction of concepts from patient clinical records is an essential facilitator of clinical research. For this reason, the 2010 i2b2/VA Natural Language Processing Challenges for Clinical Records introduced a concept extraction task aimed at identifying and classifying concepts into predefined categories (i.e., treatments, tests and problems). State-of-the-art concept extraction approaches heavily rely on handcrafted features and domain-specific resources which are hard to collect and define. For this reason, this paper proposes an alternative, streamlined approach: a recurrent neural network (the bidirectional LSTM with CRF decoding) initialized with general-purpose, off-the-shelf word embeddings. The experimental results achieved on the 2010 i2b2/VA reference corpora using the proposed framework outperform all recent methods and ranks closely to the best submission from the original 2010 i2b2/VA challenge.",0
"Title: ""Bidirectional LSTM-CRF for Clinical Concept Extction"" Authors: (list authors) Institutions: (list institutions) Contact Author: contact author email  Abstract: This study presents a method that combines bidirectional Long Short-Term Memory (LSTM) networks and Conditional Random Fields (CRF) for clinical concept extraction from electronic health records. Our approach addresses two key challenges in traditional approaches: handling irregularly formatted data and capturing interdependencies between concepts through contextual window expansion. We evaluate our model on three benchmark datasets and achieve state-of-the art results. Results suggest bidirectional LSTM-CRF models have promise as a tool for extracting clinical concepts from real-world patient data. ======================  Bidirectional LSTM-CRF for Clinical Concept Extraction ----------------------------------------------------  Electronic Health Records (EHRs) contain valuable information that can inform medical research, treatment planning, and public health surveillance. However, manually reviewing EHRs is time-consuming and expensive. Therefore, there is growing interest in developing Natural Language Processing (NLP) techniques that automate information retrieval directly from raw text [2]. In particular, identifying clinical concepts such as diseases, treatments, medications, and symptoms within free-text notes is crucial for translating large amounts of unstructured data into actionable knowledge [4]. Traditionally, rule-based systems were used for concept extraction, but these methods often struggle due to their reliance on predefined patterns that cannot adapt to variations found in natural language texts. Alternatively, machine learning algorithms including Hidden Markov Models (HMM) [5], Support Vector Machines (SVM) [6], and Recurrent Neural Networks (RNNs) [7] h",1
"Complex nonlinear models such as deep neural network (DNNs) have become an important tool for image classification, speech recognition, natural language processing, and many other fields of application. These models however lack transparency due to their complex nonlinear structure and to the complex data distributions to which they typically apply. As a result, it is difficult to fully characterize what makes these models reach a particular decision for a given input. This lack of transparency can be a drawback, especially in the context of sensitive applications such as medical analysis or security. In this short paper, we summarize a recent technique introduced by Bach et al. [1] that explains predictions by decomposing the classification decision of DNN models in terms of input variables.",0
"In recent years there has been a surge of interest in interpreting machine learning (ML) models due to their increasingly widespread use in decision making across various fields such as finance, healthcare, law, etc. To ensure that these decisions are fair and unbiased, it is crucial to have transparency into how these models make predictions. However, many modern complex ML algorithms can suffer from a lack of interpretability, often due to deep neural networks having millions of parameters and non-linear interactions among them. This makes it challenging to identify which factors influence the model’s predictions, potentially leading to skepticism about their trustworthiness. To address this challenge, researchers at the University of California developed a method called layer-wise relevance propagation (LRP), which explains individual prediction outputs based on input features attribution. Through experiments using various datasets and architectures, they demonstrated that LRP outperforms competitive methods in terms of both effectiveness and efficiency. Furthermore, they showcased LRP’s applicability to real-world applications through case studies involving natural language processing tasks and visualization techniques. Overall, the study highlighted the importance of explainable artificial intelligence in fostering trust and ensuring accountability in decision-making processes involving ML models.",1
"We address a question answering task on real-world images that is set up as a Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Ask Your Neurons, a scalable, jointly trained, end-to-end formulation to this problem.   In contrast to previous efforts, we are facing a multi-modal problem where the language output (answer) is conditioned on visual and natural language inputs (image and question). We provide additional insights into the problem by analyzing how much information is contained only in the language part for which we provide a new human baseline. To study human consensus, which is related to the ambiguities inherent in this challenging task, we propose two novel metrics and collect additional answers which extend the original DAQUAR dataset to DAQUAR-Consensus.   Moreover, we also extend our analysis to VQA, a large-scale question answering about images dataset, where we investigate some particular design choices and show the importance of stronger visual models. At the same time, we achieve strong performance of our model that still uses a global image representation. Finally, based on such analysis, we refine our Ask Your Neurons on DAQUAR, which also leads to a better performance on this challenging task.",0
"In recent years, deep learning has revolutionized the field of computer vision by enabling machines to achieve human-level performance on challenging tasks such as image classification, object detection, and segmentation. However, answering questions about images remains a difficult task that requires more advanced understanding and reasoning capabilities than simple recognition. This paper presents a novel approach to visual question answering (VQA) that leverages deep neural networks to predict answers to natural language queries about images. Our method uses a combination of attention mechanisms, memory modules, and contextual embeddings to effectively integrate visual and linguistic representations, allowing the model to reason about complex relationships and interactions within and across images. Extensive experiments on several benchmark datasets demonstrate that our approach significantly outperforms state-of-the-art methods while maintaining high interpretability and robustness against adversarial attacks. Overall, we believe that our work represents a significant step towards developing intelligent agents capable of interacting with the world through natural language and visual perception.",1
"Automatically generating natural language descriptions of videos plays a fundamental challenge for computer vision community. Most recent progress in this problem has been achieved through employing 2-D and/or 3-D Convolutional Neural Networks (CNN) to encode video content and Recurrent Neural Networks (RNN) to decode a sentence. In this paper, we present Long Short-Term Memory with Transferred Semantic Attributes (LSTM-TSA)---a novel deep architecture that incorporates the transferred semantic attributes learnt from images and videos into the CNN plus RNN framework, by training them in an end-to-end manner. The design of LSTM-TSA is highly inspired by the facts that 1) semantic attributes play a significant contribution to captioning, and 2) images and videos carry complementary semantics and thus can reinforce each other for captioning. To boost video captioning, we propose a novel transfer unit to model the mutually correlated attributes learnt from images and videos. Extensive experiments are conducted on three public datasets, i.e., MSVD, M-VAD and MPII-MD. Our proposed LSTM-TSA achieves to-date the best published performance in sentence generation on MSVD: 52.8% and 74.0% in terms of BLEU@4 and CIDEr-D. Superior results when compared to state-of-the-art methods are also reported on M-VAD and MPII-MD.",0
"In recent years, video captioning has become increasingly important due to growing accessibility needs and demands for multimedia content. However, creating accurate captions that capture all relevant details can still pose significant challenges. In this paper, we propose a novel approach for generating semantic attributes based on transferred knowledge from other domains such as text classification and object detection. We introduce a method called ""Video Captioning with Transferred Semantic Attributes"" (VCSA) which leverages these pretrained models to enhance the quality and comprehensiveness of generated captions. Our experiments demonstrate that VCSA significantly improves overall accuracy while reducing computational requirements compared to previous methods. This research represents a step forward in bridging the gap between human expertise and machine learning techniques, paving the way towards more intelligent and robust systems in the field of video processing.",1
"Attention mechanisms have recently been introduced in deep learning for various tasks in natural language processing and computer vision. But despite their popularity, the ""correctness"" of the implicitly-learned attention maps has only been assessed qualitatively by visualization of several examples. In this paper we focus on evaluating and improving the correctness of attention in neural image captioning models. Specifically, we propose a quantitative evaluation metric for the consistency between the generated attention maps and human annotations, using recently released datasets with alignment between regions in images and entities in captions. We then propose novel models with different levels of explicit supervision for learning attention maps during training. The supervision can be strong when alignment between regions and caption entities are available, or weak when only object segments and categories are provided. We show on the popular Flickr30k and COCO datasets that introducing supervision of attention maps during training solidly improves both attention correctness and caption quality, showing the promise of making machine perception more human-like.",0
This is an important paper as there have been little work done on attention correction before. We first study human annotators which show that they use different cue for attention correction than models previously used by computer vision researchers. Secondly we propose a new method called Relative Visual Grounding Graph (RVGG) which uses relative spatial relationship between objects in images to improve localization for image caption generation without using bounding boxes. RVG,1
"State of the art machine learning algorithms are highly optimized to provide the optimal prediction possible, naturally resulting in complex models. While these models often outperform simpler more interpretable models by order of magnitudes, in terms of understanding the way the model functions, we are often facing a ""black box"".   In this paper we suggest a simple method to interpret the behavior of any predictive model, both for regression and classification. Given a particular model, the information required to interpret it can be obtained by studying the partial derivatives of the model with respect to the input. We exemplify this insight by interpreting convolutional and multi-layer neural networks in the field of natural language processing.",0
"This paper presents a novel approach for interpreting prediction models using the input gradient. We show that by analyzing the relationship between small perturbations in the inputs and their corresponding changes in predictions, we can gain insight into how these models make decisions. Our method allows us to visualize and quantify the contributions of different parts of the input space to the final prediction, providing a more intuitive understanding of model behavior. Experiments on several benchmark datasets demonstrate the effectiveness of our approach in revealing important features and patterns in complex models. Overall, our work contributes to the growing field of interpretability research, helping to build trust in machine learning systems by making them more transparent and comprehensible to users.",1
"Video captioning which automatically translates video clips into natural language sentences is a very important task in computer vision. By virtue of recent deep learning technologies, e.g., convolutional neural networks (CNNs) and recurrent neural networks (RNNs), video captioning has made great progress. However, learning an effective mapping from visual sequence space to language space is still a challenging problem. In this paper, we propose a Multimodal Memory Model (M3) to describe videos, which builds a visual and textual shared memory to model the long-term visual-textual dependency and further guide global visual attention on described targets. Specifically, the proposed M3 attaches an external memory to store and retrieve both visual and textual contents by interacting with video and sentence with multiple read and write operations. First, text representation in the Long Short-Term Memory (LSTM) based text decoder is written into the memory, and the memory contents will be read out to guide an attention to select related visual targets. Then, the selected visual information is written into the memory, which will be further read out to the text decoder. To evaluate the proposed model, we perform experiments on two publicly benchmark datasets: MSVD and MSR-VTT. The experimental results demonstrate that our method outperforms the state-of-theart methods in terms of BLEU and METEOR.",0
"This paper presents a novel approach for multimodal memory modelling that can generate accurate video captions. Our model uses a combination of visual features extracted from frames within a given clip along with audio features such as speech transcriptions, speaker embeddings, and prosody to predict natural language descriptions of actions occurring in the scene. We evaluate our method on two benchmark datasets, MSR-VTT and VATEX, and demonstrate state-of-the-art results. Additionally, we perform ablation studies to investigate the impact of each component in our system and show that our proposed model significantly improves over baseline models. Finally, we qualitatively analyze generated captions to confirm their accuracy and provide insights into the performance of our approach. -----",1
"Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. To incorporate attributes, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework achieves superior results when compared to state-of-the-art deep models. Most remarkably, we obtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used and publicly available splits in (Karpathy & Fei-Fei, 2015) when extracting image representations by GoogleNet and achieve to date top-1 performance on COCO captioning Leaderboard.",0
"This work presents a novel approach to image caption generation using attribute-augmented convolutional neural networks (CNNs). Our method improves upon previous state-of-the-art techniques by incorporating high-level semantic attributes into the CNN model, which provides a richer representation of objects within images. We demonstrate that our method outperforms existing approaches on challenging benchmark datasets, achieving significant improvements in automatic evaluation metrics such as caption quality and diversity measures. Furthermore, we show qualitatively through human evaluations that our generated captions better capture important details and context present in the input images compared to baseline methods. Overall, our results indicate that integrating attribute annotations in the training process can effectively enhance the performance of image captioning models, opening up new research directions for future progress in the field.",1
"Convolutional Neural Networks (CNN) have demon- strated its successful applications in computer vision, speech recognition, and natural language processing. For object recog- nition, CNNs might be limited by its strict label requirement and an implicit assumption that images are supposed to be target- object-dominated for optimal solutions. However, the labeling procedure, necessitating laying out the locations of target ob- jects, is very tedious, making high-quality large-scale dataset prohibitively expensive. Data augmentation schemes are widely used when deep networks suffer the insufficient training data problem. All the images produced through data augmentation share the same label, which may be problematic since not all data augmentation methods are label-preserving. In this paper, we propose a weakly supervised CNN framework named Multiple Instance Learning Convolutional Neural Networks (MILCNN) to solve this problem. We apply MILCNN framework to object recognition and report state-of-the-art performance on three benchmark datasets: CIFAR10, CIFAR100 and ILSVRC2015 classification dataset.",0
"In recent years, convolutional neural networks (CNN) have shown remarkable performance on object recognition tasks. However, one major drawback of these methods is their lack of robustness against changes in scale, rotation, translation, and illumination conditions. To address this issue, we propose a novel approach based on multiple instance learning (MIL), which can learn discriminative features that generalize across different instances of objects. Our method utilizes deep CNN architectures along with a modified MIL formulation for object recognition in images. We demonstrate through extensive experiments that our proposed method significantly outperforms existing state-of-the-art approaches on standard benchmark datasets such as PASCAL VOC 2007 and 2012, achieving new levels of accuracy and robustness under varying conditions. The main contributions of this work are: 1) introducing a CNN architecture optimized for object detection, where each filter learns features corresponding to high level concepts; 2) developing a feature extraction strategy for localizing objects within images by learning filters sensitive to both intra-class variations and inter-class differences between classes; 3) presenting experimental results showing significant improvements over prior methods on challenging benchmark dataset. Overall, our findings highlight the potential benefits of combining deep CNN models with classical MIL techniques for tackling complex real-world problems in computer vision.",1
"Recently neural networks and multiple instance learning are both attractive topics in Artificial Intelligence related research fields. Deep neural networks have achieved great success in supervised learning problems, and multiple instance learning as a typical weakly-supervised learning method is effective for many applications in computer vision, biometrics, nature language processing, etc. In this paper, we revisit the problem of solving multiple instance learning problems using neural networks. Neural networks are appealing for solving multiple instance learning problem. The multiple instance neural networks perform multiple instance learning in an end-to-end way, which take a bag with various number of instances as input and directly output bag label. All of the parameters in a multiple instance network are able to be optimized via back-propagation. We propose a new multiple instance neural network to learn bag representations, which is different from the existing multiple instance neural networks that focus on estimating instance label. In addition, recent tricks developed in deep learning have been studied in multiple instance networks, we find deep supervision is effective for boosting bag classification accuracy. In the experiments, the proposed multiple instance networks achieve state-of-the-art or competitive performance on several MIL benchmarks. Moreover, it is extremely fast for both testing and training, e.g., it takes only 0.0003 second to predict a bag and a few seconds to train on a MIL datasets on a moderate CPU.",0
"Artificial neural networks (ANN) have been applied successfully to many pattern recognition problems such as speech and image classification tasks. One common approach is to train a single instance network on each example, treating each problem separately. This can lead to high computation costs and often requires human engineering for feature extraction. Another option is to use multiple instances per task by training one network which sees examples from all tasks. However, this leads to the curse of dimensionality since the number of parameters grows significantly faster than the number of input dimensions. To solve this issue, we propose revisiting Multiple Instance Learning using Neural Networks which allows us to learn good representations directly from raw input space data. By utilizing deep convolutional neural networks (CNN), our method has achieved state of art results in several benchmark datasets while significantly reducing computational cost over traditional methods. We further improve these results by introducing batch normalization and weight decay regularizations techniques. Our analysis shows that MILNN outperforms other deep learning architectures including fully connected networks, support vector machines, decision trees, random forest models and even performs better compared to weakly supervised CNN based systems for some difficult datasets like CIFAR-10. Additionally, we prove that our model indeed learns nonlinear features that correspond well enough to ground truth target classes. These promising experimental findings indicate that MILNN is a very effective tool to address multi-instance classification scenarios that often arise in real applications. As future work, we plan to extend our studies towards time-series prediction with missing values and semi-supervised settings where only a few labeled samples are available during training.",1
"Together with the development of more accurate methods in Computer Vision and Natural Language Understanding, holistic architectures that answer on questions about the content of real-world images have emerged. In this tutorial, we build a neural-based approach to answer questions about images. We base our tutorial on two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the models that we present here can achieve a competitive performance on both datasets, in fact, they are among the best methods that use a combination of LSTM with a global, full frame CNN representation of an image. We hope that after reading this tutorial, the reader will be able to use Deep Learning frameworks, such as Keras and introduced Kraino, to build various architectures that will lead to a further performance improvement on this challenging task.",0
"An abstract describes in one paragraph the content of a scientific paper. To make it easier to read you should follow these guidelines: * Capitalize only first letter of sentences (but also first letter of any proper noun) * Use section headings in your text (introduction / related work / problem statement / methodology / results / conclusions etc.) In my example below I use section headings separated by dashes ""-"", but real abstract usually uses ""Background"" instead of introduction for example. And please note that abstract is NOT intended to explain every detail from paper - but rather describe all important points! Here is sample based on following description: There has been recent interest in answering questions using deep learning methods due to their ability to jointly perform multiple processing steps in a single pass. We develop two techniques to improve performance over prior state-of-the-art methods: incorporating knowledge distillation from a large model into smaller models during training, and pretraining question answering systems to predict masked tokens within images. Our experiments show improvements across four different datasets over several baseline architectures and variants. **Introduction** Recent years have seen rapid progress in developing computer vision models capable of understanding natural language and generating descriptive outputs. While most past research focused on generating image descriptions, newer lines of inquiry explore models which can understand structured representations, such as semantic parsing from scene graphs. Related works from CVPR’17, ICCV’19, VISAPP’20 already explored fine-grained analysis in the space of answers given to concrete factual questions. On the other hand there were relatively few papers at top venues addressing general object detection and segmentati",1
"Deep learning is a branch of artificial intelligence employing deep neural network architectures that has significantly advanced the state-of-the-art in computer vision, speech recognition, natural language processing and other domains. In November 2015, Google released $\textit{TensorFlow}$, an open source deep learning software library for defining, training and deploying machine learning models. In this paper, we review TensorFlow and put it in context of modern deep learning concepts and software. We discuss its basic computational paradigms and distributed execution model, its programming interface as well as accompanying visualization toolkits. We then compare TensorFlow to alternative libraries such as Theano, Torch or Caffe on a qualitative as well as quantitative basis and finally comment on observed use-cases of TensorFlow in academia and industry.",0
"This paper provides an overview and tutorial on using the popular open source machine learning library, TensorFlow. We begin by discussing how to install TensorFlow and setting up a development environment. Next we walk through some examples to illustrate the basics of working with TensorFlow, including how to define and train simple models, and how to make predictions from existing trained models. Along the way we point out key features of the API that enable efficient use of modern hardware and flexible model architecture design. Finally, we provide resources for going further on your own, whether you want to explore more advanced topics such as debugging and optimization techniques, integrating with other libraries like PyTorch, or contributing back to the project itself by reporting issues, writing documentation, or implementing new features yourself!",1
"Learning a joint language-visual embedding has a number of very appealing properties and can result in variety of practical application, including natural language image/video annotation and search. In this work, we study three different joint language-visual neural network model architectures. We evaluate our models on large scale LSMDC16 movie dataset for two tasks: 1) Standard Ranking for video annotation and retrieval 2) Our proposed movie multiple-choice test. This test facilitate automatic evaluation of visual-language models for natural language video annotation based on human activities. In addition to original Audio Description (AD) captions, provided as part of LSMDC16, we collected and will make available a) manually generated re-phrasings of those captions obtained using Amazon MTurk b) automatically generated human activity elements in ""Predicate + Object"" (PO) phrases based on ""Knowlywood"", an activity knowledge mining model. Our best model archives Recall@10 of 19.2% on annotation and 18.9% on video retrieval tasks for subset of 1000 samples. For multiple-choice test, our best model achieve accuracy 58.11% over whole LSMDC16 public test-set.",0
"This paper presents a novel method for understanding movies using natural language processing techniques and visual embeddings. We propose a model that learns from both textual descriptions of movie scenes and the corresponding frames themselves, allowing us to jointly represent visual and linguistic aspects of the movies. Our approach uses deep neural networks to learn a shared embedding space for both modalities, enabling efficient cross-modal alignment and fusion. Experimental results on benchmark datasets show that our method outperforms state-of-the-art baselines in terms of accuracy, consistency, and robustness, demonstrating the effectiveness of integrating natural language and vision for high-level video understanding tasks such as action recognition and scene classification. Additionally, we provide an ablation study analyzing different components of our model, highlighting the importance of each part in achieving good performance. Finally, we qualitatively analyze some examples to gain insights into how our model makes predictions and what kind of knowledge it gains by fusing language and vision. Overall, our work offers new opportunities for researchers in computer vision and natural language processing communities to exploit the synergy between these two domains and develop innovative solutions for challenging multimedia analysis problems.",1
"Visual Question Answering (VQA) is the task of answering natural-language questions about images. We introduce the novel problem of determining the relevance of questions to images in VQA. Current VQA models do not reason about whether a question is even related to the given image (e.g. What is the capital of Argentina?) or if it requires information from external resources to answer correctly. This can break the continuity of a dialogue in human-machine interaction. Our approaches for determining relevance are composed of two stages. Given an image and a question, (1) we first determine whether the question is visual or not, (2) if visual, we determine whether the question is relevant to the given image or not. Our approaches, based on LSTM-RNNs, VQA model uncertainty, and caption-question similarity, are able to outperform strong baselines on both relevance tasks. We also present human studies showing that VQA models augmented with such question relevance reasoning are perceived as more intelligent, reasonable, and human-like.",0
"Effective question answering systems rely on relevant questions to generate accurate responses. Visual Question Answering (VQA) requires more than textual input as it involves both image data and natural language queries. This study focuses on identifying non-visual and false premise questions by analyzing the types of questions posed to the VQA model. We examine how these two categories of questions affect accuracy and present examples from prior research to highlight their effects. Our findings suggest that improving question relevancy could increase VQA system performance significantly. Furthermore, we discuss potential approaches to mitigate these problematic questions in future VQA designs. Overall, our work contributes new insights into evaluating VQA systems and informs ongoing efforts towards creating robust conversational agents capable of understanding complex, real world questions effectively.",1
"We present an approach to simultaneously perform semantic segmentation and prepositional phrase attachment resolution for captioned images. Some ambiguities in language cannot be resolved without simultaneously reasoning about an associated image. If we consider the sentence ""I shot an elephant in my pajamas"", looking at language alone (and not using common sense), it is unclear if it is the person or the elephant wearing the pajamas or both. Our approach produces a diverse set of plausible hypotheses for both semantic segmentation and prepositional phrase attachment resolution that are then jointly reranked to select the most consistent pair. We show that our semantic segmentation and prepositional phrase attachment resolution modules have complementary strengths, and that joint reasoning produces more accurate results than any module operating in isolation. Multiple hypotheses are also shown to be crucial to improved multiple-module reasoning. Our vision and language approach significantly outperforms the Stanford Parser (De Marneffe et al., 2006) by 17.91% (28.69% relative) and 12.83% (25.28% relative) in two different experiments. We also make small improvements over DeepLab-CRF (Chen et al., 2015).",0
"Automatically resolve language ambiguity by using images Captions often contain multiple interpretations due to uncertainties such as missing data, incorrect labels, or inherent structural challenges. To handle these problems more effectively we propose to use joint inference techniques that combine both textual and visual reasoning steps, such as semantic image segmentation and preposition attachment resolution, into one integrated system. In particular, our approach uses a transformer network architecture based on the ViT framework and trained using Hungarian Matching losses which can efficiently exploit both modalities at once. We evaluate two variants of our model. One variant only performs preposition attachment resolution while the other variant also attempts to perform semantic segmentation. While our methods have limited accuracy compared to separate state-of-the art approaches for each modality, their combined performance results show clear advantages over standalone image interpretation systems across four different datasets. Our work demonstrates that effective incorporation of vision in NLP models can lead to significantly improved task execution even without the need to fine tune architectures. This suggests that multimodal integration may provide significant benefits beyond individual capabilities.",1
"Methods based on representation learning currently hold the state-of-the-art in many natural language processing and knowledge base inference tasks. Yet, a major challenge is how to efficiently incorporate commonsense knowledge into such models. A recent approach regularizes relation and entity representations by propositionalization of first-order logic rules. However, propositionalization does not scale beyond domains with only few entities and rules. In this paper we present a highly efficient method for incorporating implication rules into distributed representations for automated knowledge base construction. We map entity-tuple embeddings into an approximately Boolean space and encourage a partial ordering over relation embeddings based on implication rules mined from WordNet. Surprisingly, we find that the strong restriction of the entity-tuple embedding space does not hurt the expressiveness of the model and even acts as a regularizer that improves generalization. By incorporating few commonsense rules, we achieve an increase of 2 percentage points mean average precision over a matrix factorization baseline, while observing a negligible increase in runtime.",0
This should summarize the main contributions. | How can I assist you today?,1
"Visual Question Answering (VQA) task has showcased a new stage of interaction between language and vision, two of the most pivotal components of artificial intelligence. However, it has mostly focused on generating short and repetitive answers, mostly single words, which fall short of rich linguistic capabilities of humans. We introduce Full-Sentence Visual Question Answering (FSVQA) dataset, consisting of nearly 1 million pairs of questions and full-sentence answers for images, built by applying a number of rule-based natural language processing techniques to original VQA dataset and captions in the MS COCO dataset. This poses many additional complexities to conventional VQA task, and we provide a baseline for approaching and evaluating the task, on top of which we invite the research community to build further improvements.",0
"This paper presents a large scale dataset for full sentence visual question answering based on images from Conceptual Captions. We introduce two variants of FSVQA - one where answers can only use entities within the image that appears along side the caption, and another variant where any entity mentioned anywhere is allowed as the answer. We evaluate several state-of-the-art baselines against our task, including fine tuned BERT models, pretrained CLIP-based Vision Language MoDel (VLMo) and Win-Knowledge Distilled Transformer (Waikiki), as well as a late fusion approach combining both vision and language features. Our findings indicate that while current SOTA methods perform reasonably well on our new benchmark task, there remains room for improvement in terms of accuracy which we hope to spur further research into the field through the release of our dataset.",1
"Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. Finally, given the recent surge of interest in this task, a competition was organized in 2015 using the newly released COCO dataset. We describe and analyze the various improvements we applied to our own baseline and show the resulting performance in the competition, which we won ex-aequo with a team from Microsoft Research, and provide an open source implementation in TensorFlow.",0
"This article presents an analysis of the results obtained by participating teams at the 2015 Microsoft COCO (Common Objects in Context) image captioning challenge. We first describe the dataset used in the competition which consisted of over 160,000 images along with five captions each that were generated through Amazon Mechanical Turk. We then present an evaluation of the top performing approaches using several metrics such as BLEU score, METEOR, ROUGE-L and CIDEr. Our findings reveal that while deep learning methods have made significant progress towards generating high quality natural language descriptions of images, there remains room for improvement in terms of accuracy and coherence of the generated captions. In conclusion, we discuss some of the key lessons learnt from the 2015 MSCOCO challenge which could inform future research directions in the field of computer vision and natural language processing.",1
Several machine learning problems arising in natural language processing can be modeled as a sequence labeling problem. We provide Gaussian process models based on pseudo-likelihood approximation to perform sequence labeling. Gaussian processes (GPs) provide a Bayesian approach to learning in a kernel based framework. The pseudo-likelihood model enables one to capture long range dependencies among the output components of the sequence without becoming computationally intractable. We use an efficient variational Gaussian approximation method to perform inference in the proposed model. We also provide an iterative algorithm which can effectively make use of the information from the neighboring labels to perform prediction. The ability to capture long range dependencies makes the proposed approach useful for a wide range of sequence labeling problems. Numerical experiments on some sequence labeling data sets demonstrate the usefulness of the proposed approach.,0
"Title: Gaussian Processes for Sequence Labeling =============================================  Gaussian processes have emerged as powerful tools for machine learning tasks involving regression or classification on high-dimensional input spaces. In recent years, they have been used successfully for diverse applications such as time series forecasting, computer vision, natural language processing, and more. However, one major drawback of using Gaussian process models in sequence labeling problems is their computational complexity, which can become prohibitively expensive for large datasets or complex kernels. This work proposes a novel approach based on pseudo-likelihood that provides fast approximations while maintaining good predictive performance. By leveraging the concept of sparse variational inference from Gaussian mixture modeling, we develop efficient algorithms that trade off some of the accuracy gained through full GP modeling, allowing us to scale our methodology to real-world application scenarios. Experimental results across several benchmark data sets demonstrate the effectiveness of our proposed technique, showing competitive performance against other state-of-the-art methods at significantly reduced computing cost. Overall, these findings open up new possibilities for applying Gaussian process models to challenging sequence labeling problems in a variety of domains where scalability remains a crucial concern.",1
"The task of associating images and videos with a natural language description has attracted a great amount of attention recently. Rapid progress has been made in terms of both developing novel algorithms and releasing new datasets. Indeed, the state-of-the-art results on some of the standard datasets have been pushed into the regime where it has become more and more difficult to make significant improvements. Instead of proposing new models, this work investigates the possibility of empirically establishing performance upper bounds on various visual captioning datasets without extra data labelling effort or human evaluation. In particular, it is assumed that visual captioning is decomposed into two steps: from visual inputs to visual concepts, and from visual concepts to natural language descriptions. One would be able to obtain an upper bound when assuming the first step is perfect and only requiring training a conditional language model for the second step. We demonstrate the construction of such bounds on MS-COCO, YouTube2Text and LSMDC (a combination of M-VAD and MPII-MD). Surprisingly, despite of the imperfect process we used for visual concept extraction in the first step and the simplicity of the language model for the second step, we show that current state-of-the-art models fall short when being compared with the learned upper bounds. Furthermore, with such a bound, we quantify several important factors concerning image and video captioning: the number of visual concepts captured by different models, the trade-off between the amount of visual elements captured and their accuracy, and the intrinsic difficulty and blessing of different datasets.",0
"Visual caption generation has become increasingly important as data becomes more accessible through APIs, which allows developers to create applications that can make sense out of images without human interaction. This process relies heavily on the use of Oracles, large text corpora used to generate image descriptions. This paper focuses on analyzing the impact different aspects such as the size of the corpus, quality of annotations, and type of language model have on the accuracy of the generated captions. Results from our experiments show that larger Oracles result in better captions but only up to a certain point after which there is no significant improvement. Additionally, high-quality annotations lead to higher accuracy compared to low-quality ones. Finally, we found that pre-training models with an Oracle leads to improvements over training them without any prior knowledge. Overall, these findings provide insights into how to optimize the performance of image description systems using Oracles.",1
"We develop an automated variational inference method for Bayesian structured prediction problems with Gaussian process (GP) priors and linear-chain likelihoods. Our approach does not need to know the details of the structured likelihood model and can scale up to a large number of observations. Furthermore, we show that the required expected likelihood term and its gradients in the variational objective (ELBO) can be estimated efficiently by using expectations over very low-dimensional Gaussian distributions. Optimization of the ELBO is fully parallelizable over sequences and amenable to stochastic optimization, which we use along with control variate techniques and state-of-the-art incremental optimization to make our framework useful in practice. Results on a set of natural language processing tasks show that our method can be as good as (and sometimes better than) hard-coded approaches including SVM-struct and CRFs, and overcomes the scalability limitations of previous inference algorithms based on sampling. Overall, this is a fundamental step to developing automated inference methods for Bayesian structured prediction.",0
"In many applications of machine learning, one needs to make predictions based on noisy observations of some unknown underlying functions that maps input variables into outcomes. A popular approach to this problem is to use probabilistic models, such as Gaussian processes (GPs), which can capture complex patterns and provide uncertainty estimates alongside their predictions. However, GP models often require large amounts of training data to achieve good predictive performance. This can become a bottleneck in practice, especially when dealing with high-dimensional inputs or expensive-to-compute objectives. To mitigate these limitations, researchers have developed methods for combining observed data with prior knowledge encoded in more interpretable functional forms, leading to semiparametric models that trade off flexibility and model complexity for improved efficiency. While successful, these approaches still rely heavily on tuning specific choices of basis functions, kernels, or other parameterized representations, which may not always generalize well across different tasks or domains. We propose an alternative strategy that instead combines ideas from gray-box identification in control theory—which uses both physical insights and measurement feedback to infer system parameters—with black-box GP regression. Our method learns to construct customized basis functions for capturing task-specific structures embedded within otherwise uninterpretable nonlinear covariance functions, enabling robust and efficient prediction even from relatively few samples or sparse datasets. Under mild assumptions, we prove oracle convergence rates and quantify adaptivity gains over standard kernel-basedGP",1
"A popular approach to semantic image understanding is to manually tag images with keywords and then learn a mapping from vi- sual features to keywords. Manually tagging images is a subjective pro- cess and the same or very similar visual contents are often tagged with different keywords. Furthermore, not all tags have the same descriptive power for visual contents and large vocabulary available from natural language could result in a very diverse set of keywords. In this paper, we propose an unsupervised visual theme discovery framework as a better (more compact, efficient and effective) alternative to semantic represen- tation of visual contents. We first show that tag based annotation lacks consistency and compactness for describing visually similar contents. We then learn the visual similarity between tags based on the visual features of the images containing the tags. At the same time, we use a natural language processing technique (word embedding) to measure the seman- tic similarity between tags. Finally, we cluster tags into visual themes based on their visual similarity and semantic similarity measures using a spectral clustering algorithm. We conduct user studies to evaluate the effectiveness and rationality of the visual themes discovered by our unsu- pervised algorithm and obtains promising result. We then design three common computer vision tasks, example based image search, keyword based image search and image labelling to explore potential applica- tion of our visual themes discovery framework. In experiments, visual themes significantly outperforms tags on semantic image understand- ing and achieve state-of-art performance in all three tasks. This again demonstrate the effectiveness and versatility of proposed framework.",0
"In recent years, there has been increasing interest in developing algorithms that can automatically discover meaningful visual themes from large image datasets. These themes can provide valuable insights into human behavior and preferences, as well as assist in organizing and searching through vast collections of images. However, traditional methods for extracting themes rely on analyzing either text or images alone, which may miss important contextual cues present in both modalities. This paper proposes a novel method for jointly modeling images and their associated texts to discover visual themes. We use advanced deep learning techniques to identify underlying patterns and relationships across the two modalities, enabling us to capture nuanced features and distinctions that would be difficult to achieve using separate models. Our approach outperforms state-of-the-art baseline methods across several evaluation metrics, demonstrating the effectiveness of our proposed model. Overall, our work represents a significant step forward in automatic visual theme discovery and opens up new possibilities for exploring complex visual-text data sets.",1
"Dictionary plays an important role in multi-instance data representation. It maps bags of instances to histograms. Earth mover's distance (EMD) is the most effective histogram distance metric for the application of multi-instance retrieval. However, up to now, there is no existing multi-instance dictionary learning methods designed for EMD based histogram comparison. To fill this gap, we develop the first EMD-optimal dictionary learning method using stochastic optimization method. In the stochastic learning framework, we have one triplet of bags, including one basic bag, one positive bag, and one negative bag. These bags are mapped to histograms using a multi-instance dictionary. We argue that the EMD between the basic histogram and the positive histogram should be smaller than that between the basic histogram and the negative histogram. Base on this condition, we design a hinge loss. By minimizing this hinge loss and some regularization terms of the dictionary, we update the dictionary instances. The experiments over multi-instance retrieval applications shows its effectiveness when compared to other dictionary learning methods over the problems of medical image retrieval and natural language relation classification.",0
"This research presents a novel method for improving the accuracy of histogram comparison using the earth mover's distance (EMD) by learning from stochastic instances of the problem. Traditional EMD-based histogram comparison methods require careful tuning of parameters to achieve good results, which can become computationally expensive and time-consuming. To address these issues, we propose a multi-instance dictionary approach that learns from multiple random samples of each image and automatically adjusts the weights assigned to different features, reducing the need for manual parameter tuning. Our algorithm uses a genetic algorithm to optimize the feature weights, resulting in improved accuracy while still maintaining low computational cost. Experimental results on several datasets show significant improvements over existing methods in terms of both accuracy and efficiency. Overall, our proposed method represents a promising new direction for histogram comparison that could lead to better performance in a wide range of applications.",1
"Visual Question Answering (VQA) is the task of taking as input an image and a free-form natural language question about the image, and producing an accurate answer. In this work we view VQA as a ""feature extraction"" module to extract image and caption representations. We employ these representations for the task of image-caption ranking. Each feature dimension captures (imagines) whether a fact (question-answer pair) could plausibly be true for the image and caption. This allows the model to interpret images and captions from a wide variety of perspectives. We propose score-level and representation-level fusion models to incorporate VQA knowledge in an existing state-of-the-art VQA-agnostic image-caption ranking model. We find that incorporating and reasoning about consistency between images and captions significantly improves performance. Concretely, our model improves state-of-the-art on caption retrieval by 7.1% and on image retrieval by 4.4% on the MSCOCO dataset.",0
"This research paper presents a new method for image caption ranking using visual question answering (VQA). VQA involves asking questions about images and then generating answers based on both the content of the image and natural language processing techniques. By leveraging VQA for image caption ranking, we aim to improve the accuracy and relevance of generated captions compared to traditional methods. We propose a novel approach that integrates VQA features with attention mechanisms to rank candidate captions for a given image. Our experiments show promising results in terms of precision, recall, and F1 score. Furthermore, our proposed model outperforms state-of-the-art baseline models, demonstrating the effectiveness of combining VQA features with caption ranking. Overall, our work represents an important step towards developing more advanced image captioning systems capable of generating high quality descriptions of complex visual scenes.",1
"Recurrent neural networks have recently been used for learning to describe images using natural language. However, it has been observed that these models generalize poorly to scenes that were not observed during training, possibly depending too strongly on the statistics of the text in the training data. Here we propose to describe images using short structured representations, aiming to capture the crux of a description. These structured representations allow us to tease-out and evaluate separately two types of generalization: standard generalization to new images with similar scenes, and generalization to new combinations of known entities. We compare two learning approaches on the MS-COCO dataset: a state-of-the-art recurrent network based on an LSTM (Show, Attend and Tell), and a simple structured prediction model on top of a deep network. We find that the structured model generalizes to new compositions substantially better than the LSTM, ~7 times the accuracy of predicting structured representations. By providing a concrete method to quantify generalization for unseen combinations, we argue that structured representations and compositional splits are a useful benchmark for image captioning, and advocate compositional models that capture linguistic and visual structure.",0
"This paper presents a method for teaching machines how to automatically learn to recognize objects in images that they have never seen before. We use deep learning techniques to analyze large collections of labeled data from previously seen categories, and then apply these learned representations to novel situations where we only have unlabeled images available. By doing so, our model can effectively discover which features correspond to different object classes, even without explicit supervision during training. To evaluate our approach, we compare it against several state-of-the-art baselines on several challenging datasets, demonstrating significant improvements in performance. Our results show promise towards developing systems capable of robustly handling real world applications involving zero shot recognition tasks.",1
"We present a self-contained system for constructing natural language models for use in text compression. Our system improves upon previous neural network based models by utilizing recent advances in syntactic parsing -- Google's SyntaxNet -- to augment character-level recurrent neural networks. RNNs have proven exceptional in modeling sequence data such as text, as their architecture allows for modeling of long-term contextual information.",0
"This abstract describes recent work that uses recurrent neural networks (RNN) in natural language processing tasks such as text compression, where the goal is to reduce the length of text while preserving meaning. RNN models have been shown to perform well on these tasks because they can capture dependencies over time, but there has been limited success using them specifically for text compression due to computational constraints and difficulties capturing longer term context.  We address these limitations by introducing a new model called the Synchronized Stacking Model (SSM), which synchronizes multiple stacked RNN layers with shared weights and biases. We find that SSM outperforms traditional LSTM models and other state-of-the-art methods across a range of benchmark datasets, achieving significantly better performance on all metrics including F1 score, accuracy, and perplexity. Our results demonstrate the potential for using deep learning techniques to improve natural language understanding tasks like text compression, even under resource constrained environments.  Our contributions are twofold: we introduce a novel architecture, the Synchronized Stacking Model, which leverages synchronization among stacked RNNs to overcome limitations of standard LSTM architectures; and we empirically validate our approach on four publicly available datasets. To facilitate further research in this area, we make our code and data publicly available online.",1
"Humans refer to objects in their environments all the time, especially in dialogue with other people. We explore generating and comprehending natural language referring expressions for objects in images. In particular, we focus on incorporating better measures of visual context into referring expression models and find that visual comparison to other objects within an image helps improve performance significantly. We also develop methods to tie the language generation process together, so that we generate expressions for all objects of a particular category jointly. Evaluation on three recent datasets - RefCOCO, RefCOCO+, and RefCOCOg, shows the advantages of our methods for both referring expression generation and comprehension.",0
Inferential contexts play a crucial role in our understanding of referring expressions such as “the”,1
"Our objective is video retrieval based on natural language queries. In addition, we consider the analogous problem of retrieving sentences or generating descriptions given an input video. Recent work has addressed the problem by embedding visual and textual inputs into a common space where semantic similarities correlate to distances. We also adopt the embedding approach, and make the following contributions: First, we utilize web image search in sentence embedding process to disambiguate fine-grained visual concepts. Second, we propose embedding models for sentence, image, and video inputs whose parameters are learned simultaneously. Finally, we show how the proposed model can be applied to description generation. Overall, we observe a clear improvement over the state-of-the-art methods in the video and sentence retrieval tasks. In description generation, the performance level is comparable to the current state-of-the-art, although our embeddings were trained for the retrieval tasks.",0
"Video understanding has progressed rapidly thanks to large scale datasets that provide pixel level annotations on millions of images and videos. However, these annotated data are often expensive to obtain and cannot capture the nuanced context present in natural language descriptions of the same video clip. To enable efficient transfer learning from textual descriptions, we learn joint representations of videos and sentences using an unpaired framework based on web image search. By enforcing cycle consistency between video frames, their generated captions, and retrieved web images, our model develops strong representations that can generalize across domains. We showcase compelling results comparing against methods relying exclusively on annotated datasets as well as those solely fine tuning on web data. Our approach could significantly lower annotation costs required for state-of-the-art approaches enabling wider adoption of advanced computer vision techniques into application scenarios.",1
"LightNet is a lightweight, versatile and purely Matlab-based deep learning framework. The idea underlying its design is to provide an easy-to-understand, easy-to-use and efficient computational platform for deep learning research. The implemented framework supports major deep learning architectures such as Multilayer Perceptron Networks (MLP), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). The framework also supports both CPU and GPU computation, and the switch between them is straightforward. Different applications in computer vision, natural language processing and robotics are demonstrated as experiments.",0
"In recent years, deep learning has become one of the most powerful techniques available for solving complex problems across a range of domains, from image classification and speech recognition to natural language processing and autonomous robots. Despite the successes achieved by deep learning algorithms, developing and training them can still be challenging due to the complexity involved and the need for specialized hardware resources. To address these issues, we present LightNet, a versatile and standalone MATLAB environment designed specifically for deep learning applications. LightNet provides users with access to state-of-the-art tools for building and training neural networks, as well as support for deploying models on both local devices and remote servers such as GPUs and cloud computing platforms like Amazon Web Services (AWS). In addition, LightNet offers advanced visualization capabilities that allow users to interactively explore their data sets and model performance metrics, making it easier than ever before to gain insights into how different parameters affect deep learning results. Overall, our contributions provide researchers and practitioners alike with a flexible platform capable of handling a wide variety of deep learning tasks while minimizing development time and computational expense compared to traditional methods. Our initial evaluation shows promising results that demonstrate the effectiveness and ease-of-use of LightNet for real world applications.",1
"Visual Question Answering (VQA) is a challenging task that has received increasing attention from both the computer vision and the natural language processing communities. Given an image and a question in natural language, it requires reasoning over visual elements of the image and general knowledge to infer the correct answer. In the first part of this survey, we examine the state of the art by comparing modern approaches to the problem. We classify methods by their mechanism to connect the visual and textual modalities. In particular, we examine the common approach of combining convolutional and recurrent neural networks to map images and questions to a common feature space. We also discuss memory-augmented and modular architectures that interface with structured knowledge bases. In the second part of this survey, we review the datasets available for training and evaluating VQA systems. The various datatsets contain questions at different levels of complexity, which require different capabilities and types of reasoning. We examine in depth the question/answer pairs from the Visual Genome project, and evaluate the relevance of the structured annotations of images with scene graphs for VQA. Finally, we discuss promising future directions for the field, in particular the connection to structured knowledge bases and the use of natural language processing models.",0
"Recent advances in deep learning have led to significant progress in the field of computer vision, enabling machines to perform tasks such as object detection, image segmentation, and scene understanding. One area that has received particular attention in recent years is visual question answering (VQA), which involves identifying relevant features from images and videos and providing natural language responses to questions posed by users. In this survey, we provide an overview of the methods used in VQA and discuss the current state of research on the topic. We also describe the major datasets available for training and evaluating VQA models and examine the challenges faced by these systems in terms of accuracy and interpretability. Finally, we review some future directions for research in VQA, including multi-modal input processing and zero-shot generalization across domains. Overall, this work seeks to provide a comprehensive summary of existing literature on VQA and identify promising areas for further study.",1
"Annotated datasets are commonly used in the training and evaluation of tasks involving natural language and vision (image description generation, action recognition and visual question answering). However, many of the existing datasets reflect problems that emerge in the process of data selection and annotation. Here we point out some of the difficulties and problems one confronts when creating and validating annotated vision and language datasets.",0
"""Vision and language is becoming one of the most promising research areas in artificial intelligence (AI) due to the availability of large datasets such as COCO, Visual Genome, and ImageNet. These datasets have played a crucial role in enabling groundbreaking advancements in AI models. However, creating high-quality vision and language datasets requires significant amounts of annotation labor, which can be time-consuming and expensive. This study focuses on developing efficient and scalable annotation methodologies that reduce the cost and effort required while maintaining high levels of accuracy and quality. We present various strategies for dataset creation that use active learning, crowd-sourcing, human feedback loops, hierarchical labeling schemes, and multi-label classification techniques. Our approaches were evaluated using standard metrics for annotation efficiency, inter-annotator agreement, intra-class correlation coefficient, F1 score, precision, recall, accuracy, and receiver operating characteristic curves. Results show that our methods significantly improve annotation speed and quality compared to traditional approaches. Additionally, we discuss potential applications of these methodologies for building real-world systems and their implications for future research.""",1
"Social network platforms can use the data produced by their users to serve them better. One of the services these platforms provide is recommendation service. Recommendation systems can predict the future preferences of users using their past preferences. In the recommendation systems literature there are various techniques, such as neighborhood based methods, machine-learning based methods and matrix-factorization based methods. In this work, a set of well known methods from natural language processing domain, namely Word2Vec, is applied to recommendation systems domain. Unlike previous works that use Word2Vec for recommendation, this work uses non-textual features, the check-ins, and it recommends venues to visit/check-in to the target users. For the experiments, a Foursquare check-in dataset is used. The results show that use of continuous vector space representations of items modeled by techniques of Word2Vec is promising for making recommendations.",0
This paper presents an overview of current advancements in natural language processing (NLP) that have made possible the creation of high-dimensional representations called word embeddings.,1
"We study the problem of structured prediction under test-time budget constraints. We propose a novel approach applicable to a wide range of structured prediction problems in computer vision and natural language processing. Our approach seeks to adaptively generate computationally costly features during test-time in order to reduce the computational cost of prediction while maintaining prediction performance. We show that training the adaptive feature generation system can be reduced to a series of structured learning problems, resulting in efficient training using existing structured learning algorithms. This framework provides theoretical justification for several existing heuristic approaches found in literature. We evaluate our proposed adaptive system on two structured prediction tasks, optical character recognition (OCR) and dependency parsing and show strong performance in reduction of the feature costs without degrading accuracy.",0
"This paper considers how structured prediction models can be improved by accounting for resource constraints during training. We propose a framework that leverages recent advances in continuous time modeling to optimize over both quality and computational cost. Our approach allows for flexible specification of computational costs for different operations, making it applicable to a wide range of problems. Experiments on several benchmark datasets show that our method leads to significant improvements over state-of-the-art algorithms while still maintaining competitive accuracy. Additionally, we provide insights into interpreting computational complexity as a proxy for human judgment effort, allowing us to make more informed decisions when designing complex machine learning systems. Overall, our work highlights the importance of considering resource constraints in developing reliable and efficient predictive models.",1
"Margin-based structured prediction commonly uses a maximum loss over all possible structured outputs \cite{Altun03,Collins04b,Taskar03}. In natural language processing, recent work \cite{Zhang14,Zhang15} has proposed the use of the maximum loss over random structured outputs sampled independently from some proposal distribution. This method is linear-time in the number of random structured outputs and trivially parallelizable. We study this family of loss functions in the PAC-Bayes framework under Gaussian perturbations \cite{McAllester07}. Under some technical conditions and up to statistical accuracy, we show that this family of loss functions produces a tighter upper bound of the Gibbs decoder distortion than commonly used methods. Thus, using the maximum loss over random structured outputs is a principled way of learning the parameter of structured prediction models. Besides explaining the experimental success of \cite{Zhang14,Zhang15}, our theoretical results show that more general techniques are possible.",0
"Abstract: This work presents a framework for structured prediction problems that enables efficient inference by leveraging Gaussian perturbations and linear-time principled algorithms. The proposed methodology overcomes traditional computational bottlenecks associated with complex models while maintaining competitive predictive performance on standard benchmark datasets. Our approach is based on a unifying optimization formulation which allows for seamless integration of existing methods within the same framework. Through extensive experimental evaluation, we demonstrate that our approach achieves state-of-the-art results across diverse application domains such as natural language processing, computer vision, and bioinformatics. Overall, our contributions enhance the applicability and scalability of structured prediction techniques for real-world applications.",1
"Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or ""temporally deep"", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are ""doubly deep""' in that they can be compositional in spatial and temporal ""layers"". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized.",0
"Title: ""Long-term Recurrent Convolutional Networks for Visual Recognition and Description""  Abstract: In recent years, deep learning has revolutionized the field of computer vision by achieving state-of-the-art performance on numerous tasks such as image classification, object detection, and semantic segmentation. One of the key architectures that have contributed to these successes is convolutional neural networks (CNNs). However, CNNs suffer from the problem of limited temporal context due to their fixed-size receptive fields, which can lead to difficulties in modeling relationships between distant elements within an image sequence. To overcome this limitation, we propose using Long-Term Recurrent Convolutional Neural Networks (LRCNN), which combine the strengths of both recurrent and convolutional models. Our proposed architecture utilizes LSTM cells within the network to capture long-range dependencies and maintain information over time, while retaining the capacity for spatial pattern recognition provided by convolutional layers. We evaluate our approach on several benchmark datasets for visual recognition and description tasks, including ImageNet and COCO, and demonstrate significant improvements compared to traditional CNN baselines. Our results show that incorporating memory into convolutional networks enables them to learn more complex representations, resulting in better generalization across different domains and increased accuracy in challenging tasks.",1
"Recent advances in image captioning task have led to increasing interests in video captioning task. However, most works on video captioning are focused on generating single input of aggregated features, which hardly deviates from image captioning process and does not fully take advantage of dynamic contents present in videos. We attempt to generate video captions that convey richer contents by temporally segmenting the video with action localization, generating multiple captions from multiple frames, and connecting them with natural language processing techniques, in order to generate a story-like caption. We show that our proposed method can generate captions that are richer in contents and can compete with state-of-the-art method without explicitly using video-level features as input.",0
"In video understanding tasks, automatic transcription has become more important due to recent advancements such as large language models like GPT-4 and datasets that provide ground truth text outputs (Ball et al., 2023). Automatic speech recognition systems can extract subtitles from spoken audio tracks with high accuracy. However, one major challenge in achieving better performance lies within the quality and structure of generated subtitles themselves. This research focuses on analyzing how natural language models generate multiple sentences per captions and explores whether these additional details improve their ability to convey complex narratives compared to single sentence summaries commonly used today (Nakamura et al., 2019). Our study analyzes over 5 million English YouTube videos where caption generation occurs through two distinct methods. We investigate both human transcribers who create detailed multi-sentence descriptions and those generated by modern LLMs like T5 and GPT-4 trained solely on text data without external supervision. Previous work highlighted limitations in using LLMs for video processing since they only predict short snippets that fail to capture temporal context effectively (Abu Bakar & Wahid, 2020; Zhang et al., 2022), which leads to poor translations with low coherency and meaningless content (Liu et al., 2021). Conversely, our experiments reveal that advanced LLMs achieve decent accuracy when generating longer sequences resembling full descriptions present in most movies or TV shows even without fine-tuning on specific domains (Ravuri et al., 2022). We observe diverse distributions across various domains ranging from news broadcasts, entertainment programs, gaming streams to educational courses. Notably, we also find generatio",1
"State-of-the-art methods for zero-shot visual recognition formulate learning as a joint embedding problem of images and side information. In these formulations the current best complement to visual features are attributes: manually encoded vectors describing shared characteristics among categories. Despite good performance, attributes have limitations: (1) finer-grained recognition requires commensurately more attributes, and (2) attributes do not provide a natural language interface. We propose to overcome these limitations by training neural language models from scratch; i.e. without pre-training and only consuming words and characters. Our proposed models train end-to-end to align with the fine-grained and category-specific content of images. Natural language provides a flexible and compact way of encoding only the salient visual aspects for distinguishing categories. By training on raw text, our model can do inference on raw text as well, providing humans a familiar mode both for annotation and retrieval. Our model achieves strong performance on zero-shot text-based image retrieval and significantly outperforms the attribute-based state-of-the-art for zero-shot classification on the Caltech UCSD Birds 200-2011 dataset.",0
"This sounds like an interesting topic! Abstracts are summaries that give readers an idea of what your paper covers without going into detail on every point. Here’s a possible version:  Our work focuses on learning deep representations of fine-grained visual descriptions. We aim to address a key challenge faced by computer vision systems – how to effectively represent the details present in images and video streams? Traditional methods rely on handcrafted features which may miss important subtleties present in real world data. In contrast, our approach leverages advances in deep learning architectures allowing models to learn richer representation directly from raw pixel input. Our experiments validate this direction demonstrating improved performance over prior art while using less computational resources. To foster reproducibility we make our code open source and provide detailed analyses discussing tradeoffs encountered during model development. Overall this research has the potential to broaden the scope of applications where computers can reliably operate within complex scenes involving diverse entities. Ultimately such technology stands to enhance human lives as decision making across numerous domains becomes augmented with robust artificial intelligence.",1
"Simple, short, and compact hashtags cover a wide range of information on social networks. Although many works in the field of natural language processing (NLP) have demonstrated the importance of hashtag recommendation, hashtag recommendation for images has barely been studied. In this paper, we introduce the HARRISON dataset, a benchmark on hashtag recommendation for real world images in social networks. The HARRISON dataset is a realistic dataset, composed of 57,383 photos from Instagram and an average of 4.5 associated hashtags for each photo. To evaluate our dataset, we design a baseline framework consisting of visual feature extractor based on convolutional neural network (CNN) and multi-label classifier based on neural network. Based on this framework, two single feature-based models, object-based and scene-based model, and an integrated model of them are evaluated on the HARRISON dataset. Our dataset shows that hashtag recommendation task requires a wide and contextual understanding of the situation conveyed in the image. As far as we know, this work is the first vision-only attempt at hashtag recommendation for real world images in social networks. We expect this benchmark to accelerate the advancement of hashtag recommendation.",0
"This benchmark provides state-of-the-art researchers and practitioners from all over the world with challenges related to image retrieval and hashtag recommendation using real images taken by users in social networks. Harrison includes five tasks where participants must build systems that can perform specific objectives such as generating relevant hashtags based on content extracted from photos, identifying whether two given tags share semantic meaning and recommending tags which could possibly go viral among users. By introducing these problems into a common framework under one competition, Harrison allows researchers to showcase their abilities in solving multi-disciplinary subproblems involving computer vision, natural language processing, and user behavior modeling. Moreover, the data provided alongside with clear rules enable participants without access to large datasets and computational resources to participate and compare their models against those built by experienced institutions and independent competitors alike. Overall, Harrison represents an excellent opportunity to bring together individuals dedicated to improving the performance of technologies used in social media platforms today while promoting collaborative advancements towards groundbreaking developments tomorrow.",1
"Traditional graph-based semi-supervised learning (SSL) approaches, even though widely applied, are not suited for massive data and large label scenarios since they scale linearly with the number of edges $|E|$ and distinct labels $m$. To deal with the large label size problem, recent works propose sketch-based methods to approximate the distribution on labels per node thereby achieving a space reduction from $O(m)$ to $O(\log m)$, under certain conditions. In this paper, we present a novel streaming graph-based SSL approximation that captures the sparsity of the label distribution and ensures the algorithm propagates labels accurately, and further reduces the space complexity per node to $O(1)$. We also provide a distributed version of the algorithm that scales well to large data sizes. Experiments on real-world datasets demonstrate that the new method achieves better performance than existing state-of-the-art algorithms with significant reduction in memory footprint. We also study different graph construction mechanisms for natural language applications and propose a robust graph augmentation strategy trained using state-of-the-art unsupervised deep learning architectures that yields further significant quality gains.",0
"In recent years, there has been a growing interest in semi-supervised learning (SSL) techniques which have achieved significant successes in a wide range of applications such as computer vision, natural language processing, and speech recognition. Despite their promising results, most SSL methods are designed for small-scale problems where data can fit into memory, limiting their applicability to large scale datasets. Furthermore, traditional centralized approaches often face challenges due to scalability issues caused by distributing massive amounts of data over networks. To address these limitations, we present a novel distributed algorithm that combines streaming approximation techniques and SSL ideas to learn robust models on very large datasets. Our approach makes use of both labeled and unlabeled data and adapts the model iteratively based on mini-batch stochastic gradient descent using streaming random projections. We evaluate our method on several benchmark datasets and demonstrate consistent improvement compared with state-of-the-art SSL algorithms while achieving significantly faster training times. Finally, we highlight key insights from experimental analyses that shed light on design tradeoffs between different components of our system. Overall, our work provides evidence that large scale distributed SSL systems are feasible and can deliver improved performance compared to current centralized alternatives.",1
"Learning the true ordering between objects by aggregating a set of expert opinion rank order lists is an important and ubiquitous problem in many applications ranging from social choice theory to natural language processing and search aggregation. We study the problem of unsupervised rank aggregation where no ground truth ordering information in available, neither about the true preference ordering between any set of objects nor about the quality of individual rank lists. Aggregating the often inconsistent and poor quality rank lists in such an unsupervised manner is a highly challenging problem, and standard consensus-based methods are often ill-defined, and difficult to solve. In this manuscript we propose a novel framework to bypass these issues by using object attributes to augment the standard rank aggregation framework. We design algorithms that learn joint models on both rank lists and object features to obtain an aggregated rank ordering that is more accurate and robust, and also helps weed out rank lists of dubious validity. We validate our techniques on synthetic datasets where our algorithm is able to estimate the true rank ordering even when the rank lists are corrupted. Experiments on three real datasets, MQ2008, MQ2008 and OHSUMED, show that using object features can result in significant improvement in performance over existing rank aggregation methods that do not use object information. Furthermore, when at least some of the rank lists are of high quality, our methods are able to effectively exploit their high expertise to output an aggregated rank ordering of great accuracy.",0
"This paper presents a novel approach for unsupervised rank aggregation using object features. We introduce monotone retargeting (MR), which applies a monotonic transformation to the input feature vectors so that they can be directly compared for ranking purposes. By doing so, we eliminate the need for explicit domain knowledge and complex training procedures. Our method outperforms state-of-the-art methods on several benchmark datasets while being computationally efficient and scalable. The results demonstrate the effectiveness of our approach for tackling challenges such as noise handling, inconsistency management, and preference modeling. Overall, MR provides a promising solution for unsupervised rank aggregation with object features by achieving superior performance while remaining simple and easy to implement.",1
"The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.",0
"Here is a possible abstract:  This paper presents a new method for generating sentences from a continuous space of vectors. We show how any continuous vector field can be converted into a sentence using our technique, which involves selecting appropriate words based on their semantic meaning and ordering them according to their grammatical structure. Our approach has two key advantages over previous methods that generate sentences from discrete spaces of tokens. First, we demonstrate empirically that sentences generated using our technique often have higher semantic coherence than those produced by other methods. Second, since our model operates directly on raw text data rather than preprocessed representations, such as word embeddings, our results generalize more easily across different domains and tasks. Overall, we believe that our work represents a significant step forward in natural language generation research and could have important applications in areas such as chatbots, conversational assistants, and automatic content creation.",1
"Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster.",0
"This paper provides an overview of our approach to using recurrent neural networks (RNN) to generate sequences of data. We demonstrate how sequence level training can be applied to a variety of tasks such as language modeling, sentiment analysis, and more. Our proposed method achieves state of the art performance on several benchmark datasets and outperforms traditional RNN models by a significant margin.",1
"Structured prediction is used in areas such as computer vision and natural language processing to predict structured outputs such as segmentations or parse trees. In these settings, prediction is performed by MAP inference or, equivalently, by solving an integer linear program. Because of the complex scoring functions required to obtain accurate predictions, both learning and inference typically require the use of approximate solvers. We propose a theoretical explanation to the striking observation that approximations based on linear programming (LP) relaxations are often tight on real-world instances. In particular, we show that learning with LP relaxed inference encourages integrality of training instances, and that tightness generalizes from train to test data.",0
"This paper presents a study on the effectiveness of linear programming (LP) relaxation techniques as a tool for structured prediction problems. We evaluate the train and test tightness of these relaxations by comparing their predictions to ground truth data, analyzing convergence rates and optimality gaps, and examining sensitivity analysis results. Our findings show that while LP relaxations can provide reasonable solutions, their accuracy may vary depending on the problem complexity and the strength of the LP formulation. In some cases, we observe significant differences between the training error of the LP relaxations and the test error on unseen data, which highlights the need for careful evaluation and validation of such methods. Additionally, our analysis reveals insights into how different factors such as feature scaling and regularization affect the performance of LP relaxations, providing guidance on how to optimize them for specific applications. Overall, our work provides valuable insight into the utility of LP relaxations for structured prediction tasks and contributes towards establishing best practices for their use.",1
"Understanding images with people often entails understanding their \emph{interactions} with other objects or people. As such, given a novel image, a vision system ought to infer which other objects/people play an important role in a given person's activity. However, existing methods are limited to learning action-specific interactions (e.g., how the pose of a tennis player relates to the position of his racquet when serving the ball) for improved recognition, making them unequipped to reason about novel interactions with actions or objects unobserved in the training data.   We propose to predict the ""interactee"" in novel images---that is, to localize the \emph{object} of a person's action. Given an arbitrary image with a detected person, the goal is to produce a saliency map indicating the most likely positions and scales where that person's interactee would be found. To that end, we explore ways to learn the generic, action-independent connections between (a) representations of a person's pose, gaze, and scene cues and (b) the interactee object's position and scale. We provide results on a newly collected UT Interactee dataset spanning more than 10,000 images from SUN, PASCAL, and COCO. We show that the proposed interaction-informed saliency metric has practical utility for four tasks: contextual object detection, image retargeting, predicting object importance, and data-driven natural language scene description. All four scenarios reveal the value in linking the subject to its object in order to understand the story of an image.",0
"This paper presents a study on how individuals perceive and interact with different objects and entities in their environment, and the impact that these interactions have on their sense of importance and priority. By examining the ways in which individuals localize and prioritize certain interactees over others, we aim to provide insights into the subjective experience of object significance within a person-centric framework. Our findings suggest that there are complex relationships between subjects and their objects, and that these relationships can vary greatly depending on contextual factors such as culture, age, gender, and personal history. Overall, our research highlights the need for a more nuanced understanding of human interaction with objects and environments, and offers new perspectives on how we might approach questions of meaning and value in everyday life.",1
"We propose a method for visual question answering which combines an internal representation of the content of an image with information extracted from a general knowledge base to answer a broad range of image-based questions. This allows more complex questions to be answered using the predominant neural network-based approach than has previously been possible. It particularly allows questions to be asked about the contents of an image, even when the image itself does not contain the whole answer. The method constructs a textual representation of the semantic content of an image, and merges it with textual information sourced from a knowledge base, to develop a deeper understanding of the scene viewed. Priming a recurrent neural network with this combined information, and the submitted question, leads to a very flexible visual question answering approach. We are specifically able to answer questions posed in natural language, that refer to information not contained in the image. We demonstrate the effectiveness of our model on two publicly available datasets, Toronto COCO-QA and MS COCO-VQA and show that it produces the best reported results in both cases.",0
"This paper presents a new approach to natural language processing that enables computers to find answers to questions asked by users without any prior training. Unlike traditional methods, which require large amounts of data and manual annotations, our method allows the computer to use external sources such as search engines to retrieve relevant information on demand. We evaluate our system using benchmark datasets and show that it outperforms state-of-the-art systems while providing more accurate and informative responses. Our approach has the potential to revolutionize how we interact with digital devices, making them truly intelligent assistants that can provide us with realtime knowledge about any topic.",1
"Impressive progress has been made in the fields of computer vision and natural language processing. However, it remains a challenge to find the best point of interaction for these very different modalities. In this chapter we discuss how attributes allow us to exchange information between the two modalities and in this way lead to an interaction on a semantic level. Specifically we discuss how attributes allow using knowledge mined from language resources for recognizing novel visual categories, how we can generate sentence description about images and video, how we can ground natural language in visual content, and finally, how we can answer natural language questions about images.",0
"This paper presents a unique approach towards bridging the gap between natural language understanding and computer vision tasks such as object recognition and detection. We propose that attributes can act as semantic units to represent knowledge from both modalities in a shared space. Our model learns these attributes by mining large amounts of image and text data using deep learning techniques, allowing for transfer learning across multiple domains. In experiments on several benchmark datasets, our method outperforms state-of-the-art approaches based on visual features alone, demonstrating the effectiveness of incorporating attribute knowledge into computer vision systems. Overall, we believe this work represents a significant step forward in integrating language and vision, opening up new possibilities for intelligent applications that rely on a combination of these two important sensory channels.",1
"With the recent popularity of animated GIFs on social media, there is need for ways to index them with rich metadata. To advance research on animated GIF understanding, we collected a new dataset, Tumblr GIF (TGIF), with 100K animated GIFs from Tumblr and 120K natural language descriptions obtained via crowdsourcing. The motivation for this work is to develop a testbed for image sequence description systems, where the task is to generate natural language descriptions for animated GIFs or video clips. To ensure a high quality dataset, we developed a series of novel quality controls to validate free-form text input from crowdworkers. We show that there is unambiguous association between visual content and natural language descriptions in our dataset, making it an ideal benchmark for the visual content captioning task. We perform extensive statistical analyses to compare our dataset to existing image and video description datasets. Next, we provide baseline results on the animated GIF description task, using three representative techniques: nearest neighbor, statistical machine translation, and recurrent neural networks. Finally, we show that models fine-tuned from our animated GIF description dataset can be helpful for automatic movie description.",0
"This paper presents a new dataset and benchmark focused on describing animated GIFs using natural language. We introduce a large-scale dataset of over one million human-generated descriptions paired with animated GIFs from social media platforms such as Reddit and Twitter. Our data collection pipeline ensures high quality annotations that capture both visual content and contextual meaning behind the animations. To evaluate the performance of state-of-the-art natural language processing models on our dataset, we create a diverse set of baseline models including pre-trained transformers, retrieval-based systems, and rule-based methods. Additionally, we present novel evaluation metrics tailored specifically for the task of animation description which better align with human judgement compared to traditional metrics used in image and video captioning tasks. Our experiments demonstrate promising results across all models and lay the groundwork for future research aimed at improving automatic animation understanding by generating rich, detailed, and accurate natural language descriptions.",1
"In this paper, we address the task of natural language object retrieval, to localize a target object within a given image based on a natural language query of the object. Natural language object retrieval differs from text-based image retrieval task as it involves spatial information about objects within the scene and global scene context. To address this issue, we propose a novel Spatial Context Recurrent ConvNet (SCRC) model as scoring function on candidate boxes for object retrieval, integrating spatial configurations and global scene-level contextual information into the network. Our model processes query text, local image descriptors, spatial configurations and global context features through a recurrent network, outputs the probability of the query text conditioned on each candidate box as a score for the box, and can transfer visual-linguistic knowledge from image captioning domain to our task. Experimental results demonstrate that our method effectively utilizes both local and global information, outperforming previous baseline methods significantly on different datasets and scenarios, and can exploit large scale vision and language datasets for knowledge transfer.",0
"In this paper, we present a new method for natural language object retrieval that utilizes deep learning techniques to effectively identify objects in images based on textual descriptions. Our approach uses convolutional neural networks (CNNs) to learn features from both image and text representations, enabling accurate identification of objects even if they appear in different contexts or under varying conditions. We evaluate our system on several benchmark datasets and demonstrate significantly improved performance compared to existing methods, achieving state-of-the-art results on many tasks. Our work shows the potential of using deep learning techniques for natural language object retrieval, and opens up new possibilities for advancing the field through continued research and development.",1
"Finding an informative subset of a large collection of data points or models is at the center of many problems in computer vision, recommender systems, bio/health informatics as well as image and natural language processing. Given pairwise dissimilarities between the elements of a `source set' and a `target set,' we consider the problem of finding a subset of the source set, called representatives or exemplars, that can efficiently describe the target set. We formulate the problem as a row-sparsity regularized trace minimization problem. Since the proposed formulation is, in general, NP-hard, we consider a convex relaxation. The solution of our optimization finds representatives and the assignment of each element of the target set to each representative, hence, obtaining a clustering. We analyze the solution of our proposed optimization as a function of the regularization parameter. We show that when the two sets jointly partition into multiple groups, our algorithm finds representatives from all groups and reveals clustering of the sets. In addition, we show that the proposed framework can effectively deal with outliers. Our algorithm works with arbitrary dissimilarities, which can be asymmetric or violate the triangle inequality. To efficiently implement our algorithm, we consider an Alternating Direction Method of Multipliers (ADMM) framework, which results in quadratic complexity in the problem size. We show that the ADMM implementation allows to parallelize the algorithm, hence further reducing the computational time. Finally, by experiments on real-world datasets, we show that our proposed algorithm improves the state of the art on the two problems of scene categorization using representative images and time-series modeling and segmentation using representative~models.",0
"This paper proposes a novel approach to sparse subset selection based on dissimilarity measures. The proposed method selects a small number of features that maximize their separation from the rest of the dataset while minimizing redundancy among themselves. We demonstrate through experimental results that our algorithm outperforms existing methods across a variety of data types and sparsity levels. Our framework provides significant flexibility by allowing different types of dissimilarities, such as Euclidean distance, Jaccard index, and Hamming distance, which can be chosen depending on the specific problem at hand. Additionally, we provide theoretical analysis showing that our method converges to a solution under mild assumptions and efficiently computes approximate solutions in practice. Our work has important applications in areas such as feature extraction, dimensionality reduction, and anomaly detection where identifying informative features is crucial.",1
"Tree kernels have demonstrated their ability to deal with hierarchical data, as the intrinsic tree structure often plays a discriminative role. While such kernels have been successfully applied to various domains such as nature language processing and bioinformatics, they mostly concentrate on ordered trees and whose nodes are described by symbolic data. Meanwhile, hierarchical representations have gained increasing interest to describe image content. This is particularly true in remote sensing, where such representations allow for revealing different objects of interest at various scales through a tree structure. However, the induced trees are unordered and the nodes are equipped with numerical features. In this paper, we propose a new structured kernel for hierarchical image representations which is built on the concept of subpath kernel. Experimental results on both artificial and remote sensing datasets show that the proposed kernel manages to deal with the hierarchical nature of the data, leading to better classification rates.",0
"Hierarchical image representations have emerged as a popular method for learning discriminative features from large datasets such as those used for object recognition and segmentation tasks. In recent years, kernel methods have proven effective at producing state-of-the-art results on these challenging problems. However, traditional kernel methods suffer from two limitations: they either require explicitly modeling subpaths using handcrafted features, or learn these models automatically but do so without considering how these paths may interact hierarchically to form more complex relationships. To address these issues, we propose a new algorithm that uses a novel ""subpath"" representation to capture both local and global relationships within images. Our approach takes advantage of advances in deep learning techniques while also leveraging the interpretability provided by path-based representations. We evaluate our method against several state-of-the-art alternatives and demonstrate improved performance across multiple benchmarks. Our work contributes towards the development of efficient, scalable, and explainable approaches for image classification and segmentation tasks.",1
"Visual Question and Answering (VQA) problems are attracting increasing interest from multiple research disciplines. Solving VQA problems requires techniques from both computer vision for understanding the visual contents of a presented image or video, as well as the ones from natural language processing for understanding semantics of the question and generating the answers. Regarding visual content modeling, most of existing VQA methods adopt the strategy of extracting global features from the image or video, which inevitably fails in capturing fine-grained information such as spatial configuration of multiple objects. Extracting features from auto-generated regions -- as some region-based image recognition methods do -- cannot essentially address this problem and may introduce some overwhelming irrelevant features with the question. In this work, we propose a novel Focused Dynamic Attention (FDA) model to provide better aligned image content representation with proposed questions. Being aware of the key words in the question, FDA employs off-the-shelf object detector to identify important regions and fuse the information from the regions and global features via an LSTM unit. Such question-driven representations are then combined with question representation and fed into a reasoning unit for generating the answers. Extensive evaluation on a large-scale benchmark dataset, VQA, clearly demonstrate the superior performance of FDA over well-established baselines.",0
"In recent years, there has been significant progress in developing models that can effectively interpret and generate natural language using deep learning techniques such as recurrent neural networks (RNNs) and transformer architectures. However, these models often struggle with understanding complex visual scenes and answering questions related to them. This paper proposes a novel model called ""Focused Dynamic Attention"" (FDA) that improves upon existing methods for Visual Question Answering (VQA).  The key challenge facing VQA algorithms is the difficulty of integrating textual knowledge with visual representations. Traditional approaches have used static attention mechanisms that fix their focus on specific parts of the image, making it difficult to adaptively weigh different regions based on changing context. Our proposed approach addresses this limitation by incorporating dynamic attention weights which change dynamically during both training and inference time.  Our model uses multi-step reasoning to iteratively refine its internal representation of the question and image. At each step, our FDA module selectively attends to different parts of the input based on current needs, allowing our model to gradually build up more detailed and accurate answers. We show through extensive experiments that our approach outperforms several state-of-the-art baselines across multiple benchmark datasets.  Overall, our work demonstrates the effectiveness of combining dynamic attention and multi-step reasoning for solving challenging tasks like VQA. By leveraging advanced computational tools and deep learning techniques, we can develop models that better understand and interact with complex visual environments. These findings have important implications for areas such as computer vision, human-computer interaction, and artificial intelligence, where being able to process and respond to visually rich inputs is critical.",1
"We propose a novel attention based deep learning architecture for visual question answering task (VQA). Given an image and an image related natural language question, VQA generates the natural language answer for the question. Generating the correct answers requires the model's attention to focus on the regions corresponding to the question, because different questions inquire about the attributes of different image regions. We introduce an attention based configurable convolutional neural network (ABC-CNN) to learn such question-guided attention. ABC-CNN determines an attention map for an image-question pair by convolving the image feature map with configurable convolutional kernels derived from the question's semantics. We evaluate the ABC-CNN architecture on three benchmark VQA datasets: Toronto COCO-QA, DAQUAR, and VQA dataset. ABC-CNN model achieves significant improvements over state-of-the-art methods on these datasets. The question-guided attention generated by ABC-CNN is also shown to reflect the regions that are highly relevant to the questions.",0
"This should provide a good outline of the structure of your paper. Be concise, but clear and complete.  This work proposes a novel attention based convolutional neural network (ABC-CNN) for visual question answering (VQA). VQA is a challenging task that requires understanding both image content and natural language queries. To address this problem, we propose an end-to-end trainable deep learning model that jointly reasons over vision and text. Our proposed architecture uses selective regional attention mechanisms inspired by human attentional processes to focus on relevant regions of the input images during encoding. We employ several layers of dense nonlinear processing which lead to higher level representations from which the answers can be easily extracted using bilinear pooling techniques. We evaluate our method on standard benchmarks such as CLEVR, GQA, and HFQA and show consistent improvements over prior state-of-the-art models across all datasets while requiring less computational overhead. In summary, we demonstrate the effectiveness of our proposed ABC-CNN approach on multiple VQA tasks and provide insights into future directions for researchers working in this field.",1
"Learning from multiple-relational data which contains noise, ambiguities, or duplicate entities is essential to a wide range of applications such as statistical inference based on Web Linked Data, recommender systems, computational biology, and natural language processing. These tasks usually require working with very large and complex datasets - e.g., the Web graph - however, current approaches to multi-relational learning are not practical for such scenarios due to their high computational complexity and poor scalability on large data.   In this paper, we propose a novel and scalable approach for multi-relational factorization based on consensus optimization. Our model, called ConsMRF, is based on the Alternating Direction Method of Multipliers (ADMM) framework, which enables us to optimize each target relation using a smaller set of parameters than the state-of-the-art competitors in this task.   Due to ADMM's nature, ConsMRF can be easily parallelized which makes it suitable for large multi-relational data. Experiments on large Web datasets - derived from DBpedia, Wikipedia and YAGO - show the efficiency and performance improvement of ConsMRF over strong competitors. In addition, ConsMRF near-linear scalability indicates great potential to tackle Web-scale problem sizes.",0
"Multi-relational learning refers to the task of modeling complex relationships between entities represented as nodes in multiple graphs or networks. This problem arises naturally in many domains such as social network analysis, biological pathway inference, and semantic knowledge representation. In practice, real world data often contains millions or billions of edges connecting entities, making it difficult to learn meaningful relationships efficiently. In this work, we propose to use Alternating Direction Method of Multipliers (ADMM) to solve large scale multi-relational problems while guaranteeing convergence to optimal solutions. Our framework scales well on both dense and sparse datasets, outperforming baseline methods by up to two orders of magnitude speedup on some tasks. We provide theoretical guarantees on solution quality and tractability of our algorithm under mild assumptions. Furthermore, we demonstrate applications of our method across several domains including bioinformatics and natural language processing, showing promising results for future research opportunities. Overall, our contributions open new possibilities for large scale relational learning that were previously impractical due to computational limitations.",1
"Understanding language goes hand in hand with the ability to integrate complex contextual information obtained via perception. In this work, we present a novel task for grounded language understanding: disambiguating a sentence given a visual scene which depicts one of the possible interpretations of that sentence. To this end, we introduce a new multimodal corpus containing ambiguous sentences, representing a wide range of syntactic, semantic and discourse ambiguities, coupled with videos that visualize the different interpretations for each sentence. We address this task by extending a vision model which determines if a sentence is depicted by a video. We demonstrate how such a model can be adjusted to recognize different interpretations of the same underlying sentence, allowing to disambiguate sentences in a unified fashion across the different ambiguity types.",0
"Understanding language can often involve resolving ambiguity, which arises from the flexibility of natural languages. While there have been many studies on how context affects comprehension, few consider the visual modality during text processing. We present findings showing that visual context (in particular, pictures) can resolve linguistic ambiguity beyond semantic constraints. In three experiments, we explore whether/how seeing a picture affects how readers interpret an ambiguous sentence: one where an adjective has two meanings based on syntax or semantics. Results reveal cross-modal interactions during language understanding via attentional and representational factors. They highlight the importance of considering multiple modalities in developing predictive models for human cognition.",1
"Building upon recent Deep Neural Network architectures, current approaches lying in the intersection of computer vision and natural language processing have achieved unprecedented breakthroughs in tasks like automatic captioning or image retrieval. Most of these learning methods, though, rely on large training sets of images associated with human annotations that specifically describe the visual content. In this paper we propose to go a step further and explore the more complex cases where textual descriptions are loosely related to the images. We focus on the particular domain of News articles in which the textual content often expresses connotative and ambiguous relations that are only suggested but not directly inferred from images. We introduce new deep learning methods that address source detection, popularity prediction, article illustration and geolocation of articles. An adaptive CNN architecture is proposed, that shares most of the structure for all the tasks, and is suitable for multitask and transfer learning. Deep Canonical Correlation Analysis is deployed for article illustration, and a new loss function based on Great Circle Distance is proposed for geolocation. Furthermore, we present BreakingNews, a novel dataset with approximately 100K news articles including images, text and captions, and enriched with heterogeneous meta-data (such as GPS coordinates and popularity metrics). We show this dataset to be appropriate to explore all aforementioned problems, for which we provide a baseline performance using various Deep Learning architectures, and different representations of the textual and visual features. We report very promising results and bring to light several limitations of current state-of-the-art in this kind of domain, which we hope will help spur progress in the field.",0
"In recent years, article annotation has become increasingly popular due to its potential applications in fields such as education, research, and news reporting. However, traditional article annotation methods require manual input and can be time-consuming and subjective. This paper proposes a novel method that utilizes image and text processing techniques to automate article annotation. Our proposed approach uses natural language processing (NLP) algorithms to extract important entities from the article, followed by computer vision techniques to find images relevant to those entities. These annotations provide additional contextual information for readers and enable more effective communication of complex ideas. Furthermore, our system provides a user interface that allows users to add their own comments and share these annotated articles with others, further facilitating collaboration and knowledge sharing. Overall, our work presents a significant advancement towards automatic article annotation using NLP and computer vision.",1
"In this paper we approach the novel problem of segmenting an image based on a natural language expression. This is different from traditional semantic segmentation over a predefined set of semantic classes, as e.g., the phrase ""two men sitting on the right bench"" requires segmenting only the two people on the right bench and no one standing or sitting on another bench. Previous approaches suitable for this task were limited to a fixed set of categories and/or rectangular regions. To produce pixelwise segmentation for the language expression, we propose an end-to-end trainable recurrent and convolutional network model that jointly learns to process visual and linguistic information. In our model, a recurrent LSTM network is used to encode the referential expression into a vector representation, and a fully convolutional network is used to a extract a spatial feature map from the image and output a spatial response map for the target object. We demonstrate on a benchmark dataset that our model can produce quality segmentation output from the natural language expression, and outperforms baseline methods by a large margin.",0
"An abstract can give readers a concise summary of your entire paper, so make sure that you address all major points within the scope of your research study. This includes discussing any background work leading up to your investigation, as well as your own findings and conclusions drawn from them. Make sure to cite other papers if necessary. Try to keep the language simple yet academic sounding.",1
"Generating natural language descriptions for in-the-wild videos is a challenging task. Most state-of-the-art methods for solving this problem borrow existing deep convolutional neural network (CNN) architectures (AlexNet, GoogLeNet) to extract a visual representation of the input video. However, these deep CNN architectures are designed for single-label centered-positioned object classification. While they generate strong semantic features, they have no inherent structure allowing them to detect multiple objects of different sizes and locations in the frame. Our paper tries to solve this problem by integrating the base CNN into several fully convolutional neural networks (FCNs) to form a multi-scale network that handles multiple receptive field sizes in the original image. FCNs, previously applied to image segmentation, can generate class heat-maps efficiently compared to sliding window mechanisms, and can easily handle multiple scales. To further handle the ambiguity over multiple objects and locations, we incorporate the Multiple Instance Learning mechanism (MIL) to consider objects in different positions and at different scales simultaneously. We integrate our multi-scale multi-instance architecture with a sequence-to-sequence recurrent neural network to generate sentence descriptions based on the visual representation. Ours is the first end-to-end trainable architecture that is capable of multi-scale region processing. Evaluation on a Youtube video dataset shows the advantage of our approach compared to the original single-scale whole frame CNN model. Our flexible and efficient architecture can potentially be extended to support other video processing tasks.",0
"Title: A Multi-scale Approach for Video Captioning using Multiple Instance Learning Abstract: Video description generation is challenged by diverse scenes where objects and actions are distributed differently across frames and scales, requiring multi-level understanding and representation. We present a novel approach that leverages multiple instance learning (MIL) to integrate both intra-frame and inter-frame contexts. Our method first generates object proposals at different scales, which then serve as instances for frame-level MIL classification. To effectively incorporate spatiotemporal relations between frames, we propose a temporal pyramid framework to encode global video features for each frame, enabling efficient aggregation of evidence from adjacent frames in a sliding window manner. Experiments on two benchmark datasets demonstrate our method achieves state-of-the-art performance, significantly outperforming prior work under all evaluation metrics. Overall, our multi-scale MIL network provides more comprehensive and precise descriptions, opening up promising directions for further research into video analysis tasks.",1
"Automatically generating a natural language description of an image has attracted interests recently both because of its importance in practical applications and because it connects two major artificial intelligence fields: computer vision and natural language processing. Existing approaches are either top-down, which start from a gist of an image and convert it into words, or bottom-up, which come up with words describing various aspects of an image and then combine them. In this paper, we propose a new algorithm that combines both approaches through a model of semantic attention. Our algorithm learns to selectively attend to semantic concept proposals and fuse them into hidden states and outputs of recurrent neural networks. The selection and fusion form a feedback connecting the top-down and bottom-up computation. We evaluate our algorithm on two public benchmarks: Microsoft COCO and Flickr30K. Experimental results show that our algorithm significantly outperforms the state-of-the-art approaches consistently across different evaluation metrics.",0
"This may sound like too many constraints! However, here goes:  Image caption generation has been gaining popularity due to recent advances in computer vision and natural language processing (NLP). In this work we propose a novel approach that combines these two fields using semantic attention. Our model takes as input both visual features from convolutional neural networks (CNNs) and contextual features from recurrent NLP models such as Long Short-Term Memory (LSTM) networks. We then apply a combination of spatial and temporal attention mechanisms to generate semantically meaningful captions. Experimental results show improved performance on standard benchmark datasets over baseline methods. Overall, our method represents another step towards intelligent systems capable of generating human-like descriptions of images.",1
"Motivated by the recent progress in generative models, we introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. After training on Microsoft COCO, we compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.",0
"Abstract: In natural language processing tasks such as machine translation, summarization, or question answering, attention mechanisms have shown great success by focusing on salient regions of the input sequence. Motivated by these findings, we explore the use of attentional techniques in computer vision. We present two novel models, Show & Tell (S&T) and ImageNet Roulette (IR), that generate images from descriptions by selecting and reweighting regions of real images according to their relevance to a given textual prompt. Both models build upon a pretrained convolutional neural network (CNN) using region-specific affine transformations to emphasize attended areas while minimizing distraction from irrelevant image content. Our approach outperforms alternative methods based on random cropping, pixel manipulation, or patch selection and demonstrates significant improvement over state-of-the-art text-to-image systems. To further study the capabilities of our models, we conduct analyses investigating the generated samples, visualized attended regions, and system behavior under adversarial settings. Code and pretrained models accompany our work to encourage reproducibility and future research opportunities.",1
"Recurrent neural network(RNN) has been broadly applied to natural language processing(NLP) problems. This kind of neural network is designed for modeling sequential data and has been testified to be quite efficient in sequential tagging tasks. In this paper, we propose to use bi-directional RNN with long short-term memory(LSTM) units for Chinese word segmentation, which is a crucial preprocess task for modeling Chinese sentences and articles. Classical methods focus on designing and combining hand-craft features from context, whereas bi-directional LSTM network(BLSTM) does not need any prior knowledge or pre-designing, and it is expert in keeping the contextual information in both directions. Experiment result shows that our approach gets state-of-the-art performance in word segmentation on both traditional Chinese datasets and simplified Chinese datasets.",0
"Automatic word segmentation of texts written in the Chinese language has been challenging due to the complexities associated with characters that may represent more than one word (logographs) in the text. Most state-of-the-art methods employ machine learning techniques such as Hidden Markov Models (HMM), Conditional Random Fields (CRF), or Artificial Neural Networks (ANN). This study proposes using a bi-directional Long Short Term Memory (LSTM) recurrent neural network model to perform word segmentation on Chinese text data. Experimental results show that our method outperforms previously reported accuracy levels achieved by other models. We evaluate three benchmark datasets consisting of news articles, narratives, and microblog posts. Our approach obtains F1 scores ranging from 92.8% to 96%, achieving significant improvements over existing methods. Moreover, we demonstrate how the use of bidirectional training can effectively capture dependencies between sequential data points in both forward and backward directions within the LSTM architecture. In summary, this research presents an effective solution towards accurate word segmentation for Chinese natural language processing tasks.",1
"Gaussian Processes (GPs) provide a general and analytically tractable way of modeling complex time-varying, nonparametric functions. The Automatic Bayesian Covariance Discovery (ABCD) system constructs natural-language description of time-series data by treating unknown time-series data nonparametrically using GP with a composite covariance kernel function. Unfortunately, learning a composite covariance kernel with a single time-series data set often results in less informative kernel that may not give qualitative, distinctive descriptions of data. We address this challenge by proposing two relational kernel learning methods which can model multiple time-series data sets by finding common, shared causes of changes. We show that the relational kernel learning methods find more accurate models for regression problems on several real-world data sets; US stock data, US house price index data and currency exchange rate data.",0
"What kind of relationships can we identify among statistics? And how do these relate to our understanding of the world? These questions guide us through this research project focused on exploring how relational thinking can improve our comprehension and use of statistical techniques. Our findings suggest that viewing data as embedded within networks sheds light on their meaning and provides novel ways of analyzing them. With applications spanning diverse fields such as genetics and finance, this study establishes the importance of adopting a holistic approach towards statistics rather than relying solely on traditional reductionist methods. By considering interactions and dependencies between variables, we open up new perspectives on existing problems and uncover hidden patterns that were previously unknown. Overall, we demonstrate that embracing complexity can significantly enhance the power of statistics to describe and explain complex systems, ultimately leading to better predictions and decision making processes.",1
"The proliferation of sensor devices monitoring human activity generates voluminous amount of temporal sequences needing to be interpreted and categorized. Moreover, complex behavior detection requires the personalization of multi-sensor fusion algorithms. Conditional random fields (CRFs) are commonly used in structured prediction tasks such as part-of-speech tagging in natural language processing. Conditional probabilities guide the choice of each tag/label in the sequence conflating the structured prediction task with the sequence classification task where different models provide different categorization of the same sequence. The claim of this paper is that CRF models also provide discriminative models to distinguish between types of sequence regardless of the accuracy of the labels obtained if we calibrate the class membership estimate of the sequence. We introduce and compare different neural network based linear-chain CRFs and we present experiments on two complex sequence classification and structured prediction tasks to support this claim.",0
"In this work, we propose a novel approach for sequence classification using neural conditional random fields (NCRF). We build upon traditional CRF models by replacing fixed feature functions with deep neural networks that learn distributed representations of input sequences. Our model can capture complex interactions within sequences and handle high-dimensional inputs, while still enjoying efficient inference using exact MAP predictions with polynomial running time in the size of the output vocabulary. Empirical results on challenging benchmark datasets show substantial improvements over state-of-the-art baselines across different tasks and metrics, demonstrating the effectiveness of our proposed methodology.",1
"Generating natural language descriptions for images is a challenging task. The traditional way is to use the convolutional neural network (CNN) to extract image features, followed by recurrent neural network (RNN) to generate sentences. In this paper, we present a new model that added memory cells to gate the feeding of image features to the deep neural network. The intuition is enabling our model to memorize how much information from images should be fed at each stage of the RNN. Experiments on Flickr8K and Flickr30K datasets showed that our model outperforms other state-of-the-art models with higher BLEU scores.",0
"Artificial intelligence has made great strides recently, allowing researchers to create systems that can describe images using deep recurrent neural networks (RNNs) paired with memory cells. This approach relies on processing features from the image itself instead of external contextual information such as metadata. By employing attention mechanisms and memory cells, these models have shown improved performance compared to previous methods. In particular, our work focuses on utilizing a specific variant called Differential Neural Computer (DNC). We evaluate several different settings for training and testing this model by conducting experiments on two benchmark datasets: SSBJ and COCO. Our results demonstrate that our method outperforms strong baselines significantly while providing more accurate descriptions of images overall. This study provides insight into how artificial intelligence can generate natural language descriptions of visual data effectively. As future work, we aim to explore alternative architectures beyond DNC as well as incorporating cross-modal input sources to improve accuracy even further. Keywords: Deep Learning, Recurrent Neural Networks, Memory Cells, Attention Mechanism, Image Description",1
"This paper presents stacked attention networks (SANs) that learn to answer natural language questions from images. SANs use semantic representation of a question as query to search for the regions in an image that are related to the answer. We argue that image question answering (QA) often requires multiple steps of reasoning. Thus, we develop a multiple-layer SAN in which we query an image multiple times to infer the answer progressively. Experiments conducted on four image QA data sets demonstrate that the proposed SANs significantly outperform previous state-of-the-art approaches. The visualization of the attention layers illustrates the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer.",0
"Title: Improving Image Question Answering using Stacked Attention Networks Abstract In recent years, image question answering (QA) has emerged as a challenging task that requires both visual recognition and natural language understanding abilities. Many approaches have been proposed to tackle this problem, including attention mechanisms, multimodal fusion techniques, and transfer learning from large datasets. However, these methods often struggle to capture important spatial relationships among object regions within images, which can limit their performance. To address this limitation, we introduce stacked attention networks, a novel architecture that utilizes multiple attention layers to progressively identify and focus on relevant image regions. Our approach builds upon existing state-of-the-art QA models by integrating additional attention modules that operate at different levels of granularity. This allows us to model complex dependencies between objects and their attributes more effectively. We evaluate our method on several benchmark datasets and demonstrate substantial improvements over baseline models across all metrics. Additionally, we perform ablation studies to investigate the contribution of each component in our design. Our results confirm that stacked attention networks provide a powerful framework for image QA, capable of achieving top performances while outperforming competitive methods. Overall, this work offers new insights into how deep neural architectures can be designed to better handle intricate data modalities like images and natural language.",1
"Suppose that we are given a set of videos, along with natural language descriptions in the form of multiple sentences (e.g., manual annotations, movie scripts, sport summaries etc.), and that these sentences appear in the same temporal order as their visual counterparts. We propose in this paper a method for aligning the two modalities, i.e., automatically providing a time stamp for every sentence. Given vectorial features for both video and text, we propose to cast this task as a temporal assignment problem, with an implicit linear mapping between the two feature modalities. We formulate this problem as an integer quadratic program, and solve its continuous convex relaxation using an efficient conditional gradient algorithm. Several rounding procedures are proposed to construct the final integer solution. After demonstrating significant improvements over the state of the art on the related task of aligning video with symbolic labels [7], we evaluate our method on a challenging dataset of videos with associated textual descriptions [36], using both bag-of-words and continuous representations for text.",0
"In recent years, advances in natural language processing have enabled machines to better understand human textual input, but these models often lack alignment with the visual world. This presents a challenge in video understanding tasks where both modalities need to work together. Unlike other approaches that rely on fully supervised methods which require large amounts of labeled data, we propose a weakly-supervised approach for aligning video frames with their corresponding text descriptions. Our method utilizes pretrained representations from both vision and language models and exploits temporal consistency constraints using unlabeled videos as well as weak annotator guidance. Experimental results demonstrate significant improvements over previous methods without requiring massive quantities of annotations. This work addresses the problem of aligned video representation learning under limited supervision and has potential applications in areas such as machine translation, summarization, question answering, and retrieval.",1
"This paper presents a restricted visual Turing test (VTT) for story-line based deep understanding in long-term and multi-camera captured videos. Given a set of videos of a scene (such as a multi-room office, a garden, and a parking lot.) and a sequence of story-line based queries, the task is to provide answers either simply in binary form ""true/false"" (to a polar query) or in an accurate natural language description (to a non-polar query). Queries, polar or non-polar, consist of view-based queries which can be answered from a particular camera view and scene-centered queries which involves joint inference across different cameras. The story lines are collected to cover spatial, temporal and causal understanding of input videos. The data and queries distinguish our VTT from recently proposed visual question answering in images and video captioning. A vision system is proposed to perform joint video and query parsing which integrates different vision modules, a knowledge base and a query engine. The system provides unified interfaces for different modules so that individual modules can be reconfigured to test a new method. We provide a benchmark dataset and a toolkit for ontology guided story-line query generation which consists of about 93.5 hours videos captured in four different locations and 3,426 queries split into 127 story lines. We also provide a baseline implementation and result analyses.",0
"In recent years, deep learning methods have achieved remarkable performance on computer vision tasks such as object recognition, semantic segmentation, and image generation. However, these models often rely heavily on large amounts of annotated data and can struggle when presented with novel scenarios that they haven't seen during training. To address these limitations, we propose a restricted version of the classic Turing test called the ""Restricted Visual Turing Test"" (RVTT), which focuses specifically on scene and event understanding tasks. Our approach uses pre-trained Convolutional Neural Networks (CNN) as visual encoders, along with recurrent units and attention mechanisms to model temporal dependencies. We evaluate our method on several challenging datasets and show significant improvements over baseline methods, demonstrating the effectiveness of our approach at task generalization across diverse scenarios. This work has important implications for advancing artificial intelligence towards more human-like understanding and problem-solving abilities, particularly in complex domains such as robotics and virtual reality applications.",1
"Deep learning has recently achieved very promising results in a wide range of areas such as computer vision, speech recognition and natural language processing. It aims to learn hierarchical representations of data by using deep architecture models. In a smart city, a lot of data (e.g. videos captured from many distributed sensors) need to be automatically processed and analyzed. In this paper, we review the deep learning algorithms applied to video analytics of smart city in terms of different research topics: object detection, object tracking, face recognition, image classification and scene labeling.",0
"Abstract: This survey paper provides an overview of deep learning algorithms and their applications to video analytics for smart cities. We begin by introducing the concept of smart cities and explaining how video analytics plays a crucial role in improving city management and infrastructure. Next, we discuss different types of video data that can be analyzed using deep learning techniques. Then, we provide a detailed review of state-of-the-art deep learning methods used in video analytics for smart cities, including object detection, tracking, classification, action recognition, semantic segmentation, and anomaly detection. Finally, we conclude with future directions and open challenges in this field. Overall, this survey paper serves as a comprehensive resource for researchers and practitioners who are interested in exploring the potential of deep learning for enhancing the safety and efficiency of modern urban environments.",1
"Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standard for various challenging machine learning problems, e.g., image classification, natural language processing or human action recognition. Although these methods perform impressively well, they have a significant disadvantage, the lack of transparency, limiting the interpretability of the solution and thus the scope of application in practice. Especially DNNs act as black boxes due to their multilayer nonlinear structure. In this paper we introduce a novel methodology for interpreting generic multilayer neural networks by decomposing the network classification decision into contributions of its input elements. Although our focus is on image classification, the method is applicable to a broad set of input data, learning tasks and network architectures. Our method is based on deep Taylor decomposition and efficiently utilizes the structure of the network by backpropagating the explanations from the output to the input layer. We evaluate the proposed method empirically on the MNIST and ILSVRC data sets.",0
"This should work as well: ""This is an excellent abstract for a research paper on Explaining NonLinear Classification Decisions with Deep Taylor Decomposition. In this paper, we propose a novel framework that utilizes deep Taylor decomposition to explain nonlinear classification decisions made by machine learning models. We show how our method can effectively identify important features and dependencies, providing insight into model behavior. Our experiments demonstrate the effectiveness of our approach, highlighting its ability to provide meaningful explanations even for complex models and datasets. Overall, this work represents an important contribution to the field, with implications for a variety of applications such as fraud detection, medical diagnosis, and self-driving cars.""",1
"Programming language processing (similar to natural language processing) is a hot research topic in the field of software engineering; it has also aroused growing interest in the artificial intelligence community. However, different from a natural language sentence, a program contains rich, explicit, and complicated structural information. Hence, traditional NLP models may be inappropriate for programs. In this paper, we propose a novel tree-based convolutional neural network (TBCNN) for programming language processing, in which a convolution kernel is designed over programs' abstract syntax trees to capture structural information. TBCNN is a generic architecture for programming language processing; our experiments show its effectiveness in two different program analysis tasks: classifying programs according to functionality, and detecting code snippets of certain patterns. TBCNN outperforms baseline methods, including several neural models for NLP.",0
"In this research we introduce DeepTrees: convolutional neural networks (CNN) over tree structures, which generalize traditional flat CNN architectures to trees. We present DeepTreeParsers, a novel parsing method that combines deep learning techniques from CNNs with combinatorial optimization inspired by deep precedence parsing rules. Our approach outperforms previous state-of-the-art systems such as those used in widely popular code editors like Sublime Text, Visual Studio Code, and Atom. For example, on C/C++ files, our model achieves up to twice faster parser speed while being more accurate than other methods on large software repositories available online. Moreover, we empirically show that our proposed approach improves fault localization capabilities, helping developers identify bugs faster. Finally, our study provides insights into how different programming constructs affect the performance of parsing models. These results demonstrate the potential of DeepTrees and CNNs over tree structures in natural language processing applications beyond programming languages. However, there remain important open questions regarding the design space of combining machine learning with rule-based programming language processing methods, including hybrid approaches using both CNNs and traditional context-free grammars. Future work should focus on exploring how to balance automation and human control, thus allowing users to have greater influence over their coding experience while reaping the benefits of advanced technologies.",1
"We introduce the dense captioning task, which requires a computer vision system to both localize and describe salient regions in images in natural language. The dense captioning task generalizes object detection when the descriptions consist of a single word, and Image Captioning when one predicted region covers the full image. To address the localization and description task jointly we propose a Fully Convolutional Localization Network (FCLN) architecture that processes an image with a single, efficient forward pass, requires no external regions proposals, and can be trained end-to-end with a single round of optimization. The architecture is composed of a Convolutional Network, a novel dense localization layer, and Recurrent Neural Network language model that generates the label sequences. We evaluate our network on the Visual Genome dataset, which comprises 94,000 images and 4,100,000 region-grounded captions. We observe both speed and accuracy improvements over baselines based on current state of the art approaches in both generation and retrieval settings.",0
"In recent years, computer vision models have made significant strides towards understanding images at a high level, including generating natural language descriptions of visual scenes (image captioning). However, most existing methods still struggle to accurately localize specific objects within those scenes. Here we introduce DenseCap, a new model that addresses both image captioning and dense object localization tasks jointly by utilizing fully convolutional neural networks. Our approach maps each object proposal onto a scaled feature map representing the entire input image, which can then be used as the receptive field for detectors trained to predict bounding box offsets relative to the proposal. We experimentally demonstrate substantial improvements over prior state-of-the-art approaches on benchmark datasets, achieving compelling results for both caption generation and fine-grained spatial location estimation simultaneously. This work represents an important step towards enabling real-world applications such as human-AI collaboration in photo editing or robotic manipulation of specific scene elements.",1
"Integrating higher level visual and linguistic interpretations is at the heart of human intelligence. As automatic visual category recognition in images is approaching human performance, the high level understanding in the dynamic spatiotemporal domain of videos and its translation into natural language is still far from being solved. While most works on vision-to-text translations use pre-learned or pre-established computational linguistic models, in this paper we present an approach that uses vision alone to efficiently learn how to translate into language the video content. We discover, in simple form, the story played by main actors, while using only visual cues for representing objects and their interactions. Our method learns in a hierarchical manner higher level representations for recognizing subjects, actions and objects involved, their relevant contextual background and their interaction to one another over time. We have a three stage approach: first we take in consideration features of the individual entities at the local level of appearance, then we consider the relationship between these objects and actions and their video background, and third, we consider their spatiotemporal relations as inputs to classifiers at the highest level of interpretation. Thus, our approach finds a coherent linguistic description of videos in the form of a subject, verb and object based on their role played in the overall visual story learned directly from training data, without using a known language model. We test the efficiency of our approach on a large scale dataset containing YouTube clips taken in the wild and demonstrate state-of-the-art performance, often superior to current approaches that use more complex, pre-learned linguistic knowledge.",0
"Recent advances in computer vision have enabled the development of systems that can automatically translate videos into textual representations such as subtitles or captions. These systems typically rely on visual features extracted from images or frames within the video, which may not always capture the full context of the scene or convey important nuances in meaning. In this work, we propose a novel approach called ""Stories in the Eye"" that leverages both visual and temporal aspects of video content to enhance translation accuracy. We show how our method outperforms existing techniques by effectively capturing relevant interactions among objects, actions, and events within each frame, while taking into account their relationships across time. Our system also learns from large amounts of data to adapt to different domains and languages, making it more versatile than traditional approaches. Overall, our findings demonstrate the potential of contextually rich translations for improving accessibility and communication through multimedia content.",1
"We describe a method for visual question answering which is capable of reasoning about contents of an image on the basis of information extracted from a large-scale knowledge base. The method not only answers natural language questions using concepts not contained in the image, but can provide an explanation of the reasoning by which it developed its answer. The method is capable of answering far more complex questions than the predominant long short-term memory-based approach, and outperforms it significantly in the testing. We also provide a dataset and a protocol by which to evaluate such methods, thus addressing one of the key issues in general visual ques- tion answering.",0
"Recent advancements in computer vision have enabled the development of visual question answering (VQA) systems that can effectively process natural language queries and generate accurate responses based on image content. However, most existing VQA models rely heavily on implicit knowledge representations such as learned embeddings and convolutional features, which may lead to limited generalization capabilities and poor interpretability. This paper presents an explicit knowledge-based reasoning approach for VQA that utilizes structured external domain knowledge graphs for improved performance and explainability. We propose a novel framework that integrates semantic parsing techniques with graph traversal algorithms to extract relevant facts from knowledge bases and reason over them jointly with visual evidence. Our extensive experimental evaluation demonstrates that our method significantly outperforms state-of-the-art VQA approaches across multiple benchmark datasets while providing more interpretable answers through leveraging external knowledge sources. These findings underscore the importance of incorporating explicit domain knowledge in building effective VQA systems, paving the way for future research in this direction.",1
"In this paper we propose the construction of linguistic descriptions of images. This is achieved through the extraction of scene description graphs (SDGs) from visual scenes using an automatically constructed knowledge base. SDGs are constructed using both vision and reasoning. Specifically, commonsense reasoning is applied on (a) detections obtained from existing perception methods on given images, (b) a ""commonsense"" knowledge base constructed using natural language processing of image annotations and (c) lexical ontological knowledge from resources such as WordNet. Amazon Mechanical Turk(AMT)-based evaluations on Flickr8k, Flickr30k and MS-COCO datasets show that in most cases, sentences auto-constructed from SDGs obtained by our method give a more relevant and thorough description of an image than a recent state-of-the-art image caption based approach. Our Image-Sentence Alignment Evaluation results are also comparable to that of the recent state-of-the art approaches.",0
"Title: ""From Images to Sentences: Leveraging Commonsense Reasoning and Knowledge to Generate Descriptions from Scenes""  This research presents a novel approach for generating natural language descriptions of scenes from images, utilizing commonsense reasoning and knowledge. Our method builds upon recent advances in scene understanding by first constructing a structured representation of the scene known as a Scene Description Graph (SDG). This graph captures relationships among objects, attributes, and interactions within the scene. We then apply commonsense reasoning rules to infer additional details about the scene that are missing from the image itself. Finally, we generate human-like textual descriptions of the scene using a latent variable model, trained on large datasets of paired image and sentence pairs. Experiments demonstrate that our proposed method significantly outperforms strong baselines across multiple metrics, including human evaluation. Additionally, qualitative results showcase the ability of our system to generate detailed and coherent descriptions of complex scenes, exhibiting a high degree of fidelity to the original image content. Overall, this work represents an important step towards achieving automated image description that can effectively capture relevant information and context for a wide range of applications.",1
"We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a ""pretraining"" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.",0
"Machine learning algorithms have been successfully used in many applications such as image recognition, natural language processing, robotics, among others. In most of these cases, large amounts of labeled data are required during training so that the algorithm can learn complex patterns from them. However, obtaining labeled datasets is time consuming, laborious, and expensive which limits their usage in new domains. This paper presents semi-supervised sequence learning (SSL), a novel approach that uses both unlabeled and labeled data during training to improve performance on tasks where only small amounts of labeled data are available. SSL leverages pretext models by using self-supervision via contrastive representation learning which encourages representations learned on one task to generalize well across multiple other related tasks. SSL has been applied in different NLP domains including sentiment analysis and text generation achieving competitive results compared to fully supervised methods.",1
"Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks (RNNs) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a self-contained explication of the state of the art together with a historical perspective and references to primary research.",0
"Deep learning has revolutionized the field of artificial intelligence through the development of complex neural networks that can learn from large amounts of data. One popular type of deep learning architecture is recurrent neural networks (RNN), which have been successfully used for sequence learning tasks such as language modeling and speech recognition. However, despite their widespread use, RNNs have several limitations that hinder their performance on certain tasks. This review examines these limitations and discusses alternative approaches to address them. We first provide an overview of how RNNs work, focusing on vanilla RNNs and gated units such as Long Short-Term Memory (LSTM) cells and Gated Recurrent Units (GRUs). Next, we highlight some common challenges encountered by RNNs, including gradient disappearance and exploding gradients, vanishing and exploding activations, and issues related to parallelization. These issues result in difficulties training RNNs effectively on longer sequences and make optimization more difficult. To overcome these shortcomings, researchers have developed new architectures, including Echo State Networks (ESNs), Reservoir Computing models, and others. While these newer models show promising results in certain cases, they require further investigation to fully evaluate their effectiveness. By understanding both the advantages and limitations of RNNs and their variants, scientists can develop better solutions for sequence learning tasks. Ultimately, this study aims to shed light on important considerations for applying RNNs and identify future directions for improving their performance.",1
"This is a machine learning application paper involving big data. We present high-accuracy prediction methods of rare events in semi-structured machine log files, which are produced at high velocity and high volume by NORC's computer-assisted telephone interviewing (CATI) network for conducting surveys. We judiciously apply natural language processing (NLP) techniques and data-mining strategies to train effective learning and prediction models for classifying uncommon error messages in the log---without access to source code, updated documentation or dictionaries. In particular, our simple but effective approach of features preallocation for learning from imbalanced data coupled with naive Bayes classifiers can be conceivably generalized to supervised or semi-supervised learning and prediction methods for other critical events such as cyberattack detection.",0
"In recent years, machine learning has proven to be an effective tool for extracting valuable insights from complex data sets. This study presents an application of machine learning techniques to Minecraft player movement data collected through Computer Assisted Telephone Interview (CATI) networks. Using raw movement data gathered by interviews conducted over phone calls, we developed models that can predict both successful mining attempts and player behaviors such as the likelihood of standing still versus moving while on a call. We demonstrate how these predictions compare favorably against those made using other methods. Overall, our results suggest that there may be significant potential for applying similar methods to other domains where telephonic interactions provide the data source. While more work remains to fully realize this potential, the current study provides encouragement for further exploration.",1
"Gibbs sampling on factor graphs is a widely used inference technique, which often produces good empirical results. Theoretical guarantees for its performance are weak: even for tree structured graphs, the mixing time of Gibbs may be exponential in the number of variables. To help understand the behavior of Gibbs sampling, we introduce a new (hyper)graph property, called hierarchy width. We show that under suitable conditions on the weights, bounded hierarchy width ensures polynomial mixing time. Our study of hierarchy width is in part motivated by a class of factor graph templates, hierarchical templates, which have bounded hierarchy width---regardless of the data used to instantiate them. We demonstrate a rich application from natural language processing in which Gibbs sampling provably mixes rapidly and achieves accuracy that exceeds human volunteers.",0
"This paper presents a new method for rapidly mixing Gibbs sampling on factor graphs that have hierarchical dependencies among variables. We focus on a class of factor graphs where each variable depends only on a constant number of other variables within a given depth of the hierarchy, measured by the ""hierarchy width"". Our approach improves upon existing methods for such graphs by exploiting these structural properties to design more efficient update rules for Gibbs sampling. We prove bounds on the mixing time of our algorithm under certain conditions and provide experimental results demonstrating the effectiveness of our method. Our work has applications in areas like machine learning and signal processing, which rely heavily on inference on graphical models. By efficiently computing approximate posteriors on large datasets, our method can facilitate further advances in these fields.",1
"We address a question answering task on real-world images that is set up as a Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Neural-Image-QA, an end-to-end formulation to this problem for which all parts are trained jointly. In contrast to previous efforts, we are facing a multi-modal problem where the language output (answer) is conditioned on visual and natural language input (image and question). Our approach Neural-Image-QA doubles the performance of the previous best approach on this problem. We provide additional insights into the problem by analyzing how much information is contained only in the language part for which we provide a new human baseline. To study human consensus, which is related to the ambiguities inherent in this challenging task, we propose two novel metrics and collect additional answers which extends the original DAQUAR dataset to DAQUAR-Consensus.",0
"This paper presents a neural network model that can accurately answer questions related to images. Our approach uses convolutional neural networks (CNNs) to extract relevant features from the images and then passes them through several layers of fully connected neurons before outputting the answers. We evaluate our method on a dataset of image queries and show that our model outperforms traditional computer vision techniques in terms of accuracy and speed. Additionally, we investigate the robustness of our system by testing it under different conditions such as varying levels of noise, distortion and compression. Overall, the results demonstrate the effectiveness of using deep learning methods to solve complex visual tasks and pave the way towards more advanced artificial intelligence systems capable of understanding natural language and processing visual data simultaneously.",1
"Recent progress in using recurrent neural networks (RNNs) for image description has motivated the exploration of their application for video description. However, while images are static, working with videos requires modeling their dynamic temporal structure and then properly integrating that information into a natural language description. In this context, we propose an approach that successfully takes into account both the local and global temporal structure of videos to produce descriptions. First, our approach incorporates a spatial temporal 3-D convolutional neural network (3-D CNN) representation of the short temporal dynamics. The 3-D CNN representation is trained on video action recognition tasks, so as to produce a representation that is tuned to human motion and behavior. Second we propose a temporal attention mechanism that allows to go beyond local temporal modeling and learns to automatically select the most relevant temporal segments given the text-generating RNN. Our approach exceeds the current state-of-art for both BLEU and METEOR metrics on the Youtube2Text dataset. We also present results on a new, larger and more challenging dataset of paired video and natural language descriptions.",0
"This study presents a novel approach for describing videos using temporal structure analysis. We propose a method that leverages hierarchical recurrent neural networks (HRNNs) to model the relationship between video frames and their corresponding audio descriptions. Our system exploits spatiotemporal features extracted from both modalities to capture the dynamics underlying video content. By employing a two-stage framework consisting of frame selection and description generation, we effectively utilize sequential information while reducing computational complexity. Extensive experiments on multiple benchmark datasets demonstrate significant improvements over state-of-the-art methods in terms of quantitative metrics and subjective assessments. This work shows promising results towards automatic video description tasks for visually impaired users.",1
"We introduce Segment-Phrase Table (SPT), a large collection of bijective associations between textual phrases and their corresponding segmentations. Leveraging recent progress in object recognition and natural language semantics, we show how we can successfully build a high-quality segment-phrase table using minimal human supervision. More importantly, we demonstrate the unique value unleashed by this rich bimodal resource, for both vision as well as natural language understanding. First, we show that fine-grained textual labels facilitate contextual reasoning that helps in satisfying semantic constraints across image segments. This feature enables us to achieve state-of-the-art segmentation results on benchmark datasets. Next, we show that the association of high-quality segmentations to textual phrases aids in richer semantic understanding and reasoning of these textual phrases. Leveraging this feature, we motivate the problem of visual entailment and visual paraphrasing, and demonstrate its utility on a large dataset.",0
"This is an interesting paper that proposes a new approach to semantic segmentation, visual entailment, and paraphrasing called the Segment-Phrase Table (S-PT). The S-PT is a novel method for representing relationships between image regions and text phrases, which allows for efficient inference of these relationships across tasks. The authors demonstrate how the S-PT can effectively capture complex relationships between images and text and enable efficient computation of entailment probabilities between pairs of region descriptions and phrase meanings. They evaluate the effectiveness of their method on several benchmark datasets and show significant improvements over baseline models, particularly on more challenging tasks such as fine-grained visual entailment and zero-shot paraphrasing. Overall, this work presents a promising new direction for research in computer vision and natural language processing, and offers valuable insights into the design and evaluation of multimodal systems.",1
"The ability to describe images with natural language sentences is the hallmark for image and language understanding. Such a system has wide ranging applications such as annotating images and using natural sentences to search for images.In this project we focus on the task of bidirectional image retrieval: such asystem is capable of retrieving an image based on a sentence (image search) andretrieve sentence based on an image query (image annotation). We present asystem based on a global ranking objective function which uses a combinationof convolutional neural networks (CNN) and multi layer perceptrons (MLP).It takes a pair of image and sentence and processes them in different channels,finally embedding it into a common multimodal vector space. These embeddingsencode abstract semantic information about the two inputs and can be comparedusing traditional information retrieval approaches. For each such pair, the modelreturns a score which is interpretted as a similarity metric. If this score is high,the image and sentence are likely to convey similar meaning, and if the score is low then they are likely not to.   The visual input is modeled via deep convolutional neural network. On theother hand we explore three models for the textual module. The first one isbag of words with an MLP. The second one uses n-grams (bigram, trigrams,and a combination of trigram & skip-grams) with an MLP. The third is morespecialized deep network specific for modeling variable length sequences (SSE).We report comparable performance to recent work in the field, even though ouroverall model is simpler. We also show that the training time choice of how wecan generate our negative samples has a significant impact on performance, and can be used to specialize the bi-directional system in one particular task.",0
"In recent years, there has been significant progress in the field of deep learning applied to computer vision tasks such as object detection and image classification. However, few studies have focused on applying these techniques specifically to the task of matching images and text descriptions. This study proposes a novel approach that combines Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) to simultaneously match images and text descriptions of objects. To evaluate the effectiveness of our method, we conduct experiments using two large datasets containing both images and text descriptions of objects: COCO and MSCOCO. Our results demonstrate that our proposed model outperforms traditional approaches by achieving higher accuracy and robustness in image and text matching. Additionally, through ablation studies, we show that each component in our method contributes significantly to improving overall performance. Overall, this work demonstrates the potential applications of deep learning models in solving complex multimedia data fusion problems.",1
"This paper introduces a new method to solve the cross-domain recognition problem. Different from the traditional domain adaption methods which rely on a global domain shift for all classes between source and target domain, the proposed method is more flexible to capture individual class variations across domains. By adopting a natural and widely used assumption -- ""the data samples from the same class should lay on a low-dimensional subspace, even if they come from different domains"", the proposed method circumvents the limitation of the global domain shift, and solves the cross-domain recognition by finding the compact joint subspaces of source and target domain. Specifically, given labeled samples in source domain, we construct subspaces for each of the classes. Then we construct subspaces in the target domain, called anchor subspaces, by collecting unlabeled samples that are close to each other and highly likely all fall into the same class. The corresponding class label is then assigned by minimizing a cost function which reflects the overlap and topological structure consistency between subspaces across source and target domains, and within anchor subspaces, respectively.We further combine the anchor subspaces to corresponding source subspaces to construct the compact joint subspaces. Subsequently, one-vs-rest SVM classifiers are trained in the compact joint subspaces and applied to unlabeled data in the target domain. We evaluate the proposed method on two widely used datasets: object recognition dataset for computer vision tasks, and sentiment classification dataset for natural language processing tasks. Comparison results demonstrate that the proposed method outperforms the comparison methods on both datasets.",0
In recent years there has been growing interest in developing methods that can unlock human ability to perform recognition tasks across domains without requiring labeled data from any of these new domains. This problem of cross domain recognition has proven challenging due to differences in appearance and texture of objects between different datasets as well as within them which hinders performance on related task like few shot learning. One method to mitigate such problems involves identifying a joint subspace where all the data lie close together regardless of their source domain. We present a novel approach Unsupervised Cross-domain Recognition via Disentangling Shared Representations (CDR)which identifies compact joint spaces using only unlabelled samples. We apply this model to several benchmarks demonstrating high accuracy even compared to supervised competitors. Our results indicate CDR can successfully generalize across many different domains and provides state of the art performance.,1
"Automatically describing video content with natural language is a fundamental challenge of multimedia. Recurrent Neural Networks (RNN), which models sequence dynamics, has attracted increasing attention on visual interpretation. However, most existing approaches generate a word locally with given previous words and the visual content, while the relationship between sentence semantics and visual content is not holistically exploited. As a result, the generated sentences may be contextually correct but the semantics (e.g., subjects, verbs or objects) are not true.   This paper presents a novel unified framework, named Long Short-Term Memory with visual-semantic Embedding (LSTM-E), which can simultaneously explore the learning of LSTM and visual-semantic embedding. The former aims to locally maximize the probability of generating the next word given previous words and visual content, while the latter is to create a visual-semantic embedding space for enforcing the relationship between the semantics of the entire sentence and visual content. Our proposed LSTM-E consists of three components: a 2-D and/or 3-D deep convolutional neural networks for learning powerful video representation, a deep RNN for generating sentences, and a joint embedding model for exploring the relationships between visual content and sentence semantics. The experiments on YouTube2Text dataset show that our proposed LSTM-E achieves to-date the best reported performance in generating natural sentences: 45.3% and 31.0% in terms of BLEU@4 and METEOR, respectively. We also demonstrate that LSTM-E is superior in predicting Subject-Verb-Object (SVO) triplets to several state-of-the-art techniques.",0
"This work proposes a novel method for bridging video and language by jointly modeling embedding and translation. Our approach uses a shared multimodal representation, allowing the model to learn alignments across both modalities. We leverage recent advances in unsupervised machine learning techniques such as contrastive pretraining and self-supervised learning. Extensive experiments on three challenging benchmarks demonstrate that our proposed method significantly outperforms strong baseline models on tasks ranging from activity recognition to crossmodal retrieval. In addition, we provide qualitative analysis showing improved alignment between visual features and linguistic representations. Overall, our results highlight the effectiveness of using multi-modality in understanding complex relationships between video and text data. Future directions may focus on refining the model architecture or exploring new applications of these methods.",1
"Automatically describing an image with a sentence is a long-standing challenge in computer vision and natural language processing. Due to recent progress in object detection, attribute classification, action recognition, etc., there is renewed interest in this area. However, evaluating the quality of descriptions has proven to be challenging. We propose a novel paradigm for evaluating image descriptions that uses human consensus. This paradigm consists of three main parts: a new triplet-based method of collecting human annotations to measure consensus, a new automated metric (CIDEr) that captures consensus, and two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences describing each image. Our simple metric captures human judgment of consensus better than existing metrics across sentences generated by various sources. We also evaluate five state-of-the-art image description approaches using this new protocol and provide a benchmark for future comparisons. A version of CIDEr named CIDEr-D is available as a part of MS COCO evaluation server to enable systematic evaluation and benchmarking.",0
"""Considering images as one of our most natural ways to communicate in our daily lives, generating descriptions that convey meaningful information plays a crucial role in modern image understanding systems. One important challenge is evaluating how well these descriptors capture important aspects of real world objects.""",1
"In this paper, we introduce a new dataset consisting of 360,001 focused natural language descriptions for 10,738 images. This dataset, the Visual Madlibs dataset, is collected using automatically produced fill-in-the-blank templates designed to gather targeted descriptions about: people and objects, their appearances, activities, and interactions, as well as inferences about the general scene or its broader context. We provide several analyses of the Visual Madlibs dataset and demonstrate its applicability to two new description generation tasks: focused description generation, and multiple-choice question-answering for images. Experiments using joint-embedding and deep learning methods show promising results on these tasks.",0
"In recent years, generative models have made significant progress towards producing realistic images from text descriptions. However, current methods still rely heavily on human annotators and expert engineers to design these systems. As a result, generating images that match user intent remains challenging. In this work, we propose Visual MadLibs - a method capable of filling in blanks in natural language queries to generate accurate answers without requiring any external annotations or explicit engineering beyond setting up the model. We validate our approach through diverse experiments which demonstrate improved question answering accuracy compared to previous state-of-the art techniques while drastically reducing the need for expensive data collection and annotation efforts. Our source code has been released publicly to promote further research in this exciting direction.",1
"Despite tremendous progress in computer vision, there has not been an attempt for machine learning on very large-scale medical image databases. We present an interleaved text/image deep learning system to extract and mine the semantic interactions of radiology images and reports from a national research hospital's Picture Archiving and Communication System. With natural language processing, we mine a collection of representative ~216K two-dimensional key images selected by clinicians for diagnostic reference, and match the images with their descriptions in an automated manner. Our system interleaves between unsupervised learning and supervised learning on document- and sentence-level text collections, to generate semantic labels and to predict them given an image. Given an image of a patient scan, semantic topics in radiology levels are predicted, and associated key-words are generated. Also, a number of frequent disease types are detected as present or absent, to provide more specific interpretation of a patient scan. This shows the potential of large-scale learning and prediction in electronic patient records available in most modern clinical institutions.",0
"In order to develop automated methods that can accurately interpret radiological images without human input, there has been growing interest in using large-scale datasets consisting of both textual descriptions and image data. This approach involves ""mining"" through these datasets for patterns and correlations that can aid in developing machine learning models capable of performing accurate image interpretation tasks. However, the application of deep mining techniques to large scale radiology databases containing both text and images remains under explored, particularly in regards to interleaving data from multiple sources in a way that effectively captures contextual relationships between image features and descriptive text.  The authors of this paper propose an innovative methodology for conducting interleaved text/image deep mining on large-scale radiology databases. The proposed method consists of three main stages: preprocessing, mining and post processing. During preprocessing stage, the raw dataset was filtered, normalized, transformed into appropriate formats for feature extraction and fed into deep generative latent variable model (DGLVM). At next step, mining phase DGLVMs are fine tuned and used as generators in generative adversarial networks (GANs) for generating synthetic cases which were added back into original database which creates expanded dataset consisted of real and generated data points. Lastly, during postprocessing stage, we apply GAN trained model as classifier and evaluate effectiveness of our method by showing state-of-the-art results against non deep mined baseline models on two standard benchmark radiology datasets, Kaggle's Pneumonia Detection Challenge and Digital Imaging",1
"Solving the visual symbol grounding problem has long been a goal of artificial intelligence. The field appears to be advancing closer to this goal with recent breakthroughs in deep learning for natural language grounding in static images. In this paper, we propose to translate videos directly to sentences using a unified deep neural network with both convolutional and recurrent structure. Described video datasets are scarce, and most existing methods have been applied to toy domains with a small vocabulary of possible words. By transferring knowledge from 1.2M+ images with category labels and 100,000+ images with captions, our method is able to create sentence descriptions of open-domain videos with large vocabularies. We compare our approach with recent work using language generation metrics, subject, verb, and object prediction accuracy, and a human evaluation.",0
"A novel approach to translating videos into natural language is introduced using deep recurrent neural networks (RNN). This method allows for more accurate and efficient video understanding by incorporating sequential data processing techniques that learn from the input sequence. In addition, the model uses multi-modal representations to better capture temporal relationships among different frames. Experiments show improved accuracy compared to existing methods on benchmark datasets. Overall, the proposed system provides a promising solution for automated video description generation.",1
"This technical report provides extra details of the deep multimodal similarity model (DMSM) which was proposed in (Fang et al. 2015, arXiv:1411.4952). The model is trained via maximizing global semantic similarity between images and their captions in natural language using the public Microsoft COCO database, which consists of a large set of images and their corresponding captions. The learned representations attempt to capture the combination of various visual concepts and cues.",0
"In today’s world where technology has taken over, artificial intelligence (AI) has become one of the most sought after technologies across industries. One of the key components that drive modern-day AI systems is deep learning, which involves training large neural networks on massive amounts of data. In particular, jointly learning distributed representations of images and texts can greatly improve performance in areas like image classification, question answering, and cross modal retrieval tasks. This paper proposes a novel model architecture for joint representation learning using multi task convolutional recurrent units. By combining convolutions with gating mechanisms from RNNs, our model allows each layer to learn a unique combination of selective and global processing capabilities. We show through experiments that such a model significantly outperforms prior state-of-the-art methods in both image classification as well as cross modal retrieval scenarios. With these promising results, we hope to further advance research in multi modality applications that rely heavily on combined visual and textual understanding.",1
"Within the natural language processing (NLP) community, active learning has been widely investigated and applied in order to alleviate the annotation bottleneck faced by developers of new NLP systems and technologies. This paper presents the first theoretical analysis of stopping active learning based on stabilizing predictions (SP). The analysis has revealed three elements that are central to the success of the SP method: (1) bounds on Cohen's Kappa agreement between successively trained models impose bounds on differences in F-measure performance of the models; (2) since the stop set does not have to be labeled, it can be made large in practice, helping to guarantee that the results transfer to previously unseen streams of examples at test/application time; and (3) good (low variance) sample estimates of Kappa between successive models can be obtained. Proofs of relationships between the level of Kappa agreement and the difference in performance between consecutive models are presented. Specifically, if the Kappa agreement between two models exceeds a threshold T (where $T0$), then the difference in F-measure performance between those models is bounded above by $\frac{4(1-T)}{T}$ in all cases. If precision of the positive conjunction of the models is assumed to be $p$, then the bound can be tightened to $\frac{4(1-T)}{(p+1)T}$.",0
"This study evaluates the effectiveness of stopping active learning (SAL) by assessing how well stabilized predictions can determine when to stop collecting data points. SAL is a method used in machine learning that iteratively selects unlabelled data points and queries them until a certain criterion is met. However, determining when to stop actively querying additional data points remains challenging. Stabilized predictions use a measure of model confidence to estimate how likely a prediction will change over time as more training data becomes available. By utilizing these stabilized predictions as a criteria for stopping, we aimed to improve upon existing methods for selecting when to halt active learning processes. Our experimental results showed that using stabilized predictions led to better performance than relying solely on the uncertainty level of the model. These findings suggest that incorporating measures of stability into active learning strategies may significantly enhance their overall effectiveness.",1
"Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.",0
"Our proposed image caption generator network uses novel multi-modal fusion techniques which lead to state-of-the-art results on popular benchmark datasets. We introduce two new variants of our architecture that we show are important components towards achieving these improvements. The first variant fuses different attention mechanisms at multiple levels, increasing accuracy by recognizing global dependencies as well as local features. The second variant utilizes high level feature maps instead of low level ones, demonstrating that object detection is possible without explicit spatial awareness. Finally, we present extensive evaluations of each component which show their positive impact on generating descriptive natural language captions.",1
"Feature learning forms the cornerstone for tackling challenging learning problems in domains such as speech, computer vision and natural language processing. In this paper, we consider a novel class of matrix and tensor-valued features, which can be pre-trained using unlabeled samples. We present efficient algorithms for extracting discriminative information, given these pre-trained features and labeled samples for any related task. Our class of features are based on higher-order score functions, which capture local variations in the probability density function of the input. We establish a theoretical framework to characterize the nature of discriminative information that can be extracted from score-function features, when used in conjunction with labeled samples. We employ efficient spectral decomposition algorithms (on matrices and tensors) for extracting discriminative components. The advantage of employing tensor-valued features is that we can extract richer discriminative information in the form of an overcomplete representations. Thus, we present a novel framework for employing generative models of the input for discriminative learning.",0
"Recent years have seen rapid advances in discriminative learning methods, thanks to their ability to learn from complex data distributions and handle highly nonlinear patterns. However, designing effective scoring functions has become increasingly challenging due to the intricate nature of modern datasets. This paper addresses these difficulties by introducing novel score function features that enable efficient model training while significantly improving prediction accuracy across several benchmark tasks. We first analyze existing scoring strategies to identify limitations and present a unified framework based on Taylor expansion techniques. Our method allows fine-grained control over approximations, enables flexible incorporation of problem knowledge, and ensures computational efficiency through smart approximations. Experimental results show consistent improvements over state-of-the-art models on diverse problems ranging from binary classification to regression and structured output predictions. Our contributions provide new insights into the design of efficient score functions, facilitating further progress in discriminative machine learning research.",1
"We consider the problem of closeness testing for two discrete distributions in the practically relevant setting of \emph{unequal} sized samples drawn from each of them. Specifically, given a target error parameter $\varepsilon  0$, $m_1$ independent draws from an unknown distribution $p,$ and $m_2$ draws from an unknown distribution $q$, we describe a test for distinguishing the case that $p=q$ from the case that $||p-q||_1 \geq \varepsilon$. If $p$ and $q$ are supported on at most $n$ elements, then our test is successful with high probability provided $m_1\geq n^{2/3}/\varepsilon^{4/3}$ and $m_2 = \Omega(\max\{\frac{n}{\sqrt m_1\varepsilon^2}, \frac{\sqrt n}{\varepsilon^2}\});$ we show that this tradeoff is optimal throughout this range, to constant factors. These results extend the recent work of Chan et al. who established the sample complexity when the two samples have equal sizes, and tightens the results of Acharya et al. by polynomials factors in both $n$ and $\varepsilon$. As a consequence, we obtain an algorithm for estimating the mixing time of a Markov chain on $n$ states up to a $\log n$ factor that uses $\tilde{O}(n^{3/2} \tau_{mix})$ queries to a ""next node"" oracle, improving upon the $\tilde{O}(n^{5/3}\tau_{mix})$ query algorithm of Batu et al. Finally, we note that the core of our testing algorithm is a relatively simple statistic that seems to perform well in practice, both on synthetic data and on natural language data.",0
"In many real-world applications, data often comes in different sizes for each group or category being compared. Standard testing procedures assume equal sample sizes and may lead to biased results if applied without modification. This paper proposes a novel methodology to test statistical significance when dealing with unequal sample size scenarios. We compare our approach against existing methods using simulated datasets and demonstrate that our proposed method leads to more accurate inferences by reducing Type I error rates while maintaining acceptable power levels. The practical implications are wide ranging across fields as varied as biological sciences, social sciences, economics and engineering where researchers often face challenges arising from non-equal sample sizes.",1
"We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.",0
"Title: ""Deep Visual Semantic Alignment Methods for Automatically Describing Images"" Abstract Human vision enables us to effortlessly perceive and interpret visual scenes. However, replicating this ability in machines remains a challenging task. Existing methods have primarily focused on either using highlevel semantic representations (such as class labels), which may not capture relevant details at the pixel level; or employing lowlevel feature extraction techniques that can become computationally expensive. To address these limitations, we propose novel deep neural network architectures that learn to align image features from different layers of a convolutional network with their corresponding natural language descriptions. Our models rely solely on large amounts of weak supervision provided by human image and text pairs available online, without any explicit bounding box annotations or other forms of guidance. We evaluate our method extensively against alternative stateoftheart approaches on four benchmark datasets comprising diverse object categories and complex scene understanding tasks. The experimental results demonstrate that our model produces descriptions that are significantly more accurate, informative, and interpretable than those generated by existing competitors. Furthermore, we present detailed analyses of our method's behavior in terms of robustness to noisy training data, scalability to more complex settings, and sensitivity to architectural choices within the alignment framework. The developed techniques constitute key components towards enabling intelligent systems to comprehend vast repositories of visual content based purely on implicit feedback obtained through written narratives. Keywords: Convolutional networks; Deep learning; Multi modal representation; Natural language description; Supervised learning; Weak supervi",1
"Incorporating the side information of text corpus, i.e., authors, time stamps, and emotional tags, into the traditional text mining models has gained significant interests in the area of information retrieval, statistical natural language processing, and machine learning. One branch of these works is the so-called Author Topic Model (ATM), which incorporates the authors's interests as side information into the classical topic model. However, the existing ATM needs to predefine the number of topics, which is difficult and inappropriate in many real-world settings. In this paper, we propose an Infinite Author Topic (IAT) model to resolve this issue. Instead of assigning a discrete probability on fixed number of topics, we use a stochastic process to determine the number of topics from the data itself. To be specific, we extend a gamma-negative binomial process to three levels in order to capture the author-document-keyword hierarchical structure. Furthermore, each document is assigned a mixed gamma process that accounts for the multi-author's contribution towards this document. An efficient Gibbs sampling inference algorithm with each conditional distribution being closed-form is developed for the IAT model. Experiments on several real-world datasets show the capabilities of our IAT model to learn the hidden topics, authors' interests on these topics and the number of topics simultaneously.",0
"This paper presents a new topic model called the Infinite Author Topic Model (IATM) that uses a mixed Gamma-negative binomial process (GNBP). We use a hierarchical Bayesian framework to estimate the number of topics in the data as well as their mixing proportions across documents. Our approach extends traditional Latent Dirichlet Allocation (LDA) models by allowing for infinite mixtures of distributions over topics. With IATM we aim to capture more complex underlying patterns in the data without sacrificing interpretability. Experiments using real datasets show that our method outperforms LDA in terms of perplexity metrics and provides better qualitative results due to improved capturing of subtle distinctions within topics. Furthermore, experiments on artificially generated datasets demonstrate robustness of inference under varying data generation conditions. Overall, our work represents an advance in nonparametric Bayesian methods for text analysis.",1
"In this work, we introduce a dataset of video annotated with high quality natural language phrases describing the visual content in a given segment of time. Our dataset is based on the Descriptive Video Service (DVS) that is now encoded on many digital media products such as DVDs. DVS is an audio narration describing the visual elements and actions in a movie for the visually impaired. It is temporally aligned with the movie and mixed with the original movie soundtrack. We describe an automatic DVS segmentation and alignment method for movies, that enables us to scale up the collection of a DVS-derived dataset with minimal human intervention. Using this method, we have collected the largest DVS-derived dataset for video description of which we are aware. Our dataset currently includes over 84.6 hours of paired video/sentences from 92 DVDs and is growing.",0
"Title: Creating a Large Data Source for Video Annotation Research through Descriptive Video Services  This study presents a novel approach to creating a large data set for video annotation research by utilizing descriptive video services (DVS). DVS provides audio descriptions of visual content for individuals who are blind or have low vision, allowing them to better understand media without relying solely on their sense of hearing. By leveraging these existing descriptions, we can generate a comprehensive dataset that can be used to train machine learning algorithms for improved automation of video description generation. Our methodology involves extracting relevant metadata from online sources such as YouTube videos and using natural language processing techniques to identify key features such as objects, actions, and scene changes within each video. This extracted data is then combined with corresponding DVS transcriptions to create a dataset suitable for training and testing automated systems. In addition to providing a valuable resource for researchers working in computer vision and accessibility technologies, our work has significant potential applications outside academia including improving search functionality for visually impaired users, enhancing accessibility for streaming platforms, and facilitating the development of related assistive technologies. Overall, this study demonstrates how combining innovative methods and existing resources can lead to meaningful advancements in digital media research.",1
"A classic tension exists between exact inference in a simple model and approximate inference in a complex model. The latter offers expressivity and thus accuracy, but the former provides coverage of the space, an important property for confidence estimation and learning with indirect supervision. In this work, we introduce a new approach, reified context models, to reconcile this tension. Specifically, we let the amount of context (the arity of the factors in a graphical model) be chosen ""at run-time"" by reifying it---that is, letting this choice itself be a random variable inside the model. Empirically, we show that our approach obtains expressivity and coverage on three natural language tasks.",0
"Artificial intelligence (AI) has made significant progress over recent years, especially in fields such as natural language processing and computer vision. However, one key challenge that remains is building models that can effectively reason about contextual information and make accurate predictions in real-world scenarios. In order to address this issue, we propose the use of reified context models, which allow us to represent and manipulate contextual knowledge explicitly within our machine learning algorithms. These models have been shown to significantly improve performance on tasks that rely heavily on understanding contextual relationships between entities and events. By incorporating these techniques into modern AI systems, we believe that we can take another step towards creating intelligent machines that truly understand their environments and can interact with them seamlessly.",1
"Recently, fully-connected and convolutional neural networks have been trained to achieve state-of-the-art performance on a wide variety of tasks such as speech recognition, image classification, natural language processing, and bioinformatics. For classification tasks, most of these ""deep learning"" models employ the softmax activation function for prediction and minimize cross-entropy loss. In this paper, we demonstrate a small but consistent advantage of replacing the softmax layer with a linear support vector machine. Learning minimizes a margin-based loss instead of the cross-entropy loss. While there have been various combinations of neural nets and SVMs in prior art, our results using L2-SVMs show that by simply replacing softmax with linear SVMs gives significant gains on popular deep learning datasets MNIST, CIFAR-10, and the ICML 2013 Representation Learning Workshop's face expression recognition challenge.",0
"Title: ""Linear Support Vector Machine Based Approaches for Solving Complex Tasks""  The field of deep learning has revolutionized numerous domains including computer vision, natural language processing, robotics, amongst many others. In recent years, linear support vector machines (SVM) have been utilized as key components within several state-of-the-art deep learning models. These models have achieved remarkable results on challenging tasks such as image classification, speech recognition, and text understanding, while leveraging SVMs' unique properties like margin maximization and nonlinear embeddings of high dimensional data. This work presents various applications of linear SVM based approaches across diverse problems ranging from image classification to reinforcement learning and provides insights into how these techniques can lead to substantial improvements over traditional methods. Additionally, we analyze different formulations of linear programming used to train SVM models and their impact on performance. Our contributions emphasize the importance of SVM in modern deep learning research and its potential to drive future advancements in artificial intelligence.",1
"While computer and communication technologies have provided effective means to scale up many aspects of education, the submission and grading of assessments such as homework assignments and tests remains a weak link. In this paper, we study the problem of automatically grading the kinds of open response mathematical questions that figure prominently in STEM (science, technology, engineering, and mathematics) courses. Our data-driven framework for mathematical language processing (MLP) leverages solution data from a large number of learners to evaluate the correctness of their solutions, assign partial-credit scores, and provide feedback to each learner on the likely locations of any errors. MLP takes inspiration from the success of natural language processing for text data and comprises three main steps. First, we convert each solution to an open response mathematical question into a series of numerical features. Second, we cluster the features from several solutions to uncover the structures of correct, partially correct, and incorrect solutions. We develop two different clustering approaches, one that leverages generic clustering algorithms and one based on Bayesian nonparametrics. Third, we automatically grade the remaining (potentially large number of) solutions based on their assigned cluster and one instructor-provided grade per cluster. As a bonus, we can track the cluster assignment of each step of a multistep solution and determine when it departs from a cluster of correct solutions, which enables us to indicate the likely locations of errors to learners. We test and validate MLP on real-world MOOC data to demonstrate how it can substantially reduce the human effort required in large-scale educational platforms.",0
"This paper presents methods for automatic grading and feedback generation for open response mathematical questions. These methods use natural language processing techniques to analyze student responses and determine their correctness, as well as provide targeted feedback to improve student understanding. We evaluate these methods on a dataset of over 2,000 student responses from a large introductory calculus course at a major university. Our results show that our method is able to accurately grade and generate feedback for a wide range of mathematical concepts. Additionally, we compare our system against human graders and find that our method performs comparably while offering advantages such as speed and consistency. Overall, this work shows promise in improving the accuracy and scalability of automated assessment systems for mathematics education.",1
"Word2vec, as an efficient tool for learning vector representation of words has shown its effectiveness in many natural language processing tasks. Mikolov et al. issued Skip-Gram and Negative Sampling model for developing this toolbox. Perozzi et al. introduced the Skip-Gram model into the study of social network for the first time, and designed an algorithm named DeepWalk for learning node embedding on a graph. We prove that the DeepWalk algorithm is actually factoring a matrix M where each entry M_{ij} is logarithm of the average probability that node i randomly walks to node j in fix steps.",0
"Abstract: In recent years, deep learning has emerged as one of the most powerful tools for data analysis and pattern recognition tasks. Amongst these methods, matrix factorization techniques have gained considerable attention due to their simplicity and effectiveness in dimensionality reduction and feature extraction problems. One such technique, known as Deep Walk, is particularly popular due to its ability to learn latent representations that can capture complex relationships within large datasets.  In this work, we propose a new framework for understanding Deep Walk as a form of matrix factorization. By viewing Deep Walk through this lens, we demonstrate how traditional linear algebraic techniques from signal processing theory can provide insights into the behavior and performance characteristics of the algorithm. Our approach leverages the power of linear algebra to simplify the original model and derive closed-form solutions for key quantities, providing valuable theoretical insights and intuition into the inner workings of Deep Walk.  We validate our proposed framework by comparing numerical simulations of the original Deep Walk model with those obtained using our derived expressions. Results show excellent agreement between both approaches, supporting the validity of our theoretical development. Moreover, by highlighting connections between Deep Walk and other well-known matrix factorization algorithms like PCA and NMF, we offer a fresh perspective on this intriguing method, shedding light onto its potential applications beyond merely generating synthetic data samples as surrogates for missing observations.  Our work serves as a stepping stone towards further investigation of matrix factorization based deep generative models and offers exciting opportunities for researchers interested in bridging seemingly disparate fields. Ultimately, our hope is that this effort helps pave the way for better integrating ideas from machine learning, mathematics, and statistics to address challenges posed by big data analytics and artificial intelligence.",1
"Feature learning forms the cornerstone for tackling challenging learning problems in domains such as speech, computer vision and natural language processing. In this paper, we consider a novel class of matrix and tensor-valued features, which can be pre-trained using unlabeled samples. We present efficient algorithms for extracting discriminative information, given these pre-trained features and labeled samples for any related task. Our class of features are based on higher-order score functions, which capture local variations in the probability density function of the input. We establish a theoretical framework to characterize the nature of discriminative information that can be extracted from score-function features, when used in conjunction with labeled samples. We employ efficient spectral decomposition algorithms (on matrices and tensors) for extracting discriminative components. The advantage of employing tensor-valued features is that we can extract richer discriminative information in the form of an overcomplete representations. Thus, we present a novel framework for employing generative models of the input for discriminative learning.",0
"Score functions have been foundational tools for supervised learning tasks like classification and regression. When combined with discriminative models such as decision trees, support vector machines (SVMs), boosting algorithms, etc., score function features often lead to significant improvements on accuracy metrics. This paper presents novel tensor methods that exploit higher order information from score functions. In particular, we consider how tensors can represent high-dimensional feature vectors in Euclidean space or Grassmannian manifolds using data drawn from complex domains such as graphs or point clouds. Our framework leverages the concept of matrix product states which represent highly entangled quantum many body systems under certain circumstances. We demonstrate via empirical studies on several challenging datasets from different application areas that these tensor representations improve discriminability over standard scalar field based representations. Finally, we discuss potential future research directions involving nonlinearity constraints on the tensor components as well as hybrid architectures combining our approach with other popular machine learning techniques.",1
"To cope with the high level of ambiguity faced in domains such as Computer Vision or Natural Language processing, robust prediction methods often search for a diverse set of high-quality candidate solutions or proposals. In structured prediction problems, this becomes a daunting task, as the solution space (image labelings, sentence parses, etc.) is exponentially large. We study greedy algorithms for finding a diverse subset of solutions in structured-output spaces by drawing new connections between submodular functions over combinatorial item sets and High-Order Potentials (HOPs) studied for graphical models. Specifically, we show via examples that when marginal gains of submodular diversity functions allow structured representations, this enables efficient (sub-linear time) approximate maximization by reducing the greedy augmentation step to inference in a factor graph with appropriately constructed HOPs. We discuss benefits, tradeoffs, and show that our constructions lead to significantly better proposals.",0
"In today's world, data sets have become increasingly complex and large. As a result, finding relevant subsets from these huge data sets has become a major challenge. One approach to tackle this problem is through submodular optimization, which seeks to maximize a set function subject to certain constraints. However, traditional methods based on unstructured data are limited in their ability to effectively handle structured item sets that contain valuable relationships between items. To address this issue, we present a novel framework called ""Submodular Meets Structured"" (SMS) that combines submodularity with structural information to efficiently find diverse subsets in exponentially-large structured item sets. Our method leverages graph theory to capture important relationships between items while maintaining computational efficiency. Through extensive experimental evaluations, we demonstrate the effectiveness of our proposed framework across a range of domains including social network analysis, image processing, natural language processing, and bioinformatics. Overall, SMS offers a powerful tool for identifying high quality subsets in large, structured item sets, improving upon existing approaches.",1
"Dropout training, originally designed for deep neural networks, has been successful on high-dimensional single-layer natural language tasks. This paper proposes a theoretical explanation for this phenomenon: we show that, under a generative Poisson topic model with long documents, dropout training improves the exponent in the generalization bound for empirical risk minimization. Dropout achieves this gain much like a marathon runner who practices at altitude: once a classifier learns to perform reasonably well on training examples that have been artificially corrupted by dropout, it will do very well on the uncorrupted test set. We also show that, under similar conditions, dropout preserves the Bayes decision boundary and should therefore induce minimal bias in high dimensions.",0
"In this work we analyze deep neural networks that use dropout during training, but whose weights are constrained to lie within narrow bounds so as to achieve better robustness guarantees on their output uncertainty. We show strong theoretical results establishing tight VC dimension bounds based on these constraints and prove learning and generalization performance matching these bounds across several benchmark datasets. These findings significantly improve upon previous work bounding single-layer ReLU nets through either sparsity or marginalization regularizers, offering new perspectives into model construction for practitioners seeking provably stable and accurate deep models under adverse environmental conditions (e.g., hardware failures or adversarial attacks). Our codebase for these experiments is publicly available at https://github.com/facebookresearch/AltitudeTraining. Supplementary materials can be found on arXiv: https://arxiv.org/abs/2109.08607. Code and supplementary material will be updated regularly; research questions may be directed towards @altitudetuning.",1
"Deep learning is a broad set of techniques that uses multiple layers of representation to automatically learn relevant features directly from structured data. Recently, such techniques have yielded record-breaking results on a diverse set of difficult machine learning tasks in computer vision, speech recognition, and natural language processing. Despite the enormous success of deep learning, relatively little is understood theoretically about why these techniques are so successful at feature learning and compression. Here, we show that deep learning is intimately related to one of the most important and successful techniques in theoretical physics, the renormalization group (RG). RG is an iterative coarse-graining scheme that allows for the extraction of relevant features (i.e. operators) as a physical system is examined at different length scales. We construct an exact mapping from the variational renormalization group, first introduced by Kadanoff, and deep learning architectures based on Restricted Boltzmann Machines (RBMs). We illustrate these ideas using the nearest-neighbor Ising Model in one and two-dimensions. Our results suggests that deep learning algorithms may be employing a generalized RG-like scheme to learn relevant features from data.",0
"Title: Mapping Variational Renormalization Group to Deep Learning Model  Abstract: Deep learning has gained significant attention from researchers as well as practitioners due to their successes in numerous applications, including computer vision, natural language processing, speech recognition, and many others. At the core of deep learning algorithms lie neural networks that can learn complex patterns in data by adjusting their parameters during training. In recent years, the variational renormalization group (VRG) methodology has emerged as a promising technique for solving complex problems related to statistical physics, signal processing, and machine learning. This work presents a precise correspondence between VRG and the popular deep learning architectures such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Generative Adversarial Networks (GANs).  In essence, we show how the message passing mechanism used in VRG closely resembles the backpropagation algorithm employed in deep learning models. By leveraging these similarities, our study demonstrates that one can map any CNN, RNN, or GAN architecture onto a VRG framework. Furthermore, since VRG admits efficient numerical implementations compared to Monte Carlo methods, our findings open up new opportunities for designing scalable deep learning models using VRG schemes. Our results have far-reaching implications on several fronts; they bridge two disparate fields of study, provide insight into their structural parallels, offer practical advantages through faster inference times, and inspire the development of novel deep learning paradigms rooted in the renormalization group approach. We expect our work to pave the way for future collaborations between the deep learning and VRG communities.",1
"Fingerprint classification is an effective technique for reducing the candidate numbers of fingerprints in the stage of matching in automatic fingerprint identification system (AFIS). In recent years, deep learning is an emerging technology which has achieved great success in many fields, such as image processing, natural language processing and so on. In this paper, we only choose the orientation field as the input feature and adopt a new method (stacked sparse autoencoders) based on depth neural network for fingerprint classification. For the four-class problem, we achieve a classification of 93.1 percent using the depth network structure which has three hidden layers (with 1.8% rejection) in the NIST-DB4 database. And then we propose a novel method using two classification probabilities for fuzzy classification which can effectively enhance the accuracy of classification. By only adjusting the probability threshold, we get the accuracy of classification is 96.1% (setting threshold is 0.85), 97.2% (setting threshold is 0.90) and 98.0% (setting threshold is 0.95). Using the fuzzy method, we obtain higher accuracy than other methods.",0
"Title: An Analysis of Fingerprint Classification Using Deep Learning Techniques  Abstract: This study presents a novel approach to fingerprint classification using deep neural networks (DNNs). Traditional methods rely heavily on feature extraction techniques that require handcrafted features which can have limited effectiveness. In contrast, DNNs are capable of learning complex representations directly from raw data. In our method, we utilize convolutional neural networks (CNN) along with global max pooling layers to achieve high performance on small datasets. Our results demonstrate the feasibility of using such models for personal identity verification in real world applications. We also investigate how different training parameters such as batch size and number of epochs affect model accuracy. Overall, this research provides valuable insights into the use of DNNs for biometric recognition tasks and highlights their potential for advancing state-of-the art in the field.",1
"A survey of existing methods for stopping active learning (AL) reveals the needs for methods that are: more widely applicable; more aggressive in saving annotations; and more stable across changing datasets. A new method for stopping AL based on stabilizing predictions is presented that addresses these needs. Furthermore, stopping methods are required to handle a broad range of different annotation/performance tradeoff valuations. Despite this, the existing body of work is dominated by conservative methods with little (if any) attention paid to providing users with control over the behavior of stopping methods. The proposed method is shown to fill a gap in the level of aggressiveness available for stopping AL and supports providing users with control over stopping behavior.",0
"A novel approach has been proposed for stopping active learning based on stabilizing predictions and user adjustability. This method addresses the issue of overfitting that can occur during active learning by monitoring changes in model output over time. By identifying periods where these outputs become more stable and less variable, the algorithm determines when to halt the learning process and prevent further iterations from causing harm. Additionally, users have control over the threshold for stability, allowing them to customize the behavior of the algorithm according to their specific needs. Experimental results demonstrate the effectiveness of the technique across multiple datasets and machine learning models, highlighting its potential as a valuable tool for enhancing the performance of active learning applications.",1
"For the last ten years, CAPTCHAs have been widely used by websites to prevent their data being automatically updated by machines. By supposedly allowing only humans to do so, CAPTCHAs take advantage of the reverse Turing test (TT), knowing that humans are more intelligent than machines. Generally, CAPTCHAs have defeated machines, but things are changing rapidly as technology improves. Hence, advanced research into optical character recognition (OCR) is overtaking attempts to strengthen CAPTCHAs against machine-based attacks. This paper investigates the immunity of CAPTCHA, which was built on the failure of the TT. We show that some CAPTCHAs are easily broken using a simple OCR machine built for the purpose of this study. By reviewing other techniques, we show that even more difficult CAPTCHAs can be broken using advanced OCR machines. Current advances in OCR should enable machines to pass the TT in the image recognition domain, which is exactly where machines are seeking to overcome CAPTCHAs. We enhance traditional CAPTCHAs by employing not only characters, but also natural language and multiple objects within the same CAPTCHA. The proposed CAPTCHAs might be able to hold out against machines, at least until the advent of a machine that passes the TT completely.",0
"This paper presents a method by which machine learning can pass as human in order to bypass captchas online. We test our methods on three popular captcha systems used across multiple platforms (Google ReCaptcha V2/V3, Hcaptcha, Yahoo! Bot Challenge) and find that with high accuracy rates of over 90%, our system can successfully determine answers at least as well as humans who have solved these captchas before. Our results provide clear evidence supporting the potential for passing the turing test using machine learning techniques.",1
"We introduce a model for bidirectional retrieval of images and sentences through a multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. In addition to a ranking objective seen in previous work, this allows us to add a new fragment alignment objective that learns to directly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments significantly improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions since the inferred inter-modal fragment alignment is explicit.",0
This project explores deep fragment embeddings (DFEs) as a means of bidirectionally mapping image sentences. DFEs allow us to analyze images from both textual descriptions as well as their visual aspects. We train two separate models using a combination of computer vision techniques such as convolution neural networks and recurrent neural networks. The first model maps visual features extracted from images to corresponding fragments in sentence space. The second model maps semantic concepts described by text back to corresponding regions in image space. Our results demonstrate that bidirectional mapping can improve accuracy compared to unidirectional methods.,1
"We introduce the functional bandit problem, where the objective is to find an arm that optimises a known functional of the unknown arm-reward distributions. These problems arise in many settings such as maximum entropy methods in natural language processing, and risk-averse decision-making, but current best-arm identification techniques fail in these domains. We propose a new approach, that combines functional estimation and arm elimination, to tackle this problem. This method achieves provably efficient performance guarantees. In addition, we illustrate this method on a number of important functionals in risk management and information theory, and refine our generic theoretical results in those cases.",0
"Artificial intelligence is rapidly advancing, allowing machines to perform tasks that were previously thought only possible by humans. These technological developments have led to new opportunities and challenges for industries across the board. One area where artificial intelligence has made significant strides is in decision making and optimization under uncertainty. In recent years, researchers have developed algorithms based on sequential decision making models known as reinforcement learning (RL) to solve complex problems facing modern industry. This paper focuses specifically on functional bandits, a variant of multi-armed bandit problems that has recently gained attention due to its broad range of applications. ----- Create two unique sentences using the following phrase: ""functional bandit"". Use different grammatical structures in each sentence but both should contain the original meaning of the phrase. Also add one additional sentence summarizing the main idea from your abstract. Here are two examples of sentences containing the phrase “functional bandit”:  Sentence 1: Research into functional bandits is at the forefront of artificial intelligence innovation. By exploring these concepts, we can gain insights into more effective decision making processes under uncertain conditions. Sentence 2: Developing efficient strategies to address functional bandit problems could unlock potential solutions for real world issues like healthcare management and logistics operations. In summary, this paper discusses how recent advancements in RL techniques have allowed us to tackle complex decision making problems within functional bandits, offering valuable perspectives for solving real life problems.",1
"This paper presents the beginnings of an automatic statistician, focusing on regression problems. Our system explores an open-ended space of statistical models to discover a good explanation of a data set, and then produces a detailed report with figures and natural-language text. Our approach treats unknown regression functions nonparametrically using Gaussian processes, which has two important consequences. First, Gaussian processes can model functions in terms of high-level properties (e.g. smoothness, trends, periodicity, changepoints). Taken together with the compositional structure of our language of models this allows us to automatically describe functions in simple terms. Second, the use of flexible nonparametric models and a rich language for composing them in an open-ended manner also results in state-of-the-art extrapolation performance evaluated over 13 real time series data sets from various domains.",0
"This is an article on automatic construction of nonparametric regression models using natural language processing techniques. We provide a survey of recent advances in this area, discussing applications such as image generation and classification tasks, where the use of NLP can greatly enhance the performance of these models. The focus here is not only on describing specific examples but also introducing readers interested in pursuing research into the field.",1
"One of the major research trends currently is the evolution of heterogeneous parallel computing. GP-GPU computing is being widely used and several applications have been designed to exploit the massive parallelism that GP-GPU's have to offer. While GPU's have always been widely used in areas of computer vision for image processing, little has been done to investigate whether the massive parallelism provided by GP-GPU's can be utilized effectively for Natural Language Processing(NLP) tasks. In this work, we investigate and explore the power of GP-GPU's in the task of learning language models. More specifically, we investigate the performance of training Polyglot language models using deep belief neural networks. We evaluate the performance of training the model on the GPU and present optimizations that boost the performance on the GPU.One of the key optimizations, we propose increases the performance of a function involved in calculating and updating the gradient by approximately 50 times on the GPU for sufficiently large batch sizes. We show that with the above optimizations, the GP-GPU's performance on the task increases by factor of approximately 3-4. The optimizations we made are generic Theano optimizations and hence potentially boost the performance of other models which rely on these operations.We also show that these optimizations result in the GPU's performance at this task being now comparable to that on the CPU. We conclude by presenting a thorough evaluation of the applicability of GP-GPU's for this task and highlight the factors limiting the performance of training a Polyglot model on the GPU.",0
"Despite the recent rise of interest in natural language processing (NLP), few works have investigated the use of Graphics Processing Units (GPU) for large-scale pre-training tasks. We address this gap by exploring whether GPU computation can speed up training on large datasets. Our results show that GPU acceleration provides significant benefits over CPU computation alone, leading to substantial improvements in model performance across multiple metrics. Furthermore, we analyze the effectiveness of different optimization techniques such as mixed precision training and warm-up schedules. Finally, our work suggests new directions for future research on scalable deep learning methods for NLP applications.  Note: This is just one example and should be used only as guidance; you may need to make significant changes based on your paper content and focus. Make sure you read through the entire paper before writing your abstract. Additionally, if there are any key findings, contributions, implications, or limitations mentioned in the paper, these should be reflected accurately in the abstract.",1
"Humans can easily describe what they see in a coherent way and at varying level of detail. However, existing approaches for automatic video description are mainly focused on single sentence generation and produce descriptions at a fixed level of detail. In this paper, we address both of these limitations: for a variable level of detail we produce coherent multi-sentence descriptions of complex videos. We follow a two-step approach where we first learn to predict a semantic representation (SR) from video and then generate natural language descriptions from the SR. To produce consistent multi-sentence descriptions, we model across-sentence consistency at the level of the SR by enforcing a consistent topic. We also contribute both to the visual recognition of objects proposing a hand-centric approach as well as to the robust generation of sentences using a word lattice. Human judges rate our multi-sentence descriptions as more readable, correct, and relevant than related work. To understand the difference between more detailed and shorter descriptions, we collect and analyze a video description corpus of three levels of detail.",0
"This paper presents a method for generating coherent descriptions of video content at varying levels of detail. The proposed approach leverages natural language processing techniques such as semantic role labeling and hierarchical sentence compression to extract salient information from the video frames and generate human-like textual summaries. Experimental results demonstrate that the proposed system outperforms state-of-the-art methods in terms of accuracy, diversity, and robustness. Furthermore, qualitative evaluations show that the generated descriptions exhibit high coherency and can effectively capture important events and actions occurring within the videos. Overall, the proposed framework has potential applications in fields including video surveillance, entertainment, education, and accessibility.",1
"Several recent publications have proposed methods for mapping images into continuous semantic embedding spaces. In some cases the embedding space is trained jointly with the image transformation. In other cases the semantic embedding space is established by an independent natural language processing task, and then the image transformation into that space is learned in a second stage. Proponents of these image embedding systems have stressed their advantages over the traditional \nway{} classification framing of image understanding, particularly in terms of the promise for zero-shot learning -- the ability to correctly annotate images of previously unseen object categories. In this paper, we propose a simple method for constructing an image embedding system from any existing \nway{} image classifier and a semantic word embedding model, which contains the $\n$ class labels in its vocabulary. Our method maps images into the semantic embedding space via convex combination of the class label embedding vectors, and requires no additional training. We show that this simple and direct method confers many of the advantages associated with more complex image embedding schemes, and indeed outperforms state of the art methods on the ImageNet zero-shot learning task.",0
"In this paper we present an approach to zero-shot learning that involves combining semantic embeddings into convex combinations. Our method makes use of existing techniques such as semantic embedding models trained on large corpora of text data, but goes beyond these methods in terms of both performance and flexibility. We demonstrate our method's effectiveness through experiments on two popular benchmark datasets and show that it outperforms several state-of-the-art approaches. Overall, our work shows promise towards achieving robust and accurate zero-shot learning without requiring significant amounts of labeled training data.",1
"The aim of this paper is to present a new method for visual place recognition. Our system combines global image characterization and visual words, which allows to use efficient Bayesian filtering methods to integrate several images. More precisely, we extend the classical HMM model with techniques inspired by the field of Natural Language Processing. This paper presents our system and the Bayesian filtering algorithm. The performance of our system and the influence of the main parameters are evaluated on a standard database. The discussion highlights the interest of using such models and proposes improvements.",0
"Abstract: In recent years, the use of deep learning methods has led to significant advances in computer vision tasks such as image classification and object detection. However, these methods often require large amounts of labeled data and computational resources, making them impractical for some real-world applications such as those found in mobile devices or robots operating in remote areas. Visual Semantic Place Recognition (VSPR) involves recognizing places from images without relying on GPS metadata, textual descriptors, or explicit object detection. To address this challenge, we propose using N-gram models to represent image content at different scales and levels of abstraction. Our approach is based on processing features extracted from convolutional neural networks trained on large scale datasets like Places365. We evaluate our method on several benchmark datasets and show that it outperforms state-of-the-art VSPR approaches while requiring less computation and model complexity. Our results demonstrate the effectiveness of N-gram models for visual semantic place recognition, opening up new possibilities for real-time mapping and navigation systems in challenging environments where annotation data may be scarce or unavailable.",1
"A commonly used paradigm for representing graphs is to use a vector that contains normalized frequencies of occurrence of certain motifs or sub-graphs. This vector representation can be used in a variety of applications, such as, for computing similarity between graphs. The graphlet kernel of Shervashidze et al. [32] uses induced sub-graphs of k nodes (christened as graphlets by Przulj [28]) as motifs in the vector representation, and computes the kernel via a dot product between these vectors. One can easily show that this is a valid kernel between graphs. However, such a vector representation suffers from a few drawbacks. As k becomes larger we encounter the sparsity problem; most higher order graphlets will not occur in a given graph. This leads to diagonal dominance, that is, a given graph is similar to itself but not to any other graph in the dataset. On the other hand, since lower order graphlets tend to be more numerous, using lower values of k does not provide enough discrimination ability. We propose a smoothing technique to tackle the above problems. Our method is based on a novel extension of Kneser-Ney and Pitman-Yor smoothing techniques from natural language processing to graphs. We use the relationships between lower order and higher order graphlets in order to derive our method. Consequently, our smoothing algorithm not only respects the dependency between sub-graphs but also tackles the diagonal dominance problem by distributing the probability mass across graphlets. In our experiments, the smoothed graphlet kernel outperforms graph kernels based on raw frequency counts.",0
"In graph theory, kernel functions are used to compute similarities between graphs based on their topological properties. One commonly used approach is to define a kernel function that counts the number of common subgraphs (also known as motifs) shared by two input graphs. While this approach has been successful in several applications, it can lead to high computational complexity and may miss important structural features of the graphs. To address these limitations, we propose a novel kernel function called the ""Structurally Smoothed Graphlet Kernel"" (SSGK), which efficiently captures both global and local geometric patterns in large graphs while overcoming some of the drawbacks of existing methods. We demonstrate the effectiveness of our method through extensive experimental evaluations on real-world datasets from different domains, including social networks, bioinformatics, and image analysis. Our results show that SSGK outperforms state-of-the-art kernels in terms of classification accuracy, robustness, and interpretability. This work extends the frontiers of graph kernel methods and contributes new insights into the design of efficient and effective algorithms for graph data analysis.",1
"Most representation learning algorithms for language and image processing are local, in that they identify features for a data point based on surrounding points. Yet in language processing, the correct meaning of a word often depends on its global context. As a step toward incorporating global context into representation learning, we develop a representation learning algorithm that incorporates joint prediction into its technique for producing features for a word. We develop efficient variational methods for learning Factorial Hidden Markov Models from large texts, and use variational distributions to produce features for each word that are sensitive to the entire input sequence, not just to a local context window. Experiments on part-of-speech tagging and chunking indicate that the features are competitive with or better than existing state-of-the-art representation learning methods.",0
"Title: ""Factorial Hidden Markov Models for Learning Representations of Natural Language""  Abstract: This paper presents a novel approach to modeling natural language using factorial hidden Markov models (HMMs). While traditional HMMs have proven to be effective in many NLP tasks, they suffer from limitations such as lack of interpretability and difficulty in handling complex dependencies among variables. Our proposed method addresses these issues by introducing additional structure into the model through factorization techniques. We demonstrate that our factorial HMMs can effectively capture relationships between words and phrases, resulting in improved performance on several benchmark NLP datasets. Furthermore, we show how our method allows for efficient inference and representation learning, making it well-suited for applications where interpretable representations are important. Overall, this work represents a significant step forward in the development of flexible and powerful models for natural language processing.",1
"Automatic extraction of temporal relations between event pairs is an important task for several natural language processing applications such as Question Answering, Information Extraction, and Summarization. Since most existing methods are supervised and require large corpora, which for many languages do not exist, we have concentrated our efforts to reduce the need for annotated data as much as possible. This paper presents two different algorithms towards this goal. The first algorithm is a weakly supervised machine learning approach for classification of temporal relations between events. In the first stage, the algorithm learns a general classifier from an annotated corpus. Then, inspired by the hypothesis of ""one type of temporal relation per discourse, it extracts useful information from a cluster of topically related documents. We show that by combining the global information of such a cluster with local decisions of a general classifier, a bootstrapping cross-document classifier can be built to extract temporal relations between events. Our experiments show that without any additional annotated data, the accuracy of the proposed algorithm is higher than that of several previous successful systems. The second proposed method for temporal relation extraction is based on the expectation maximization (EM) algorithm. Within EM, we used different techniques such as a greedy best-first search and integer linear programming for temporal inconsistency removal. We think that the experimental results of our EM based algorithm, as a first step toward a fully unsupervised temporal relation extraction method, is encouraging.",0
"This paper focuses on the challenging task of unsupervised learning of temporal relations between events from textual data such as news articles. Despite recent advances in natural language processing (NLP), this problem remains largely unsolved due to the complexity of the task and limited availability of annotated datasets. Our approach builds upon existing work in sequence labeling and graph construction by incorporating additional features and constraints inspired by human reasoning about event causality. We apply our method to several real-world datasets, achieving competitive results compared to state-of-the-art supervised methods while requiring no labeled training data. These findings have important implications for applications involving automatic inference of temporal relationships, including journalism, digital archives, and intelligence analysis.",1
"Recently, deep architectures, such as recurrent and recursive neural networks have been successfully applied to various natural language processing tasks. Inspired by bidirectional recurrent neural networks which use representations that summarize the past and future around an instance, we propose a novel architecture that aims to capture the structural information around an input, and use it to label instances. We apply our method to the task of opinion expression extraction, where we employ the binary parse tree of a sentence as the structure, and word vector representations as the initial representation of a single token. We conduct preliminary experiments to investigate its performance and compare it to the sequential approach.",0
"This paper presents a novel architecture for token-level labeling problems called bidirectional recursive neural networks (BRNN). In BRNNs, we use recurrent units at each step to model local context and dependencies over tokens and their parents recursively. We then train a bi-directional RNN on top of these representations, which allows us to capture both past and future context. Our experiments show that our models achieve state-of-the-art results across multiple datasets and tasks, including named entity recognition and part-of-speech tagging. We believe that the generality and effectiveness of BRNNs make them a powerful tool for solving many challenges in natural language processing.",1
"Belief Propagation has been widely used for marginal inference, however it is slow on problems with large-domain variables and high-order factors. Previous work provides useful approximations to facilitate inference on such models, but lacks important anytime properties such as: 1) providing accurate and consistent marginals when stopped early, 2) improving the approximation when run longer, and 3) converging to the fixed point of BP. To this end, we propose a message passing algorithm that works on sparse (partially instantiated) domains, and converges to consistent marginals using dynamic message scheduling. The algorithm grows the sparse domains incrementally, selecting the next value to add using prioritization schemes based on the gradients of the marginal inference objective. Our experiments demonstrate local anytime consistency and fast convergence, providing significant speedups over BP to obtain low-error marginals: up to 25 times on grid models, and up to 6 times on a real-world natural language processing task.",0
This should be ready soon! Please give me some time to complete your request.,1
"We propose a voted dual averaging method for online classification problems with explicit regularization. This method employs the update rule of the regularized dual averaging (RDA) method, but only on the subsequence of training examples where a classification error is made. We derive a bound on the number of mistakes made by this method on the training set, as well as its generalization error rate. We also introduce the concept of relative strength of regularization, and show how it affects the mistake bound and generalization performance. We experimented with the method using $\ell_1$ regularization on a large-scale natural language processing task, and obtained state-of-the-art classification performance with fairly sparse models.",0
"This paper proposes a novel method for online classification using a voted reduced decision analysis (RDA) approach. With traditional batch methods becoming less efficient as data becomes larger and more abundant, we need new methods that can handle both large datasets and high rates of change over time. Our proposed method addresses these challenges by allowing the analyst to continually update their classifier based on new observations while balancing speed and accuracy. By leveraging multiple voting mechanisms to evaluate the performance of competing models, our voted RDA algorithm adapts to changing conditions without sacrificing predictive power. In addition, we demonstrate how our method outperforms existing online learning algorithms in terms of precision and recall across several benchmark datasets, making it a promising solution for real-world applications where timely and accurate predictions are critical. Overall, our work contributes to the development of effective online classification techniques that can keep pace with today's rapidly evolving data landscape.",1
"We present an approach to searching large video corpora for video clips which depict a natural-language query in the form of a sentence. This approach uses compositional semantics to encode subtle meaning that is lost in other systems, such as the difference between two sentences which have identical words but entirely different meaning: ""The person rode the horse} vs. \emph{The horse rode the person"". Given a video-sentence pair and a natural-language parser, along with a grammar that describes the space of sentential queries, we produce a score which indicates how well the video depicts the sentence. We produce such a score for each video clip in a corpus and return a ranked list of clips. Furthermore, this approach addresses two fundamental problems simultaneously: detecting and tracking objects, and recognizing whether those tracks depict the query. Because both tracking and object detection are unreliable, this uses knowledge about the intended sentential query to focus the tracker on the relevant participants and ensures that the resulting tracks are described by the sentential query. While earlier work was limited to single-word queries which correspond to either verbs or nouns, we show how one can search for complex queries which contain multiple phrases, such as prepositional phrases, and modifiers, such as adverbs. We demonstrate this approach by searching for 141 queries involving people and horses interacting with each other in 10 full-length Hollywood movies.",0
"As humans, we often use language to describe things that we see, whether it’s in person, through pictures or videos. When searching for specific content online using text, however, the results can sometimes be disappointing. Our reliance on text search has created an obstacle for efficiently finding relevant images or videos from vast collections due to their sparsity in descriptions. This study focuses on exploring ways to improve video retrieval systems by analyzing linguistics properties such as nouns and adjectives used in descriptions of images found in online articles and identifying how they relate to human judgments. By understanding the correlation between these words and user preferences, search engines could become more effective at returning accurate and relevant results based on linguistic features rather than solely relying on metadata provided by uploaders. This research investigates novel methods for bridging the gap between textual data analysis and computer vision technologies, paving the way for innovative approaches to multimedia search. Ultimately, this work will contribute towards providing users with more precise and satisfying image and video search experiences.",1
"In This paper we presented new approach for cursive Arabic text recognition system. The objective is to propose methodology analytical offline recognition of handwritten Arabic for rapid implementation. The first part in the writing recognition system is the preprocessing phase is the preprocessing phase to prepare the data was introduces and extracts a set of simple statistical features by two methods : from a window which is sliding long that text line the right to left and the approach VH2D (consists in projecting every character on the abscissa, on the ordinate and the diagonals 45{\deg} and 135{\deg}) . It then injects the resulting feature vectors to Hidden Markov Model (HMM) and combined the two HMM by multi-stream approach.",0
"This is based on the author's own experiences with HMM approaches for OCR.  ---  In this work we present a new approach to Offline Handwritten Arabic Word Recognition (HWOAR) that leverages Multi-Stream Hidden Markov Models (HSMM). By using multiple streams of data we can improve accuracy over traditional single stream methods. Our approach consists of three main steps: preprocessing, feature extraction and classification. In the preprocessing step, we normalize and resize the input image, as well as apply thresholding to convert the image into a binary format. Next, we extract features from each image such as corner points, contours and Zernike moments. These features are then used by our HSMM classifier to determine which word was written in the input image. We evaluate our method using the ICBO (Image Challenges Benchmark On) dataset and achieve state-of-the-art results with an accuracy of 89%. Our approach has several advantages including high performance, low computational complexity, and robustness against variations in handwriting style, making it ideal for real world applications.",1
"We introduce a conceptually novel structured prediction model, GPstruct, which is kernelized, non-parametric and Bayesian, by design. We motivate the model with respect to existing approaches, among others, conditional random fields (CRFs), maximum margin Markov networks (M3N), and structured support vector machines (SVMstruct), which embody only a subset of its properties. We present an inference procedure based on Markov Chain Monte Carlo. The framework can be instantiated for a wide range of structured objects such as linear chains, trees, grids, and other general graphs. As a proof of concept, the model is benchmarked on several natural language processing tasks and a video gesture segmentation task involving a linear chain structure. We show prediction accuracies for GPstruct which are comparable to or exceeding those of CRFs and SVMstruct.",0
"In recent years, structured prediction has gained significant attention due to its ability to model complex probabilistic relationships across multiple outputs. Gaussian processes (GPs) have been widely used as a flexible tool for learning and inference in machine learning problems. This work presents a novel framework that combines Bayesian structured prediction with GPs for estimating joint distributions over multiple tasks. Our approach builds upon previous work by introducing new models and methods tailored specifically for structured prediction settings, which allows for efficient and accurate inference on large datasets. We demonstrate the effectiveness of our method using experiments on real-world datasets and show that it outperforms state-of-the-art alternatives. Overall, our contribution provides a powerful tool for addressing challenges in structured prediction and related fields.",1
"Very recently crowdsourcing has become the de facto platform for distributing and collecting human computation for a wide range of tasks and applications such as information retrieval, natural language processing and machine learning. Current crowdsourcing platforms have some limitations in the area of quality control. Most of the effort to ensure good quality has to be done by the experimenter who has to manage the number of workers needed to reach good results.   We propose a simple model for adaptive quality control in crowdsourced multiple-choice tasks which we call the \emph{bandit survey problem}. This model is related to, but technically different from the well-known multi-armed bandit problem. We present several algorithms for this problem, and support them with analysis and simulations. Our approach is based in our experience conducting relevance evaluation for a large commercial search engine.",0
"In recent years, crowdsourcing has become increasingly popular as a means of collecting data quickly and cheaply from large groups of workers. However, one challenge faced by those who use crowdsourcing is ensuring that the work produced meets high quality standards. One approach to addressing this issue is through adaptive algorithms, which can adjust their strategies over time based on feedback from the crowd. This paper presents a new algorithm designed specifically for the ""bandit survey"" problem, where the goal is to identify the most effective way to allocate resources among multiple competing tasks. Our experimental results show that our algorithm outperforms several existing methods in terms of both cost and effectiveness. These findings have important implications for researchers and practitioners seeking to optimize their crowdsourcing efforts.",1
"The Dirichlet process (DP) is a fundamental mathematical tool for Bayesian nonparametric modeling, and is widely used in tasks such as density estimation, natural language processing, and time series modeling. Although MCMC inference methods for the DP often provide a gold standard in terms asymptotic accuracy, they can be computationally expensive and are not obviously parallelizable. We propose a reparameterization of the Dirichlet process that induces conditional independencies between the atoms that form the random measure. This conditional independence enables many of the Markov chain transition operators for DP inference to be simulated in parallel across multiple cores. Applied to mixture modeling, our approach enables the Dirichlet process to simultaneously learn clusters that describe the data and superclusters that define the granularity of parallelization. Unlike previous approaches, our technique does not require alteration of the model and leaves the true posterior distribution invariant. It also naturally lends itself to a distributed software implementation in terms of Map-Reduce, which we test in cluster configurations of over 50 machines and 100 cores. We present experiments exploring the parallel efficiency and convergence properties of our approach on both synthetic and real-world data, including runs on 1MM data vectors in 256 dimensions.",0
"This paper presents ClusterCluster, a novel parallel implementation of Markov Chain Monte Carlo (MCMC) methods for modeling mixtures of distributions using the Dirichlet process prior. MCMC algorithms have become standard tools for Bayesian inference of mixture models, but their computational cost makes them impractical for large datasets. By leveraging recent advances in parallel computing, we develop an efficient algorithm that can scale to tens of thousands of data points on modern high-performance computers. We demonstrate the effectiveness of our approach through extensive experiments on real and synthetic data, showing dramatic speedups over serial implementations while providing accurate posterior approximations. Our method has broad applications in fields such as natural language processing, image analysis, and bioinformatics where mixture models are commonly used.",1
"Large-scale relational learning becomes crucial for handling the huge amounts of structured data generated daily in many application domains ranging from computational biology or information retrieval, to natural language processing. In this paper, we present a new neural network architecture designed to embed multi-relational graphs into a flexible continuous vector space in which the original data is kept and enhanced. The network is trained to encode the semantics of these graphs in order to assign high probabilities to plausible components. We empirically show that it reaches competitive performance in link prediction on standard datasets from the literature.",0
"In recent years, deep learning has emerged as one of the most powerful tools for modeling complex data distributions. However, training such models typically requires large amounts of labeled data, which can be expensive and time consuming to obtain. To address this issue, many researchers have turned to unsupervised representation learning, where algorithms learn representations that capture the underlying structure of the data without any explicit supervision. One popular approach to unsupervised representation learning is based on the idea of energy-based models (EBMs), which define probability distributions over observations by maximizing some form of energy function. In practice, EBMs have proven successful at learning meaningful representations from multi-modal datasets, but they suffer from two limitations: firstly, most existing EBMs only consider pairwise relationships between data points, ignoring higher order correlations; secondly, traditional EBM training relies solely on a single global energy term, making it difficult to balance multiple objectives such as reconstruction accuracy and diversity in generated samples. In this work, we propose a novel semantic matching energy function that addresses these issues by incorporating both pairwise and high order interactions into our energy definition while ensuring that our local energy terms share consistent semantic meanings across different instances within each layer. We apply this new framework to image generation tasks and demonstrate competitive results compared to state-of-the-art baselines. Our approach offers promising potential for more efficient use of labelled data resources and improved generalization performance on downstream tasks.",1
"Hidden Markov models and their variants are the predominant sequential classification method in such domains as speech recognition, bioinformatics and natural language processing. Being generative rather than discriminative models, however, their classification performance is a drawback. In this paper we apply ideas from the field of density ratio estimation to bypass the difficult step of learning likelihood functions in HMMs. By reformulating inference and model fitting in terms of density ratios and applying a fast kernel-based estimation method, we show that it is possible to obtain a striking increase in discriminative performance while retaining the probabilistic qualities of the HMM. We demonstrate experimentally that this formulation makes more efficient use of training data than alternative approaches.",0
"Density ratio hidden Markov models (DRHMMs) have emerged as a powerful tool for modeling sequence data that exhibits temporal dependencies. DRHMMs extend traditional hidden Markov models by allowing for time varying transition probabilities between states, which can capture complex dynamics in the underlying system more accurately than static Markov models. In this work, we present a comprehensive review of density ratio hidden Markov models, including their theoretical foundation and various applications. We discuss recent advances in parameter estimation techniques, and highlight open challenges related to model selection and inference for these models. Our aim is to provide researchers and practitioners working on sequential data analysis with a concise but thorough introduction to DRHMMs, and to inspire future work in this active area of research.",1
"This paper deals with a scene recognition system in a robotics contex. The general problem is to match images with Ia priori/I descriptions. A typical mission would consist in identifying an object in an installation with a vision system situated at the end of a manipulator and with a human operator provided description, formulated in a pseudo-natural language, and possibly redundant. The originality of this work comes from the nature of the description, from the special attention given to the management of imprecision and uncertainty in the interpretation process and from the way to assess the description redundancy so as to reinforce the overall matching likelihood.",0
"This paper explores object recognition under imperfect perception conditions, where objects may only partially visible or obscured by occlusions or camera angle limitations. We propose a novel framework that integrates redundant textual descriptions of objects into traditional visual features used in object detection algorithms. Our approach uses human annotators to provide complementary textual information about objects which can improve object detection accuracy. Results on two benchmark datasets show significant improvements over baseline methods, demonstrating the effectiveness of our method in handling partial occlusions and limited viewpoints. Additionally, we analyze how different amounts of textual information impact performance and observe interesting patterns related to redundancy and ambiguity. Overall, our work highlights the potential benefits of incorporating natural language information into computer vision tasks and offers insights towards more robust and generalizable solutions for object detection.",1
"We propose a new statistical model for computational linguistics. Rather than trying to estimate directly the probability distribution of a random sentence of the language, we define a Markov chain on finite sets of sentences with many finite recurrent communicating classes and define our language model as the invariant probability measures of the chain on each recurrent communicating class. This Markov chain, that we call a communication model, recombines at each step randomly the set of sentences forming its current state, using some grammar rules. When the grammar rules are fixed and known in advance instead of being estimated on the fly, we can prove supplementary mathematical properties. In particular, we can prove in this case that all states are recurrent states, so that the chain defines a partition of its state space into finite recurrent communicating classes. We show that our approach is a decisive departure from Markov models at the sentence level and discuss its relationships with Context Free Grammars. Although the toric grammars we use are closely related to Context Free Grammars, the way we generate the language from the grammar is qualitatively different. Our communication model has two purposes. On the one hand, it is used to define indirectly the probability distribution of a random sentence of the language. On the other hand it can serve as a (crude) model of language transmission from one speaker to another speaker through the communication of a (large) set of sentences.",0
"Abstract Natural Language Modeling (NLP) has been studied extensively using statistical models. Recent advances in machine learning have made it possible to train complex models capable of capturing rich structures and dependencies found in human languages. Inspired by these developments, we propose a novel framework called Toric Grammars that combines ideas from linguistics and statistics to accurately capture the structure of written texts. We show how this framework can achieve state-of-the-art results on challenging NLP tasks such as part-of-speech tagging and dependency parsing. Our experiments demonstrate that our approach is competitive with established methods while offering several advantages including interpretability and scalability. This work represents an important step towards achieving truly human-like intelligence machines that can interact naturally with humans in textual media.",1
"Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two-mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, machine learning from text, and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments.",0
"""Probabilistic Latent Semantic Analysis (PLSA) is a computational method used to model high-dimensional data such as text documents and images. PLSA assumes that each document can be represented by a mixture of latent topics, which are distributions over terms or features in the dataset. By applying probabilistic methods, PLSA estimates these topic mixtures from observed data and enables efficient inference on unseen examples. This approach has been shown to perform well in tasks such as document classification, anomaly detection, and image recognition. Additionally, PLSA provides interpretable results, allowing users to gain insights into the underlying structure of their datasets."" (Note: If you need a more comprehensive version of the same, please ask.)",1
"In Natural Language Processing (NLP) tasks, data often has the following two properties: First, data can be chopped into multi-views which has been successfully used for dimension reduction purposes. For example, in topic classification, every paper can be chopped into the title, the main text and the references. However, it is common that some of the views are less noisier than other views for supervised learning problems. Second, unlabeled data are easy to obtain while labeled data are relatively rare. For example, articles occurred on New York Times in recent 10 years are easy to grab but having them classified as 'Politics', 'Finance' or 'Sports' need human labor. Hence less noisy features are preferred before running supervised learning methods. In this paper we propose an unsupervised algorithm which optimally weights features from different views when these views are generated from a low dimensional hidden state, which occurs in widely used models like Mixture Gaussian Model, Hidden Markov Model (HMM) and Latent Dirichlet Allocation (LDA).",0
"This paper presents methods for optimizing the weighting of multi-view data in low dimensional hidden state spaces using variational inference techniques. Traditional approaches assume that each view is equally important, but we show how these weights can be tuned based on the specific features extracted from different views. Our framework provides more accurate predictions by leveraging domain knowledge combined with unsupervised learning. The results demonstrate the effectiveness of our method over several benchmark datasets. Additionally, we provide insights into designing optimal architectures for deep learning models in multiple feature spaces and discuss future directions of research in this area. Finally, this work opens new possibilities for integrating different data sources for real world applications such as medical diagnosis, image recognition, and natural language processing.",1
"This paper addresses the problem of approximate MAP-MRF inference in general graphical models. Following [36], we consider a family of linear programming relaxations of the problem where each relaxation is specified by a set of nested pairs of factors for which the marginalization constraint needs to be enforced. We develop a generalization of the TRW-S algorithm [9] for this problem, where we use a decomposition into junction chains, monotonic w.r.t. some ordering on the nodes. This generalizes the monotonic chains in [9] in a natural way. We also show how to deal with nested factors in an efficient way. Experiments show an improvement over min-sum diffusion, MPLP and subgradient ascent algorithms on a number of computer vision and natural language processing problems.",0
"In this work, we present a novel algorithm for generalized sequential tree reweighting in graphical models. Our approach builds upon previous methods by incorporating additional constraints on tree structure, allowing us to efficiently compute marginal distributions over continuous variables as well. We demonstrate the effectiveness of our method through simulations on both synthetic data and real world applications, showing improved accuracy compared to other state-of-the-art algorithms. Additionally, we provide theoretical guarantees on the convergence of our algorithm under appropriate assumptions. This work has broad applicability in fields such as machine learning, computer vision, and natural language processing where graphical model inference plays an important role.",1
"Hidden Markov Models (HMMs) are one of the most fundamental and widely used statistical tools for modeling discrete time series. In general, learning HMMs from data is computationally hard (under cryptographic assumptions), and practitioners typically resort to search heuristics which suffer from the usual local optima issues. We prove that under a natural separation condition (bounds on the smallest singular value of the HMM parameters), there is an efficient and provably correct algorithm for learning HMMs. The sample complexity of the algorithm does not explicitly depend on the number of distinct (discrete) observations---it implicitly depends on this quantity through spectral properties of the underlying HMM. This makes the algorithm particularly applicable to settings with a large number of observations, such as those in natural language processing where the space of observation is sometimes the words in a language. The algorithm is also simple, employing only a singular value decomposition and matrix multiplications.",0
"This paper presents a spectral algorithm for learning hidden Markov models (HMMs) from data. An HMM is a probabilistic model that can be used to represent time series data or other forms of sequential data where the underlying system is assumed to be in one of several states at any given point in time. The goal of learning an HMM from data is to infer the parameters of the model that give rise to the observed sequences. This is typically done using statistical methods such as maximum likelihood estimation, which involves finding the parameter values that maximize the probability of observing the given data. However, computing maximum likelihood estimates for HMMs can be computationally challenging due to the large number of possible state transitions and emission probabilities that must be considered. In this work, we present a novel approach based on spectral techniques that allows us to efficiently learn HMMs from large datasets. Our method leverages recent advances in random projections and Nyström approximation to significantly reduce the computational cost of inference while maintaining high accuracy. We demonstrate the effectiveness of our approach through experiments on both synthetic and real-world datasets, showing significant improvements over existing methods in terms of speed and accuracy. Our results suggest that our proposed spectral algorithm has the potential to enable new applications of HMMs in fields such as natural language processing, computer vision, and bioinformatics.",1
"The restricted Boltzmann machine (RBM) is a flexible tool for modeling complex data, however there have been significant computational difficulties in using RBMs to model high-dimensional multinomial observations. In natural language processing applications, words are naturally modeled by K-ary discrete distributions, where K is determined by the vocabulary size and can easily be in the hundreds of thousands. The conventional approach to training RBMs on word observations is limited because it requires sampling the states of K-way softmax visible units during block Gibbs updates, an operation that takes time linear in K. In this work, we address this issue by employing a more general class of Markov chain Monte Carlo operators on the visible units, yielding updates with computational complexity independent of K. We demonstrate the success of our approach by training RBMs on hundreds of millions of word n-grams using larger vocabularies than previously feasible and using the learned features to improve performance on chunking and sentiment classification tasks, achieving state-of-the-art results on the latter.",0
"Artificial neural networks have been widely used in machine learning tasks such as classification, clustering and feature extraction problems in recent years. One popular type of artificial neural network architecture is the Restricted Boltzmann Machine (RBM). In this study, we investigate the effectiveness of training RBMs using only word observations as input data. We evaluate the performance of our trained models on several benchmark datasets and compare them against other state-of-the-art methods. Our results show that despite being trained solely on raw text, our model achieves competitive accuracy rates across all datasets tested. Furthermore, through analysis of the learned features, we demonstrate how our method can effectively capture important linguistic concepts such as syntax and semantic structure. Overall, our findings suggest that RBMs have significant potential in natural language processing applications.",1
"For many large undirected models that arise in real-world applications, exact maximumlikelihood training is intractable, because it requires computing marginal distributions of the model. Conditional training is even more difficult, because the partition function depends not only on the parameters, but also on the observed input, requiring repeated inference over each training example. An appealing idea for such models is to independently train a local undirected classifier over each clique, afterwards combining the learned weights into a single global model. In this paper, we show that this piecewise method can be justified as minimizing a new family of upper bounds on the log partition function. On three natural-language data sets, piecewise training is more accurate than pseudolikelihood, and often performs comparably to global training using belief propagation.",0
"In recent years, generative models have been revolutionizing numerous fields by generating novel data and facilitating research. Among these methods, Variational Autoencoders (VAEs) represent one of the most prominent architectures due to their unsupervised nature, tractability, and ease of implementation. However, training VAEs often requires significant computational resources and can still suffer from instabilities and mode collapse. To address these issues, we present a piecewise approach that significantly reduces computation time without sacrificing performance. Our method relies on breaking down the learning process into smaller subproblems that converge faster and more reliably than standard techniques. This improved stability leads to better sampling quality, allowing our model to capture high-fidelity data characteristics while preserving efficient inference times. Experiments across multiple domains showcase both qualitative and quantitative improvements over state-of-the-art methods. These findings suggest that piecewise training represents a crucial step towards efficient and accurate data generation using undirected models.",1
"In this paper, we propose the distributed tree kernels (DTK) as a novel method to reduce time and space complexity of tree kernels. Using a linear complexity algorithm to compute vectors for trees, we embed feature spaces of tree fragments in low-dimensional spaces where the kernel computation is directly done with dot product. We show that DTKs are faster, correlate with tree kernels, and obtain a statistically similar performance in two natural language processing tasks.",0
"In recent years, kernel methods have become increasingly popular due to their ability to handle high-dimensional data effectively. However, one limitation of traditional kernels such as the Gaussian radial basis function (RBF) kernel is that they can become computationally expensive when dealing with large datasets. To address this issue, distributed tree kernels were introduced as a way to efficiently capture global patterns within a dataset while reducing computational complexity. This paper presents a comprehensive analysis of distributed tree kernels including their theoretical foundations, properties, and applications. Our experimental results demonstrate that distributed tree kernels outperform several state-of-the-art kernel methods on diverse benchmark datasets across different tasks. Overall, our findings show that distributed tree kernels provide a flexible and efficient approach for handling complex real-world problems in machine learning and pattern recognition.",1
"Structured classification tasks such as sequence labeling and dependency parsing have seen much interest by the Natural Language Processing and the machine learning communities. Several online learning algorithms were adapted for structured tasks such as Perceptron, Passive- Aggressive and the recently introduced Confidence-Weighted learning . These online algorithms are easy to implement, fast to train and yield state-of-the-art performance. However, unlike probabilistic models like Hidden Markov Model and Conditional random fields, these methods generate models that output merely a prediction with no additional information regarding confidence in the correctness of the output. In this work we fill the gap proposing few alternatives to compute the confidence in the output of non-probabilistic algorithms.We show how to compute confidence estimates in the prediction such that the confidence reflects the probability that the word is labeled correctly. We then show how to use our methods to detect mislabeled words, trade recall for precision and active learning. We evaluate our methods on four noun-phrase chunking and named entity recognition sequence labeling tasks, and on dependency parsing for 14 languages.",0
"This paper presents a new method for estimating confidence in structured prediction tasks, such as natural language processing. Our approach uses multiple techniques to generate confidence estimates, including probabilistic models and machine learning algorithms. We demonstrate how our system can improve performance on several benchmark datasets by providing more accurate predictions than previous methods. Additionally, we provide qualitative analysis showing that our algorithm generates confident but conservative predictions that effectively capture uncertainty in the data. Overall, our results show that our method provides a significant improvement over existing approaches for confidence estimation in structured prediction tasks.",1
"The most basic assumption used in statistical learning theory is that training data and test data are drawn from the same underlying distribution. Unfortunately, in many applications, the ""in-domain"" test data is drawn from a distribution that is related, but not identical, to the ""out-of-domain"" distribution of the training data. We consider the common case in which labeled out-of-domain data is plentiful, but labeled in-domain data is scarce. We introduce a statistical formulation of this problem in terms of a simple mixture model and present an instantiation of this framework to maximum entropy classifiers and their linear chain counterparts. We present efficient inference algorithms for this special case based on the technique of conditional expectation maximization. Our experimental results show that our approach leads to improved performance on three real world tasks on four different data sets from the natural language processing domain.",0
"Domain adaptation refers to the process of adapting a statistical classifier trained on one domain to perform well on another related but different domain. This problem arises in many applications such as image classification where the training data is collected from one distribution and the test data comes from a new distribution that may differ significantly in terms of lighting conditions, object appearances, etc. In this paper, we propose several methods to address this challenge by aligning the feature representations learned by the source model and the target model so that they can generalize better to the new unseen images. Our approach combines techniques from transfer learning and adversarial training to effectively learn invariant features across domains. We evaluate our method on multiple benchmark datasets and demonstrate state-of-the-art performance in comparison to existing approaches. Our results show that domain adaptation is possible even for models trained without access to labeled target data, making them more widely applicable in practice. Overall, this work provides insights into how to design effective machine learning systems that can generalize well across diverse data distributions.",1
"We first present our work in machine translation, during which we used aligned sentences to train a neural network to embed n-grams of different languages into an $d$-dimensional space, such that n-grams that are the translation of each other are close with respect to some metric. Good n-grams to n-grams translation results were achieved, but full sentences translation is still problematic. We realized that learning semantics of sentences and documents was the key for solving a lot of natural language processing problems, and thus moved to the second part of our work: sentence compression. We introduce a flexible neural network architecture for learning embeddings of words and sentences that extract their semantics, propose an efficient implementation in the Torch framework and present embedding results comparable to the ones obtained with classical neural language models, while being more powerful.",0
"In recent years, there has been significant interest in developing methods that can analyze text data without relying on traditional hand-engineered features such as bag of words representations. One popular approach to accomplishing this task is through the use of so-called ""semantic vector machines."" These models represent words, documents, and other types of input data using high-dimensional vectors of numbers that capture some inherent meaning or structure within the language. They have proven to be highly effective at tasks like sentiment analysis, machine translation, and summarization, among others. This paper describes a novel method for training semantic vector machines based on a carefully constructed dictionary of terms. By incorporating prior knowledge about the relationships between words and their meanings into the model, we are able to achieve improved performance over state-of-the-art baselines on several challenging benchmark datasets. We conclude by discussing potential applications of our approach and outlining future research directions in this area.",1
"We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.",0
"This paper describes natural language processing techniques that can be used to build powerful systems without relying on large pretrained models or massive amounts of training data. We describe methods for identifying named entities, parsing sentences, generating summaries, machine translation, and more using simple algorithms that don’t require deep learning or neural networks. Our approach leverages decades of research in computational linguistics while avoiding complex frameworks and libraries, making it accessible to anyone with a basic understanding of programming concepts. By walking through example implementations of these techniques, we aim to empower readers to apply them in their own projects without needing extensive background knowledge. Additionally, we discuss tradeoffs involved in different design choices and provide tips for working effectively within constraints. Overall, our goal is to demystify natural language processing and make fundamental NLP capabilities available to those who might otherwise struggle to access them due to resource limitations or lack of specialized expertise. Abstract: Natural language processing has been revolutionized by advancements in deep learning and artificial intelligence over recent years. However, many approaches still rely heavily on large amounts of pre-existing knowledge or resources such as vast databases or advanced computer hardware. In contrast, this paper presents an alternative methodology based on foundational principles of linguistics and computation, providing a framework for constructing effective natural language applications with minimal requirements. The study focuses specifically on building systems that identify named entities within text, parse and analyze sentence structure, generate concise summaries, and even facilitate language translations. Throughout each step, attention is given to simplicity versus comprehensiveness, highlighting both benefits and shortcomings inherent in any technique. Examples demonstrate how to implement these practices into actual programs designed for utilization in various fields. Ultimately, this work seeks to broaden the potential for individuals to harness natural language processing abilities while mitigating reliance on advanced technical proficiencies or substantial data sets.",1
"Often we wish to predict a large number of variables that depend on each other as well as on other observed variables. Structured prediction methods are essentially a combination of classification and graphical modeling, combining the ability of graphical models to compactly model multivariate data with the ability of classification methods to perform prediction using large sets of input features. This tutorial describes conditional random fields, a popular probabilistic method for structured prediction. CRFs have seen wide application in natural language processing, computer vision, and bioinformatics. We describe methods for inference and parameter estimation for CRFs, including practical issues for implementing large scale CRFs. We do not assume previous knowledge of graphical modeling, so this tutorial is intended to be useful to practitioners in a wide variety of fields.",0
"In recent years, conditional random fields (CRFs) have gained increasing attention as a powerful tool for modeling complex dependencies in data. CRFs provide a flexible framework for modeling interactions between pairs of variables that can capture both pairwise relationships and higher order interactions, making them well suited for tasks such as sequence labeling, structured prediction, and natural language processing. This work presents an introduction to CRFs, providing a comprehensive overview of their underlying principles, advantages, and limitations. We begin by discussing the key concepts involved in defining and implementing CRF models, including features, potential functions, inference algorithms, and learning techniques. Next, we explore several applications of CRFs across different domains, including bioinformatics, computer vision, speech recognition, and text analysis, demonstrating the broad applicability of these models. Finally, we offer concluding remarks on future research directions and open challenges in the field.",1
"We present Searn, an algorithm for integrating search and learning to solve complex structured prediction problems such as those that occur in natural language, speech, computational biology, and vision. Searn is a meta-algorithm that transforms these complex problems into simple classification problems to which any binary classifier may be applied. Unlike current algorithms for structured learning that require decomposition of both the loss function and the feature functions over the predicted structure, Searn is able to learn prediction functions for any loss function and any class of features. Moreover, Searn comes with a strong, natural theoretical guarantee: good performance on the derived classification problems implies good performance on the structured prediction problem.",0
"In recent years, there has been significant interest in developing machine learning models that can tackle complex prediction tasks involving structured output formats such as sequences, trees, graphs, etc. These tasks often involve predicting multiple outputs simultaneously, making them challenging to address using traditional learning techniques. To overcome these limitations, search-based methods have emerged as promising approaches to solving structured prediction problems.  Search-based structured prediction involves applying search algorithms to explore the space of possible solutions and find the most accurate one. By modeling the problem as a search over a space of candidate solutions rather than just optimizing parameters directly, these methods can discover novel predictions that may not have been found otherwise. They have proven successful at improving accuracy on many real-world applications, including natural language processing, computer vision, robotics, and more.  The key challenge in designing effective search-based structured prediction algorithms lies in defining appropriate heuristics and diversification strategies to guide the exploration process towards high-quality solutions efficiently. This paper presents a comprehensive survey of recent advances in search-based structured prediction, highlighting state-of-the-art approaches from different fields, and identifying commonalities and differences among them. We discuss popular search techniques used in practice, present various criteria used for guiding the search, examine techniques used to maintain adequate levels of diversity during search, and provide examples of how these principles have led to breakthroughs in applied research areas.  Our goal is to provide both a theoretical foundation for understanding and building search-based structured prediction systems, as well as practical guidance for researchers and practitioners seeking to apply these methods effectively in their own domains. Overall, we believe that the field of search-base",1
"This paper applies machine learning techniques to student modeling. It presents a method for discovering high-level student behaviors from a very large set of low-level traces corresponding to problem-solving actions in a learning environment. Basic actions are encoded into sets of domain-dependent attribute-value patterns called cases. Then a domain-independent hierarchical clustering identifies what we call general attitudes, yielding automatic diagnosis expressed in natural language, addressed in principle to teachers. The method can be applied to individual students or to entire groups, like a class. We exhibit examples of this system applied to thousands of students' actions in the domain of algebraic transformations.",0
"This paper presents a novel approach for inducing high-level behaviors from problem-solving traces using machine learning tools. We propose a method that extracts features from problem-solving trace data and uses these features as input for a supervised learning algorithm. Our approach is able to learn complex patterns and relationships within the problem-solving process, resulting in accurate predictions of high-level behaviors such as strategies and tactics used by human experts. We evaluate our method on a dataset consisting of programming tasks and demonstrate its effectiveness through comparison with state-of-the-art methods. Our findings have important implications for researchers working on computational models of human intelligence, as well as practitioners interested in developing intelligent tutoring systems that can effectively guide novices towards expert-like performance. Overall, this work represents an important step forward in our understanding of how humans solve problems and demonstrates the potential of machine learning algorithms to model and simulate complex cognitive processes.",1
"We present basic notions of Gold's ""learnability in the limit"" paradigm, first presented in 1967, a formalization of the cognitive process by which a native speaker gets to grasp the underlying grammar of his/her own native language by being exposed to well formed sentences generated by that grammar. Then we present Lambek grammars, a formalism issued from categorial grammars which, although not as expressive as needed for a full formalization of natural languages, is particularly suited to easily implement a natural interface between syntax and semantics. In the last part of this work, we present a learnability result for Rigid Lambek grammars from structured examples.",0
"This study examines how learnability can impact the ability of rigid Lambek grammars. Previous research has suggested that the use of specific grammar formulations may influence how easily a language can be learned by both human and nonhuman learners alike. In this investigation, we aimed to explore whether and how rigidity affects these processes. To accomplish this goal, we utilized computational modeling techniques designed to simulate learning scenarios across diverse linguistic domains. Our findings provide insights into how learnability may relate to grammatical complexity, suggesting that there exists an optimal balance point where the ease of learning and expressiveness of language coexist. These results contribute important new knowledge to the broader field of language acquisition and formal language theory. Implications for future research and applications are discussed.",1
"We present an algorithm for learning from unlabeled text, based on the Vector Space Model (VSM) of information retrieval, that can solve verbal analogy questions of the kind found in the SAT college entrance exam. A verbal analogy has the form A:B::C:D, meaning ""A is to B as C is to D""; for example, mason:stone::carpenter:wood. SAT analogy questions provide a word pair, A:B, and the problem is to select the most analogous word pair, C:D, from a set of five choices. The VSM algorithm correctly answers 47% of a collection of 374 college-level analogy questions (random guessing would yield 20% correct; the average college-bound senior high school student answers about 57% correctly). We motivate this research by applying it to a difficult problem in natural language processing, determining semantic relations in noun-modifier pairs. The problem is to classify a noun-modifier pair, such as ""laser printer"", according to the semantic relation between the noun (printer) and the modifier (laser). We use a supervised nearest-neighbour algorithm that assigns a class to a given noun-modifier pair by finding the most analogous noun-modifier pair in the training data. With 30 classes of semantic relations, on a collection of 600 labeled noun-modifier pairs, the learning algorithm attains an F value of 26.5% (random guessing: 3.3%). With 5 classes of semantic relations, the F value is 43.2% (random: 20%). The performance is state-of-the-art for both verbal analogies and noun-modifier relations.",0
"Artificial intelligence (AI) learning approaches that rely on large corpora of examples have become increasingly popular over recent years due to their ability to leverage vast amounts of data to improve task performance across many different domains. In particular, techniques such as deep learning and neural networks have proven effective in addressing challenges related to natural language processing by enabling systems to learn complex representations from raw text inputs. One important aspect of human cognition involves understanding relationships and similarities among concepts; these can come in several forms including analogies and semantic relations. There is evidence suggesting that humans rely heavily upon analogy making during problem solving and comprehension activities, while the structure of our knowledge representation appears to take advantage of relational information. We propose herein a corpus-based method to enable computer models to acquire meaningful insights regarding both analogies and semantic relations within large collections of text. By analyzing co-occurrences and distributions within text documents at scale, we observe consistent patterns that support existing psychological theories and lead us to describe novel relationships between sets of concepts. These results demonstrate how AI methods may benefit from incorporating more sophisticated ways of dealing with textual input beyond simply considering isolated sentences and words; instead, it becomes possible for machine models to gain richer knowledge that enables better responses to natural language queries or generation tasks.",1
"Existing statistical approaches to natural language problems are very coarse approximations to the true complexity of language processing. As such, no single technique will be best for all problem instances. Many researchers are examining ensemble methods that combine the output of multiple modules to create more accurate solutions. This paper examines three merging rules for combining probability distributions: the familiar mixture rule, the logarithmic rule, and a novel product rule. These rules were applied with state-of-the-art results to two problems used to assess human mastery of lexical semantics -- synonym questions and analogy questions. All three merging rules result in ensembles that are more accurate than any of their component modules. The differences among the three rules are not statistically significant, but it is suggestive that the popular mixture rule is not the best rule for either of the two problems.",0
"In this work we address the problem of lexical multiple choice problems, where given a question and several potential answers, only one can actually correspond to the correct definition from some reference source. Previous approaches have been successful at extracting semantically meaningful features through pretraining on large amounts of data but they still struggle to achieve high scores on hard examples. We show that combining models trained independently on different feature types (such as BERT embeddings) improves performance over most baselines and previous state of art results. By using simple ensembling methods like averaging scores we outperform all previously published results with our implementation achieving an accuracy score of .924. We hope this approach could be used to build more efficient solutions to large language understanding tasks.",1
"We present an algorithm for learning from unlabeled text, based on the Vector Space Model (VSM) of information retrieval, that can solve verbal analogy questions of the kind found in the Scholastic Aptitude Test (SAT). A verbal analogy has the form A:B::C:D, meaning ""A is to B as C is to D""; for example, mason:stone::carpenter:wood. SAT analogy questions provide a word pair, A:B, and the problem is to select the most analogous word pair, C:D, from a set of five choices. The VSM algorithm correctly answers 47% of a collection of 374 college-level analogy questions (random guessing would yield 20% correct). We motivate this research by relating it to work in cognitive science and linguistics, and by applying it to a difficult problem in natural language processing, determining semantic relations in noun-modifier pairs. The problem is to classify a noun-modifier pair, such as ""laser printer"", according to the semantic relation between the noun (printer) and the modifier (laser). We use a supervised nearest-neighbour algorithm that assigns a class to a given noun-modifier pair by finding the most analogous noun-modifier pair in the training data. With 30 classes of semantic relations, on a collection of 600 labeled noun-modifier pairs, the learning algorithm attains an F value of 26.5% (random guessing: 3.3%). With 5 classes of semantic relations, the F value is 43.2% (random: 20%). The performance is state-of-the-art for these challenging problems.",0
"This paper proposes a novel approach to learning analogical relations between concepts by leveraging semantic similarity data. Our model uses deep neural networks to jointly learn embeddings that represent both the concepts themselves and their relationships. We evaluate our model on several benchmark datasets and show that we achieve state-of-the-art performance across all tasks while requiring significantly fewer labeled training examples than previous methods. Additionally, our method can handle arbitrary concept types such as text, images, and videos. Overall, these results demonstrate the effectiveness of using deep learning techniques to capture high-quality representations of analogical knowledge and suggest potential applications in areas such as question answering, natural language understanding, and machine translation.",1
"Although speech and gesture recognition has been studied extensively, all the successful attempts of combining them in the unified framework were semantically motivated, e.g., keyword-gesture cooccurrence. Such formulations inherited the complexity of natural language processing. This paper presents a Bayesian formulation that uses a phenomenon of gesture and speech articulation for improving accuracy of automatic recognition of continuous coverbal gestures. The prosodic features from the speech signal were coanalyzed with the visual signal to learn the prior probability of co-occurrence of the prominent spoken segments with the particular kinematical phases of gestures. It was found that the above co-analysis helps in detecting and disambiguating visually small gestures, which subsequently improves the rate of continuous gesture recognition. The efficacy of the proposed approach was demonstrated on a large database collected from the weather channel broadcast. This formulation opens new avenues for bottom-up frameworks of multimodal integration.",0
This should be a comprehensive summary of the paper that can stand alone as free text. Use full sentences without citation format such as [4] like you would see in actual journal articles rather than research papers/thesis. |,1
"This thesis introduces a new unsupervised learning framework, called Alignment-Based Learning, which is based on the alignment of sentences and Harris's (1951) notion of substitutability. Instances of the framework can be applied to an untagged, unstructured corpus of natural language sentences, resulting in a labelled, bracketed version of that corpus.   Firstly, the framework aligns all sentences in the corpus in pairs, resulting in a partition of the sentences consisting of parts of the sentences that are equal in both sentences and parts that are unequal. Unequal parts of sentences can be seen as being substitutable for each other, since substituting one unequal part for the other results in another valid sentence. The unequal parts of the sentences are thus considered to be possible (possibly overlapping) constituents, called hypotheses.   Secondly, the selection learning phase considers all hypotheses found by the alignment learning phase and selects the best of these. The hypotheses are selected based on the order in which they were found, or based on a probabilistic function.   The framework can be extended with a grammar extraction phase. This extended framework is called parseABL. Instead of returning a structured version of the unstructured input corpus, like the ABL system, this system also returns a stochastic context-free or tree substitution grammar.   Different instances of the framework have been tested on the English ATIS corpus, the Dutch OVIS corpus and the Wall Street Journal corpus. One of the interesting results, apart from the encouraging numerical results, is that all instances can (and do) learn recursive structures.",0
"This paper describes a method called bootstrapping structure into language (BSL). BSL aims to teach machines to learn and communicate like humans by providing them with minimal guidance at first but increasingly more as they make progress through trial and error. In doing so, BSL creates systems that can interact with human users through natural language while also aligning their internal representations of concepts with those held by humans. As such, these artificial intelligence systems have been found effective in tasks requiring understanding of complex structures and concepts, particularly in fields where data and examples may be limited, including programming languages, social media analytics, and robotic assembly lines. By combining machine learning techniques with domain expertise and human judgment, BSL offers a promising approach for scaling up AI applications to real-world settings. Ultimately, BSL could provide new insights into how humans acquire knowledge and might even inform novel ways of teaching children.",1
"This paper introduces a new type of unsupervised learning algorithm, based on the alignment of sentences and Harris's (1951) notion of interchangeability. The algorithm is applied to an untagged, unstructured corpus of natural language sentences, resulting in a labelled, bracketed version of the corpus. Firstly, the algorithm aligns all sentences in the corpus in pairs, resulting in a partition of the sentences consisting of parts of the sentences that are similar in both sentences and parts that are dissimilar. This information is used to find (possibly overlapping) constituents. Next, the algorithm selects (non-overlapping) constituents. Several instances of the algorithm are applied to the ATIS corpus (Marcus et al., 1993) and the OVIS (Openbaar Vervoer Informatie Systeem (OVIS) stands for Public Transport Information System.) corpus (Bonnema et al., 1997). Apart from the promising numerical results, the most striking result is that even the simplest algorithm based on alignment learns recursion.",0
"In natural language processing (NLP), parsing is a crucial step that involves analyzing a sentence into its constituents such as noun phrases, verb phrases, and prepositional phrases. Traditionally, NLP parsers use manual feature engineering based on linguistic insights, which can be laborious and error-prone. To overcome these challenges, we propose bootstrapping syntax and recursion by employing alignment-based learning methods without requiring explicit feature engineering. Our approach uses a small set of annotated data from diverse languages and automatically learns parse structures through alignments with gold standards. We evaluate our model on several benchmark datasets across different domains and achieve comparable results to state-of-the-art models trained on large amounts of manually engineered features. Additionally, our method generalizes well across unseen languages, demonstrating its effectiveness in zero-shot cross-linguistic transfer. This work contributes to advancing NLP research by establishing a novel paradigm forbootstrapped parsing through alignment learning, paving the way for efficient and scalable solutions in low-resource settings.",1
"With the huge amount of information available online, the World Wide Web is a fertile area for data mining research. The Web mining research is at the cross road of research from several research communities, such as database, information retrieval, and within AI, especially the sub-areas of machine learning and natural language processing. However, there is a lot of confusions when comparing research efforts from different point of views. In this paper, we survey the research in the area of Web mining, point out some confusions regarded the usage of the term Web mining and suggest three Web mining categories. Then we situate some of the research with respect to these three categories. We also explore the connection between the Web mining categories and the related agent paradigm. For the survey, we focus on representation issues, on the process, on the learning algorithm, and on the application of the recent works as the criteria. We conclude the paper with some research issues.",0
"This paper presents a survey on web mining research which has been conducted over the last two decades. Web mining involves extracting valuable patterns from web data such as text, images, videos, audio, etc., through different techniques including machine learning, natural language processing (NLP), databases, artificial intelligence (AI) and more recently deep learning algorithms. Web mining can be divided into several subareas based on their application domains, tasks, and types of data mined, e.g., Web usage mining, Web content mining, Social network analysis, Sentiment analysis. Various approaches have been proposed to address challenges specific to each subarea such as scalability, privacy preservation, noise filtering, evaluation metrics, domain knowledge integration among others. In summary, we provide insights into the main advancements made within web mining along with open issues that need further investigation. Our aim is to assist other researchers interested in exploring web mining by providing them with literature references categorized per area, challenges, approach type, targeted platform/toolkit as well as applications across industries like healthcare, finance, marketing, entertainment and many others.",1
"A large class of machine-learning problems in natural language require the characterization of linguistic context. Two characteristic properties of such problems are that their feature space is of very high dimensionality, and their target concepts refer to only a small subset of the features in the space. Under such conditions, multiplicative weight-update algorithms such as Winnow have been shown to have exceptionally good theoretical properties. We present an algorithm combining variants of Winnow and weighted-majority voting, and apply it to a problem in the aforementioned class: context-sensitive spelling correction. This is the task of fixing spelling errors that happen to result in valid words, such as substituting ""to"" for ""too"", ""casual"" for ""causal"", etc. We evaluate our algorithm, WinSpell, by comparing it against BaySpell, a statistics-based method representing the state of the art for this task. We find: (1) When run with a full (unpruned) set of features, WinSpell achieves accuracies significantly higher than BaySpell was able to achieve in either the pruned or unpruned condition; (2) When compared with other systems in the literature, WinSpell exhibits the highest performance; (3) The primary reason that WinSpell outperforms BaySpell is that WinSpell learns a better linear separator; (4) When run on a test set drawn from a different corpus than the training set was drawn from, WinSpell is better able than BaySpell to adapt, using a strategy we will present that combines supervised learning on the training set with unsupervised learning on the (noisy) test set.",0
"This paper presents a new approach to context-sensitive spelling correction that utilizes natural language processing techniques to identify potential errors and suggest corrections based on their linguistic context. Our method uses a combination of n-gram analysis and machine learning algorithms to model correct spellings as functions of nearby words, allowing us to make more accurate predictions than traditional methods. We evaluate our system using several benchmark datasets and demonstrate that it outperforms state-of-the-art systems by significant margins across all metrics. Additionally, we show how our approach can be easily integrated into existing proofreading tools to enhance their performance. Overall, our work represents a major step forward in automatic spelling correction technology and has important implications for the field of Natural Language Processing (NLP).",1
"This paper discusses a data-driven, empirically-based framework to make algorithmic decisions or recommendations without expert knowledge. We improve the performance of two algorithmic case studies: the selection of a pivot rule for the Simplex method and the selection of an all-pair shortest paths algorithm. We train machine learning methods to select the optimal algorithm for given data without human expert opinion. We use two types of techniques, neural networks and boosted decision trees. We concluded, based on our experiments, that:   1) Our selection framework recommends various pivot rules that improve overall total performance over just using a fixed default pivot rule.   Over many years experts identified steepest-edge pivot rule as a favorite pivot rule. Our data analysis corroborates that the number of iterations by steepest-edge is no more than 4 percent more than the optimal selection which corroborates human expert knowledge, but this time the knowledge was obtained using machine learning. Here our recommendation system is best when using gradient boosted trees.   2) For the all-pairs shortest path problem, the models trained made a large improvement and our selection is on average .07 percent away from the optimal choice. The conclusions do not seem to be affected by the machine learning method we used.   We tried to make a parallel analysis of both algorithmic problems, but it is clear that there are intrinsic differences. For example, in the all-pairs shortest path problem the graph density is a reasonable predictor, but there is no analogous single parameter for decisions in the Simplex method.",0
"Machine learning has proven itself as a powerful tool that can improve the performance of algorithms beyond their original design capabilities. Recent work has shown that machine learning can be used to learn heuristics for discrete algorithms such as satisfiability modulo theories (SMT), constraint solving, and planning. In particular, machine learning has been applied successfully to speed up these solvers by improving both their search strategies and variable ordering. These approaches have led to significant improvements over traditional handcrafted heuristics and have even achieved state-of-the-art results on many benchmarks.  The main goal of our research is to investigate how machine learning can be effectively integrated into discrete algorithms, particularly those that involve complex search processes, to significantly enhance empirical performance. We present several methods based on machine learning techniques for constructing domain-specific representations for problem variables and learning effective search strategies that directly map to the given algorithm. Our approach uses structured prediction techniques combined with gradient boosting to learn models of local problem structure which guide the choice of atom instantiation. Furthermore, we introduce a meta-learning framework for learning heuristic selectors from multiple predictive regression models that generalize across domains and problems.  We conduct experiments using SAT and CSP solvers, comparing our proposed machine learning approaches against various baseline methods, including different search strategies, heuristics, and learned modeling functions. Results show consistent improvement in most cases compared to standard discrete solvers, demonstrating the effectiveness of combining machine learning with discrete optimization algorithms. This opens new opportunities to apply data-driven machine learning techniques in other areas of computer science where complex combinatorial search is required. Overall, our research provides insight into how machine learning can efficiently guide the search process i",1
"Prediction rule ensembles (PRE) provide interpretable prediction models with relatively high accuracy.PRE obtain a large set of decision rules from a (boosted) decision tree ensemble, and achieves sparsitythrough application of Lasso-penalized regression. This article examines the use of surrogate modelsto improve performance of PRE, wherein the Lasso regression is trained with the help of a massivedataset generated by the (boosted) decision tree ensemble. This use of model-based data generationmay improve the stability and consistency of the Lasso step, thus leading to improved overallperformance. We propose two surrogacy approaches, and evaluate them on simulated and existingdatasets, in terms of sparsity and predictive accuracy. The results indicate that the use of surrogacymodels can substantially improve the sparsity of PRE, while retaining predictive accuracy, especiallythrough the use of a nested surrogacy approach.",0
"In recent years, there has been growing interest in using machine learning algorithms to develop clinical decision support tools that can accurately predict patient outcomes and inform treatment decisions. One approach to building such models involves combining multiple existing prediction rules into a single ensemble model that leverages their collective strengths while minimizing their individual weaknesses. However, building high-quality ensemble models requires large amounts of high-quality training data, which may not always be readily available. This paper presents a novel methodology for generating synthetic training data by integrating statistical models of disease progression with real-world patient outcome data. Our results demonstrate that our proposed method improves the accuracy and robustness of resulting ensemble predictions across a variety of metrics compared to traditional approaches. The work represents an important step towards developing more accurate and reliable clinical decision support systems that could ultimately improve healthcare outcomes.",1
"Deep neural networks are able to learn multi-layered representation via back propagation (BP). Although the gradient boosting decision tree (GBDT) is effective for modeling tabular data, it is non-differentiable with respect to its input, thus suffering from learning multi-layered representation. In this paper, we propose a framework of learning multi-layered GBDT via BP. We approximate the gradient of GBDT based on linear regression. Specifically, we use linear regression to replace the constant value at each leaf ignoring the contribution of individual samples to the tree structure. In this way, we estimate the gradient for intermediate representations, which facilitates BP for multi-layered GBDT. Experiments show the effectiveness of the proposed method in terms of performance and representation ability. To the best of our knowledge, this is the first work of optimizing multi-layered GBDT via BP. This work provides a new possibility of exploring deep tree based learning and combining GBDT with neural networks.",0
"Abstract: In this paper we present a new method for training Gradient Boosting Decision Trees (GBDT) called ""Learning Multi-Layered GBDT via Backpropagation"" (LMG). Our approach involves splitting each tree in the forest into multiple sub-trees that represent different layers of abstraction in the data. These sub-trees can then be trained independently using backpropagation, which allows us to efficiently optimize each layer separately while still ensuring that the overall ensemble achieves high performance on the target task. We demonstrate the effectiveness of our method through extensive experiments on several benchmark datasets, showing that LMG outperforms state-of-the-art baselines across a variety of tasks, including both regression and classification problems. Additionally, we provide analysis and ablation studies to further investigate the strengths and weaknesses of our technique compared to existing approaches in the field. Overall, our results suggest that LMG represents a significant step forward in the development of powerful machine learning algorithms for handling complex real-world problems.",1
"We investigate the fairness concerns of training a machine learning model using data with missing values. Even though there are a number of fairness intervention methods in the literature, most of them require a complete training set as input. In practice, data can have missing values, and data missing patterns can depend on group attributes (e.g. gender or race). Simply applying off-the-shelf fair learning algorithms to an imputed dataset may lead to an unfair model. In this paper, we first theoretically analyze different sources of discrimination risks when training with an imputed dataset. Then, we propose an integrated approach based on decision trees that does not require a separate process of imputation and learning. Instead, we train a tree with missing incorporated as attribute (MIA), which does not require explicit imputation, and we optimize a fairness-regularized objective function. We demonstrate that our approach outperforms existing fairness intervention methods applied to an imputed dataset, through several experiments on real-world datasets.",0
"This paper presents a decision tree approach that can accurately predict class labels while accounting for missing values in feature inputs. Our method, called ""Fairness without Imputation,"" leverages data imputations techniques such as mean, median, and mode imputation to improve prediction accuracy without relying on assumptions about the underlying distribution of the missing values. We evaluate our approach using both real-world datasets and simulated data experiments, demonstrating consistent improvements over traditional decision trees trained with complete features. Additionally, we showcase how our model outperforms other state-of-the-art imputation methods, making it an attractive choice for practitioners dealing with incomplete datasets. Overall, Fairness without Imputation provides a simple yet effective solution for ensuring fair predictions in machine learning tasks under incomplete settings.",1
"Prompt severity assessment model of confirmed patients who were infected with infectious diseases could enable efficient diagnosis and alleviate the burden on the medical system. This paper provides the development processes of the severity assessment model using machine learning techniques and its application on SARS-CoV-2 patients. Here, we highlight that our model only requires basic patients' basic personal data, allowing for them to judge their own severity. We selected the boosting-based decision tree model as a classifier and interpreted mortality as a probability score after modeling. Specifically, hyperparameters that determine the structure of the tree model were tuned using the Bayesian optimization technique without any knowledge of medical information. As a result, we measured model performance and identified the variables affecting the severity through the model. Finally, we aim to establish a medical system that allows patients to check their own severity and informs them to visit the appropriate clinic center based on the past treatment details of other patients with similar severity.",0
"In this study, we developed a patient triage algorithm using machine learning techniques on data collected from a nationwide COVID-19 registry. The aim was to create an automated system that can quickly identify high-risk patients and prioritize them for medical attention.  The dataset used consisted of demographic and clinical information of over one million patients infected with COVID-19 across multiple hospitals in a country. We applied feature engineering techniques to extract relevant features such as age, gender, comorbidities, and laboratory values. Next, we employed different supervised learning algorithms including decision trees, random forest, support vector machines, and neural networks to build predictive models.  Our results showed that all four machine learning methods achieved high accuracy (> 85%) in predicting severe outcomes among COVID-19 cases. We observed better performance from neural network model compared to other methods, achieving an area under the receiver operating characteristic curve (AUC) score of 0.92. Moreover, our proposed triage algorithm effectively stratified patients into low, medium, and high risk categories, which could potentially aid healthcare providers in allocating limited resources more efficiently during crisis situations.  In conclusion, our findings demonstrated the feasibility of developing an efficient patient triage algorithm utilizing large-scale registry data. This approach may provide valuable insights for public health officials and policymakers seeking strategies to mitigate healthcare disparities and improve pandemic response efforts worldwide. Further research could explore incorporation of real-time data sources such as wearables, social determinants, and geospatial factors to enhance the performance of patient triage systems.",1
"Symbolic learning represents the most straightforward approach to interpretable modeling, but its applications have been hampered by a single structural design choice: the adoption of propositional logic as the underlying language. Recently, more-than-propositional symbolic learning methods have started to appear, in particular for time-dependent data. These methods exploit the expressive power of modal temporal logics in powerful learning algorithms, such as temporal decision trees, whose classification capabilities are comparable with the best non-symbolic ones, while producing models with explicit knowledge representation.   With the intent of following the same approach in the case of spatial data, in this paper we: i) present a theory of spatial decision tree learning; ii) describe a prototypical implementation of a spatial decision tree learning algorithm based, and strictly extending, the classical C4.5 algorithm; and iii) perform a series of experiments in which we compare the predicting power of spatial decision trees with that of classical propositional decision trees in several versions, for a multi-class image classification problem, on publicly available datasets. Our results are encouraging, showing clear improvements in the performances from the propositional to the spatial models, which in turn show higher levels of interpretability.",0
"""Decision tree learning is a popular method used to analyze data and make predictions based on that analysis. However, traditional decision tree models can sometimes struggle with handling uncertainty and imprecise knowledge. In this paper, we propose using spatial modal logics as a tool for improving the accuracy of decision trees in uncertain environments. We show how these logical systems can be integrated into decision tree algorithms to better account for unknown values and inconsistent observations. Our approach builds upon previous work in this area while adding new features and capabilities. Through experimental evaluation, we demonstrate the effectiveness of our method in comparison to existing approaches. This research has important implications for many fields where decision making relies on uncertain data.""",1
"Time series models with recurrent neural networks (RNNs) can have high accuracy but are unfortunately difficult to interpret as a result of feature-interactions, temporal-interactions, and non-linear transformations. Interpretability is important in domains like healthcare where constructing models that provide insight into the relationships they have learned are required to validate and trust model predictions. We want accurate time series models where users can understand the contribution of individual input features. We present the Interpretable-RNN (I-RNN) that balances model complexity and accuracy by forcing the relationship between variables in the model to be additive. Interactions are restricted between hidden states of the RNN and additively combined at the final step. I-RNN specifically captures the unique characteristics of clinical time series, which are unevenly sampled in time, asynchronously acquired, and have missing data. Importantly, the hidden state activations represent feature coefficients that correlate with the prediction target and can be visualized as risk curves that capture the global relationship between individual input features and the outcome. We evaluate the I-RNN model on the Physionet 2012 Challenge dataset to predict in-hospital mortality, and on a real-world clinical decision support task: predicting hemodynamic interventions in the intensive care unit. I-RNN provides explanations in the form of global and local feature importances comparable to highly intelligible models like decision trees trained on hand-engineered features while significantly outperforming them. I-RNN remains intelligible while providing accuracy comparable to state-of-the-art decay-based and interpolation-based recurrent time series models. The experimental results on real-world clinical datasets refute the myth that there is a tradeoff between accuracy and interpretability.",0
"This paper presents a novel approach for interpreting additive recurrent neural networks (RNNs) used for modeling multivariate clinical time series data. We focus on RNNs because they have proven effective at capturing complex temporal relationships present in such datasets. However, interpreting the learned representations remains challenging due to their deep architectures and nonlinear transformations. To address these issues, we propose a method that leverages attention mechanisms to selectively weight the contribution of each input variable in explaining the dynamics of the outputs. Our approach allows us to identify important features driving the predictions and provides insights into the underlying physiological processes governing the system behavior. Experiments conducted on two publicly available datasets demonstrate the effectiveness of our proposed method in identifying meaningful patterns from complex multivariate signals. Overall, our work advances the interpretability of blackbox models commonly used in healthcare applications, paving the way towards more reliable decision support systems in critical care settings.",1
"Businesses, governmental bodies and NGO's have an ever-increasing amount of data at their disposal from which they try to extract valuable information. Often, this needs to be done not only accurately but also within a short time frame. Clean and consistent data is therefore crucial. Data matching is the field that tries to identify instances in data that refer to the same real-world entity. In this study, machine learning techniques are combined with string similarity functions to the field of data matching. A dataset of invoices from a variety of businesses and organizations was preprocessed with a grouping scheme to reduce pair dimensionality and a set of similarity functions was used to quantify similarity between invoice pairs. The resulting invoice pair dataset was then used to train and validate a neural network and a boosted decision tree. The performance was compared with a solution from FISCAL Technologies as a benchmark against currently available deduplication solutions. Both the neural network and boosted decision tree showed equal to better performance.",0
"Machine Learning is becoming increasingly popular as a methodology for solving complex problems across many domains, including Data Matching. In traditional methods of Data Mining and Information Retrieval, one often has to rely on explicit knowledge sources like dictionaries or thesauri to find semantically similar documents. But such approaches fail when applied in real world scenarios where similarity measures have to capture many different types of relationships. Supervised Machine Learning allows us to extract features from raw signals that capture all these nuances and learn similarity metrics directly from labeled examples of similarity judgements. This leads to better performance than more heuristics driven approaches which try to infer semantic meaning from signal without explicitly capturing nuanced differences. We provide an overview of recent work applying supervised machine learning algorithms (like Neural Networks) for feature extraction along with kernel machines (e.g., Support Vector Machines) and graph partitioning schemes to solve data matching tasks using labelled pairs of matched instances. Our approach combines multiple similarity functions into ensembles which perform well when compared against simple baseline classifiers, even without incorporating semantic knowledge sources, but we achieve state-of-the art results by combining such a function ensemble with external knowledge resources. Our study compares these two approaches experimentally by evaluating them on benchmark datasets of varying sizes and degrees of heterogeneity. Both our approaches prove effective irrespective of whether they use large or small training sets; indeed, one can argue that our framework is particularly suitable if only few labelled pair comparisons are available since it then heavily relies on external knowledge to guide the inference process to overcome shortcomings related to small sample size of the comparison dataset. We believe that future progress in developing effective data mining applications will benefit strongly from integrating various kinds of prior knowledge about application domains to enable accurate decision making supported by high quality machine learning components.",1
"Heart failure (HF) is a leading cause of morbidity, mortality, and health care costs. Prolonged conduction through the myocardium can occur with HF, and a device-driven approach, termed cardiac resynchronization therapy (CRT), can improve left ventricular (LV) myocardial conduction patterns. While a functional benefit of CRT has been demonstrated, a large proportion of HF patients (30-50%) receiving CRT do not show sufficient improvement. Moreover, identifying HF patients that would benefit from CRT prospectively remains a clinical challenge. Accordingly, strategies to effectively predict those HF patients that would derive a functional benefit from CRT holds great medical and socio-economic importance. Thus, we used machine learning methods of classifying HF patients, namely Cluster Analysis, Decision Trees, and Artificial neural networks, to develop predictive models of individual outcomes following CRT. Clinical, functional, and biomarker data were collected in HF patients before and following CRT. A prospective 6-month endpoint of a reduction in LV volume was defined as a CRT response. Using this approach (418 responders, 412 non-responders), each with 56 parameters, we could classify HF patients based on their response to CRT with more than 95% success. We have demonstrated that using machine learning approaches can identify HF patients with a high probability of a positive CRT response (95% accuracy), and of equal importance, identify those HF patients that would not derive a functional benefit from CRT. Developing this approach into a clinical algorithm to assist in clinical decision-making regarding the use of CRT in HF patients would potentially improve outcomes and reduce health care costs.",0
"Title: ""Early Recommendations for Cardiac Resynchronization Therapy using Machine Learning""  Cardiovascular disease (CVD) is one of the leading causes of death worldwide, and cardiac resynchronization therapy (CRT) has been shown to improve outcomes for patients suffering from heart failure. However, CRT is underutilized due to complex patient selection criteria and variability in clinician decision making. In order to address these issues, we aimed to develop a machine learning model that can accurately predict which heart failure patients would benefit most from early recommendations for CRT.  We utilized a large dataset consisting of demographic, clinical, and laboratory data from over 26,000 patients diagnosed with heart failure at two tertiary care centers between January 2004 and December 2018. Our machine learning model was trained on features such as age, gender, comorbidities, ejection fraction, blood pressure, QRS duration, and left bundle branch block (LBBB). We then evaluated our model by testing it on a separate validation set.  Our results showed high accuracy and robustness for our machine learning algorithm, achieving an area under the receiver operating characteristic curve (AUC-ROC) value greater than 0.9. Furthermore, our model demonstrated good calibration overall and across subgroups, indicating that our predictions were consistent with actual outcomes. Using sensitivity analysis, we identified several key variables that contributed significantly to our model performance. These findings suggest that our machine learning approach could potentially enhance the early recommendation process for CRT in eligible heart failure patients.",1
"Gradient Boosting Machines (GBM) are among the go-to algorithms on tabular data, which produce state of the art results in many prediction tasks. Despite its popularity, the GBM framework suffers from a fundamental flaw in its base learners. Specifically, most implementations utilize decision trees that are typically biased towards categorical variables with large cardinalities. The effect of this bias was extensively studied over the years, mostly in terms of predictive performance. In this work, we extend the scope and study the effect of biased base learners on GBM feature importance (FI) measures. We show that although these implementation demonstrate highly competitive predictive performance, they still, surprisingly, suffer from bias in FI. By utilizing cross-validated (CV) unbiased base learners, we fix this flaw at a relatively low computational cost. We demonstrate the suggested framework in a variety of synthetic and real-world setups, showing a significant improvement in all GBM FI measures while maintaining relatively the same level of prediction accuracy.",0
"This abstract focuses on studying feature importance measurements used in gradient boosting trees. One popular methodology involves calculating variable importances using permutation feature selection via cross-validation. However, previous research has shown that this approach may lead to less accurate estimations. Therefore, this study aimed to explore alternative techniques for measuring feature significance in order to obtain more reliable results. Three different methods were evaluated: mean decrease impurity (MDI), random forest feature importances, and Shapley additive ex post facto explanation values. Empirical experiments demonstrated significant differences between these methods, indicating that MDI was superior compared to random forest feature importances and Shapley values. Our findings suggest that careful consideration must be given when choosing the appropriate technique since each one provides distinct insights into model performance. Furthermore, we provide recommendations on which methods should be utilized based on specific use cases such as tree size management or interpretability considerations. In conclusion, our work enhances the understanding of feature importance measurement in gradient boosting trees and contributes towards improving their accuracy and reliability.",1
"In practical situations, the ensemble tree model is one of the most popular models along with neural networks. A soft tree is one of the variants of a decision tree. Instead of using a greedy method for searching splitting rules, the soft tree is trained using a gradient method in which the whole splitting operation is formulated in a differentiable form. Although ensembles of such soft trees have been increasingly used in recent years, little theoretical work has been done for understanding their behavior. In this paper, by considering an ensemble of infinite soft trees, we introduce and study the Tree Neural Tangent Kernel (TNTK), which provides new insights into the behavior of the infinite ensemble of soft trees. Using the TNTK, we succeed in theoretically finding several non-trivial properties, such as the effect of the oblivious tree structure and the degeneracy of the TNTK induced by the deepening of the trees. Moreover, we empirically examine the performance of an ensemble of infinite soft trees using the TNTK.",0
"This paper presents a novel perspective on infinite tree ensembles using neural tangent kernels (NTKs). Traditionally, infinite tree ensembles have been analyzed through mathematical techniques such as stability theory, but these approaches can be limited in their ability to capture the nuances of how these models function in practice. By contrast, NTKs provide a powerful framework for understanding deep learning algorithms, allowing us to analyze them from a more intuitive and accessible perspective. We show that the NTK of an infinite tree ensemble exhibits interesting properties that shed light on the behavior of these models, including smoothness, locality, and equivariance. Our analysis demonstrates that NTKs provide a valuable tool for studying infinite tree ensembles, opening up new opportunities for theoretical exploration and practical application.",1
"Decision trees are a popular choice of explainable model, but just like neural networks, they suffer from adversarial examples. Existing algorithms for fitting decision trees robust against adversarial examples are greedy heuristics and lack approximation guarantees. In this paper we propose ROCT, a collection of methods to train decision trees that are optimally robust against user-specified attack models. We show that the min-max optimization problem that arises in adversarial learning can be solved using a single minimization formulation for decision trees with 0-1 loss. We propose such formulations in Mixed-Integer Linear Programming and Maximum Satisfiability, which widely available solvers can optimize. We also present a method that determines the upper bound on adversarial accuracy for any model using bipartite matching. Our experimental results demonstrate that the existing heuristics achieve close to optimal scores while ROCT achieves state-of-the-art scores.",0
"In recent years, deep learning models have achieved state-of-the-art performance on many tasks such as image classification and object detection. However, these models can often be vulnerable to adversarial examples, which are inputs designed to fool the model into making incorrect predictions. One approach to mitigating the effects of adversarial attacks on neural networks is through use of optimal decision trees. Decision trees can provide more interpretable results than complex black box models like deep neural nets, but traditional tree induction methods suffer from overfitting issues that reduce their effectiveness against adversaries. This work presents a new method called ROTATE (Robust Optimal TreEs Against TErminal Ensembles) that addresses these issues by utilizing optimal prediction paths found via Monte Carlo sampling based hill climbing search procedures. Our experimental evaluation shows that our solution achieves better robustness against strong iterative adversarial attacks compared to alternative whitebox defenses commonly used today while also providing competitive accuracy against clean test data. As such, we believe ROTATE provides a powerful tool for defending machine learning systems from adversarial attacks without sacrificing too heavily in terms of accuracy.",1
"Gradient Boosted Decision Trees (GBDTs) are widely used for building ranking and relevance models in search and recommendation. Considerations such as latency and interpretability dictate the use of as few features as possible to train these models. Feature selection in GBDT models typically involves heuristically ranking the features by importance and selecting the top few, or by performing a full backward feature elimination routine. On-the-fly feature selection methods proposed previously scale suboptimally with the number of features, which can be daunting in high dimensional settings. We develop a scalable forward feature selection variant for GBDT, via a novel group testing procedure that works well in high dimensions, and enjoys favorable theoretical performance and computational guarantees. We show via extensive experiments on both public and proprietary datasets that the proposed method offers significant speedups in training time, while being as competitive as existing GBDT methods in terms of model performance metrics. We also extend the method to the multitask setting, allowing the practitioner to select common features across tasks, as well as selecting task-specific features.",0
"This research presents a new approach to scalable feature selection for gradient boosted trees, allowing for efficient processing of large datasets by reducing computational complexity without sacrificing accuracy. The proposed method leverages multitask learning techniques to identify the most important features across tasks, enabling accurate prediction even when dealing with high-dimensional data. Experiments on several benchmark datasets demonstrate that our method outperforms state-of-the-art methods in terms of both speed and accuracy. Additionally, we provide theoretical analysis and insights into why the proposed method works well and discuss potential applications in areas such as computer vision and natural language processing. Our work represents an important step towards scalable machine learning models that can effectively handle big data problems.",1
"Differences in data size per class, also known as imbalanced data distribution, have become a common problem affecting data quality. Big Data scenarios pose a new challenge to traditional imbalanced classification algorithms, since they are not prepared to work with such amount of data. Split data strategies and lack of data in the minority class due to the use of MapReduce paradigm have posed new challenges for tackling the imbalance between classes in Big Data scenarios. Ensembles have shown to be able to successfully address imbalanced data problems. Smart Data refers to data of enough quality to achieve high performance models. The combination of ensembles and Smart Data, achieved through Big Data preprocessing, should be a great synergy. In this paper, we propose a novel Smart Data driven Decision Trees Ensemble methodology for addressing the imbalanced classification problem in Big Data domains, namely SD_DeTE methodology. This methodology is based on the learning of different decision trees using distributed quality data for the ensemble process. This quality data is achieved by fusing Random Discretization, Principal Components Analysis and clustering-based Random Oversampling for obtaining different Smart Data versions of the original data. Experiments carried out in 21 binary adapted datasets have shown that our methodology outperforms Random Forest.",0
"Title: A Novel Approach to Handle Imbalanced Class Distributions in Big Data Sets Using Data Driven Decision Tree Ensembles  Abstract: Big data analysis has become increasingly important due to the vast amount of data generated from various sources. However, one major challenge that arises during big data analysis is handling imbalanced class distributions. This problem can lead to biased results as well as poor predictive performance. In order to address these issues, there is a need for robust algorithms capable of effectively dealing with imbalanced data sets. One such approach is decision tree ensembles which have shown promising results in various applications. In this work, we propose a novel method called smart data driven decision trees ensemble (SDDT) which is designed specifically for handling imbalanced big data sets. Our proposed algorithm uses an advanced sampling technique based on the concept of smartness parameter to balance out the dataset while creating decision trees. Furthermore, our framework uses an iterative process where multiple decision trees are created using different combinations of features and then combined together into a final model. We demonstrate the effectiveness of our algorithm through extensive experiments performed on publicly available datasets across several domains including finance, healthcare, and social media. Results show that our algorithm significantly improves the accuracy and F1 score compared to other state-of-the-art methods for imbalanced classification problems. Overall, our findings suggest that the use of SDDT ensembles provides a powerful tool for tackling imbalanced big data sets, leading to more reliable predictions and better decision making processes.",1
"In this study, we examine a set of primary data collected from 484 students enrolled in a large public university in the Mid-Atlantic United States region during the early stages of the COVID-19 pandemic. The data, called Ties data, included students' demographic and support network information. The support network data comprised of information that highlighted the type of support, (i.e. emotional or educational; routine or intense). Using this data set, models for predicting students' academic achievement, quantified by their self-reported GPA, were created using Chi-Square Automatic Interaction Detection (CHAID), a decision tree algorithm, and cforest, a random forest algorithm that uses conditional inference trees. We compare the methods' accuracy and variation in the set of important variables suggested by each algorithm. Each algorithm found different variables important for different student demographics with some overlap. For White students, different types of educational support were important in predicting academic achievement, while for non-White students, different types of emotional support were important in predicting academic achievement. The presence of differing types of routine support were important in predicting academic achievement for cisgender women, while differing types of intense support were important in predicting academic achievement for cisgender men.",0
"This study aimed to explore how predictive models based on decision trees can effectively analyze college students' academic achievements through their support networks. To achieve this goal, we collected data from undergraduate students at two different universities and examined their academic outcomes, demographic characteristics, social networks, and environmental factors that influence academic performance. We then applied several decision tree techniques to create predictive models and evaluated their accuracy using standard metrics such as mean squared error and F1 score. Our results showed that these models have significant potential in accurately forecasting students' grades based on various network and non-network features. Additionally, our findings highlight the importance of considering diverse factors beyond individual intelligence and hard work in creating successful prediction tools. Overall, our research contributes to advancing knowledge in the field of education and psychology by providing insights into the use of network analysis and machine learning algorithms in understanding student success.",1
"Recently, deep reinforcement learning (DRL) methods have achieved impressive performance on tasks in a variety of domains. However, neural network policies produced with DRL methods are not human-interpretable and often have difficulty generalizing to novel scenarios. To address these issues, prior works explore learning programmatic policies that are more interpretable and structured for generalization. Yet, these works either employ limited policy representations (e.g. decision trees, state machines, or predefined program templates) or require stronger supervision (e.g. input/output state pairs or expert demonstrations). We present a framework that instead learns to synthesize a program, which details the procedure to solve a task in a flexible and expressive manner, solely from reward signals. To alleviate the difficulty of learning to compose programs to induce the desired agent behavior from scratch, we propose to first learn a program embedding space that continuously parameterizes diverse behaviors in an unsupervised manner and then search over the learned program embedding space to yield a program that maximizes the return for a given task. Experimental results demonstrate that the proposed framework not only learns to reliably synthesize task-solving programs but also outperforms DRL and program synthesis baselines while producing interpretable and more generalizable policies. We also justify the necessity of the proposed two-stage learning scheme as well as analyze various methods for learning the program embedding.",0
"In recent years, there has been increasing interest in using machine learning (ML) to synthesize programs from input/output examples. Existing approaches, however, often lack interpretability, generalizability, and robustness, which hinders their adoption by practitioners. This study presents a novel approach that addresses these challenges by learning interpretable and generalizable policies, enabling effective program synthesis across multiple domains. Our method first learns an initial policy by clustering similar example pairs into distinct groups according to similarity and dissimilarity metrics. Then, we refine this policy iteratively through an exploration process guided by human feedback on a small subset of inputs. By combining these two components, our system can effectively learn and generate new programs that align with desired behaviors while providing useful explanations and visualizations along the way. We evaluate our approach on several real-world applications, showing improved performance over baseline methods, higher levels of user satisfaction, and more accurate predictions. These results suggest that our work paves the way towards making ML-based programming accessible and reliable for non-experts, ultimately broadening the impact of AI systems beyond specialized tasks.",1
"Cardiovascular disease, especially heart failure is one of the major health hazard issues of our time and is a leading cause of death worldwide. Advancement in data mining techniques using machine learning (ML) models is paving promising prediction approaches. Data mining is the process of converting massive volumes of raw data created by the healthcare institutions into meaningful information that can aid in making predictions and crucial decisions. Collecting various follow-up data from patients who have had heart failures, analyzing those data, and utilizing several ML models to predict the survival possibility of cardiovascular patients is the key aim of this study. Due to the imbalance of the classes in the dataset, Synthetic Minority Oversampling Technique (SMOTE) has been implemented. Two unsupervised models (K-Means and Fuzzy C-Means clustering) and three supervised classifiers (Random Forest, XGBoost and Decision Tree) have been used in our study. After thorough investigation, our results demonstrate a superior performance of the supervised ML algorithms over unsupervised models. Moreover, we designed and propose a supervised stacked ensemble learning model that can achieve an accuracy, precision, recall and F1 score of 99.98%. Our study shows that only certain attributes collected from the patients are imperative to successfully predict the surviving possibility post heart failure, using supervised ML algorithms.",0
"Heart failure (HF) refers to the inability of the heart to efficiently pump blood throughout the body and has become one of the major health concerns worldwide. Early identification of HF patients at high risk for adverse outcomes is crucial for effective management of disease progression. In recent years, machine learning algorithms have been applied extensively to medical data to improve survival prediction accuracy by identifying hidden patterns from complex datasets. However, no single algorithm can always achieve optimal results due to differences in dataset characteristics and model assumptions. This study aimed to develop a stacked ensemble method that integrates multiple models (random forest, neural network, gradient boosting machines, support vector machines) with diverse capabilities to provide more accurate predictions than any individual model. We analyzed echocardiographic findings obtained through noninvasive tests (echocardiography and laboratory reports) of 2486 HF patients collected over five years across two centers in Saudi Arabia. Our proposed approach achieved higher accuracy rates compared to existing methods reported in the literature. Specifically, our algorithm was able to predict death with sensitivity of 97% and specificity of 82%. These promising results suggest the potential utility of multi-model ensembling as a powerful tool to aid clinicians in making critical decisions related to patient care. Ultimately, further research should focus on evaluating the impact of such approaches on improving patient outcomes before widespread implementation into routine care settings.",1
"Data-driven models created by machine learning gain in importance in all fields of design and engineering. They have high potential to assists decision-makers in creating novel artefacts with a better performance and sustainability. However, limited generalization and the black-box nature of these models induce limited explainability and reusability. These drawbacks provide significant barriers retarding adoption in engineering design. To overcome this situation, we propose a component-based approach to create partial component models by machine learning (ML). This component-based approach aligns deep learning to systems engineering (SE). By means of the example of energy efficient building design, we first demonstrate generalization of the component-based method by accurately predicting the performance of designs with random structure different from training data. Second, we illustrate explainability by local sampling, sensitivity information and rules derived from low-depth decision trees and by evaluating this information from an engineering design perspective. The key for explainability is that activations at interfaces between the components are interpretable engineering quantities. In this way, the hierarchical component system forms a deep neural network (DNN) that directly integrates information for engineering explainability. The large range of possible configurations in composing components allows the examination of novel unseen design cases with understandable data-driven models. The matching of parameter ranges of components by similar probability distribution produces reusable, well-generalizing, and trustworthy models. The approach adapts the model structure to engineering methods of systems engineering and domain knowledge.",0
"Artificial intelligence (AI) has proven itself as an essential tool across various domains including engineering design and manufacturing. Despite significant advancements, there remains concerns on interpretability, explainability and transparency of most modern state of art AI models. In contrast to black box solutions that have been criticized for lacking robustness, reliability, trustworthiness and safety, many recent efforts have shifted towards Explainable Artificial Intelligence (XAI). This research work aims at bridging the gap by presenting a novel unified approach which incorporates XAI principles to engineering design through Component Based Deep Learning. Our proposed framework capitalizes upon knowledge extracted from domain experts in Systems Engineering who have vast experience in managing complex projects in real world scenarios. We demonstrate how our methodology can improve decision making by identifying weaknesses, redundancies and providing suggestions to streamline processes, reduce costs and lead time while enhancing product quality and performance. By explaining every aspect of the modeling process, we aspire to develop reliable and efficient tools capable of supporting human engineers throughout their daily tasks in the entire life cycle of development of engineered products, while keeping them informed about both negative and positive aspects of different design choices. Our experiments validate superiority and efficiency of our proposed technique over conventional state of art methods in several metrics, indicating broader applicability",1
"This paper investigates the optimal signal detection problem with a particular interest in large-scale multiple-input multiple-output (MIMO) systems. The problem is NP-hard and can be solved optimally by searching the shortest path on the decision tree. Unfortunately, the existing optimal search algorithms often involve prohibitively high complexities, which indicates that they are infeasible in large-scale MIMO systems. To address this issue, we propose a general heuristic search algorithm, namely, hyperaccelerated tree search (HATS) algorithm. The proposed algorithm employs a deep neural network (DNN) to estimate the optimal heuristic, and then use the estimated heuristic to speed up the underlying memory-bounded search algorithm. This idea is inspired by the fact that the underlying heuristic search algorithm reaches the optimal efficiency with the optimal heuristic function. Simulation results show that the proposed algorithm reaches almost the optimal bit error rate (BER) performance in large-scale systems, while the memory size can be bounded. In the meanwhile, it visits nearly the fewest tree nodes. This indicates that the proposed algorithm reaches almost the optimal efficiency in practical scenarios, and thereby it is applicable for large-scale systems. Besides, the code for this paper is available at https://github.com/skypitcher/hats.",0
"In recent years, there has been increasing interest in developing machine learning algorithms that can accurately predict how well different choices of search parameters affect the performance of large-scale Multiple Input Multiple Output (MIMO) systems. With these predictions, engineers and researchers could quickly determine which parameters yield optimal performance and reduce time-consuming trial and error approaches. This paper presents our approach towards optimally efficient search with deep learning for large-scale MIMO systems using a customized version of DARTS. Our method involves training models on a small subset of data, then fine-tuning them on additional data to improve their accuracy. We evaluate our model’s predictions against actual system measurements obtained from simulations and experiments and show that they closely match. Furthermore, we demonstrate that our algorithm significantly reduces the search space compared to other methods. Our results suggest that applying machine learning to large-scale MIMO design problems could greatly benefit industry by reducing development costs and improving network performance. As future work, we plan to extend our approach to other wireless communication systems and explore ways to further enhance the efficiency of our algorithm. Overall, our study contributes to the growing body of literature focused on leveraging machine learning to optimize engineering designs.",1
"With the unprecedented proliferation of machine learning software, there is an ever-increasing need to generate efficient code for such applications. State-of-the-art deep-learning compilers like TVM and Halide incorporate a learning-based performance model to search the space of valid implementations of a given deep learning algorithm. For a given application, the model generates a performance metric such as the run time without executing the application on hardware. Such models speed up the compilation process by obviating the need to benchmark an enormous number of candidate implementations, referred to as schedules, on hardware. Existing performance models employ feed-forward networks, recurrent networks, or decision tree ensembles to estimate the performance of different implementations of a neural network. Graphs present a natural and intuitive way to model deep-learning networks where each node represents a computational stage or operation. Incorporating the inherent graph structure of these workloads in the performance model can enable a better representation and learning of inter-stage interactions. The accuracy of a performance model has direct implications on the efficiency of the search strategy, making it a crucial component of this class of deep-learning compilers. In this work, we develop a novel performance model that adopts a graph representation. In our model, each stage of computation represents a node characterized by features that capture the operations performed by the stage. The interaction between nodes is achieved using graph convolutions. Experimental evaluation shows a 7:75x and 12x reduction in prediction error compared to the Halide and TVM models, respectively.",0
"This paper presents a method using graph neural networks (GNN) to predict the performance of deep neural network architectures, including but not limited to image classification models such as ResNet and VGG. We formulate architecture design as a node attribute prediction problem where the GNN learns to make predictions based on both internal features of individual nodes and their relationships. Additionally, we evaluate two types of evaluations: intrinsic task and extrinsic tasks, which can directly measure how well a predicted model would perform against ground truth without overfitting. Our results show that GNN achieves better performance than baseline methods by up to 20% margin on benchmark datasets. Furthermore, our approach improves over previous work due to additional regularization techniques used to prevent overfitting, ensuring more robust performance across different datasets.",1
"Rest is essential for a high-level physiological and psychological performance. It is also necessary for the muscles to repair, rebuild, and strengthen. There is a significant correlation between the quality of rest and the resting posture. Therefore, identification of the resting position is of paramount importance to maintain a healthy life. Resting postures can be classified into four basic categories: Lying on the back (supine), facing of the left / right sides and free-fall position. The later position is already considered to be an unhealthy posture by researchers equivocally and hence can be eliminated. In this paper, we analyzed the other three states of resting position based on the data collected from the physiological parameters: Electrogastrogram (EGG), Electrocardiogram (ECG), Respiration Rate, Heart Rate, and Oxygen Saturation (SpO2). Based on these parameters, the resting position is classified using a hybrid stacked ensemble machine learning model designed using the Decision tree, Random Forest, and Xgboost algorithms. Our study demonstrates a 100% accurate prediction of the resting position using the hybrid model. The proposed method of identifying the resting position based on physiological parameters has the potential to be integrated into wearable devices. This is a low cost, highly accurate and autonomous technique to monitor the body posture while maintaining the user privacy by eliminating the use of RGB camera conventionally used to conduct the polysomnography (sleep Monitoring) or resting position studies.",0
"This paper presents a new method for identifying the resting position of patients based on noninvasive physiological signals such as EGG, ECG, respiration rate, and SPO2 data collected using a wearable sensor device. The proposed approach combines multiple machine learning algorithms through stacked ensemble learning to improve accuracy and robustness. In addition, we develop an algorithm that uses a decision tree structure to identify transitions from sitting, standing, lying down, and walking positions using the same set of inputs. We evaluate the performance of our method using real clinical datasets and demonstrate improved accuracy over state-of-the-art methods. Our results indicate that our approach has potential applications in ambulatory monitoring systems and telemedicine platforms where accurate identification of patient posture is critical for remote healthcare delivery.",1
"From the past few years, due to advancements in technologies, the sedentary living style in urban areas is at its peak. This results in individuals getting a victim of obesity at an early age. There are various health impacts of obesity like Diabetes, Heart disease, Blood pressure problems, and many more. Machine learning from the past few years is showing its implications in all expertise like forecasting, healthcare, medical imaging, sentiment analysis, etc. In this work, we aim to provide a framework that uses machine learning algorithms namely, Random Forest, Decision Tree, XGBoost, Extra Trees, and KNN to train models that would help predict obesity levels (Classification), Bodyweight, and fat percentage levels (Regression) using various parameters. We also applied and compared various hyperparameter optimization (HPO) algorithms such as Genetic algorithm, Random Search, Grid Search, Optuna to further improve the accuracy of the models. The website framework contains various other features like making customizable Diet plans, workout plans, and a dashboard to track the progress. The framework is built using the Python Flask. Furthermore, a weighing scale using the Internet of Things (IoT) is also integrated into the framework to track calories and macronutrients from food intake.",0
"This framework uses machine learning and IoT technology to reduce obesity by monitoring weight loss progress through various IoT devices and recommending personalized workout programs based on individual fitness goals, health metrics, activity levels, sleep patterns, nutrition consumption, daily routine, stress level, weather conditions, and social environment, among other factors that influence weight loss outcomes. Our model predicts changes in BMI and suggests interventions at different stages such as precontemplation, contemplation, preparation, action, maintenance, and termination. We show that our system improves prediction accuracy over traditional approaches in simulated experiments and real world trials on a diverse population across different demographics. In addition, we use adversarial training techniques to improve robustness against malicious attacks from bad users or external agents attempting to manipulate their results.",1
"In recent years, gradient boosted decision trees have become popular in building robust machine learning models on big data. The primary technique that has enabled these algorithms success has been distributing the computation while building the decision trees. A distributed decision tree building, in turn, has been enabled by building quantiles of the big datasets and choosing the candidate split points from these quantile sets. In XGBoost, for instance, a sophisticated quantile building algorithm is employed to identify the candidate split points for the decision trees. This method is often projected to yield better results when the computation is distributed. In this paper, we dispel the notion that these methods provide more accurate and scalable methods for building decision trees in a distributed manner. In a significant contribution, we show theoretically and empirically that choosing the split points uniformly at random provides the same or even better performance in terms of accuracy and computational efficiency. Hence, a simple random selection of points suffices for decision tree building compared to more sophisticated methods.",0
"In recent years machine learning has gained widespread acceptance as a powerful tool for making predictions from data. One popular method used in predictive modelling is decision trees. These models can capture complex relationships among features by recursively partitioning the input space into simple subsets based on values of each feature. However, constructing such models requires careful tuning and evaluation of many parameters which makes them time consuming. Random Forests have been proposed as an extension that reduces overfitting and computation time required for building decision trees. This study compares the use of random sampling versus bootstrap aggregating for constructing decision tree ensembles and concludes that the former approach outperforms the latter in terms of speed while producing comparable results. Our experiments show that using smaller sample sizes during training significantly speeds up model construction without affecting accuracy. Overall our findings highlight the benefits of simplicity and efficiency in machine learning algorithms.",1
"Bootstrap aggregation, known as bagging, is one of the most popular ensemble methods used in machine learning (ML). An ensemble method is a ML method that combines multiple hypotheses to form a single hypothesis used for prediction. A bagging algorithm combines multiple classifiers modeled on different sub-samples of the same data set to build one large classifier. Banks, and their retail banking activities, are nowadays using the power of ML algorithms, including decision trees and random forests, to optimize their processes. However, banks have to comply with regulators and governance and, hence, delivering effective ML solutions is a challenging task. It starts with the bank's validation and governance department, followed by the deployment of the solution in a production environment up to the external validation of the national financial regulator. Each proposed ML model has to be validated and clear rules for every algorithm-based decision must be justified. In this context, we propose XtracTree, an algorithm capable of efficiently converting an ML bagging classifier, such as a random forest, into simple ""if-then"" rules satisfying the requirements of model validation. We use a public loan data set from Kaggle to illustrate the usefulness of our approach. Our experiments demonstrate that using XtracTree, one can convert an ML model into a rule-based algorithm, leading to easier model validation by national financial regulators and the bank's validation department. The proposed approach allowed our banking institution to reduce up to 50% the time of delivery of our AI solutions to the end-user.",0
"In recent years, retail banking has become increasingly reliant on data analytics and machine learning algorithms to make informed decisions that drive business success. One popular technique used in this field is bagging (Bootstrap Aggregating), which involves training multiple decision trees on subsets of the original dataset to create ensembles of predictors that can improve model performance and reduce overfitting risk. However, validating these models to ensure their accuracy and fairness remains a significant challenge.  This paper presents a novel method called ""Xtractree"" that addresses this issue by providing a simple yet effective approach for regulator validation of bagging methods in retail banking applications. Our method leverages the power of random forest models, combining feature extraction techniques from natural language processing (NLP) with traditional financial measures such as credit scores and debit/credit card transactions history. This allows our system to capture both qualitative and quantitative factors affecting creditworthiness while reducing complexity compared to existing approaches.  We demonstrate the effectiveness of our XtracTree framework through extensive experiments using real-world datasets from major European banks. Results show that our method achieves higher prediction accuracy than state-of-the-art competitors across various metrics including Gini coefficient and log loss functions, making it ideal for regulatory compliance testing. Additionally, we provide visualizations and explainability tools to aid interpretability and transparency, crucial requirements for any successful deployment within the industry.  In summary, our research introduces XtracTree, a game-changing solution for regulator validation of bagging methods in retail banking, addressing critical concerns surrounding model accuracy, fairness, and accountability. By bridging technical advancements in NLP, machine learning, and finance, this work provides a unique perspective on how modern technologies can better serve society's need for responsible innovation within one of the most i",1
"Diabetes prediction is an important data science application in the social healthcare domain. There exist two main challenges in the diabetes prediction task: data heterogeneity since demographic and metabolic data are of different types, data insufficiency since the number of diabetes cases in a single medical center is usually limited. To tackle the above challenges, we employ gradient boosting decision trees (GBDT) to handle data heterogeneity and introduce multi-task learning (MTL) to solve data insufficiency. To this end, Task-wise Split Gradient Boosting Trees (TSGB) is proposed for the multi-center diabetes prediction task. Specifically, we firstly introduce task gain to evaluate each task separately during tree construction, with a theoretical analysis of GBDT's learning objective. Secondly, we reveal a problem when directly applying GBDT in MTL, i.e., the negative task gain problem. Finally, we propose a novel split method for GBDT in MTL based on the task gain statistics, named task-wise split, as an alternative to standard feature-wise split to overcome the mentioned negative task gain problem. Extensive experiments on a large-scale real-world diabetes dataset and a commonly used benchmark dataset demonstrate TSGB achieves superior performance against several state-of-the-art methods. Detailed case studies further support our analysis of negative task gain problems and provide insightful findings. The proposed TSGB method has been deployed as an online diabetes risk assessment software for early diagnosis.",0
"In recent years, machine learning has become increasingly important in healthcare applications such as predictive modeling and decision support systems. One popular method used in these applications is gradient boosting trees (GBTs), which have been shown to perform well on a variety of tasks including multi-centric prediction problems.  One challenge faced by practitioners who use GBTs is how to split data into training and testing sets in order to evaluate performance. Traditionally, a random splitting approach was used, but this can lead to overfitting if not properly addressed. An alternative approach is task-wise splitting, where each task is assigned its own training/testing set ratio based on a predefined metric related to that specific task, ensuring more balanced evaluation across all tasks.  This paper proposes a new method called ""Task-Wise Split Gradient Boosting Trees"" (TSG) specifically designed for multi-task classification problems like diabetes prediction. TSG uses a task decomposition strategy to automatically assign different train-test ratios based on cross-validation metrics from previous iterations of the algorithm. This leads to better generalization ability compared to traditional methods, especially on small datasets. We evaluated our proposed method using three publicly available datasets and found promising results, outperforming several state-of-the-art baselines on most measures. Our contributions provide valuable insights for researchers interested in applying machine learning algorithms to multi-centric medical predictions, especially those using GBTs.",1
"While deep reinforcement learning has achieved promising results in challenging decision-making tasks, the main bones of its success --- deep neural networks are mostly black-boxes. A feasible way to gain insight into a black-box model is to distill it into an interpretable model such as a decision tree, which consists of if-then rules and is easy to grasp and be verified. However, the traditional model distillation is usually a supervised learning task under a stationary data distribution assumption, which is violated in reinforcement learning. Therefore, a typical policy distillation that clones model behaviors with even a small error could bring a data distribution shift, resulting in an unsatisfied distilled policy model with low fidelity or low performance. In this paper, we propose to address this issue by changing the distillation objective from behavior cloning to maximizing an advantage evaluation. The novel distillation objective maximizes an approximated cumulative reward and focuses more on disastrous behaviors in critical states, which controls the data shift effect. We evaluate our method on several Gym tasks, a commercial fight game, and a self-driving car simulator. The empirical results show that the proposed method can preserve a higher cumulative reward than behavior cloning and learn a more consistent policy to the original one. Moreover, by examining the extracted rules from the distilled decision trees, we demonstrate that the proposed method delivers reasonable and robust decisions.",0
"In recent years, reinforcement learning (RL) has emerged as a powerful tool for solving complex problems in domains ranging from games to robotics. One key challenge in RL is scalability: how can we efficiently learn policies that perform well across a wide range of environments? In this work, we address this challenge by introducing neural-to-tree policy distillation with policy improvement criterion (NTPD-PIC), a novel method for training large-scale, distributed ensembles of deep reinforcement learning agents. NTPD-PIC leverages the representational power of deep neural networks while retaining interpretability through tree-structured policies. We show that our approach leads to significant improvements over baseline methods on several benchmark control tasks, including both discrete actions and continuous actions. Our contributions provide new insights into the design of efficient, interpretable, and effective deep reinforcement learning algorithms for real-world applications.",1
"The highest strength-to-weight ratio criterion has fascinated curiosity increasingly in virtually all areas where heft reduction is indispensable. Lightweight materials and their joining processes are also a recent point of research demands in the manufacturing industries. Friction Stir Welding (FSW) is one of the recent advancements for joining materials without adding any third material (filler rod) and joining below the melting point of the parent material. The process is widely used for joining similar and dissimilar metals, especially lightweight non-ferrous materials like aluminum, copper, and magnesium alloys. This paper presents verdicts of optimum process parameters on attaining enhanced mechanical properties of the weld joint. The experiment was conducted on a 5 mm 6061 aluminum alloy sheet. Process parameters; tool material, rotational speed, traverse speed, and axial forces were utilized. Mechanical properties of the weld joint are examined employing a tensile test, and the maximum joint strength efficiency was reached 94.2%. Supervised Machine Learning based Regression algorithms such as Decision Trees, Random Forest, and Gradient Boosting Algorithm were used. The results showed that the Random Forest algorithm yielded highest coefficient of determination value of 0.926 which means it gives a best fit in comparison to other algorithms.",0
"Optimization of process parameters for friction stir welding (FSW) has been gaining significant attention due to its potential benefits such as reduced distortion, improved mechanical properties, and cost savings through rapid processing times. In order to achieve optimal parameters, supervised machine learning regression algorithms have emerged as promising tools for predictive modeling and analysis of FSW processes. This research focuses on optimizing process parameters for Friction Stir Welding on AA6061 aluminum alloys, using a novel experimental design approach combining multiple linear regression models (MLRMs) with artificial neural networks (ANN). Four different MLRMs were developed based on input variables including tool rotation speed, traverse velocity, axial force, and pin profile angle, which were identified from a comprehensive literature review. The datasets generated by experiments were used to train the models, after which their predictions were validated against new unseen data sets. Finally, sensitivity analyses were performed to determine the most influential factors affecting the response variable, i.e., tensile strength. Results showed that the proposed hybrid regression method outperformed individual ANN and MLRM models, providing more accurate predictions with lower mean squared error values. Overall, the study demonstrated the effectiveness of utilizing supervised machine learning regression techniques in optimizing process parameters for FSW processes on AA6061 materials, potentially reducing trial and error approaches in industry while enhancing product quality and manufacturing efficiency.",1
"The large and still increasing popularity of deep learning clashes with a major limit of neural network architectures, that consists in their lack of capability in providing human-understandable motivations of their decisions. In situations in which the machine is expected to support the decision of human experts, providing a comprehensible explanation is a feature of crucial importance. The language used to communicate the explanations must be formal enough to be implementable in a machine and friendly enough to be understandable by a wide audience. In this paper, we propose a general approach to Explainable Artificial Intelligence in the case of neural architectures, showing how a mindful design of the networks leads to a family of interpretable deep learning models called Logic Explained Networks (LENs). LENs only require their inputs to be human-understandable predicates, and they provide explanations in terms of simple First-Order Logic (FOL) formulas involving such predicates. LENs are general enough to cover a large number of scenarios. Amongst them, we consider the case in which LENs are directly used as special classifiers with the capability of being explainable, or when they act as additional networks with the role of creating the conditions for making a black-box classifier explainable by FOL formulas. Despite supervised learning problems are mostly emphasized, we also show that LENs can learn and provide explanations in unsupervised learning settings. Experimental results on several datasets and tasks show that LENs may yield better classifications than established white-box models, such as decision trees and Bayesian rule lists, while providing more compact and meaningful explanations.",0
"This is an open access article distributed under the Creative Commons Attribution License which permits unrestricted use, distribution, and reproduction in any medium provided the original work is properly cited.  Logical networks are systems of nodes that transmit data using symbols according to formal rules. They can be used to model natural phenomena such as language structure, decision making processes, neural activity patterns, gene regulation and protein–protein interaction networks. In this research we aim at creating novel methods that allow us to study how logical networks change over time by measuring their ability to process new sets of data. The proposed methodologies provide insights into both intra and inter species biological evolution and human cultural evolution. Furthermore, these studies support the idea of cooperation among individuals within and across species leading to complex adaptive behaviors emerging through shared learning rather than just selection. We present examples from different domains including simple computer programs written in the Python programming language and Boolean circuits implemented on FPGA chips. Our results demonstrate the applicability of our methodology and highlight the potential implications for understanding the fundamental nature of computation itself.",1
"Hateful memes are an emerging method of spreading hate on the internet, relying on both images and text to convey a hateful message. We take an interpretable approach to hateful meme detection, using machine learning and simple heuristics to identify the features most important to classifying a meme as hateful. In the process, we build a gradient-boosted decision tree and an LSTM-based model that achieve comparable performance (73.8 validation and 72.7 test auROC) to the gold standard of humans and state-of-the-art transformer models on this challenging task.",0
This is likely a technical paper. Can you provide some more details on the content?,1
"There is growing interest in neural network architectures for tabular data. Many general-purpose tabular deep learning models have been introduced recently, with performance sometimes rivaling gradient boosted decision trees (GBDTs). These recent models draw inspiration from various sources, including GBDTs, factorization machines, and neural networks from other application domains. Previous tabular neural networks are also drawn upon, but are possibly under-considered, especially models associated with specific tabular problems. This paper focuses on several such models, and proposes modifications for improving their performance. When modified, these models are shown to be competitive with leading general-purpose tabular models, including GBDTs.",0
"Recent advancements in deep learning have led to significant improvements in image classification accuracy using convolutional neural networks (CNNs). Despite these successes, tabular data analysis still largely relies on traditional machine learning methods like linear regression and decision trees. While there has been some work done on applying CNN architectures to tabular data, these approaches suffer from high computational cost and lack interpretability due to their complex and opaque nature. In this study, we present simple modifications to existing tabular neural network models that can improve performance while retaining transparency and efficiency. We introduce two new variants: i) Column Attention Module (CAM), which allows selective consideration of specific columns during training, improving model focus, and ii) Channel Concatenation (CC), which effectively combines channel-wise features into one vector before feeding them through dense layers. Experimental results on benchmark datasets demonstrate the effectiveness of our proposed techniques compared to previous methods. Our approach shows promise as a competitive alternative for tabular data analysis problems.",1
"A family of concurrent data predictors is derived from the decision tree classifier by removing the limitation of sequentially evaluating attributes. By evaluating attributes concurrently, the decision tree collapses into a flat structure. Experiments indicate improvements of the prediction accuracy.",0
"Artificial intelligence (AI) prediction tools are becoming more widely used in business decision making as they allow professionals to make more informed choices based on data analysis. However, many current AI systems rely on tree structures for data visualization that can become complex and difficult to navigate. In contrast, our study proposes using a concurrent data predictor that collapses these decision trees into simple and easy to interpret models. Our findings suggest that by utilizing the concurrent data predictor, professionals can quickly identify trends and patterns within their datasets, leading to better decision making without the need for extensive training. This approach has the potential to revolutionize the use of AI in the business world by providing users with faster and more accurate predictions. Ultimately, our results demonstrate how collapsing the decision tree can significantly improve data analysis efficiency. Note - this could also have application in other disciplines beyond business where time to insight is key.",1
"This article proposes two different approaches to automatically create a map for valid on-street car parking spaces. For this, we use car sharing park-out events data. The first one uses spatial aggregation and the second a machine learning algorithm. For the former, we chose rasterization and road sectioning; for the latter we chose decision trees. We compare the results of these approaches and discuss their advantages and disadvantages. Furthermore, we show our results for a neighborhood in the city of Berlin and report a classification accuracy of 91.6\% on the original imbalanced data. Finally, we discuss further work; from gathering more data over a longer period of time to fitting spatial Gaussian densities to the data and the usage of apps for manual validation and annotation of parking spaces to improve ground truth data.",0
"Automatic Extraction of On-Street Parking Spaces Using Park-Out Events Data On-street parking plays a vital role in urban mobility management, as public transportation systems cannot accommodate all citizens’ travel needs due to their limited capacity. Therefore, identifying available on-street parking spots can reduce traffic congestion by minimizing cruising time caused by drivers searching for vacant lots. Existing approaches use GPS sensors to track parked vehicles; however, such methods are costly, require extensive maintenance, and raise privacy concerns. This study proposes a new approach based on processing park-out events from smartphone applications. We first establish the feasibility of our methodology through analyzing thousands of park-out records from three cities across Europe. Second, we present two novel models, one exploiting sequential patterns within park-out sequences and another extracting clusters from park-in locations. Our results reveal that these patterns capture meaningful contexts related to user behavior in real cities (e.g., visit durations at popular destinations). With more than 89% precision regarding the exact position of each detected space, our solution presents promising alternatives to overcome current limitations. Finally, we outline future research directions aimed at integrating additional features into our framework and broadening its scope beyond simple detection tasks. Overall, this work paves the way toward efficient computational intelligence techniques capable of supporting city authorities with essential services while preserving users' privacy.",1
"Decision tree ensembles are widely used in practice. In this work, we study in ensemble settings the effectiveness of replacing the split strategy for the state-of-the-art online tree learner, Hoeffding Tree, with a rigorous but more eager splitting strategy that we had previously published as Hoeffding AnyTime Tree. Hoeffding AnyTime Tree (HATT), uses the Hoeffding Test to determine whether the current best candidate split is superior to the current split, with the possibility of revision, while Hoeffding Tree aims to determine whether the top candidate is better than the second best and if a test is selected, fixes it for all posterity. HATT converges to the ideal batch tree while Hoeffding Tree does not. We find that HATT is an efficacious base learner for online bagging and online boosting ensembles. On UCI and synthetic streams, HATT as a base learner outperforms HT within a 0.05 significance level for the majority of tested ensembles on what we believe is the largest and most comprehensive set of testbenches in the online learning literature. Our results indicate that HATT is a superior alternative to Hoeffding Tree in a large number of ensemble settings.",0
"Avoid referencing other researchers in the field or citation numbers. Just focus on describing your approach and why you believe it is novel and significant. Use past tense because these approaches have already been proposed and tested so we know they work. --- A recently introduced technique called eager learning has shown promising results by improving tree accuracy while maintaining low computational cost. Our new strategy builds upon this methodology by focusing on the splitting process and proposing a more efficient algorithmic solution for faster training times. This approach significantly reduces computation time during training compared to traditional decision trees, resulting in improved overall performance. By optimizing parameter settings through a rigorous experimental evaluation, our method achieves state-of-the art predictive accuracies across several benchmark datasets. This advance offers both practitioners and researchers alike a powerful tool for addressing complex problems where speedy decision making is critical, including applications in fraud detection, medical diagnosis, and risk assessment. With its demonstrated benefits and ease of implementation, this new method promises to play a vital role in shaping future advances in machine learning.",1
"Machine learning techniques have been paramount throughout the last years, being applied in a wide range of tasks, such as classification, object recognition, person identification, and image segmentation. Nevertheless, conventional classification algorithms, e.g., Logistic Regression, Decision Trees, and Bayesian classifiers, might lack complexity and diversity, not suitable when dealing with real-world data. A recent graph-inspired classifier, known as the Optimum-Path Forest, has proven to be a state-of-the-art technique, comparable to Support Vector Machines and even surpassing it in some tasks. This paper proposes a Python-based Optimum-Path Forest framework, denoted as OPFython, where all of its functions and classes are based upon the original C language implementation. Additionally, as OPFython is a Python-based library, it provides a more friendly environment and a faster prototyping workspace than the C language.",0
"Abstract: This paper presents OPFython, a novel approach to forest classification that draws inspiration from the flexibility and power of Python programming language. We introduce the concept of ""optimum-path forest,"" which extends traditional decision trees by allowing branches to split on any subset of features, rather than just one feature at a time. By utilizing this optimum-path framework, we can build more expressive classifiers that better capture complex relationships in high dimensional data.  The core idea behind OPFython is to train multiple decision trees, each with different splits and stop criteria, and then combine their predictions using a voting scheme inspired by Python's operator chaining syntax. Our method allows us to effectively prune away poorly performing trees while retaining informative ones, leading to improved accuracy and interpretability compared to standard bagging ensembles.  We demonstrate the effectiveness of OPFython through extensive experiments on a range of real world datasets including both binary and multi-class problems, as well as regression tasks. Results show significant improvements over state-of-the-art methods across all evaluation metrics. In addition, we provide detailed ablation studies that highlight the importance of key components within our framework and analyze how different choices impact overall performance.  OPFython offers several benefits over existing methods for tree-based ensemble learning, such as random forests and gradient boosting machines (GBM). Firstly, our approach incorporates a broader spectrum of splitting rules, making it possible to detect nonlinear dependencies among variables in high dimensions without imposing additional complexity penalties. Secondly, our algorithm is computationally efficient, scaling linearly with the number of samples due to parallelization during tree building and aggregation steps. Lastly, our model provides interpretable output in the form of graphical representation of individual trees and importance scores derived from the voting process, which can aid in exploratory analysis and explanation generation tasks. Overall, OPFython represents a powerful tool for data scientists and machine learners interested in leveraging advanced machine learning techniques for solving challenging prediction problems.",1
"Densely connected convolutional networks (DenseNet) behave well in image processing. However, for regression tasks, convolutional DenseNet may lose essential information from independent input features. To tackle this issue, we propose a novel DenseNet regression model where convolution and pooling layers are replaced by fully connected layers and the original concatenation shortcuts are maintained to reuse the feature. To investigate the effects of depth and input dimension of proposed model, careful validations are performed by extensive numerical simulation. The results give an optimal depth (19) and recommend a limited input dimension (under 200). Furthermore, compared with the baseline models including support vector regression, decision tree regression, and residual regression, our proposed model with the optimal depth performs best. Ultimately, DenseNet regression is applied to predict relative humidity, and the outcome shows a high correlation (0.91) with observations, which indicates that our model could advance environmental data analysis.",0
"Nonlinear regression using neural networks (NNs) has been of significant interest due to their universal approximation properties. However, traditional fully connected feedforward NNs can struggle to capture complex dependencies within high dimensional inputs. To overcome these limitations, we propose densely connected feedforward neural networks that preserve more information during forward propagation by directly connecting each neuron to all input features. We show that our approach outperforms state-of-the-art methods on benchmark datasets while requiring fewer parameters. Our analysis suggests that dense connectivity allows NNs to model complex relationships between input variables and achieve improved performance on challenging tasks. In summary, densely connected NNs provide a powerful alternative for nonlinear regression problems where accurate predictions require capturing intricate interactions among features.",1
"Boosting is a method for finding a highly accurate hypothesis by linearly combining many ``weak"" hypotheses, each of which may be only moderately accurate. Thus, boosting is a method for learning an ensemble of classifiers. While boosting has been shown to be very effective for decision trees, its impact on neural networks has not been extensively studied. We prove one important difference between sums of decision trees compared to sums of convolutional neural networks (CNNs) which is that a sum of decision trees cannot be represented by a single decision tree with the same number of parameters while a sum of CNNs can be represented by a single CNN. Next, using standard object recognition datasets, we verify experimentally the well-known result that a boosted ensemble of decision trees usually generalizes much better on testing data than a single decision tree with the same number of parameters. In contrast, using the same datasets and boosting algorithms, our experiments show the opposite to be true when using neural networks (both CNNs and multilayer perceptrons (MLPs)). We find that a single neural network usually generalizes better than a boosted ensemble of smaller neural networks with the same total number of parameters.",0
"In recent years, boosted neural networks have become increasingly popular due to their ability to achieve state-of-the-art results on a wide range of tasks. However, despite their successes, these models can often suffer from issues such as overfitting, underperformance on out-of-distribution data, and lack of robustness against adversarial attacks. This paper seeks to examine the limitations of boosting techniques in deep learning frameworks and explore the challenges that arise when using them. We will discuss current methods used to address these limitations and propose potential solutions for improving boosted neural networks. Our findings contribute new insights into the capabilities and shortcomings of boosting techniques and provide guidance for practitioners working with large datasets and complex models. By understanding the limits of boosted neural networks, we hope to inspire further research and development in the field of artificial intelligence, ultimately leading to more effective and reliable machine learning systems.",1
"Autonomous vehicles must balance a complex set of objectives. There is no consensus on how they should do so, nor on a model for specifying a desired driving behavior. We created a dataset to help address some of these questions in a limited operating domain. The data consists of 92 traffic scenarios, with multiple ways of traversing each scenario. Multiple annotators expressed their preference between pairs of scenario traversals. We used the data to compare an instance of a rulebook, carefully hand-crafted independently of the dataset, with several interpretable machine learning models such as Bayesian networks, decision trees, and logistic regression trained on the dataset. To compare driving behavior, these models use scores indicating by how much different scenario traversals violate each of 14 driving rules. The rules are interpretable and designed by subject-matter experts. First, we found that these rules were enough for these models to achieve a high classification accuracy on the dataset. Second, we found that the rulebook provides high interpretability without excessively sacrificing performance. Third, the data pointed to possible improvements in the rulebook and the rules, and to potential new rules. Fourth, we explored the interpretability vs performance trade-off by also training non-interpretable models such as a random forest. Finally, we make the dataset publicly available to encourage a discussion from the wider community on behavior specification for AVs. Please find it at github.com/bassam-motional/Reasonable-Crowd.",0
"This should summarize your main claims and contributions without spoiling any key results (as we don’t yet have them). Focus on why these questions matter, not just how you plan to address them. Please use 3rd person perspective throughout the entire document. -- Thank you!",1
"Recent years have seen a surge in research on deep interpretable neural networks with decision trees as one of the most commonly incorporated tools. There are at least three advantages of using decision trees over logistic regression classification models: they are easy to interpret since they are based on binary decisions, they can make decisions faster, and they provide a hierarchy of classes. However, one of the well-known drawbacks of decision trees, as compared to decision graphs, is that decision trees cannot reuse the decision nodes. Nevertheless, decision graphs were not commonly used in deep learning due to the lack of efficient gradient-based training techniques. In this paper, we fill this gap and provide a general paradigm based on Markov processes, which allows for efficient training of the special type of decision graphs, which we call Self-Organizing Neural Graphs (SONG). We provide an extensive theoretical study of SONG, complemented by experiments conducted on Letter, Connect4, MNIST, CIFAR, and TinyImageNet datasets, showing that our method performs on par or better than existing decision models.",0
"ABSTRACT: This paper introduces SONG (Self-Organizing Neural Graph), a novel approach to modeling complex graphs using deep learning techniques. Unlike traditional methods that rely on manual feature engineering, SONG automatically learns low-dimensional representations of nodes by optimizing them across multiple graph datasets. This allows the method to capture highly nonlinear relationships among node features while preserving their key characteristics. Experimental results demonstrate the effectiveness of SONG in generating high-quality embeddings on several benchmark data sets, outperforming state-of-the-art models such as DeepWalk, Node2Vec, and LINE. Overall, SONG provides researchers with a powerful tool for studying graph structures and offers potential applications in diverse fields such as social network analysis, biological networks, and knowledge representation.",1
"The increasing concerns about data privacy and security drive an emerging field of studying privacy-preserving machine learning from isolated data sources, i.e., federated learning. A class of federated learning, vertical federated learning, where different parties hold different features for common users, has a great potential of driving a more variety of business cooperation among enterprises in many fields. In machine learning, decision tree ensembles such as gradient boosting decision tree (GBDT) and random forest are widely applied powerful models with high interpretability and modeling efficiency. However, the interpretability is compromised in state-of-the-art vertical federated learning frameworks such as SecureBoost with anonymous features to avoid possible data breaches. To address this issue in the inference process, in this paper, we propose Fed-EINI to protect data privacy and allow the disclosure of feature meaning by concealing decision paths with a communication-efficient secure computation method for inference outputs. The advantages of Fed-EINI will be demonstrated through both theoretical analysis and extensive numerical results.",0
"Title: Improving Collaborative Model Training via Enhanced Feature Selection and Ensemble Integration in Distributed Systems  Abstract: This research presents an approach for enhancing decision tree ensemble inference within federated learning frameworks through feature selection and efficient model integration techniques. Our proposed methodology, called Fed-EINI (Enhanced Inference for Federated Trees), addresses limitations of traditional distributed machine learning approaches by incorporating novel data reduction strategies designed to optimize resource utilization across decentralized devices while maximizing collaborative model performance. Through extensive experimentation involving real-world datasets from diverse domains, we demonstrate that Fed-EINI can effectively identify informative features and enhance model interpretability without compromising predictive accuracy. By leveraging our framework, organizations operating large-scale distributed environments may benefit from improved model training efficiency and collaboration, ultimately facilitating better decision support capabilities and organizational insights generation. Furthermore, our findings indicate significant potential for further research into the development of robust federated learning solutions capable of navigating complex deployment scenarios characterized by variable connectivity conditions, data privacy constraints, and domain specificities. Overall, this work represents a valuable contribution towards democratizing access to advanced analytics tools through effective coordination of geographically dispersed computing resources, particularly those embedded in edge computing systems or mobile settings.",1
"In this paper, Bayesian based aggregation of decision trees in an ensemble (decision forest) is investigated. The focus is laid on multi-class classification with number of samples significantly skewed toward one of the classes. The algorithm leverages out-of-bag datasets to estimate prediction errors of individual trees, which are then used in accordance with the Bayes rule to refine the decision of the ensemble. The algorithm takes prevalence of individual classes into account and does not require setting of any additional parameters related to class weights or decision-score thresholds. Evaluation is based on publicly available datasets as well as on an proprietary dataset comprising network traffic telemetry from hundreds of enterprise networks with over a million of users overall. The aim is to increase the detection capabilities of an operating malware detection system. While we were able to keep precision of the system higher than 94\%, that is only 6 out of 100 detections shown to the network administrator are false alarms, we were able to achieve increase of approximately 7\% in the number of detections. The algorithm effectively handles large amounts of data, and can be used in conjunction with most of the state-of-the-art algorithms used to train decision forests.",0
In general the decision forest approach has shown to provide accurate results on difficult tasks like image classification. We take advantage of these properties by treating each base classifier vote as input into our ensemble system and then taking the thresholded majority vote outputted from the final level of decision forest as the confidence that we have for any given prediction. Our model outperforms state-of-the-art solutions across all evaluation metrics at every budget in all datasets. Additionally we see improved performance over traditional random subspace methods by large margins even in cases where there might only be a single anomaly sample per class. These results show promise towards using decision forest ensembles as powerful tools which can accurately detect rare events while providing better precision than previous approaches.,1
"Text data are increasingly handled in an automated fashion by machine learning algorithms. But the models handling these data are not always well-understood due to their complexity and are more and more often referred to as ""black-boxes."" Interpretability methods aim to explain how these models operate. Among them, LIME has become one of the most popular in recent years. However, it comes without theoretical guarantees: even for simple models, we are not sure that LIME behaves accurately. In this paper, we provide a first theoretical analysis of LIME for text data. As a consequence of our theoretical findings, we show that LIME indeed provides meaningful explanations for simple models, namely decision trees and linear models.",0
"This paper provides an analysis of Local Interpretable Model-agnostic Explanations (LIME), a technique used to explain the decisions made by machine learning models on text data. We examine how LIME works, its strengths and weaknesses, and provide case studies demonstrating its application in natural language processing tasks such as sentiment analysis, named entity recognition, and question answering. Our results show that while LIME can effectively generate meaningful explanations for some text classification problems, it may struggle with more complex or nuanced linguistic phenomena. We conclude by discussing future directions for research in this area, including improving the interpretability of LIME and exploring other techniques for explaining model predictions in NLP applications. Overall, we hope that this work contributes to a better understanding of the potential and limitations of using LIME to interpret black box models in natural language processing.",1
"SHAP (SHapley Additive exPlanation) values provide a game theoretic interpretation of the predictions of machine learning models based on Shapley values. While exact calculation of SHAP values is computationally intractable in general, a recursive polynomial-time algorithm called TreeShap is available for decision tree models. However, despite its polynomial time complexity, TreeShap can become a significant bottleneck in practical machine learning pipelines when applied to large decision tree ensembles. We present GPUTreeShap, a modified TreeShap algorithm suitable for massively parallel computation on graphics processing units. Our approach first preprocesses each decision tree to isolate variable sized sub-problems from the original recursive algorithm, then solves a bin packing problem, and finally maps sub-problems to single-instruction, multiple-thread (SIMT) tasks for parallel execution with specialised hardware instructions. With a single NVIDIA Tesla V100-32 GPU, we achieve speedups of up to 19x for SHAP values, and speedups of up to 340x for SHAP interaction values, over a state-of-the-art multi-core CPU implementation executed on two 20-core Xeon E5-2698 v4 2.2 GHz CPUs. We also experiment with multi-GPU computing using eight V100 GPUs, demonstrating throughput of 1.2M rows per second -- equivalent CPU-based performance is estimated to require 6850 CPU cores.",0
"In recent years, tree ensembles have become increasingly popular due to their ability to achieve high performance on many different tasks. One key challenge in working with tree ensembles is understanding how individual features contribute to predictions made by the model. To address this issue, we present a new method called ""GPUTreeSHAP"" that enables massively parallel exact calculation of SHAP (SHapley Additive exPlanations) scores for tree ensembles. These scores can provide insights into feature importance, interactions among features, and potential biases in the data. Our approach leverages the power of modern graphics processing units (GPUs) to distribute computation across multiple cores, enabling efficient computation even on large datasets. We demonstrate the effectiveness of our method using several case studies and show that it achieves speedups of up to four orders of magnitude compared to traditional sequential methods. Overall, GPUTreeSHAP offers a powerful tool for interpreting tree ensemble models and opens up new possibilities for researchers working in areas such as machine learning, finance, healthcare, and environmental science.",1
"Introduction: Real-world data generated from clinical practice can be used to analyze the real-world evidence (RWE) of COVID-19 pharmacotherapy and validate the results of randomized clinical trials (RCTs). Machine learning (ML) methods are being used in RWE and are promising tools for precision-medicine. In this study, ML methods are applied to study the efficacy of therapies on COVID-19 hospital admissions in the Valencian Region in Spain. Methods: 5244 and 1312 COVID-19 hospital admissions - dated between January 2020 and January 2021 from 10 health departments, were used respectively for training and validation of separate treatment-effect models (TE-ML) for remdesivir, corticosteroids, tocilizumab, lopinavir-ritonavir, azithromycin and chloroquine/hydroxychloroquine. 2390 admissions from 2 additional health departments were reserved as an independent test to analyze retrospectively the survival benefits of therapies in the population selected by the TE-ML models using cox-proportional hazard models. TE-ML models were adjusted using treatment propensity scores to control for pre-treatment confounding variables associated to outcome and further evaluated for futility. ML architecture was based on boosted decision-trees. Results: In the populations identified by the TE-ML models, only Remdesivir and Tocilizumab were significantly associated with an increase in survival time, with hazard ratios of 0.41 (P = 0.04) and 0.21 (P = 0.001), respectively. No survival benefits from chloroquine derivatives, lopinavir-ritonavir and azithromycin were demonstrated. Tools to explain the predictions of TE-ML models are explored at patient-level as potential tools for personalized decision making and precision medicine. Conclusion: ML methods are suitable tools toward RWE analysis of COVID-19 pharmacotherapies. Results obtained reproduce published results on RWE and validate the results from RCTs.",0
"In recent years, real world evidence (RWE) has emerged as a critical tool for evaluating medical interventions. RWE refers to data collected outside traditional clinical trials, such as electronic health records, claims databases, and patient registries. The use of RWE has been especially important during the COVID-19 pandemic, where rapid assessment of pharmacological therapies is crucial for public health decision making. This paper presents an overview of machine learning methods used in analyzing RWE for evaluating COVID-19 pharmacotherapy. We discuss how these approaches can address some of the challenges associated with using RWE for drug effectiveness studies, including heterogeneity in treatment effects, confounding by indication, and missing data. Additionally, we provide examples of successful applications of machine learning techniques in COVID-19 RWE analysis, highlighting their potential benefits and limitations. Finally, we conclude with a discussion on future research directions for leveraging machine learning in RWE analysis of COVID-19 pharmacotherapy. Our review demonstrates that machine learning approaches have great promise in generating meaningful insights from complex RWE datasets, ultimately informing better decisions regarding the use of medications in responding to the COVID-19 epidemic.",1
"Neural architecture search (NAS) with an accuracy predictor that predicts the accuracy of candidate architectures has drawn increasing attention due to its simplicity and effectiveness. Previous works usually employ neural network-based predictors which require more delicate design and are easy to overfit. Considering that most architectures are represented as sequences of discrete symbols which are more like tabular data and preferred by non-neural predictors, in this paper, we study an alternative approach which uses non-neural model for accuracy prediction. Specifically, as decision tree based models can better handle tabular data, we leverage gradient boosting decision tree (GBDT) as the predictor for NAS. We demonstrate that the GBDT predictor can achieve comparable (if not better) prediction accuracy than neural network based predictors. Moreover, considering that a compact search space can ease the search process, we propose to prune the search space gradually according to important features derived from GBDT. In this way, NAS can be performed by first pruning the search space and then searching a neural architecture, which is more efficient and effective. Experiments on NASBench-101 and ImageNet demonstrate the effectiveness of using GBDT as predictor for NAS: (1) On NASBench-101, it is 22x, 8x, and 6x more sample efficient than random search, regularized evolution, and Monte Carlo Tree Search (MCTS) in finding the global optimum; (2) It achieves 24.2% top-1 error rate on ImageNet, and further achieves 23.4% top-1 error rate on ImageNet when enhanced with search space pruning. Code is provided at https://github.com/renqianluo/GBDT-NAS.",0
"Effectively searching through a large space of possible neural architectures can be a computationally expensive task. In this work, we develop new methods that allow us to predict the accuracy of these models without training them on real data. We use non-neural machine learning algorithms to identify key features of each architecture, such as depth and width, which we then use to train a model that predicts accuracy. Our method achieves comparable results to those obtained by trained models and outperforms alternative approaches based on surrogate models or random search. These advances have important implications for automating the design of artificial intelligence systems, reducing computational costs and accelerating progress towards human level performance.",1
"Precision agriculture system is an arising idea that refers to overseeing farms utilizing current information and communication technologies to improve the quantity and quality of yields while advancing the human work required. The automation requires the assortment of information given by the sensors such as soil, water, light, humidity, temperature for additional information to furnish the operator with exact data to acquire excellent yield to farmers. In this work, a study is proposed that incorporates all common state-of-the-art approaches for precision agriculture use. Technologies like the Internet of Things (IoT) for data collection, machine Learning for crop damage prediction, and deep learning for crop disease detection is used. The data collection using IoT is responsible for the measure of moisture levels for smart irrigation, n, p, k estimations of fertilizers for best yield development. For crop damage prediction, various algorithms like Random Forest (RF), Light gradient boosting machine (LGBM), XGBoost (XGB), Decision Tree (DT) and K Nearest Neighbor (KNN) are used. Subsequently, Pre-Trained Convolutional Neural Network (CNN) models such as VGG16, Resnet50, and DenseNet121 are also trained to check if the crop was tainted with some illness or not.",0
"Title: Using IoT and machine learning to improve precision agriculture  Precision agriculture is becoming increasingly important as farmers seek to optimize crop yields while reducing waste and environmental impact. To achieve these goals, there is a need for systems that can collect data from multiple sources across different stages of the agricultural process and use advanced algorithms to analyze and make recommendations based on that data. This study proposes such a system by integrating Internet of Things (IoT) sensors with machine learning techniques. Our proposed multimodal approach uses both real-time sensor readings and historical weather patterns to predict optimal planting times, water usage, fertilization levels, and harvest schedules. We demonstrate how our system can accurately estimate yield potentials for two major crops – corn and wheat – in the United States, thus providing valuable insights into improved management practices to farmers. Through extensive testing, we show how our method outperforms state-of-the-art alternatives in terms of prediction accuracy and generalizability. Overall, our work represents an important step towards more efficient, sustainable, and profitable agriculture through technology integration and innovation.",1
"Interpretability in machine learning (ML) is crucial for high stakes decisions and troubleshooting. In this work, we provide fundamental principles for interpretable ML, and dispel common misunderstandings that dilute the importance of this crucial topic. We also identify 10 technical challenge areas in interpretable machine learning and provide history and background on each problem. Some of these problems are classically important, and some are recent problems that have arisen in the last few years. These problems are: (1) Optimizing sparse logical models such as decision trees; (2) Optimization of scoring systems; (3) Placing constraints into generalized additive models to encourage sparsity and better interpretability; (4) Modern case-based reasoning, including neural networks and matching for causal inference; (5) Complete supervised disentanglement of neural networks; (6) Complete or even partial unsupervised disentanglement of neural networks; (7) Dimensionality reduction for data visualization; (8) Machine learning models that can incorporate physics and other generative or causal constraints; (9) Characterization of the ""Rashomon set"" of good models; and (10) Interpretable reinforcement learning. This survey is suitable as a starting point for statisticians and computer scientists interested in working in interpretable machine learning.",0
"As machine learning (ML) has been rapidly integrated into our daily lives over the past decade, understanding how these models make decisions remains one of the biggest obstacles preventing widespread adoption of ML applications. Despite recent advances in interpretability research, we still lack fundamental principles guiding model interpretations and evaluations that ensure transparency and trustworthiness. To tackle this challenge, we propose ten grand challenges that can drive future progress in interpreting and explaining complex ML models and systems. This article presents an analysis of existing interpretability frameworks, highlighting their strengths and limitations, along with the formulation of essential questions that underlie each grand challenge. We conclude by discussing open directions that could further advance the field of interpretable ML towards ensuring robustness, fairness, security, responsibility, privacy, accountability, accessibility, sustainability, ethics, and social impact assessments. Our work will serve as a foundation for both researchers and practitioners pursuing transparent, reliable, and explainable artificial intelligence.",1
"Model explainability is essential for the creation of trustworthy Machine Learning models in healthcare. An ideal explanation resembles the decision-making process of a domain expert and is expressed using concepts or terminology that is meaningful to the clinicians. To provide such an explanation, we first associate the hidden units of the classifier to clinically relevant concepts. We take advantage of radiology reports accompanying the chest X-ray images to define concepts. We discover sparse associations between concepts and hidden units using a linear sparse logistic regression. To ensure that the identified units truly influence the classifier's outcome, we adopt tools from Causal Inference literature and, more specifically, mediation analysis through counterfactual interventions. Finally, we construct a low-depth decision tree to translate all the discovered concepts into a straightforward decision rule, expressed to the radiologist. We evaluated our approach on a large chest x-ray dataset, where our model produces a global explanation consistent with clinical knowledge.",0
"In recent years, there has been increased interest in developing techniques that can provide deep learning models with more human-like interpretability. To date, most explanations of conceptually learned representations focus on describing activation patterns and attention weights as if they could explain complex concepts such as “a person riding a horse.” But these descriptions ignore how these representations come into existence; specifically: which factors cause specific features of deep neural networks (DNNs) to develop at each layer. This work addresses this gap by using causality analysis of DNNs to determine the origins of their deep representations, i.e., we seek to establish cause–effect relationships between input data statistics and hidden feature activations throughout training. We show that our approach provides valuable insights into the reasoning and decision-making processes of stateof-the-art DNNs, allowing us to identify common mistakes made during training, uncover regularities across tasks/domains, and gain insight into the internal functioning of popular architectures like ResNets, ViTs, etc. Our technique builds upon recently proposed methods such as Neuron Introspection [Chen et al., 2021] and Attention Rollouts [Kaplan et al., 2020], but extends them beyond simple visualizations or saliency maps to actual quantitative evaluations and error diagnoses of DNN behavior. We hope that this new type of diagnostic tool leads to improved understanding of deep learning models and helps guide future design choices for better generalization performance across different environments.",1
"While deep learning has revolutionized research and applications in NLP and computer vision, this has not yet been the case for behavioral modeling and behavioral health applications. This is because the domain's datasets are smaller, have heterogeneous datatypes, and typically exhibit a large degree of missingness. Therefore, off-the-shelf deep learning models require significant, often prohibitive, adaptation. Accordingly, many research applications still rely on manually coded features with boosted tree models, sometimes with task-specific features handcrafted by experts. Here, we address these challenges by providing a neural architecture framework for mobile sensing data that can learn generalizable feature representations from time series and demonstrates the feasibility of transfer learning on small data domains through finetuning. This architecture combines benefits from CNN and Trans-former architectures to (1) enable better prediction performance by learning directly from raw minute-level sensor data without the need for handcrafted features by up to 0.33 ROC AUC, and (2) use pretraining to outperform simpler neural models and boosted decision trees with data from as few a dozen participants.",0
"In our modern lives we rely heavily on mobile devices which can sense both user behavior as well as context (location). These devices have become rich sources of data that scientists leverage to build models capable of classifying and understanding human activity. For these approaches however small datasets remain problematic. Data sizes need to be big enough so that neural networks can capture all the complexities of the underlying phenomena they seek to explain. To address these problems transformers, large language processing networks, were introduced which have achieved state of the art results. Here we show that using behavior representation learning based on the use of transformer architectures applied to relatively small datasets enables more accurate classification than prior methods. This represents a huge step forward in enabling application developers to draw insights from their users' interactions with their applications by utilizing simple sensor based modalities like accelerometry, GPS and location. We demonstrate the potential benefits on two different behaviors: gym workouts consisting of several exercises where label noise exists in some instances; and real world activities performed at home such as cleaning dishes and laundry etc. Results show improved accuracy over traditional recurrent architectures including LSTMs. Our contribution shows how transformers provide a new toolkit that allows for transferring knowledge across tasks from larger, labeled datasets allowing researchers and app developers alike to drive innovative experiences and better health outcomes.",1
"This paper contributes to interpretable machine learning via visual knowledge discovery in parallel coordinates. The concepts of hypercubes and hyper-blocks are used as easily understandable by end-users in the visual form in parallel coordinates. The Hyper algorithm for classification with mixed and pure hyper-blocks (HBs) is proposed to discover hyper-blocks interactively and automatically in individual, multiple, overlapping, and non-overlapping setting. The combination of hyper-blocks with linguistic description of visual patterns is presented too. It is shown that Hyper models generalize decision trees. The Hyper algorithm was tested on the benchmark data from UCI ML repository. It allowed discovering pure and mixed HBs with all data and then with 10-fold cross validation. The links between hyper-blocks, dimension reduction and visualization are established. Major benefits of hyper-block technology and the Hyper algorithm are in their ability to discover and observe hyper-blocks by end-users including side by side visualizations making patterns visible for all classes. Another advantage of sets of HBs relative to the decision trees is the ability to avoid both data overgeneralization and overfitting.",0
"In recent years, there has been growing interest in developing machine learning models that can produce interpretable results. This is because such models enable humans to better understand how these systems make predictions by providing transparent explanations of their decision making processes. One popular approach towards building interpretable machine learning models is through parallel coordinates, which allows visualization of high dimensional data as lines in a two-dimensional space. However, existing methods for discovering interpretable machine learning models in parallel coordinates have several limitations. These approaches often rely on heuristics and manual feature engineering techniques, which may result in suboptimal performance. Furthermore, current state-of-the-art parallel coordinate methods typically require serial processing, limiting their scalability for large datasets. To address these challenges, we propose a new methodology for finding interpretable machine learning models in parallel coordinates using hyperparameter optimization and feature selection techniques. Our approach employs an iterative algorithm that alternates between optimizing model parameters and selecting relevant features from high-dimensional spaces. We demonstrate the effectiveness of our approach using comprehensive experiments on benchmark datasets and showcase its superiority compared to traditional methods. By enabling more efficient discovery of interpretable machine learning models in parallel coordinates, our work paves the way for advanced applications in fields like medicine, finance, and social sciences where interpretability is crucial.",1
"Solar energy is a clean and renewable energy. Photovoltaic (PV) power is an important way to utilize solar energy. Accurate PV power forecast is crucial to the large-scale application of PV power and the stability of electricity grid. This paper proposes a novel method for short-term photovoltaic power forecast using deep convolutional long short-term memory (ConvLSTM) network and kernel density estimation (KDE). In the proposed method, ConvLSTM is used to forecast the future photovoltaic power and KDE is used for estimating the joint probabilistic density function and giving the probabilistic confidence interval. Experiments in an actual photovoltaic power station verify the effectiveness of the proposed method. Comparison experiments with convolutional neural network (CNN) and long short-term memory network (LSTM)shows that ConvLSTM can combine the advantages of both CNN and LSTM and significantly outperform CNN and LSTM in terms of forecast accuracy. Through further comparison with other five conventional methods including multilayer perceptron (MLP), support vector regression (SVR), extreme learning machine (ELM), classification and regression tree (CART) and gradient boosting decision tree (GBDT), ConvLSTM can significantly improve the forecast accuracy by more than 20% for most of the five methods and the superiorities of ConvLSTM are further verified.",0
"Here we present a new method using LSTMs in conjunction with kernel density estimation (KDE) for making probabilistic PV power output predictions on an hourly timescale for up to one day into the future. We show that our approach can provide good results compared to both simple baseline models and other state of art approaches. Our main innovation consists of training several different models under different random initializations, each producing slightly different predicted outputs. By aggregating these models together with KDE to produce a probability distribution over possible futures, our model significantly outperforms standard neural networks in terms of both raw accuracy and quantile-based rankings.",1
"Can a machine learn Machine Learning? This work trains a machine learning model to solve machine learning problems from a University undergraduate level course. We generate a new training set of questions and answers consisting of course exercises, homework, and quiz questions from MIT's 6.036 Introduction to Machine Learning course and train a machine learning model to answer these questions. Our system demonstrates an overall accuracy of 96% for open-response questions and 97% for multiple-choice questions, compared with MIT students' average of 93%, achieving grade A performance in the course, all in real-time. Questions cover all 12 topics taught in the course, excluding coding questions or questions with images. Topics include: (i) basic machine learning principles; (ii) perceptrons; (iii) feature extraction and selection; (iv) logistic regression; (v) regression; (vi) neural networks; (vii) advanced neural networks; (viii) convolutional neural networks; (ix) recurrent neural networks; (x) state machines and MDPs; (xi) reinforcement learning; and (xii) decision trees. Our system uses Transformer models within an encoder-decoder architecture with graph and tree representations. An important aspect of our approach is a data-augmentation scheme for generating new example problems. We also train a machine learning model to generate problem hints. Thus, our system automatically generates new questions across topics, answers both open-response questions and multiple-choice questions, classifies problems, and generates problem hints, pushing the envelope of AI for STEM education.",0
"One approach to solving machine learning problems is through the use of transfer learning techniques. Transfer learning refers to training an existing model on new data, allowing for more efficient optimization by leveraging previously learned features. This method has been shown to improve generalization performance across a range of applications, including image classification and speech recognition tasks. In this work, we propose a novel transfer learning algorithm that utilizes dynamic weight generation methods to adaptively update the model parameters during fine-tuning. Our method outperforms traditional fine-tuning approaches, achieving state-of-the-art results on popular benchmark datasets.",1
"Many clustering algorithms are guided by certain cost functions such as the widely-used $k$-means cost. These algorithms divide data points into clusters with often complicated boundaries, creating difficulties in explaining the clustering decision. In a recent work, Dasgupta, Frost, Moshkovitz, and Rashtchian (ICML'20) introduced explainable clustering, where the cluster boundaries are axis-parallel hyperplanes and the clustering is obtained by applying a decision tree to the data. The central question here is: how much does the explainability constraint increase the value of the cost function?   Given $d$-dimensional data points, we show an efficient algorithm that finds an explainable clustering whose $k$-means cost is at most $k^{1 - 2/d}\mathrm{poly}(d\log k)$ times the minimum cost achievable by a clustering without the explainability constraint, assuming $k,d\ge 2$. Combining this with an independent work by Makarychev and Shan (ICML'21), we get an improved bound of $k^{1 - 2/d}\mathrm{polylog}(k)$, which we show is optimal for every choice of $k,d\ge 2$ up to a poly-logarithmic factor in $k$. For $d = 2$ in particular, we show an $O(\log k\log\log k)$ bound, improving exponentially over the previous best bound of $\widetilde O(k)$.",0
"This paper presents a new algorithm called ""Near-Optimal Explainable k-means"" (NOXKM) which optimizes clustering performance while simultaneously providing insights into how clusters form through explainability techniques. Previous methods have often prioritized one goal over another, but NOXKM achieves near-optimal results while maintaining transparency. We evaluate our method against several state-of-the-art alternatives on both synthetic and real-world datasets and demonstrate that NOXKM outperforms competitors in terms of accuracy and interpretability. Our approach represents a significant step forward towards more effective and accessible data analysis tools.",1
"Gradual argumentation frameworks represent arguments and their relationships in a weighted graph. Their graphical structure and intuitive semantics makes them a potentially interesting tool for interpretable machine learning. It has been noted recently that their mechanics are closely related to neural networks, which allows learning their weights from data by standard deep learning frameworks. As a first proof of concept, we propose a genetic algorithm to simultaneously learn the structure of argumentative classification models. To obtain a well interpretable model, the fitness function balances sparseness and accuracy of the classifier. We discuss our algorithm and present first experimental results on standard benchmarks from the UCI machine learning repository. Our prototype learns argumentative classification models that are comparable to decision trees in terms of learning performance and interpretability.",0
"Gradual argumentation frameworks are graphical models used to represent arguments where each node represents either an attack on another proposition (i.e., evidence that the proposition is false) or support for another proposition (i.e., evidence that the proposition is true). These frameworks can be used to reason about the acceptability of propositions based on their relationships with other propositions and attacks. In recent years, there has been increasing interest in automating the construction of gradual argumentation frameworks. One approach to doing so is through genetic algorithms, which use principles of natural selection to evolve solutions to problems over time by selecting better performing individuals from successive generations. This paper presents a new method for learning gradual argumentation frameworks using genetic algorithms. We show how our approach outperforms previous methods both quantitatively and qualitatively, producing more accurate and robust argumentative structures that better capture real-world reasoning processes. Overall, our results demonstrate the effectiveness of genetic algorithmic approaches to learning gradual argumentation frameworks, holding promise for future applications in artificial intelligence and computer science fields such as machine learning, expert systems, multiagent systems and others.",1
"We address the problem of inferring descriptions of system behavior using Linear Temporal Logic (LTL) from a finite set of positive and negative examples. Most of the existing approaches for solving such a task rely on predefined templates for guiding the structure of the inferred formula. The approaches that can infer arbitrary LTL formulas, on the other hand, are not robust to noise in the data. To alleviate such limitations, we devise two algorithms for inferring concise LTL formulas even in the presence of noise. Our first algorithm infers minimal LTL formulas by reducing the inference problem to a problem in maximum satisfiability and then using off-the-shelf MaxSAT solvers to find a solution. To the best of our knowledge, we are the first to incorporate the usage of MaxSAT solvers for inferring formulas in LTL. Our second learning algorithm relies on the first algorithm to derive a decision tree over LTL formulas based on a decision tree learning algorithm. We have implemented both our algorithms and verified that our algorithms are efficient in extracting concise LTL descriptions even in the presence of noise.",0
"""Abstract"" should appear at the beginning. ------ Abstract: This paper presents a novel approach using a weighted sum of boolean satisfiability (MaxSAT) problems as a framework for learning linear temporal properties from noisy data. We propose an iterative method that uses noise models and constraint generation techniques to increase the robustness and accuracy of property inference. Our algorithm efficiently infers nonlinear temporal properties while mitigating the effect of imprecise observation data. In addition, we introduce novel search heuristics based on variable elimination that significantly reduce computational cost without sacrificing precision. Our experimental results demonstrate the superiority of our approach compared to state-of-the-art methods across diverse domains including autonomous systems control, sensor validation, and system diagnosis. Overall, our work contributes new insights into knowledge representation and reasoning under uncertainty, paving the way towards more reliable machine learning applications. Keywords: MaxSat, Linear Temporal Logic, Property Inference, Knowledge Representation and Reasoning",1
"Federated learning is emerging as a machine learning technique that trains a model across multiple decentralized parties. It is renowned for preserving privacy as the data never leaves the computational devices, and recent approaches further enhance its privacy by hiding messages transferred in encryption. However, we found that despite the efforts, federated learning remains privacy-threatening, due to its interactive nature across different parties. In this paper, we analyze the privacy threats in industrial-level federated learning frameworks with secure computation, and reveal such threats widely exist in typical machine learning models such as linear regression, logistic regression and decision tree. For the linear and logistic regression, we show through theoretical analysis that it is possible for the attacker to invert the entire private input of the victim, given very few information. For the decision tree model, we launch an attack to infer the range of victim's private inputs. All attacks are evaluated on popular federated learning frameworks and real-world datasets.",0
"Effective data privacy protection has become increasingly important as massive amounts of sensitive information are collected by organizations every day. Many companies use machine learning techniques like federated learning (FL) to analyze their data without exposing private user information. While FL offers significant benefits over traditional centralized machine learning approaches, there remain unsolved challenges associated with ensuring that individual users’ private information remains safe during model training. This paper aims to provide a thorough analysis of privacy threats in FL systems, identifying potential attack vectors that could compromise users’ personal information. The research suggests ways to mitigate these threats and improve overall security for individuals participating in distributed machine learning efforts. By addressing privacy concerns head-on, we hope to promote further development and adoption of effective federated learning technologies while maintaining strong ethical standards.",1
"Explainability techniques for Graph Neural Networks still have a long way to go compared to explanations available for both neural and decision decision tree-based models trained on tabular data. Using a task that straddles both graphs and tabular data, namely Entity Matching, we comment on key aspects of explainability that are missing in GNN model explanations.",0
"Graph Neural Network (GNN) models have gained significant attention as powerful tools for analyzing graph data such as social networks, recommendation systems, protein interaction graphs, knowledge bases, etc. However, one key challenge that arises in deploying GNNs for real world applications is providing easy-to-understand explanations/explanatory inputs to users so they can interpret/trust their predictions. Inspired by recent works on explaining tabular data using natural language questions, we study how we might adapt these techniques towards explaining GNN outputs. We first discuss challenges specific to explaining GNNs like large neighborhood sizes and inherent noise in node representations due to message passing. We then propose a novel methodology to generate human interpretable explanations using natural language queries that directly ask GNNs why certain nodes were predicted as belonging to certain classes. Our approach combines both generative models of tabular explanations with retrieval strategies based on textual similarity search to achieve efficient answers for arbitrary size GNN datasets while ensuring high fidelity in capturing important patterns of the underlying distributions. Experiments on multiple real world benchmark GNN datasets demonstrate effectiveness of our methods via user studies and automated evaluation metrics over generated rationales. Finally, we highlight extensions towards explainable AI in other non-graph domains including tabular settings where our work provides alternative perspectives. This research has potential impact across many downstream industries utilizing predictive modeling tools trained using deep learning such as healthcare diagnostics, fraud detection, autonomous decision making for robots, customer behavior analysis, climate science, biotech drug design, computational material discovery, etc.",1
"The diagnosis of blood-based diseases often involves identifying and characterizing patient blood samples. Automated methods to detect and classify blood cell subtypes have important medical applications. Automated medical image processing and analysis offers a powerful tool for medical diagnosis. In this work we tackle the problem of white blood cell classification based on the morphological characteristics of their outer contour, color. The work we would explore a set of preprocessing and segmentation (Color-based segmentation, Morphological processing, contouring) algorithms along with a set of features extraction methods (Corner detection algorithms and Histogram of Gradients(HOG)), dimensionality reduction algorithms (Principal Component Analysis(PCA)) that are able to recognize and classify through various Unsupervised(k-nearest neighbors) and Supervised (Support Vector Machine, Decision Trees, Linear Discriminant Analysis, Quadratic Discriminant Analysis, Naive Bayes) algorithms different categories of white blood cells to Eosinophil, Lymphocyte, Monocyte, and Neutrophil. We even take a step forwards to explore various Deep Convolutional Neural network architecture (Sqeezent, MobilenetV1,MobilenetV2, InceptionNet etc.) without preprocessing/segmentation and with preprocessing. We would like to explore many algorithms to identify the robust algorithm with least time complexity and low resource requirement. The outcome of this work can be a cue to selection of algorithms as per requirement for automated blood cell classification.",0
"This paper presents a comprehensive computer vision approach towards automating blood cell analysis by classifying different types of blood cells into their corresponding categories using deep learning techniques. We use transfer learning from pre-trained models such as InceptionV3, ResNet-50 and MobileNets to train our model on a dataset consisting of images captured under microscope at varying magnifications. After extensive experiments, we achieved a test accuracy of over 92% across 6 classes - Basophils, Eosinophils, Lymphocytes, Monocytes, Neutrophils and Thrombocytes, outperforming prior art by margin on all three evaluation metrics: precision, recall and F1-score. Our work shows that the proposed method has potential to reduce human error, increase throughput and enhance the capabilities of traditional microscopic approaches in hematological research fields like medical diagnostics, pathology, pharmaceuticals etc. As far as we know, this is the first complete end-to-end classification framework aimed specifically at blood cell multi-class image recognition for clinical purposes which makes this approach more efficient than current state-of-the-art methods. Our future work includes exploring unsupervised domain adaptation to handle new data modalities and extending this technique to other areas such as retinal blood vessel segmentation, chest x-ray lesion detection etc.",1
"The necessity of deep learning for tabular data is still an unanswered question addressed by a large number of research efforts. The recent literature on tabular DL proposes several deep architectures reported to be superior to traditional ""shallow"" models like Gradient Boosted Decision Trees. However, since existing works often use different benchmarks and tuning protocols, it is unclear if the proposed models universally outperform GBDT. Moreover, the models are often not compared to each other, therefore, it is challenging to identify the best deep model for practitioners.   In this work, we start from a thorough review of the main families of DL models recently developed for tabular data. We carefully tune and evaluate them on a wide range of datasets and reveal two significant findings. First, we show that the choice between GBDT and DL models highly depends on data and there is still no universally superior solution. Second, we demonstrate that a simple ResNet-like architecture is a surprisingly effective baseline, which outperforms most of the sophisticated models from the DL literature. Finally, we design a simple adaptation of the Transformer architecture for tabular data that becomes a new strong DL baseline and reduces the gap between GBDT and DL models on datasets where GBDT dominates.",0
"In recent years, deep learning models have become increasingly popular for handling tabular data due to their ability to automatically extract features from raw input and achieve state-of-the-art performance on various tasks such as classification, regression, clustering, etc. However, there are still several challenges that need to be addressed before these models can be used effectively in real-world applications, including interpretability, scalability, and robustness against noise and outliers. In this paper, we aim to revisit some existing deep learning architectures commonly used for tabular data analysis and explore potential solutions to overcome these limitations. We perform extensive experiments using both synthetic datasets and real-world benchmarks and compare our results against traditional machine learning algorithms. Our findings show promising improvements over baseline methods across all three dimensions mentioned above, demonstrating the feasibility and effectiveness of applying deep learning techniques to handle complex tabular problems with high stakes. Furthermore, we provide insights into the design choices and hyperparameter tuning strategies for achieving optimal model performance while maintaining transparency and explainability. Overall, this work serves as a step towards bridging the gap between theory and practice in deep learning for tabular data and paves the way for future research directions towards more advanced and generalizable solutions.",1
"Tabular datasets are the last ""unconquered castle"" for deep learning, with traditional ML methods like Gradient-Boosted Decision Trees still performing strongly even against recent specialized neural architectures. In this paper, we hypothesize that the key to boosting the performance of neural networks lies in rethinking the joint and simultaneous application of a large set of modern regularization techniques. As a result, we propose regularizing plain Multilayer Perceptron (MLP) networks by searching for the optimal combination/cocktail of 13 regularization techniques for each dataset using a joint optimization over the decision on which regularizers to apply and their subsidiary hyperparameters. We empirically assess the impact of these regularization cocktails for MLPs on a large-scale empirical study comprising 40 tabular datasets and demonstrate that (i) well-regularized plain MLPs significantly outperform recent state-of-the-art specialized neural network architectures, and (ii) they even outperform strong traditional ML methods, such as XGBoost.",0
"In recent years, deep neural networks have achieved state-of-the-art results across a wide range of tasks, from image classification to natural language processing. However, their success often comes at the cost of increased complexity and computational requirements. This raises the question whether simple neural networks that process tabular data would perform well if regularized appropriately. Our study investigates this issue by testing the performance of a variety of common regularizers applied to a feedforward neural network (FNN) model trained on three real world datasets. We show that FNNs, even without hidden layers, can achieve competitive accuracy compared to more complex models such as convolutional neural networks (CNNs), and support vector machines (SVM). More importantly, we demonstrate that regularization plays a crucial role in achieving high accuracy. Specifically, dropout regularization yields consistent improvement over other methods including weight decay and early stopping. Lastly, our results suggest that for problems involving tabular data, simpler architectures may suffice provided they are adequately regularized.",1
"This paper shows that decision trees constructed with Classification and Regression Trees (CART) methodology are universally consistent in an additive model context, even when the number of predictor variables scales exponentially with the sample size, under certain $1$-norm sparsity constraints. The consistency is universal in the sense that there are no a priori assumptions on the distribution of the predictor variables. Amazingly, this adaptivity to (approximate or exact) sparsity is achieved with a single tree, as opposed to what might be expected for an ensemble. Finally, we show that these qualitative properties of individual trees are inherited by Breiman's random forests. Another surprise is that consistency holds even when the ""mtry"" tuning parameter vanishes as a fraction of the number of predictor variables, thus speeding up computation of the forest. A key step in the analysis is the establishment of an oracle inequality, which precisely characterizes the goodness-of-fit and complexity tradeoff for a misspecified model.",0
"In this paper we address the following question: When should one trust decision trees that they have grown on datasets that may contain spurious relationships? To study this problem we present new upper bounds on the VC dimension of decision tree ensembles that hold over all dimensions n. Our results rely crucially on recent advances in statistical learning theory and high dimensional geometry; specifically, we need (i) concentration bounds for uniform measure over arbitrary sets; and (ii) concentration bounds for Gaussian processes indexed by unit vectors under the uniform probability measure in the high dimensional sphere. Using these ingredients we prove that randomised decision trees in any fixed space R^n with depth d=O(log n), constructed by picking half spaces uniformly at random up to a given maximum degree can achieve zero misclassification error at rate O(exp(-ckd)), where c>0 depends only on epsilon_0. We then use existing tools from uniform convergence bounds for kNN algorithms together with Talagrand's generic chaining inequality and the union bound to obtain the following consequence: For every distribution D supported on R^n there exists a constant C, depending only on ||D||_1=eigenfunction epsion_1 and which may depend on dimensioin n, such that if you take any dataset drawn iid from D according to ||D||_1, run a decision tree ensemble with max outward point constraint equal to the number of data points p=||D||_1, average training set size |T|=n/2 and depth d=O(log n) then there exist universal constants r and R (independent of n!) such that T has radius r with confidence at least R - exp(-ckd). Furthermore",1
"In an ever expanding set of research and application areas, deep neural networks (DNNs) set the bar for algorithm performance. However, depending upon additional constraints such as processing power and execution time limits, or requirements such as verifiable safety guarantees, it may not be feasible to actually use such high-performing DNNs in practice. Many techniques have been developed in recent years to compress or distill complex DNNs into smaller, faster or more understandable models and controllers. This work seeks to identify reduced models that not only preserve a desired performance level, but also, for example, succinctly explain the latent knowledge represented by a DNN. We illustrate the effectiveness of the proposed approach on the evaluation of decision tree variants and kernel machines in the context of benchmark reinforcement learning tasks.",0
"In recent years, deep reinforcement learning has emerged as a powerful paradigm for training agents that can learn complex behaviors from raw sensory input. However, many existing approaches struggle to produce interpretable representations of their behavior. While interpretability is often viewed as a desirable property, there is ongoing debate regarding how important it is in practice. This paper presents two contributions aimed at improving our understanding of the tradeoffs involved: (i) We provide a general framework for constructing interpretable approximations to deep reinforcement learning policies using Gaussian processes; and (ii) we conduct an extensive empirical evaluation comparing the performance and interpretability of these approximations against standard deep reinforcement learning models across a range of challenging domains. Our results show that while deep reinforcement learning models tend to outperform interpretable alternatives in terms of raw performance, they frequently fail to produce meaningful insights into their decision making process. In contrast, our interpretable approximations are able to capture key features of the problem domain and generate more human-like explanations, albeit at the cost of some reduction in overall performance. These findings have significant implications for both theory and applications of artificial intelligence, highlighting the importance of balancing model complexity with interpretability when designing intelligent systems.",1
"Food security is more prominent on the policy agenda today than it has been in the past, thanks to recent food shortages at both the regional and global levels as well as renewed promises from major donor countries to combat chronic hunger. One field where machine learning can be used is in the classification of household food insecurity. In this study, we establish a robust methodology to categorize whether or not a household is being food secure and food insecure by machine learning algorithms. In this study, we have used ten machine learning algorithms to classify the food security status of the Household. Gradient Boosting (GB), Random Forest (RF), Extra Tree (ET), Bagging, K-Nearest Neighbor (KNN), Decision Tree (DT), Support Vector Machine (SVM), Logistic Regression (LR), Ada Boost (AB) and Naive Bayes were the classification algorithms used throughout this study (NB). Then, we perform classification tasks from developing data set for household food security status by gathering data from HICE survey data and validating it by Domain Experts. The performance of all classifiers has better results for all performance metrics. The performance of the Random Forest and Gradient Boosting models are outstanding with a testing accuracy of 0.9997 and the other classifier such as Bagging, Decision tree, Ada Boost, Extra tree, K-nearest neighbor, Logistic Regression, SVM and Naive Bayes are scored 0.9996, 0.09996, 0.9994, 0.95675, 0.9415, 0.8915, 0.7853 and 0.7595, respectively.",0
"This is an abstract for a study that evaluates different classification models on data sets from household income, consumption and expenditure surveys conducted by government agencies worldwide over several decades. These data provide insights into how individuals allocate their budgets among essential goods and services within households. To evaluate these models effectively, we use statistical techniques such as accuracy measures, confusion matrices and KS tests, which allow us to compare how well each model performs against real survey results. We find that some models perform better than others but overall none can capture all the complexity underlying consumer behaviour at the level of detail found across the globe. Our work provides new evidence on both benefits and limitations of applying machine learning algorithms on complex social science problems that aim to inform policy related to poverty alleviation programmes.",1
"The Internet-of-Things, complex sensor networks, multi-agent cyber-physical systems are all examples of spatially distributed systems that continuously evolve in time. Such systems generate huge amounts of spatio-temporal data, and system designers are often interested in analyzing and discovering structure within the data. There has been considerable interest in learning causal and logical properties of temporal data using logics such as Signal Temporal Logic (STL); however, there is limited work on discovering such relations on spatio-temporal data. We propose the first set of algorithms for unsupervised learning for spatio-temporal data. Our method does automatic feature extraction from the spatio-temporal data by projecting it onto the parameter space of a parametric spatio-temporal reach and escape logic (PSTREL). We propose an agglomerative hierarchical clustering technique that guarantees that each cluster satisfies a distinct STREL formula. We show that our method generates STREL formulas of bounded description complexity using a novel decision-tree approach which generalizes previous unsupervised learning techniques for Signal Temporal Logic. We demonstrate the effectiveness of our approach on case studies from diverse domains such as urban transportation, epidemiology, green infrastructure, and air quality monitoring.",0
"In recent years, spatially distributed systems have become increasingly prevalent, ranging from autonomous vehicles and smart cities to industrial control systems and cyber-physical systems. Ensuring their correct operation is crucial for safety, security, efficiency, reliability, and scalability purposes. This work presents a novel approach for mining interpretable spatiotemporal logic properties that describe desirable behavior patterns or defects in such systems. Our method leverages state-of-the-art model checking techniques with symbolic representation and abstraction refinement strategies that improve scalability without compromising accuracy. We demonstrate our method on several case studies across different application domains including autonomous robots navigating in complex environments, self-driving cars interacting with traffic signal controllers, and drones conducting search-and-rescue missions over unstructured terrains. Results show that our technique can effectively identify previously unknown properties, confirm existing ones, discover inconsistencies among requirements and specifications, reduce simulation costs by up to two orders of magnitude compared with standard methods, reveal design flaws earlier during development cycles, and facilitate reasoning and decision making in complex settings.",1
"One of the key challenges when developing a predictive model is the capability to describe the domain knowledge and the cause-effect relationships in a simple way. Decision rules are a useful and important methodology in this context, justifying their application in several areas, in particular in clinical practice. Several machine-learning classifiers have exploited the advantageous properties of decision rules to build intelligent prediction models, namely decision trees and ensembles of trees (ETs). However, such methodologies usually suffer from a trade-off between interpretability and predictive performance. Some procedures consider a simplification of ETs, using heuristic approaches to select an optimal reduced set of decision rules. In this paper, we introduce a novel step to those methodologies. We create a new component to predict if a given rule will be correct or not for a particular patient, which introduces personalization into the procedure. Furthermore, the validation results using three public clinical datasets show that it also allows to increase the predictive performance of the selected set of rules, improving the mentioned trade-off.",0
"In recent years, there has been growing interest in using machine learning algorithms in healthcare, particularly for diagnosis and treatment planning in medical conditions. However, many traditional ML methods suffer from poor interpretability, meaning that their predictions cannot easily be explained or rationalized by human experts. This lack of transparency can make it difficult for clinicians to trust the model's recommendations and limit adoption. One approach that can balance accuracy, interpretability, and customizability is through the use of rule-based models. Rules encode expert knowledge as a set of logical statements that define relationships between features and outcomes, making them transparent and interpretable. But due to high dimensionality and complex interactions among features, building effective rules remains challenging. We propose an algorithm called ""Grammar-Guided Rule Learning"" (GGRL), which builds compact and accurate decision trees from sets of if–then sentences, based on an analogy with natural language. Experiments performed on real medical datasets demonstrate GGRL's potential as a valuable tool in improving patient care through more transparent and personalized decision support. The results show that our method significantly improves both accuracy and interpretability over state-of-the-art baselines while preserving good calibration and stability across cross validation iterations. Furthermore, thanks to its rule format, GGRL naturally allows physicians to refine and tailor the prediction process according to their preferences and new insights from patient data. This study paves the way towards improved collaboration between rule-based systems and human professionals in healthcare, leading to better informed decisions and ultimately enhanced patient outcomes.",1
"We address the problem of learning binary decision trees that partition data for some downstream task. We propose to learn discrete parameters (i.e., for tree traversals and node pruning) and continuous parameters (i.e., for tree split functions and prediction functions) simultaneously using argmin differentiation. We do so by sparsely relaxing a mixed-integer program for the discrete parameters, to allow gradients to pass through the program to continuous parameters. We derive customized algorithms to efficiently compute the forward and backward passes. This means that our tree learning procedure can be used as an (implicit) layer in arbitrary deep networks, and can be optimized with arbitrary loss functions. We demonstrate that our approach produces binary trees that are competitive with existing single tree and ensemble approaches, in both supervised and unsupervised settings. Further, apart from greedy approaches (which do not have competitive accuracies), our method is faster to train than all other tree-learning baselines we compare with. The code for reproducing the results is available at https://github.com/vzantedeschi/LatentTrees.",0
"This work proposes a new method for learning binary decision trees using argmin differentiation, which is derived from gradient descent. We show that our approach outperforms traditional tree learning methods such as ID3 and C4.5 on both synthetic and real-world datasets. Our method achieves state-of-the-art results while remaining simple to implement and computationally efficient. Furthermore, we demonstrate that our method can effectively learn deep trees, which has been previously challenging due to overfitting issues. The contributions of this work include a novel algorithm for learning binary decision trees and comprehensive experiments showing the effectiveness of our approach across multiple domains.",1
"We consider the problem of the extraction of semantic attributes, supervised only with classification labels. For example, when learning to classify images of birds into species, we would like to observe the emergence of features that zoologists use to classify birds. To tackle this problem, we propose training a neural network with discrete features in the last layer, which is followed by two heads: a multi-layered perceptron (MLP) and a decision tree. Since decision trees utilize simple binary decision stumps we expect those discrete features to obtain semantic meaning. We present a theoretical analysis as well as a practical method for learning in the intersection of two hypothesis classes. Our results on multiple benchmarks show an improved ability to extract a set of features that are highly correlated with the set of unseen attributes.",0
"This should summarize your main findings without including data. This paper presents a weakly supervised approach to semantic attribute recovery from natural language text by exploiting knowledge graph embeddings trained on large scale corpora. Our method leverages unlabeled sentences and their corresponding concepts as anchor points to guide the learning process while making use of external resources like WordNet and ConceptNet to further enrich the representation space of our model. Through extensive experiments, we demonstrate that our method can effectively recover fine-grained semantic attributes across diverse domains and outperforms state-of-the-art models on benchmark datasets, achieving new highs in terms of both precision and recall metrics. Overall, our work presents a simple yet effective framework capable of producing high quality results even with limited labeled training data available, opening up exciting opportunities for future research in natural language understanding under constrained settings.  This study explores the problem of recovering semantic attributes from unstructured text using only weak supervision. By utilizing pretrained knowledge graph embeddings along with external resources such as WordNet and ConceptNet, our approach is able to leverage unlabeled examples to learn a high-quality representation of semantic attributes. Experimental results show that our method significantly outperforms existing approaches and sets a new standard in both precision and recall metrics across multiple domains. These promising results highlight the potential of weak supervision for scaling natural language processing tasks in resource-constrained environments. The proposed framework provides a feasible solution to the challenge of dealing with scarce labelled data and opens up possibilities for further investigation in NLP, especially when only small amounts of annotated data are available.",1
"In transfer learning, we wish to make inference about a target population when we have access to data both from the distribution itself, and from a different but related source distribution. We introduce a flexible framework for transfer learning in the context of binary classification, allowing for covariate-dependent relationships between the source and target distributions that are not required to preserve the Bayes decision boundary. Our main contributions are to derive the minimax optimal rates of convergence (up to poly-logarithmic factors) in this problem, and show that the optimal rate can be achieved by an algorithm that adapts to key aspects of the unknown transfer relationship, as well as the smoothness and tail parameters of our distributional classes. This optimal rate turns out to have several regimes, depending on the interplay between the relative sample sizes and the strength of the transfer relationship, and our algorithm achieves optimality by careful, decision tree-based calibration of local nearest-neighbour procedures.",0
"In recent years, there has been increasing interest in developing machine learning algorithms that can learn from data quickly and efficiently. One approach that has gained popularity is transfer learning, which involves using pre-trained models as a starting point for training on new tasks. However, traditional transfer learning approaches often require large amounts of labeled data, which may not always be available. As such, adaptive transfer learning techniques have emerged as a promising alternative, allowing for faster adaptation to novel environments while requiring fewer labeled examples. This work presents a comprehensive overview of the state-of-the art in adaptive transfer learning, including both model-based and metric-learning based methods. We provide a detailed comparison of these different approaches, highlighting their strengths and weaknesses and discussing open research directions. Our results show that adaptive transfer learning methods significantly outperform conventional techniques, offering a powerful toolkit for a wide range of real-world applications.",1
"Gradient Boosting Machines (GBM) are hugely popular for solving tabular data problems. However, practitioners are not only interested in point predictions, but also in probabilistic predictions in order to quantify the uncertainty of the predictions. Creating such probabilistic predictions is difficult with existing GBM-based solutions: they either require training multiple models or they become too computationally expensive to be useful for large-scale settings. We propose Probabilistic Gradient Boosting Machines (PGBM), a method to create probabilistic predictions with a single ensemble of decision trees in a computationally efficient manner. PGBM approximates the leaf weights in a decision tree as a random variable, and approximates the mean and variance of each sample in a dataset via stochastic tree ensemble update equations. These learned moments allow us to subsequently sample from a specified distribution after training. We empirically demonstrate the advantages of PGBM compared to existing state-of-the-art methods: (i) PGBM enables probabilistic estimates without compromising on point performance in a single model, (ii) PGBM learns probabilistic estimates via a single model only (and without requiring multi-parameter boosting), and thereby offers a speedup of up to several orders of magnitude over existing state-of-the-art methods on large datasets, and (iii) PGBM achieves accurate probabilistic estimates in tasks with complex differentiable loss functions, such as hierarchical time series problems, where we observed up to 10% improvement in point forecasting performance and up to 300% improvement in probabilistic forecasting performance.",0
"In recent years, probabilistic gradient boosting machines have emerged as powerful tools for large-scale regression tasks due to their ability to model complex relationships between inputs and outputs. This study proposes a novel approach based on these models that improves upon existing methods by utilizing their inherent uncertainty estimates and scalability features. We demonstrate the effectiveness of our method using simulated data and real-world benchmarks, showing superior performance compared to other state-of-the-art techniques. Our work provides a promising framework for researchers and practitioners seeking solutions to challenging large-scale regression problems across diverse fields such as finance, healthcare, and engineering.",1
"In this paper, we propose a generic model transfer scheme to make Convlutional Neural Networks (CNNs) interpretable, while maintaining their high classification accuracy. We achieve this by building a differentiable decision forest on top of CNNs, which enjoys two characteristics: 1) During training, the tree hierarchies of the forest are learned in a top-down manner under the guidance from the category semantics embedded in the pre-trained CNN weights; 2) During inference, a single decision tree is dynamically selected from the forest for each input sample, enabling the transferred model to make sequential decisions corresponding to the attributes shared by semantically-similar categories, rather than directly performing flat classification. We name the transferred model deep Dynamic Sequential Decision Forest (dDSDF). Experimental results show that dDSDF not only achieves higher classification accuracy than its conuterpart, i.e., the original CNN, but has much better interpretability, as qualitatively it has plausible hierarchies and quantitatively it leads to more precise saliency maps.",0
"""Interpretability has become an increasingly important consideration in recent years as neural networks have grown more complex and powerful. Convolutional Neural Networks (CNNs) are particularly opaque due to their reliance on hierarchical feature extraction. One approach to make these models interpretable is to use dynamic sequential decision forests that build on top of the existing hierarchy. Our method utilizes top-down hierarchy learning to create highly efficient decision trees from dense features extracted from deep convolutional layers. These trees can then be analyzed to provide insight into which regions contributed most significantly to the final prediction. We demonstrate the effectiveness of our approach through experiments on multiple benchmark datasets, including CIFAR10, STL10 and ImageNet.""",1
"Recent work proposed $\delta$-relevant inputs (or sets) as a probabilistic explanation for the predictions made by a classifier on a given input. $\delta$-relevant sets are significant because they serve to relate (model-agnostic) Anchors with (model-accurate) PI- explanations, among other explanation approaches. Unfortunately, the computation of smallest size $\delta$-relevant sets is complete for ${NP}^{PP}$, rendering their computation largely infeasible in practice. This paper investigates solutions for tackling the practical limitations of $\delta$-relevant sets. First, the paper alternatively considers the computation of subset-minimal sets. Second, the paper studies concrete families of classifiers, including decision trees among others. For these cases, the paper shows that the computation of subset-minimal $\delta$-relevant sets is in NP, and can be solved with a polynomial number of calls to an NP oracle. The experimental evaluation compares the proposed approach with heuristic explainers for the concrete case of the classifiers studied in the paper, and confirms the advantage of the proposed solution over the state of the art.",0
"In recent years, there has been increasing interest in developing models that can generate natural language explanations for their predictions. These explainable systems have many potential applications, from helping users better understand complex decisions made by artificial intelligence systems to improving transparency in decision making processes. One approach to generating these explanations is through the use of ""relevant sets"". A relevant set is a subset of features used by a model to make predictions, selected based on their importance to the prediction at hand. By providing only the most relevant information to the user, these sets can greatly simplify and clarify explanations, while still retaining accuracy. This paper proposes a method for efficiently calculating and presenting relevant sets as part of a larger framework for generating natural language explanations. We demonstrate the effectiveness of our method using several case studies, showing how relevant sets can improve clarity, precision, and usability compared to other explanation methods. Our work contributes to the broader goal of building more transparent, interpretable machine learning algorithms.",1
"An Objective Structured Practical Examination (OSPE) is an effective and robust, but resource-intensive, means of evaluating anatomical knowledge. Since most OSPEs employ short answer or fill-in-the-blank style questions, the format requires many people familiar with the content to mark the exams. However, the increasing prevalence of online delivery for anatomy and physiology courses could result in students losing the OSPE practice that they would receive in face-to-face learning sessions. The purpose of this study was to test the accuracy of Decision Trees (DTs) in marking OSPE questions as a potential first step to creating an intelligent, online OSPE tutoring system. The study used the results of the winter 2020 semester final OSPE from McMaster University's anatomy and physiology course in the Faculty of Health Sciences (HTHSCI 2FF3/2LL3/1D06) as the data set. Ninety percent of the data set was used in a 10-fold validation algorithm to train a DT for each of the 54 questions. Each DT was comprised of unique words that appeared in correct, student-written answers. The remaining 10% of the data set was marked by the generated DTs. When the answers marked by the DT were compared to the answers marked by staff and faculty, the DT achieved an average accuracy of 94.49% across all 54 questions. This suggests that machine learning algorithms such as DTs are a highly effective option for OSPE grading and are suitable for the development of an intelligent, online OSPE tutoring system.",0
"This is very important: When using abbreviations that should always be spelled out on first use, you need to provide both versions when searching so the correct version can be selected automatically. For example: ""Automated Grading of Anatomical Objective Structured Practical Exams (OSPEs) Using Decision Trees."" Abbreviations that don't have to be spelled out like ""Dr."", ""Mr."", etc., don't require a second mention if they aren't part of the main body text, but will still need a reference citation listing them as such at least once per article if used therein. If unsure whether an abbreviation needs to be expanded, erring toward expansion is advised unless your style guide specifically forbids it; any omissions in citations of this sort would still result in rejection regardless of allowances elsewhere throughout the article.",1
"Decision trees and their ensembles are very popular models of supervised machine learning. In this paper we merge the ideas underlying decision trees, their ensembles and FCA by proposing a new supervised machine learning model which can be constructed in polynomial time and is applicable for both classification and regression problems. Specifically, we first propose a polynomial-time algorithm for constructing a part of the concept lattice that is based on a decision tree. Second, we describe a prediction scheme based on a concept lattice for solving both classification and regression tasks with prediction quality comparable to that of state-of-the-art models.",0
"Title: Comparing Decision Concept Lattices and Decision Trees/Random Forests  Decision making in complex domains involves analyzing large amounts of data, which can often lead to confusion and uncertainty among stakeholders involved in decision processes. In such cases, decision support systems (DSS) may provide valuable insights that facilitate better understanding and make more informed decisions possible. Two popular DSS techniques used today are Decision Concept Lattices (DCLs) and Decision Trees (DTs)/Random Forests (RF). However, despite their widespread use, there exists limited research comparing these two approaches head to head using real datasets. This article compares the performance of both techniques based on accuracy metrics, interpretability, scalability, and computational efficiency, providing insights into how they differ from each other depending on certain conditions. The study shows the strengths and weaknesses of both methods, offering guidance to practitioners who are considering selecting one approach over another to enhance decision quality.  Keywords: Decision Concept Lattice, Decision Tree, Random Forest, Decision Support System, Accuracy Metrics, Interpretability, Scalability, Computational Efficiency",1
"Rapid advancements in deep learning have led to many recent breakthroughs. While deep learning models achieve superior performance, often statistically better than humans, their adaption into safety-critical settings, such as healthcare or self-driving cars is hindered by their inability to provide safety guarantees or to analyze the inner workings of the model. We present MoET, a novel model based on Mixture of Experts, consisting of decision tree experts and a generalized linear model gating function. While decision boundaries of decision trees (used in an existing verifiable approach), are axis-perpendicular hyperplanes, MoET supports hyperplanes of arbitrary orientation as the boundaries. To support non-differentiable decision trees as experts we formulate a novel training procedure. In addition, we introduce a hard thresholding version, MoET_h, in which predictions are made solely by a single expert chosen via the gating function. Thanks to that property, MoET_h allows each prediction to be easily decomposed into a set of logical rules. Such rules can be translated into a manageable SMT formula providing rich means for verification. While MoET is a general use model, we illustrate its power in the reinforcement learning setting. By training MoET models using an imitation learning procedure on deep RL agents we outperform the previous state-of-the-art technique based on decision trees while preserving the verifiability of the models.",0
"In recent years there has been a growing interest in developing machine learning algorithms that can learn from few examples in order to make predictions on unseen data. One approach that has gained popularity recently is called mixture of expert trees (MoET), which allows for efficient modeling of complex relationships using tree ensembles.  In this paper we introduce MoET and demonstrate how it can be used in conjunction with verifiable reinforcement learning (VRL) to improve the efficiency and safety of decision making systems. We showcase our methodology through several experiments in different domains such as traffic sign recognition and stock price prediction, where MoET achieved state-of-the-art results while adhering to VRL constraints. Our findings suggest that MoET may have great potential in applications that require both high accuracy and strong interpretability. Overall, we believe that the combination of MoET and VRL holds significant promise for advancing artificial intelligence research towards more robust, reliable, and explainable models.",1
"In China, stroke is the first leading cause of death in recent years. It is a major cause of long-term physical and cognitive impairment, which bring great pressure on the National Public Health System. Evaluation of the risk of getting stroke is important for the prevention and treatment of stroke in China. A data set with 2000 hospitalized stroke patients in 2018 and 27583 residents during the year 2017 to 2020 is analyzed in this study. Due to data incompleteness, inconsistency, and non-structured formats, missing values in the raw data are filled with -1 as an abnormal class. With the cleaned features, three models on risk levels of getting stroke are built by using machine learning methods. The importance of ""8+2"" factors from China National Stroke Prevention Project (CSPP) is evaluated via decision tree and random forest models. Except for ""8+2"" factors the importance of features and SHAP1 values for lifestyle information, demographic information, and medical measurement are evaluated and ranked via a random forest model. Furthermore, a logistic regression model is applied to evaluate the probability of getting stroke for different risk levels. Based on the census data in both communities and hospitals from Shanxi Province, we investigate different risk factors of getting stroke and their ranking with interpretable machine learning models. The results show that Hypertension (Systolic blood pressure, Diastolic blood pressure), Physical Inactivity (Lack of sports), and Overweight (BMI) are ranked as the top three high-risk factors of getting stroke in Shanxi province. The probability of getting stroke for a person can also be predicted via our machine learning model.",0
"Stroke can have devastating effects on individuals and their families, but many strokes could be prevented by understanding the underlying risks. In our study, we sought to identify the most significant risk factors contributing to stroke incidence in Shanxi province, China. We analyzed data from over 20,000 patients hospitalized with acute stroke between January 2017 and December 2018. The data was collected using standardized forms developed according to the Guidelines for Early Management of Patients with Acute Cerebrovascular Disease (Chinese Medical Association) as well as medical records and other documents such as imaging reports. Our results indicate that hypertension remains one of the top modifiable risk factors for both men and women. Diabetes mellitus has become increasingly important in recent years and showed increased odds ratios. Furthermore, atrial fibrillation appears to exert greater impact than coronary artery disease on stroke occurrence among males. Nonetheless, lifestyle choices such as smoking and alcohol consumption remain common and potentially preventable risk factors. This study offers valuable insight into specific interventions which may contribute to reducing stroke rates in Shanxi and similar regions. Future research is warranted to confirm these findings and determine whether effective implementation of recommendations aimed at managing these identified risk factors translates into reduced stroke incidence.",1
"Choosing a decision threshold is one of the challenging job in any classification tasks. How much the model is accurate, if the deciding boundary is not picked up carefully, its entire performance would go in vain. On the other hand, for imbalance classification where one of the classes is dominant over another, relying on the conventional method of choosing threshold would result in poor performance. Even if the threshold or decision boundary is properly chosen based on machine learning strategies like SVM and decision tree, it will fail at some point for dynamically varying databases and in case of identity-features that are more or less similar, like in face recognition and person re-identification models. Hence, with the need for adaptability of the decision threshold selection for imbalanced classification and incremental database size, an online optimization-based statistical feature learning adaptive technique is developed and tested on the LFW datasets and self-prepared athletes datasets. This method of adopting adaptive threshold resulted in 12-45% improvement in the model accuracy compared to the fixed threshold {0.3,0.5,0.7} that are usually taken via the hit-and-trial method in any classification and identification tasks. Source code for the complete algorithm is available at: https://github.com/Varat7v2/adaptive-threshold",0
"This paper presents the use of adaptive thresholding techniques for online object recognition and re-id tasks. Our approach works by applying different threshold values based on varying light conditions and image quality. We demonstrate that our method outperforms traditional static thresholding approaches by achieving higher accuracy rates and faster processing times across multiple benchmark datasets. Additionally, we showcase real-world applications where adaptive thresholding can improve efficiency in public surveillance systems.",1
"Machine learning is often applied in health science to obtain predictions and new understandings of complex phenomena and relationships, but an availability of sufficient data for model training is a widespread problem. Traditional machine learning techniques, such as random forests and gradient boosting, tend to overfit when working with data sets of only a few hundred observations. This study demonstrates that for small training sets of 250 observations, symbolic regression generalises better to out-of-sample data than traditional machine learning frameworks, as measured by the coefficient of determination R2 on the validation set. In 132 out of 240 cases, symbolic regression achieves a higher R2 than any of the other models on the out-of-sample data. Furthermore, symbolic regression also preserves the interpretability of linear models and decision trees, an added benefit to its superior generalisation. The second best algorithm was found to be a random forest, which performs best in 37 of the 240 cases. When restricting the comparison to interpretable models, symbolic regression performs best in 184 out of 240 cases.",0
"In the world of machine learning, symbolic regression has emerged as a powerful technique that can generate mathematical expressions from experimental data. Recent studies have shown that symbolic regression can effectively model complex relationships among variables in a wide range of domains including biology, chemistry, finance, engineering, and physics. However, there remains controversy over whether symbolic regression should be used instead of traditional modeling techniques such as linear regression, decision trees, neural networks, and support vector machines. In our recent study, we compared the performance of symbolic regression against these alternative approaches using both simulation experiments and real-world datasets. Our findings suggest that symbolic regression outperformed all four competitor methods across every metric considered on average, particularly for small datasets which was counterintuitive since smaller data sets often lead to less accurate models. The results indicate that researchers may benefit by considering the use of symbolic regression as their method-of-choice. The implications of these findings could change the face of machine learning research, revolutionizing how researchers approach problems ranging from bioinformatics to computational astronomy. Overall, these findings raise new questions regarding how symbolic regression works at a fundamental level, suggesting further investigation into why symbolic regression performs so well even on very small datasets.",1
"We present a workflow for clinical data analysis that relies on Bayesian Structure Learning (BSL), an unsupervised learning approach, robust to noise and biases, that allows to incorporate prior medical knowledge into the learning process and that provides explainable results in the form of a graph showing the causal connections among the analyzed features. The workflow consists in a multi-step approach that goes from identifying the main causes of patient's outcome through BSL, to the realization of a tool suitable for clinical practice, based on a Binary Decision Tree (BDT), to recognize patients at high-risk with information available already at hospital admission time. We evaluate our approach on a feature-rich COVID-19 dataset, showing that the proposed framework provides a schematic overview of the multi-factorial processes that jointly contribute to the outcome. We discuss how these computational findings are confirmed by current understanding of the COVID-19 pathogenesis. Further, our approach yields to a highly interpretable tool correctly predicting the outcome of 85% of subjects based exclusively on 3 features: age, a previous history of chronic obstructive pulmonary disease and the PaO2/FiO2 ratio at the time of arrival to the hospital. The inclusion of additional information from 4 routine blood tests (Creatinine, Glucose, pO2 and Sodium) increases predictive accuracy to 94.5%.",0
"Causal Learning Framework for the Analysis and Interpretation of COVID-19 Clinical Data: A Paper Abstract  Covid-19 has been a global pandemic that has affected millions worldwide. In order to effectively address the crisis, medical professionals have relied on vast amounts of clinical data. However, analyzing and interpreting this data can be challenging due to confounding variables, selection bias, and other limitations. This paper presents a novel framework for analyzing and interpreting COVID-19 clinical data using causal reasoning techniques.  The proposed framework utilizes structural equation modeling (SEM) to identify relationships among key variables such as age, gender, smoking status, underlying health conditions, and treatment outcomes. By incorporating counterfactual theory and propensity score matching methods, we can assess the causal effects of different treatments and interventions on patient outcomes. Furthermore, we validate our approach by comparing results from SEM with those obtained through traditional regression models and randomized controlled trials.  Our findings demonstrate that the use of causal learning frameworks improves the accuracy and precision of clinical data analyses. We provide evidence supporting the effectiveness of certain treatments over others and highlight areas where further research is necessary. Overall, our work contributes to the development of better evidence-based practices for managing and preventing infectious diseases like Covid-19.",1
"Over the past decade, wind energy has gained more attention in the world. However, owing to its indirectness and volatility properties, wind power penetration has increased the difficulty and complexity in dispatching and planning of electric power systems. Therefore, it is needed to make the high-precision wind power prediction in order to balance the electrical power. For this purpose, in this study, the prediction performance of linear regression, k-nearest neighbor regression and decision tree regression algorithms is compared in detail. k-nearest neighbor regression algorithm provides lower coefficient of determination values, while decision tree regression algorithm produces lower mean absolute error values. In addition, the meteorological parameters of wind speed, wind direction, barometric pressure and air temperature are evaluated in terms of their importance on the wind power parameter. The biggest importance factor is achieved by wind speed parameter. In consequence, many useful assessments are made for wind power predictions.",0
"This paper compares the performance of different machine learning algorithms on predicting wind turbine power generation. Wind energy has become increasingly important as a source of renewable energy, and accurate prediction of power generation can greatly benefit both energy providers and consumers. In this study, four commonly used machine learning algorithms - Decision Trees, Random Forest, Support Vector Machines (SVM), and Artificial Neural Networks (ANN) - were trained using historical data from operating wind farms. The models were evaluated based on their accuracy, precision, recall, F1 score, and mean squared error. Results showed that ANN outperformed other algorithms in terms of all evaluation metrics, followed by SVM and decision trees. However, random forest did not perform well in comparison. These findings have implications for future research and development of more advanced machine learning techniques for enhanced wind power predictions.",1
"Decision tree learning is a widely used approach in machine learning, favoured in applications that require concise and interpretable models. Heuristic methods are traditionally used to quickly produce models with reasonably high accuracy. A commonly criticised point, however, is that the resulting trees may not necessarily be the best representation of the data in terms of accuracy and size. In recent years, this motivated the development of optimal classification tree algorithms that globally optimise the decision tree in contrast to heuristic methods that perform a sequence of locally optimal decisions. We follow this line of work and provide a novel algorithm for learning optimal classification trees based on dynamic programming and search. Our algorithm supports constraints on the depth of the tree and number of nodes. The success of our approach is attributed to a series of specialised techniques that exploit properties unique to classification trees. Whereas algorithms for optimal classification trees have traditionally been plagued by high runtimes and limited scalability, we show in a detailed experimental study that our approach uses only a fraction of the time required by the state-of-the-art and can handle datasets with tens of thousands of instances, providing several orders of magnitude improvements and notably contributing towards the practical realisation of optimal decision trees.",0
"This paper presents a novel algorithm called MurTree that solves a classic machine learning problem, namely how to split data into different classes using decision trees. Our approach uses dynamic programming, search techniques, and several heuristics to construct optimal binary classification trees (BCTs) efficiently. We benchmark our method against state-of-the-art algorithms on multiple datasets and show that MurTree significantly outperforms them in terms of accuracy, speed, and memory usage. Additionally, we provide insights into the properties and characteristics of BCTs by analyzing their structures and node distributions. Finally, we discuss potential extensions and future work related to multi-label classifications and non-binary splits. Overall, our contributions demonstrate the effectiveness of dynamic programming combined with search techniques for solving complex optimization problems in machine learning, particularly those involving decision tree construction.",1
"We give a quasipolynomial-time algorithm for learning stochastic decision trees that is optimally resilient to adversarial noise. Given an $\eta$-corrupted set of uniform random samples labeled by a size-$s$ stochastic decision tree, our algorithm runs in time $n^{O(\log(s/\varepsilon)/\varepsilon^2)}$ and returns a hypothesis with error within an additive $2\eta + \varepsilon$ of the Bayes optimal. An additive $2\eta$ is the information-theoretic minimum.   Previously no non-trivial algorithm with a guarantee of $O(\eta) + \varepsilon$ was known, even for weaker noise models. Our algorithm is furthermore proper, returning a hypothesis that is itself a decision tree; previously no such algorithm was known even in the noiseless setting.",0
"This is an important paper that presents novel methods for learning stochastic decision trees (SDTs) using statistical techniques. SDTs have been shown to provide excellent performance in a wide variety of applications such as prediction and classification tasks across different domains including finance, healthcare, social media analysis and natural language processing among others. While previous work has focused on training SDTs using heuristics based approaches or ensemble learning frameworks, this study proposes new methods that leverage recent advances in probabilistic programming languages to learn SDTs from data directly. We demonstrate through experiments conducted on real world datasets that our proposed approach results in significantly better predictive accuracy compared to other state-of-the art techniques available in literature. Our findings have significant implications for researchers interested in building machine learning models that can capture complex relationships present in large scale datasets while providing high levels of interpretability. -----  Abstract: This paper introduces a novel method for learning stochastic decision trees (SDTs) utilizing advanced probabilistic programming techniques. Stochastic decision trees have proven to be exceptionally accurate in fields like finance, medicine, social media analysis, and natural language processing. Previous studies have used rule-based approaches or ensemble strategies to train these models; however, this study takes advantage of modern software tools to improve predictions by constructing decision trees directly from raw data. Through comprehensive testing on actual datasets, we show that our technique yields substantially improved outcomes over existing industry standards. These results suggest exciting possibilities for machine learning experts seeking more effective ways to represent intricate patterns within massive collections of information without sacrificing interpretability.",1
"We introduce a novel clean-label targeted poisoning attack on learning mechanisms. While classical poisoning attacks typically corrupt data via addition, modification and omission, our attack focuses on data omission only. Our attack misclassifies a single, targeted test sample of choice, without manipulating that sample. We demonstrate the effectiveness of omission attacks against a large variety of learners including deep neural networks, SVM and decision trees, using several datasets including MNIST, IMDB and CIFAR. The focus of our attack on data omission only is beneficial as well, as it is simpler to implement and analyze. We show that, with a low attack budget, our attack's success rate is above 80%, and in some cases 100%, for white-box learning. It is systematically above the reference benchmark for black-box learning. For both white-box and black-box cases, changes in model accuracy are negligible, regardless of the specific learner and dataset. We also prove theoretically in a simplified agnostic PAC learning framework that, subject to dataset size and distribution, our omission attack succeeds with high probability against any successful simplified agnostic PAC learner.",0
"Abstract: Targeted data sample attacks have recently emerged as a significant threat to machine learning models, where attackers strategically manipulate training data to cause harm by crafting malicious inputs that deceive machine learning systems into making incorrect predictions. In contrast to existing works on targeted evasion attacks, which aim at causing misclassifications by modifying individual instances near the decision boundary of the classifier, we focus on broadly applicable targeted data sample omission attacks (DSOA) designed to achieve high success rates over diverse datasets and models trained across different tasks, including both image classification and text classification scenarios. We present two approaches based on feature importance analysis: one for generating effective queries efficiently and adaptively, while another leverages generative adversarial networks (GANs). Our studies reveal that DSOAs can significantly suppress model performance in most cases, even when using very few modified data samples, demonstrating their feasibility and effectiveness in practice. These findings call attention to new challenges related to the robustness and reliability of modern AI systems. Our work paves the way towards more comprehensive evaluations of AI security and robustness, particularly amidst increasing deployments of these systems in real-world applications.",1
"The black-box nature of machine learning models hinders the deployment of some high-accuracy models in medical diagnosis. It is risky to put one's life in the hands of models that medical researchers do not fully understand. However, through model interpretation, black-box models can promptly reveal significant biomarkers that medical practitioners may have overlooked due to the surge of infected patients in the COVID-19 pandemic.   This research leverages a database of 92 patients with confirmed SARS-CoV-2 laboratory tests between 18th Jan. 2020 and 5th Mar. 2020, in Zhuhai, China, to identify biomarkers indicative of severity prediction. Through the interpretation of four machine learning models, decision tree, random forests, gradient boosted trees, and neural networks using permutation feature importance, Partial Dependence Plot (PDP), Individual Conditional Expectation (ICE), Accumulated Local Effects (ALE), Local Interpretable Model-agnostic Explanations (LIME), and Shapley Additive Explanation (SHAP), we identify an increase in N-Terminal pro-Brain Natriuretic Peptide (NTproBNP), C-Reaction Protein (CRP), and lactic dehydrogenase (LDH), a decrease in lymphocyte (LYM) is associated with severe infection and an increased risk of death, which is consistent with recent medical research on COVID-19 and other research using dedicated models. We further validate our methods on a large open dataset with 5644 confirmed patients from the Hospital Israelita Albert Einstein, at S\~ao Paulo, Brazil from Kaggle, and unveil leukocytes, eosinophils, and platelets as three indicative biomarkers for COVID-19.",0
"Artificial intelligence has been applied successfully in predicting severity levels of patients infected with SARS-CoV2 (COVID-19) by analyzing Electronic Health Records (EHR). In many cases these models have led to high accuracy but lack interpretability which raises concerns among healthcare practitioners in trusting decisions solely based on these predictions. This study presents an empirical evaluation comparing two popular methods of machine learning, Random Forest and XGBoost, using the EHR dataset from MIMIC III. Our focus lies within identifying critical features that impact patient outcomes through feature importance measures available in both models and human interpretation. We believe that interpreting black box models can increase medical professionals’ acceptance towards artificial intelligence assisted decision making, thus improving public health management during pandemics like COVID-19. Ultimately, we aim to provide insights into which subsets of data and what parameters should drive clinicians’ attention upon evaluating potentially severe cases. Keywords: COVID-19, SARS-Cov2; Severity prediction; Empirical evaluation; Feature importance measures; Public health management.",1
"We present the backbone method, a generic framework that enables sparse and interpretable supervised machine learning methods to scale to ultra-high dimensional problems. We solve sparse regression problems with $10^7$ features in minutes and $10^8$ features in hours, as well as decision tree problems with $10^5$ features in minutes. The proposed method operates in two phases; we first determine the backbone set, that consists of potentially relevant features, by solving a number of tractable subproblems; then, we solve a reduced problem, considering only the backbone features. For the sparse regression problem, we show that, under certain assumptions and with high probability, the backbone set consists of the true relevant features. Numerical experiments on both synthetic and real-world datasets demonstrate that our method outperforms or competes with state-of-the-art methods in ultra-high dimensional problems, and competes with optimal solutions in problems where exact methods scale, both in terms of recovering the true relevant features and in its out-of-sample predictive performance.",0
"In recent years, machine learning has become increasingly important in various fields such as computer vision, natural language processing, and scientific modeling. However, many applications require dealing with ultra-high dimensional data (UHD), which pose significant challenges due to the so-called ""curse of dimensionality"" problem. As a result, traditional sparse methods may suffer from low performance or even numerical instability. To address these issues, we introduce a new method called the backbone method for ultra-high dimensional sparse machine learning (BM).  Our approach focuses on constructing a compact and discriminative backbone representation that captures essential features across multiple tasks, which can then be used to guide subsequent task-specific regularization. We show how BM effectively combines global and local structure preservation to improve generalizability while maintaining parsimonious model complexity. Our results demonstrate superior performance over several baseline models on three real-world datasets spanning different domains: web page classification, biological network inference, and brain functional connectivity analysis.  In summary, our proposed method provides a promising solution for UHD sparse machine learning problems by exploiting the underlying structures present in multiple related tasks. Our work opens up opportunities for researchers working in diverse areas involving high-dimensional data to leverage the advantages offered by multi-task regularization under computational constraints, paving the way for more efficient and effective solutions.",1
"The task of morphological classification is complex for simple parameterization, but important for research in the galaxy evolution field. Future galaxy surveys (e.g. EUCLID) will collect data about more than a $10^9$ galaxies. To obtain morphological information one needs to involve people to mark up galaxy images, which requires either a considerable amount of money or a huge number of volunteers. We propose an effective semi-supervised approach for galaxy morphology classification task, based on active learning of adversarial autoencoder (AAE) model. For a binary classification problem (top level question of Galaxy Zoo 2 decision tree) we achieved accuracy 93.1% on the test part with only 0.86 millions markup actions, this model can easily scale up on any number of images. Our best model with additional markup achieves accuracy of 95.5%. To the best of our knowledge it is a first time AAE semi-supervised learning model used in astronomy.",0
"Morphological classification of astronomical images is a very active field of research due to their large scale and variability. However, annotating these datasets can be laborious, time consuming, and subjective. As such, there exists a need to develop automated image recognition methods that can accurately classify objects from the dataset while minimizing human labeling efforts. We address this challenge by introducing a novel morphological classification framework that incorporates texture-based features along with edge detection techniques on both grayscale and color images. Our approach employs an efficient region growing technique to segment the input image into meaningful object components which later serve as inputs to our proposed feature extraction module. By utilizing a minimal number of labeled samples (<1%) combined with relevant meta data such as location in galactic coordinates, we achieve promising results. These findings provide encouragement towards the development of more accurate models capable of handling astronomical images under constrained settings. This work has significant implications towards automation of big data sets, reducing human effort required during annotation process.",1
"As we gain access to a greater depth and range of health-related information about individuals, three questions arise: (1) Can we build better models to predict individual-level risk of ill health? (2) How much data do we need to effectively predict ill health? (3) Are new methods required to process the added complexity that new forms of data bring? The aim of the study is to apply a machine learning approach to identify the relative contribution of personal, social, health-related, biomarker and genetic data as predictors of future health in individuals. Using longitudinal data from 6830 individuals in the UK from Understanding Society (2010-12 to 2015-17), the study compares the predictive performance of five types of measures: personal (e.g. age, sex), social (e.g. occupation, education), health-related (e.g. body weight, grip strength), biomarker (e.g. cholesterol, hormones) and genetic single nucleotide polymorphisms (SNPs). The predicted outcome variable was limiting long-term illness one and five years from baseline. Two machine learning approaches were used to build predictive models: deep learning via neural networks and XGBoost (gradient boosting decision trees). Model fit was compared to traditional logistic regression models. Results found that health-related measures had the strongest prediction of future health status, with genetic data performing poorly. Machine learning models only offered marginal improvements in model accuracy when compared to logistic regression models, but also performed well on other metrics e.g. neural networks were best on AUC and XGBoost on precision. The study suggests that increasing complexity of data and methods does not necessarily translate to improved understanding of the determinants of health or performance of predictive models of ill health.",0
"This study evaluates the utility of integrating different types of data (personal, social, health-related, biomarkers, and genetics) for predicting individual health outcomes over time. Data from a large, diverse sample were collected at baseline and again one year later, allowing for a prospective evaluation of each type of data source. Machine learning models were trained on these datasets and compared for their ability to accurately classify participants based on their predicted health trajectories. Our results suggest that while all forms of data contribute valuable information for prediction purposes, genetic data may have the greatest impact overall due to their unique influence on disease risk and progression. However, our findings must be interpreted cautiously given the limited number of outcome events available for model training and validation, as well as potential sources of bias present in the sample. Further research is necessary to fully assess the clinical implications of these findings and determine optimal integration strategies for multiple types of data in support of precision medicine efforts.",1
"Federated learning (FL) is an emerging paradigm for facilitating multiple organizations' data collaboration without revealing their private data to each other. Recently, vertical FL, where the participating organizations hold the same set of samples but with disjoint features and only one organization owns the labels, has received increased attention. This paper presents several feature inference attack methods to investigate the potential privacy leakages in the model prediction stage of vertical FL. The attack methods consider the most stringent setting that the adversary controls only the trained vertical FL model and the model predictions, relying on no background information. We first propose two specific attacks on the logistic regression (LR) and decision tree (DT) models, according to individual prediction output. We further design a general attack method based on multiple prediction outputs accumulated by the adversary to handle complex models, such as neural networks (NN) and random forest (RF) models. Experimental evaluations demonstrate the effectiveness of the proposed attacks and highlight the need for designing private mechanisms to protect the prediction outputs in vertical FL.",0
This can come later as part of your conclusion paragraph where you refer back to the paper at hand and summarize the key findings. Keeping these tips in mind write an informative and concise summary that provides all necessary background context without going into too much detail which should ideally not exceed one page including keywords and authors (name of the group) listed after keyword list followed by affiliations (also under the author name).,1
"The pandemic COVID-19 disease has had a dramatic impact on almost all countries around the world so that many hospitals have been overwhelmed with Covid-19 cases. As medical resources are limited, deciding on the proper allocation of these resources is a very crucial issue. Besides, uncertainty is a major factor that can affect decisions, especially in medical fields. To cope with this issue, we use fuzzy logic (FL) as one of the most suitable methods in modeling systems with high uncertainty and complexity. We intend to make use of the advantages of FL in decisions on cases that need to treat in ICU. In this study, an interval type-2 fuzzy expert system is proposed for prediction of ICU admission in COVID-19 patients. For this prediction task, we also developed an adaptive neuro-fuzzy inference system (ANFIS). Finally, the results of these fuzzy systems are compared to some well-known classification methods such as Naive Bayes (NB), Case-Based Reasoning (CBR), Decision Tree (DT), and K Nearest Neighbor (KNN). The results show that the type-2 fuzzy expert system and ANFIS models perform competitively in terms of accuracy and F-measure compared to the other system modeling techniques.",0
"In many countries across the globe, Intensive Care Unit (ICU) admission rates among patients diagnosed with COVID-19 have been steadily increasing over time. To address this growing concern, healthcare providers need accurate tools that can predict which patients are at high risk of requiring intensive care. One such tool is fuzzy expert systems, which utilize vague linguistic terms to model complex relationships between variables. This study aimed to develop a fuzzy expert system capable of accurately predicting ICU admissions among COVID-19 patients. Data were collected from 168 patients admitted to two hospitals located in South Korea during the first wave of the pandemic. Using this data, a fuzzy rule base was constructed consisting of four inputs: age, chronic pulmonary disease, underlying liver disease, and mechanical ventilation within seven days after hospitalization. The proposed system achieved an accuracy of 94% in predicting ICU admission. These results indicate that fuzzy expert systems may prove valuable as decision support tools for clinicians managing COVID-19 cases. Further research is warranted to evaluate the generalizability of these findings and assess the impact of incorporating additional patient factors on prediction performance.",1
"Reinforcement learning techniques achieved human-level performance in several tasks in the last decade. However, in recent years, the need for interpretability emerged: we want to be able to understand how a system works and the reasons behind its decisions. Not only we need interpretability to assess the safety of the produced systems, we also need it to extract knowledge about unknown problems. While some techniques that optimize decision trees for reinforcement learning do exist, they usually employ greedy algorithms or they do not exploit the rewards given by the environment. This means that these techniques may easily get stuck in local optima. In this work, we propose a novel approach to interpretable reinforcement learning that uses decision trees. We present a two-level optimization scheme that combines the advantages of evolutionary algorithms with the advantages of Q-learning. This way we decompose the problem into two sub-problems: the problem of finding a meaningful and useful decomposition of the state space, and the problem of associating an action to each state. We test the proposed method on three well-known reinforcement learning benchmarks, on which it results competitive with respect to the state-of-the-art in both performance and interpretability. Finally, we perform an ablation study that confirms that using the two-level optimization scheme gives a boost in performance in non-trivial environments with respect to a one-layer optimization technique.",0
"""Learning interpretable decision trees from data can provide valuable insights into complex problems while maintaining transparency and simplicity. However, traditional tree construction methods suffer from various shortcomings such as suboptimality, overfitting, lack of interpretability, and instability across different runs due to randomness. In recent years, evolutionary algorithms have shown promise in generating high-quality decision trees that overcome these limitations. This study proposes a novel approach called `EvolDecTrees` which integrates advanced features of genetic programming (GP) into tree induction by leveraging existing software tools such as DEAP. We evaluate the effectiveness and efficiency of our method using several benchmark datasets covering diverse domains, demonstrating its superiority compared to state-of-the-art GP variants for decision tree induction in terms of fitness, accuracy, complexity, stability, and interpretability.""",1
"Prototype-based methods use interpretable representations to address the black-box nature of deep learning models, in contrast to post-hoc explanation methods that only approximate such models. We propose the Neural Prototype Tree (ProtoTree), an intrinsically interpretable deep learning method for fine-grained image recognition. ProtoTree combines prototype learning with decision trees, and thus results in a globally interpretable model by design. Additionally, ProtoTree can locally explain a single prediction by outlining a decision path through the tree. Each node in our binary tree contains a trainable prototypical part. The presence or absence of this learned prototype in an image determines the routing through a node. Decision making is therefore similar to human reasoning: Does the bird have a red throat? And an elongated beak? Then it's a hummingbird! We tune the accuracy-interpretability trade-off using ensemble methods, pruning and binarizing. We apply pruning without sacrificing accuracy, resulting in a small tree with only 8 learned prototypes along a path to classify a bird from 200 species. An ensemble of 5 ProtoTrees achieves competitive accuracy on the CUB-200- 2011 and Stanford Cars data sets. Code is available at https://github.com/M-Nauta/ProtoTree",0
"In recent years, fine-grained image recognition has become increasingly important due to its applications in fields such as biology, medicine, and anthropology. However, current methods often suffer from low interpretability and lack of human understanding, making it difficult to trust their results. To address these limitations, we propose a new approach based on neural prototype trees (NPTs). NPTs are learned by clustering similar patches from an input dataset into semantic regions, which can then be used to classify new images using simple shape and color features. Our method achieves state-of-the-art performance while providing interpretable results that are easy for humans to understand. Furthermore, our model is lightweight and efficient, making it suitable for real-world deployment on mobile devices. We demonstrate the effectiveness of our approach on several benchmark datasets and compare it against other popular methods. Overall, our work shows the potential of NPTs as a powerful tool for fine-grained image recognition tasks.",1
"Any intelligent traffic monitoring system must be able to detect anomalies such as traffic accidents in real time. In this paper, we propose a Decision-Tree - enabled approach powered by Deep Learning for extracting anomalies from traffic cameras while accurately estimating the start and end time of the anomalous event. Our approach included creating a detection model, followed by anomaly detection and analysis. YOLOv5 served as the foundation for our detection model. The anomaly detection and analysis step entail traffic scene background estimation, road mask extraction, and adaptive thresholding. Candidate anomalies were passed through a decision tree to detect and analyze final anomalies. The proposed approach yielded an F1 score of 0.8571, and an S4 score of 0.5686, per the experimental validation.",0
"This paper presents a novel approach to traffic anomaly detection using deep learning techniques combined with decision tree algorithms. Our system utilizes computer vision methods to extract relevant features from video footage of road scenes, which are then fed into a convolutional neural network (CNN) for classification. In order to improve the accuracy and robustness of our model, we have incorporated both local binary pattern histograms and pixel intensity values as input features. To further enhance the performance of our method, we have trained separate classifiers for each type of anomaly – namely vehicle accidents and road obstructions – allowing us to accurately detect different types of abnormal events on the roads. The experimental results demonstrate that our proposed framework achieves high precision and recall rates, outperforming existing approaches in many cases. Overall, our system represents a promising step forward in intelligent transportation systems and has significant potential applications in real-world scenarios.",1
"Many decisions involve choosing an uncertain course of actions in deep and wide decision trees, as when we plan to visit an exotic country for vacation. In these cases, exhaustive search for the best sequence of actions is not tractable due to the large number of possibilities and limited time or computational resources available to make the decision. Therefore, planning agents need to balance breadth (exploring many actions at each level of the tree) and depth (exploring many levels in the tree) to allocate optimally their finite search capacity. We provide efficient analytical solutions and numerical analysis to the problem of allocating finite sampling capacity in one shot to large decision trees. We find that in general the optimal policy is to allocate few samples per level so that deep levels can be reached, thus favoring depth over breadth search. In contrast, in poor environments and at low capacity, it is best to broadly sample branches at the cost of not sampling deeply, although this policy is marginally better than deep allocations. Our results provide a theoretical foundation for the optimality of deep imagination for planning and show that it is a generally valid heuristic that could have evolved from the finite constraints of cognitive systems.",0
"Improving planning efficiency is crucial in large decision tree models where computational constraints limit the amount of search that can be performed. One promising approach to achieve efficient planning is by employing imagination, or generating novel state sequences through simulation. However, current methods suffer from high computational cost and may generate irrelevant imagined states, making them impractical. In this work, we present deep imagination, which combines deep learning techniques such as reinforcement learning and neural network based planning algorithms to generate relevant, diverse, and feasible imagination rollouts efficiently. Our method outperforms previous approaches across various benchmark domains, demonstrating the effectiveness of using deep imagination as a near optimal policy for planning in large decision trees under resource limitations. By enabling more efficient exploration of state spaces, our method has the potential to significantly improve planning performance in complex systems. Overall, our results highlight the promise of combining machine learning techniques with classical planning strategies to address real-world planning challenges.",1
"Integrated interpretability without sacrificing the prediction accuracy of decision making algorithms has the potential of greatly improving their value to the user. Instead of assigning a label to an image directly, we propose to learn iterative binary sub-decisions, inducing sparsity and transparency in the decision making process. The key aspect of our model is its ability to build a decision tree whose structure is encoded into the memory representation of a Recurrent Neural Network jointly learned by two models communicating through message passing. In addition, our model assigns a semantic meaning to each decision in the form of binary attributes, providing concise, semantic and relevant rationalizations to the user. On three benchmark image classification datasets, including the large-scale ImageNet, our model generates human interpretable binary decision sequences explaining the predictions of the network while maintaining state-of-the-art accuracy.",0
"This paper introduces a novel approach to learning decision trees through recurring communication with humans. Inspired by human learning processes, which involve constant interaction and refinement of knowledge, we propose a model that improves over time by actively seeking out human feedback on its decisions. Our method leverages recent advances in natural language processing and machine learning, allowing us to create a powerful and versatile tool that can be applied across a wide range of applications. We evaluate our system against state-of-the-art alternatives and demonstrate significant improvements in accuracy and interpretability, paving the way towards more effective decision making in complex domains.",1
"When making decisions, people often overlook critical information or are overly swayed by irrelevant information. A common approach to mitigate these biases is to provide decision-makers, especially professionals such as medical doctors, with decision aids, such as decision trees and flowcharts. Designing effective decision aids is a difficult problem. We propose that recently developed reinforcement learning methods for discovering clever heuristics for good decision-making can be partially leveraged to assist human experts in this design process. One of the biggest remaining obstacles to leveraging the aforementioned methods is that the policies they learn are opaque to people. To solve this problem, we introduce AI-Interpret: a general method for transforming idiosyncratic policies into simple and interpretable descriptions. Our algorithm combines recent advances in imitation learning and program induction with a new clustering method for identifying a large subset of demonstrations that can be accurately described by a simple, high-performing decision rule. We evaluate our new algorithm and employ it to translate information-acquisition policies discovered through metalevel reinforcement learning. The results of large behavioral experiments showed that prividing the decision rules generated by AI-Interpret as flowcharts significantly improved people's planning strategies and decisions across three diferent classes of sequential decision problems. Moreover, another experiment revealed that this approach is significantly more effective than training people by giving them performance feedback. Finally, a series of ablation studies confirmed that AI-Interpret is critical to the discovery of interpretable decision rules. We conclude that the methods and findings presented herein are an important step towards leveraging automatic strategy discovery to improve human decision-making.",0
"In recent years, artificial intelligence (AI) has made significant strides in the field of planning, which involves solving complex problems by decomposing them into simpler subproblems. However, most planners still require extensive manual engineering of domain knowledge, control flow logic, and search procedures. This limits their ability to generalize across tasks and domains, and can lead to brittle solutions that perform poorly under changing conditions.  In this paper, we present a novel approach for automatically discovering interpretable planning strategies using machine learning techniques. Our method leverages large amounts of data collected from human expert demonstrations to learn patterns and structures common to successful plans. These learned patterns can then be used as ""plan skeletons"" to guide the construction of new plans for different situations.  To evaluate our approach, we conducted experiments on a variety of planning benchmarks and compared the performance of our automatically generated plan skeletons against those produced manually by experts. We found that our automatic skeletons were able to achieve comparable levels of success while requiring significantly less time and effort to generate. Furthermore, we demonstrated how these automatically discovered skeletons could serve as building blocks for constructing more advanced, hybrid planners that combine multiple planning paradigms such as forward decomposition and partial order reduction.  Overall, our work represents a step towards creating autonomous agents capable of robust decision making under uncertainty and changing environments, through the use of flexible and adaptive problem solving abilities. Our results indicate promising directions for future research aimed at developing more intelligent AI systems capable of tackling real world problems without relying exclusively on handcrafted rules and heuristics.",1
"The widespread deployment of deep nets in practical applications has lead to a growing desire to understand how and why such black-box methods perform prediction. Much work has focused on understanding what part of the input pattern (an image, say) is responsible for a particular class being predicted, and how the input may be manipulated to predict a different class. We focus instead on understanding which of the internal features computed by the neural net are responsible for a particular class. We achieve this by mimicking part of the neural net with an oblique decision tree having sparse weight vectors at the decision nodes. Using the recently proposed Tree Alternating Optimization (TAO) algorithm, we are able to learn trees that are both highly accurate and interpretable. Such trees can faithfully mimic the part of the neural net they replaced, and hence they can provide insights into the deep net black box. Further, we show we can easily manipulate the neural net features in order to make the net predict, or not predict, a given class, thus showing that it is possible to carry out adversarial attacks at the level of the features. These insights and manipulations apply globally to the entire training and test set, not just at a local (single-instance) level. We demonstrate this robustly in the MNIST and ImageNet datasets with LeNet5 and VGG networks.",0
"Title: Sparse Oblique Decision Trees: A tool for understanding and manipulating neural network features  In recent years, artificial intelligence (AI) has seen rapid advancement due to the success of deep learning methods, particularly convolutional neural networks (CNNs). These models achieve impressive accuracy on various tasks but their decision boundaries and feature representations remain difficult to interpret. In order to apply these techniques effectively in fields such as healthcare and finance where transparency is critical, there remains a need to develop tools that can explain how CNNs make decisions and extract meaningful insights from their learned features. This work presents one possible method towards addressing this problem - sparse oblique decision trees (SODT), which utilize CNN features to grow decision tree structures in a manner analogous to CART. Our results show that SODT performs well compared against traditional random forest classifiers and other methods of interpretation such as saliency maps and guided backpropagation. We discuss use cases where our approach could serve as a valuable addition to current methods of interpretation.",1
"Black-box AI induction methods such as deep reinforcement learning (DRL) are increasingly being used to find optimal policies for a given control task. Although policies represented using a black-box AI are capable of efficiently executing the underlying control task and achieving optimal closed-loop performance, the developed control rules are often complex and neither interpretable nor explainable. In this paper, we use a recently proposed nonlinear decision-tree (NLDT) approach to find a hierarchical set of control rules in an attempt to maximize the open-loop performance for approximating and explaining the pre-trained black-box DRL (oracle) agent using the labelled state-action dataset. Recent advances in nonlinear optimization approaches using evolutionary computation facilitates finding a hierarchical set of nonlinear control rules as a function of state variables using a computationally fast bilevel optimization procedure at each node of the proposed NLDT. Additionally, we propose a re-optimization procedure for enhancing closed-loop performance of an already derived NLDT. We evaluate our proposed methodologies (open and closed-loop NLDTs) on different control problems having multiple discrete actions. In all these problems our proposed approach is able to find relatively simple and interpretable rules involving one to four non-linear terms per rule, while simultaneously achieving on par closed-loop performance when compared to a trained black-box DRL agent. A post-processing approach for simplifying the NLDT is also suggested. The obtained results are inspiring as they suggest the replacement of complicated black-box DRL policies involving thousands of parameters (making them non-interpretable) with relatively simple interpretable policies. Results are encouraging and motivating to pursue further applications of proposed approach in solving more complex control tasks.",0
"This paper presents a novel approach to inducing interpretable artificial intelligence (AI) policies from data. The proposed method uses evolutionary nonlinear decision trees (ENDTs), which are able to model complex relationships between inputs and outputs, while still providing interpretability through their tree structures. The resulting policies can then be used by humans to better understand how decisions were made and to make informed modifications as necessary.  In particular, we focus on discrete action systems, where the AI must choose one of several actions at each time step based on available sensor readings. We propose a new algorithm that iteratively builds ENDT models to predict the probability of success for different action sequences, given a history of observations and previous actions taken. By optimizing these predictions over multiple iterations, our algorithm effectively learns a policy that maximizes expected success rates.  We demonstrate the effectiveness of our approach using two case studies: a robot navigation task and a traffic signal control problem. Our experimental results show that ENDT policies consistently outperform state-of-the-art methods across various metrics, including success rate, distance traveled, and time saved. Moreover, the learned policies are highly interpretable due to their structure as decision trees.  Overall, our work addresses a critical challenge in modern AI research by developing a framework for inducing transparent and explainable policies from real-world datasets. These interpretable policies have the potential to greatly enhance trust in AI systems, allowing users to confidently rely on automation in mission-critical applications.",1
"One of the main sources of error in multi-atlas segmentation propagation approaches comes from the use of atlas databases that are morphologically dissimilar to the target image. In this work, we exploit the segmentation errors associated with poor atlas selection to build a computer aided diagnosis (CAD) system for pathological classification in post-operative dextro-transposition of the great arteries (d-TGA). The proposed approach extracts a set of features, which describe the quality of a segmentation, and introduces them into a logical decision tree that provides the final diagnosis. We have validated our method on a set of 60 whole heart MR images containing healthy cases and two different forms of post-operative d-TGA. The reported overall CAD system accuracy was of 93.33%.",0
"This paper proposes a novel approach for pathological stratification of d-TGA congenital heart disease using multi-atlas analysis. The proposed method leverages recent advances in deep learning and computer vision techniques to analyze cardiac magnetic resonance images (MRI) from patients diagnosed with d-TGA congenital heart disease. By combining multiple expert labeled Atlases, our approach provides more robust and accurate segmentation results compared to single atlas methods. We evaluate the performance of our approach on a large dataset of 4D flow MRI scans and demonstrate improved precision over traditional approaches. Additionally, we provide detailed evaluation of the results obtained by our methodology through careful statistical analysis, demonstrating significant improvements in accuracy across all clinical metrics tested. Overall, this work presents a promising new direction for advanced medical image analysis, leading to better patient outcomes and improved healthcare delivery systems worldwide.",1
"Traditionally, for most machine learning settings, gaining some degree of explainability that tries to give users more insights into how and why the network arrives at its predictions, restricts the underlying model and hinders performance to a certain degree. For example, decision trees are thought of as being more explainable than deep neural networks but they lack performance on visual tasks. In this work, we empirically demonstrate that applying methods and architectures from the explainability literature can, in fact, achieve state-of-the-art performance for the challenging task of domain generalization while offering a framework for more insights into the prediction and training process. For that, we develop a set of novel algorithms including DivCAM, an approach where the network receives guidance during training via gradient based class activation maps to focus on a diverse set of discriminative features, as well as ProDrop and D-Transformers which apply prototypical networks to the domain generalization task, either with self-challenging or attention alignment. Since these methods offer competitive performance on top of explainability, we argue that the proposed methods can be used as a tool to improve the robustness of deep neural network architectures.",0
"In recent years, deep learning has shown remarkable performance on various tasks such as image classification, object detection, and natural language processing. However, these models often lack transparency and interpretability, making it difficult to explain their predictions. This issue becomes even more pronounced in domain generalization where models need to perform well on multiple unseen domains after training only on limited labeled data from a source domain. To address this challenge, we propose an approach that combines visual attention mechanisms with adversarially trained explanations to improve the transferability of neural networks across domains while preserving accuracy on the source domain. We evaluate our method using state-of-the-art benchmarks and demonstrate significant improvements over baseline methods in terms of both accuracy and robustness. Our results suggest that incorporating explainability into domain generalization leads to better performance and greater insight into how these systems make decisions. Overall, this work contributes to a growing effort to bridge the gap between high performance machine learning algorithms and human interpretable explanations.",1
"We show the equivalence of discrete choice models and a forest of binary decision trees. This suggests that standard machine learning techniques based on random forests can serve to estimate discrete choice models with an interpretable output: the underlying trees can be viewed as the internal choice process of customers. Our data-driven theoretical results show that random forests can predict the choice probability of any discrete choice model consistently. Moreover, our algorithm predicts unseen assortments with mechanisms and errors that can be theoretically analyzed. We also prove that the splitting criterion in random forests, the Gini index, is capable of recovering preference rankings of customers. The framework has unique practical advantages: it can capture behavioral patterns such as irrationality or sequential searches; it handles nonstandard formats of training data that result from aggregation; it can measure product importance based on how frequently a random customer would make decisions depending on the presence of the product; it can also incorporate price information and customer features. Our numerical results show that using random forests to estimate customer choices can outperform the best parametric models in synthetic and real datasets when presented with enough data or when the underlying discrete choice model cannot be correctly specified by existing parametric models.",0
"Title: Using Forest Ensembles To Model And Predict discrete choices. Abstract: In recent years, machine learning techniques have become increasingly important tools for analyzing and modeling complex decisions. One particularly powerful approach has been the use of binary choice forest models (BCFs), which can effectively capture patterns in data that traditional methods may miss.  In this work we explore the use of BCFs as a means of understanding and predicting discrete choices. We demonstrate how these models can accurately estimate choice probabilities, and showcase their effectiveness through comparisons against other commonly used techniques.  By leveraging the power of decision trees within our ensemble framework, we demonstrate that binary choice forests provide improved accuracy over classical regression approaches. Furthermore, we highlight cases where BCFs can excel even in high dimensions, where traditional statistical models may struggle.  Our results suggest that binary choice forests offer a promising new tool for researchers studying discrete choices across a variety of domains. This novel technique holds great potential for future applications in fields such as economics, marketing, and political science. --- Title: Exploring Binary Choice Forests for Modelling Discrete Decision Making. Abstract: Understanding human behavior when making discrete choices under uncertainty remains a challenging task in many scientific disciplines. Traditional econometric models often fail to account for the complexity inherent in real world choices. Recently, machine learning algorithms have emerged as a powerful toolset for modelling such preferences. In particular, binary choice forests have shown promising results in estimating individual level choice probability distributions.  This study examines the suitability of binary choice forests as a methodological instrument to analyze discrete preference formation processes. First, w",1
"Localizing the camera in a known indoor environment is a key building block for scene mapping, robot navigation, AR, etc. Recent advances estimate the camera pose via optimization over the 2D/3D-3D correspondences established between the coordinates in 2D/3D camera space and 3D world space. Such a mapping is estimated with either a convolution neural network or a decision tree using only the static input image sequence, which makes these approaches vulnerable to dynamic indoor environments that are quite common yet challenging in the real world. To address the aforementioned issues, in this paper, we propose a novel outlier-aware neural tree which bridges the two worlds, deep learning and decision tree approaches. It builds on three important blocks: (a) a hierarchical space partition over the indoor scene to construct the decision tree; (b) a neural routing function, implemented as a deep classification network, employed for better 3D scene understanding; and (c) an outlier rejection module used to filter out dynamic points during the hierarchical routing process. Our proposed algorithm is evaluated on the RIO-10 benchmark developed for camera relocalization in dynamic indoor environments. It achieves robust neural routing through space partitions and outperforms the state-of-the-art approaches by around 30% on camera pose accuracy, while running comparably fast for evaluation.",0
"In dynamic indoor environments where changes occur frequently due to human activities such as rearrangement of furniture, camera relocalization remains a challenging task. Existing approaches often rely on global parameterizations or precomputed shape descriptors that can become invalid after perturbations to the environment. To address this issue, we propose a novel neural routing method based on space partitioning which enables efficient relocalization through learning local geometric features in depth images. We first generate a space partition using random sampling, then train an end-to-end deep network that directly predicts a set of points resembling the input image from the learned semantic partition. At runtime, we use these predicted points as intermediate keypoints during local relocalization under new viewpoint poses. Extensive evaluations demonstrate the effectiveness of our approach in terms of accuracy and robustness compared to state-of-the-art methods. Our framework achieves significant improvements particularly in highly dynamic scenes while maintaining high computational efficiency. This work contributes to a better understanding of how advanced machine intelligence techniques can be applied to real world applications for improving the reliability of smart cameras.",1
"For many practical, high-risk applications, it is essential to quantify uncertainty in a model's predictions to avoid costly mistakes. While predictive uncertainty is widely studied for neural networks, the topic seems to be under-explored for models based on gradient boosting. However, gradient boosting often achieves state-of-the-art results on tabular data. This work examines a probabilistic ensemble-based framework for deriving uncertainty estimates in the predictions of gradient boosting classification and regression models. We conducted experiments on a range of synthetic and real datasets and investigated the applicability of ensemble approaches to gradient boosting models that are themselves ensembles of decision trees. Our analysis shows that ensembles of gradient boosting models successfully detect anomalous inputs while having limited ability to improve the predicted total uncertainty. Importantly, we also propose a concept of a virtual ensemble to get the benefits of an ensemble via only one gradient boosting model, which significantly reduces complexity.",0
"This paper presents a novel approach to address uncertainty in gradient boosting through ensembling techniques. We propose using multiple models trained on slightly different subsets of data to capture model uncertainty due to randomness during training. By combining predictions from these individual models into a final ensemble prediction, we show that our method can effectively reduce uncertainty while improving accuracy compared to traditional methods. Our extensive experiments across several real-world datasets demonstrate the efficacy of our proposed methodology. Through careful analysis, we provide insights into how ensemble size impacts performance metrics and discuss tradeoffs between uncertainty reduction and classification error. Overall, our work provides valuable contributions to the field by advancing the state of art in handling uncertainty within machine learning models.",1
"Conventionally, random forests are built from ""greedy"" decision trees which each consider only one split at a time during their construction. The sub-optimality of greedy implementation has been well-known, yet mainstream adoption of more sophisticated tree building algorithms has been lacking. We examine under what circumstances an implementation of less greedy decision trees actually yields outperformance. To this end, a ""stepwise lookahead"" variation of the random forest algorithm is presented for its ability to better uncover binary feature interdependencies. In contrast to the greedy approach, the decision trees included in this random forest algorithm, each simultaneously consider three split nodes in tiers of depth two. It is demonstrated on synthetic data and financial price time series that the lookahead version significantly outperforms the greedy one when (a) certain non-linear relationships between feature-pairs are present and (b) if the signal-to-noise ratio is particularly low. A long-short trading strategy for copper futures is then backtested by training both greedy and stepwise lookahead random forests to predict the signs of daily price returns. The resulting superior performance of the lookahead algorithm is at least partially explained by the presence of ""XOR-like"" relationships between long-term and short-term technical indicators. More generally, across all examined datasets, when no such relationships between features are present, performance across random forests is similar. Given its enhanced ability to understand the feature-interdependencies present in complex systems, this lookahead variation is a useful extension to the toolkit of data scientists, in particular for financial machine learning, where conditions (a) and (b) are typically met.",0
"In high-noise environments where data quality varies widely, feature interdependency analysis plays a crucial role in decision making. Traditional methods often fail due to their limited ability to handle noisy data and complex relationships among features. This study proposes the use of stepwise lookahead decision forests (SLDF) as a powerful tool to address these limitations. SLDFs effectively model dependencies among multiple variables by recursively splitting based on the highest reduction in impurity at each node. By using iterative pruning techniques, we ensure that trees have minimal complexity while preserving accuracy. Experimental results demonstrate that our proposed method outperforms existing state-of-the-art algorithms across different datasets and evaluation metrics. Our findings highlight the potential of SLDFs to revolutionize feature interdependency analysis in real-world applications operating under challenging conditions.",1
"Graph neural networks (GNNs) are powerful models that have been successful in various graph representation learning tasks. Whereas gradient boosted decision trees (GBDT) often outperform other machine learning methods when faced with heterogeneous tabular data. But what approach should be used for graphs with tabular node features? Previous GNN models have mostly focused on networks with homogeneous sparse features and, as we show, are suboptimal in the heterogeneous setting. In this work, we propose a novel architecture that trains GBDT and GNN jointly to get the best of both worlds: the GBDT model deals with heterogeneous features, while GNN accounts for the graph structure. Our model benefits from end-to-end optimization by allowing new trees to fit the gradient updates of GNN. With an extensive experimental comparison to the leading GBDT and GNN models, we demonstrate a significant increase in performance on a variety of graphs with tabular features. The code is available: https://github.com/nd7141/bgnn.",0
This sounds like an exciting topic! Is there a specific question or research problem that you would like me to address in the abstract? Please provide some guidance so I can write an effective summary of your work.,1
"We consider the task of enforcing individual fairness in gradient boosting. Gradient boosting is a popular method for machine learning from tabular data, which arise often in applications where algorithmic fairness is a concern. At a high level, our approach is a functional gradient descent on a (distributionally) robust loss function that encodes our intuition of algorithmic fairness for the ML task at hand. Unlike prior approaches to individual fairness that only work with smooth ML models, our approach also works with non-smooth models such as decision trees. We show that our algorithm converges globally and generalizes. We also demonstrate the efficacy of our algorithm on three ML problems susceptible to algorithmic bias.",0
"This paper introduces Individually Fair Gradient Boosting (IFGB), a new method for decision tree ensemble modeling that takes into account individual fairness considerations during training. The authors argue that while previous approaches have focused on group fairness metrics such as demographic parity or statistical disparities, IFGB offers greater flexibility by allowing different groups to make tradeoffs between accuracy and bias separately. The proposed method incorporates three key components: feature selection based on permutation feature importance scores; subsampled gradient boosting; and weighted loss function minimization. By using individualized weights, IFGB encourages each base learner in the forest to fit data points from underrepresented groups more closely than those from overrepresented ones, thus reducing predictive disparities at the individual level. Experiments conducted across several benchmark datasets demonstrate the effectiveness of IFGB in achieving improved individual-level fairness without sacrificing overall prediction accuracy. Importantly, the authors showcase real-world applications where their approach leads to better decision making through fairer treatment of minority populations. In summary, IFGB represents a significant step forward in addressing ethical concerns surrounding algorithmic decision-making processes and offers practical solutions to balance competing objectives in machine learning.",1
"Human Activity Recognition (HAR) is considered a valuable research topic in the last few decades. Different types of machine learning models are used for this purpose, and this is a part of analyzing human behavior through machines. It is not a trivial task to analyze the data from wearable sensors for complex and high dimensions. Nowadays, researchers mostly use smartphones or smart home sensors to capture these data. In our paper, we analyze these data using machine learning models to recognize human activities, which are now widely used for many purposes such as physical and mental health monitoring. We apply different machine learning models and compare performances. We use Logistic Regression (LR) as the benchmark model for its simplicity and excellent performance on a dataset, and to compare, we take Decision Tree (DT), Support Vector Machine (SVM), Random Forest (RF), and Artificial Neural Network (ANN). Additionally, we select the best set of parameters for each model by grid search. We use the HAR dataset from the UCI Machine Learning Repository as a standard dataset to train and test the models. Throughout the analysis, we can see that the Support Vector Machine performed (average accuracy 96.33%) far better than the other methods. We also prove that the results are statistically significant by employing statistical significance test methods.",0
"In recent years, advances in machine learning have enabled new opportunities for analyzing human activity patterns from smartphone data. This study presents a novel framework that uses mobile sensor data and machine learning algorithms to identify, classify, and model different types of activities performed by individuals. Our approach utilizes both supervised and unsupervised methods, including clustering techniques and deep learning models, to automatically recognize daily routines such as exercise, driving, walking, sitting, socializing, and more. We evaluate our method on datasets collected from multiple sources, demonstrating high accuracy rates for various activities across diverse populations. Furthermore, we propose potential applications of our system in healthcare, wellness, security, and personalized services. Overall, this research contributes to the growing field of ubiquitous computing and presents significant implications for future development in human behavior understanding through smart devices.",1
We present a novel method of stacking decision trees by projection into an ordered time split out-of-fold (OOF) one nearest neighbor (1NN) space. The predictions of these one nearest neighbors are combined through a linear model. This process is repeated many times and averaged to reduce variance. Generalized Linear Tree Space Nearest Neighbor (GLTSNN) is competitive with respect to Mean Squared Error (MSE) compared to Random Forest (RF) on several publicly available datasets. Some of the theoretical and applied advantages of GLTSNN are discussed. We conjecture a classifier based upon the GLTSNN would have an error that is asymptotically bounded by twice the Bayes error rate like k = 1 Nearest Neighbor.,0
"Abstract:  Nearest neighbor search has been a fundamental problem in computer vision and pattern recognition for many years. Recently, tree space representations have emerged as a powerful tool for high-dimensional data due to their ability to capture hierarchical relationships among points and efficiently represent complex geometric structures. However, most previous work on nearest neighbor search in tree space only considers specific types of trees such as KD-trees or ball trees. In this paper, we present a generalized framework for nearest neighbor search in tree space that allows for arbitrary tree decompositions of point sets. We show how to preprocess a tree decomposition to obtain a compact index structure that can be used to efficiently find nearest neighbors under different distance measures. Our approach supports exact nearest neighbor queries within polylogarithmic time complexity and achieves provably good approximation guarantees for approximate nearest neighbor search. Experiments on real-world datasets demonstrate the effectiveness of our method compared to state-of-the-art alternatives.",1
"Cancer is responsible for millions of deaths worldwide every year. Although significant progress has been achieved in cancer medicine, many issues remain to be addressed for improving cancer therapy. Appropriate cancer patient stratification is the prerequisite for selecting appropriate treatment plan, as cancer patients are of known heterogeneous genetic make-ups and phenotypic differences. In this study, built upon deep phenotypic characterizations extractable from Mayo Clinic electronic health records (EHRs) and genetic test reports for a collection of cancer patients, we developed a system leveraging a joint of phenotypic and genetic features for cancer patient subgrouping.   The workflow is roughly divided into three parts: feature preprocessing, cancer patient classification, and cancer patient clustering based. In feature preprocessing step, we performed filtering, retaining the most relevant features. In cancer patient classification, we utilized joint categorical features to build a patient-feature matrix and applied nine different machine learning models, Random Forests (RF), Decision Tree (DT), Support Vector Machine (SVM), Naive Bayes (NB), Logistic Regression (LR), Multilayer Perceptron (MLP), Gradient Boosting (GB), Convolutional Neural Network (CNN), and Feedforward Neural Network (FNN), for classification purposes. Finally, in the cancer patient clustering step, we leveraged joint embeddings features and patient-feature associations to build an undirected feature graph and then trained the cancer feature node embeddings.",0
"Recent advances in cancer genomics have led to increased understanding of the genetic factors that contribute to tumor development and progression. However, there remains a need for improved methods of identifying patient subgroups based on both phenotypic and genetic features, as these subgroups may respond differently to treatment and exhibit different clinical outcomes. In this study, we present a novel approach for leveraging a joint of phenotypic and genetic features to identify cancer patient subgroups with high accuracy. Our method integrates multiple sources of data, including gene expression profiles, somatic mutations, and clinical variables such as age and gender. We demonstrate the effectiveness of our approach through extensive simulations and apply it to real datasets from breast and lung cancer patients. Our results show that our algorithm successfully identifies meaningful subgroups of patients who differ significantly in their clinical outcomes, providing valuable insights into the heterogeneity of cancer populations and paving the way for more personalized approaches to therapy. Overall, this work represents a significant contribution to the field of precision medicine and holds great promise for improving the diagnosis and management of cancer.",1
"It is of extreme importance to monitor and manage the battery health to enhance the performance and decrease the maintenance cost of operating electric vehicles. This paper concerns the machine-learning-enabled state-of-health (SoH) prognosis for Li-ion batteries in electric trucks, where they are used as energy sources. The paper proposes methods to calculate SoH and cycle life for the battery packs. We propose autoregressive integrated modeling average (ARIMA) and supervised learning (bagging with decision tree as the base estimator; BAG) for forecasting the battery SoH in order to maximize the battery availability for forklift operations. As the use of data-driven methods for battery prognostics is increasing, we demonstrate the capabilities of ARIMA and under circumstances when there is little prior information available about the batteries. For this work, we had a unique data set of 31 lithium-ion battery packs from forklifts in commercial operations. On the one hand, results indicate that the developed ARIMA model provided relevant tools to analyze the data from several batteries. On the other hand, BAG model results suggest that the developed supervised learning model using decision trees as base estimator yields better forecast accuracy in the presence of large variation in data for one battery.",0
"Abstract: This research investigates dynamic battery state-of-health forecasting models for electric trucks using lithium ion batteries as a case study. The goal is to develop accurate and efficient methods that can predict future SOH values based on real-time data from various sensors on the vehicle. Such predictions would enable fleet operators to optimize their charging strategies, reduce downtime due to unexpected degradation events, and extend overall battery life. The proposed approach combines machine learning techniques with physics-based models to formulate multiple regression equations that relate sensor signals to battery SOH over time. Three different datasets were used to train and validate the model, one collected from simulations, another from field tests involving light-duty vehicles, and the last obtained through controlled laboratory experiments on commercial heavy-duty EV batteries. The results demonstrate significant improvements in prediction accuracy compared to conventional single-parameter monitoring approaches currently employed by most manufacturers and OEMs. Furthermore, the adaptability and scalability features built into the model ensure ease of implementation across various types and sizes of battery pack configurations. Overall, this work presents a valuable contribution towards enhancing EV sustainability and reducing operational costs through optimized battery management systems.",1
"Deep Reinforcement Learning (DRL) has recently achieved significant advances in various domains. However, explaining the policy of RL agents still remains an open problem due to several factors, one being the complexity of explaining neural networks decisions. Recently, a group of works have used decision-tree-based models to learn explainable policies. Soft decision trees (SDTs) and discretized differentiable decision trees (DDTs) have been demonstrated to achieve both good performance and share the benefit of having explainable policies. In this work, we further improve the results for tree-based explainable RL in both performance and explainability. Our proposal, Cascading Decision Trees (CDTs) apply representation learning on the decision path to allow richer expressivity. Empirical results show that in both situations, where CDTs are used as policy function approximators or as imitation learners to explain black-box policies, CDTs can achieve better performances with more succinct and explainable models than SDTs. As a second contribution our study reveals limitations of explaining black-box policies via imitation learning with tree-based explainable models, due to its inherent instability.",0
"Title: ""Explainable Reinforcement Learning through Cascading Decision Trees""  Abstract: This research presents a new approach to explainable reinforcement learning using cascading decision trees (CDT). In recent years, there has been growing interest in developing methods that can provide insights into how machine learning models make decisions. While many approaches have focused on supervised learning tasks, our work addresses the problem of generating explanations for reinforcement learning agents, which operate in complex and uncertain environments. Our proposed method uses cascading decision trees as a post hoc explanation tool to analyze the behavior of the agent at different stages of decision making. By breaking down the agent's policy into multiple smaller policies represented by individual decision trees, we aim to shed light on the reasoning process behind the agent's actions. We evaluate our approach across several challenging domains and demonstrate its effectiveness in providing interpretable explanations for both experts and non-experts alike. Our results show that CDT leads to significant improvements in human understanding of the agent's behavior compared to other state-of-the-art methods, thus highlighting its potential impact on enhancing transparency and trust in artificial intelligence systems.",1
"Decision trees are among the most popular machine learning models and are used routinely in applications ranging from revenue management and medicine to bioinformatics. In this paper, we consider the problem of learning optimal binary classification trees. Literature on the topic has burgeoned in recent years, motivated both by the empirical suboptimality of heuristic approaches and the tremendous improvements in mixed-integer optimization (MIO) technology. Yet, existing MIO-based approaches from the literature do not leverage the power of MIO to its full extent: they rely on weak formulations, resulting in slow convergence and large optimality gaps. To fill this gap in the literature, we propose an intuitive flow-based MIO formulation for learning optimal binary classification trees. Our formulation can accommodate side constraints to enable the design of interpretable and fair decision trees. Moreover, we show that our formulation has a stronger linear optimization relaxation than existing methods. We exploit the decomposable structure of our formulation and max-flow/min-cut duality to derive a Benders' decomposition method to speed-up computation. We propose a tailored procedure for solving each decomposed subproblem that provably generates facets of the feasible set of the MIO as constraints to add to the main problem. We conduct extensive computational experiments on standard benchmark datasets on which we show that our proposed approaches are 31 times faster than state-of-the art MIO-based techniques and improve out of sample performance by up to 8%.",0
"In recent years, there has been increasing interest in developing machine learning algorithms that can handle complex real-world problems. One approach to solving these types of problems is through the use of classification trees, which have proven to be very effective at identifying patterns and making predictions based on large datasets. However, existing methods for constructing classification trees are often limited by their reliance on heuristics and ad hoc procedures, leading to suboptimal results. This paper proposes a novel method called strong optimal classification trees (SOCT), which uses rigorous mathematical principles to achieve better accuracy than previous approaches. We demonstrate the effectiveness of SOCT using extensive experiments on several benchmark data sets, including both synthetic and real-world examples. Our results show that SOCT significantly outperforms other popular tree construction methods across all measures. Additionally, we provide theoretical justification for why SOCT works so well and discuss future directions for research in this area. Overall, our work represents an important contribution to the field of machine learning and highlights the potential for more powerful decision-making tools based on advanced mathematics.",1
"Can deep learning solve multiple tasks simultaneously, even when they are unrelated and very different? We investigate how the representations of the underlying tasks affect the ability of a single neural network to learn them jointly. We present theoretical and empirical findings that a single neural network is capable of simultaneously learning multiple tasks from a combined data set, for a variety of methods for representing tasks -- for example, when the distinct tasks are encoded by well-separated clusters or decision trees over certain task-code attributes. More concretely, we present a novel analysis that shows that families of simple programming-like constructs for the codes encoding the tasks are learnable by two-layer neural networks with standard training. We study more generally how the complexity of learning such combined tasks grows with the complexity of the task codes; we find that combining many tasks may incur a sample complexity penalty, even though the individual tasks are easy to learn. We provide empirical support for the usefulness of the learning bounds by training networks on clusters, decision trees, and SQL-style aggregation.",0
"Recent years have seen a rise in interest within the machine learning community surrounding how neural networks should be designed to perform complex tasks. Two prominent formulations that have gained significant traction are monolithic task formulations and modular task formulations. In these formulations, the network is either trained end-to-end as one large model, or broken down into smaller subtasks which can be learned independently before being combined. While there has been a great deal of research exploring both approaches, the question remains whether one type of architecture is inherently better than another. This work seeks to provide a systematic comparison between monolithic task formulations and modular task formulations across a wide range of benchmark datasets and evaluation metrics. Our results show that while no single approach consistently outperforms the other, modularity can offer advantages in terms of interpretability and ease of deployment. Ultimately, we conclude that neither monolithism nor modularity provides a universal solution, and practitioners must carefully consider their specific use case when deciding on an appropriate formulation.",1
"Random forests on the one hand, and neural networks on the other hand, have met great success in the machine learning community for their predictive performance. Combinations of both have been proposed in the literature, notably leading to the so-called deep forests (DF) (Zhou \& Feng,2019). In this paper, our aim is not to benchmark DF performances but to investigate instead their underlying mechanisms. Additionally, DF architecture can be generally simplified into more simple and computationally efficient shallow forest networks. Despite some instability, the latter may outperform standard predictive tree-based methods. We exhibit a theoretical framework in which a shallow tree network is shown to enhance the performance of classical decision trees. In such a setting, we provide tight theoretical lower and upper bounds on its excess risk. These theoretical results show the interest of tree-network architectures for well-structured data provided that the first layer, acting as a data encoder, is rich enough.",0
"This paper seeks to analyze the tree-layer structure of deep forests by studying their physical makeup, chemical components, and biological interactions. Through field research and laboratory analysis, we aim to gain a better understanding of how these factors contribute to the unique properties of trees in different ecosystems. Our findings have important implications for conservation efforts and forest management practices, as well as broader applications within other scientific disciplines. By examining the intricate relationships among layers, we can gain insights into complex systems that influence the health and sustainability of our natural environment. Keywords: Deep forests, Tree layer, Physical structure, Chemical composition, Biological interaction.",1
"The Gradient Boosting Decision Tree (GBDT) is a popular machine learning model for various tasks in recent years. In this paper, we study how to improve model accuracy of GBDT while preserving the strong guarantee of differential privacy. Sensitivity and privacy budget are two key design aspects for the effectiveness of differential private models. Existing solutions for GBDT with differential privacy suffer from the significant accuracy loss due to too loose sensitivity bounds and ineffective privacy budget allocations (especially across different trees in the GBDT model). Loose sensitivity bounds lead to more noise to obtain a fixed privacy level. Ineffective privacy budget allocations worsen the accuracy loss especially when the number of trees is large. Therefore, we propose a new GBDT training algorithm that achieves tighter sensitivity bounds and more effective noise allocations. Specifically, by investigating the property of gradient and the contribution of each tree in GBDTs, we propose to adaptively control the gradients of training data for each iteration and leaf node clipping in order to tighten the sensitivity bounds. Furthermore, we design a novel boosting framework to allocate the privacy budget between trees so that the accuracy loss can be further reduced. Our experiments show that our approach can achieve much better model accuracy than other baselines.",0
"In recent years there has been growing concern surrounding the collection, storage and dissemination of personal data. As such, privacy preservation techniques have become increasingly important, particularly in machine learning applications where sensitive data may be used as part of training processes. One area that remains largely unexplored however is gradient boosting decision trees (GBDTs), which are commonly utilized due to their strong accuracy performance but pose significant risks to individual privacy due to the release of raw data during model development. To address this issue, we propose the application of differential privacy principles to GBDT models through random noising mechanisms, ensuring that each new data entry cannot compromise the privacy guarantees of individuals whose data was already present within the dataset. Our experimental evaluations demonstrate notable tradeoffs between utility and privacy risk levels while highlighting promising opportunities for deploying these techniques across diverse contexts.",1
"We study the spatio-temporal prediction problem and introduce a novel point-process-based prediction algorithm. Spatio-temporal prediction is extensively studied in Machine Learning literature due to its critical real-life applications such as crime, earthquake, and social event prediction. Despite these thorough studies, specific problems inherent to the application domain are not yet fully explored. Here, we address the non-stationary spatio-temporal prediction problem on both densely and sparsely distributed sequences. We introduce a probabilistic approach that partitions the spatial domain into subregions and models the event arrivals in each region with interacting point-processes. Our algorithm can jointly learn the spatial partitioning and the interaction between these regions through a gradient-based optimization procedure. Finally, we demonstrate the performance of our algorithm on both simulated data and two real-life datasets. We compare our approach with baseline and state-of-the-art deep learning-based approaches, where we achieve significant performance improvements. Moreover, we also show the effect of using different parameters on the overall performance through empirical results and explain the procedure for choosing the parameters.",0
"In recent years, point process models have emerged as powerful tools for modeling spatio-temporal phenomena in fields ranging from urban studies to epidemiology. However, prediction tasks involving point processes often require addressing complex spatial dependencies that challenge traditional statistical methods. To tackle these challenges, we propose a novel approach based on self-organizing decision trees (SODTs), which can capture nonlinear spatio-temporal relationships while adapting to varying local characteristics. Our method effectively combines the strengths of SODTs in capturing complex decision rules, tree ensembles for stable predictions, and kernel density estimation techniques to accommodate spatial dependency structures. We apply our framework to three real datasets across different domains: disease incidence data in South Korea, bike sharing demand forecasting in New York City, and wildfire occurrence predictions in California. Experimental results demonstrate significant improvements over state-of-the-art baselines, showcasing the effectiveness of our proposal. By providing accurate predictions while accounting for spatial complexity, our work has valuable implications for practitioners seeking advanced predictive models in diverse applications where space and time play crucial roles.",1
"Machine Learning models should ideally be compact and robust. Compactness provides efficiency and comprehensibility whereas robustness provides resilience. Both topics have been studied in recent years but in isolation. Here we present a robust model compression scheme which is independent of model types: it can compress ensembles, neural networks and other types of models into diverse types of small models. The main building block is the notion of depth derived from robust statistics. Originally, depth was introduced as a measure of the centrality of a point in a sample such that the median is the deepest point. This concept was extended to classification functions which makes it possible to define the depth of a hypothesis and the median hypothesis. Algorithms have been suggested to approximate the median but they have been limited to binary classification. In this study, we present a new algorithm, the Multiclass Empirical Median Optimization (MEMO) algorithm that finds a deep hypothesis in multi-class tasks, and prove its correctness. This leads to our Compact Robust Estimated Median Belief Optimization (CREMBO) algorithm for robust model compression. We demonstrate the success of this algorithm empirically by compressing neural networks and random forests into small decision trees, which are interpretable models, and show that they are more accurate and robust than other comparable methods. In addition, our empirical study shows that our method outperforms Knowledge Distillation on DNN to DNN compression.",0
"In practice, deep learning models require large amounts of computation time, storage space, and power consumption for inference. Therefore, model compression techniques that can compress deep neural networks without substantially degrading performance on downstream tasks have become increasingly important in recent years. Existing methods typically either focus solely on reducing model size (e.g., quantization) at the cost of losing expressivity, or seek to preserve accuracy through more complex post-training approaches such as pruning or knowledge distillation. This paper presents Deep Hypothesis, a novel approach to model compression that combines the efficiency gains from these two streams of research by adaptively adjusting the capacity of each layer based on the data rather than applying uniform restrictions across all layers. By selectively disabling certain channels/filters (groups of neurons) during training and testing, our method learns implicit hypotheses about which parts of the network contribute most strongly to task performance. Our extensive evaluation shows that our approach leads to improved accuracy over existing pruning and knowledge distillation baselines while maintaining competitive parameter counts, FLOPs, and latency. Additionally, we demonstrate that our method effectively unlocks larger models that were previously impractical due to memory constraints. Finally, since the model determines which features matter, it is robust to hyperparameter settings, datasets, activation functions, and architectures. Thus, DeepHypothesis provides a flexible framework for enhancing model interpretability, controllability, and transfer learning potential beyond standard neural architecture design considerations. We believe that this work paves the way towards usin",1
"Artificial neural networks (ANNs) are commonly labelled as black-boxes, lacking interpretability. This hinders human understanding of ANNs' behaviors. A need exists to generate a meaningful sequential logic for the production of a specific output. Decision trees exhibit better interpretability and expressive power due to their representation language and the existence of efficient algorithms to generate rules. Growing a decision tree based on the available data could produce larger than necessary trees or trees that do not generalise well. In this paper, we introduce two novel multivariate decision tree (MDT) algorithms for rule extraction from an ANN: an Exact-Convertible Decision Tree (EC-DT) and an Extended C-Net algorithm to transform a neural network with Rectified Linear Unit activation functions into a representative tree which can be used to extract multivariate rules for reasoning. While the EC-DT translates the ANN in a layer-wise manner to represent exactly the decision boundaries implicitlylearned by the hidden layers of the network, the Extended C-Net inherits the decompositional approach from EC-DT and combines with a C5 tree learning algorithm to construct the decision rules. The results suggest that while EC-DT is superior in preserving the structure and the accuracy of ANN, Extended C-Net generates the most compact and highly effective trees from ANN. Both proposed MDT algorithms generate rules including combinations of multiple attributes for precise interpretation of decision-making processes.",0
"In recent years, deep neural networks have shown great success in many applications such as image classification, natural language processing, and speech recognition. However, their opaque nature has been raising concerns regarding their interpretability, especially in safety-critical domains where clear explanations of decisions can mean the difference between life and death. To address these issues, researchers have explored ways to explain and interpret the predictions made by neural networks through methods such as visualizations and attribution techniques. One promising direction is to transform black box models into white box interpretable ones without sacrificing accuracy. This study proposes an exact transformation method that maps any trained multi-class neural network to an equivalent multivariate decision tree model while maintaining the same level of performance on both seen and unseen test sets. We conducted extensive experiments on several benchmark datasets using different activation functions, regularization techniques, and architectures. Our results show that our proposed method achieves higher transparency while preserving comparable predictive power compared to the original neural networks. The resulting decision trees provide intuitive explanations of how input features contribute to the final prediction, making them more suitable for use in high-stakes environments demanding interpretability. Overall, we believe that our work offers a step towards bridging the gap between accuracy and interpretability in artificial intelligence.",1
"Traffic control optimization is a challenging task for various traffic centers around the world and the majority of existing approaches focus only on developing adaptive methods under normal (recurrent) traffic conditions. Optimizing the control plans when severe incidents occur still remains an open problem, especially when a high number of lanes or entire intersections are affected.   This paper aims at tackling this problem and presents a novel methodology for optimizing the traffic signal timings in signalized urban intersections, under non-recurrent traffic incidents. With the purpose of producing fast and reliable decisions, we combine the fast running Machine Learning (ML) algorithms and the reliable Genetic Algorithms (GA) into a single optimization framework. As a benchmark, we first start with deploying a typical GA algorithm by considering the phase duration as the decision variable and the objective function to minimize the total travel time in the network. We fine tune the GA for crossover, mutation, fitness calculation and obtain the optimal parameters. Secondly, we train various machine learning regression models to predict the total travel time of the studied traffic network, and select the best performing regressor which we further hyper-tune to find the optimal training parameters. Lastly, we propose a new algorithm BGA-ML combining the GA algorithm and the extreme-gradient decision-tree, which is the best performing regressor, together in a single optimization framework. Comparison and results show that the new BGA-ML is much faster than the original GA algorithm and can be successfully applied under non-recurrent incident conditions.",0
"Traffic congestion is one of the main issues faced by urban areas worldwide. Congested roads lead to longer travel times, increased air pollution, wasted fuel, frustrated drivers, and decreased quality of life for residents. This research explores how artificial intelligence can improve traffic flow management systems and optimize routes based on real-time data collected from sensors deployed throughout road networks. By integrating machine learning techniques into traditional genetic algorithms, we demonstrate that routing decisions made by vehicles can be refined in response to changing road conditions, reducing overall travel time across highways during peak hours while maintaining safety standards. Our results show promising improvements in traffic flow efficiency and suggest potential applications for smart cities where traffic density is a significant challenge. With advancements in transportation technologies such as self-driving cars further complicating traffic models, the proposed framework provides a scalable approach towards managing complex urban mobility patterns while minimizing disruptions caused by unexpected events and sudden rush hour surges.",1
"Deep neural networks have enhanced the performance of decision making systems in many applications including image understanding, and further gains can be achieved by constructing ensembles. However, designing an ensemble of deep networks is often not very beneficial since the time needed to train the networks is very high or the performance gain obtained is not very significant. In this paper, we analyse error correcting output coding (ECOC) framework to be used as an ensemble technique for deep networks and propose different design strategies to address the accuracy-complexity trade-off. We carry out an extensive comparative study between the introduced ECOC designs and the state-of-the-art ensemble techniques such as ensemble averaging and gradient boosting decision trees. Furthermore, we propose a combinatory technique which is shown to achieve the highest classification performance amongst all.",0
"Here's how you can write an abstract: ""This paper presents a novel approach to ensembling deep convolutional neural networks (DCNN) by leveraging Expectation-Conditional Optimization over Classifiers (ECOC). Our method combines multiple DCNN models into one ensemble model that achieves state-of-the-art performance on several benchmark datasets, including CIFAR-10, SVHN, and ImageNet. By utilizing ECOC, we effectively balance accuracy and diversity among our network predictions, resulting in improved generalization capabilities. We discuss the details of our implementation and evaluate our results against other popular ensemble methods. This work demonstrates the effectiveness of DCNN ensembles using ECOC as a powerful tool for image classification tasks.""",1
"The desire to apply machine learning techniques in safety-critical environments has renewed interest in the learning of partial functions for distinguishing between positive, negative and unclear observations. We contribute to the understanding of the hardness of this problem. Specifically, we consider partial Boolean functions defined by a pair of Boolean functions $f, g \colon \{0,1\}^J \to \{0,1\}$ such that $f \cdot g = 0$ and such that $f$ and $g$ are defined by disjunctive normal forms or binary decision trees. We show: Minimizing the sum of the lengths or depths of these forms while separating disjoint sets $A \cup B = S \subseteq \{0,1\}^J$ such that $f(A) = \{1\}$ and $g(B) = \{1\}$ is inapproximable to within $(1 - \epsilon) \ln (|S|-1)$ for any $\epsilon  0$, unless P=NP.",0
"This is an interesting abstract regarding the topic of minimization of two sets defined by either Disjunctive Normal Form (DNF) or binary decision trees. The author presents findings on their work related to determining the complexity of optimizing this function. They argue that there exists no algorithm that can perform minimization faster than brute force search. Additionally, they show how these results generalize previously known inapproximability bounds for boolean functions in both size and depth. Overall, this research contributes new understanding into complex optimization problems and provides valuable insight for future studies exploring the limits of efficient algorithms. -----",1
"We consider counterfactual explanations, the problem of minimally adjusting features in a source input instance so that it is classified as a target class under a given classifier. This has become a topic of recent interest as a way to query a trained model and suggest possible actions to overturn its decision. Mathematically, the problem is formally equivalent to that of finding adversarial examples, which also has attracted significant attention recently. Most work on either counterfactual explanations or adversarial examples has focused on differentiable classifiers, such as neural nets. We focus on classification trees, both axis-aligned and oblique (having hyperplane splits). Although here the counterfactual optimization problem is nonconvex and nondifferentiable, we show that an exact solution can be computed very efficiently, even with high-dimensional feature vectors and with both continuous and categorical features, and demonstrate it in different datasets and settings. The results are particularly relevant for finance, medicine or legal applications, where interpretability and counterfactual explanations are particularly important.",0
"Title: Exact and efficient algorithms for counterfactual explanations on oblique decision trees Abstract Decision tree algorithms have been widely used as a powerful tool for data analysis due to their ease of interpretation and high predictive power. Recent advances in artificial intelligence (AI) and machine learning (ML) have led to increased interest in developing models that can provide accurate and interpretable predictions based on complex datasets. However, despite these developments, there remains a need for techniques that can reliably explain how these models make decisions. One approach to addressing this issue is through counterfactual reasoning, which involves identifying alternative scenarios where the outcome may differ from the observed one given certain changes in input features. In this study, we present exact and efficient algorithms for generating counterfactual explanations on oblique decision trees. Specifically, our proposed methods compute counterfactuals along all the branches of the decision tree simultaneously, allowing us to identify multiple possible reasons why a prediction was made. We demonstrate the effectiveness of our approach by conducting experiments using synthetic and real-world datasets across several domains, including finance, marketing, healthcare, and social sciences. Our results show that our method significantly outperforms existing approaches both in terms of computational time and accuracy, making it suitable for use in large-scale applications. Overall, our work provides a valuable contribution towards bridging the gap between model interpretability and performance, thus supporting the responsible development and deployment of ML/AI systems.",1
"We compare machine learning explainability methods based on the theory of atomic (Shapley, 1953) and infinitesimal (Aumann and Shapley, 1974) games, in a theoretical and experimental investigation into how the model and choice of integration path can influence the resulting feature attributions. To gain insight into differences in attributions resulting from interventional Shapley values (Sundararajan and Najmi, 2019; Janzing et al., 2019; Chen et al., 2019) and Generalized Integrated Gradients (GIG) (Merrill et al., 2019) we note interventional Shapley is equivalent to a multi-path integration along $n!$ paths where $n$ is the number of model input features. Applying Stoke's theorem we show that the path symmetry of these two methods results in the same attributions when the model is composed of a sum of separable functions of individual features and a sum of two-feature products. We then perform a series of experiments with varying degrees of data missingness to demonstrate how interventional Shapley's multi-path approach can yield less consistent attributions than the single straight-line path of Aumann-Shapley. We argue this is because the multiple paths employed by interventional Shapley extend away from the training data manifold and are therefore more likely to pass through regions where the model has little support. In the absence of a more meaningful path choice, we therefore advocate the straight-line path since it will almost always pass closer to the data manifold. Among straight-line path attribution algorithms, GIG is uniquely robust since it will still yield Shapley values for atomic games modeled by decision trees.",0
"This paper presents an investigation into how the specifics of path choices can affect the results generated by game-theoretical attribution methods. By examining the impact of different paths on attribution outcomes, we aim to better understand these techniques’ strengths and limitations, as well as their potential applications across fields such as economics, political science, sociology, computer science, law, psychology, business strategy, history, engineering, mathematics, biological systems analysis, environmental sustainability, neuroscience, philosophy, anthropology, physics, linguistics, geography, architecture, archaeology, literature, film studies, cultural studies, music theory, dance theory, communication studies, public health, gender studies, sexuality studies, critical race theory, postcolonialism, disability studies, education, design thinking, library and information science, management studies, and data science. We hope that our findings contribute to a more nuanced understanding of game-theoretical modeling and its role in analyzing complex human behavior and decision making. Keywords: Game theory, Attribution, Modeling complexity, Human behavior analysis, Decision making analysis",1
"Current work in explainable reinforcement learning generally produces policies in the form of a decision tree over the state space. Such policies can be used for formal safety verification, agent behavior prediction, and manual inspection of important features. However, existing approaches fit a decision tree after training or use a custom learning procedure which is not compatible with new learning techniques, such as those which use neural networks. To address this limitation, we propose a novel Markov Decision Process (MDP) type for learning decision tree policies: Iterative Bounding MDPs (IBMDPs). An IBMDP is constructed around a base MDP so each IBMDP policy is guaranteed to correspond to a decision tree policy for the base MDP when using a method-agnostic masking procedure. Because of this decision tree equivalence, any function approximator can be used during training, including a neural network, while yielding a decision tree policy for the base MDP. We present the required masking procedure as well as a modified value update step which allows IBMDPs to be solved using existing algorithms. We apply this procedure to produce IBMDP variants of recent reinforcement learning methods. We empirically show the benefits of our approach by solving IBMDPs to produce decision tree policies for the base MDPs.",0
"Abstract: Developing interpretable policies for Reinforcement Learning (RL) algorithms has become increasingly important as RL has gained popularity in real-world applications. However, many state-of-the-art methods rely on black box models that lack interpretability. In this paper, we introduce Iterative Bounding Markov Decision Processes (MDPs), which extend traditional MDPs by allowing agents to make decisions based on upper bounds of their values instead of exact estimates. We show how these bounds can be used to guide the learning process towards more interpretable policies, even when using non-interpretable learning algorithms. Our approach leads to significant improvements over baseline methods across a range of environments, demonstrating the effectiveness of our framework for learning interpretable policies. By leveraging the power of non-interpretable RL methods while still ensuring interpretability, our work represents a step forward in the development of practical and reliable RL systems.",1
"In a recent paper, Celis et al. (2020) introduced a new approach to fairness that corrects the data distribution itself. The approach is computationally appealing, but its approximation guarantees with respect to the target distribution can be quite loose as they need to rely on a (typically limited) number of constraints on data-based aggregated statistics; also resulting in a fairness guarantee which can be data dependent.   Our paper makes use of a mathematical object recently introduced in privacy -- mollifiers of distributions -- and a popular approach to machine learning -- boosting -- to get an approach in the same lineage as Celis et al. but without the same impediments, including in particular, better guarantees in terms of accuracy and finer guarantees in terms of fairness. The approach involves learning the sufficient statistics of an exponential family. When the training data is tabular, the sufficient statistics can be defined by decision trees whose interpretability can provide clues on the source of (un)fairness. Experiments display the quality of the results for simulated and real-world data.",0
"Title: Boosting Fairness by Debiasing Machine Learning Datasets Authors: Yoshua Bengio (Université de Montréal), Ian Goodfellow (OpenAI), Ilya Sutskever (OpenAI) Abstract: One goal common to many machine learning applications is that decisions should not unfairly advantage one group over another based on sensitive attributes such as race or gender. In practice data may contain spurious correlations between these groups which can lead to biases against protected classes. Here we consider debiasing methods from domain knowledge, fair metric learning or adversarial training, where the goal is either improving accuracy while maintaining individual class performance or minimizing overall error subject to constraints which reduce the gap between different subgroups within error rate. We propose adding a boosted fair mollifier to models trained using any of these approaches, where the hyperparameters controlling both model components learn together end-to-end during optimization, promoting consistency. Experiments across diverse datasets demonstrate improvements from our method, including under adversarial attacks on demographic parity. Code and preprocessed dataset available online at https://github.com/google/boosted_mollification.",1
"This paper introduces the first provably accurate algorithms for differentially private, top-down decision tree learning in the distributed setting (Balcan et al., 2012). We propose DP-TopDown, a general privacy preserving decision tree learning algorithm, and present two distributed implementations. Our first method NoisyCounts naturally extends the single machine algorithm by using the Laplace mechanism. Our second method LocalRNM significantly reduces communication and added noise by performing local optimization at each data holder. We provide the first utility guarantees for differentially private top-down decision tree learning in both the single machine and distributed settings. These guarantees show that the error of the privately-learned decision tree quickly goes to zero provided that the dataset is sufficiently large. Our extensive experiments on real datasets illustrate the trade-offs of privacy, accuracy and generalization when learning private decision trees in the distributed setting.",0
"In recent years, there has been a growing interest in developing algorithms that can provide accurate predictions while ensuring privacy for sensitive data. One popular approach to achieve these goals is differentially private distributed decision tree learning (DDDL). This technique allows data from different sources to be combined without revealing any sensitive information, providing strong privacy guarantees while still allowing for efficient model training on large datasets. However, existing approaches for DDDL suffer from either scalability issues or lack of rigorous theoretical analysis proving their accuracy. To address these challenges, we propose two new scalable and provably accurate algorithms for DDDL: one based on randomized response theory and another using sensitivity bounds. Our proposed methods overcome limitations of previous work by efficiently balancing privacy protection and prediction accuracy through formal mathematical analysis and extensive empirical evaluation. Our results show significant improvements over state-of-the-art techniques, demonstrating the effectiveness and versatility of our approach across diverse application domains. Overall, this research contributes important advancements in designing reliable and scalable systems for secure data-driven decision making, paving the way for future developments in privacy-preserving machine learning.",1
"Artificial intelligence methods have been increasingly turning into a potentially powerful tool in the diagnosis and management of diseases. In this study, we utilized logistic regression (LR), decision tree (DT), gradient boosted decision tree (GBDT), support vector machine (SVM), and multilayer perceptron (MLP) as machine learning models to rapidly diagnose the mycoplasma pneumoniae pneumonia (MPP) in children patients. The classification task was carried out after applying the preprocessing procedure to the MPP dataset. The most efficient results are obtained by GBDT. It provides the best performance with an accuracy of 93.7%. In contrast to standard raw feature weighting, the feature importance takes the underlying correlation structure of the features into account. The most crucial feature of GBDT is the ""pulmonary infiltrates range"" with a score of 0.5925, followed by ""cough"" (0.0953) and ""pleural effusion"" (0.0492). We publicly share our full implementation with the dataset and trained models at https://github.com/zhenguonie/2021_AI4MPP.",0
"Title: ""AI Driven Acceleration of Mycoplasma Pneumoniae Pneumonia Diagnostics in Pediatric Populations""  Mycoplasma pneumoniae (Mp) is one of the most common causes of respiratory tract infections in children. Quickly diagnosing Mp pneumonia can prevent severe complications, but current diagnostic methods are time consuming and unreliable. This study proposes using artificial intelligence (AI) enhanced rapid diagnostic tests (RDTs) as a solution. The authors tested two commercial RDT kits, each consisting of multiple test strips, on pediatric patient samples from two separate hospitals. Each strip was evaluated by human readers following manufacturer instructions and compared against an automated AI reader. Results showed that both RDT kits produced high sensitivity and specificity rates for detecting Mp infection. Automated AI analysis outperformed manual review, increasing accuracy of negative results while maintaining high levels of performance in positive cases. Overall, these findings suggest that AI driven enhancement of RDTs could greatly improve diagnostic speed and reliability for Mp pneumonia in pediatrics. Future research should explore integrating these tools into clinical practice.",1
"High-resolution mapping of cells and tissue structures provides a foundation for developing interpretable machine-learning models for computational pathology. Deep learning algorithms can provide accurate mappings given large numbers of labeled instances for training and validation. Generating adequate volume of quality labels has emerged as a critical barrier in computational pathology given the time and effort required from pathologists. In this paper we describe an approach for engaging crowds of medical students and pathologists that was used to produce a dataset of over 220,000 annotations of cell nuclei in breast cancers. We show how suggested annotations generated by a weak algorithm can improve the accuracy of annotations generated by non-experts and can yield useful data for training segmentation algorithms without laborious manual tracing. We systematically examine interrater agreement and describe modifications to the MaskRCNN model to improve cell mapping. We also describe a technique we call Decision Tree Approximation of Learned Embeddings (DTALE) that leverages nucleus segmentations and morphologic features to improve the transparency of nucleus classification models. The annotation data produced in this study are freely available for algorithm development and benchmarking at: https://sites.google.com/view/nucls.",0
"This paper presents NuCLS, a new method based on convolutional neural networks and crowd sourced data that can accurately classify, locate, and segment nuclei from microscopy images of human chromosomes. Our approach first uses deep learning algorithms trained on labeled images provided by professional annotators. Next, we collect unlabeled images via Amazon Mechanical Turk (MTurk) tasks, where each worker can label one or multiple cells per task and receive payment as part of our reward system. We then use active learning techniques to identify which MTurk workers contribute high quality annotations, and only use their labels for fine tuning of our models. To evaluate the performance of our method, we create a benchmark dataset of both high quality professional annotations and low quality annotations collected through MTurk. Experimental results show that our approach achieves state-of-the-art accuracy on both datasets and improves over previous methods. Additionally, our active learning framework leads to significant cost savings while maintaining high accuracy. Overall, NuCLS provides a promising solution for accurate and efficient nucleus classification, localization, and segmentation.",1
"Reducing outcome variance is an essential task in deep learning based medical image analysis. Bootstrap aggregating, also known as bagging, is a canonical ensemble algorithm for aggregating weak learners to become a strong learner. Random forest is one of the most powerful machine learning algorithms before deep learning era, whose superior performance is driven by fitting bagged decision trees (weak learners). Inspired by the random forest technique, we propose a simple bagging ensemble deep segmentation (BEDs) method to train multiple U-Nets with partial training data to segment dense nuclei on pathological images. The contributions of this study are three-fold: (1) developing a self-ensemble learning framework for nucleus segmentation; (2) aggregating testing stage augmentation with self-ensemble learning; and (3) elucidating the idea that self-ensemble and testing stage stain augmentation are complementary strategies for a superior segmentation performance. Implementation Detail: https://github.com/xingli1102/BEDs.",0
"In the field of biomedical image analysis, one of the primary tasks is nuclei segmentation. This involves separating individual cells from their surrounding tissue context using computational methods. To achieve accurate results in this task, we present the Bags Ensemble Deep Segmenter (BEDS). Our approach combines three key components - bagging ensembles, deep learning based semantic segmen",1
"Decision trees provide a rich family of highly non-linear but efficient models, due to which they continue to be the go-to family of predictive models by practitioners across domains. But learning trees is a challenging problem due to their highly discrete and non-differentiable decision boundaries. The state-of-the-art techniques use greedy methods that exploit the discrete tree structure but are tailored to specific problem settings (say, categorical vs real-valued predictions). In this work, we propose a reformulation of the tree learning problem that provides better conditioned gradients, and leverages successful deep network learning techniques like overparameterization and straight-through estimators. Our reformulation admits an efficient and {\em accurate} gradient-based algorithm that allows us to deploy our solution in disparate tree learning settings like supervised batch learning and online bandit feedback based learning.   Using extensive validation on standard benchmarks, we observe that in the supervised learning setting, our general method is competitive to, and in some cases more accurate than, existing methods that are designed {\em specifically} for the supervised settings. In contrast, for bandit settings, where most of the existing techniques are not applicable, our models are still accurate and significantly outperform the applicable state-of-the-art methods.",0
"Title: Learning Accurate Decision Trees with Bandit Feedback via Quantized Gradient Descent  Decision trees have been widely used in machine learning due to their interpretability and modeling flexibility. However, constructing accurate decision tree models can be challenging due to the high dimensionality of data and the large number of features that may be irrelevant for predictive performance. In order to address these issues, we propose a novel algorithm called quantized gradient descent (QGD) which learns accurate decision trees by optimizing feature selection as well as structural splits based on bandit feedback. This approach allows for efficient exploration of the feature space while providing strong guarantees on solution quality compared to traditional methods such as XGBoost or LightGBM. Our experiments showcase significant improvements over state-of-the-art decision tree algorithms across several real-world datasets including both tabular data as well as image classification tasks demonstrating QGD’s effectiveness at identifying important features and achieving competitive accuracy even in challenging scenarios.",1
"Recent research has recognized interpretability and robustness as essential properties of trustworthy classification. Curiously, a connection between robustness and interpretability was empirically observed, but the theoretical reasoning behind it remained elusive. In this paper, we rigorously investigate this connection. Specifically, we focus on interpretation using decision trees and robustness to $l_{\infty}$-perturbation. Previous works defined the notion of $r$-separation as a sufficient condition for robustness. We prove upper and lower bounds on the tree size in case the data is $r$-separated. We then show that a tighter bound on the size is possible when the data is linearly separated. We provide the first algorithm with provable guarantees both on robustness, interpretability, and accuracy in the context of decision trees. Experiments confirm that our algorithm yields classifiers that are both interpretable and robust and have high accuracy. The code for the experiments is available at https://github.com/yangarbiter/interpretable-robust-trees .",0
"This is a great opportunity to showcase your expertise on decision trees, interpretability and robustness. Please write at least 2 paragraphs that describe how you connect interpretability and robustness using separation techniques.",1
"The price of explainability for a clustering task can be defined as the unavoidable loss,in terms of the objective function, if we force the final partition to be explainable.   Here, we study this price for the following clustering problems: $k$-means, $k$-medians, $k$-centers and maximum-spacing. We provide upper and lower bounds for a natural model where explainability is achieved via decision trees. For the $k$-means and $k$-medians problems our upper bounds improve those obtained by [Moshkovitz et. al, ICML 20] for low dimensions.   Another contribution is a simple and efficient algorithm for building explainable clusterings for the $k$-means problem. We provide empirical evidence that its performance is better than the current state of the art for decision-tree based explainable clustering.",0
"Clustering is a fundamental data mining technique that has been widely used for exploratory data analysis (EDA), anomaly detection, feature selection, pattern recognition, image segmentation, information retrieval, and recommendation systems, among others. Despite their popularity, many existing algorithms rely on heuristics rather than formal guarantees of correctness, which can lead to suboptimal solutions or even incorrect results. Furthermore, even state-of-the-art methods often suffer from a lack of interpretability and explainability, making it difficult for users to gain insights into the clustering process and evaluate the quality of the obtained solutions. In this study, we investigate the price of explainability for several important clustering problems, including k-means, hierarchical clustering, spectral clustering, density-based clustering, matrix factorization-based clustering, graph partitioning-based clustering, etc. We examine different aspects of explainability, such as transparency, comprehensibility, traceability, interpretability, reproducibility, robustness, and accountability. Our findings suggest that while there may be tradeoffs involved, achieving greater explainability can often improve both the efficiency and effectiveness of cluster analysis. By identifying key challenges and opportunities, our work contributes to the broader research agenda on interpretable machine learning, paving the way for more trustworthy and responsible applications in various domains.",1
"Circuit representations are becoming the lingua franca to express and reason about tractable generative and discriminative models. In this paper, we show how complex inference scenarios for these models that commonly arise in machine learning -- from computing the expectations of decision tree ensembles to information-theoretic divergences of deep mixture models -- can be represented in terms of tractable modular operations over circuits. Specifically, we characterize the tractability of a vocabulary of simple transformations -- sums, products, quotients, powers, logarithms, and exponentials -- in terms of sufficient structural constraints of the circuits they operate on, and present novel hardness results for the cases in which these properties are not satisfied. Building on these operations, we derive a unified framework for reasoning about tractable models that generalizes several results in the literature and opens up novel tractable inference scenarios.",0
"In recent years there has been great interest in developing methods for characterizing circuits that can perform complex tasks efficiently. However, our understanding of circuit design remains largely piecemeal due to the lack of general compositionality principles. Here we present a comprehensive framework, called compositional atlas, that allows us to systematically explore how simple transformations on fundamental gates (such as AND, XOR) scale up to form complex queries with desired properties. We show that under mild conditions, any transformation implemented by a composed circuit can be realized using only basic primitives - a striking property that sheds light onto the nature of computation. As a proof of concept, we demonstrate that our methodology provides new insights into longstanding open questions such as evaluating the complexity of polynomial equality over binary fields. Finally, we discuss applications of our results to emerging areas like machine learning.",1
"Decline in gait features is common in older adults and an indicator of disability and mortality. Cortical control of gait, specifically in the pre-frontal cortex as measured by functional near infrared spectroscopy (fNIRS), during dual task walking has shown to be moderated by age, gender, cognitive status, and various age-related disease conditions. In this study, we develop classification models using machine learning methods to classify active walking tasks in older adults based on fNIRS signals into either Single-Task-Walk (STW) or Dual-Task-Walk (DTW) conditions. In this study, we develop classification models using machine learning methods to classify active walking tasks in older adults based on fNIRS signals into either single-task walking (STW) or dual-task walking (DTW). The fNIRS measurements included oxyhemoglobin (HbO2) and deoxyhemoglobin (Hb) signals obtained from prefrontal cortex (PFC) of the subject performing on the ground active walking tasks with or without a secondary cognitive task. We extract the fNIRS-related features by calculating the minimum, maximum, mean, skewness and kurtosis values of Hb and Hbo2 signals. We then use feature encoding to map the values into binary space. Using these features, we apply and evaluate various machine learning methods including logistic regression (LR), decision tree (DT), support vector machine (SVM), k-nearest neighbors (kNN), multilayer perceptron (MLP), and Random Forest (RF). Results showed that the machine learning models can achieve around 97\% classification accuracy.",0
"Title: Machine Learning-Based Classification of Active Walking Tasks in Older Adults Using fNIRS Abstract In this study, we explore the use of machine learning techniques to classify active walking tasks in older adults using functional Near Infrared Spectroscopy (fNIRS). With the aging population growing worldwide, there is a need for reliable and accurate assessment tools that can monitor mobility decline in older individuals. fNIRS is a non-invasive neuroimaging technique that measures changes in cortical oxygenation, which has been shown to be sensitive to cognitive and motor processing during walking tasks. By leveraging the power of machine learning algorithms, we aim to develop a classification model that can differentiate between different types of walking activities based on fNIRS signals collected from elderly participants. Our results demonstrate that our proposed method achieved high accuracy in discriminating between normal vs. fast-paced walking, regular vs. dual task walking, and steady state vs. change of direction walking conditions. These findings suggest that machine learning approaches have great potential as an alternative or complementary tool to traditional clinical tests for evaluating gait alterations in elderly populations. This work may contribute towards personalized medicine practices by providing more precise analysis of individual abilities, ultimately improving fall prevention strategies and patient care management.",1
"We introduce SIRUS (Stable and Interpretable RUle Set) for regression, a stable rule learning algorithm which takes the form of a short and simple list of rules. State-of-the-art learning algorithms are often referred to as ""black boxes"" because of the high number of operations involved in their prediction process. Despite their powerful predictivity, this lack of interpretability may be highly restrictive for applications with critical decisions at stake. On the other hand, algorithms with a simple structure-typically decision trees, rule algorithms, or sparse linear models-are well known for their instability. This undesirable feature makes the conclusions of the data analysis unreliable and turns out to be a strong operational limitation. This motivates the design of SIRUS, which combines a simple structure with a remarkable stable behavior when data is perturbed. The algorithm is based on random forests, the predictive accuracy of which is preserved. We demonstrate the efficiency of the method both empirically (through experiments) and theoretically (with the proof of its asymptotic stability). Our R/C++ software implementation sirus is available from CRAN.",0
"Artificial Intelligence (AI) has made significant strides over recent years, becoming increasingly integrated into various aspects of our lives. One field that has been particularly impacted by these advancements is the area of machine learning (ML), where algorithms have become progressively more adept at solving complex problems and making predictions on large datasets. However, as these models continue to grow in both capacity and complexity, so too grows their ""black box"" nature, leading to difficulties in interpretability and explainability. This can make it challenging for humans to comprehend how certain decisions were arrived at, causing concern in numerous applications. To address this issue, researchers have developed methods aimed at enhancing transparency and comprehension of ML systems. In one such method, known as Interpretable Random Forests (iRFs), random decision trees are extracted from existing ensembles of decision trees using backward elimination, resulting in simplified yet accurate approximations capable of providing clear explanations behind outcomes. Through experiments evaluating iRFs against other interpretable ML techniques, this study demonstrates the superiority of iRFs in terms of accuracy and interpretability. Overall, iRFs present a promising solution for achieving high-quality predictions while maintaining transparency and accountability, thereby fostering greater trust in complex computational systems.",1
"With increasing focus on privacy protection, alternative methods to identify vehicle operator without the use of biometric identifiers have gained traction for automotive data analysis. The wide variety of sensors installed on modern vehicles enable autonomous driving, reduce accidents and improve vehicle handling. On the other hand, the data these sensors collect reflect drivers' habit. Drivers' use of turn indicators, following distance, rate of acceleration, etc. can be transformed to an embedding that is representative of their behavior and identity. In this paper, we develop a deep learning architecture (Driver2vec) to map a short interval of driving data into an embedding space that represents the driver's behavior to assist in driver identification. We develop a custom model that leverages performance gains of temporal convolutional networks, embedding separation power of triplet loss and classification accuracy of gradient boosting decision trees. Trained on a dataset of 51 drivers provided by Nervtech, Driver2vec is able to accurately identify the driver from a short 10-second interval of sensor data, achieving an average pairwise driver identification accuracy of 83.1% from this 10-second interval, which is remarkably higher than performance obtained in previous studies. We then analyzed performance of Driver2vec to show that its performance is consistent across scenarios and that modeling choices are sound.",0
"This paper presents ""Driver2vec"", a novel approach towards driver identification using automotive data, specifically telemetry logs. Driver profiling has been of significant interest due to growing concerns over road safety, traffic management, and insurance claims processing. In our study, we have analyzed vehicle data such as acceleration patterns, speeding habits, cornering behaviors, braking tendencies, etc., captured through onboard sensors. These signals were then processed using advanced signal processing techniques followed by dimensionality reduction and machine learning algorithms. We demonstrate that our method outperforms existing approaches based on classification accuracy (97%) and inter-driver similarity analysis (86%). Additionally, we provide case studies illustrating how our model can identify individuals who may pose potential threats while driving, which could assist authorities and private agencies in enhancing road security measures. Our findings contribute significantly to understanding human behavior behind the wheel, providing valuable insights for future research directions in transportation science and engineering.",1
"The explosive growth of easily-accessible unlabeled data has lead to growing interest in active learning, a paradigm in which data-hungry learning algorithms adaptively select informative examples in order to lower prohibitively expensive labeling costs. Unfortunately, in standard worst-case models of learning, the active setting often provides no improvement over non-adaptive algorithms. To combat this, a series of recent works have considered a model in which the learner may ask enriched queries beyond labels. While such models have seen success in drastically lowering label costs, they tend to come at the expense of requiring large amounts of memory. In this work, we study what families of classifiers can be learned in bounded memory. To this end, we introduce a novel streaming-variant of enriched-query active learning along with a natural combinatorial parameter called lossless sample compression that is sufficient for learning not only with bounded memory, but in a query-optimal and computationally efficient manner as well. Finally, we give three fundamental examples of classifier families with small, easy to compute lossless compression schemes when given access to basic enriched queries: axis-aligned rectangles, decision trees, and halfspaces in two dimensions.",0
"This paper presents a new approach to active learning called bounded memory active learning through enriched queries (BMALEQ) that allows users to train machine learning models more efficiently by requesting labels only on instances they deem important while keeping the number of label requests limited to a preset value. The proposed method addresses several drawbacks of traditional active learning approaches such as query strategies based solely on uncertainty sampling or diversity sampling which can lead to redundant or poor quality training data over time. Additionally, it incorporates user feedback into its model selection process, ensuring that its recommendations align with the objectives of the task at hand. Our experiments show that our method outperforms other state-of-the-art methods across a variety of datasets and tasks. In particular, we demonstrate significant improvements in predictive accuracy using less than half the number of label requests compared to baseline methods. Furthermore, we provide analysis of how well different components of our algorithm work together under varying conditions. This research has implications for practitioners who need to quickly build accurate ML models on small budgets, scientists seeking high impact applications of machine learning in their fields, and educators looking to create accessible tutorials for teaching active learning techniques. We hope this research inspires further exploration of interactive machine learning algorithms that place human input front and center throughout the learning process.",1
"In recent years, storing large volumes of data on distributed devices has become commonplace. Applications involving sensors, for example, capture data in different modalities including image, video, audio, GPS and others. Novel algorithms are required to learn from this rich distributed data. In this paper, we present consensus based multi-layer perceptrons for resource-constrained devices. Assuming nodes (devices) in the distributed system are arranged in a graph and contain vertically partitioned data, the goal is to learn a global function that minimizes the loss. Each node learns a feed-forward multi-layer perceptron and obtains a loss on data stored locally. It then gossips with a neighbor, chosen uniformly at random, and exchanges information about the loss. The updated loss is used to run a back propagation algorithm and adjust weights appropriately. This method enables nodes to learn the global function without exchange of data in the network. Empirical results reveal that the consensus algorithm converges to the centralized model and has performance comparable to centralized multi-layer perceptrons and tree-based algorithms including random forests and gradient boosted decision trees.",0
"Artificial neural networks (ANNs) have been widely applied to diverse applications, ranging from image classification to speech recognition. However, their success relies on large amounts of data and powerful hardware which renders them impractical for edge computing scenarios. To bridge this gap, we introduce consensus based multi-layer perceptrons (CMLPs), which use a combination of feature selection and ensemble learning techniques. Our approach achieves state-of-the-art performance while being efficient enough for deployment on resource constrained devices such as smartphones and embedded sensors. Furthermore, our results show that our model generalizes better across different datasets, resulting in improved robustness against changes in input distributions. We hope that CMLPs can serve as a valuable tool for enabling artificial intelligence at the network edge, facilitating new possibilities for distributed machine learning and intelligent IoT systems.",1
"As online auto-grading systems appear, information obtained from those systems can potentially enable researchers to create predictive models to predict student behaviour and performances. In the University of Waterloo, the ECE 150 (Fundamentals of Programming) Instructional Team wants to get an insight into how to allocate the limited teaching resources better to achieve improved educational outcomes. Currently, the Instructional Team allocates tutoring time in a reactive basis. They help students ""as-requested"". This approach serves those students with the wherewithal to request help; however, many of the students who are struggling do not reach out for assistance. Therefore, we, as the Research Team, want to explore if we can determine students which need help by looking into the data from our auto-grading system, Marmoset.   In this paper, we conducted experiments building decision-tree and linear-regression models with various features extracted from the Marmoset auto-grading system, including passing rate, testcase outcomes, number of submissions and submission time intervals (the time interval between the student's first reasonable submission and the deadline). For each feature, we interpreted the result at the confusion matrix level. Specifically for poor-performance students, we show that the linear-regression model using submission time intervals performs the best among all models in terms of Precision and F-Measure. We also show that for students who are misclassified into poor-performance students, they have the lowest actual grades in the linear-regression model among all models. In addition, we show that for the midterm, the submission time interval of the last assignment before the midterm predicts the midterm performance the most. However, for the final exam, the midterm performance contributes the most on the final exam performance.",0
"Title: Data Analytics For Education: Using Auto-Grading System To Enhance Student Performance Prediction Abstract: Efficient tools for monitoring academic progress can provide valuable insights that can enhance instructional design and facilitate learning outcomes. This study examines how data from an auto-grading system (AGS) can be used for predicting student performance on class assignments, as well as inform overall course assessment. By analyzing data collected over multiple semesters, we were able to identify patterns related to submission time, assignment difficulty, and grading results. Our findings show strong correlations between these factors and student performance, indicating that AGS can serve as an effective tool for measuring student engagement and gauging their readiness for exams or future assignments. We conclude by discussing implications of our research for educators looking to leverage technology in creating personalized feedback loops for students. Our work emphasizes the importance of integrating AGS with other educational platforms to improve teacher decision making, foster active participation, and increase students' motivation towards higher achievement.",1
"Modern machine learning models (such as deep neural networks and boosting decision tree models) have become increasingly popular in financial market prediction, due to their superior capacity to extract complex non-linear patterns. However, since financial datasets have very low signal-to-noise ratio and are non-stationary, complex models are often very prone to overfitting and suffer from instability issues. Moreover, as various machine learning and data mining tools become more widely used in quantitative trading, many trading firms have been producing an increasing number of features (aka factors). Therefore, how to automatically select effective features becomes an imminent problem. To address these issues, we propose DoubleEnsemble, an ensemble framework leveraging learning trajectory based sample reweighting and shuffling based feature selection. Specifically, we identify the key samples based on the training dynamics on each sample and elicit key features based on the ablation impact of each feature via shuffling. Our model is applicable to a wide range of base models, capable of extracting complex patterns, while mitigating the overfitting and instability issues for financial market prediction. We conduct extensive experiments, including price prediction for cryptocurrencies and stock trading, using both DNN and gradient boosting decision tree as base models. Our experiment results demonstrate that DoubleEnsemble achieves a superior performance compared with several baseline methods.",0
"This paper proposes a new ensemble method called ""DoubleEnsemble"" that combines two popular techniques - sample reweighting and feature selection - for improving the accuracy of financial data analysis. Weighted average voting (WAV) methods have been successfully used in many fields as an alternative to the traditional majority vote approach. Inspired by WAV, we introduce an extension of bagging called double bagging, which trains each base model twice to improve accuracy. Our method then applies weighted averaging using these duplicates as input, enabling us to select features that minimize intraclass variability. Empirical studies show that our approach outperforms other ensembles such as Random Forest and Gradient Boosting Machine while maintaining comparable computational efficiency. Overall, DoubleEnsemble offers a powerful technique for addressing challenges associated with noisy datasets and high-dimensional spaces encountered in financial data analysis.",1
"Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.",0
"Title: NBDT: Neural-Backed Decision Trees  Abstract: Decision trees have been widely used as one of the fundamental building blocks of machine learning models due to their interpretability and ability to handle complex relationships among features. However, constructing accurate decision trees relies heavily on manual feature engineering, which can be time-consuming and requires domain expertise. In recent years, deep neural networks (DNNs) have shown state-of-the-art performance across many domains due to their powerful model capacity and automatic feature extraction capabilities. Motivated by these successes, we propose a novel approach called Neural-backed Decision Trees (NBDT), which combines the interpretability and ease of deployment of decision trees with the representation learning power of DNNs.  The key idea behind NBDT is to use a pre-trained DNN as a feature extractor that maps input data into a high-dimensional space, where a decision tree can then be constructed using traditional methods such as ID3 or C4.5 algorithms. By leveraging the learned representations from the DNN, our method effectively integrates prior knowledge from vast amounts of training data, leading to improved generalization performance compared to vanilla decision trees without any significant increase in complexity. We demonstrate through extensive experiments on benchmark datasets across different domains (e.g., image classification, text analysis, and tabular prediction) that our proposed model outperforms other popular baselines, including both linear and nonlinear models. Overall, our work highlights the potential benefits of combining classical machine learning techniques with modern deep learning architectures, paving the way towards more effective hybrid models in real-world applications.",1
"Based on decision trees, it is efficient to handle tabular data. Conventional decision tree growth methods often result in suboptimal trees because of their greedy nature. Their inherent structure limits the options of hardware to implement decision trees in parallel. Here is a compact representation of binary decision trees to overcome these deficiencies. We explicitly formulate the dependence of prediction on binary tests for binary decision trees and construct a function to guide the input sample from the root to the appropriate leaf node. And based on this formulation we introduce a new interpretation of binary decision trees. Then we approximate this formulation via continuous functions. Finally, we interpret the decision tree as a model combination method. And we propose the selection-prediction scheme to unify a few learning methods.",0
"This paper presents a novel approach to understanding decision trees as combination methods rather than black box models. We argue that by viewing decision tree ensembles as mixtures of simpler base models, we can gain insight into their properties and improve their interpretability. To support our claims, we conduct experiments on several real world datasets and demonstrate the effectiveness of our method compared to traditional approaches. Our findings have important implications for both academics and practitioners who use decision trees in machine learning applications, as they provide new tools for analyzing and interpreting these models.",1
"Recently it has become essential to search for and retrieve high-resolution and efficient images easily due to swift development of digital images, many present annotation algorithms facing a big challenge which is the variance for represent the image where high level represent image semantic and low level illustrate the features, this issue is known as semantic gab. This work has been used MPEG-7 standard to extract the features from the images, where the color feature was extracted by using Scalable Color Descriptor (SCD) and Color Layout Descriptor (CLD), whereas the texture feature was extracted by employing Edge Histogram Descriptor (EHD), the CLD produced high dimensionality feature vector therefore it is reduced by Principal Component Analysis (PCA). The features that have extracted by these three descriptors could be passing to the classifiers (Naive Bayes and Decision Tree) for training. Finally, they annotated the query image. In this study TUDarmstadt image bank had been used. The results of tests and comparative performance evaluation indicated better precision and executing time of Naive Bayes classification in comparison with Decision Tree classification.",0
"This paper proposes a new method for automatic image annotation based on two popular machine learning algorithms: Naive Bayes and decision tree classifiers. To achieve high accuracy in classification, we use the combination of these methods along with features extracted from the images using MPEG-7 descriptors. Our experiments demonstrate that our approach outperforms previous state-of-the-art techniques and achieves promising results in terms of precision, recall, and F1 score. Additionally, we provide insights into which combinations of descriptors and classifiers perform better for specific categories of images, enabling further improvements in performance. Overall, our work represents a significant contribution to the field of computer vision and automatic image annotation, paving the way for more advanced applications in areas such as content-based image retrieval and multimedia databases.",1
"Image ordinal estimation is to predict the ordinal label of a given image, which can be categorized as an ordinal regression problem. Recent methods formulate an ordinal regression problem as a series of binary classification problems. Such methods cannot ensure that the global ordinal relationship is preserved since the relationships among different binary classifiers are neglected. We propose a novel ordinal regression approach, termed Convolutional Ordinal Regression Forest or CORF, for image ordinal estimation, which can integrate ordinal regression and differentiable decision trees with a convolutional neural network for obtaining precise and stable global ordinal relationships. The advantages of the proposed CORF are twofold. First, instead of learning a series of binary classifiers \emph{independently}, the proposed method aims at learning an ordinal distribution for ordinal regression by optimizing those binary classifiers \emph{simultaneously}. Second, the differentiable decision trees in the proposed CORF can be trained together with the ordinal distribution in an end-to-end manner. The effectiveness of the proposed CORF is verified on two image ordinal estimation tasks, i.e. facial age estimation and image aesthetic assessment, showing significant improvements and better stability over the state-of-the-art ordinal regression methods.",0
"Abstract: Image ordinal regression (IR) tasks aim at estimating continuous labels as rankings rather than single numbers. In computer vision research, deep learning models have been extensively applied due to their promising results. However, existing methods often ignore the relationships among features by solely relying on pairwise comparisons, which limits further improvements in performance. To address this issue, we introduce Convolutional Ordinal Regression Forest (CORF), a novel approach that explicitly captures feature interactions through tree ensembles. Our method integrates both convolutional neural networks (CNNs) and random forests (RFs) into an end-to-end trainable model, enabling effective utilization of global image representations and local contextual information respectively. Experiments conducted on two IR benchmark datasets show significant accuracy gains over state-of-the-art alternatives. This work demonstrates the potential of CORF for advancing IR in image understanding applications. Abstract: In recent years, deep learning has proven highly effective in computer vision tasks such as object detection and classification. However, many tasks require ranking data instead of absolute values, where standard CNN architectures fall short. This paper presents Convolutional Ordinal Regression Forest (CORF), an innovative framework combining CNNs and Random Forests to improve performance in image ordinal estimation problems. By leveraging interactions between multiple features via RF ensembles, our approach enhances overall predictions compared to baselines that rely only on pairwise comparisons. Quantitative evaluations across two popular benchmark datasets validate these advantages, making CORF a valuable contribution to the field of image ordinal regression.",1
"We aim to mine temporal causal sequences that explain observed events (consequents) in time-series traces. Causal explanations of key events in a time-series has applications in design debugging, anomaly detection, planning, root-cause analysis and many more. We make use of decision trees and interval arithmetic to mine sequences that explain defining events in the time-series. We propose modified decision tree construction metrics to handle the non-determinism introduced by the temporal dimension. The mined sequences are expressed in a readable temporal logic language that is easy to interpret. The application of the proposed methodology is illustrated through various examples.",0
"""The authors propose a novel deep learning algorithm called 'TempoCause' that can automatically learn causal relationships in time series data by exploiting their temporal dynamics. Traditional approaches have focused on extracting static features from the data and then using statistical models to infer the underlying structure. In contrast, TempoCause learns how different events affect each other over time, leading to more accurate predictions of future states. Experimental results show significant improvements compared to existing methods across a variety of datasets representing different domains, such as healthcare, finance, and manufacturing.""",1
"A decision tree looks like a simple computational graph without cycles, where only the leaf nodes specify the output values and the non-terminals specify their tests or split conditions. From the numerical perspective, we express decision trees in the language of computational graph. We explicitly parameterize the test phase, traversal phase and prediction phase of decision trees based on the bitvectors of non-terminal nodes. As shown later, the decision tree is a shallow binary network in some sense. Especially, we introduce the bitvector matrix to implement the tree traversal in numerical approach, where the core is to convert the logical `AND' operation to arithmetic operations. And we apply this numerical representation to extend and unify diverse decision trees in concept.",0
"In recent years there has been a growing interest in developing new algorithms that can effectively represent binary decision trees (BDTs). Traditional methods such as ID3 and C4.5 have been shown to be effective but suffer from drawbacks including overfitting and poor generalization. One popular alternative method is the mathematical demonstration approach which uses linear algebra techniques to construct BDT models directly in high dimensional spaces.  The aim of our research was to investigate whether this mathematical demonstration approach could provide a more accurate representation of BDTs than traditional methods by using real world datasets. We designed experiments to compare the performance of our proposed method against several state-of-the-art BDT algorithms on publicly available benchmark datasets. Our results showed that our method outperformed all other approaches in terms of accuracy, interpretability and computational efficiency, confirming its potential as a valuable tool for data miners, machine learning practitioners and domain experts alike. The use of high level mathematics to model complex relationships provides a promising direction for future research into knowledge discovery and pattern recognition tasks.",1
"Based on decision trees, many fields have arguably made tremendous progress in recent years. In simple words, decision trees use the strategy of ""divide-and-conquer"" to divide the complex problem on the dependency between input features and labels into smaller ones. While decision trees have a long history, recent advances have greatly improved their performance in computational advertising, recommender system, information retrieval, etc. We introduce common tree-based models (e.g., Bayesian CART, Bayesian regression splines) and training techniques (e.g., mixed integer programming, alternating optimization, gradient descent). Along the way, we highlight probabilistic characteristics of tree-based models and explain their practical and theoretical benefits. Except machine learning and data mining, we try to show theoretical advances on tree-based models from other fields such as statistics and operation research. We list the reproducible resource at the end of each method.",0
"This paper explores decision tree algorithms such as CART, ID3, and Random Forest. Using a case study approach, we demonstrate how these methods can effectively make predictions by analyzing patterns within datasets. We also discuss how these models perform under different conditions and their potential limitations. Finally, we provide recommendations on which types of problems and data sets might benefit most from using decision trees. By highlighting both theoretical concepts and real-world applications, our work provides valuable insights that can improve understanding of these powerful tools and contribute new knowledge to the field of machine learning. Overall, this research contributes important advancements toward building effective, efficient, and accurate predictors for complex problems.",1
"Policy specification is a process by which a human can initialize a robot's behaviour and, in turn, warm-start policy optimization via Reinforcement Learning (RL). While policy specification/design is inherently a collaborative process, modern methods based on Learning from Demonstration or Deep RL lack the model interpretability and accessibility to be classified as such. Current state-of-the-art methods for policy specification rely on black-box models, which are an insufficient means of collaboration for non-expert users: These models provide no means of inspecting policies learnt by the agent and are not focused on creating a usable modality for teaching robot behaviour. In this paper, we propose a novel machine learning framework that enables humans to 1) specify, through natural language, interpretable policies in the form of easy-to-understand decision trees, 2) leverage these policies to warm-start reinforcement learning and 3) outperform baselines that lack our natural language initialization mechanism. We train our approach by collecting a first-of-its-kind corpus mapping free-form natural language policy descriptions to decision tree-based policies. We show that our novel framework translates natural language to decision trees with a 96% and 97% accuracy on a held-out corpus across two domains, respectively. Finally, we validate that policies initialized with natural language commands are able to significantly outperform relevant baselines (p  0.001) that do not benefit from our natural language-based warm-start technique.",0
"This paper presents a novel approach to policy specification and synthesis that combines natural language processing (NLP) techniques with reinforcement learning (RL). The proposed method allows users to specify policies using natural language commands and provides interpretable explanations of how these policies were derived from raw sensory data. To achieve this goal, we first develop a deep NLP model capable of understanding complex human instructions and translating them into low-level control actions. Next, we use RL algorithms to learn a policy that maximizes the expected reward based on the user-specified objectives and constraints. Our experiments demonstrate the effectiveness and efficiency of our approach, as well as its ability to generate interpretable and explainable policies that meet desired specifications. Overall, our work represents an important step towards developing intelligent agents that can communicate effectively with humans and adapt their behavior to complex environments.",1
"Many countries are now experiencing the third wave of the COVID-19 pandemic straining the healthcare resources with an acute shortage of hospital beds and ventilators for the critically ill patients. This situation is especially worse in India with the second largest load of COVID-19 cases and a relatively resource-scarce medical infrastructure. Therefore, it becomes essential to triage the patients based on the severity of their disease and devote resources towards critically ill patients. Yan et al. 1 have published a very pertinent research that uses Machine learning (ML) methods to predict the outcome of COVID-19 patients based on their clinical parameters at the day of admission. They used the XGBoost algorithm, a type of ensemble model, to build the mortality prediction model. The final classifier is built through the sequential addition of multiple weak classifiers. The clinically operable decision rule was obtained from a 'single-tree XGBoost' and used lactic dehydrogenase (LDH), lymphocyte and high-sensitivity C-reactive protein (hs-CRP) values. This decision tree achieved a 100% survival prediction and 81% mortality prediction. However, these models have several technical challenges and do not provide an out of the box solution that can be deployed for other populations as has been reported in the ""Matters Arising"" section of Yan et al. Here, we show the limitations of this model by deploying it on one of the largest datasets of COVID-19 patients containing detailed clinical parameters collected from India.",0
"Mortality prediction models play a crucial role in understanding disease outcomes and guiding patient management decisions. In India, where COVID-19 has had a significant impact, accurate prediction of patient prognosis is essential to allocate resources efficiently and improve overall healthcare quality. This study aimed to explore the challenges associated with applying a mortality prediction model designed for one population (in this case, non-Indian) to another distinct population (the Indian cohort). We identified four key areas that presented obstacles in our attempt to apply the model: differences in baseline characteristics such as age distribution and comorbidities; variations in treatment practices across regions within India; cultural dissimilarities affecting healthcare access and behavior; and data quality issues arising from incomplete or inconsistent documentation. Our findings highlight the need for careful consideration and adaptation before using predictive models developed elsewhere, particularly for populations with marked demographic diversity. Further research investigating locally derived models tailored specifically to different subgroups within India could potentially provide more reliable predictions and support effective COVID-19 management nationwide. While limitations imposed by available data prevent us from providing specific recommendations at this time, we hope our insights inspire future studies focused on developing customized tools suitable for diverse patient populations globally. By better addressing these challenges, clinicians and public health officials can work towards improved risk stratification, evidence-based decision making, and ultimately better care for COVID-19 patients in India and beyond.",1
"Cancer is responsible for millions of deaths worldwide every year. Although significant progress hasbeen achieved in cancer medicine, many issues remain to be addressed for improving cancer therapy.Appropriate cancer patient stratification is the prerequisite for selecting appropriate treatment plan, ascancer patients are of known heterogeneous genetic make-ups and phenotypic differences. In thisstudy, built upon deep phenotypic characterizations extractable from Mayo Clinic electronic healthrecords (EHRs) and genetic test reports for a collection of cancer patients, we evaluated variousgraph neural networks (GNNs) leveraging a joint of phenotypic and genetic features for cancer typeclassification. Models were applied and fine-tuned on the Mayo Clinic cancer disease dataset. Theassessment was done through the reported accuracy, precision, recall, and F1 values as well as throughF1 scores based on the disease class. Per our evaluation results, GNNs on average outperformed thebaseline models with mean statistics always being higher that those of the baseline models (0.849 vs0.772 for accuracy, 0.858 vs 0.794 for precision, 0.843 vs 0.759 for recall, and 0.843 vs 0.855 for F1score). Among GNNs, ChebNet, GraphSAGE, and TAGCN showed the best performance, while GATshowed the worst. We applied and compared eight GNN models including AGNN, ChebNet, GAT,GCN, GIN, GraphSAGE, SGC, and TAGCN on the Mayo Clinic cancer disease dataset and assessedtheir performance as well as compared them with each other and with more conventional machinelearning models such as decision tree, gradient boosting, multi-layer perceptron, naive bayes, andrandom forest which we used as the baselines.",0
"In recent years, deep learning techniques have shown promising results in medical image analysis tasks such as cancer classification. One popular approach is using graph neural networks (GNN) which leverage both phenotypic and genetic features from patient data to improve predictions. This study compares different GNN architectures, including their strengths and limitations, to determine the optimal model for cancer classification. We present comprehensive evaluations of these models on multiple datasets consisting of MRI scans and molecular data. Our experiments reveal that a specific architecture outperforms other state-of-the-art methods in terms of accuracy while taking into account computational complexity. Additionally, we analyze the effectiveness of incorporating additional modalities into our framework and showcase how these insights could potentially aid radiologists in making more accurate diagnoses.",1
"The Tesla vehicles became very popular in the car industry as it was affordable in the consumer market and it left no carbon footprint. Due to the large decline in the stock prices of Tesla Inc. at the beginning of 2019, Tesla owners started selling their vehicles in the used car market. These used car prices depended on attributes such as the model of the vehicle, year of production, miles driven, and the battery used for the vehicle. Prices were different for a specific vehicle in different months. In this paper, it is discussed how a machine learning technique is being implemented in order to develop a second-hand Teslavehicle price prediction system. To reach this goal, different machine learning techniques such as decision trees, support vector machine (SVM), random forest, and deep learning were investigated and finally was implemented with boosted decision tree regression. I the future, it is intended to use a more sophisticated algorithm for better accuracy.",0
"This research paper presents a novel approach to predicting the price of second hand Tesla vehicles by using machine learning algorithms on data collected from online marketplaces. The methodology involves preprocessing raw data into features such as mileage, battery condition, and available options which are then used as input variables in regression models. Our results show that our model accurately predicts the prices of these vehicles with high accuracy. We conclude by discussing potential future work and implications for both buyers and sellers of second hand electric cars.",1
"With the advent of Internet of Thing (IoT), and ubiquitous data collected every moment by either portable (smart phone) or fixed (sensor) devices, it is important to gain insights and meaningful information from the sensor data in context-aware computing environments. Many researches have been implemented by scientists in different fields, to analyze such data for the purpose of security, energy efficiency, building reliability and smart environments. One study, that many researchers are interested in, is to utilize Machine Learning techniques for occupancy detection where the aforementioned sensors gather information about the environment. This paper provides a solution to detect occupancy using sensor data by using and testing several variables. Additionally we show the analysis performed over the gathered data using Machine Learning and pattern recognition mechanisms is possible to determine the occupancy of indoor environments. Seven famous algorithms in Machine Learning, namely as Decision Tree, Random Forest, Gradient Boosting Machine, Logistic Regression, Naive Bayes, Kernelized SVM and K-Nearest Neighbors are tested and compared in this study.",0
"This paper presents an approach for detecting occupancy in rooms using sensor data. The proposed method leverages existing sensors present in many households and workplaces such as temperature and humidity sensors, light switches, motion sensors, etc. We show that these diverse types of sensory inputs can be combined effectively to create meaningful inferences on whether humans are occupying spaces within buildings. Our model processes time-series data from different sources and combines them in order to increase overall accuracy of occupancy detection. Experiments conducted with real building datasets demonstrate high levels of accuracy across several metrics commonly used in literature (precision, recall, F1 score). Lastly, our method makes no prior assumptions on user behavior, thus making it robust across different environments and schedules.",1
"In this article, we present a new machine learning model by imitation based on the linguistic description of complex phenomena. The idea consists of, first, capturing the behaviour of human players by creating a computational perception network based on the execution traces of the games and, second, representing it using fuzzy logic (linguistic variables and if-then rules). From this knowledge, a set of data (dataset) is automatically created to generate a learning model based on decision trees. This model will be used later to automatically control the movements of a bot. The result is an artificial agent that mimics the human player. We have implemented, tested and evaluated this technology. The results obtained are interesting and promising, showing that this method can be a good alternative to design and implement the behaviour of intelligent agents in video game development.",0
"Learning new skills through imitation has been a successful method since early human civilizations, as demonstrated by the proverb ""monkey see, monkey do."" With the advancement of technology, imitation can now take place through artificial agents that learn from demonstrations given in natural language instructions. This research focuses on utilizing natural language descriptions of player actions to train game characters. By using decision trees in conjunction with these natural language inputs, we can create more intelligent NPCs capable of performing complex tasks within virtual environments. Our experiments showcase the effectiveness of our approach, achieving high levels of accuracy in training character behaviors and resulting in improved overall gameplay experience. The implications of our work open up exciting possibilities for creating even more realistic video games, while paving the way for future developments in fields such as robotics and education.",1
"We study the problem of formally verifying individual fairness of decision tree ensembles, as well as training tree models which maximize both accuracy and individual fairness. In our approach, fairness verification and fairness-aware training both rely on a notion of stability of a classification model, which is a variant of standard robustness under input perturbations used in adversarial machine learning. Our verification and training methods leverage abstract interpretation, a well established technique for static program analysis which is able to automatically infer assertions about stability properties of decision trees. By relying on a tool for adversarial training of decision trees, our fairness-aware learning method has been implemented and experimentally evaluated on the reference datasets used to assess fairness properties. The experimental results show that our approach is able to train tree models exhibiting a high degree of individual fairness w.r.t. the natural state-of-the-art CART trees and random forests. Moreover, as a by-product, these fair decision trees turn out to be significantly compact, thus enhancing the interpretability of their fairness properties.",0
"Effective training of decision tree classifiers can be challenging due to their complex structure and potential vulnerability to overfitting. In recent years, several methods have been proposed to improve the reliability and interpretability of these models. This study focuses on exploring the impact of different parameter settings and feature selection techniques on the fairness of decision trees trained for binary classification problems. Using datasets from real-world applications, we analyze how variations in parameters such as minimum sample size, maximum depth, and pruning strategies affect model accuracy, bias, precision, recall, and F1 score. We also compare the performance of different feature selection algorithms, including filter methods like chi-squared test and mutual information, wrapper methods like forward selection and recursive feature elimination, and embedded methods like Lasso and Ridge regression. Our results indicate that careful tuning of hyperparameters combined with judicious feature subset selection can significantly enhance the equity of decision tree based predictions while minimizing loss of generalization ability. Our work provides insight into the design choices necessary for building more ethical machine learning systems and contributes to the growing literature on explainable artificial intelligence.",1
"To improve patient survival and treatment outcomes, early diagnosis of brain tumors is an essential task. It is a difficult task to evaluate the magnetic resonance imaging (MRI) images manually. Thus, there is a need for digital methods for tumor diagnosis with better accuracy. However, it is still a very challenging task in assessing their shape, volume, boundaries, tumor detection, size, segmentation, and classification. In this proposed work, we propose a hybrid ensemble method using Random Forest (RF), K-Nearest Neighbour, and Decision Tree (DT) (KNN-RF-DT) based on Majority Voting Method. It aims to calculate the area of the tumor region and classify brain tumors as benign and malignant. In the beginning, segmentation is done by using Otsu's Threshold method. Feature Extraction is done by using Stationary Wavelet Transform (SWT), Principle Component Analysis (PCA), and Gray Level Co-occurrence Matrix (GLCM), which gives thirteen features for classification. The classification is done by hybrid ensemble classifier (KNN-RF-DT) based on the Majority Voting method. Overall it aimed at improving the performance by traditional classifiers instead of going to deep learning. Traditional classifiers have an advantage over deep learning algorithms because they require small datasets for training and have low computational time complexity, low cost to the users, and can be easily adopted by less skilled people. Overall, our proposed method is tested upon dataset of 2556 images, which are used in 85:15 for training and testing respectively and gives good accuracy of 97.305%.",0
"In this paper, we aimed at evaluating the performance of different feature extraction techniques used in brain tumor detection and classification using magnetic resonance imaging (MRI) scans. We proposed a hybrid ensemble classifier model that integrates multiple features extracted from MRI images such as grey level intensity, Gabor wavelet texture features and geometric shapes. Our method achieved better results compared to individual feature sets alone, demonstrating the importance of utilizing multiple sources of data to improve accuracy. The obtained findings can aid clinicians in diagnosing and grading gliomas faster and more accurately, which could ultimately lead to earlier treatment and improved patient outcomes. Additionally, our research highlights the promise of incorporating advanced machine learning methods into radiology practice, paving the way for future developments in medical image analysis.",1
"Historical features are important in ads click-through rate (CTR) prediction, because they account for past engagements between users and ads. In this paper, we study how to efficiently construct historical features through counting features. The key challenge of such problem lies in how to automatically identify counting keys. We propose a tree-based method for counting key selection. The intuition is that a decision tree naturally provides various combinations of features, which could be used as counting key candidate. In order to select personalized counting features, we train one decision tree model per user, and the counting keys are selected across different users with a frequency-based importance measure. To validate the effectiveness of proposed solution, we conduct large scale experiments on Twitter video advertising data. In both online learning and offline training settings, the automatically identified counting features outperform the manually curated counting features.",0
"Here is your new abstract:  As advertisers seek more advanced methods for predicting user interests and preferences, there has been growing interest in using historical data to inform future predictions. However, accurately analyzing large volumes of past user behavioral patterns can be challenging due to their intrinsic tree structures, which may lead to significant computational overhead and suboptimal results. To address these issues, we propose a novel approach that leverages recursive feature generation techniques based on decision trees (RFGDT) to extract meaningful features from complex tree structures in ad prediction tasks. Our method effectively identifies important characteristics and relationships within the dataset by recursively splitting tree nodes into smaller subsets, which enables fine-grained feature extraction without incurring excessive computation costs. In our experiments, RFGDT achieves promising results compared against state-of-the-art models, demonstrating substantial improvements across multiple evaluation metrics. We hope this work will contribute towards building efficient, scalable, and accurate solutions for predicting users’ preferences in real-world applications.",1
"Despite the latest prevailing success of deep neural networks (DNNs), several concerns have been raised against their usage, including the lack of intepretability the gap between DNNs and other well-established machine learning models, and the growingly expensive computational costs. A number of recent works [1], [2], [3] explored the alternative to sequentially stacking decision tree/random forest building blocks in a purely feed-forward way, with no need of back propagation. Since decision trees enjoy inherent reasoning transparency, such deep forest models can also facilitate the understanding of the internaldecision making process. This paper further extends the deep forest idea in several important aspects. Firstly, we employ a probabilistic tree whose nodes make probabilistic routing decisions, a.k.a., soft routing, rather than hard binary decisions.Besides enhancing the flexibility, it also enables non-greedy optimization for each tree. Second, we propose an innovative topology learning strategy: every node in the ree now maintains a new learnable hyperparameter indicating the probability that it will be a leaf node. In that way, the tree will jointly optimize both its parameters and the tree topology during training. Experiments on the MNIST dataset demonstrate that our empowered deep forests can achieve better or comparable performance than [1],[3] , with dramatically reduced model complexity. For example,our model with only 1 layer of 15 trees can perform comparably with the model in [3] with 2 layers of 2000 trees each.",0
"In recent years, deep learning has revolutionized many fields due to advances in both hardware and software technology. One key component that enables these achievements is efficient computation using high quality differentiable implementations that enable data propagation back through complex models during backpropagation. Despite significant progress made on the accuracy and efficiency fronts by modern autodifferentiable systems like JAX, there still exists a lot of room for improvement, especially in terms of memory usage, speedup from parallelization and more generally model expressiveness. To that end, we present Growing Deep Forests (GDF), which is designed as an optimized framework for training machine learning models while addressing memory constraints, speed bottlenecks and scalability issues. Our work leverages soft routing techniques to learn connectivity, allowing us to dynamically grow networks at every layer based on their importance. We demonstrate that our method significantly reduces memory overheads and leads to large speedups compared to existing frameworks, while maintaining competitive performance across a wide range of image classification benchmark tasks. Ultimately, we hope that our novel approach can serve as a stepping stone towards enabling even deeper neural networks on modest computational resources.",1
"Machine Learning applications have brought new insights into a secondary analysis of medical data. Machine Learning helps to develop new drugs, define populations susceptible to certain illnesses, identify predictors of many common diseases. At the same time, Machine Learning results depend on convolution of many factors, including feature selection, class (im)balance, algorithm preference, and performance metrics. In this paper, we present explainable multi-class classification of a large medical data set. We in details discuss knowledge-based feature engineering, data set balancing, best model selection, and parameter tuning. Six algorithms are used in this study: Support Vector Machine (SVM), Na\""ive Bayes, Gradient Boosting, Decision Trees, Random Forest, and Logistic Regression. Our empirical evaluation is done on the UCI Diabetes 130-US hospitals for years 1999-2008 dataset, with the task to classify patient hospital re-admission stay into three classes: 0 days, 30 days, or  30 days. Our results show that using 23 medication features in learning experiments improves Recall of five out of the six applied learning algorithms. This is a new result that expands the previous studies conducted on the same data. Gradient Boosting and Random Forest outperformed other algorithms in terms of the three-class classification Accuracy.",0
"Incorporate all keywords listed under ""keywords"" if possible. Do use keyword ""medical data"".",1
"We put forward a novel learning methodology for ensembles of decision trees based on a genetic algorithm which is able to train a decision tree for maximizing both its accuracy and its robustness to adversarial perturbations. This learning algorithm internally leverages a complete formal verification technique for robustness properties of decision trees based on abstract interpretation, a well known static program analysis technique. We implemented this genetic adversarial training algorithm in a tool called Meta-Silvae (MS) and we experimentally evaluated it on some reference datasets used in adversarial training. The experimental results show that MS is able to train robust models that compete with and often improve on the current state-of-the-art of adversarial training of decision trees while being much more compact and therefore interpretable and efficient tree models.",0
"In recent years, decision trees have become increasingly popular as powerful models in various domains such as computer vision and natural language processing. However, due to their complexity and sensitivity to input features, decision trees can often suffer from overfitting, leading to poor generalization performance on unseen data. This problem has motivated researchers to explore techniques that could improve the robustness and accuracy of decision tree models. One promising approach is genetic adversarial training (GAT), which combines two competing objectives: one maximizes model utility while minimizing vulnerability to attacks, and the other maximizes attack strength by inducing structured perturbations into input features to deceive the model. By integrating these two objectives using gradient ascent methods, we show that GAT leads to more robust decision trees that achieve higher test accuracy compared to state-of-the-art baselines across multiple benchmark datasets. Our analysis further reveals interesting insights into how GAT affects the structure of decision trees, providing new perspectives on the role of regularization in deep learning. Overall, our work presents evidence that GAT is a simple yet effective method that could significantly advance the field of machine learning.",1
"In the present day we use machine learning for sensitive tasks that require models to be both understandable and robust. Although traditional models such as decision trees are understandable, they suffer from adversarial attacks. When a decision tree is used to differentiate between a user's benign and malicious behavior, an adversarial attack allows the user to effectively evade the model by perturbing the inputs the model receives. We can use algorithms that take adversarial attacks into account to fit trees that are more robust. In this work we propose an algorithm, GROOT, that is two orders of magnitude faster than the state-of-the-art-work while scoring competitively on accuracy against adversaries. GROOT accepts an intuitive and permissible threat model. Where previous threat models were limited to distance norms, we allow each feature to be perturbed with a user-specified parameter: either a maximum distance or constraints on the direction of perturbation. Previous works assumed that both benign and malicious users attempt model evasion but we allow the user to select which classes perform adversarial attacks. Additionally, we introduce a hyperparameter rho that allows GROOT to trade off performance in the regular and adversarial settings.",0
"Here we present a novel approach to training decision trees that makes them more robust against adversarial examples, while still maintaining high accuracy on clean inputs. Our method involves regularizing the tree during training by adding random noise to both the input features and labels. This ensures that the tree learns patterns that are less likely to be fooled by small perturbations to the input data, which can cause the tree to make incorrect predictions. In addition, our method allows for efficient training and testing of large decision trees, making it scalable to real-world applications. We empirically demonstrate the effectiveness of our approach using several benchmark datasets across different domains. Overall, our results show that our method significantly reduces the susceptibility of decision trees to adversarial attacks, without compromising their performance on clean inputs.",1
"This work was done with the aim of developing the fundamental breast cancer early differential diagnosis foundations based on modeling the space-time temperature distribution using the microwave radiothermometry method and obtained data intelligent analysis. The article deals with the machine learning application in the microwave radiothermometry data analysis. The problems associated with the construction mammary glands temperature fields computer models for patients with various diagnostics classes, are also discussed. With the help of a computer experiment, based on the machine learning algorithms set (logistic regression, naive Bayesian classifier, support vector machine, decision tree, gradient boosting, K-nearest neighbors, etc.) usage, the mammary glands temperature fields computer models set adequacy.",0
"This paper presents an application of computer simulation techniques and machine learning algorithms in order to analyze and interpret microwave radiometric thermometer (MRT) measurements. MRTs have been widely used as nonintrusive temperature sensors in industrial and scientific applications due to their high accuracy and reliability compared to traditional methods such as resistance thermometers or infrared thermocouples. Computer simulations were performed using the finite element method (FEM) software COMSOL Multiphysics to model MRT systems in different environments. Simulations allowed us to obtain detailed profiles of MRT output signals and compare them with experimental findings. Machine learning techniques including artificial neural networks (ANN), decision trees, and random forest regression were then applied to process large amounts of raw MRT data collected from real-world experiments. By analyzing the relationship between input parameters and measured temperatures, these models provided improved accuracy and precision of thermal profiling over conventional linear fitting algorithms. The novel combination of FEM simulation and machine learning demonstrated promising benefits for efficient analysis and interpretation of complex MRT systems and has great potential for further development in industrial and scientific research fields.",1
"Adverse Drug Reactions (ADRs) are characterized within randomized clinical trials and postmarketing pharmacovigilance, but their molecular mechanism remains unknown in most cases. Aside from clinical trials, many elements of knowledge about drug ingredients are available in open-access knowledge graphs. In addition, drug classifications that label drugs as either causative or not for several ADRs, have been established. We propose to mine knowledge graphs for identifying biomolecular features that may enable reproducing automatically expert classifications that distinguish drug causative or not for a given type of ADR. In an explainable AI perspective, we explore simple classification techniques such as Decision Trees and Classification Rules because they provide human-readable models, which explain the classification itself, but may also provide elements of explanation for molecular mechanisms behind ADRs. In summary, we mine a knowledge graph for features; we train classifiers at distinguishing, drugs associated or not with ADRs; we isolate features that are both efficient in reproducing expert classifications and interpretable by experts (i.e., Gene Ontology terms, drug targets, or pathway names); and we manually evaluate how they may be explanatory. Extracted features reproduce with a good fidelity classifications of drugs causative or not for DILI and SCAR. Experts fully agreed that 73% and 38% of the most discriminative features are possibly explanatory for DILI and SCAR, respectively; and partially agreed (2/3) for 90% and 77% of them. Knowledge graphs provide diverse features to enable simple and explainable models to distinguish between drugs that are causative or not for ADRs. In addition to explaining classifications, most discriminative features appear to be good candidates for investigating ADR mechanisms further.",0
"Mining data from Knowledge Graphs can provide insights into alternative dispute resolution (ADR) mechanisms by identifying patterns, trends, and relationships among parties involved in disputes. By using artificial intelligence techniques like natural language processing, machine learning, and graph theory algorithms, we can gain deeper understanding of how different factors such as case type, location, and party characteristics influence the outcome of ADR processes. This study presents an approach that combines these methods to analyze large datasets from legal domains to uncover hidden correlations and relationships that could not have been discovered through traditional manual analysis alone. Our results demonstrate how this innovative methodology can contribute significantly towards improving our knowledge on the effectiveness of ADR mechanisms and inform decision making for policymakers and practitioners. We anticipate that further development and application of explainable AI in this area will drive significant advancements in resolving conflicts more effectively, efficiently, and fairly.",1
"State-of-the-art learning algorithms, such as random forests or neural networks, are often qualified as ""black-boxes"" because of the high number and complexity of operations involved in their prediction mechanism. This lack of interpretability is a strong limitation for applications involving critical decisions, typically the analysis of production processes in the manufacturing industry. In such critical contexts, models have to be interpretable, i.e., simple, stable, and predictive. To address this issue, we design SIRUS (Stable and Interpretable RUle Set), a new classification algorithm based on random forests, which takes the form of a short list of rules. While simple models are usually unstable with respect to data perturbation, SIRUS achieves a remarkable stability improvement over cutting-edge methods. Furthermore, SIRUS inherits a predictive accuracy close to random forests, combined with the simplicity of decision trees. These properties are assessed both from a theoretical and empirical point of view, through extensive numerical experiments based on our R/C++ software implementation sirus available from CRAN.",0
"This paper presents a new method for generating stable and interpretable rule sets for classification tasks. Our approach, called SIRUS (Stable and Interpretable RuleSet), builds on recent advances in symbolic regression to construct decision rules that can accurately classify data points while providing insight into how those decisions are made. We evaluate our method using several benchmark datasets and compare its performance to state-of-the-art methods for generating interpretable models. Our results demonstrate that SIRUS achieves competitive accuracy while producing more stable and easier-to-understand rule sets than alternative approaches. In addition, we provide a comprehensive analysis of how different hyperparameter settings affect the stability and interpretability of the generated rule sets. Overall, we believe that SIRUS represents a promising step towards creating more reliable and transparent machine learning models.",1
"Verifiable training has shown success in creating neural networks that are provably robust to a given amount of noise. However, despite only enforcing a single robustness criterion, its performance scales poorly with dataset complexity. On CIFAR10, a non-robust LeNet model has a 21.63% error rate, while a model created using verifiable training and a L-infinity robustness criterion of 8/255, has an error rate of 57.10%. Upon examination, we find that when labeling visually similar classes, the model's error rate is as high as 61.65%. We attribute the loss in performance to inter-class similarity. Similar classes (i.e., close in the feature space) increase the difficulty of learning a robust model. While it's desirable to train a robust model for a large robustness region, pairwise class similarities limit the potential gains. Also, consideration must be made regarding the relative cost of mistaking similar classes. In security or safety critical tasks, similar classes are likely to belong to the same group, and thus are equally sensitive.   In this work, we propose a new approach that utilizes inter-class similarity to improve the performance of verifiable training and create robust models with respect to multiple adversarial criteria. First, we use agglomerate clustering to group similar classes and assign robustness criteria based on the similarity between clusters. Next, we propose two methods to apply our approach: (1) Inter-Group Robustness Prioritization, which uses a custom loss term to create a single model with multiple robustness guarantees and (2) neural decision trees, which trains multiple sub-classifiers with different robustness guarantees and combines them in a decision tree architecture. On Fashion-MNIST and CIFAR10, our approach improves clean performance by 9.63% and 30.89% respectively. On CIFAR100, our approach improves clean performance by 26.32%.",0
"This paper presents a novel approach to verifiable training that leverages pairwise class similarity to enhance performance and flexibility. We introduce a methodology based on adaptively selecting pairs of classes during each iteration of the training process. Our framework enables fine-grained control over the trade-off between accuracy and computational efficiency by adjusting key hyperparameters controlling the selection frequency and size of class pairs. Additionally, our technique ensures robustness against overfitting through regularization by using cosine similarities calculated from model outputs as opposed to popular heuristics such as Jaccard distance or Manhattan distance commonly used in literature. Results demonstrate significant improvement compared to state-of-the-art methods across various benchmark datasets under different experimental settings including imbalanced data regimes. These results validate the effectiveness of our proposed approach paving the way towards more efficient, accurate and verifiable deep learning applications.",1
"Machine learning with application to questions in the physical sciences has become a widely used tool, successfully applied to classification, regression and optimization tasks in many areas. Research focus mostly lies in improving the accuracy of the machine learning models in numerical predictions, while scientific understanding is still almost exclusively generated by human researchers analysing numerical results and drawing conclusions. In this work, we shift the focus on the insights and the knowledge obtained by the machine learning models themselves. In particular, we study how it can be extracted and used to inspire human scientists to increase their intuitions and understanding of natural systems. We apply gradient boosting in decision trees to extract human interpretable insights from big data sets from chemistry and physics. In chemistry, we not only rediscover widely know rules of thumb but also find new interesting motifs that tell us how to control solubility and energy levels of organic molecules. At the same time, in quantum physics, we gain new understanding on experiments for quantum entanglement. The ability to go beyond numerics and to enter the realm of scientific insight and hypothesis generation opens the door to use machine learning to accelerate the discovery of conceptual understanding in some of the most challenging domains of science.",0
"Machine Learning (ML) has revolutionized many fields including astronomy, biology and neuroscience through image analysis, signal processing and data mining. In science, progress often originates from unexpected discoveries based on new working hypotheses whose predictions end up being correct. Inspired by these successful applications, we explore here the potential benefits that artificial intelligence (AI) can bring to scientific research beyond mere automation by discussing two concrete examples of how unsupervised machine learning algorithms could have provided scientists unexpected insights into their respective topics. Firstly, we showcase how AI helped generate more accurate cosmological simulations thanks to neural network optimization techniques. Secondly, we describe how deep learning models can lead to novel drug discovery paths via virtual organic synthesis routes identification. These case studies demonstrate that even though the current generation of machines might lack general human understanding, they already contain enough knowledge and cleverness to act as genuinely creative tools at the service of human curiosity.  Keywords: Machine Learning - Artificial Intelligence - Cosmology - Pharmacy - Simulations – Drug Discovery – Deep Learning - Neural Networks - Optimization – Virtual Synthesis Routes – Hypotheses Generation",1
"Explainable components in XAI algorithms often come from a familiar set of models, such as linear models or decision trees. We formulate an approach where the type of explanation produced is guided by a specification. Specifications are elicited from the user, possibly using interaction with the user and contributions from other areas. Areas where a specification could be obtained include forensic, medical, and scientific applications. Providing a menu of possible types of specifications in an area is an exploratory knowledge representation and reasoning task for the algorithm designer, aiming at understanding the possibilities and limitations of efficiently computable modes of explanations. Two examples are discussed: explanations for Bayesian networks using the theory of argumentation, and explanations for graph neural networks. The latter case illustrates the possibility of having a representation formalism available to the user for specifying the type of explanation requested, for example, a chemical query language for classifying molecules. The approach is motivated by a theory of explanation in the philosophy of science, and it is related to current questions in the philosophy of science on the role of machine learning.",0
"In recent years, there has been increasing interest in the field of artificial intelligence (AI) in understanding how humans explain complex concepts to each other, particularly through language. One approach that has gained popularity is the idea that human explanation can be formalized as a type of logical reasoning called ""explanatory deduction."" This method involves starting with a general set of principles or axioms, then using deductive logic to derive specific explanations for particular phenomena based on these principles. However, while this method has shown promise in some domains, it faces several challenges in practice. For example, it may struggle to capture the nuances and subtleties of natural language, and it may require extensive knowledge engineering to specify the relevant axioms and inference rules. To address these limitations, we propose a new framework for explaining from specification. Our approach builds upon traditional methods but incorporates advanced machine learning techniques to improve scalability, robustness, and flexibility. We demonstrate the effectiveness of our framework on a variety of tasks, including text summarization, question answering, and visual reasoning, achieving state-of-the-art results across multiple benchmarks. Overall, our work represents an important step towards developing more powerful AI systems capable of producing high-quality explanations and supporting human decision making in complex situations.",1
"Federated Learning (FL) is an approach to collaboratively train a model across multiple parties without sharing data between parties or an aggregator. It is used both in the consumer domain to protect personal data as well as in enterprise settings, where dealing with data domicile regulation and the pragmatics of data silos are the main drivers. While gradient boosted tree implementations such as XGBoost have been very successful for many use cases, its federated learning adaptations tend to be very slow due to using cryptographic and privacy methods and have not experienced widespread use. We propose the Party-Adaptive XGBoost (PAX) for federated learning, a novel implementation of gradient boosting which utilizes a party adaptive histogram aggregation method, without the need for data encryption. It constructs a surrogate representation of the data distribution for finding splits of the decision tree. Our experimental results demonstrate strong model performance, especially on non-IID distributions, and significantly faster training run-time across different data sets than existing federated implementations. This approach makes the use of gradient boosted trees practical in enterprise federated learning.",0
"Abstract: This research proposes the use of adaptive histogram-based gradient boosted trees (AHGBT) for federated learning. The main objective is to improve the accuracy of distributed machine learning models by effectively utilizing data from multiple sources. AHGBT has been shown to perform well on imbalanced datasets and can handle high dimensionality problems efficiently. By using gradient boosting methods, we can develop accurate predictors while controlling overfitting. Additionally, we propose a novel strategy that leverages histogram binning techniques during model training, which helps achieve better performance compared to traditional gradient boosting algorithms. Our evaluation shows that our approach leads to significant improvements in both accuracy and efficiency across several benchmark datasets. These results demonstrate the potential benefits of incorporating AHGBT into federated learning frameworks.",1
"Decision trees are machine learning models commonly used in various application scenarios. In the era of big data, traditional decision tree induction algorithms are not suitable for learning large-scale datasets due to their stringent data storage requirement. Online decision tree learning algorithms have been devised to tackle this problem by concurrently training with incoming samples and providing inference results. However, even the most up-to-date online tree learning algorithms still suffer from either high memory usage or high computational intensity with dependency and long latency, making them challenging to implement in hardware. To overcome these difficulties, we introduce a new quantile-based algorithm to improve the induction of the Hoeffding tree, one of the state-of-the-art online learning models. The proposed algorithm is light-weight in terms of both memory and computational demand, while still maintaining high generalization ability. A series of optimization techniques dedicated to the proposed algorithm have been investigated from the hardware perspective, including coarse-grained and fine-grained parallelism, dynamic and memory-based resource sharing, pipelining with data forwarding. Following this, we present Hard-ODT, a high-performance, hardware-efficient and scalable online decision tree learning system on a field-programmable gate array (FPGA) with system-level optimization techniques. Performance and resource utilization are modeled for the complete learning system for early and fast analysis of the trade-off between various design metrics. Finally, we propose a design flow in which the proposed learning system is applied to FPGA run-time power monitoring as a case study.",0
"This paper presents a new algorithm and system for online decision tree learning that is designed to be hardware friendly. The proposed approach, called Hard-ODT, leverages recent advances in parallel computing and memory management technologies to achieve high performance on modern computer systems while minimizing resource utilization. Experimental results show that Hard-ODT significantly outperforms state-of-the-art online decision tree algorithms in terms of both accuracy and speed, making it a promising solution for large-scale real-time classification tasks. In addition, the hardware friendliness of the algorithm enables it to be deployed efficiently across a wide range of platforms, from cloud servers to embedded devices. Overall, this work represents an important contribution towards enabling efficient machine learning at scale.",1
"Decision trees and their ensembles are endowed with a rich set of diagnostic tools for ranking and screening variables in a predictive model. Despite the widespread use of tree based variable importance measures, pinning down their theoretical properties has been challenging and therefore largely unexplored. To address this gap between theory and practice, we derive finite sample performance guarantees for variable selection in nonparametric models using a single-level CART decision tree (a decision stump). Under standard operating assumptions in variable screening literature, we find that the marginal signal strength of each variable and ambient dimensionality can be considerably weaker and higher, respectively, than state-of-the-art nonparametric variable selection methods. Furthermore, unlike previous marginal screening methods that attempt to directly estimate each marginal projection via a truncated basis expansion, the fitted model used here is a simple, parsimonious decision stump, thereby eliminating the need for tuning the number of basis terms. Thus, surprisingly, even though decision stumps are highly inaccurate for estimation purposes, they can still be used to perform consistent model selection.",0
"This study presents a nonparametric approach to variable screening using decision stumps as a model selection method. We propose a novel technique that involves selecting optimal decision trees based on statistical significance testing. Our method differs from traditional approaches by considering both categorical and continuous variables simultaneously. We conduct simulations to evaluate the performance of our proposed method compared to existing methods, including random forest feature selection. Results demonstrate improved accuracy and stability across diverse scenarios. Finally, we apply the methodology to real datasets to illustrate its applicability and effectiveness in identifying meaningful predictors. Overall, our work contributes new insights into the use of decision tree models for variable screening in complex data sets.",1
"Incomplete data are a common feature in many domains, from clinical trials to industrial applications. Bayesian networks (BNs) are often used in these domains because of their graphical and causal interpretations. BN parameter learning from incomplete data is usually implemented with the Expectation-Maximisation algorithm (EM), which computes the relevant sufficient statistics (""soft EM"") using belief propagation. Similarly, the Structural Expectation-Maximisation algorithm (Structural EM) learns the network structure of the BN from those sufficient statistics using algorithms designed for complete data. However, practical implementations of parameter and structure learning often impute missing data (""hard EM"") to compute sufficient statistics instead of using belief propagation, for both ease of implementation and computational speed. In this paper, we investigate the question: what is the impact of using imputation instead of belief propagation on the quality of the resulting BNs? From a simulation study using synthetic data and reference BNs, we find that it is possible to recommend one approach over the other in several scenarios based on the characteristics of the data. We then use this information to build a simple decision tree to guide practitioners in choosing the EM algorithm best suited to their problem.",0
"Abstract: This work presents a new method for learning Bayesian networks from incomplete data using hard and soft evidence mixture (HEM) techniques. HEM combines both hard and soft constraints by defining intervals over probabilities consistent with partial observed evidence. We use Markov Chain Monte Carlo sampling methods to fit statistical models to the incomplete data using our novel method, which allows us to estimate model parameters while ensuring that all probabilities lie within these intervals. Our experimental results demonstrate that our approach significantly improves the accuracy of learned Bayesian network structures compared to state-of-the-art algorithms. Additionally, we show how our method can handle nonlinear dependencies among variables and incorporate prior knowledge into the learning process. Overall, our contributions advance the field of Bayesian network learning under imperfect observation scenarios, promoting greater flexibility and robustness in learning accurate probabilistic representations of real-world systems.",1
"We propose a novel high-performance and interpretable canonical deep tabular data learning architecture, TabNet. TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features. We demonstrate that TabNet outperforms other neural network and decision tree variants on a wide range of non-performance-saturated tabular datasets and yields interpretable feature attributions plus insights into the global model behavior. Finally, for the first time to our knowledge, we demonstrate self-supervised learning for tabular data, significantly improving performance with unsupervised representation learning when unlabeled data is abundant.",0
"Introduce me to Tabnet! Abstract:  TabNet presents an innovative tabular learning model that incorporates attentiveness and interpretability for enhanced performance on structured data processing tasks. By leveraging attention mechanisms during inference, our method dynamically weights input features based on their relevance at each step of inference, improving predictive accuracy while providing detailed insight into the decision-making process. Combined with a novel approach to visualizing attention weights as heatmaps over tables, we provide unparalleled accessibility to complex patterns found within table-structured datasets, facilitating human understanding and post-hoc analysis of results. With extensive experiments across multiple benchmarks and real-world applications, including handwritten digit recognition, credit risk prediction, and drug safety signal detection from electronic health records, TabNet consistently demonstrates state-of-the-art performance compared against contemporary methods without compromising transparency or explainability. These contributions significantly advance the capabilities and utility of machine learning models operating on tabular data, empowering domain experts to more effectively collaborate alongside autonomous systems and make better decisions supported by accurate and interpretable findings.",1
"We consider the problem of determining which classes of functions can be tested more efficiently than they can be learned, in the distribution-free sample-based model that corresponds to the standard PAC learning setting. Our main result shows that while VC dimension by itself does not always provide tight bounds on the number of samples required to test a class of functions in this model, it can be combined with a closely-related variant that we call ""lower VC"" (or LVC) dimension to obtain strong lower bounds on this sample complexity.   We use this result to obtain strong and in many cases nearly optimal lower bounds on the sample complexity for testing unions of intervals, halfspaces, intersections of halfspaces, polynomial threshold functions, and decision trees. Conversely, we show that two natural classes of functions, juntas and monotone functions, can be tested with a number of samples that is polynomially smaller than the number of samples required for PAC learning.   Finally, we also use the connection between VC dimension and property testing to establish new lower bounds for testing radius clusterability and testing feasibility of linear constraint systems.",0
"This paper presents a theoretical framework for distribution-free sample-based testing based on the notion of VC dimension. We introduce the concept of VC dimension as an informational measure of complexity for sets of samples, which enables us to define meaningful criteria that can be used in the design and analysis of sample-based tests for arbitrary data distributions. Our approach offers several advantages over existing methods: it provides explicit error bounds, allows for easy adaptation to varying amounts of data, admits straightforward implementation using existing algorithms, and is compatible with many applications including supervised learning and neural networks. As part of our development, we establish results on the rates at which errors converge under generalization from the training set to independent test points drawn from any unknown distribution. This shows the power of VC dimension as a tool for understanding learning theory and demonstrates the value of combining statistical ideas with computer science tools. To validate our findings, we provide extensive experimental evidence demonstrating the effectiveness of the proposed methodology across multiple domains. In summary, this work advances knowledge in the field by providing insights into how one might apply computational techniques inspired by machine learning problems to obtain optimal guarantees for classical scientific computing tasks involving random matrices.  Note: You should always ensure there is no plagiarism before submitting your paper and you should never submit someone else’s content as your own.",1
"Deep learning-based methods have achieved promising performance in early detection and classification of lung nodules, most of which discard unsure nodules and simply deal with a binary classification -- malignant vs benign. Recently, an unsure data model (UDM) was proposed to incorporate those unsure nodules by formulating this problem as an ordinal regression, showing better performance over traditional binary classification. To further explore the ordinal relationship for lung nodule classification, this paper proposes a meta ordinal regression forest (MORF), which improves upon the state-of-the-art ordinal regression method, deep ordinal regression forest (DORF), in three major ways. First, MORF can alleviate the biases of the predictions by making full use of deep features while DORF needs to fix the composition of decision trees before training. Second, MORF has a novel grouped feature selection (GFS) module to re-sample the split nodes of decision trees. Last, combined with GFS, MORF is equipped with a meta learning-based weighting scheme to map the features selected by GFS to tree-wise weights while DORF assigns equal weights for all trees. Experimental results on the LIDC-IDRI dataset demonstrate superior performance over existing methods, including the state-of-the-art DORF.",0
"In recent years, lung nodule detection has become increasingly important due to the rise in lung cancer cases worldwide. This research proposes the use of meta ordinal regression forest (MORF) as a method for learning with unsure lung nodules. MORF is a machine learning algorithm that utilizes decision trees and random forests to optimize classification and regression problems by combining multiple models into one framework. The proposed approach uses MORF to learn from multiple datasets containing uncertain lung nodules and then integrate these predictions using a multi-class support vector machine classifier. Results showed significant improvements over traditional approaches such as binary classifiers and ensemble methods like bagging and boosting. Overall, our study demonstrates the potential of using MORF for effective diagnosis of unsure lung nodules.",1
"We propose an approach for learning optimal tree-based prescription policies directly from data, combining methods for counterfactual estimation from the causal inference literature with recent advances in training globally-optimal decision trees. The resulting method, Optimal Policy Trees, yields interpretable prescription policies, is highly scalable, and handles both discrete and continuous treatments. We conduct extensive experiments on both synthetic and real-world datasets and demonstrate that these trees offer best-in-class performance across a wide variety of problems.",0
This is because we want reviewers to evaluate your work independently from other papers (as if they haven't seen the actual titles). Reviewers should focus on evaluating only based on content rather than author/team reputation etc. Thanks!,1
"A central aspect of online decision tree solutions is evaluating the incoming data and enabling model growth. For such, trees much deal with different kinds of input features and partition them to learn from the data. Numerical features are no exception, and they pose additional challenges compared to other kinds of features, as there is no trivial strategy to choose the best point to make a split decision. The problem is even more challenging in regression tasks because both the features and the target are continuous. Typical online solutions evaluate and store all the points monitored between split attempts, which goes against the constraints posed in real-time applications. In this paper, we introduce the Quantization Observer (QO), a simple yet effective hashing-based algorithm to monitor and evaluate split point candidates in numerical features for online tree regressors. QO can be easily integrated into incremental decision trees, such as Hoeffding Trees, and it has a monitoring cost of $O(1)$ per instance and sub-linear cost to evaluate split candidates. Previous solutions had a $O(\log n)$ cost per insertion (in the best case) and a linear cost to evaluate split points. Our extensive experimental setup highlights QO's effectiveness in providing accurate split point suggestions while spending much less memory and processing time than its competitors.",0
"Online tree regression requires fast decision making, but often suffers from high computational cost during training. To address these issues, we present a novel method that uses dynamical quantization to reduce computation while maintaining good performance on two benchmark datasets: CASIA NIR-VIS 2.0 Face Liveness Detection dataset and UWA Pedestrian dataset. Our approach dynamically chooses which branches of a classification tree to quantize using heuristics based on each individual sample’s complexity. We show that our method can significantly speed up both training and testing time without sacrificing accuracy compared to traditional methods such as random forest classifiers. In addition, our model achieves competitive results with state-of-the-art approaches at inference time. These findings demonstrate the potential utility of dynamic quantization for real-time applications where accurate decisions must be made quickly.",1
"Machine learned models often must abide by certain requirements (e.g., fairness or legal). This has spurred interested in developing approaches that can provably verify whether a model satisfies certain properties. This paper introduces a generic algorithm called Veritas that enables tackling multiple different verification tasks for tree ensemble models like random forests (RFs) and gradient boosting decision trees (GBDTs). This generality contrasts with previous work, which has focused exclusively on either adversarial example generation or robustness checking. Veritas formulates the verification task as a generic optimization problem and introduces a novel search space representation. Veritas offers two key advantages. First, it provides anytime lower and upper bounds when the optimization problem cannot be solved exactly. In contrast, many existing methods have focused on exact solutions and are thus limited by the verification problem being NP-complete. Second, Veritas produces full (bounded suboptimal) solutions that can be used to generate concrete examples. We experimentally show that Veritas outperforms the previous state of the art by (a) generating exact solutions more frequently, (b) producing tighter bounds when (a) is not possible, and (c) offering orders of magnitude speed ups. Subsequently, Veritas enables tackling more and larger real-world verification scenarios.",0
"Title: Abstract: Versatile Verification of Tree Ensembles  This study addresses the challenge of verifying tree ensembles, which have become increasingly popular as predictive models due to their ability to handle complex data and perform well on a variety of tasks. However, current methods for validating these models can be time-consuming and limited in scope. To overcome these limitations, we propose a versatile approach that combines multiple techniques to provide comprehensive evaluation of tree ensemble performance. Our methodology integrates sensitivity analysis, feature importance measures, visualization tools, and statistical tests, enabling detailed assessment of model reliability, robustness, and interpretability. Experiments conducted on several real-world datasets demonstrate the effectiveness of our approach in identifying strengths and weaknesses of tree ensembles, pinpointing areas requiring improvement, and guiding model selection and optimization. This work contributes to the field by offering a flexible framework for efficient and thorough examination of tree ensembles, facilitating more confident deployment of these powerful prediction tools across diverse applications.",1
"Monte Carlo Tree Search (MCTS) is a branch of stochastic modeling that utilizes decision trees for optimization, mostly applied to artificial intelligence (AI) game players. This project imagines a game in which an AI player searches for a stationary target within a 2-D lattice. We analyze its behavior with different target distributions and compare its efficiency to the Levy Flight Search, a model for animal foraging behavior. In addition to simulated data analysis we prove two theorems about the convergence of MCTS when computation constraints neglected.",0
"In general game playing tasks, the problem is often simplified by using Monte Carlo tree search (MCTS) as a heuristic search method. However, the algorithm is still difficult to implement in some cases since there exist multiple goals at the same time. To overcome this problem, MCTS has been improved so that it can handle a one-to-one confrontation game called ""a single target search"" for two players competing on a 2-D grid board. Our approach extends previous studies and uses CFR (upper confidence bound for trees), IDA* (improved lower bound heuristics), and UCT (upper confidence bounds applied to policy selection) to find out how effective the enhanced MCTS can perform compared to other algorithms such as minimax. As a result of experimenting, we found that our improved MCTS performed better than previous methods when searching for paths that minimize regret values in games. This new algorithm should contribute significantly to future research on developing MCTS for multi-target decision making problems like cooperative game systems under uncertainty.",1
"Data-driven decision making is gaining prominence with the popularity of various machine learning models. Unfortunately, real-life data used in machine learning training may capture human biases, and as a result the learned models may lead to unfair decision making. In this paper, we provide a solution to this problem for decision trees and random forests. Our approach converts any decision tree or random forest into a fair one with respect to a specific data set, fairness criteria, and sensitive attributes. The \emph{FairRepair} tool, built based on our approach, is inspired by automated program repair techniques for traditional programs. It uses an SMT solver to decide which paths in the decision tree could have their outcomes flipped to improve the fairness of the model. Our experiments on the well-known adult dataset from UC Irvine demonstrate that FairRepair scales to realistic decision trees and random forests. Furthermore, FairRepair provides formal guarantees about soundness and completeness of finding a repair. Since our fairness-guided repair technique repairs decision trees and random forests obtained from a given (unfair) data-set, it can help to identify and rectify biases in decision-making in an organisation.",0
"This paper proposes a novel approach for improving decision trees and random forests by incorporating fairness considerations into their construction process using symmetrical mutlicriteria optimization (SMO). The proposed method addresses concerns regarding group fairness, which ensures that different subgroups in the population receive comparable predictions across multiple metrics. By considering both accuracy and fairness simultaneously during training, our model achieves better predictive performance than state-of-the-art approaches while maintaining competitive levels of fairness. We evaluate our method on several benchmark datasets and demonstrate the effectiveness of our approach through comprehensive experiments and comparison studies. Our work provides insights into balancing the trade-offs between prediction quality and subgroup protection while offering a new perspective on enhancing model interpretability and accountability. Overall, we believe that this research contributes significantly towards building trustworthy artificial intelligence systems that align better with societal values and expectations.",1
"Decision Trees (DTs) and Random Forests (RFs) are powerful discriminative learners and tools of central importance to the everyday machine learning practitioner and data scientist. Due to their discriminative nature, however, they lack principled methods to process inputs with missing features or to detect outliers, which requires pairing them with imputation techniques or a separate generative model. In this paper, we demonstrate that DTs and RFs can naturally be interpreted as generative models, by drawing a connection to Probabilistic Circuits, a prominent class of tractable probabilistic models. This reinterpretation equips them with a full joint distribution over the feature space and leads to Generative Decision Trees (GeDTs) and Generative Forests (GeFs), a family of novel hybrid generative-discriminative models. This family of models retains the overall characteristics of DTs and RFs while additionally being able to handle missing features by means of marginalisation. Under certain assumptions, frequently made for Bayes consistency results, we show that consistency in GeDTs and GeFs extend to any pattern of missing input features, if missing at random. Empirically, we show that our models often outperform common routines to treat missing data, such as K-nearest neighbour imputation, and moreover, that our models can naturally detect outliers by monitoring the marginal probability of input features.",0
"This paper investigates random forest ensembles that employ joint sampling techniques during their construction process. These methods can reduce overfitting by combining both randomness from bagging and variance reduction due to early stopping. We study three commonly used random forest approaches: individual tree selection (CFP), feature bagging (FB) and subspace splitting (SBS). Extensive experiments on benchmark datasets demonstrate that our proposed method (JRF) outperforms existing joint variants significantly (p<0.05), achieving state-of-the art results in 26 out of 37 tasks across all categories, including regression and classification problems. Our findings show that using joint sampling techniques in conjunction with random forest models is effective at reducing overfitting and improving accuracy.",1
"Decision trees with binary splits are popularly constructed using Classification and Regression Trees (CART) methodology. For regression models, this approach recursively divides the data into two near-homogenous daughter nodes according to a split point that maximizes the reduction in sum of squares error (the impurity) along a particular variable. This paper aims to study the statistical properties of regression trees constructed with CART methodology. In doing so, we find that the training error is governed by the Pearson correlation between the optimal decision stump and response data in each node, which we bound by constructing a prior distribution on the split points and solving a nonlinear optimization problem. We leverage this connection between the training error and Pearson correlation to show that CART with cost-complexity pruning achieves an optimal complexity/goodness-of-fit tradeoff when the depth scales with the logarithm of the sample size. Data dependent quantities, which adapt to the dimensionality and latent structure of the regression model, are seen to govern the rates of convergence of the prediction error.",0
"Title: Learning sparse linear models using Cartesian coordinate descent  Abstract: In recent years, there has been growing interest in developing methods that can effectively perform feature selection while simultaneously fitting a predictive model to data. This task becomes increasingly important as we collect more high-dimensional data sets with many variables that may have little influence on the outcome variable of interest. One popular approach to addressing these challenges is by incorporating structured regularization terms into the objective function, which encourage sparsity (i.e., zero coefficients) in the estimated parameter vector. This leads to improved interpretability and computational efficiency, as well as potentially better out-of-sample performance compared to dense models.  A variety of approaches have been proposed to enforce structured constraints on the parameters of machine learning algorithms, including Lasso regression, Ridge regression, Elastic Net, and Group Lasso. However, these methods can suffer from some limitations, such as slow convergence rates or poor scalability for large datasets. To overcome these drawbacks, we propose a novel method based on Cartesian coordinate descent that allows efficient optimization of a wide range of convex penalties under general conditions on the loss function and penalty term. Our approach extends previous work on coordinate descent for Lasso regression and enjoys several advantages over existing techniques:  * Efficient numerical implementation using standard software libraries (in particular, scikit-learn). We provide open source Python code to reproduce our experiments. * Strong theoretical guarantees on recovery bounds, matching the state-of-the art results known for other computationally intensive methods like Adaptive Lasso or Stagewise Hard Thresholding. These results hold both under the common assumption of Gaussian design matrices and more generally for sub-Gaussian noise. * Faster empirical convergence than competitors, due to the use of gradient information along all dimensions rather tha",1
"The deployment of reinforcement learning (RL) in the real world comes with challenges in calibrating user trust and expectations. As a step toward developing RL systems that are able to communicate their competencies, we present a method of generating human-interpretable abstract behavior models that identify the experiential conditions leading to different task execution strategies and outcomes. Our approach consists of extracting experiential features from state representations, abstracting strategy descriptors from trajectories, and training an interpretable decision tree that identifies the conditions most predictive of different RL behaviors. We demonstrate our method on trajectory data generated from interactions with the environment and on imagined trajectory data that comes from a trained probabilistic world model in a model-based RL setting.",0
"Incorporate some keywords related to RL such as agents, environment, exploration, exploitation, policy gradients. In this paper we investigate the use of real and imagined data to explain conditions for reinforcement learning behaviors in artificial intelligence (AI) systems. We focus on how both types of data can be used to train and evaluate agents that interact with complex environments. Our approach involves using deep neural networks to model agent policies and value functions, which allows us to efficiently explore large state spaces and exploit optimal actions. Keywords: Reinforcement Learning, Agent Systems, Deep Neural Networks",1
"The typical problem in Data Science is creating a structure that encodes the occurrence frequency of unique elements in rows and relations between different rows of a data frame. We present the probability tree abstract data structure, an extension of the decision tree, that facilitates more than two choices with assigned probabilities. Such a tree represents statistical relations between different rows of the data frame. The Probability Tree algorithmic structure is supplied with the Generator module that is a Monte Carlo generator that traverses through the tree. These two components are implemented in TreeGen Python package. The package can be used in increasing data multiplicity, compressing data preserving its statistical information, constructing hierarchical models, exploring data, and in feature extraction.",0
"Title: ""TreeGen: A Monte Carlo Generator for Data Frames"" Absract: This paper presents TreeGen, a new Monte Carlo generative model specifically designed to generate tree structures that closely resemble real datasets. TreeGen builds on recent advances in deep learning models such as GPT-4 by utilizing a latent variable framework that captures complex relationships within the dataset. In particular, our model uses Monte Carlo sampling techniques to explore parameter space efficiently, allowing us to overcome many of the computational limitations faced by previous approaches. We demonstrate the effectiveness of our method using several benchmarking experiments and provide qualitative examples comparing generated trees against their ground truth counterparts. Our results showcase the state-of-the-art performance achieved by TreeGen across multiple domains, making it well suited for applications where generating meaningful synthetic data is critical. Additionally, we release our codebase publicly to promote future research and development in this important area.",1
"This work introduces TrimTuner, the first system for optimizing machine learning jobs in the cloud to exploit sub-sampling techniques to reduce the cost of the optimization process while keeping into account user-specified constraints. TrimTuner jointly optimizes the cloud and application-specific parameters and, unlike state of the art works for cloud optimization, eschews the need to train the model with the full training set every time a new configuration is sampled. Indeed, by leveraging sub-sampling techniques and data-sets that are up to 60x smaller than the original one, we show that TrimTuner can reduce the cost of the optimization process by up to 50x. Further, TrimTuner speeds-up the recommendation process by 65x with respect to state of the art techniques for hyper-parameter optimization that use sub-sampling techniques. The reasons for this improvement are twofold: i) a novel domain specific heuristic that reduces the number of configurations for which the acquisition function has to be evaluated; ii) the adoption of an ensemble of decision trees that enables boosting the speed of the recommendation process by one additional order of magnitude.",0
"""Achieving efficient optimization of machine learning jobs in the cloud can be challenging due to the large computational resources required for training complex models. In order to address this issue, we present TrimTuner: a sub-sampling approach that effectively reduces the amount of data processing while maintaining accuracy. Our method starts by taking random samples from the dataset, which allows us to minimize the impact of outliers and noise. We then train and evaluate several models using these sub-sampled datasets, finding the model that performs well on both training and validation sets. Finally, we apply our chosen model to the full dataset to produce predictions. By utilizing sub-sampling, we achieve significant reductions in computation time without sacrificing model performance.""",1
"Several recent publications report advances in training optimal decision trees (ODT) using mixed-integer programs (MIP), due to algorithmic advances in integer programming and a growing interest in addressing the inherent suboptimality of heuristic approaches such as CART. In this paper, we propose a novel MIP formulation, based on a 1-norm support vector machine model, to train a multivariate ODT for classification problems. We provide cutting plane techniques that tighten the linear relaxation of the MIP formulation, in order to improve run times to reach optimality. Using 36 data-sets from the University of California Irvine Machine Learning Repository, we demonstrate that our formulation outperforms its counterparts in the literature by an average of about 10% in terms of mean out-of-sample testing accuracy across the data-sets. We provide a scalable framework to train multivariate ODT on large data-sets by introducing a novel linear programming (LP) based data selection method to choose a subset of the data for training. Our method is able to routinely handle large data-sets with more than 7,000 sample points and outperform heuristics methods and other MIP based techniques. We present results on data-sets containing up to 245,000 samples. Existing MIP-based methods do not scale well on training data-sets beyond 5,500 samples.",0
"This paper presents a novel method for learning optimal multivariate decision trees. The method uses mixed integer programming (MIP) formulations that allow for exact inference over discrete random variables. In order to solve these MIPs efficiently at scale, we propose several algorithmic improvements that take advantage of special structures present in tree search problems. We demonstrate the effectiveness of our approach on two types of real-world tasks: feature selection from high-dimensional data sets and ranking sports teams based on their performance records. Our results show that the learned decision trees provide better predictions than state-of-the-art methods and capture meaningful patterns in the data. Furthermore, sensitivity analysis shows how sensitive different parts of the model are to changes in input features. Overall, the proposed framework provides a powerful tool for solving combinatorial prediction problems in various application domains.",1
"Predictive clustering trees (PCTs) are a well established generalization of standard decision trees, which can be used to solve a variety of predictive modeling tasks, including structured output prediction. Combining them into ensembles yields state-of-the-art performance. Furthermore, the ensembles of PCTs can be interpreted by calculating feature importance scores from the learned models. However, their learning time scales poorly with the dimensionality of the output space. This is often problematic, especially in (hierarchical) multi-label classification, where the output can consist of hundreds of potential labels. Also, learning of PCTs can not exploit the sparsity of data to improve the computational efficiency, which is common in both input (molecular fingerprints, bag of words representations) and output spaces (in multi-label classification, examples are often labeled with only a fraction of possible labels). In this paper, we propose oblique predictive clustering trees, capable of addressing these limitations. We design and implement two methods for learning oblique splits that contain linear combinations of features in the tests, hence a split corresponds to an arbitrary hyperplane in the input space. The methods are efficient for high dimensional data and capable of exploiting sparse data. We experimentally evaluate the proposed methods on 60 benchmark datasets for 6 predictive modeling tasks. The results of the experiments show that oblique predictive clustering trees achieve performance on-par with state-of-the-art methods and are orders of magnitude faster than standard PCTs. We also show that meaningful feature importance scores can be extracted from the models learned with the proposed methods.",0
"This paper presents a new approach to clustering called oblique predictive clustering trees (OCTs). OCTs are a type of hierarchical cluster model that use regression coefficients as inputs rather than distances between data points. The resulting clusters capture nonlinear relationships between variables and can identify complex patterns in high dimensional datasets. We showcase the performance of OCTs on several simulated datasets and compare their results against existing methods such as k-means clustering and other tree-based approaches like CART. Our findings demonstrate that OCTs outperform these traditional methods in many cases by producing more accurate and interpretable clusters. In addition, we provide guidance on choosing appropriate hyperparameters for OCTs through simulation studies and real-data applications. Overall, our work contributes to the field of unsupervised learning and demonstrates the potential utility of OCTs in identifying meaningful subpopulations within large datasets.",1
"With the current ongoing debate about fairness, explainability and transparency of machine learning models, their application in high-impact clinical decision-making systems must be scrutinized. We consider a real-life example of risk estimation before surgery and investigate the potential for bias or unfairness of a variety of algorithms. Our approach creates transparent documentation of potential bias so that the users can apply the model carefully. We augment a model-card like analysis using propensity scores with a decision-tree based guide for clinicians that would identify predictable shortcomings of the model. In addition to functioning as a guide for users, we propose that it can guide the algorithm development and informatics team to focus on data sources and structures that can address these shortcomings.",0
"Include at least three keywords from the article in the abstract and mention their significance. Also use 2 citation from other sources mentioned in your article. The unfair consequences of using predictive models have raised ethical concerns that must be addressed by healthcare providers before implementing such technologies into clinical practice. Many post-surgery prediction models rely on data from previous hospital visits, which may contain systemic biases against certain groups based on demographic factors like age and race. These biases can result in predictions that underestimate risks for some patients while overestimating them for others, perpetuating existing disparities within the healthcare system. This study aimed to identify fairness issues related to post-operative complications prediction systems in surgical settings. An ethnographic approach was adopted to investigate the contextual factors affecting patient outcomes after surgery. Patient interviews were used to explore social determinants of health as well as health literacy barriers in accessing high-quality care. By understanding these variables, we identified ways to ensure future predictive algorithms account for differences in access to healthcare resources among different racial/ethnic populations. Two external citations support our findings: [1], [2]. Ultimately, ensuring equitable treatment is integral to achieving better health outcomes for all populations in need.  Keywords: Unfairness, post-operative complications, prediction models, ethics, healthcare providers, demographics, bias  Citations: [1] Sarkar U., Gourley D.G., Lyles CR, Tieu l., Katz SJ. Risk adjustment for socioeconomic factors and hospital readmissions: development of a new algorithm. Health Serv Res. 2017;52(4):1491–1508. [2] Brennan TA., Leape LL. What practices should be used to reduce preventable adverse events? Evidence Report/Technology Assessment No. 69.(prepared by the Stanford University School of Medicine, Stanford, California). Rockville, MD: Agency fo",1
"We show that top-down decision tree learning heuristics are amenable to highly efficient learnability estimation: for monotone target functions, the error of the decision tree hypothesis constructed by these heuristics can be estimated with polylogarithmically many labeled examples, exponentially smaller than the number necessary to run these heuristics, and indeed, exponentially smaller than information-theoretic minimum required to learn a good decision tree. This adds to a small but growing list of fundamental learning algorithms that have been shown to be amenable to learnability estimation.   En route to this result, we design and analyze sample-efficient minibatch versions of top-down decision tree learning heuristics and show that they achieve the same provable guarantees as the full-batch versions. We further give ""active local"" versions of these heuristics: given a test point $x^\star$, we show how the label $T(x^\star)$ of the decision tree hypothesis $T$ can be computed with polylogarithmically many labeled examples, exponentially smaller than the number necessary to learn $T$.",0
"Decision trees have been widely used as a powerful machine learning algorithm due to their interpretability and effectiveness on many tasks. However, designing effective decision rules often requires a large amount of training data which can lead to high computational cost and limited scalability. In this work, we introduce a novel approach that estimates the learnability of decision trees using a polynomial function of the number of samples in log form (polylogarithmic). We show through extensive experiments that our proposed method significantly reduces both time and sample complexity while still achieving comparable performance compared to traditional decision tree algorithms. Our findings suggest that this new paradigm has great potential for improving the efficiency and applicability of decision trees in a variety of domains.",1
"Machine learning models, such as neural networks, decision trees, random forests, and gradient boosting machines, accept a feature vector, and provide a prediction. These models learn in a supervised fashion where we provide feature vectors mapped to the expected output. It is common practice to engineer new features from the provided feature set. Such engineered features will either augment or replace portions of the existing feature vector. These engineered features are essentially calculated fields based on the values of the other features.   Engineering such features is primarily a manual, time-consuming task. Additionally, each type of model will respond differently to different kinds of engineered features. This paper reports empirical research to demonstrate what kinds of engineered features are best suited to various machine learning model types. We provide this recommendation by generating several datasets that we designed to benefit from a particular type of engineered feature. The experiment demonstrates to what degree the machine learning model can synthesize the needed feature on its own. If a model can synthesize a planned feature, it is not necessary to provide that feature. The research demonstrated that the studied models do indeed perform differently with various types of engineered features.",0
"In recent years, predictive modeling has emerged as an important tool in many fields such as finance, marketing, healthcare, etc. However, building accurate models can be challenging due to issues related to data quality, noise, missing values, and high dimensionality. This study aims to investigate the impact of feature engineering on improving the accuracy of predictive models through an empirical analysis. We first conducted a systematic literature review to identify popular techniques used in feature engineering, their benefits and limitations. Subsequently, we performed experiments using real world datasets from different domains to evaluate how feature engineering can influence key metrics that measure the performance of predictive models like precision, recall and F1 score. Our results demonstrate that carefully designed feature engineering methods can significantly improve predictive model performance by reducing overfitting and underfitting while handling complex relationships among features. Moreover, our findings contribute new insights into the design choices available to researchers developing predictive models to solve problems in their respective domains.",1
"There is a rich and growing literature on producing local contrastive/counterfactual explanations for black-box models (e.g. neural networks).   In these methods, for an input, an explanation is in the form of a contrast point differing in very few features from the original input and lying in a different class. Other works try to build globally interpretable models like decision trees and rule lists based on the data using actual labels or based on the black-box models predictions. Although these interpretable global models can be useful, they may not be consistent with local explanations from a specific black-box of choice. In this work, we explore the question: Can we produce a transparent global model that is simultaneously accurate and consistent with the local (contrastive) explanations of the black-box model? We introduce a natural local consistency metric that quantifies if the local explanations and predictions of the black-box model are also consistent with the proxy global transparent model. Based on a key insight we propose a novel method where we create custom boolean features from sparse local contrastive explanations of the black-box model and then train a globally transparent model on just these, and showcase empirically that such models have higher local consistency compared with other known strategies, while still being close in performance to models that are trained with access to the original data.",0
"This paper presents a new method for learning global transparent models consistent with local contrastive explanations (LTMs). The goal of LTMs is to learn a model that can generate high quality explanations for arbitrary input points using a local approach, while still being able to accurately predict outputs globally on unseen data. To achieve this, we propose a two stage process: first, training the model locally using contrastive explanation techniques; second, aligning these local explanations to create a global representation of the data. We demonstrate through experiments on several real world datasets that our proposed method outperforms state-of-the-art baseline methods in terms of accuracy and interpretability. In conclusion, our work represents an important step towards creating more interpretable machine learning models by incorporating local explanations into their design.",1
"Turing machine and decision tree have developed independently for a long time. With the recent development of differentiable models, there is an intersection between them. Neural turing machine(NTM) opens door for the memory network. It use differentiable attention mechanism to read/write external memory bank. Differentiable forest brings differentiable properties to classical decision tree. In this short note, we show the deep connection between these two models. That is: differentiable forest is a special case of NTM. Differentiable forest is actually decision tree based neural turing machine. Based on this deep connection, we propose a response augmented differential forest (RaDF). The controller of RaDF is differentiable forest, the external memory of RaDF are response vectors which would be read/write by leaf nodes.",0
"This paper presents a new approach to natural language processing called the Neural Turing Machine (NTM). The NTM combines deep learning techniques with explicit memory structures inspired by cognitive science. In particular, the authors propose a variant of the NTM that uses decision trees as internal representations, which they call Decision Tree Based Neural Turing Machines (DTB-NTMs). Experiments show that DTB-NTMs achieve state-of-the-art performance across several benchmark datasets in tasks such as text classification, question answering and machine translation.",1
"Learning algorithms produce software models for realising critical classification tasks. Decision trees models are simpler than other models such as neural network and they are used in various critical domains such as the medical and the aeronautics. Low or unknown learning ability algorithms does not permit us to trust the produced software models, which lead to costly test activities for validating the models and to the waste of learning time in case the models are likely to be faulty due to the learning inability. Methods for evaluating the decision trees learning ability, as well as that for the other models, are needed especially since the testing of the learned models is still a hot topic. We propose a novel oracle-centered approach to evaluate (the learning ability of) learning algorithms for decision trees. It consists of generating data from reference trees playing the role of oracles, producing learned trees with existing learning algorithms, and determining the degree of correctness (DOE) of the learned trees by comparing them with the oracles. The average DOE is used to estimate the quality of the learning algorithm. the We assess five decision tree learning algorithms based on the proposed approach.",0
"""Evaluating learning algorithms used in decision trees can be challenging due to their inherent complexity and numerous configuration options. However, accurate evaluation is crucial for understanding how these algorithms perform and which one is most suitable for specific applications. In this study, we propose a novel approach to evaluate two popular decision tree construction methods: Random Forest (RF) and Gradient Boosted Trees (GBT). We focus on evaluating each algorithm using multiple metrics over several datasets from different domains to obtain a comprehensive analysis. Our results show that RF consistently outperforms GBT across all metrics, with significant differences observed for accuracy, precision, recall, F1 score, Gini impurity index, and feature selection criteria. However, our analysis indicates that there may still exist cases where GBT could outperform RF based on dataset characteristics and model performance goals. Overall, this work contributes new insights into the relative strengths and weaknesses of these competing algorithms, providing valuable guidance for practitioners choosing among them.""",1
"Decision trees are flexible models that are well suited for many statistical regression problems. In a Bayesian framework for regression trees, Markov Chain Monte Carlo (MCMC) search algorithms are required to generate samples of tree models according to their posterior probabilities. The critical component of such an MCMC algorithm is to construct good Metropolis-Hastings steps for updating the tree topology. However, such algorithms frequently suffering from local mode stickiness and poor mixing. As a result, the algorithms are slow to converge. Hitherto, authors have primarily used discrete-time birth/death mechanisms for Bayesian (sums of) regression tree models to explore the model space. These algorithms are efficient only if the acceptance rate is high which is not always the case. Here we overcome this issue by developing a new search algorithm which is based on a continuous-time birth-death Markov process. This search algorithm explores the model space by jumping between parameter spaces corresponding to different tree structures. In the proposed algorithm, the moves between models are always accepted which can dramatically improve the convergence and mixing properties of the MCMC algorithm. We provide theoretical support of the algorithm for Bayesian regression tree models and demonstrate its performance.",0
"This article presents a new method for Markov Chain Monte Carlo (MCMC) sampling using continuous time birth-death processes for estimating Bayesian regression tree models. We introduce a novel algorithm that allows for efficient simulation from these processes by adapting ideas from Euler discretization methods commonly used in numerical analysis. Our approach utilizes a logarithmic scale to ensure validity over a wide range of parameter values, while also providing fast convergence rates. Additionally, we investigate important properties such as ergodicity and computational efficiency through extensive simulations. Finally, we apply our model to real data sets and demonstrate its utility for nonparametric regression problems. Overall, our contributions provide researchers with a flexible toolset for exploring complex relationships in large datasets where traditional approaches may struggle to produce reliable results.",1
"We study the problem of efficient adversarial attacks on tree based ensembles such as gradient boosting decision trees (GBDTs) and random forests (RFs). Since these models are non-continuous step functions and gradient does not exist, most existing efficient adversarial attacks are not applicable. Although decision-based black-box attacks can be applied, they cannot utilize the special structure of trees. In our work, we transform the attack problem into a discrete search problem specially designed for tree ensembles, where the goal is to find a valid ""leaf tuple"" that leads to mis-classification while having the shortest distance to the original input. With this formulation, we show that a simple yet effective greedy algorithm can be applied to iteratively optimize the adversarial example by moving the leaf tuple to its neighborhood within hamming distance 1. Experimental results on several large GBDT and RF models with up to hundreds of trees demonstrate that our method can be thousands of times faster than the previous mixed-integer linear programming (MILP) based approach, while also providing smaller (better) adversarial examples than decision-based black-box attacks on general $\ell_p$ ($p=1, 2, \infty$) norm perturbations. Our code is available at https://github.com/chong-z/tree-ensemble-attack.",0
"Title: Adversarial Attacks on Decision Trees and Random Forests ABSTRACT In recent years, there has been growing interest in understanding adversarial attacks against machine learning models. These types of attacks aim to alter inputs in small ways that cause large changes in model outputs, making them more difficult to use reliably in practice. Many studies have focused specifically on attacking deep neural networks (DNNs), but relatively few papers consider other popular algorithms like decision trees (DT) and random forests (RF). We propose a novel approach based on gradient ascent using the Fast Gradient Sign Method (FGSM) to generate adversarial examples targeted at these tree ensemble methods. Our method effectively finds input perturbations that lead to incorrect predictions by maximizing the margin loss of individual decision trees within the RF. We evaluate our attack across several datasets and find strong performance compared to state-of-the-art approaches, indicating the importance of considering robustness of tree ensembles to such attacks. Additionally, we demonstrate that our technique can significantly degrade the accuracy of these models while preserving semantic meaning, highlighting potential vulnerabilities in real-world applications.",1
"A decision tree is commonly restricted to use a single hyperplane to split the covariate space at each of its internal nodes. It often requires a large number of nodes to achieve high accuracy, hurting its interpretability. In this paper, we propose convex polytope trees (CPT) to expand the family of decision trees by an interpretable generalization of their decision boundary. The splitting function at each node of CPT is based on the logical disjunction of a community of differently weighted probabilistic linear decision-makers, which also geometrically corresponds to a convex polytope in the covariate space. We use a nonparametric Bayesian prior at each node to infer the community's size, encouraging simpler decision boundaries by shrinking the number of polytope facets. We develop a greedy method to efficiently construct CPT and scalable end-to-end training algorithms for the tree parameters when the tree structure is given. We empirically demonstrate the efficiency of CPT over existing state-of-the-art decision trees in several real-world classification and regression tasks from diverse domains.",0
"Abstract: This study investigates convex polytopal trees, which are geometric objects that can model protein structures and other molecular systems. We define these trees as hierarchical arrangements of convex polytopal elements that form closed shells at each level. We then develop algorithms for generating such trees and demonstrate their effectiveness on several benchmark examples. Our results show that these algorithms provide efficient solutions to relevant problems and open up new possibilities in applications ranging from materials science to drug discovery. In conclusion, we believe our work provides important insights into understanding the fundamental properties of these fascinating objects and expands the frontiers of computational geometry.",1
"Decision trees (DTs) epitomize what have become to be known as interpretable machine learning (ML) models. This is informally motivated by paths in DTs being often much smaller than the total number of features. This paper shows that in some settings DTs can hardly be deemed interpretable, with paths in a DT being arbitrarily larger than a PI-explanation, i.e. a subset-minimal set of feature values that entails the prediction. As a result, the paper proposes a novel model for computing PI-explanations of DTs, which enables computing one PI-explanation in polynomial time. Moreover, it is shown that enumeration of PI-explanations can be reduced to the enumeration of minimal hitting sets. Experimental results were obtained on a wide range of publicly available datasets with well-known DT-learning tools, and confirm that in most cases DTs have paths that are proper supersets of PI-explanations.",0
"This paper presents a method for explaining decision trees, which can often seem complex and opaque to end users. We show how to generate explanations that clearly highlight the most important features used by the model, as well as any interactions among those features. To achieve this goal, we employ a novel technique called SHAP (SHapley Additive exPlanations) values, which measures the marginal contribution of each feature to the prediction. Our approach also includes interactive visualizations that allow end users to explore different aspects of the explanation. Experimental results demonstrate the effectiveness of our approach, showing that users are able to more accurately interpret the predictions made by the decision tree after viewing the explanations generated by our system. Overall, our work provides a valuable tool for improving transparency in machine learning models and increasing user trust in automated decision making systems.",1
"Machine Learning (ML) adoption in the enterprise requires simpler and more efficient software infrastructure---the bespoke solutions typical in large web companies are simply untenable. Model scoring, the process of obtaining predictions from a trained model over new data, is a primary contributor to infrastructure complexity and cost as models are trained once but used many times. In this paper we propose HUMMINGBIRD, a novel approach to model scoring, which compiles featurization operators and traditional ML models (e.g., decision trees) into a small set of tensor operations. This approach inherently reduces infrastructure complexity and directly leverages existing investments in Neural Network compilers and runtimes to generate efficient computations for both CPU and hardware accelerators. Our performance results are intriguing: despite replacing imperative computations (e.g., tree traversals) with tensor computation abstractions, HUMMINGBIRD is competitive and often outperforms hand-crafted kernels on micro-benchmarks on both CPU and GPU, while enabling seamless end-to-end acceleration of ML pipelines. We have released HUMMINGBIRD as open source.",0
"Artificial intelligence (AI) has made significant advances over recent years in terms of automation and prediction, and researchers have focused on developing tensor compilers that can compile models into optimized code for efficient deployment. To ensure high performance across different hardware architectures, new compilation methods that optimize deep learning workloads using dynamic scheduling are necessary. This study presents a unified compiler framework called ""T2S"" that performs model parallelism, pipeline execution, loop fusion/fission, operator fusion/defusion, and register blocking automatically. Experimental results demonstrate significant speedups in serving latency compared with existing state-of-the-art tools and frameworks. This approach simplifies model building, training, and serving while providing consistent gains in efficiency and scalability. Overall, our findings suggest that T2S represents a promising direction for enabling high-performance machine learning inference in diverse application domains. Title: A Comprehensive Approach to Improve Deep Learning Inference Efficiency  Abstract: As artificial intelligence continues to advance at an accelerating pace, there remains a critical need for improved approaches to automate and speed up machine learning predictions. One essential aspect of achieving these goals lies in optimizing the deployment process through advanced tensor compilers capable of leveraging dynamic scheduling techniques for efficient deployment across various hardware platforms. In this study, we present a novel framework named ""T2S,"" designed to improve the performance of modern deep learning applications by incorporating multiple optimization strategies such as model parallelism, pipelining, operator fusing, and others. Our experimental evaluation demonstrates notable improvements in serving latencies compared to prevailing industry standards. These gains contribute toward streamlining model development processes by integrating crucial acceleration components seamlessly within the same platform. Ultimately, our research opens exciting possibilities for exploring further advancements i",1
"Technology advancements made it easy to measure non-invasive and high-quality electroencephalograph (EEG) signals from human's brain. Hence, development of robust and high-performance AI algorithms becomes crucial to properly process the EEG signals and recognize the patterns, which lead to an appropriate control signal. Despite the advancements in processing the motor imagery EEG signals, the healthcare applications, such as emotion detection, are still in the early stages of AI design. In this paper, we propose a modular framework for the recognition of vowels as the AI part of a brain computer interface system. We carefully designed the modules to discriminate the English vowels given the raw EEG signals, and meanwhile avoid the typical issued with the data-poor environments like most of the healthcare applications. The proposed framework consists of appropriate signal segmentation, filtering, extraction of spectral features, reducing the dimensions by means of principle component analysis, and finally a multi-class classification by decision-tree-based support vector machine (DT-SVM). The performance of our framework was evaluated by a combination of test-set and resubstitution (also known as apparent) error rates. We provide the algorithms of the proposed framework to make it easy for future researchers and developers who want to follow the same workflow.",0
"Title: Learning Patterns in Imaginary Vowels for an Intelligent Brain Computer Interface (BCI) Design Abstract: This study aims to explore the feasibility of using imaginary vowels as a control input method for brain computer interfaces (BCIs). BCI technology allows individuals to interact with computers through brain signals alone, offering potential applications for those with mobility impairments. However, current BCI methods often rely on complex training protocols that can be difficult for users to master and may limit their effectiveness. In contrast, imaginary vocalizations have been shown to produce distinct patterns of neural activity in the motor cortex that could potentially serve as a control signal for BCIs. Our research investigates whether these patterns can be learned by participants without prior singing experience, allowing them to modulate specific vowel sounds solely through thought. Participants underwent neuroimaging sessions during which they were trained to imagine producing different vowel sounds. Results indicate that imagined vocalization produces unique activation profiles across several regions involved in speech production, including areas typically recruited during overt speech tasks. These findings suggest that imaginary vowels hold promise as a noninvasive yet robust control mechanism for future BCI designs, offering increased accessibility and versatility compared to conventional techniques. Future work should aim to further develop and refine this approach for use in real-world settings. Keywords: brain computer interface, imaginary vocalizations, speech production, neuroimaging",1
"We propose a simple extension of top-down decision tree learning heuristics such as ID3, C4.5, and CART. Our algorithm achieves provable guarantees for all target functions $f: \{-1,1\}^n \to \{-1,1\}$ with respect to the uniform distribution, circumventing impossibility results showing that existing heuristics fare poorly even for simple target functions. The crux of our extension is a new splitting criterion that takes into account the correlations between $f$ and small subsets of its attributes. The splitting criteria of existing heuristics (e.g. Gini impurity and information gain), in contrast, are based solely on the correlations between $f$ and its individual attributes.   Our algorithm satisfies the following guarantee: for all target functions $f : \{-1,1\}^n \to \{-1,1\}$, sizes $s\in \mathbb{N}$, and error parameters $\epsilon$, it constructs a decision tree of size $s^{\tilde{O}((\log s)^2/\epsilon^2)}$ that achieves error $\le O(\mathsf{opt}_s) + \epsilon$, where $\mathsf{opt}_s$ denotes the error of the optimal size $s$ decision tree. A key technical notion that drives our analysis is the noise stability of $f$, a well-studied smoothness measure.",0
"Decision trees have been widely used as powerful tools for modeling complex data structures and making predictions. However, their accuracy can often suffer from overfitting, which occurs when a decision tree becomes excessively complex and starts fitting noise rather than underlying patterns in the data. In order to address this issue, researchers have developed methods that aim at finding more interpretable models by pruning decision trees during training. One such method is the use of a higher-order splitting criterion (HOSC), which evaluates the quality of splits based on both individual examples and groups of similar examples. This HOSC approach has shown promising results in reducing overfitting and improving interpretability in decision trees, but its universality across different datasets remains unclear.  In this study, we investigate whether the use of a HOSC can lead to universal guarantees for decision tree induction. We conduct extensive experiments using multiple publicly available datasets across diverse domains, including image classification, text classification, regression, and other tasks. Our findings demonstrate that using a HOSC consistently leads to significant improvements in prediction accuracy compared to traditional splitting criteria that only consider individual examples. Moreover, our proposed framework enables us to provide theoretical bounds on the minimum number of samples required to achieve desired levels of accuracy, regardless of the dataset size or complexity. These results suggest that the HOSC approach provides a general solution to mitigating overfitting in decision trees, making them applicable to a wide range of applications. Additionally, we present insights into how the choice of parameters affects the performance of HOSC, providing guidance for practitioners. Overall, our work contributes to the development of robust machine learning algorithms capable of achieving high accuracy while retaining interpretability.",1
"Hoeffding trees are the state-of-the-art methods in decision tree learning for evolving data streams. These very fast decision trees are used in many real applications where data is created in real-time due to their efficiency. In this work, we extricate explanations for why these streaming decision tree algorithms for stationary and nonstationary streams (HoeffdingTree and HoeffdingAdaptiveTree) work as well as they do. In doing so, we identify thirteen unique unspecified design decisions in both the theoretical constructs and their implementations with substantial and consequential effects on predictive accuracy---design decisions that, without necessarily changing the essence of the algorithms, drive algorithm performance. We begin a larger conversation about explainability not just of the model but also of the processes responsible for an algorithm's success.",0
"This research presents a novel approach to decision tree algorithms that allows for emergent and unspecified behaviors in streaming data analysis. By using a distributed architecture based on graph theory principles, the proposed method enables efficient processing of high volume and velocity data streams while supporting complex decision making processes. Our evaluation shows significant improvements over traditional centralized approaches, including reduced latency and increased adaptability to evolving patterns in the data stream. Overall, our work has important implications for real-time monitoring and predictive analytics applications across various domains.",1
"Deep learning models are favored in many research and industry areas and have reached the accuracy of approximating or even surpassing human level. However they've long been considered by researchers as black-box models for their complicated nonlinear property. In this paper, we propose a multi-level decision framework to provide comprehensive interpretation for the deep neural network model.   In this multi-level decision framework, by fitting decision trees for each neuron and aggregate them together, a multi-level decision structure (MLD) is constructed at first, which can approximate the performance of the target neural network model with high efficiency and high fidelity. In terms of local explanation for sample, two algorithms are proposed based on MLD structure: forward decision generation algorithm for providing sample decisions, and backward rule induction algorithm for extracting sample rule-mapping recursively. For global explanation, frequency-based and out-of-bag based methods are proposed to extract important features in the neural network decision. Furthermore, experiments on the MNIST and National Free Pre-Pregnancy Check-up (NFPC) dataset are carried out to demonstrate the effectiveness and interpretability of MLD framework. In the evaluation process, both functionally-grounded and human-grounded methods are used to ensure credibility.",0
"In recent years, deep learning has emerged as a powerful technique for solving complex problems across a wide range of domains. However, despite their successes, these models can often suffer from poor interpretability, making it difficult to understand how they arrive at their decisions. To address this challenge, we propose a novel method based on rule extraction that allows us to explain the behavior of deep learning models in terms of human-readable rules. Our approach consists of training a set of decision trees that mimic the predictions of the original model, using techniques such as randomization and ablation to identify which input features contribute most significantly to each prediction. We demonstrate the effectiveness of our approach through extensive experiments on real-world datasets, showing that it yields interpretable explanations that capture key insights into the workings of deep learning systems. Overall, our method provides a valuable tool for practitioners who need to gain insight into black box predictors, while offering new research directions towards building more transparent machine learning systems.",1
"While artificial intelligence (AI)-based decision-making systems are increasingly popular, significant concerns on the potential discrimination during the AI decision-making process have been observed. For example, the distribution of predictions is usually biased and dependents on the sensitive attributes (e.g., gender and ethnicity). Numerous approaches have therefore been proposed to develop decision-making systems that are discrimination-conscious by-design, which are typically batch-based and require the simultaneous availability of all the training data for model learning. However, in the real-world, the data streams usually come on the fly which requires the model to process each input data once ""on arrival"" and without the need for storage and reprocessing. In addition, the data streams might also evolve over time, which further requires the model to be able to simultaneously adapt to non-stationary data distributions and time-evolving bias patterns, with an effective and robust trade-off between accuracy and fairness. In this paper, we propose a novel framework of online decision tree with fairness in the data stream with possible distribution drifting. Specifically, first, we propose two novel fairness splitting criteria that encode the data as well as possible, while simultaneously removing dependence on the sensitive attributes, and further adapts to non-stationary distribution with fine-grained control when needed. Second, we propose two fairness decision tree online growth algorithms that fulfills different online fair decision-making requirements. Our experiments show that our algorithms are able to deal with discrimination in massive and non-stationary streaming environments, with a better trade-off between fairness and predictive performance.",0
"Here is my attempt: Paper Abstract: The study proposes novel techniques for training decision trees on imbalanced data sets while ensuring fairness towards certain subpopulations. We present two methods that improve upon existing state-of-the art approaches by accounting for unequal distribution of features within each class and reducing sensitivity to the selection of tuning parameters such as sampling rate and cost parameter. Our experiments demonstrate that these techniques effectively mitigate unfair bias against protected groups across multiple real-world datasets from different domains, including credit risk assessment, medical diagnosis, and criminal sentencing. Overall, our findings highlight the importance of developing more inclusive machine learning models, which can promote social welfare and reduce discrimination in automated decision making systems.",1
"We introduce a new notion of generalization -- Distributional Generalization -- which roughly states that outputs of a classifier at train and test time are close *as distributions*, as opposed to close in just their average error. For example, if we mislabel 30% of dogs as cats in the train set of CIFAR-10, then a ResNet trained to interpolation will in fact mislabel roughly 30% of dogs as cats on the *test set* as well, while leaving other classes unaffected. This behavior is not captured by classical generalization, which would only consider the average error and not the distribution of errors over the input domain. Our formal conjectures, which are much more general than this example, characterize the form of distributional generalization that can be expected in terms of problem parameters: model architecture, training procedure, number of samples, and data distribution. We give empirical evidence for these conjectures across a variety of domains in machine learning, including neural networks, kernel machines, and decision trees. Our results thus advance our empirical understanding of interpolating classifiers.",0
"This is how the AI would write an Abstract for the paper. --------------------------- Abstract In order for humans to function effectively within their environment, they must be able to generalize from learned concepts to new situations that have never been explicitly encountered before. Recent research has shown that humans rely on both featural representations and distributional semantics (DS) when making these kinds of judgements, but there is disagreement over which approach plays the most important role. To better understand human conceptual learning, we conducted four experiments using a novel paradigm that allows us to investigate whether features or DS dominates across different types of material while controlling for complexity and structure in each type. Our results show that, although both feature-based and semantic generalizations play significant roles in human concepts, the type of generalization that results depends on two other factors: structural similarity among members of a category and categorical exclusivity. These findings offer insight into how our brains form and use categories and provide evidence that category formation relies heavily on more than one aspect of knowledge representation, supporting recent suggestions made by cognitive scientists and philosophers alike. We discuss implications for cognition, category formation and future directions in our discussion section. Overall, our study provides compelling support for the importance of multiple sources of information in forming new categories and understanding why certain concepts behave differently from others in terms of distributional generalization. Keywords Concepts; Categorisation; Distributed Semantic Representation; Feature Based Learning.",1
"A method for the local and global interpretation of a black-box model on the basis of the well-known generalized additive models is proposed. It can be viewed as an extension or a modification of the algorithm using the neural additive model. The method is based on using an ensemble of gradient boosting machines (GBMs) such that each GBM is learned on a single feature and produces a shape function of the feature. The ensemble is composed as a weighted sum of separate GBMs resulting a weighted sum of shape functions which form the generalized additive model. GBMs are built in parallel using randomized decision trees of depth 1, which provide a very simple architecture. Weights of GBMs as well as features are computed in each iteration of boosting by using the Lasso method and then updated by means of a specific smoothing procedure. In contrast to the neural additive model, the method provides weights of features in the explicit form, and it is simply trained. A lot of numerical experiments with an algorithm implementing the proposed method on synthetic and real datasets demonstrate its efficiency and properties for local and global interpretation.",0
"This could be used as the final summary paragraph:  Scientists have made considerable progress on understanding how human brains process language, including developing neural network models that mimic brain functions. However, until now there has been little attempt to create interpretable machine learning systems inspired by human cognition. In this work we investigate whether ensembles of gradient boosting machines can function as interpretable surrogates for complex neural networks at inference time. We show that ensembles of gradient boosting machines achieve results comparable to state-of-the-art neural models while offering several advantages over black box approaches. Our findings suggest that ensembles of gradient boosting machines offer a promising direction towards creating interpretable machine learning systems capable of high performance natural language processing tasks.",1
"Decision trees are popular machine learning models that are simple to build and easy to interpret. Even though algorithms to learn decision trees date back to almost 50 years, key properties affecting their generalization error are still weakly bounded. Hence, we revisit binary decision trees on real-valued features from the perspective of partitions of the data. We introduce the notion of partitioning function, and we relate it to the growth function and to the VC dimension. Using this new concept, we are able to find the exact VC dimension of decision stumps, which is given by the largest integer $d$ such that $2\ell \ge \binom{d}{\left\lfloor\frac{d}{2}\right\rfloor}$, where $\ell$ is the number of real-valued features. We provide a recursive expression to bound the partitioning functions, resulting in a upper bound on the growth function of any decision tree structure. This allows us to show that the VC dimension of a binary tree structure with $N$ internal nodes is of order $N \log(N\ell)$. Finally, we elaborate a pruning algorithm based on these results that performs better than the CART algorithm on a number of datasets, with the advantage that no cross-validation is required.",0
"In this study, we explore the use of decision trees as partitioning machines to investigate their generalization properties. We begin by examining the classical hypothesis testing framework that has been used to analyze decision tree algorithms. However, we find that this framework fails to capture some important aspects of these algorithms, particularly those related to overfitting and underfitting. Therefore, we propose a new approach based on decision theory that addresses these limitations. Our method uses decision trees as partitions of data, which allows us to quantify the amount of uncertainty present in each leaf node. By analyzing these uncertainties, we can characterize the degree of generalization achieved by different decision tree models. Our experiments demonstrate that our proposed method leads to more accurate predictions than previous approaches, even when applied to high-dimensional datasets. This work highlights the importance of considering decision trees as partitioning machines rather than just prediction tools, and provides insights into how they make decisions. Overall, this research contributes to the understanding of decision trees from a statistical perspective and could lead to improved algorithms in the future.",1
"Learning from data streams is among the most vital fields of contemporary data mining. The online analysis of information coming from those potentially unbounded data sources allows for designing reactive up-to-date models capable of adjusting themselves to continuous flows of data. While a plethora of shallow methods have been proposed for simpler low-dimensional streaming problems, almost none of them addressed the issue of learning from complex contextual data, such as images or texts. The former is represented mainly by adaptive decision trees that have been proven to be very efficient in streaming scenarios. The latter has been predominantly addressed by offline deep learning. In this work, we attempt to bridge the gap between these two worlds and propose Adaptive Deep Forest (ADF) - a natural combination of the successful tree-based streaming classifiers with deep forest, which represents an interesting alternative idea for learning from contextual data. The conducted experiments show that the deep forest approach can be effectively transformed into an online algorithm, forming a model that outperforms all state-of-the-art shallow adaptive classifiers, especially for high-dimensional complex streams.",0
"In the age of big data, it has become increasingly important to develop efficient methods for learning from streaming data. As new data becomes available over time, traditional machine learning models can drift away from their original objectives. To address this issue, we propose the use of adaptive deep forest (ADF) modeling techniques to learn from online streams of dynamic data. Our approach combines the strengths of both randomized decision trees and neural networks by leveraging the power of gradient boosting on randomly generated feature subsets. By using mini-batch gradient descent updates, our algorithm adapts incrementally to changes in the underlying distribution, while maintaining low computational complexity. Empirical evaluation demonstrates significant improvements across various benchmark datasets compared against state-of-the-art streaming learning algorithms.",1
"Classic decision tree learning is a binary classification algorithm that constructs models with first-class transparency - every classification has a directly derivable explanation. However, learning decision trees on modern datasets generates large trees, which in turn generate decision paths of excessive depth, obscuring the explanation of classifications. To improve the comprehensibility of classifications, we propose a new decision tree model that we call Cascading Decision Trees. Cascading Decision Trees shorten the size of explanations of classifications, without sacrificing model performance overall. Our key insight is to separate the notion of a decision path and an explanation path. Utilizing this insight, instead of having one monolithic decision tree, we build several smaller decision subtrees and cascade them in sequence. Our cascading decision subtrees are designed to specifically target explanations for positive classifications. This way each subtree identifies the smallest set of features that can classify as many positive samples as possible, without misclassifying any negative samples. Applying cascading decision trees to new samples results in a significantly shorter and succinct explanation, if one of the subtrees detects a positive classification. In that case, we immediately stop and report the decision path of only the current subtree to the user as an explanation for the classification. We evaluate our algorithm on standard datasets, as well as new real-world applications and find that our model shortens the explanation depth by over 40.8% for positive classifications compared to the classic decision tree model.",0
"Title: ""Succinct Explanations with Cascading Decision Trees""  Abstract: In recent years, there has been significant interest in developing interpretable machine learning models that can generate succinct explanations for their predictions. One popular approach in this direction is the use of decision trees, which provide easy-to-interpret decision rules for predicting outcomes. However, traditional decision tree algorithms suffer from several limitations, such as overfitting, high computational complexity, and poor scalability to large datasets. To address these challenges, we propose a new algorithm called cascading decision trees (CDTs), which combines the benefits of decision trees with efficient parallel processing techniques and regularization methods to improve model performance and interpretability. We demonstrate the effectiveness of CDTs on several benchmark datasets, showing that our method consistently yields higher accuracy than state-of-the-art decision tree and random forest approaches while providing transparent, human-readable explanations for complex prediction tasks. Our results suggest that CDTs have strong potential for applications in a wide range of domains where interpretability and efficiency are critical requirements.",1
"We study rare-event simulation for a class of problems where the target hitting sets of interest are defined via modern machine learning tools such as neural networks and random forests. This problem is motivated from fast emerging studies on the safety evaluation of intelligent systems, robustness quantification of learning models, and other potential applications to large-scale simulation in which machine learning tools can be used to approximate complex rare-event set boundaries. We investigate an importance sampling scheme that integrates the dominating point machinery in large deviations and sequential mixed integer programming to locate the underlying dominating points. Our approach works for a range of neural network architectures including fully connected layers, rectified linear units, normalization, pooling and convolutional layers, and random forests built from standard decision trees. We provide efficiency guarantees and numerical demonstration of our approach using a classification model in the UCI Machine Learning Repository.",0
"This is an abstract that describes the approach taken by researchers who used rare-event simulation techniques to evaluate predictive models built using neural networks (NNs) and random forest algorithms (RFAs). These methods were evaluated on datasets containing both categorical and continuous variables, and their performance was compared against each other as well as traditional approaches. Key findings showed that NNs outperformed RFAs across most metrics, but there was some evidence suggesting that different data preprocessing steps can improve RFA results significantly. Ultimately, these simulations provide valuable insights into how machine learning algorithms can be optimized for use in risk modeling applications. Abstract: This study aimed to assess the effectiveness of rare-event simulation techniques for evaluating predictive models generated from neural network (NN) and random forest algorithm (RFA) training sets. Data with mixed variable types (continuous and categorical) were employed, allowing comparison between the two methods for diverse cases. The authors observed that while NN models generally performed better than those produced by RFAs across multiple evaluation criteria, specific data preparation adjustments could noticeably enhance RFA accuracy. Findings suggest that utilizing unique machine learning optimizations within risk prediction systems is advisable for effective decision making processes. The presented work provides novel perspectives regarding improvement possibilities for artificial intelligence forecasting tools under uncertain conditions, expanding upon existing literature.",1
"The United States is experiencing an opioid epidemic, and there were more than 10 million opioid misusers aged 12 or older each year. Identifying patients at high risk of Opioid Use Disorder (OUD) can help to make early clinical interventions to reduce the risk of OUD. Our goal is to predict OUD patients among opioid prescription users through analyzing electronic health records with machine learning and deep learning methods. This will help us to better understand the diagnoses of OUD, providing new insights on opioid epidemic. Electronic health records of patients who have been prescribed with medications containing active opioid ingredients were extracted from Cerner Health Facts database between January 1, 2008 and December 31, 2017. Long Short-Term Memory (LSTM) models were applied to predict opioid use disorder risk in the future based on recent five encounters, and compared to Logistic Regression, Random Forest, Decision Tree and Dense Neural Network. Prediction performance was assessed using F-1 score, precision, recall, and AUROC. Our temporal deep learning model provided promising prediction results which outperformed other methods, with a F1 score of 0.8023 and AUCROC of 0.9369. The model can identify OUD related medications and vital signs as important features for the prediction. LSTM based temporal deep learning model is effective on predicting opioid use disorder using a patient past history of electronic health records, with minimal domain knowledge. It has potential to improve clinical decision support for early intervention and prevention to combat the opioid epidemic.",0
"This paper presents a deep learning approach for identifying patients at high risk of opioid use disorder (OUD) who are taking opioid medications. OUD is a serious and growing public health issue that can have devastating consequences for individuals and their families. While opioids can be effective for managing pain, they carry a significant risk of abuse and dependence. Therefore, there is a need for accurate and timely identification of patients who may develop OUD so that appropriate interventions can be put into place.  The proposed method leverages electronic health record data from large patient populations to identify patterns and characteristics associated with increased risk of developing OUD. Specifically, we train a deep learning model on features extracted from patient records such as demographic information, medical history, prescription drug history, laboratory results, diagnoses, and clinical notes. Our model is able to accurately predict which patients are likely to progress to OUD based on these factors.  We evaluate our approach using a well-established dataset consisting of over 9 million patients and show that our model outperforms traditional statistical models in terms of accuracy, sensitivity, specificity, precision, F1 score, area under receiver operating characteristic curve (AUROC), and overall performance metrics. Furthermore, we conduct extensive analyses to investigate feature importance, robustness, interpretability, generalization ability, stability, and scalability of our model.  Overall, our work provides a promising new direction for detecting OUD risk among patients receiving opioid therapy, paving the way for early identification and prevention efforts. By improving the understanding of how to recognize those most at risk, our approach has the potential to reduce morbidity and mortality associated with opioid misuse while minimizing overtreatment concerns for low-risk groups. Further validation studies using larger datasets and more diverse patient populations are warranted to strengthen the evidence base for t",1
"AI and Machine Learning can offer powerful tools to help in the fight against Covid-19. In this paper we present a study and a concrete tool based on machine learning to predict the prognosis of hospitalised patients with Covid-19. In particular we address the task of predicting the risk of death of a patient at different times of the hospitalisation, on the base of some demographic information, chest X-ray scores and several laboratory findings. Our machine learning models use ensembles of decision trees trained and tested using data from more than 2000 patients. An experimental evaluation of the models shows good performance in solving the addressed task.",0
"This research study aimed to develop a model that can predict prognoses for COVID-19 patients using laboratory tests and X-ray data. The proposed method uses random decision trees, which have been shown to effectively handle high-dimensional datasets and imbalanced classes. To evaluate the performance of the model, we used a dataset consisting of over 2,000 COVID-19 patient records containing demographic information, clinical observations, laboratory test results, chest radiograph reports, and outcome variables indicating whether or not each patient experienced complications following their hospitalization. We found that our randomized decision tree model was able to accurately predict outcomes with an overall accuracy rate of 86%, sensitivity of 74.42% and specificity of 82%. These promising results suggest that our approach has potential applications in assisting physicians in making informed decisions on the management of COVID-19 patients based on their individual characteristics. However further studies need to be conducted before these models could become mainstream part of medical practice.",1
"The increasing availability of healthcare data requires accurate analysis of disease diagnosis, progression, and realtime monitoring to provide improved treatments to the patients. In this context, Machine Learning (ML) models are used to extract valuable features and insights from high-dimensional and heterogeneous healthcare data to detect different diseases and patient activities in a Smart Healthcare System (SHS). However, recent researches show that ML models used in different application domains are vulnerable to adversarial attacks. In this paper, we introduce a new type of adversarial attacks to exploit the ML classifiers used in a SHS. We consider an adversary who has partial knowledge of data distribution, SHS model, and ML algorithm to perform both targeted and untargeted attacks. Employing these adversarial capabilities, we manipulate medical device readings to alter patient status (disease-affected, normal condition, activities, etc.) in the outcome of the SHS. Our attack utilizes five different adversarial ML algorithms (HopSkipJump, Fast Gradient Method, Crafting Decision Tree, Carlini & Wagner, Zeroth Order Optimization) to perform different malicious activities (e.g., data poisoning, misclassify outputs, etc.) on a SHS. Moreover, based on the training and testing phase capabilities of an adversary, we perform white box and black box attacks on a SHS. We evaluate the performance of our work in different SHS settings and medical devices. Our extensive evaluation shows that our proposed adversarial attack can significantly degrade the performance of a ML-based SHS in detecting diseases and normal activities of the patients correctly, which eventually leads to erroneous treatment.",0
"This study investigates adversarial attacks on machine learning (ML) systems used for healthcare applications. Despite their increasing popularity and potential benefits for improving patient outcomes, ML models remain vulnerable to adversarial examples that can cause incorrect predictions by adding small perturbations to input data. Our research examines whether such malicious inputs pose significant risks in smart healthcare environments where critical decisions may depend on automation. We first analyze three common ML architectures widely employed in medical diagnostics: convolutional neural networks, recurrent neural networks, and random decision forests. Next, we generate realistic yet synthetic patient data with added noise as attack vectors to determine which model variants yield more consistent results when facing adversarial inputs. Our experiments demonstrate that even minor distortions to clinical datasets can lead to drastic changes in classifications across all tested ML frameworks. These findings indicate a pressing need to address security concerns regarding ML technologies designed for healthcare scenarios. Finally, our work discusses possible defenses against adversarial examples to improve the robustness of modern intelligent health informatics infrastructures. Overall, our contributions raise awareness of potential pitfalls associated with trusting advanced analytics methods in sensitive medical contexts without adequate protection measures.",1
"We study the problem of balancing effectiveness and efficiency in automated feature selection. After exploring many feature selection methods, we observe a computational dilemma: 1) traditional feature selection is mostly efficient, but difficult to identify the best subset; 2) the emerging reinforced feature selection automatically navigates to the best subset, but is usually inefficient. Can we bridge the gap between effectiveness and efficiency under automation? Motivated by this dilemma, we aim to develop a novel feature space navigation method. In our preliminary work, we leveraged interactive reinforcement learning to accelerate feature selection by external trainer-agent interaction. In this journal version, we propose a novel interactive and closed-loop architecture to simultaneously model interactive reinforcement learning (IRL) and decision tree feedback (DTF). Specifically, IRL is to create an interactive feature selection loop and DTF is to feed structured feature knowledge back to the loop. First, the tree-structured feature hierarchy from decision tree is leveraged to improve state representation. In particular, we represent the selected feature subset as an undirected graph of feature-feature correlations and a directed tree of decision features. We propose a new embedding method capable of empowering graph convolutional network to jointly learn state representation from both the graph and the tree. Second, the tree-structured feature hierarchy is exploited to develop a new reward scheme. In particular, we personalize reward assignment of agents based on decision tree feature importance. In addition, observing agents' actions can be feedback, we devise another reward scheme, to weigh and assign reward based on the feature selected frequency ratio in historical action records. Finally, we present extensive experiments on real-world datasets to show the improved performance.",0
"Title: Interactive Reinforcement Learning for Feature Selection with Decision Tree in the Loop Abstract: This paper presents a novel approach to interactive feature selection that combines reinforcement learning (RL) and decision tree (DT) algorithms. The proposed method uses RL to learn a ranking function for selecting features based on their importance for a given task. The ranking function is trained using a reward signal provided by a DT algorithm, which acts as a surrogate model for the target function. During each iteration of the algorithm, the top ranked features selected by the RL agent are added to the DT, improving its performance and providing better guidance for future iterations. This iterative process allows the system to converge faster than traditional methods and results in more accurate and interpretable models. Our experiments show that our method achieves state-of-the-art performance across multiple benchmark datasets while reducing computational costs significantly compared to other feature selection techniques. Overall, the presented approach offers a promising solution for automating data preprocessing tasks in machine learning applications where interpretability is crucial.",1
"Differentiable forest is an ensemble of decision trees with full differentiability. Its simple tree structure is easy to use and explain. With full differentiability, it would be trained in the end-to-end learning framework with gradient-based optimization method. In this paper, we propose tree attention block(TAB) in the framework of differentiable forest. TAB block has two operations, squeeze and regulate. The squeeze operation would extract the characteristic of each tree. The regulate operation would learn nonlinear relations between these trees. So TAB block would learn the importance of each tree and adjust its weight to improve accuracy. Our experiment on large tabular dataset shows attention augmented differentiable forest would get comparable accuracy with gradient boosted decision trees(GBDT), which is the state-of-the-art algorithm for tabular datasets. And on some datasets, our model has higher accuracy than best GBDT libs (LightGBM, Catboost, and XGBoost). Differentiable forest model supports batch training and batch size is much smaller than the size of training set. So on larger data sets, its memory usage is much lower than GBDT model. The source codes are available at https://github.com/closest-git/QuantumForest.",0
"This should convey a good understanding of our work and highlight its contribution without adding any hyperboles. | | | | | | | |  Abstract: Our approach presents a novel deep learning architecture designed specifically to address common issues associated with processing tabular data such as class imbalance, skewed distributions, and high dimensionality while leveraging attention mechanisms and gradient boosting. Our model utilizes differentiable forest layers which enable end-to-end training with backpropagation through these computation graph operations. Extensive experiments on several real-world benchmark datasets demonstrate that our method outperforms state-of-the-art methods across multiple metrics and tasks such as regression and classification. Overall, we believe that our work constitutes an important step towards enabling more effective and efficient handling of structured big data using machine learning techniques. Please note that this is only a draft version and may need revision before final submission. Feel free to suggest changes if necessary.  -----  Is there something specific you would like me to help you with?",1
"Recent papers have demonstrated that ensemble stumps and trees could be vulnerable to small input perturbations, so robustness verification and defense for those models have become an important research problem. However, due to the structure of decision trees, where each node makes decision purely based on one feature value, all the previous works only consider the $\ell_\infty$ norm perturbation. To study robustness with respect to a general $\ell_p$ norm perturbation, one has to consider the correlation between perturbations on different features, which has not been handled by previous algorithms. In this paper, we study the problem of robustness verification and certified defense with respect to general $\ell_p$ norm perturbations for ensemble decision stumps and trees. For robustness verification of ensemble stumps, we prove that complete verification is NP-complete for $p\in(0, \infty)$ while polynomial time algorithms exist for $p=0$ or $\infty$. For $p\in(0, \infty)$ we develop an efficient dynamic programming based algorithm for sound verification of ensemble stumps. For ensemble trees, we generalize the previous multi-level robustness verification algorithm to $\ell_p$ norm. We demonstrate the first certified defense method for training ensemble stumps and trees with respect to $\ell_p$ norm perturbations, and verify its effectiveness empirically on real datasets.",0
"In this paper we examine the robustness properties of ensemble stumps and trees under $\ell_p$ perturbations. We show that while individual stump and tree classifiers can be highly sensitive to such perturbations, ensembling them leads to dramatic improvements in robustness across all values of $p$. Our analysis provides new insights into why ensembling improves robustness by identifying specific characteristics of each model type that contribute to their enhanced performance in the presence of adversarial attacks. By evaluating several popular machine learning algorithms as both single models and ensembles, our results provide valuable guidance on how to design more resilient systems. Overall, this work advances the understanding of the tradeoffs between accuracy and robustness, shedding light on promising directions for future research in adversarial learning.",1
"A major requirement for credit scoring models is to provide a maximally accurate risk prediction. Additionally, regulators demand these models to be transparent and auditable. Thus, in credit scoring, very simple predictive models such as logistic regression or decision trees are still widely used and the superior predictive power of modern machine learning algorithms cannot be fully leveraged. Significant potential is therefore missed, leading to higher reserves or more credit defaults. This paper works out different dimensions that have to be considered for making credit scoring models understandable and presents a framework for making ``black box'' machine learning models transparent, auditable and explainable. Following this framework, we present an overview of techniques, demonstrate how they can be applied in credit scoring and how results compare to the interpretability of score cards. A real world case study shows that a comparable degree of interpretability can be achieved while machine learning techniques keep their ability to improve predictive power.",0
"This paper explores the importance of transparency, auditability, and explainability in machine learning models used for credit scoring. With more financial institutions using automated decision making processes, there is a growing need to ensure that these systems are transparent, accountable, and can be easily explained. The authors examine existing research on model interpretability and discuss the challenges faced by practitioners in implementing interpretable models in real-world settings. They then propose a framework for evaluating the trade-offs between model performance and interpretability, which can guide practitioners in selecting appropriate techniques for their specific use cases. Overall, this paper provides valuable insights into the critical issues surrounding the use of machine learning in credit scoring and highlights the urgent need for greater transparency and accountability in this field.",1
"Modern gradient boosting software frameworks, such as XGBoost and LightGBM, implement Newton descent in a functional space. At each boosting iteration, their goal is to find the base hypothesis, selected from some base hypothesis class, that is closest to the Newton descent direction in a Euclidean sense. Typically, the base hypothesis class is fixed to be all binary decision trees up to a given depth. In this work, we study a Heterogeneous Newton Boosting Machine (HNBM) in which the base hypothesis class may vary across boosting iterations. Specifically, at each boosting iteration, the base hypothesis class is chosen, from a fixed set of subclasses, by sampling from a probability distribution. We derive a global linear convergence rate for the HNBM under certain assumptions, and show that it agrees with existing rates for Newton's method when the Newton direction can be perfectly fitted by the base hypothesis at each boosting iteration. We then describe a particular realization of a HNBM, SnapBoost, that, at each boosting iteration, randomly selects between either a decision tree of variable depth or a linear regressor with random Fourier features. We describe how SnapBoost is implemented, with a focus on the training complexity. Finally, we present experimental results, using OpenML and Kaggle datasets, that show that SnapBoost is able to achieve better generalization loss than competing boosting frameworks, without taking significantly longer to tune.",0
"Here is my attempt at writing an abstract without including the phrase ""this paper"". Is there something specific I need to change? Also, please keep in mind that I am an AI language model and therefore may miss some nuances and details of academic writing conventions.  SnapBoost: A Novel Approach to Improving Learning Accuracy through Adaptive Boosting Algorithms =============================================================================================  This research presents a novel approach to boosting accuracy in machine learning models using adaptive algorithms. Our methodology, named SnapBoost, leverages the benefits of both homogenous and heterogeneous boosting techniques to create a more efficient and effective learning experience. We demonstrate how our algorithm can improve overall accuracy by identifying important features early on in the training process and focusing attention on those areas while disregarding less relevant ones. This leads to improved generalization performance compared to traditional homogenous and random forest methods alone. Using simulations and real-world datasets, we showcase the effectiveness of SnapBoost in achieving higher classification accuracy across a wide range of data types and applications. Overall, our work contributes a new perspective on boosting techniques and offers a valuable resource for practitioners seeking enhanced learning outcomes.",1
"Deep reinforcement learning has been successful in a variety of tasks, such as game playing and robotic manipulation. However, attempting to learn \textit{tabula rasa} disregards the logical structure of many domains as well as the wealth of readily available knowledge from domain experts that could help ""warm start"" the learning process. We present a novel reinforcement learning technique that allows for intelligent initialization of a neural network weights and architecture. Our approach permits the encoding domain knowledge directly into a neural decision tree, and improves upon that knowledge with policy gradient updates. We empirically validate our approach on two OpenAI Gym tasks and two modified StarCraft 2 tasks, showing that our novel architecture outperforms multilayer-perceptron and recurrent architectures. Our knowledge-based framework finds superior policies compared to imitation learning-based and prior knowledge-based approaches. Importantly, we demonstrate that our approach can be used by untrained humans to initially provide 80% increase in expected reward relative to baselines prior to training (p  0.001), which results in a 60% increase in expected reward after policy optimization (p = 0.011).",0
"An effective artificial intelligence (AI) system requires both general-purpose cognitive abilities and domain-specific knowledge that can only be acquired through learning from human experts. However, traditional approaches relying on manual programming require extensive effort and expertise in engineering and machine learning, making them impractical for many real-world problems. In our research, we propose using deep neural networks (DNNs) to encode human experts’ specialized knowledge into low-dimensional embeddings, which can then serve as a warm start for reinforcement learning algorithms. Our approach allows for rapid deployment of high-performing AI agents without the need for time-consuming hand-engineering. We evaluate our method on several benchmark tasks and demonstrate significant improvements over baseline methods. Our results show that encoding human experts' knowledge through DNNs enables more efficient reinforcement learning and improves overall performance of AI systems.",1
"Alzheimer's Dementia (AD) is an incurable, debilitating, and progressive neurodegenerative condition that affects cognitive function. Early diagnosis is important as therapeutics can delay progression and give those diagnosed vital time. Developing models that analyse spontaneous speech could eventually provide an efficient diagnostic modality for earlier diagnosis of AD. The Alzheimer's Dementia Recognition through Spontaneous Speech task offers acoustically pre-processed and balanced datasets for the classification and prediction of AD and associated phenotypes through the modelling of spontaneous speech. We exclusively analyse the supplied textual transcripts of the spontaneous speech dataset, building and comparing performance across numerous models for the classification of AD vs controls and the prediction of Mental Mini State Exam scores. We rigorously train and evaluate Support Vector Machines (SVMs), Gradient Boosting Decision Trees (GBDT), and Conditional Random Fields (CRFs) alongside deep learning Transformer based models. We find our top performing models to be a simple Term Frequency-Inverse Document Frequency (TF-IDF) vectoriser as input into a SVM model and a pre-trained Transformer based model `DistilBERT' when used as an embedding layer into simple linear models. We demonstrate test set scores of 0.81-0.82 across classification metrics and a RMSE of 4.58.",0
"In recent years, natural language processing (NLP) techniques have been increasingly used in healthcare applications such as predicting Alzheimer’s disease dementia. One NLP method that has shown promise is feature extraction from spontaneous speech through topic modeling. Another promising approach involves using machine learning models like Random Forest and Support Vector Machine on features extracted from acoustic signals obtained from voice recordings. This study compares two different methods – topic modeling and voice analysis for prediction accuracy and their ability to capture subtle changes in speech patterns indicative of early cognitive decline associated with Alzheimer’s disease dementia. Both approaches were applied on data collected from speech samples from elderly participants who were later diagnosed with mild cognitive impairment (MCI), which can lead to Alzheimer’s dementia. Results show that both methods performed equally well in differentiating MCI patients from controls but better feature selection improved performance for some models. However, the proposed approach combining both topical content and vocal characteristics was found to provide highest classification accuracy indicating that incorporation of multiple modalities may improve prediction. Ultimately, our findings demonstrate that NLP techniques combined with machine learning can effectively predict AD associated cognitive decline from spoken language, highlighting future potential for assistive technologies to monitor individuals at risk of developing Alzheimer’s dementia.",1
"Bayesian Decision Trees are known for their probabilistic interpretability. However, their construction can sometimes be costly. In this article we present a general Bayesian Decision Tree algorithm applicable to both regression and classification problems. The algorithm does not apply Markov Chain Monte Carlo and does not require a pruning step. While it is possible to construct a weighted probability tree space we find that one particular tree, the greedy-modal tree (GMT), explains most of the information contained in the numerical examples. This approach seems to perform similarly to Random Forests.",0
"This paper proposes a novel algorithm called ""Bayesian Decision Tree"" (BDTree) that leverages the power of probabilistic reasoning and decision trees to make accurate predictions on both supervised and unsupervised learning problems. BDTree models uncertainty by representing each tree node as a probability distribution over possible values and their associated decisions. These distributions can then be used to infer likelihoods and predictive probabilities from input data. By incorporating probabilistic inference into decision tree construction, we are able to produce more robust models that adapt well to noisy or incomplete datasets. Experiments conducted on standard benchmark datasets demonstrate significant improvements over existing methods. Our contributions aim to bridge the gap between classical statistical modeling techniques and modern machine learning algorithms, enabling more effective use of uncertainty estimates within classification pipelines. Overall, the proposed method has great potential for applications across a range of domains including healthcare, finance, and security.",1
"Clustering is a popular form of unsupervised learning for geometric data. Unfortunately, many clustering algorithms lead to cluster assignments that are hard to explain, partially because they depend on all the features of the data in a complicated way. To improve interpretability, we consider using a small decision tree to partition a data set into clusters, so that clusters can be characterized in a straightforward manner. We study this problem from a theoretical viewpoint, measuring cluster quality by the $k$-means and $k$-medians objectives: Must there exist a tree-induced clustering whose cost is comparable to that of the best unconstrained clustering, and if so, how can it be found? In terms of negative results, we show, first, that popular top-down decision tree algorithms may lead to clusterings with arbitrarily large cost, and second, that any tree-induced clustering must in general incur an $\Omega(\log k)$ approximation factor compared to the optimal clustering. On the positive side, we design an efficient algorithm that produces explainable clusters using a tree with $k$ leaves. For two means/medians, we show that a single threshold cut suffices to achieve a constant factor approximation, and we give nearly-matching lower bounds. For general $k \geq 2$, our algorithm is an $O(k)$ approximation to the optimal $k$-medians and an $O(k^2)$ approximation to the optimal $k$-means. Prior to our work, no algorithms were known with provable guarantees independent of dimension and input size.",0
"Title: Understanding k-Means and k-Medians Cluster Analysis through Human Interpretable Explanations  In recent years, there has been growing interest in developing machine learning algorithms that can provide transparent and interpretable explanations of their decisions. This requirement becomes even more important in applications where clustering plays a critical role, such as data analysis, anomaly detection, image segmentation, and recommendation systems. Two popular clustering methods used in these areas are k-means and k-medians. However, these algorithms suffer from several limitations that prevent them from providing easily interpretable results, including their sensitivity to initial conditions and difficulty in identifying noise points. In this work, we aim to address these issues by proposing novel approaches based on explainable $k$-means and $k$-medians clustering techniques. Our proposed methods generate human-interpretable explanations for both cluster assignment and feature attributions, making it easier for users to understand and validate clustering results. We evaluate our approach using real-world datasets and show that it outperforms traditional clustering methods while also providing insightful explanations of the underlying clusters. Overall, this research demonstrates the importance of interpretability in clustering and provides new tools for analyzing complex data sets.",1
"We propose two algorithms for interpretation and boosting of tree-based ensemble methods. Both algorithms make use of mathematical programming models that are constructed with a set of rules extracted from an ensemble of decision trees. The objective is to obtain the minimum total impurity with the least number of rules that cover all the samples. The first algorithm uses the collection of decision trees obtained from a trained random forest model. Our numerical results show that the proposed rule covering approach selects only a few rules that could be used for interpreting the random forest model. Moreover, the resulting set of rules closely matches the accuracy level of the random forest model. Inspired by the column generation algorithm in linear programming, our second algorithm uses a rule generation scheme for boosting decision trees. We use the dual optimal solutions of the linear programming models as sample weights to obtain only those rules that would improve the accuracy. With a computational study, we observe that our second algorithm performs competitively with the other well-known boosting methods. Our implementations also demonstrate that both algorithms can be trivially coupled with the existing random forest and decision tree packages.",0
"""Rule covering"" refers to techniques which aim to assign rules to observations so that each observation is covered by at least one rule. This can be used as a technique to build interpretable models, particularly from large datasets where it may be difficult to directly interpret individual features or complex model structures. In this work we introduce new approaches to generating rule sets using different combinations of decision trees and logistic regression. We evaluate our methods on several real world data sets including breast cancer diagnosis, adult income prediction, and customer churn detection. Our experiments show that rule covering can improve predictive performance while simultaneously providing human readable rules that explain how predictions are made. Further, by combining rule covering with feature selection based on statistical significance tests, we demonstrate improved results over state of art benchmarks in certain cases. This work contributes to the growing literature on interpretability in machine learning and demonstrates novel ways to generate meaningful explanations for complex black box models.",1
"Decision tree algorithms have been among the most popular algorithms for interpretable (transparent) machine learning since the early 1980's. The problem that has plagued decision tree algorithms since their inception is their lack of optimality, or lack of guarantees of closeness to optimality: decision tree algorithms are often greedy or myopic, and sometimes produce unquestionably suboptimal models. Hardness of decision tree optimization is both a theoretical and practical obstacle, and even careful mathematical programming approaches have not been able to solve these problems efficiently. This work introduces the first practical algorithm for optimal decision trees for binary variables. The algorithm is a co-design of analytical bounds that reduce the search space and modern systems techniques, including data structures and a custom bit-vector library. Our experiments highlight advantages in scalability, speed, and proof of optimality.",0
"Include a hook statement that draws readers into your work. You can use quotes if you like. If you find relevant data from existing literature, please state the source so I can verify later on. Your final abstract should be at least five sentences long. Please note: The content should remain the same, but some details may change for readability purposes. Please consider revisions as part of our collaboration before submitting a draft for my approval. Thank you!",1
"Competitions play an invaluable role in the field of forecasting, as exemplified through the recent M4 competition. The competition received attention from both academics and practitioners and sparked discussions around the representativeness of the data for business forecasting. Several competitions featuring real-life business forecasting tasks on the Kaggle platform has, however, been largely ignored by the academic community. We believe the learnings from these competitions have much to offer to the forecasting community and provide a review of the results from six Kaggle competitions. We find that most of the Kaggle datasets are characterized by higher intermittence and entropy than the M-competitions and that global ensemble models tend to outperform local single models. Furthermore, we find the strong performance of gradient boosted decision trees, increasing success of neural networks for forecasting, and a variety of techniques for adapting machine learning models to the forecasting task.",0
"Forecasting competitions offer unprecedented opportunities for data scientists from both academia and industry to gain hands-on experience in developing cutting-edge predictive models while making meaningful contributions to scientific knowledge. This study examines the potential of kaggle forecasting competitions as a pedagogical tool and highlights their advantages over traditional classroom settings by showcasing successful cases of students applying concepts learned through these competitions to real-world problems outside of their coursework. Additionally, we demonstrate how participating teams can significantly enhance their skills through collaboration and knowledge transfer among peers and mentors. Finally, our analysis shows that competitive forecasting platforms serve as powerful incubators fostering innovation in methods ranging from traditional time-series decomposition techniques to deep learning architectures, thus advancing state-of-the art predictions across diverse domains such as economics, finance, climate science, epidemiology, computer vision, natural language processing etc. Overall, our findings suggest that kaggle forecasting competitions represent a valuable yet underutilized resource that could benefit both individuals and organizations seeking professional development, academic institutions wishing to modernize curricula, and research communities looking for new approaches in developing novel solutions for complex challenges. By recognizing the unique benefits offered by these contests, participants can set themselves apart from others by leveraging them to achieve personal growth, career advancement, and scientific breakthroughs.",1
"In this paper, we study the problem of balancing effectiveness and efficiency in automated feature selection. Feature selection is a fundamental intelligence for machine learning and predictive analysis. After exploring many feature selection methods, we observe a computational dilemma: 1) traditional feature selection methods (e.g., mRMR) are mostly efficient, but difficult to identify the best subset; 2) the emerging reinforced feature selection methods automatically navigate feature space to explore the best subset, but are usually inefficient. Are automation and efficiency always apart from each other? Can we bridge the gap between effectiveness and efficiency under automation? Motivated by such a computational dilemma, this study is to develop a novel feature space navigation method. To that end, we propose an Interactive Reinforced Feature Selection (IRFS) framework that guides agents by not just self-exploration experience, but also diverse external skilled trainers to accelerate learning for feature exploration. Specifically, we formulate the feature selection problem into an interactive reinforcement learning framework. In this framework, we first model two trainers skilled at different searching strategies: (1) KBest based trainer; (2) Decision Tree based trainer. We then develop two strategies: (1) to identify assertive and hesitant agents to diversify agent training, and (2) to enable the two trainers to take the teaching role in different stages to fuse the experiences of the trainers and diversify teaching process. Such a hybrid teaching strategy can help agents to learn broader knowledge, and, thereafter, be more effective. Finally, we present extensive experiments on real-world datasets to demonstrate the improved performances of our method: more efficient than existing reinforced selection and more effective than classic selection.",0
"This paper presents AutoFS, a system that leverages automated feature selection based on diversity-aware interactive reinforcement learning techniques. Our approach aims to improve upon traditional methods that rely solely on statistical measures such as correlation coefficients or mutual information. Instead, we propose using diverse sets of features generated from different subsets of data points combined with evolutionary strategies inspired by natural selection. These populations evolve through interactions with users and machine agents using multi-armed bandit reward models. We show empirically how our method outperforms state-of-the-art algorithms across three real world datasets while improving user experience. Additionally, we introduce insights into the role of interactivity in feature selection and discuss potential future directions for researchers interested in expanding current automation capabilities. Overall, AutoFS provides a new framework for unsupervised feature selection that emphasizes both efficiency and effectiveness.",1
"This paper presents an experimental comparison among four Automated Machine Learning (AutoML) methods for recommending the best classification algorithm for a given input dataset. Three of these methods are based on Evolutionary Algorithms (EAs), and the other is Auto-WEKA, a well-known AutoML method based on the Combined Algorithm Selection and Hyper-parameter optimisation (CASH) approach. The EA-based methods build classification algorithms from a single machine learning paradigm: either decision-tree induction, rule induction, or Bayesian network classification. Auto-WEKA combines algorithm selection and hyper-parameter optimisation to recommend classification algorithms from multiple paradigms. We performed controlled experiments where these four AutoML methods were given the same runtime limit for different values of this limit. In general, the difference in predictive accuracy of the three best AutoML methods was not statistically significant. However, the EA evolving decision-tree induction algorithms has the advantage of producing algorithms that generate interpretable classification models and that are more scalable to large datasets, by comparison with many algorithms from other learning paradigms that can be recommended by Auto-WEKA. We also observed that Auto-WEKA has shown meta-overfitting, a form of overfitting at the meta-learning level, rather than at the base-learning level.",0
"Abstract: Recommender systems have become increasingly important tools for selecting appropriate models for machine learning tasks such as classification. In recent years, automated methods based on data mining, statistical analysis, and artificial intelligence have been developed for generating recommendations. However, little research has focused specifically on recommending algorithms for classification problems using these techniques. This paper presents an extensive experimental evaluation of several state-of-the-art automated recommendation approaches for classification, including a novel hybrid method combining multiple strategies. Our findings indicate that while individual methods perform well in specific cases, no single approach consistently outperforms all others across different datasets. Additionally, we identify dataset characteristics impacting performance, emphasizing the importance of considering problem-specific factors when choosing an algorithm selection strategy. We provide detailed comparisons of our methods on a wide range of real-world datasets, enabling practitioners to select an appropriate recommendation approach suited to their needs. Finally, we discuss future directions and potential improvements for further advancing automated classification algorithm recommendation.",1
"In this paper, we solve the arms exponential exploding issue in multivariate Multi-Armed Bandit (Multivariate-MAB) problem when the arm dimension hierarchy is considered. We propose a framework called path planning (TS-PP) which utilizes decision graph/trees to model arm reward success rate with m-way dimension interaction, and adopts Thompson sampling (TS) for heuristic search of arm selection. Naturally, it is quite straightforward to combat the curse of dimensionality using a serial processes that operates sequentially by focusing on one dimension per each process. For our best acknowledge, we are the first to solve Multivariate-MAB problem using graph path planning strategy and deploying alike Monte-Carlo tree search ideas. Our proposed method utilizing tree models has advantages comparing with traditional models such as general linear regression. Simulation studies validate our claim by achieving faster convergence speed, better efficient optimal arm allocation and lower cumulative regret.",0
"In recent years, there has been increasing interest in developing efficient algorithms for solving multivariate bandit problems. These algorithms seek to maximize total reward by optimizing a sequence of actions based on partial feedback about their outcomes. Existing solutions are limited, however, due to the computational cost required to explore all possible paths through the action space. This paper proposes an algorithm that addresses this challenge by incorporating path planning into the optimization process. By representing paths as graphs and using graph search techniques, our approach reduces computation time while maintaining high levels of accuracy in determining optimal policies. Our empirical results demonstrate significant improvements over state-of-the-art methods in terms of both speed and quality of solution.",1
"Nonlinear metrics, such as the F1-score, Matthews correlation coefficient, and Fowlkes-Mallows index, are often used to evaluate the performance of machine learning models, in particular, when facing imbalanced datasets that contain more samples of one class than the other. Recent optimal decision tree algorithms have shown remarkable progress in producing trees that are optimal with respect to linear criteria, such as accuracy, but unfortunately nonlinear metrics remain a challenge. To address this gap, we propose a novel algorithm based on bi-objective optimisation, which treats misclassifications of each binary class as a separate objective. We show that, for a large class of metrics, the optimal tree lies on the Pareto frontier. Consequently, we obtain the optimal tree by using our method to generate the set of all nondominated trees. To the best of our knowledge, this is the first method to compute provably optimal decision trees for nonlinear metrics. Our approach leads to a trade-off when compared to optimising linear metrics: the resulting trees may be more desirable according to the given nonlinear metric at the expense of higher runtimes. Nevertheless, the experiments illustrate that runtimes are reasonable for majority of the tested datasets.",0
"In ""Optimal Decision Trees for Nonlinear Metrics"" we introduce an algorithm that can learn optimal decision trees from nonlinear data. Unlike previous algorithms, which relied on approximations or linearizations of complex functions, our method uses advanced techniques from machine learning and computer graphics to build highly accurate models directly from raw sensor data. We demonstrate the effectiveness of our approach using extensive simulations as well as real world examples drawn from domains ranging from astronomy to medicine. This work has important applications in fields such as robotics, image recognition, and scientific modeling, where accuracy is critical but traditional methods fall short.",1
"The random subspace method, known as the pillar of random forests, is good at making precise and robust predictions. However, there is not a straightforward way yet to combine it with deep learning. In this paper, we therefore propose Neural Random Subspace (NRS), a novel deep learning based random subspace method. In contrast to previous forest methods, NRS enjoys the benefits of end-to-end, data-driven representation learning, as well as pervasive support from deep learning software and hardware platforms, hence achieving faster inference speed and higher accuracy. Furthermore, as a non-linear component to be encoded into Convolutional Neural Networks (CNNs), NRS learns non-linear feature representations in CNNs more efficiently than previous higher-order pooling methods, producing good results with negligible increase in parameters, floating point operations (FLOPs) and real running time. Compared with random subspaces, random forests and gradient boosting decision trees (GBDTs), NRS achieves superior performance on 35 machine learning datasets. Moreover, on both 2D image and 3D point cloud recognition tasks, integration of NRS with CNN architectures achieves consistent improvements with minor extra cost. Code is available at https://github.com/CupidJay/NRS_pytorch.",0
"In recent years, deep learning has revolutionized many fields through advances in machine perception and natural language understanding. However, designing neural network architectures remains challenging due to computational requirements and uncertainty about what constitutes optimal models. To address these issues, researchers have proposed the use of random subspaces as a means to reduce model complexity while maintaining strong generalization performance. This paper describes an approach that leverages neural networks to search and learn from random subspaces using gradient descent. Through empirical evaluation on several benchmark datasets, we demonstrate that our method significantly improves accuracy over traditional methods while requiring less compute time. Our results suggest that neural random subspace provides a promising new direction for efficient architecture selection and knowledge transfer among related tasks. We plan to continue exploring applications of this technique in other domains.",1
"This paper discusses an approach with machine-learning probability models to evaluate the difference between good and bad data quality in a dataset. A decision tree algorithm is used to predict data quality based on no domain knowledge of the datasets under examination. It is shown that for the data examined, the ability to predict the quality of data based on simple good/bad pre-labelled learning examples is accurate, however in general it may not be sufficient for useful production data quality assessment.",0
"""Data quality evaluation plays a crucial role in many applications such as data warehousing, business intelligence, scientific research, and decision making. In practice, evaluating the quality of data is often subjective and based on human judgment alone. However, objective methods that rely on mathematical models can provide a more consistent and accurate assessment of data quality. This paper proposes the use of probability models to evaluate data quality by examining the distribution and variability of attribute values within the dataset. Specifically, we introduce two probabilistic measures: probability dispersion measure (PDM) and entropy measure (EM), which quantify the uncertainty and diversity present in the data. These measures are evaluated through experiments on both synthetic datasets and real-world datasets, demonstrating their effectiveness in identifying data anomalies and inconsistencies. Our findings suggest that these measures can improve the accuracy and robustness of data quality evaluation compared to traditional methods relying solely on statistical metrics like mean and standard deviation.""",1
"The objective of this study is to develop a good risk model for classifying business delinquency by simultaneously exploring several machine learning based methods including regularization, hyper-parameter optimization, and model ensembling algorithms. The rationale under the analyses is firstly to obtain good base binary classifiers (include Logistic Regression ($LR$), K-Nearest Neighbors ($KNN$), Decision Tree ($DT$), and Artificial Neural Networks ($ANN$)) via regularization and appropriate settings of hyper-parameters. Then two model ensembling algorithms including bagging and boosting are performed on the good base classifiers for further model improvement. The models are evaluated using accuracy, Area Under the Receiver Operating Characteristic Curve (AUC of ROC), recall, and F1 score via repeating 10-fold cross-validation 10 times. The results show the optimal base classifiers along with the hyper-parameter settings are $LR$ without regularization, $KNN$ by using 9 nearest neighbors, $DT$ by setting the maximum level of the tree to be 7, and $ANN$ with three hidden layers. Bagging on $KNN$ with $K$ valued 9 is the optimal model we can get for risk classification as it reaches the average accuracy, AUC, recall, and F1 score valued 0.90, 0.93, 0.82, and 0.89, respectively.",0
"Abstract: Effective risk management is crucial for organizations to make informed decisions and take appropriate actions to mitigate potential losses. One approach to improving risk models is by utilizing machine learning algorithms that can learn from historical data and identify patterns that may not have been apparent otherwise. In this research paper, we discuss the development and improvement of risk models through the use of advanced machine learning techniques such as neural networks and decision trees. We evaluate the performance of these models against traditional statistical methods commonly used in financial modeling. Our results show that the machine learning based algorithms outperform traditional methods in terms of accuracy, flexibility, and ability to handle complex nonlinear relationships. Furthermore, we demonstrate how to improve the interpretability and transparency of machine learning models, which can aid in their adoption in regulated industries. Overall, our findings highlight the significant potential of using machine learning algorithms to develop more effective risk models that better inform business decisions.",1
"The adoption of deep learning in healthcare is hindered by their ""black box"" nature. In this paper, we explore the RETAIN architecture for the task of glusose forecasting for diabetic people. By using a two-level attention mechanism, the recurrent-neural-network-based RETAIN model is interpretable. We evaluate the RETAIN model on the type-2 IDIAB and the type-1 OhioT1DM datasets by comparing its statistical and clinical performances against two deep models and three models based on decision trees. We show that the RETAIN model offers a very good compromise between accuracy and interpretability, being almost as accurate as the LSTM and FCN models while remaining interpretable. We show the usefulness of its interpretable nature by analyzing the contribution of each variable to the final prediction. It revealed that signal values older than one hour are not used by the RETAIN model for the 30-minutes ahead of time prediction of glucose. Also, we show how the RETAIN model changes its behavior upon the arrival of an event such as carbohydrate intakes or insulin infusions. In particular, it showed that the patient's state before the event is particularily important for the prediction. Overall the RETAIN model, thanks to its interpretability, seems to be a very promissing model for regression or classification tasks in healthcare.",0
"In healthcare applications, deep learning models have shown great potential for improving patient care and decision support. However, their use has been limited by concerns about interpretability – understanding how these complex systems make decisions. This work addresses this challenge by developing novel attention mechanisms that can enhance the interpretation of deep models trained on health data. Specifically, we focus on the problem of glucose forecasting for diabetic patients, where accurate predictions can greatly improve outcomes. We propose two new attention techniques – temporal attention and physiological attention – which allow us to better understand how our model makes predictions over time and across different physiological signals. Our experiments demonstrate significantly improved accuracy compared to state-of-the-art baselines, while also providing interpretable insights into the model’s reasoning process. Overall, this study shows the promise of enhanced interpretability through attention mechanisms, allowing clinicians to confidently deploy powerful artificial intelligence algorithms in critical healthcare contexts.",1
"This paper introduces the MCML approach for empirically studying the learnability of relational properties that can be expressed in the well-known software design language Alloy. A key novelty of MCML is quantification of the performance of and semantic differences among trained machine learning (ML) models, specifically decision trees, with respect to entire (bounded) input spaces, and not just for given training and test datasets (as is the common practice). MCML reduces the quantification problems to the classic complexity theory problem of model counting, and employs state-of-the-art model counters. The results show that relatively simple ML models can achieve surprisingly high performance (accuracy and F1-score) when evaluated in the common setting of using training and test datasets - even when the training dataset is much smaller than the test dataset - indicating the seeming simplicity of learning relational properties. However, MCML metrics based on model counting show that the performance can degrade substantially when tested against the entire (bounded) input space, indicating the high complexity of precisely learning these properties, and the usefulness of model counting in quantifying the true performance.",0
"In this paper we develop new theoretical results that explore how properties of relational learning problems can impact the learnability of these problems by machine learning algorithms like decision trees or neural networks. We frame our study as a concrete instantiation of a general approach called model counting meets machine learning or MCML for short. Our main technical result shows that under some mild conditions on the target concept class any property P that ensures finite sample complexity for learning from examples (with respect to some distribution over instances) implies a novel upper bound guarantee on excess risk achieved using an appropriately chosen class of predictors constructed via decision tree ensembles. This bound has implications for both sample efficiency and optimization of algorithm hyperparameters like depths of trees. Importantly, these guarantees go beyond existing bounds for standard settings where one learns from random labeled data under iid noise models but without structural assumptions on the problem structure which are satisfied by many real world applications from causal inference, natural language processing, program synthesis etcetera, for which there exists ample experimental evidence suggesting performance benefits derived from leveraging additional domain knowledge and/or structured representations capturing relationships between objects under consideration. To provide intuition behind why such structural assumptions may lead to more efficient learnability our companion work studies two families of simple graphical models whose learnability depends crucially on whether edges encode conditional independence constraints or alternative types of relations satisfying different sets of axioms. Additionally, our experiments showcase cases where incorporating explicit reasoning about relatonal structure can significantly speed up training procedures leading to substantially smaller models while maintaining competitive accuracy across benchmark datasets drawn from diverse domains. Altogether we believe this initial exploration into relational learning using methods inspired by theory developed for MCML demonstrates significant promise and sheds light onto open research directions at intersection of computer science, mathematics and statistics for studying the limits and opportunities underlying modern artificial intelligence systems capable",1
"Least-mean squares (LMS) solvers such as Linear / Ridge / Lasso-Regression, SVD and Elastic-Net not only solve fundamental machine learning problems, but are also the building blocks in a variety of other methods, such as decision trees and matrix factorizations.   We suggest an algorithm that gets a finite set of $n$ $d$-dimensional real vectors and returns a weighted subset of $d+1$ vectors whose sum is \emph{exactly} the same. The proof in Caratheodory's Theorem (1907) computes such a subset in $O(n^2d^2)$ time and thus not used in practice. Our algorithm computes this subset in $O(nd+d^4\log{n})$ time, using $O(\log n)$ calls to Caratheodory's construction on small but ""smart"" subsets. This is based on a novel paradigm of fusion between different data summarization techniques, known as sketches and coresets.   For large values of $d$, we suggest a faster construction that takes $O(nd)$ time (linear in the input's size) and returns a weighted subset of $O(d)$ sparsified input points. Here, sparsified point means that some of its entries were replaced by zeroes.   As an example application, we show how it can be used to boost the performance of existing LMS solvers, such as those in scikit-learn library, up to x100. Generalization for streaming and distributed (big) data is trivial. Extensive experimental results and complete open source code are also provided.",0
"Solve Linear Least Squares Problems Quickly and Correctly! In a variety of fields including statistics, economics, engineering, and computer vision, linear least squares problems arise frequently. These problems involve finding the vector x that minimizes the sum of squared distances between itself and a set of other vectors v_i: ||x - v_i||^2. This problem can have multiple solutions, but we focus on the case where there is only one unique solution.",1
"As Machine Learning (ML) becomes pervasive in various real world systems, the need for models to be understandable has increased. We focus on interpretability, noting that models often need to be constrained in size for them to be considered interpretable, e.g., a decision tree of depth 5 is easier to interpret than one of depth 50. But smaller models also tend to have high bias. This suggests a trade-off between interpretability and accuracy. We propose a model agnostic technique to minimize this trade-off. Our strategy is to first learn a powerful, possibly black-box, probabilistic model - referred to as the oracle - on the training data. Uncertainty in the oracle's predictions are used to learn a sampling distribution for the training data. The interpretable model is trained on a sample obtained using this distribution. We demonstrate that such a model often is significantly more accurate than one trained on the original data.   Determining the sampling strategy is formulated as an optimization problem. Our solution to this problem possesses the following key favorable properties: (1) the number of optimization variables is independent of the dimensionality of the data: a fixed number of seven variables are used (2) our technique is model agnostic - in that both the interpretable model and the oracle may belong to arbitrary model families.   Results using multiple real world datasets, using Linear Probability Models and Decision Trees as interpretable models, with Gradient Boosted Model and Random Forest as oracles, are presented. We observe significant relative improvements in the F1-score in most cases, occasionally seeing improvements greater than 100%. Additionally, we discuss an interesting application of our technique where a Gated Recurrent Unit network is used to improve the sequence classification accuracy of a Decision Tree that uses character n-grams as features.",0
"In recent years, there has been growing interest in developing machine learning models that are interpretable, meaning they can provide insight into how decisions are made and why certain outcomes occur. One approach to achieve this goal is through the use of an oracle, which provides ground truth explanations for model predictions. This allows researchers to train their models using both data and human expert knowledge, resulting in more accurate and transparent results. However, training such models remains challenging due to several factors including noisy and incomplete data, as well as limited access to experts. To address these issues, we present a novel methodology that leverages the power of both statistical analysis and domain expertise to learn interpretable models using an oracle. By doing so, our approach enables scientists and practitioners to gain deeper insights into complex phenomena while fostering trust in predictive models. We demonstrate the effectiveness of our methodology by applying it to real-world applications in diverse domains including medicine and finance. Our findings show that our proposed method significantly improves model interpretability without sacrificing prediction accuracy. Overall, this work contributes towards building reliable and explainable artificial intelligence systems that can benefit society at large.",1
"The widespread use of modern machine learning methods in decision making crucially depends on their interpretability or explainability. The human users (decision makers) of machine learning methods are often not only interested in getting accurate predictions or projections. Rather, as a decision-maker, the user also needs a convincing answer (or explanation) to the question of why a particular prediction was delivered. Explainable machine learning might be a legal requirement when used for decision making with an immediate effect on the health of human beings. As an example consider the computer vision of a self-driving car whose predictions are used to decide if to stop the car. We have recently proposed an information-theoretic approach to construct personalized explanations for predictions obtained from ML. This method was model-agnostic and only required some training samples of the model to be explained along with a user feedback signal. This paper uses an information-theoretic measure for the quality of an explanation to learn predictors that are intrinsically explainable to a specific user. Our approach is not restricted to a particular hypothesis space, such as linear maps or shallow decision trees, whose predictor maps are considered as explainable by definition. Rather, we regularize an arbitrary hypothesis space using a personalized measure for the explainability of a particular predictor.",0
"This paper presents a framework for Explainable Empirical Risk Minimization (EERM) that addresses both interpretability and generalizability in machine learning. EERM combines explainable modeling techniques such as lasso regression and decision trees with regularization methods like early stopping and cross-validation to balance model complexity and prediction accuracy. Our approach ensures that important features are selected based on their significance, while preventing overfitting by limiting model capacity through data preprocessing and validation techniques. Extensive experiments demonstrate that our method yields models with better interpretability and more robust performance across different datasets compared to traditional black box approaches like random forest and gradient boosting machines. Overall, EERM offers a powerful toolset for building interpretable yet accurate predictive models in real-world applications.",1
"Decision trees are machine learning models commonly used in various application scenarios. In the era of big data, traditional decision tree induction algorithms are not suitable for learning large-scale datasets due to their stringent data storage requirement. Online decision tree learning algorithms have been devised to tackle this problem by concurrently training with incoming samples and providing inference results. However, even the most up-to-date online tree learning algorithms still suffer from either high memory usage or high computational intensity with dependency and long latency, making them challenging to implement in hardware. To overcome these difficulties, we introduce a new quantile-based algorithm to improve the induction of the Hoeffding tree, one of the state-of-the-art online learning models. The proposed algorithm is light-weight in terms of both memory and computational demand, while still maintaining high generalization ability. A series of optimization techniques dedicated to the proposed algorithm have been investigated from the hardware perspective, including coarse-grained and fine-grained parallelism, dynamic and memory-based resource sharing, pipelining with data forwarding. We further present a high-performance, hardware-efficient and scalable online decision tree learning system on a field-programmable gate array (FPGA) with system-level optimization techniques. Experimental results show that our proposed algorithm outperforms the state-of-the-art Hoeffding tree learning method, leading to 0.05% to 12.3% improvement in inference accuracy. Real implementation of the complete learning system on the FPGA demonstrates a 384x to 1581x speedup in execution time over the state-of-the-art design.",0
"This paper presents an approach for efficiently accelerating online decision tree learning on field programmable gate arrays (FPGAs). Current methods for training decision trees can take several hours or even days, making them impractical for real-time applications such as anomaly detection, fraud prevention, and predictive maintenance. Our proposed method uses the Hoeffding tree algorithm, which has been shown to produce comparable results to traditional decision tree algorithms but requires fewer computations. We implement our method on an FPGA platform and demonstrate that it can achieve significant speedup compared to software implementations while maintaining accuracy. Our work shows great potential for scaling up machine learning models without sacrificing performance, enabling their use in time-sensitive applications where real-time predictions are critical.",1
"Several studies have shown that combining machine learning models in an appropriate way will introduce improvements in the individual predictions made by the base models. The key to make well-performing ensemble model is in the diversity of the base models. Of the most common solutions for introducing diversity into the decision trees are bagging and random forest. Bagging enhances the diversity by sampling with replacement and generating many training data sets, while random forest adds selecting a random number of features as well. This has made the random forest a winning candidate for many machine learning applications. However, assuming equal weights for all base decision trees does not seem reasonable as the randomization of sampling and input feature selection may lead to different levels of decision-making abilities across base decision trees. Therefore, we propose several algorithms that intend to modify the weighting strategy of regular random forest and consequently make better predictions. The designed weighting frameworks include optimal weighted random forest based on ac-curacy, optimal weighted random forest based on the area under the curve (AUC), performance-based weighted random forest, and several stacking-based weighted random forest models. The numerical results show that the proposed models are able to introduce significant improvements compared to regular random forest.",0
"In recent years, random forest algorithms have become popular methods for solving classification problems due to their high accuracy and ability to handle complex data sets. However, one drawback of traditional random forest algorithms is that they tend to overfit on smaller datasets, which can lead to poor performance. To address this issue, we propose an improved weighted random forest algorithm that utilizes class weights to balance the contribution of each sample point during tree construction. Our approach assigns higher weights to minority classes and lower weights to majority classes, leading to more balanced decision trees and better overall performance. Experimental results demonstrate significant improvements over traditional random forest algorithms, particularly on small to medium-sized datasets where overfitting can be problematic. This method has potential applications across a variety of fields, including machine learning, computer vision, natural language processing, and bioinformatics.",1
"A general fuzzy min-max (GFMM) neural network is one of the efficient neuro-fuzzy systems for classification problems. However, a disadvantage of most of the current learning algorithms for GFMM is that they can handle effectively numerical valued features only. Therefore, this paper provides some potential approaches to adapting GFMM learning algorithms for classification problems with mixed-type or only categorical features as they are very common in practical applications and often carry very useful information. We will compare and assess three main methods of handling datasets with mixed features, including the use of encoding methods, the combination of the GFMM model with other classifiers, and employing the specific learning algorithms for both types of features. The experimental results showed that the target and James-Stein are appropriate categorical encoding methods for learning algorithms of GFMM models, while the combination of GFMM neural networks and decision trees is a flexible way to enhance the classification performance of GFMM models on datasets with the mixed features. The learning algorithms with the mixed-type feature abilities are potential approaches to deal with mixed-attribute data in a natural way, but they need further improvement to achieve a better classification accuracy. Based on the analysis, we also identify the strong and weak points of different methods and propose potential research directions.",0
This will go on arXiv so there should be no citations in the abstract,1
"Image processing, computer vision, and pattern recognition have been playing a vital role in diverse agricultural applications, such as species detection, recognition, classification, identification, plant growth stages, plant disease detection, and many more. On the other hand, there is a growing need to capture high resolution images using unmanned aerial vehicles (UAV) and to develop better algorithms in order to find highly accurate and to the point results. In this paper, we propose a segmentation and extraction-based technique to detect fusarium wilt in radish crops. Recent wilt detection algorithms are either based on image processing techniques or conventional machine learning algorithms. However, our methodology is based on a hybrid algorithm, which combines image processing and machine learning. First, the crop image is divided into three segments, which include viz., healthy vegetation, ground and packing material. Based on the HSV decision tree algorithm, all the three segments are segregated from the image. Second, the extracted segments are summed together into an empty canvas of the same resolution as the image and one new image is produced. Third, this new image is compared with the original image, and a final noisy image, which contains traces of wilt is extracted. Finally, a k-means algorithm is applied to eliminate the noise and to extract the accurate wilt from it. Moreover, the extracted wilt is mapped on the original image using the contouring method. The proposed combination of algorithms detects the wilt appropriately, which surpasses the traditional practice of separately using the image processing techniques or machine learning.",0
"This paper presents a new method for automatically detecting radish wilt disease using image processing techniques and machine learning algorithms. We first collected images of healthy and diseased radishes from real farms. These images were then used as training data for our model, which was based on convolutional neural networks (CNNs). Our CNN architecture consisted of several layers and trained to extract features that could distinguish healthy and diseases radishes. To evaluate the performance of our method, we conducted experiments comparing our approach to traditional manual methods currently used by farmers to identify radish wilt. Results showed that our automatic detection system had higher accuracy than these manual methods, while also providing faster results. In addition, we found that the proposed system is promising for automating agricultural tasks and can provide useful insights into plant diseases over time. The current study contributes to the growing field of computer vision and machine learning applications in agriculture, potentially helping farmers improve crop management practices and reducing losses due to plant diseases such as radish wilt. Future work includes exploring other types of plants and expanding the scope of application of our methodology to different regions and environmental conditions",1
"Software testing is one of the important ways to ensure the quality of software. It is found that testing cost more than 50% of overall project cost. Effective and efficient software testing utilizes the minimum resources of software. Therefore, it is important to construct the procedure which is not only able to perform the efficient testing but also minimizes the utilization of project resources. The goal of software testing is to find maximum defects in the software system. More the defects found in the software ensure more efficiency is the software testing Different techniques have been proposed to detect the defects in software and to utilize the resources and achieve good results. As world is continuously moving toward data driven approach for making important decision. Therefore, in this research paper we performed the machine learning analysis on the publicly available datasets and tried to achieve the maximum accuracy. The major focus of the paper is to apply different machine learning techniques on the datasets and find out which technique produce efficient result. Particularly, we proposed an ensemble learning models and perform comparative analysis among KNN, Decision tree, SVM and Na\""ive Bayes on different datasets and it is demonstrated that performance of Ensemble method is more than other methods in term of accuracy, precision, recall and F1-score. The classification accuracy of ensemble model trained on CM1 is 98.56%, classification accuracy of ensemble model trained on KM2 is 98.18% similarly, the classification accuracy of ensemble learning model trained on PC1 is 99.27%. This reveals that Ensemble is more efficient method for making the defect prediction as compared other techniques.",0
"This paper proposes a novel approach to software defect prediction using multiple ensemble learning models based on different datasets. Our method leverages existing approaches while addressing their limitations, resulting in more accurate predictions. We present experimental results that demonstrate the effectiveness of our proposed approach compared to traditional methods. The findings of this study have important implications for practitioners seeking improved defect prediction accuracy for their software development projects.",1
"The use of machine learning algorithms in finance, medicine, and criminal justice can deeply impact human lives. As a consequence, research into interpretable machine learning has rapidly grown in an attempt to better control and fix possible sources of mistakes and biases. Tree ensembles offer a good prediction quality in various domains, but the concurrent use of multiple trees reduces the interpretability of the ensemble. Against this background, we study born-again tree ensembles, i.e., the process of constructing a single decision tree of minimum size that reproduces the exact same behavior as a given tree ensemble in its entire feature space. To find such a tree, we develop a dynamic-programming based algorithm that exploits sophisticated pruning and bounding rules to reduce the number of recursive calls. This algorithm generates optimal born-again trees for many datasets of practical interest, leading to classifiers which are typically simpler and more interpretable without any other form of compromise.",0
"Machine learning has revolutionized numerous fields by enabling computers to make informed predictions through data analysis. Trees have emerged as simple yet powerful models that can capture complex relationships among features in datasets. Their success, however, often depends on finding the optimal tree structure, which remains challenging due to combinatorial complexity. An intriguing alternative lies in ensembling trees generated from different random initializations and training sets—so called “Born-Again” methods. In this work we showcase how these approaches achieve superior accuracy on diverse benchmarks without requiring specialized hardware or increased computational budgets. We compare state-of-the-art tree-based ensembles across classification and regression tasks using five popular libraries: scikit-learn, XGBoost, LightGBM, CatBoost, and H2O. Our results demonstrate consistent gains over single trees, previously established baselines such as bagging and boosting, and several competitive deep learning algorithms including Random Forests. We further analyze the robustness of Born-Again methods under varying hyperparameters, tree depth constraints, and sampling rates. Lastly, our ablation study explores the impact of combining multiple model types within a unified framework. These findings provide compelling evidence for the efficacy of Born-Again ensembles as flexible alternatives to traditional methods, paving the way towards improved generalizability and interpretability in machine learning research and practice.",1
"Parameter calibration is a major challenge in agent-based modelling and simulation (ABMS). As the complexity of agent-based models (ABMs) increase, the number of parameters required to be calibrated grows. This leads to the ABMS equivalent of the \say{curse of dimensionality}. We propose an ABMS framework which facilitates the effective integration of different sampling methods and surrogate models (SMs) in order to evaluate how these strategies affect parameter calibration and exploration. We show that surrogate assisted methods perform better than the standard sampling methods. In addition, we show that the XGBoost and Decision Tree SMs are most optimal overall with regards to our analysis.",0
"In this article, we describe some new techniques which use artificial intelligence (AI) systems like our ""Assistant"" AI to help agents make decisions based on uncertain information from sensors (e.g., robots exploring their environment). We show how our system can assist the agent by providing probabilities that a given feature corresponds to different classes of objects or concepts. Our results demonstrate the feasibility of using surrogates with limited computational resources such as smartphones or low cost robotics platforms to achieve good results compared to expensive equipment, and provide an excellent starting point for future work in the field of intelligent agents interacting with complex environments. -----  A significant problem faced by intelligent agents operating in real world scenarios is the need to process large amounts of data collected from sensor readings. Traditional methods often rely on simplifying assumptions regarding the nature of the data or the underlying models used to interpret them. This frequently leads to suboptimal performance, as these representations may not accurately capture essential features present within the raw sensor data. To address this challenge, this study presents several novel surrogate-assisted methodologies designed to enhance the parameterization of agent-based models (ABMs). These strategies employ advanced machine learning algorithms and domain knowledge incorporation mechanisms to extract valuable insights from raw sensor signals. The proposed approaches leverage efficient modeling paradigms capable of producing accurate approximations with minimal computation requirements, enabling deployment on resource-constrained devices. Thorough experimental evaluations involving diverse ABM test cases establish the effectiveness of the presented techniques, outperforming conventional baseline schemes while demonstrating robustness under challenging conditions. The reported findings provide valuable contributions t o researchers interested i n developing smarter decision-making capabilities for intelligent agents navigating complex environments.",1
"Ensembles of decision trees perform well on many problems, but are not interpretable. In contrast to existing approaches in interpretability that focus on explaining relationships between features and predictions, we propose an alternative approach to interpret tree ensemble classifiers by surfacing representative points for each class -- prototypes. We introduce a new distance for Gradient Boosted Tree models, and propose new, adaptive prototype selection methods with theoretical guarantees, with the flexibility to choose a different number of prototypes in each class. We demonstrate our methods on random forests and gradient boosted trees, showing that the prototypes can perform as well as or even better than the original tree ensemble when used as a nearest-prototype classifier. In a user study, humans were better at predicting the output of a tree ensemble classifier when using prototypes than when using Shapley values, a popular feature attribution method. Hence, prototypes present a viable alternative to feature-based explanations for tree ensembles.",0
"The interpretability of machine learning models has become increasingly important as they continue to play larger roles in critical decision making processes. One popular approach to improving model interpretability is to use ensembles of decision trees. However, current methods for visualizing tree space prototypes suffer from limitations that hinder their effectiveness in understanding these models. This study proposes an alternative method, ""Tree Space Prototypes,"" which addresses these shortcomings by providing a more detailed and nuanced representation of ensemble behavior. By allowing users to explore different parts of the feature space and see how individual samples are mapped onto the prototype trees, our method provides insight into both global model behavior and local idiosyncrasies. Through extensive experiments on benchmark datasets, we demonstrate that our proposed method outperforms traditional approaches, enabling better interpretation of decision ensembles and improved transparency in machine learning applications. Our results contribute new insights into the field of interpretable machine learning and highlight the potential benefits of using Tree Space Prototypes for real-world decision support systems.",1
"Classification of datasets into two or more distinct classes is an important machine learning task. Many methods are able to classify binary classification tasks with a very high accuracy on test data, but cannot provide any easily interpretable explanation for users to have a deeper understanding of reasons for the split of data into two classes. In this paper, we highlight and evaluate a recently proposed nonlinear decision tree approach with a number of commonly used classification methods on a number of datasets involving a few to a large number of features. The study reveals key issues such as effect of classification on the method's parameter values, complexity of the classifier versus achieved accuracy, and interpretability of resulting classifiers.",0
"For decades, decision trees have been widely used in machine learning as a powerful tool for classification tasks. In recent years, nonlinear decision tree algorithms have emerged that aim to capture more complex relationships within the data. However, there remains a need for evaluation and comparison of these methods against established linear decision tree algorithms and other state-of-the-art approaches. This study addresses that gap by assessing multiple models, including random forest and gradient boosting machines, on both benchmark datasets and real-world applications. We provide evidence regarding their strengths and limitations, highlighting promising areas for further research and development. Our results demonstrate that while nonlinear decision trees may outperform some existing methods, they don't consistently beat all other types of classifiers across diverse domains. By shedding light on these findings, we contribute valuable insights to guide future work in decision tree model design and optimization.",1
"How to obtain a model with good interpretability and performance has always been an important research topic. In this paper, we propose rectified decision trees (ReDT), a knowledge distillation based decision trees rectification with high interpretability, small model size, and empirical soundness. Specifically, we extend the impurity calculation and the pure ending condition of the classical decision tree to propose a decision tree extension that allows the use of soft labels generated by a well-trained teacher model in training and prediction process. It is worth noting that for the acquisition of soft labels, we propose a new multiple cross-validation based method to reduce the effects of randomness and overfitting. These approaches ensure that ReDT retains excellent interpretability and even achieves fewer nodes than the decision tree in the aspect of compression while having relatively good performance. Besides, in contrast to traditional knowledge distillation, back propagation of the student model is not necessarily required in ReDT, which is an attempt of a new knowledge distillation approach. Extensive experiments are conducted, which demonstrates the superiority of ReDT in interpretability, compression, and empirical soundness.",0
"Title: ""Interpreting Deep Learning Models through Data Preparation Techniques""  Abstract: This study focuses on improving interpretability, compression, and empirical soundness of deep learning models by proposing a novel architecture based on decision trees called rectified decision trees (RDT). These structures combine decision trees with a rectification operation that removes their exponential nature, making them suitable for efficient training and interpretation. We evaluate our approach using multiple benchmark datasets and demonstrate how RDTs can effectively balance accuracy, interpretability, and model complexity. Our findings suggest that data preparation techniques, such as feature selection and dimensionality reduction, play a crucial role in achieving interpretable results while maintaining high predictive performance. Overall, our work provides insights into better understanding the inner workings of deep neural networks, paving the way for more reliable applications across different domains.",1
"Interpretability and effectiveness are two essential and indispensable requirements for adopting machine learning methods in reality. In this paper, we propose a knowledge distillation based decision trees extension, dubbed rectified decision trees (ReDT), to explore the possibility of fulfilling those requirements simultaneously. Specifically, we extend the splitting criteria and the ending condition of the standard decision trees, which allows training with soft labels while preserving the deterministic splitting paths. We then train the ReDT based on the soft label distilled from a well-trained teacher model through a novel jackknife-based method. Accordingly, ReDT preserves the excellent interpretable nature of the decision trees while having a relatively good performance. The effectiveness of adopting soft labels instead of hard ones is also analyzed empirically and theoretically. Surprisingly, experiments indicate that the introduction of soft labels also reduces the model size compared with the standard decision trees from the aspect of the total nodes and rules, which is an unexpected gift from the `dark knowledge' distilled from the teacher model.",0
"Title: Rectified Decision Trees: Exploring the Landscape of Interpretable and Effective Machine Learning  Abstract: This paper proposes a new machine learning model called rectified decision trees (RDT), which aim to bridge the gap between state-of-the-art black-box models such as deep neural networks and interpretable models like rule-based systems. RDT combines feature selection based on randomness with depthwise and widthwise pruning techniques to achieve interpretability while maintaining high accuracy and efficiency. We evaluate the performance of RDT against several popular classification algorithms including random forest, XGBoost, gradient boosting machines, and convolutional neural networks. Our results show that RDT achieves comparable performance to these baseline methods across diverse datasets. Furthermore, we demonstrate the interpretability of our model by analyzing visualizations generated from RDTs on real-world data sets. Finally, we provide insights into future directions for improving both effectiveness and interpretability through careful design of regularization terms. Overall, this work demonstrates that RDT is a promising alternative approach towards building accurate and interpretable machine learning models.",1
"Fast approximations of power flow results are beneficial in power system planning and live operation. In planning, millions of power flow calculations are necessary if multiple years, different control strategies or contingency policies are to be considered. In live operation, grid operators must assess if grid states comply with contingency requirements in a short time. In this paper, we compare regression and classification methods to either predict multi-variable results, e.g. bus voltage magnitudes and line loadings, or binary classifications of time steps to identify critical loading situations. We test the methods on three realistic power systems based on time series in 15 min and 5 min resolution of one year. We compare different machine learning models, such as multilayer perceptrons (MLPs), decision trees, k-nearest neighbours, gradient boosting, and evaluate the required training time and prediction times as well as the prediction errors. We additionally determine the amount of training data needed for each method and show results, including the approximation of untrained curtailment of generation. Regarding the compared methods, we identified the MLPs as most suitable for the task. The MLP-based models can predict critical situations with an accuracy of 97-98 % and a very low number of false negative predictions of 0.0-0.64 %.",0
"In recent years, machine learning has emerged as a promising approach for detecting contingency cases in various domains, such as finance, healthcare, and insurance. However, evaluating the performance of these models can be challenging due to the lack of standardized evaluation metrics and datasets. This paper addresses this gap by proposing a comprehensive framework for evaluating machine learning models designed for fast identification of contingency cases. We first discuss the key characteristics of contingencies that need to be captured by machine learning models. Next, we present a systematic review of existing evaluation methods used in similar studies and identify their strengths and limitations. Then, we propose new evaluation metrics specifically tailored for measuring model performance on identifying contingency cases. Finally, we provide case studies using real-world data sets from different domains to demonstrate the effectiveness of our proposed framework. Our results show significant improvement over state-of-the-art methods, providing valuable insights into how machine learning can effectively detect and mitigate unexpected events or scenarios that could lead to losses or negative consequences for individuals, organizations, or society at large. Overall, our work provides important guidelines for researchers and practitioners working in areas where timely detection of contingencies is critical.",1
"Non-parametric supervised learning algorithms represent a succinct class of supervised learning algorithms where the learning parameters are highly flexible and whose values are directly dependent on the size of the training data. In this paper, we comparatively study the properties of four nonparametric algorithms, K-Nearest Neighbours (KNNs), Support Vector Machines (SVMs), Decision trees and Random forests. The supervised learning task is a regression estimate of the time-lapse in medical insurance reimbursement. Our study is concerned precisely with how well each of the nonparametric regression models fits the training data. We quantify the goodness of fit using the R-squared metric. The results are presented with a focus on the effect of the size of the training data, the feature space dimension and hyperparameter optimization.",0
"This paper uses a non-parametric regression model to estimate the time-lag between physician visits and medical insurance reimbursements. We use a large sample of claims data from commercial insurers, Medicare Advantage plans, and Medicaid managed care plans, to explore how reimbursements vary by insurer type, healthcare service categories, and geographic region. Our results suggest that there is significant variation in reimbursement times across these dimensions, with some patients waiting up to two years or more for payment. Furthermore, we find evidence of disparities in reimbursement times based on patient race/ethnicity and socioeconomic status. Overall, our study highlights the need for greater transparency and fairness in medical insurance reimbursement processes.",1
"Artificial intelligence has been applied in wildfire science and management since the 1990s, with early applications including neural networks and expert systems. Since then the field has rapidly progressed congruently with the wide adoption of machine learning (ML) in the environmental sciences. Here, we present a scoping review of ML in wildfire science and management. Our objective is to improve awareness of ML among wildfire scientists and managers, as well as illustrate the challenging range of problems in wildfire science available to data scientists. We first present an overview of popular ML approaches used in wildfire science to date, and then review their use in wildfire science within six problem domains: 1) fuels characterization, fire detection, and mapping; 2) fire weather and climate change; 3) fire occurrence, susceptibility, and risk; 4) fire behavior prediction; 5) fire effects; and 6) fire management. We also discuss the advantages and limitations of various ML approaches and identify opportunities for future advances in wildfire science and management within a data science context. We identified 298 relevant publications, where the most frequently used ML methods included random forests, MaxEnt, artificial neural networks, decision trees, support vector machines, and genetic algorithms. There exists opportunities to apply more current ML methods (e.g., deep learning and agent based learning) in wildfire science. However, despite the ability of ML models to learn on their own, expertise in wildfire science is necessary to ensure realistic modelling of fire processes across multiple scales, while the complexity of some ML methods requires sophisticated knowledge for their application. Finally, we stress that the wildfire research and management community plays an active role in providing relevant, high quality data for use by practitioners of ML methods.",0
"This is a comprehensive survey of artificial intelligence techniques and their application to problems in wildfire science and management. We focus on recent developments in deep learning algorithms which have proven highly successful in image analysis tasks and can provide detailed spatial estimates even from coarse or imprecise data inputs such as remote sensing images (e.g. Landsat 8). Our methods allow the creation of geospatially explicit maps at fine scales that can capture sub-pixel variability and represent uncertainty quantitatively. These new capabilities open up substantial opportunities for improving our understanding of fire regimes and fire effects across large areas while providing key inputs for planning landscape restoration and conservation strategies over multiple decades. Our results are presented as examples using case studies drawn from across the western United States including California but also national parks like Yellowstone. The methods we describe should prove valuable for land managers dealing with issues of burn severity, post-fire soil erosion risk assessment, and vegetation recovery patterns following fire events. Finally we consider future research needs and potential extensions of these analytical frameworks into other areas of interest for resource scientists and practitioners engaged in natural resources planning. While some promising progress has been made there remains ample room for additional advances particularly where field validation efforts need to keep pace with rapid algorithmic development. In summary, artificial intelligence holds out great promise for answering pressing questions related to wildfire impacts on ecosystem services and biodiversity. Further work is required however before these exciting innovations are ready fo",1
"Brain-computer interface (BCI) aims to establish and improve human and computer interactions. There has been an increasing interest in designing new hardware devices to facilitate the collection of brain signals through various technologies, such as wet and dry electroencephalogram (EEG) and functional near-infrared spectroscopy (fNIRS) devices. The promising results of machine learning methods have attracted researchers to apply these methods to their data. However, some methods can be overlooked simply due to their inferior performance against a particular dataset. This paper shows how relatively simple yet powerful feature selection/ranking methods can be applied to speech imagery datasets and generate significant results. To do so, we introduce two approaches, horizontal and vertical settings, to use any feature selection and ranking methods to speech imagery BCI datasets. Our primary goal is to improve the resulting classification accuracies from support vector machines, $k$-nearest neighbour, decision tree, linear discriminant analysis and long short-term memory recurrent neural network classifiers. Our experimental results show that using a small subset of channels, we can retain and, in most cases, improve the resulting classification accuracies regardless of the classifier.",0
"Title: ""A Comprehensive Review of Feature Selection Techniques for Speech Imagery Brain-Computer Interface (BCI) Datasets""  This paper presents a comprehensive review of feature selection methods applied to speech imagery Brain-Computer Interface (BCI) datasets. In recent years, there has been significant interest in developing BCIs that allow individuals to communicate using only their thoughts. Speech imagery BCIs utilize neural signals from the brain during the mental creation of speech sounds to control devices such as computers, communication systems, or assistive technologies. However, the complex nature of these signals presents numerous challenges in terms of analyzing and interpreting them accurately.  Feature selection plays a crucial role in improving the performance of BCI systems by identifying informative features from large amounts of data collected during experiments. Various techniques have been proposed to address this problem, including statistical measures, filtering approaches, embedded methods, and wrapper/redundancy methods. This study seeks to provide a comparative analysis of these methods on different speech imagery dataset sets available in literature.  The results indicate that while different approaches show varying degrees of success across multiple benchmark datasets, no single method consistently outperforms others in all cases. Furthermore, we identify some technical aspects of feature selection that require more attention within future research, including parameter optimization strategies, evaluation metrics, and algorithm scalability. We conclude our findings suggest a holistic approach combining complementary techniques may offer greater flexibility, adaptability, and robustness in the development of efficient speech imagery BCIs. Our work serves as a valuable resource for the scientific community, providing insights into the state-of-the-art techniques available and opportunities for further exploration in enhancing the functionality of speech imagery BCIs through effective feature selection.",1
"Decision trees with binary splits are popularly constructed using Classification and Regression Trees (CART) methodology. For binary classification and regression models, this approach recursively divides the data into two near-homogenous daughter nodes according to a split point that maximizes the reduction in sum of squares error (the impurity) along a particular variable. This paper aims to study the bias and adaptive properties of regression trees constructed with CART. In doing so, we derive an interesting connection between the bias and the mean decrease in impurity (MDI) measure of variable importance---a tool widely used for model interpretability---defined as the sum of impurity reductions over all non-terminal nodes in the tree. In particular, we show that the probability content of a terminal subnode for a variable is small when the MDI for that variable is large and that this relationship is exponential---confirming theoretically that decision trees with CART have small bias and are adaptive to signal strength and direction. Finally, we apply these individual tree bounds to tree ensembles and show consistency of Breiman's random forests. The context is surprisingly general and applies to a wide variety of multivariable data generating distributions and regression functions. The main technical tool is an exact characterization of the conditional probability content of the daughter nodes arising from an optimal split, in terms of the partial dependence function and reduction in impurity.",0
"This paper presents an analysis of Collaborative Assistant by Randomization and Test (CART) as a method for model selection and visualizing high dimensional data sets. In particular, we focus on the effectiveness of CART in identifying important relationships in large datasets that may be difficult to identify using traditional approaches such as linear regression or other statistical methods. We examine how the use of randomization techniques allows for greater flexibility in exploring the relationship between variables while still providing rigorous testing to ensure robust results. Additionally, we explore the benefits and limitations of using visual representations to communicate complex findings from CART models. Our analysis provides insights into the potential uses and applications of CART in diverse fields including social sciences, engineering, healthcare, and finance. Ultimately, our work contributes to the understanding of the strengths and weaknesses of CART as a tool for data exploration and model building, enabling researchers to make informed decisions when choosing appropriate analytical techniques for their specific problems.",1
"In this paper, we propose an easily trained yet powerful representation learning approach with performance highly competitive to deep neural networks in a digital pathology image segmentation task. The method, called sparse coding driven deep decision tree ensembles that we abbreviate as ScD2TE, provides a new perspective on representation learning. We explore the possibility of stacking several layers based on non-differentiable pairwise modules and generate a densely concatenated architecture holding the characteristics of feature map reuse and end-to-end dense learning. Under this architecture, fast convolutional sparse coding is used to extract multi-level features from the output of each layer. In this way, rich image appearance models together with more contextual information are integrated by learning a series of decision tree ensembles. The appearance and the high-level context features of all the previous layers are seamlessly combined by concatenating them to feed-forward as input, which in turn makes the outputs of subsequent layers more accurate and the whole model efficient to train. Compared with deep neural networks, our proposed ScD2TE does not require back-propagation computation and depends on less hyper-parameters. ScD2TE is able to achieve a fast end-to-end pixel-wise training in a layer-wise manner. We demonstrated the superiority of our segmentation technique by evaluating it on the multi-disease state and multi-organ dataset where consistently higher performances were obtained for comparison against several state-of-the-art deep learning methods such as convolutional neural networks (CNN), fully convolutional networks (FCN), etc.",0
"In recent years, digital pathology has become increasingly important in medical diagnosis and treatment planning. One critical task in digital pathology is nuclear segmentation, which involves identifying and delineating cells within histological images. However, accurate nuclear segmentation remains challenging due to variations in staining intensity, cell density, and image quality across different patient samples. To address these issues, we propose using sparse coding driven deep decision tree ensembles (SCDDTE) for nuclei segmentation in digital pathology images. Our approach leverages the power of deep learning while retaining interpretability and model transparency. We first train a convolutional neural network (CNN) on synthetic data generated from simulated hematoxylin and eosin (H&E) stained slides using real microscopic images as texture templates. This allows us to learn features that generalize well to unseen datasets. Next, we use these learned CNN features along with traditional handcrafted features as input for our decision tree ensemble model. By combining both types of features, we can leverage complementary strengths while minimizing weaknesses. Finally, we apply Monte Carlo Dropout technique during testing to regularize the model and obtain more robust predictions. Experimental results on two publicly available benchmark datasets demonstrate significant improvements over state-of-the-art methods in terms of accuracy, precision, recall, and F1 score metrics. Our proposed method provides valuable insights into efficient nuclear segmentation techniques, enabling more reliable disease assessments and improved clinical outcomes.",1
"Decision tree optimization is notoriously difficult from a computational perspective but essential for the field of interpretable machine learning. Despite efforts over the past 40 years, only recently have optimization breakthroughs been made that have allowed practical algorithms to find optimal decision trees. These new techniques have the potential to trigger a paradigm shift where it is possible to construct sparse decision trees to efficiently optimize a variety of objective functions without relying on greedy splitting and pruning heuristics that often lead to suboptimal solutions. The contribution in this work is to provide a general framework for decision tree optimization that addresses the two significant open problems in the area: treatment of imbalanced data and fully optimizing over continuous variables. We present techniques that produce optimal decision trees over a variety of objectives including F-score, AUC, and partial area under the ROC convex hull. We also introduce a scalable algorithm that produces provably optimal results in the presence of continuous variables and speeds up decision tree construction by several orders of magnitude relative to the state-of-the art.",0
"This abstract describes a novel algorithm called Sparse Tree that combines two powerful concepts from machine learning: decision trees (DTs) as a building block and sparse regression as a training technique for constructing DTs. We argue that both ideas complement each other and can significantly improve prediction accuracy on large datasets, leading to more efficient models that generalize well on unseen data. Our approach exploits recent advances in gradient-based optimization techniques tailored for linear model selection problems. In practice, we observe improved performance against state-of-the-art methods across multiple real-world benchmarks. Overall, our work has important implications for many applications where scalability and predictive power matter, including natural language processing, computer vision, recommendation systems, and medical diagnosis.",1
"A salient approach to interpretable machine learning is to restrict modeling to simple models. In the Bayesian framework, this can be pursued by restricting the model structure and prior to favor interpretable models. Fundamentally, however, interpretability is about users' preferences, not the data generation mechanism; it is more natural to formulate interpretability as a utility function. In this work, we propose an interpretability utility, which explicates the trade-off between explanation fidelity and interpretability in the Bayesian framework. The method consists of two steps. First, a reference model, possibly a black-box Bayesian predictive model which does not compromise accuracy, is fitted to the training data. Second, a proxy model from an interpretable model family that best mimics the predictive behaviour of the reference model is found by optimizing the interpretability utility function. The approach is model agnostic -- neither the interpretable model nor the reference model are restricted to a certain class of models -- and the optimization problem can be solved using standard tools. Through experiments on real-word data sets, using decision trees as interpretable models and Bayesian additive regression models as reference models, we show that for the same level of interpretability, our approach generates more accurate models than the alternative of restricting the prior. We also propose a systematic way to measure stability of interpretabile models constructed by different interpretability approaches and show that our proposed approach generates more stable models.",0
"A novel approach is proposed for enhancing model interpretability within the framework of Bayesian decision theory. This method builds upon traditional techniques used for uncertainty quantification and risk assessment, by explicitly incorporating human judgment into the process through the use of subjective probability distributions. The resulting decision-theoretic approach enables more informed decisions that better align with real-world goals and constraints while still respecting fundamental principles of probabilistic inference. Simulation results demonstrate significant improvements over standard methods, particularly in complex systems with large uncertainties and disparate objectives. Overall, the new approach provides a powerful toolkit for practitioners seeking deeper insights into their models and greater control over their outcomes.",1
"For supervised classification problems involving design, control, other practical purposes, users are not only interested in finding a highly accurate classifier, but they also demand that the obtained classifier be easily interpretable. While the definition of interpretability of a classifier can vary from case to case, here, by a humanly interpretable classifier we restrict it to be expressed in simplistic mathematical terms. As a novel approach, we represent a classifier as an assembly of simple mathematical rules using a non-linear decision tree (NLDT). Each conditional (non-terminal) node of the tree represents a non-linear mathematical rule (split-rule) involving features in order to partition the dataset in the given conditional node into two non-overlapping subsets. This partitioning is intended to minimize the impurity of the resulting child nodes. By restricting the structure of split-rule at each conditional node and depth of the decision tree, the interpretability of the classifier is assured. The non-linear split-rule at a given conditional node is obtained using an evolutionary bilevel optimization algorithm, in which while the upper-level focuses on arriving at an interpretable structure of the split-rule, the lower-level achieves the most appropriate weights (coefficients) of individual constituents of the rule to minimize the net impurity of two resulting child nodes. The performance of the proposed algorithm is demonstrated on a number of controlled test problems, existing benchmark problems, and industrial problems. Results on two to 500-feature problems are encouraging and open up further scopes of applying the proposed approach to more challenging and complex classification tasks.",0
"Title: Bilevel Optimization for Interpretable Rule Discovery in Nonlinear Decision Tree Classifiers  This paper presents a new approach to interpreting decision rules learned by nonlinear decision trees using bilevel optimization techniques. Many machine learning algorithms have been developed that can automatically learn complex models from large datasets, but these models often lack interpretability, which is crucial in many applications such as medical diagnosis, financial fraud detection, and legal analysis. Existing rule extraction methods rely on heuristics and may generate suboptimal solutions, or they only work well under specific conditions, while others tend to produce unstable results due to noise. In contrast, our method optimizes decision rules directly through a bilevel programming problem involving upper-level maximization over tree structures and lower-level minimization for individual instances. This leads to interpretable and stable rules even for high-dimensional data with noisy labels or irrelevant features. Experiments on several real-world datasets demonstrate that our method outperforms state-of-the-art approaches for finding both simple and complex rules while maintaining low error rates. Overall, we provide a novel framework for discovering interpretable decision rules that could enable domain experts to better understand and trust machine learning predictions, ultimately leading to more informed decisions in critical domains.",1
"Palm vein identification (PVI) is a modern biometric security technique used for increasing security and authentication systems. The key characteristics of palm vein patterns include, its uniqueness to each individual, unforgettable, non-intrusive and cannot be taken by an unauthorized person. However, the extracted features from the palm vein pattern are huge with high redundancy. In this paper, we propose a combine model of two-Dimensional Discrete Wavelet Transform, Principal Component Analysis (PCA), and Particle Swarm Optimization (PSO) (2D-DWTPP) to enhance prediction of vein palm patterns. The 2D-DWT Extracts features from palm vein images, PCA reduces the redundancy in palm vein features. The system has been trained in selecting high reverent features based on the wrapper model. The PSO feeds wrapper model by an optimal subset of features. The proposed system uses four classifiers as an objective function to determine VPI which include Support Vector Machine (SVM), K Nearest Neighbor (KNN), Decision Tree (DT) and Na\""ive Bayes (NB). The empirical result shows the proposed system Iit satisfied best results with SVM. The proposed 2D-DWTPP model has been evaluated and the results shown remarkable efficiency in comparison with Alexnet and classifier without feature selection. Experimentally, our model has better accuracy reflected by (98.65) while Alexnet has (63.5) and applied classifier without feature selection has (78.79).",0
"An increasingly common technique used today in security systems is palm vein identification. This system uses near infrared light to detect patterns in the blood vessels located inside a person's hand. By analyzing these patterns, the system can identify individuals with high accuracy. However, the quality of images obtained from different capture devices varies widely and may lead to poor recognition results. Therefore, this study proposes a new approach called a Hybrid Features Selection Model (HFSM) which improves recognition performance by selecting appropriate image feature sets for specific device types. Our experiments show that HFSM consistently outperforms other methods with higher levels of precision and recall while maintaining low equal error rates across all test datasets. These findings suggest that HFSM is well suited as a standardized framework to improve overall palm vein authentication systems.",1
"Autonomous detection and classification of objects are admired area of research in many industrial applications. Though, humans can distinguish objects with high multi-granular similarities very easily; but for the machines, it is a very challenging task. The convolution neural networks (CNN) have illustrated efficient performance in multi-level representations of objects for classification. Conventionally, the existing deep learning models utilize the transformed features generated by the rearmost layer for training and testing. However, it is evident that this does not work well with multi-granular data, especially, in presence of deceptive similar classes (almost similar but different classes). The objective of the present research is to address the challenge of classification of deceptively similar multi-granular objects with an ensemble approach thfat utilizes activations from multiple layers of CNN (deep features). These multi-layer activations are further utilized to build multiple deep decision trees (known as Random forest) for classification of objects with similar appearance. The Fruits-360 dataset is utilized for evaluation of the proposed approach. With extensive trials it was observed that the proposed model outperformed over the conventional deep learning approaches.",0
"In this paper we describe a method for classifying fruit using neural networks that are trained on large datasets of labeled images from diverse sources. We show how these models can generalize well to new data, even across different varieties of fruits such as apples and bananas. Our key contribution lies in our novel approach to handling challenges posed by the significant intra-class variation present in some fruits: rather than ignoring this variation by pooling all examples of each fruit class into a single representation, we leverage recent advances in computer vision literature to learn highly discriminative feature representations at multiple scales and layers. This allows us to capture subtle differences between subclasses like red vs green apples or ripe vs unripe bananas, which would have otherwise been overlooked. Additionally, we demonstrate improved performance compared to related work using fine-grained annotation techniques that allow us to label nuanced distinctions within fruit categories. Overall, we believe our approach represents a step forward in state-of-the-art computational methods for object recognition and categorization tasks and could lead to more accurate and efficient automation solutions in agriculture, grocery supply chains, and other domains where fruit identification is essential.",1
"Decision Trees and Random Forests are among the most widely used machine learning models, and often achieve state-of-the-art performance in tabular, domain-agnostic datasets. Nonetheless, being primarily discriminative models they lack principled methods to manipulate the uncertainty of predictions. In this paper, we exploit Generative Forests (GeFs), a recent class of deep probabilistic models that addresses these issues by extending Random Forests to generative models representing the full joint distribution over the feature space. We demonstrate that GeFs are uncertainty-aware classifiers, capable of measuring the robustness of each prediction as well as detecting out-of-distribution samples.",0
"In recent years, deep learning techniques have become increasingly popular in the field of computer vision due to their ability to achieve state-of-the-art performance on a wide range of tasks. However, despite these impressive results, there remains a need for methods that can provide more robustness to overfitting and other types of model instability. To address this challenge, we propose the use of deep generative forests (DGFs), which combine the strengths of both ensemble learning and deep generative models. Our approach leverages the power of randomization provided by bagging while incorporating strong regularizers through adversarial training. We demonstrate the effectiveness of our method across several benchmark datasets and show that DGFs can outperform traditional approaches such as convolutional neural networks (CNNs) and decision trees, even when trained under limited data conditions. Overall, our work contributes towards enabling more reliable and consistent performance in complex real-world scenarios where data quality may vary significantly.",1
"Neural networks and tree ensembles are state-of-the-art learners, each with its unique statistical and computational advantages. We aim to combine these advantages by introducing a new layer for neural networks, composed of an ensemble of differentiable decision trees (a.k.a. soft trees). While differentiable trees demonstrate promising results in the literature, they are typically slow in training and inference as they do not support conditional computation. We mitigate this issue by introducing a new sparse activation function for sample routing, and implement true conditional computation by developing specialized forward and backward propagation algorithms that exploit sparsity. Our efficient algorithms pave the way for jointly training over deep and wide tree ensembles using first-order methods (e.g., SGD). Experiments on 23 classification datasets indicate over 10x speed-ups compared to the differentiable trees used in the literature and over 20x reduction in the number of parameters compared to gradient boosted trees, while maintaining competitive performance. Moreover, experiments on CIFAR, MNIST, and Fashion MNIST indicate that replacing dense layers in CNNs with our tree layer reduces the test loss by 7-53% and the number of parameters by 8x. We provide an open-source TensorFlow implementation with a Keras API.",0
"Recently we have seen tremendous success on applying deep neural networks, such as random forest models, to tasks ranging from image classification to natural language understanding. However these models come at high computational cost which can lead to difficulties when deployed on mobile devices with limited memory and energy capacity. We introduce the tree ensemble layer (TEL) in our work that has better performance than previous approaches while having lesser compute requirements. Our model uses differentiable conditional computation within trees enabling us to end-to-end train large ensembles of decision trees for complex tasks without relying on heuristics like pruning, bagging etc.. In addition TEL provides principled uncertainty estimates by propagating confidence measures directly through each node in the tree structure using dropout based approximations similar to those used in Bayesian deep learning methods making deployment faster. This methodology achieves state of art results across several benchmark datasets with fewer parameters and fewer computations compared to previous approaches. Our code is open source under Apache License 2.0 and available online for community usage . Experiments indicate significant performance improvements over existing works with more efficient computation requirements. For example we show 6% higher accuracy over fastai baseline on CIFAR10 dataset which translates into around 34 million fewer multiply add operations in the network during inference! Furthermore our implementation runs 7x faster than Hugging Face transformers with comparable accuracy which leads to improved efficiency metrics required to deploy NLP models on edge devices like smartphones efficiently . With these experiments we hope researchers find this new approach encouraging especially given its ability to improve training times",1
"Gradient boosting methods based on Structured Categorical Decision Trees (SCDT) have been demonstrated to outperform numerical and one-hot-encodings on problems where the categorical variable has a known underlying structure. However, the enumeration procedure in the SCDT is infeasible except for categorical variables with low or moderate cardinality. We propose and implement two methods to overcome the computational obstacles and efficiently perform Gradient Boosting on complex structured categorical variables. The resulting package, called StructureBoost, is shown to outperform established packages such as CatBoost and LightGBM on problems with categorical predictors that contain sophisticated structure. Moreover, we demonstrate that StructureBoost can make accurate predictions on unseen categorical values due to its knowledge of the underlying structure.",0
"Outstanding performance can often depend on incorporating domain knowledge into models. However, training machine learning models that capture complex dependencies among structured categorical variables is difficult. This paper proposes a novel approach called ""StructureBoost"" which uses gradient boosting to effectively model dependencies among structured categorical variables. Our method captures both local and global interactions among features and their categories, and is applicable to binary as well as multi-class classification tasks. We demonstrate StructureBoost's effectiveness on several benchmark datasets through experiments. Compared to state-of-the art methods such as random forests and neural networks, our model achieves substantially better accuracy across all dataset types (binary/multi-class) while offering more efficient computation time and improved interpretability thanks to feature selection. Therefore, we believe that StructureBoost offers significant advantages over existing approaches for many real world applications requiring high quality predictions on structured data.",1
"Machine learning has proved invaluable for a range of different tasks, yet it also proved vulnerable to evasion attacks, i.e., maliciously crafted perturbations of input data designed to force mispredictions. In this paper we propose a novel technique to verify the security of decision tree models against evasion attacks with respect to an expressive threat model, where the attacker can be represented by an arbitrary imperative program. Our approach exploits the interpretability property of decision trees to transform them into imperative programs, which are amenable for traditional program analysis techniques. By leveraging the abstract interpretation framework, we are able to soundly verify the security guarantees of decision tree models trained over publicly available datasets. Our experiments show that our technique is both precise and efficient, yielding only a minimal number of false positives and scaling up to cases which are intractable for a competitor approach.",0
"In modern computer systems, decision trees play a critical role in detecting attacks on networks and devices, making them essential components in cybersecurity. However, these systems can become vulnerable if attackers use evasion techniques to evade detection. To address this issue, researchers have developed program analysis methods that can certify decision tree models against such attacks. This work presents a novel approach to model certification that utilizes program analysis to verify decision trees under different inputs, ensuring their robustness against evasion attempts. By providing formal guarantees of security, our method enables decision tree-based intrusion detection systems (IDS) to better safeguard against evolving threats. Our experiments demonstrate the effectiveness of our approach, showing improved detection rates compared to existing state-of-the-art IDS. Overall, this work represents a significant step forward in enhancing cybersecurity by providing new tools to protect against advanced persistent threats (APTs).",1
"Current deep learning models are mostly build upon neural networks, i.e., multiple layers of parameterized differentiable nonlinear modules that can be trained by backpropagation. In this paper, we explore the possibility of building deep models based on non-differentiable modules. We conjecture that the mystery behind the success of deep neural networks owes much to three characteristics, i.e., layer-by-layer processing, in-model feature transformation and sufficient model complexity. We propose the gcForest approach, which generates \textit{deep forest} holding these characteristics. This is a decision tree ensemble approach, with much less hyper-parameters than deep neural networks, and its model complexity can be automatically determined in a data-dependent way. Experiments show that its performance is quite robust to hyper-parameter settings, such that in most cases, even across different data from different domains, it is able to get excellent performance by using the same default setting. This study opens the door of deep learning based on non-differentiable modules, and exhibits the possibility of constructing deep models without using backpropagation.",0
"In recent years, deep learning has become increasingly popular due to its ability to achieve state-of-the-art results on a variety of tasks such as image recognition, natural language processing, and speech recognition. However, training deep neural networks can require vast amounts of computational resources and data, making it difficult for researchers who lack access to large computing clusters or expensive datasets. One solution to this problem is to use techniques that enable smaller models to perform better without relying solely on bigger sizes or more data. In our paper ""Deep Forest,"" we propose a new method called ""Forest Distillation"" which addresses this challenge by distilling knowledge from ensembles of trees into single decision boundaries. Our approach effectively reduces model size while retaining high accuracy. We demonstrate the effectiveness of our method through extensive experiments on benchmark datasets, showing that forest distillation significantly improves over baseline methods. Overall, our work advances the field of machine learning, providing researchers with a powerful tool to train small but accurate deep forest models using limited resources.",1
"We study the spatio-temporal prediction problem, which has attracted the attention of many researchers due to its critical real-life applications. In particular, we introduce a novel approach to this problem. Our approach is based on the Hawkes process, which is a non-stationary and self-exciting point process. We extend the formulations of a standard point process model that can represent time-series data to represent a spatio-temporal data. We model the data as nonstationary in time and space. Furthermore, we partition the spatial region we are working on into subregions via an adaptive decision tree and model the source statistics in each subregion with individual but mutually interacting point processes. We also provide a gradient based joint optimization algorithm for the point process and decision tree parameters. Thus, we introduce a model that can jointly infer the source statistics and an adaptive partitioning of the spatial region. Finally, we provide experimental results on real-life data, which provides significant improvement due to space adaptation and joint optimization compared to standard well-known methods in the literature.",0
"In recent years, spatio-temporal point processes have gained increasing attention as a powerful tool for predicting events that occur at specific locations over time. These models capture important spatial dependencies between events and can account for nonstationarity, allowing them to adapt to changing patterns over time. One challenge faced by many researchers using these methods is finding effective ways to integrate prior knowledge or domain expertise into their predictions, which may improve accuracy but has been difficult due to the complex nature of the underlying models. This study presents an innovative approach combining self organizing decision trees (SODTs) with spatio-temporal point process models for prediction, addressing this challenge while providing flexibility to incorporate different types of data and features. By leveraging SODTs ability to identify interpretable patterns from large datasets, we can better understand the relationships between variables and make more informed decisions, resulting in improved predictions across diverse applications. Our results demonstrate significant improvements compared to state-of-the-art models for two real-world case studies, highlighting the effectiveness of our proposed methodology.",1
"In many application settings, the data have missing entries which make analysis challenging. An abundant literature addresses missing values in an inferential framework: estimating parameters and their variance from incomplete tables. Here, we consider supervised-learning settings: predicting a target when missing values appear in both training and testing data. We show the consistency of two approaches in prediction. A striking result is that the widely-used method of imputing with a constant, such as the mean prior to learning is consistent when missing values are not informative. This contrasts with inferential settings where mean imputation is pointed at for distorting the distribution of the data. That such a simple approach can be consistent is important in practice. We also show that a predictor suited for complete observations can predict optimally on incomplete data,through multiple imputation.Finally, to compare imputation with learning directly with a model that accounts for missing values, we analyze further decision trees. These can naturally tackle empirical risk minimization with missing values, due to their ability to handle the half-discrete nature of incomplete variables. After comparing theoretically and empirically different missing values strategies in trees, we recommend using the ""missing incorporated in attribute"" method as it can handle both non-informative and informative missing values.",0
"In many real world applications, data sets often contain missing values which can cause inconsistencies in results obtained from different machine learning models. Supervised learning with missing values has been a topic of interest among researchers but until now there is no theoretical analysis on the issue. This paper presents the first study that provides a comprehensive examination into whether consistent learning is possible given incomplete datasets by utilizing current state-of-the-art techniques and evaluates their performance under such circumstances. Results showcase how certain algorithms perform poorly while others excel in terms of stability and accuracy. Overall this work serves as a foundation for future research in the area of handling missing data during model training.",1
"Despite the popularity of explainable AI, there is limited work on effective methods for unsupervised learning. We study algorithms for $k$-means clustering, focusing on a trade-off between explainability and accuracy. Following prior work, we use a small decision tree to partition a dataset into $k$ clusters. This enables us to explain each cluster assignment by a short sequence of single-feature thresholds. While larger trees produce more accurate clusterings, they also require more complex explanations. To allow flexibility, we develop a new explainable $k$-means clustering algorithm, ExKMC, that takes an additional parameter $k' \geq k$ and outputs a decision tree with $k'$ leaves. We use a new surrogate cost to efficiently expand the tree and to label the leaves with one of $k$ clusters. We prove that as $k'$ increases, the surrogate cost is non-increasing, and hence, we trade explainability for accuracy. Empirically, we validate that ExKMC produces a low cost clustering, outperforming both standard decision tree methods and other algorithms for explainable clustering. Implementation of ExKMC available at https://github.com/navefr/ExKMC.",0
"This paper presents ExKMC (Expanding k-means clustering), which expands the explainability of traditional k-means clustering by introducing several new components that improve both interpretability and performance. Firstly, ExKMC uses a more advanced initialization method that improves accuracy and stability compared to previous methods. Additionally, it includes various interactive tools such as visualization and data drill down capabilities, allowing users to explore the clusters and their characteristics in detail. By combining these features into one platform, we aim to provide researchers and analysts with greater insights into their datasets while streamlining the entire clustering workflow process. Our experimental results demonstrate the effectiveness of ExKMC and showcase how it can significantly improve clustering interpretability over existing approaches. Overall, our work bridges the gap between technical advancements in clustering algorithms and user-friendly tool development to further advance the field of unsupervised learning.",1
"Corrosion is a major problem affecting the durability of reinforced concrete structures. Corrosion related maintenance and repair of reinforced concrete structures cost multibillion USD per annum globally. It is often triggered by the ingression of carbon dioxide and/or chloride into the pores of concrete. Estimation of these corrosion causing factors using the conventional models results in suboptimal assessment since they are incapable of capturing the complex interaction of parameters. Hygrothermal interaction also plays a role in aggravating the corrosion of reinforcement bar and this is usually counteracted by applying surface-protection systems. These systems have different degree of protection and they may even cause deterioration to the structure unintentionally. The overall objective of this dissertation is to provide a framework that enhances the assessment reliability of the corrosion controlling factors. The framework is realized through the development of data-driven carbonation depth, chloride profile and hygrothermal performance prediction models. The carbonation depth prediction model integrates neural network, decision tree, boosted and bagged ensemble decision trees. The ensemble tree based chloride profile prediction models evaluate the significance of chloride ingress controlling variables from various perspectives. The hygrothermal interaction prediction models are developed using neural networks to evaluate the status of corrosion and other unexpected deteriorations in surface-treated concrete elements. Long-term data for all models were obtained from three different field experiments. The performance comparison of the developed carbonation depth prediction model with the conventional one confirmed the prediction superiority of the data-driven model. The variable ...",0
"""Assessing corrosion damage in reinforced concrete structures can be challenging due to limitations in conventional methods such as visual inspections and electrochemical measurements. This study proposes a data-driven approach that utilizes machine learning techniques to enhance corrosion assessment accuracy and overcome these limitations. Utilizing sensors embedded within structural elements, large datasets of sensor readings were collected on real bridges exposed to different environmental conditions over time. These datasets were analyzed using advanced signal processing algorithms, resulting in improved detection capabilities for corrosion initiation and progression. Furthermore, statistical models developed from historical data can improve predictions of future corrosion patterns, allowing timely maintenance interventions and reducing infrastructure repair costs. The proposed method has been validated through extensive testing under realistic scenarios and demonstrates significant improvements compared to current practices.""",1
"Decision trees are a popular family of models due to their attractive properties such as interpretability and ability to handle heterogeneous data. Concurrently, missing data is a prevalent occurrence that hinders performance of machine learning models. As such, handling missing data in decision trees is a well studied problem. In this paper, we tackle this problem by taking a probabilistic approach. At deployment time, we use tractable density estimators to compute the ""expected prediction"" of our models. At learning time, we fine-tune parameters of already learned trees by minimizing their ""expected prediction loss"" w.r.t.\ our density estimators. We provide brief experiments showcasing effectiveness of our methods compared to few baselines.",0
"When dealing with missing data in decision trees, traditional methods often involve either removing observations with missing values or imputing them using arbitrary techniques. These approaches can lead to biased results and suboptimal tree structures. This study proposes a probabilistic approach that incorporates uncertainty from missing data into the feature selection process, improving model accuracy and interpretability.  The proposed method uses Bayesian inference to model the conditional distribution of each target variable given the predictor variables, even if some of those variables have missing values. This allows us to estimate the probability of the target variable taking on certain values given the observed inputs, as well as the uncertainties associated with these estimates. In turn, we use these probabilities to construct ensemble-based trees that account for both the uncertainty due to randomness and the variability introduced by missing data.  To evaluate our method, we conduct simulations and compare our approach against several state-of-the-art alternatives, including single-tree based methods such as mean imputation, multiple imputation, and missing completely at random (MCAR) assumption, as well as ensemble-based models like Random Forests and Extremely Randomized Trees. Our experimental results show that the proposed method consistently outperforms other benchmarks across different datasets and evaluation metrics.  In conclusion, the proposed probabilistic framework represents a promising alternative for handling missing data in decision trees. By capturing both the aleatoric and epistemic uncertainties introduced by missing values, we can obtain more accurate predictions while providing better insights into the underlying relationships among variables. Overall, our findings highlight the potential value of integrating Bayesian reasoning into ensemble learning, opening new opportunities for advancing machine learning research.",1
"Decision trees are ubiquitous in machine learning for their ease of use and interpretability. Yet, these models are not typically employed in reinforcement learning as they cannot be updated online via stochastic gradient descent. We overcome this limitation by allowing for a gradient update over the entire tree that improves sample complexity affords interpretable policy extraction. First, we include theoretical motivation on the need for policy-gradient learning by examining the properties of gradient descent over differentiable decision trees. Second, we demonstrate that our approach equals or outperforms a neural network on all domains and can learn discrete decision trees online with average rewards up to 7x higher than a batch-trained decision tree. Third, we conduct a user study to quantify the interpretability of a decision tree, rule list, and a neural network with statistically significant results ($p  0.001$).",0
"This paper presents novel optimization methods for designing interpretable differentiable decision trees (DDT) as function approximators for reinforcement learning (RL). Unlike traditional decision tree algorithms that rely on heuristics, our approach relies on gradient descent and backpropagation to optimize both accuracy and interpretability criteria. By using sparsity constraints, we show that our method can learn compact and meaningful DDT models that generalize well across different RL environments while providing human-readable explanations. Our experimental evaluation demonstrates that our DDT algorithm achieves state-of-the-art performance compared against other interpretable deep RL algorithms such as policy gradients with linear models and actor-critic methods based on randomized decision forests. Furthermore, our results reveal that these optimized DDT models capture diverse features from different domains and provide intuitive insights into complex behaviors learned by artificial agents, thus facilitating explainable AI research. Our contributions pave the way towards developing reliable intelligent systems that can interact effectively with humans through transparency and trustworthiness.",1
"Decision trees are a widely used method for classification, both by themselves and as the building blocks of multiple different ensemble learning methods. The Max-Cut decision tree involves novel modifications to a standard, baseline model of classification decision tree construction, precisely CART Gini. One modification involves an alternative splitting metric, maximum cut, based on maximizing the distance between all pairs of observations belonging to separate classes and separate sides of the threshold value. The other modification is to select the decision feature from a linear combination of the input features constructed using Principal Component Analysis (PCA) locally at each node. Our experiments show that this node-based localized PCA with the novel splitting modification can dramatically improve classification, while also significantly decreasing computational time compared to the baseline decision tree. Moreover, our results are most significant when evaluated on data sets with higher dimensions, or more classes; which, for the example data set CIFAR-100, enable a 49% improvement in accuracy while reducing CPU time by 94%. These introduced modifications dramatically advance the capabilities of decision trees for difficult classification tasks.",0
"This paper presents a new decision tree algorithm called the Max-Cut Decision Tree (MCDT) which significantly improves upon both the accuracy and running time of traditional decision trees. Inspired by recent advances in integer linear programming, MCDT uses a max cut formulation to recursively construct splits that maximize the number of examples correctly classified. By optimizing for accuracy rather than simplicity, MCDT produces more complex decision boundaries which often result in higher prediction accuracies across a variety of real world datasets. Unlike some other methods used to improve decision tree performance such as Random Forest, MCDT only builds one model which allows for easier interpretation and explanation of predictions. Furthermore, experiments show that MCDT has faster average training times compared to popular implementations of CART, ID3, and C4.5. Overall, this work shows that MCDT is a powerful addition to the family of decision tree algorithms with improved prediction quality at a lower computational cost.",1
"In multi-label classification, where the evaluation of predictions is less straightforward than in single-label classification, various meaningful, though different, loss functions have been proposed. Ideally, the learning algorithm should be customizable towards a specific choice of the performance measure. Modern implementations of boosting, most prominently gradient boosted decision trees, appear to be appealing from this point of view. However, they are mostly limited to single-label classification, and hence not amenable to multi-label losses unless these are label-wise decomposable. In this work, we develop a generalization of the gradient boosting framework to multi-output problems and propose an algorithm for learning multi-label classification rules that is able to minimize decomposable as well as non-decomposable loss functions. Using the well-known Hamming loss and subset 0/1 loss as representatives, we analyze the abilities and limitations of our approach on synthetic data and evaluate its predictive performance on multi-label benchmarks.",0
"Machine learning has become increasingly important as datasets have grown larger and more complex, and multi-label classification (MLC) problems require models that can effectively handle labels with overlapping features and varying degrees of relevance. One promising approach to MLC is gradient boosting, which involves iteratively building regression trees on top of each other until predictions converge. However, existing methods tend to rely heavily on hyperparameter tuning and may struggle to identify meaningful rules from noisy data. In our paper, we propose a new method called Learning Gradient Boosted Multi-Label Classification (LGBMLC) Rules that combines gradient boosting with feature selection techniques inspired by decision tree pruning. By incorporating regularization into the learning process, LGBMLC Rules reduces noise sensitivity while improving model interpretability through rule extraction algorithms. Our experimental results show significant improvements compared to state-of-the-art approaches across several benchmark datasets. Overall, LGBMLC Rules provides a powerful and flexible framework for solving MLC problems that balances accuracy, interpretability, and scalability.",1
"In this paper, we consider the problem of classification of $M$ high dimensional queries $y^1,\cdots,y^M\in B^S$ to $N$ high dimensional classes $x^1,\cdots,x^N\in A^S$ where $A$ and $B$ are discrete alphabets and the probabilistic model that relates data to the classes $P(x,y)$ is known. This problem has applications in various fields including the database search problem in mass spectrometry. The problem is analogous to the nearest neighbor search problem, where the goal is to find the data point in a database that is the most similar to a query point. The state of the art method for solving an approximate version of the nearest neighbor search problem in high dimensions is locality sensitive hashing (LSH). LSH is based on designing hash functions that map near points to the same buckets with a probability higher than random (far) points. To solve our high dimensional classification problem, we introduce distribution sensitive hashes that map jointly generated pairs $(x,y)\sim P$ to the same bucket with probability higher than random pairs $x\sim P^A$ and $y\sim P^B$, where $P^A$ and $P^B$ are the marginal probability distributions of $P$. We design distribution sensitive hashes using a forest of decision trees and we show that the complexity of search grows with $O(N^{\lambda^*(P)})$ where $\lambda^*(P)$ is expressed in an analytical form. We further show that the proposed hashes perform faster than state of the art approximate nearest neighbor search methods for a range of probability distributions, in both theory and simulations. Finally, we apply our method to the spectral library search problem in mass spectrometry, and show that it is an order of magnitude faster than the state of the art methods.",0
"Title: An Introduction to ForestDSH - A universal hash design for discrete probability distributions  ForestDSH is a new approach to constructing hash functions that are able to efficiently represent and store data from any given distribution. In particular, ForestDSH focuses on the use of randomness within each iteration step of the hashing process, which allows for improved performance compared to traditional methods such as SHA256. Through careful analysis and experimentation, we show how our method can outperform existing techniques across multiple datasets and applications. \end{code}  if you need more information please ask",1
"Fair machine learning works have been focusing on the development of equitable algorithms that address discrimination of certain groups. Yet, many of these fairness-aware approaches aim to obtain a unique solution to the problem, which leads to a poor understanding of the statistical limits of bias mitigation interventions. We present the first methodology that allows to explore those limits within a multi-objective framework that seeks to optimize any measure of accuracy and fairness and provides a Pareto front with the best feasible solutions. In this work, we focus our study on decision tree classifiers since they are widely accepted in machine learning, are easy to interpret and can deal with non-numerical information naturally. We conclude experimentally that our method can optimize decision tree models by being fairer with a small cost of the classification error. We believe that our contribution will help stakeholders of sociotechnical systems to assess how far they can go being fair and accurate, thus serving in the support of enhanced decision making where machine learning is used.",0
"This is the first time I see you write such a request! Are there other requests like that? If yes, would you please show me some examples so I could better understand your expectations?",1
"Model selection consists in comparing several candidate models according to a metric to be optimized. The process often involves a grid search, or such, and cross-validation, which can be time consuming, as well as not providing much information about the dataset itself. In this paper we propose a method to reduce the scope of exploration needed for the task. The idea is to quantify how much it would be necessary to depart from trained instances of a given family, reference models (RMs) carrying `rigid' decision boundaries (e.g. decision trees), so as to obtain an equivalent or better model. In our approach, this is realized by progressively relaxing the decision boundaries of the initial decision trees (the RMs) as long as this is beneficial in terms of performance measured on an analyzed dataset. More specifically, this relaxation is performed by making use of a neural decision tree, which is a neural network built from DTs. The final model produced by our method carries non-linear decision boundaries. Measuring the performance of the final model, and its agreement to its seeding RM can help the user to figure out on which family of models he should focus on.",0
"Title: Family Selection Using Neural Decision Trees for Classification  Model selection has been a critical component in machine learning research as it plays a crucial role in determining the accuracy and performance of any given model. In recent years, there has been growing interest in neural decision trees due to their ability to capture nonlinear relationships while maintaining interpretability. However, selecting the optimal combination of hyperparameters that govern the behavior of these models remains challenging. In this study, we focus on developing a methodology for family selection for classification tasks using neural decision trees. Our proposed approach leverages techniques from evolutionary computation to search through different parameter combinations and identify families of neural decision trees that perform well across multiple metrics. We evaluate our algorithm on several benchmark datasets and show promising results in terms of overall accuracy, precision, recall, F1 score, and mean decrease impurity compared to other popular methods such as grid search and random search. Our work highlights the potential benefits of employing evolutionary algorithms for model selection, particularly for complex models like neural decision trees. Our contributions can facilitate more efficient use of computational resources by reducing time spent on exhaustive searches, providing guidance on appropriate sets of parameters, and ultimately leading to improved generalization performance on new, unseen data.",1
"There has been recent interest in improving performance of simple models for multiple reasons such as interpretability, robust learning from small data, deployment in memory constrained settings as well as environmental considerations. In this paper, we propose a novel method SRatio that can utilize information from high performing complex models (viz. deep neural networks, boosted trees, random forests) to reweight a training dataset for a potentially low performing simple model of much lower complexity such as a decision tree or a shallow network enhancing its performance. Our method also leverages the per sample hardness estimate of the simple model which is not the case with the prior works which primarily consider the complex model's confidences/predictions and is thus conceptually novel. Moreover, we generalize and formalize the concept of attaching probes to intermediate layers of a neural network to other commonly used classifiers and incorporate this into our method. The benefit of these contributions is witnessed in the experiments where on 6 UCI datasets and CIFAR-10 we outperform competitors in a majority (16 out of 27) of the cases and tie for best performance in the remaining cases. In fact, in a couple of cases, we even approach the complex model's performance. We also conduct further experiments to validate assertions and intuitively understand why our method works. Theoretically, we motivate our approach by showing that the weighted loss minimized by simple models using our weighting upper bounds the loss of the complex model.",0
"This is an interesting paper that explores new ways to improve simple models by taking advantage of their existing knowledge base. The authors propose methods for leveraging the strengths of these simpler models while still producing more accurate results than they could on their own. By examining how to better utilize what we already know from these models, researchers can develop more effective techniques for enhanced model performance without requiring additional data or computational resources. Ultimately, this work has important implications for both theoretical understanding and practical applications of machine learning systems.",1
"The gradient boosting machine is a powerful ensemble-based machine learning method for solving regression problems. However, one of the difficulties of its using is a possible discontinuity of the regression function, which arises when regions of training data are not densely covered by training points. In order to overcome this difficulty and to reduce the computational complexity of the gradient boosting machine, we propose to apply the partially randomized trees which can be regarded as a special case of the extremely randomized trees applied to the gradient boosting. The gradient boosting machine with the partially randomized trees is illustrated by means of many numerical examples using synthetic and real data.",0
"Abstract: In recent years, gradient boosting machines (GBMs) have become increasingly popular as a flexible and powerful tool for modeling complex relationships in data. Partially randomized decision trees (PRDTs), which randomly select features at each split node, offer a simple yet effective method for constructing randomness into GBM models. By combining PRDTs with traditional GBM methods, we can create novel predictive models that combine the strengths of both approaches. We evaluate the performance of these new hybrid models on several real-world datasets using accuracy metrics commonly used in classification problems. Our results indicate that the combination of PRDTs and GBM yields improved predictive power over standard GBM algorithms while retaining computational efficiency. These findings provide evidence that incorporating randomization into GBMs can enhance their overall effectiveness without sacrificing interpretability. This research contributes to our understanding of how randomness can be effectively integrated into advanced statistical learning techniques.",1
"We consider the use of decision trees for decision-making problems under the predict-then-optimize framework. That is, we would like to first use a decision tree to predict unknown input parameters of an optimization problem, and then make decisions by solving the optimization problem using the predicted parameters. A natural loss function in this framework is to measure the suboptimality of the decisions induced by the predicted input parameters, as opposed to measuring loss using input parameter prediction error. This natural loss function is known in the literature as the Smart Predict-then-Optimize (SPO) loss, and we propose a tractable methodology called SPO Trees (SPOTs) for training decision trees under this loss. SPOTs benefit from the interpretability of decision trees, providing an interpretable segmentation of contextual features into groups with distinct optimal solutions to the optimization problem of interest. We conduct several numerical experiments on synthetic and real data including the prediction of travel times for shortest path problems and predicting click probabilities for news article recommendation. We demonstrate on these datasets that SPOTs simultaneously provide higher quality decisions and significantly lower model complexity than other machine learning approaches (e.g., CART) trained to minimize prediction error.",0
"In this paper, we propose the use of decision trees as a tool for making decisions within the predict-then-optimize framework. This framework involves using machine learning algorithms to make predictions, which can then be used to inform decision-making. We argue that decision trees are well suited to this task due to their interpretability, ease of use, and ability to handle complex relationships between variables. We present experimental results showing the effectiveness of our approach on a variety of real-world datasets and demonstrate how decision trees can improve decision quality compared to other methods. Our work has important implications for applications such as business intelligence, medical diagnosis, and financial risk assessment.",1
"Classifiers that can be implemented on chip with minimal computational and memory resources are essential for edge computing in emerging applications such as medical and IoT devices. This paper introduces a machine learning model based on oblique decision trees to enable resource-efficient classification on a neural implant. By integrating model compression with probabilistic routing and implementing cost-aware learning, our proposed model could significantly reduce the memory and hardware cost compared to state-of-the-art models, while maintaining the classification accuracy. We trained the resource-efficient oblique tree with power-efficient regularization (ResOT-PE) on three neural classification tasks to evaluate the performance, memory, and hardware requirements. On seizure detection task, we were able to reduce the model size by 3.4X and the feature extraction cost by 14.6X compared to the ensemble of boosted trees, using the intracranial EEG from 10 epilepsy patients. In a second experiment, we tested the ResOT-PE model on tremor detection for Parkinson's disease, using the local field potentials from 12 patients implanted with a deep-brain stimulation (DBS) device. We achieved a comparable classification performance as the state-of-the-art boosted tree ensemble, while reducing the model size and feature extraction cost by 10.6X and 6.8X, respectively. We also tested on a 6-class finger movement detection task using ECoG recordings from 9 subjects, reducing the model size by 17.6X and feature computation cost by 5.1X. The proposed model can enable a low-power and memory-efficient implementation of classifiers for real-time neurological disease detection and motor decoding.",0
"Title and Abstract as separate text blocks. Please make abstract longer than allowed by your guidelines, if possible, since there is no page limit on our website. You don't need to format it like a real scientific paper! Paper Title: ResOT: Resource-Efficient Oblique Trees for Neural Signal Classification Title: Resource-efficient oblique trees for neural signal classification  Abstract: In recent years, deep learning has shown tremendous success in solving complex problems across domains such as computer vision, natural language processing, speech recognition, and robotics. However, these models often require large amounts of data, computational resources, and time during training. To overcome these challenges, researchers have explored pruning techniques that reduce model size while minimizing performance degradation. Among different methods proposed so far, the use of decision tree structures combined with neural networks (neurotrees) has gained popularity due to their interpretability, efficiency, and accuracy. Nonetheless, most existing neurotree approaches rely solely on horizontal splits without considering other types of partitions, especially those with oblique cuts. This study introduces the resource-efficient oblique trees (ResOTs), which explore both horizontal and vertical splits in feature spaces, thus creating opportunities for more efficient decompositions at lower depths. Our experiments on several benchmark datasets showcase that ResOTs can achieve competitive results compared to state-of-the-art methods while significantly reducing computational costs and model sizes. These findings could lead to improved hardware scalability, faster inference times, and reduced energy consumption for neural signal classification applications.",1
"With the rising number of machine learning competitions, the world has witnessed an exciting race for the best algorithms. However, the involved data selection process may fundamentally suffer from evidence ambiguity and concept drift issues, thereby possibly leading to deleterious effects on the performance of various models. This paper proposes a new Reinforced Data Sampling (RDS) method to learn how to sample data adequately on the search for useful models and insights. We formulate the optimisation problem of model diversification $\delta{-div}$ in data sampling to maximise learning potentials and optimum allocation by injecting model diversity. This work advocates the employment of diverse base learners as value functions such as neural networks, decision trees, or logistic regressions to reinforce the selection process of data subsets with multi-modal belief. We introduce different ensemble reward mechanisms, including soft voting and stochastic choice to approximate optimal sampling policy. The evaluation conducted on four datasets evidently highlights the benefits of using RDS method over traditional sampling approaches. Our experimental results suggest that the trainable sampling for model diversification is useful for competition organisers, researchers, or even starters to pursue full potentials of various machine learning tasks such as classification and regression. The source code is available at https://github.com/probeu/RDS.",0
"An important aspect of training deep learning models is ensuring that they have sufficient diversity in their parameter space to produce robust predictions. In many cases, simply using different initializations of the model can lead to divergence in the learned parameters. However, it has been shown that randomly selecting subsets of the training data, called data augmentation, can further increase the diversity of solutions. This paper explores how sampling from these diverse solutions can be used as a form of regularization to improve model performance. The methodology presented uses a reinforcement learning algorithm to select which subset of the trained models should be sampled at each iteration during inference. Results show that this approach significantly improves both accuracy and calibration compared to standard methods on several benchmark datasets across multiple domains including computer vision, natural language processing, and time series forecasting. Overall, this work demonstrates the effectiveness of using reinforced data sampling to promote model diversification and achieve better generalization capabilities.",1
"The black-box nature of neural networks limits model decision interpretability, in particular for high-dimensional inputs in computer vision and for dense pixel prediction tasks like segmentation. To address this, prior work combines neural networks with decision trees. However, such models (1) perform poorly when compared to state-of-the-art segmentation models or (2) fail to produce decision rules with spatially-grounded semantic meaning. In this work, we build a hybrid neural-network and decision-tree model for segmentation that (1) attains neural network segmentation accuracy and (2) provides semi-automatically constructed visual decision rules such as ""Is there a window?"". We obtain semantic visual meaning by extending saliency methods to segmentation and attain accuracy by leveraging insights from neural-backed decision trees, a deep learning analog of decision trees for image classification. Our model SegNBDT attains accuracy within ~2-4% of the state-of-the-art HRNetV2 segmentation model while also retaining explainability; we achieve state-of-the-art performance for explainable models on three benchmark datasets -- Pascal-Context (49.12%), Cityscapes (79.01%), and Look Into Person (51.64%). Furthermore, user studies suggest visual decision rules are more interpretable, particularly for incorrect predictions. Code and pretrained models can be found at https://github.com/daniel-ho/SegNBDT.",0
"In recent years, deep learning methods have been widely used for image segmentation tasks due to their impressive performance on complex datasets. However, these models often require large amounts of annotated data and computational resources, making them less accessible to researchers without access to such resources. To address this issue, we propose a novel method called SegNBDT that uses visual decision rules for image segmentation. Our approach leverages knowledge from both human experts and machine learning algorithms to create a framework that can effectively identify objects within images using simple visual cues. We evaluated our method on several publicly available datasets and found that it achieves competitive results compared to state-of-the-art deep learning approaches while requiring significantly fewer annotations during training. Overall, SegNBDT provides a promising alternative for researchers who lack extensive annotation resources or high-performance computing capabilities.",1
"We consider multi-label classification where the goal is to annotate each data point with the most relevant $\textit{subset}$ of labels from an extremely large label set. Efficient annotation can be achieved with balanced tree predictors, i.e. trees with logarithmic-depth in the label complexity, whose leaves correspond to labels. Designing prediction mechanism with such trees for real data applications is non-trivial as it needs to accommodate sending examples to multiple leaves while at the same time sustain high prediction accuracy. In this paper we develop the LdSM algorithm for the construction and training of multi-label decision trees, where in every node of the tree we optimize a novel objective function that favors balanced splits, maintains high class purity of children nodes, and allows sending examples to multiple directions but with a penalty that prevents tree over-growth. Each node of the tree is trained once the previous node is completed leading to a streaming approach for training. We analyze the proposed objective theoretically and show that minimizing it leads to pure and balanced data splits. Furthermore, we show a boosting theorem that captures its connection to the multi-label classification error. Experimental results on benchmark data sets demonstrate that our approach achieves high prediction accuracy and low prediction time and position LdSM as a competitive tool among existing state-of-the-art approaches.",0
"This paper presents LdSM, a novel algorithm that addresses the limitations of existing streaming decision tree methods by introducing logarithmic depth trees that allow for efficient multi-label learning from continuous data streams. Our approach is based on a sliding window mechanism that incrementally builds a tree using only the most recent samples. By using binary splits that separate instances according to their sign function values at each feature dimension, we construct trees that are able to capture nonlinear relationships between features and labels without overfitting to any specific stream segment. Experiments demonstrate that our method outperforms state-of-the-art alternatives in terms of accuracy and memory usage across several benchmark datasets. As such, LdSM has the potential to enable real-time classification for applications that require up-to-date predictions while processing high-dimensional data streams.",1
"Customer behavior is often assumed to follow weak rationality, which implies that adding a product to an assortment will not increase the choice probability of another product in that assortment. However, an increasing amount of research has revealed that customers are not necessarily rational when making decisions. In this paper, we propose a new nonparametric choice model that relaxes this assumption and can model a wider range of customer behavior, such as decoy effects between products. In this model, each customer type is associated with a binary decision tree, which represents a decision process for making a purchase based on checking for the existence of specific products in the assortment. Together with a probability distribution over customer types, we show that the resulting model -- a decision forest -- is able to represent any customer choice model, including models that are inconsistent with weak rationality. We theoretically characterize the depth of the forest needed to fit a data set of historical assortments and prove that with high probability, a forest whose depth scales logarithmically in the number of assortments is sufficient to fit most data sets. We also propose two practical algorithms -- one based on column generation and one based on random sampling -- for estimating such models from data. Using synthetic data and real transaction data exhibiting non-rational behavior, we show that the model outperforms both rational and non-rational benchmark models in out-of-sample predictive ability.",0
"This paper presents a new method for modeling irrational choice based on decision forest algorithms. Unlike other methods that rely on parametric assumptions such as utility maximization, our approach does not make any such assumptions. Instead, we train a decision tree using random data samples from various scenarios involving irrational choices. We then use these trees to construct ensembles that can predict outcomes across a wide range of irrationality levels. Our experiments show that this nonparametric approach leads to better accuracy than traditional methods while allowing us to capture complex interactions between factors affecting irrational behavior. Overall, we believe decision forest offers a promising alternative for studying human choice processes.",1
"Gradient Boosting Machine has proven to be one successful function approximator and has been widely used in a variety of areas. However, since the training procedure of each base learner has to take the sequential order, it is infeasible to parallelize the training process among base learners for speed-up. In addition, under online or incremental learning settings, GBMs achieved sub-optimal performance due to the fact that the previously trained base learners can not adapt with the environment once trained. In this work, we propose the soft Gradient Boosting Machine (sGBM) by wiring multiple differentiable base learners together, by injecting both local and global objectives inspired from gradient boosting, all base learners can then be jointly optimized with linear speed-up. When using differentiable soft decision trees as base learner, such device can be regarded as an alternative version of the (hard) gradient boosting decision trees with extra benefits. Experimental results showed that, sGBM enjoys much higher time efficiency with better accuracy, given the same base learner in both on-line and off-line settings.",0
"This paper introduces ""Soft Gradient Boosting Machines,"" a new model that combines the strengths of gradient boosting machines (GBM) and softmax regression while addressing their limitations. Soft GBM uses differentiable approximation techniques to optimize a regularized loss function defined on discrete labels as well as real values. Our approach makes use of the fact that GBM models can express arbitrary functions by making small tweaks to their learning rates during training. We show how to generate interpretable outputs that are more amenable to analysis than traditional black box machine learning approaches like deep neural networks. In addition to improved interpretability, our method improves prediction accuracy compared to standard GBM models across several benchmark datasets, demonstrating the potential benefits of using soft GBMs over current state-of-the-art methods. Finally, we compare our results against other popular softening techniques such as soft label smoothing, temperature scaling, and straight through estimators. Overall, the proposed model provides a simple yet effective solution to tackle the challenges faced by existing methods while offering competitive performance and better interpretability, opening up exciting possibilities in the field of applied ML research.",1
"Programming errors that degrade the performance of systems are widespread, yet there is little tool support for analyzing these bugs. We present a method based on differential performance analysis---we find inputs for which the performance varies widely, despite having the same size. To ensure that the differences in the performance are robust (i.e. hold also for large inputs), we compare the performance of not only single inputs, but of classes of inputs, where each class has similar inputs parameterized by their size. Thus, each class is represented by a performance function from the input size to performance. Importantly, we also provide an explanation for why the performance differs in a form that can be readily used to fix a performance bug.   The two main phases in our method are discovery with fuzzing and explanation with decision tree classifiers, each of which is supported by clustering. First, we propose an evolutionary fuzzing algorithm to generate inputs. For this fuzzing task, the unique challenge is that we not only need the input class with the worst performance, but rather a set of classes exhibiting differential performance. We use clustering to merge similar input classes which significantly improves the efficiency of our fuzzer. Second, we explain the differential performance in terms of program inputs and internals. We adapt discriminant learning approaches with clustering and decision trees to localize suspicious code regions.   We applied our techniques to a set of applications. On a set of micro-benchmarks, we show that our approach outperforms state-of-the-art fuzzers in finding inputs to characterize the differential performance. On a set of case-studies, we discover and explain multiple performance bugs in popular machine learning frameworks. Four of these bugs, reported first in this paper, have since been fixed by the developers.",0
"This study seeks to investigate and comprehend real-world performance discrepancies in machine learning libraries (MLLs). Specifically, we aim at detecting inconsistencies in MLLs that lead to subpar results and understanding their causes. Our research approach includes analyzing existing bugs within popular MLLs and conducting experiments on different platforms and systems to replicate these issues. We then provide insights into how developers can address these problems effectively. Ultimately, our findings contribute towards improving the reliability of MLLs by uncovering hidden issues affecting their performance.",1
"This text discusses several popular explanatory methods that go beyond the error measurements and plots traditionally used to assess machine learning models. Some of the explanatory methods are accepted tools of the trade while others are rigorously derived and backed by long-standing theory. The methods, decision tree surrogate models, individual conditional expectation (ICE) plots, local interpretable model-agnostic explanations (LIME), partial dependence plots, and Shapley explanations, vary in terms of scope, fidelity, and suitable application domain. Along with descriptions of these methods, this text presents real-world usage recommendations supported by a use case and public, in-depth software examples for reproducibility.",0
"""In recent years, machine learning has become increasingly prevalent in many areas of society, from healthcare to finance to transportation. However, as these systems continue to grow in complexity and importance, there arises a need for greater transparency and interpretability in their decision making processes. This paper explores the art and science of explaining how machine learning models arrive at their predictions, focusing on techniques that can provide insights into both the model itself and the data it was trained on. We discuss the challenges faced by practitioners in creating accurate, efficient, and user-friendly explanations, including the tradeoffs involved in different approaches. Additionally, we highlight promising research directions in developing new methods for achieving more robust and effective explanation, ultimately leading to better informed stakeholders and improved trust in these important systems.""",1
"Contact tracing is of paramount importance when it comes to preventing the spreading of infectious diseases. Contact tracing is usually performed manually by authorized personnel. Manual contact tracing is an inefficient, error-prone, time-consuming process of limited utility to the population at large as those in close contact with infected individuals are informed hours, if not days, later. This paper introduces an alternative way to manual contact tracing. The proposed Smart Contact Tracing (SCT) system utilizes the smartphone's Bluetooth Low Energy (BLE) signals and machine learning classifier to accurately and quickly determined the contact profile. SCT's contribution is two-fold: a) classification of the user's contact as high/low-risk using precise proximity sensing, and b) user anonymity using a privacy-preserving communications protocol. SCT leverages BLE's non-connectable advertising feature to broadcast a signature packet when the user is in the public space. Both broadcasted and observed signatures are stored in the user's smartphone and they are only uploaded to a secure signature database when a user is confirmed by public health authorities to be infected. Using received signal strength (RSS) each smartphone estimates its distance from other user's phones and issues real-time alerts when social distancing rules are violated. The paper includes extensive experimentation utilizing real-life smartphone positions and a comparative evaluation of five machine learning classifiers. Reported results indicate that a decision tree classifier outperforms other states of the art classification methods in terms of accuracy. Lastly, to facilitate research in this area, and to contribute to the timely development of advanced solutions the entire data set of six experiments with about 123,000 data points is made publicly available.",0
"Many smartphones now come equipped with Bluetooth Low Energy (BLE) radios that can detect other BLE devices within range, opening up new possibilities for contact tracing in times of crisis such as the pandemic we face today. By leveraging these prevalent BLE hardware components, researchers have developed novel apps and distributed them across platforms to allow users to opt-in to their localized network using an open API key mechanism. These applications share distance estimates and nearby encounter data through a decentralized meshnet structure, making use of public blockchain technology to securely store immutable records outside of any single party’s control. This abstract details how such systems work at the technical level while evaluating their potential effectiveness under real-world conditions where user adoption rates vary region by region, ultimately concluding on recommendations for future improvements.",1
"Many complex cyber-physical systems can be modeled as heterogeneous components interacting with each other in real-time. We assume that the correctness of each component can be specified as a requirement satisfied by the output signals produced by the component, and that such an output guarantee is expressed in a real-time temporal logic such as Signal Temporal Logic (STL). In this paper, we hypothesize that a large subset of input signals for which the corresponding output signals satisfy the output requirement can also be compactly described using an STL formula that we call the environment assumption. We propose an algorithm to mine such an environment assumption using a supervised learning technique. Essentially, our algorithm treats the environment assumption as a classifier that labels input signals as good if the corresponding output signal satisfies the output requirement, and as bad otherwise. Our learning method simultaneously learns the structure of the STL formula as well as the values of the numeric constants appearing in the formula. To achieve this, we combine a procedure to systematically enumerate candidate Parametric STL (PSTL) formulas, with a decision-tree based approach to learn parameter values. We demonstrate experimental results on real world data from several domains including transportation and health care.",0
"In recent years, cyber-physical systems (CPS) have become increasingly prevalent in many aspects of our daily lives, such as transportation, energy management, healthcare, and manufacturing. These systems rely on both physical components and software controllers that interact directly with their environment. Therefore, modeling these environments accurately becomes crucial for ensuring CPS reliability and safety. Traditionally, engineers make assumptions about the environmental conditions, which can lead to overconservative results or failures if the actual environment differs from those assumptions. To address this challenge, we propose a novel approach called ""Mining Environment Assumptions"" (MEA), which automatically extracts reasonable environmental assumptions based on real-world data using machine learning techniques. Our method analyzes historical sensor readings from similar CPS deployments and identifies recurring patterns that reflect typical behaviors within different operating scenarios. With MEA, the controller can adapt more effectively to changes in the environment and improve system performance while reducing operational risks. We evaluate our proposal through several case studies, including traffic signal control and smart building temperature regulation, demonstrating its effectiveness compared with existing methods. Overall, MEA offers new opportunities for enhancing CPS resilience by bridging the gap between models and reality via automated assumption mining.",1
"Techniques for clustering student behaviour offer many opportunities to improve educational outcomes by providing insight into student learning. However, one important aspect of student behaviour, namely its evolution over time, can often be challenging to identify using existing methods. This is because the objective functions used by these methods do not explicitly aim to find cluster trends in time, so these trends may not be clearly represented in the results. This paper presents `DETECT' (Detection of Educational Trends Elicited by Clustering Time-series data), a novel divisive hierarchical clustering algorithm that incorporates temporal information into its objective function to prioritise the detection of behavioural trends. The resulting clusters are similar in structure to a decision tree, with a hierarchy of clusters defined by decision rules on features. DETECT is easy to apply, highly customisable, applicable to a wide range of educational datasets and yields easily interpretable results. Through a case study of two online programming courses (N600), this paper demonstrates two example applications of DETECT: 1) to identify how cohort behaviour develops over time and 2) to identify student behaviours that characterise exercises where many students give up.",0
"This paper presents a new hierarchical clustering algorithm called ""DETECT"" that can effectively identify behavioural trends in temporal educational data. By using a combination of mathematical models and machine learning techniques, our algorithm is able to accurately group similar student behaviours over time into distinct clusters. In addition, our approach allows for easy interpretation and analysis of the results by educators and administrators. We demonstrate the effectiveness of our method through rigorous testing on real world datasets, showing that it outperforms other state-of-the-art approaches in terms of accuracy and scalability. Our work has significant implications for education research and practice, as it enables educators to better understand their students’ learning patterns and tailor instruction accordingly.",1
"We show how neural models can be used to realize piece-wise constant functions such as decision trees. The proposed architecture, which we call locally constant networks, builds on ReLU networks that are piece-wise linear and hence their associated gradients with respect to the inputs are locally constant. We formally establish the equivalence between the classes of locally constant networks and decision trees. Moreover, we highlight several advantageous properties of locally constant networks, including how they realize decision trees with parameter sharing across branching / leaves. Indeed, only $M$ neurons suffice to implicitly model an oblique decision tree with $2^M$ leaf nodes. The neural representation also enables us to adopt many tools developed for deep networks (e.g., DropConnect (Wan et al., 2013)) while implicitly training decision trees. We demonstrate that our method outperforms alternative techniques for training oblique decision trees in the context of molecular property classification and regression tasks.",0
"In this paper we introduce the class of oblique decision trees obtained by applying suitable differentiation operators to deep neural networks trained using the Rectified Linear Unit (ReLU) activation function. We show that these trees exhibit improved accuracy compared to standard decision trees on several benchmark datasets while maintaining interpretability. Our methodology relies on two main components: training a neural network using backpropagation, and computing derivatives of each node output at test time to obtain predictions. This combination allows us to generate novel interpretable models that can achieve state-of-the-art performance across multiple domains. We evaluate our approach empirically through extensive experiments on various datasets, demonstrating its effectiveness compared to other popular tree ensembles such as random forests, gradient boosted machines, and even deep learning baselines. Aside from classification problems, we further discuss how ODTs can be extended for regression tasks. Lastly, we provide insights into hyperparameter selection and potential future research directions. By introducing ODTs into machine learning frameworks, practitioners may develop more explainable predictive systems without sacrificing model quality.",1
"Deep neural networks (DNNs) are powerful black-box predictors that have achieved impressive performance on a wide variety of tasks. However, their accuracy comes at the cost of intelligibility: it is usually unclear how they make their decisions. This hinders their applicability to high stakes decision-making domains such as healthcare. We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility of generalized additive models. NAMs learn a linear combination of neural networks that each attend to a single input feature. These networks are trained jointly and can learn arbitrarily complex relationships between their input feature and the output. Our experiments on regression and classification datasets show that NAMs are more accurate than widely used intelligible models such as logistic regression and shallow decision trees. They perform similarly to existing state-of-the-art generalized additive models in accuracy, but can be more easily applied to real-world problems.",0
"In recent years, there has been increasing interest in developing machine learning models that are both accurate and interpretable, as these models can provide valuable insights into their decision making process. One promising approach towards achieving this goal is through the use of neural additive models (NAMs). These models combine the power and flexibility of deep neural networks with the interpretability of traditional statistical models such as linear regression and decision trees. This paper presents a comprehensive review of the current state-of-the art in neural additive models, highlighting their strengths and weaknesses, discussing key challenges facing researchers working in this area, and exploring future directions for research. The ultimate aim of this work is to contribute to the development of new methods for building reliable, transparent, and efficient predictive systems based on neural additive models.",1
"A comparison of the performance of various machine learning models to predict the direction of a wall following robot is presented in this paper. The models were trained using an open-source dataset that contains 24 ultrasound sensors readings and the corresponding direction for each sample. This dataset was captured using SCITOS G5 mobile robot by placing the sensors on the robot waist. In addition to the full format with 24 sensors per record, the dataset has two simplified formats with 4 and 2 input sensor readings per record. Several control models were proposed previously for this dataset using all three dataset formats. In this paper, two primary research contributions are presented. First, presenting machine learning models with accuracies higher than all previously proposed models for this dataset using all three formats. A perfect solution for the 4 and 2 inputs sensors formats is presented using Decision Tree Classifier by achieving a mean accuracy of 100%. On the other hand, a mean accuracy of 99.82% was achieves using the 24 sensor inputs by employing the Gradient Boost Classifier. Second, presenting a comparative study on the performance of different machine learning and deep learning algorithms on this dataset. Therefore, providing an overall insight on the performance of these algorithms for similar sensor fusion problems. All the models in this paper were evaluated using Monte-Carlo cross-validation.",0
"Abstract: This paper compares several machine learning algorithms used to control a wall following robot. In recent years, robots have been increasingly integrated into our daily lives and there has been significant progress made towards developing intelligent robots that can perform tasks autonomously. Controlling a wall following robot is one such task that requires the robot to navigate through complex environments while maintaining contact with a given surface, such as a wall. Several approaches exist for controlling these types of robots, including using traditional feedback control methods, model predictive control (MPC), and reinforcement learning (RL). This study evaluates the performance of four different machine learning algorithms – Artificial Neural Networks (ANN), Support Vector Machines (SVM), Genetic Programming (GP), and Fuzzy Logic Systems (FLS) – for controlling a wall following robot. Our results show that each algorithm performs well under certain conditions but that RL outperforms all other methods overall. These findings contribute to the growing field of intelligent robotics by demonstrating the feasibility of using machine learning techniques to control complex tasks performed by autonomous systems.",1
"Standard methods of using categorical variables as predictors either endow them with an ordinal structure or assume they have no structure at all. However, categorical variables often possess structure that is more complicated than a linear ordering can capture. We develop a mathematical framework for representing the structure of categorical variables and show how to generalize decision trees to make use of this structure. This approach is applicable to methods such as Gradient Boosted Trees which use a decision tree as the underlying learner. We show results on weather data to demonstrate the improvement yielded by this approach.",0
"This paper presents new methods for exploiting categorical structure using tree-based models. By leveraging the inherent hierarchical nature of many types of data, our approach improves model accuracy and interpretability. We demonstrate the effectiveness of our method on several real-world datasets across domains such as computer vision, natural language processing, and recommender systems. Our results show significant improvements over state-of-the-art approaches while providing clear insights into how different features contribute to predictions. Furthermore, we evaluate the robustness of our algorithm through extensive experimentation and analysis of worst-case scenarios. Overall, our work provides a powerful toolkit for practitioners seeking to improve their machine learning pipelines by harnessing the power of structured data.",1
The main contribution of this paper is the development of a new decision tree algorithm. The proposed approach allows users to guide the algorithm through the data partitioning process. We believe this feature has many applications but in this paper we demonstrate how to utilize this algorithm to analyse data sets containing missing values. We tested our algorithm against simulated data sets with various missing data structures and a real data set. The results demonstrate that this new classification procedure efficiently handles missing values and produces results that are slightly more accurate and more interpretable than most common procedures without any imputations or pre-processing.,0
"In this paper we present a novel approach to handling missing data in decision trees based on random forests. Our proposed method, which we call BEST (Best Ensemble Selection Tree), integrates the advantages of two popular approaches: mean imputation and multiple imputation. We first describe how to build an unpruned decision tree using all features available in the dataset, including those containing missing values. This initial tree serves as a scaffold for constructing the final ensemble of trees. Next, we randomly select $m$ subsets from the original training set where each subset contains both complete cases only (i.e., no missing data) and incomplete cases; these subsamples form the basis for building the individual trees in the forest. To handle missing values within each subsampled tree, we use either mean imputation or multiply imputed datasets depending on whether the attribute exhibits multimodality. Finally, we combine the predictions of these $m$ trees using our unique voting mechanism; this mechanism takes into account the relative weights assigned by the pruning step to ensure that trees carrying more accurate classifications have greater influence over the final prediction than the others. Our experimental results demonstrate the superiority of our method compared to state-of-the-art methods for processing datasets with significant proportions of missing values across several benchmark collections and diverse domains. For example, with respect to average rankings, precision, recall, F1 score, accuracy, Matthews correlation coefficient, Cohen’s kappa coefficient, receiver operating characteristic curves, and lift charts, our method clearly outperforms existing alternatives even when imputations are employed before learning algorithms. While future research may refine specific components of our framework, we believe tha",1
"We consider a communication scenario, in which an intruder tries to determine the modulation scheme of the intercepted signal. Our aim is to minimize the accuracy of the intruder, while guaranteeing that the intended receiver can still recover the underlying message with the highest reliability. This is achieved by perturbing channel input symbols at the encoder, similarly to adversarial attacks against classifiers in machine learning. In image classification, the perturbation is limited to be imperceptible to a human observer, while in our case the perturbation is constrained so that the message can still be reliably decoded by the legitimate receiver, which is oblivious to the perturbation. Simulation results demonstrate the viability of our approach to make wireless communication secure against state-of-the-art intruders (using deep learning or decision trees) with minimal sacrifice in the communication performance. On the other hand, we also demonstrate that using diverse training data and curriculum learning can significantly boost the accuracy of the intruder.",0
"""Adversarial attacks have become increasingly prevalent in recent years due to their ability to manipulate machine learning systems into making incorrect predictions or taking undesired actions. In the domain of speech modulation detection, adversaries may attempt to evade detection by modifying audio signals in ways that are difficult for human listeners to detect but cause traditional signal processing algorithms to fail. This work explores methods for detecting such adversarial attacks in order to improve the robustness of modulation detection systems against malicious input perturbations. We propose several new approaches based on deep neural networks and demonstrate their effectiveness through experiments using both real and synthetic data sets.""",1
"Machine learning algorithms, however effective, are known to be vulnerable in adversarial scenarios where a malicious user may inject manipulated instances. In this work we focus on evasion attacks, where a model is trained in a safe environment and exposed to attacks at test time. The attacker aims at finding a minimal perturbation of a test instance that changes the model outcome.   We propose a model-agnostic strategy that builds a robust ensemble by training its basic models on feature-based partitions of the given dataset. Our algorithm guarantees that the majority of the models in the ensemble cannot be affected by the attacker. We experimented the proposed strategy on decision tree ensembles, and we also propose an approximate certification method for tree ensembles that efficiently assess the minimal accuracy of a forest on a given dataset avoiding the costly computation of evasion attacks.   Experimental evaluation on publicly available datasets shows that proposed strategy outperforms state-of-the-art adversarial learning algorithms against evasion attacks.",0
"In order to build robust models that can perform well in adversarial scenarios, it is important to consider feature partitioning techniques. These methods allow us to divide features into multiple subsets so that each tree in the ensemble only uses one subset at a time. This helps prevent overfitting and improves the overall performance of the model. However, determining which method of feature partitioning is most effective can be challenging. To address this issue, we propose two new methods: random feature partitioning and clustering feature partitioning. We evaluate these methods on several benchmark datasets and compare them against existing state-of-the-art approaches such as Random Forest. Our results show that both our proposed methods outperform other feature partitioning techniques across all datasets. Additionally, we also explore certified prediction using these ensembles by introducing an optimization problem that guarantees correctness in presence of any distributional shift. Overall, this work provides insights into how different feature partitioning strategies affect ensemble performance, leading to more robust and accurate predictive systems.",1
"A gradient boosting decision tree (GBDT), which aggregates a collection of single weak learners (i.e. decision trees), is widely used for data mining tasks. Because GBDT inherits the good performance from its ensemble essence, much attention has been drawn to the optimization of this model. With its popularization, an increasing need for model interpretation arises. Besides the commonly used feature importance as a global interpretation, feature contribution is a local measure that reveals the relationship between a specific instance and the related output. This work focuses on the local interpretation and proposes an unified computation mechanism to get the instance-level feature contributions for GBDT in any version. Practicality of this mechanism is validated by the listed experiments as well as applications in real industry scenarios.",0
"Explain the role of global sensitivity indices (GSIs) for measuring feature importance in Gradient Boosting Decision Trees (GBDT). Provide examples using Python code from scikit-learn library. Use these examples to explain how GSIs can be used to calculate feature importances, including mean decrease impurity index and permutation feature importance. Additionally, consider the effectiveness of different GSI methods by comparing their results on real datasets. Finally, discuss the limitations of GSIs and propose ways to address them.",1
"Current deep-learning based methods do not easily integrate to clinical protocols, neither take full advantage of medical knowledge. In this work, we propose and compare several strategies relying on curriculum learning, to support the classification of proximal femur fracture from X-ray images, a challenging problem as reflected by existing intra- and inter-expert disagreement. Our strategies are derived from knowledge such as medical decision trees and inconsistencies in the annotations of multiple experts, which allows us to assign a degree of difficulty to each training sample. We demonstrate that if we start learning ""easy"" examples and move towards ""hard"", the model can reach a better performance, even with fewer data. The evaluation is performed on the classification of a clinical dataset of about 1000 X-ray images. Our results show that, compared to class-uniform and random strategies, the proposed medical knowledge-based curriculum, performs up to 15% better in terms of accuracy, achieving the performance of experienced trauma surgeons.",0
"This paper presents Medical-based Deep Curriculum Learning (MdCL) for improving fracture classification using X-ray images. MdCL uses multiple stages that focus on different aspects of bone structure, from the most general to the most specific, while allowing for both efficient training and fast inference. Our experiments show significant improvement over baseline methods, achieving state-of-the-art performance across five public datasets: FractureX, LoDo, OUMSM, USLF, and UWO. Furthermore, we provide extensive ablation studies showing the effectiveness of each component within our proposed framework. Finally, we perform clinical comparisons against radiologists and demonstrate the potential real-world impact of automating fracture detection. Overall, our work demonstrates the feasibility of enhancing existing deep learning architectures with medical curricula, significantly boosting their diagnostic capabilities in complex scenarios involving fracture identification.",1
"We introduce a cluster evaluation technique called Tree Index. Our Tree Index algorithm aims at describing the structural information of the clustering rather than the quantitative format of cluster-quality indexes (where the representation power of clustering is some cumulative error similar to vector quantization). Our Tree Index is finding margins amongst clusters for easy learning without the complications of Minimum Description Length. Our Tree Index produces a decision tree from the clustered data set, using the cluster identifiers as labels. It combines the entropy of each leaf with their depth. Intuitively, a shorter tree with pure leaves generalizes the data well (the clusters are easy to learn because they are well separated). So, the labels are meaningful clusters. If the clustering algorithm does not separate well, trees learned from their results will be large and too detailed. We show that, on the clustering results (obtained by various techniques) on a brain dataset, Tree Index discriminates between reasonable and non-sensible clusters. We confirm the effectiveness of Tree Index through graphical visualizations. Tree Index evaluates the sensible solutions higher than the non-sensible solutions while existing cluster-quality indexes fail to do so.",0
"This paper presents Tree Index (TI), a new method for evaluating clusters that takes into account intra-cluster homogeneity, inter-cluster separation, and hierarchy within each cluster. Unlike traditional clustering techniques which focus on either separability or compactness of the clusters, TI provides a comprehensive evaluation based on three measures - IntraClustCompactness, InterClustSeparability, and HierarchicalConsistency - calculated using both numerical and graphical methods. The proposed technique is validated through simulations and real datasets, demonstrating improved performance compared to existing methods. Furthermore, visualizing the results obtained from Tree Index can aid data analysts in interpreting their findings more easily. Overall, Tree Index offers researchers and practitioners a simple yet effective tool for evaluating clustering solutions.",1
"Recent advances in machine learning and artificial intelligence are now being considered in safety-critical autonomous systems where software defects may cause severe harm to humans and the environment. Design organizations in these domains are currently unable to provide convincing arguments that their systems are safe to operate when machine learning algorithms are used to implement their software.   In this paper, we present an efficient method to extract equivalence classes from decision trees and tree ensembles, and to formally verify that their input-output mappings comply with requirements. The idea is that, given that safety requirements can be traced to desirable properties on system input-output patterns, we can use positive verification outcomes in safety arguments. This paper presents the implementation of the method in the tool VoTE (Verifier of Tree Ensembles), and evaluates its scalability on two case studies presented in current literature.   We demonstrate that our method is practical for tree ensembles trained on low-dimensional data with up to 25 decision trees and tree depths of up to 20. Our work also studies the limitations of the method with high-dimensional data and preliminarily investigates the trade-off between large number of trees and time taken for verification.",0
"Title: ""Formal Verification of Input-Output Mappings of Tree Ensembles"" (20XX) Authors: <Authors> Abstract: This paper presents a novel approach for formal verification of input-output mappings for tree ensembles, which play an essential role in machine learning and decision making systems. We describe how such mappings can be characterized using mathematical models and then prove their correctness through automated theorem proving techniques. Our methodology demonstrates the feasibility and effectiveness of applying formal methods to ensure safety and reliability in complex data processing pipelines involving tree ensembles. Experimental results on real-world datasets show that our approach leads to significantly improved robustness against adversarial inputs while incurring negligible overhead compared to state-of-the-art alternatives. Overall, we believe that this work paves the way towards more trustworthy artificial intelligence applications based on deep learning principles.",1
"One-class Classification (OCC) is an area of machine learning which addresses prediction based on unbalanced datasets. Basically, OCC algorithms achieve training by means of a single class sample, with potentially some additional counter-examples. The current OCC models give satisfaction in terms of performance, but there is an increasing need for the development of interpretable models. In the present work, we propose a one-class model which addresses concerns of both performance and interpretability. Our hybrid OCC method relies on density estimation as part of a tree-based learning algorithm, called One-Class decision Tree (OC-Tree). Within a greedy and recursive approach, our proposal rests on kernel density estimation to split a data subset on the basis of one or several intervals of interest. Thus, the OC-Tree encloses data within hyper-rectangles of interest which can be described by a set of rules. Against state-of-the-art methods such as Cluster Support Vector Data Description (ClusterSVDD), One-Class Support Vector Machine (OCSVM) and isolation Forest (iForest), the OC-Tree performs favorably on a range of benchmark datasets. Furthermore, we propose a real medical application for which the OC-Tree has demonstrated its effectiveness, through the ability to tackle interpretable diagnosis aid based on unbalanced datasets.",0
"This research presents a novel approach to one-class classification using decision trees based on kernel density estimation. The method leverages the ability of kernel density estimation (KDE) to capture complex probability distributions and uses a decision tree framework to construct a classifier that can accurately identify samples from unknown classes. The proposed algorithm first generates synthetic training data by sampling random instances near the center of the distribution defined by known class examples. These synthetic points serve as surrogates for unknown test points. By modeling the relationship between the observed values and their corresponding distances to unknown test points, our method can effectively detect anomalies and uncover hidden patterns in large datasets. Experimental results demonstrate the effectiveness of our approach in terms of both accuracy and speed. Our method achieves state-of-the-art performance across a variety of real-world data sets including image and text data while maintaining computational efficiency. Overall, the proposed decision tree based on KDE provides a robust solution for one-class classification tasks where few labeled examples are available.",1
"This paper presents a detailed comparison of a recently proposed algorithm for optimizing decision trees, tree alternating optimization (TAO), with other popular, established algorithms. We compare their performance on a number of classification and regression datasets of various complexity, different size and dimensionality, across different performance factors: accuracy and tree size (in terms of the number of leaves or the depth of the tree). We find that TAO achieves higher accuracy in nearly all datasets, often by a large margin.",0
"This study compares two decision tree algorithms: CART (Classification And Regression Trees) and Random Forests. We use three different data sets from UCI Machine Learning Repository (heart disease, Ionosphere and Pima Indians diabetes dataset) with varying number of samples to evaluate both models accuracy. The results show that Random Forests consistently produce better predictive performance than CART across all datasets. Moreover, we observe that Random Forests have higher sensitivity which means that it is more likely to correctly identify positive instances compared to negative ones. Although both methods perform well but Random Forest has shown superiority over CART.",1
"Over the past decade, random forest models have become widely used as a robust method for high-dimensional data regression tasks. In part, the popularity of these models arises from the fact that they require little hyperparameter tuning and are not very susceptible to overfitting. Random forest regression models are comprised of an ensemble of decision trees that independently predict the value of a (continuous) dependent variable; predictions from each of the trees are ultimately averaged to yield an overall predicted value from the forest. Using a suite of representative real-world datasets, we find a systematic bias in predictions from random forest models. We find that this bias is recapitulated in simple synthetic datasets, regardless of whether or not they include irreducible error (noise) in the data, but that models employing boosting do not exhibit this bias. Here we demonstrate the basis for this problem, and we use the training data to define a numerical transformation that fully corrects it. Application of this transformation yields improved predictions in every one of the real-world and synthetic datasets evaluated in our study.",0
"In a recent study aimed at developing new techniques that improve random forest regressor performance, we have proposed a method referred to as a numerical transform (NT) which has been demonstrated to address systemic biases observed in current implementations of the algorithm. By utilizing empirical data from multiple datasets, our results show significant improvements over other state-of-the art methods in terms of accuracy and precision. This work contributes to existing literature by providing novel insights into random forest regression models and presents potential applications across various domains where such predictions play critical roles. We anticipate future research opportunities exploring the generalizability and extendibility of our approach in different contexts.",1
"The lack of interpretability remains a barrier to the adoption of deep neural networks. Recently, tree regularization has been proposed to encourage deep neural networks to resemble compact, axis-aligned decision trees without significant compromises in accuracy. However, it may be unreasonable to expect that a single tree can predict well across all possible inputs. In this work, we propose regional tree regularization, which encourages a deep model to be well-approximated by several separate decision trees specific to predefined regions of the input space. Practitioners can define regions based on domain knowledge of contexts where different decision-making logic is needed. Across many datasets, our approach delivers more accurate predictions than simply training separate decision trees for each region, while producing simpler explanations than other neural net regularization schemes without sacrificing predictive power. Two healthcare case studies in critical care and HIV demonstrate how experts can improve understanding of deep models via our approach.",0
"In recent years, deep neural networks have been widely used across numerous fields due to their remarkable performance on complex tasks. However, these models can often produce results that are difficult to interpret or explain, leading to concerns about transparency and trustworthiness. To address this issue, researchers have proposed techniques such as LIME (Local Interpretable Model Explanations) which explain individual predictions by approximating the model locally around a specific data point using a simpler surrogate model. While these methods provide local explanations, they may still miss global patterns present in the data. In addition, most existing techniques require knowledge of the prediction itself, making them unsuitable for real-time applications where only raw inputs are available.  This paper presents a novel approach called ""Regional Tree Regularization"" (RTR), aimed at regularizing black box models towards producing more interpretable decision boundaries. Rather than focusing solely on individual predictions, RTR enforces global consistency constraints over groups of input points. These groups are defined based on geometric properties of the input space and captures spatial relationships between neighboring points. Specifically, we introduce tree structures that divide the feature space into regions and enforce smoothness conditions within each region while allowing different parts of the tree to capture distinct characteristics of the data distribution. This allows our method to efficiently group similar features together without prior domain knowledge. Our contributions include:  * Introducing regional structure via trees, to encourage smooth decision boundaries aligned with meaningful partitions of the input feature space. * Developing algorithms for learning decision tree hierarchies along both individual dimensions and combinations thereof, tailored to preserve benign (e.g., piecewise linear) complexity assumptions made by many machine learning models. * Providing extensive empirical studies demonstrati",1
"A crime is a punishable offence that is harmful for an individual and his society. It is obvious to comprehend the patterns of criminal activity to prevent them. Research can help society to prevent and solve crime activates. Study shows that only 10 percent offenders commits 50 percent of the total offences. The enforcement team can respond faster if they have early information and pre-knowledge about crime activities of the different points of a city. In this paper, supervised learning technique is used to predict crimes with better accuracy. The proposed system predicts crimes by analyzing data-set that contains records of previously committed crimes and their patterns. The system stands on two main algorithms - i) decision tree, and ii) k-nearest neighbor. Random Forest algorithm and Adaboost are used to increase the accuracy of the prediction. Finally, oversampling is used for better accuracy. The proposed system is feed with a criminal-activity data set of twelve years of San Francisco city.",0
"Artificial intelligence (AI) has been increasingly applied to crime prevention due to its ability to process large amounts of data in real time, which can facilitate more accurate predictions of criminal activity than traditional methods. However, current approaches often focus on static patterns, lacking consideration of spatiotemporal dependencies that play critical roles in shaping criminal behavior. This study develops a novel methodology for integrating dynamic environmental features into crime prediction models using high-resolution spatiotemporal datasets, enabling fine-grained understanding of the complex interactions between socioeconomic characteristics, weather conditions, land use structures, and crime patterns across space and time. Our results demonstrate the effectiveness of our approach in improving crime prediction accuracy compared to existing methods, providing important implications for law enforcement agencies seeking better decision support tools.",1
"One of the current challenges in machine learning is how to deal with data coming at increasing rates in data streams. New predictive learning strategies are needed to cope with the high throughput data and concept drift. One of the data stream mining tasks where new learning strategies are needed is multi-target regression, due to its applicability in a high number of real world problems. While reliable and effective learning strategies have been proposed for batch multi-target regression, few have been proposed for multi-target online learning in data streams. Besides, most of the existing solutions do not consider the occurrence of inter-target correlations when making predictions. In this work, we propose a novel online learning strategy for multi-target regression in data streams. The proposed strategy extends existing online decision tree learning algorithm to explore inter-target dependencies while making predictions. For such, the proposed strategy, called Stacked Single-target Hoeffding Tree (SST-HT), uses the inter-target dependencies as an additional information source to enhance predictive accuracy. Throughout an extensive experimental setup, we evaluate our proposal against state-of-the-art decision tree-based algorithms for online multi-target regression. According to the experimental results, SST-HT presents superior predictive accuracy, with a small increase in the processing time and memory requirements.",0
"In ""Online multi-target regression trees with stacked leaf models,"" we propose a novel approach to online multi-task regression that combines two recently popular ideas: (i) gradient tree boosting; and (ii) structured prediction via structured output support vector machines using stacked linear models. We show how these techniques can be combined effectively within one unified framework that performs both offline training as well as online update steps on new data. Our proposed method achieves state-of-the-art results across five benchmark datasets compared against other leading methods for each task individually and jointly. Furthermore, our method runs faster than previous online methods while providing improved accuracy over batch solutions and better generalization performance when used offline. Overall, our work highlights the importance of considering multiple tasks together in order to improve predictive performance while enabling efficient model updating with streaming data arrival. This study has important implications for real world applications where model updates need to happen continuously such as sensor networks, recommender systems, ad placement platforms, financial transactions monitoring systems, medical diagnosis tools among others.",1
"Predictive models are being increasingly used to support consequential decision making at the individual level in contexts such as pretrial bail and loan approval. As a result, there is increasing social and legal pressure to provide explanations that help the affected individuals not only to understand why a prediction was output, but also how to act to obtain a desired outcome. To this end, several works have proposed optimization-based methods to generate nearest counterfactual explanations. However, these methods are often restricted to a particular subset of models (e.g., decision trees or linear models) and differentiable distance functions. In contrast, we build on standard theory and tools from formal verification and propose a novel algorithm that solves a sequence of satisfiability problems, where both the distance function (objective) and predictive model (constraints) are represented as logic formulae. As shown by our experiments on real-world data, our algorithm is: i) model-agnostic ({non-}linear, {non-}differentiable, {non-}convex); ii) data-type-agnostic (heterogeneous features); iii) distance-agnostic ($\ell_0, \ell_1, \ell_\infty$, and combinations thereof); iv) able to generate plausible and diverse counterfactuals for any sample (i.e., 100% coverage); and v) at provably optimal distances.",0
"In recent years, there has been growing interest in developing explainable artificial intelligence (AI) systems that can provide clear and understandable explanations for their decisions and actions. One approach to creating such explainable AI systems is through the use of counterfactual reasoning, which involves generating hypothetical scenarios that describe how different outcomes could have resulted from alternative choices or conditions. This paper proposes a new method for generating model-agnostic counterfactual explanations for consequential decision making problems. Our method uses a novel combination of causal inference techniques and classical planning algorithms to generate plausible alternate futures that capture relevant factors influencing the outcome. We evaluate our method on real-world decision tasks with high stakes consequences, showing that it can effectively generate accurate and informative counterfactuals even when little data is available. Overall, we believe that our work represents an important step towards building more transparent and accountable AI systems that can improve decision making across a range of applications.",1
"Time-series forecasting is an important task in both academic and industry, which can be applied to solve many real forecasting problems like stock, water-supply, and sales predictions. In this paper, we study the case of retailers' sales forecasting on Tmall|the world's leading online B2C platform. By analyzing the data, we have two main observations, i.e., sales seasonality after we group different groups of retails and a Tweedie distribution after we transform the sales (target to forecast). Based on our observations, we design two mechanisms for sales forecasting, i.e., seasonality extraction and distribution transformation. First, we adopt Fourier decomposition to automatically extract the seasonalities for different categories of retailers, which can further be used as additional features for any established regression algorithms. Second, we propose to optimize the Tweedie loss of sales after logarithmic transformations. We apply these two mechanisms to classic regression models, i.e., neural network and Gradient Boosting Decision Tree, and the experimental results on Tmall dataset show that both mechanisms can significantly improve the forecasting results.",0
"This paper presents a study exploring sales forecasting on China's leading e-commerce platform, Tmall. By analyzing data from successful retailers, we aim to identify key factors that influence online sales performance. Our research focuses on understanding how retailers can optimize their product listings, marketing strategies, pricing decisions, and inventory management practices to increase sales revenue. We provide insights into which variables have significant impacts on sales growth and present findings that could aid other merchants in improving their own business outcomes. Additionally, our results contribute towards developing more accurate sales forecasting models that better predict future demand patterns across different industries on Tmall. Overall, the study offers valuable knowledge for both practitioners and academics seeking to enhance their knowledge of online retail operations within the Chinese context.",1
"Strong theoretical guarantees of robustness can be given for ensembles of classifiers generated by input randomization. Specifically, an $\ell_2$ bounded adversary cannot alter the ensemble prediction generated by an additive isotropic Gaussian noise, where the radius for the adversary depends on both the variance of the distribution as well as the ensemble margin at the point of interest. We build on and considerably expand this work across broad classes of distributions. In particular, we offer adversarial robustness guarantees and associated algorithms for the discrete case where the adversary is $\ell_0$ bounded. Moreover, we exemplify how the guarantees can be tightened with specific assumptions about the function class of the classifier such as a decision tree. We empirically illustrate these results with and without functional restrictions across image and molecule datasets.",0
"In recent years, machine learning has seen significant advances in the field of image classification through deep neural networks (DNNs). Despite their success, these models remain vulnerable to adversarial attacks, which can cause the model to make incorrect predictions even on inputs that appear visually similar to natural examples. To address this issue, researchers have focused on developing methods for improving the robustness of DNNs against such attacks. One approach involves smoothing the input data using random noise before feeding it into the model, making it more difficult for attack algorithms to craft effective perturbations. However, measuring the effectiveness of this type of defense remains challenging due to the lack of standardized evaluation metrics. This work presents a new method for evaluating the robustness of randomly smoothed classifiers by introducing tight certificates of robustness. These certificates provide guarantees on the minimum amount of allowed perturbation required to change the prediction of the model, enabling direct comparison across different models and settings. Our results show that random smoothing can significantly improve the robustness of DNNs under several types of attacks while incurring only minor loss in accuracy compared to unsmoothed models. By providing rigorous bounds on adversarial robustness, our proposed method offers a valuable tool for practitioners working on building secure machine learning systems.",1
"Neural Networks and Decision Trees: two popular techniques for supervised learning that are seemingly disconnected in their formulation and optimization method, have recently been combined in a single construct. The connection pivots on assembling an artificial Neural Network with nodes that allow for a gate-like function to mimic a tree split, optimized using the standard approach of recursively applying the chain rule to update its parameters. Yet two main challenges have impeded wide use of this hybrid approach: (a) the inability of global gradient ascent techniques to optimize hierarchical parameters (as introduced by the gate function); and (b) the construction of the tree structure, which has relied on standard decision tree algorithms to learn the network topology or incrementally (and heuristically) searching the space at random. Here we propose a probabilistic construct that exploits the idea of a node's unexplained potential (the total error channeled through the node) in order to decide where to expand further, mimicking the standard tree construction in a Neural Network setting, alongside a modified gradient ascent that first locally optimizes an expanded node before a global optimization. The probabilistic approach allows us to evaluate each new split as a ratio of likelihoods that balances the statistical improvement in explaining the evidence against the additional model complexity --- thus providing a natural stopping condition. The result is a novel classification and regression technique that leverages the strength of both: a tree-structure that grows naturally and is simple to interpret with the plasticity of Neural Networks that allow for soft margins and slanted boundaries.",0
"Sure! Here's an example:  This paper presents a novel approach for adapting deep learning models using bayesian networks. Traditional methods have focused on fixed architectures that require manual tuning of hyperparameters to achieve high performance. However, these approaches can struggle when faced with new tasks, as they lack flexibility and may overfit the training data. To address these limitations, we propose a Bayesian reticulum architecture that allows deep neural networks (DNNs) to dynamically select between multiple layers based on the complexity of the task at hand. We use variational inference to approximate posterior distributions over the DNN weights, which enables efficient sampling during test time. Our method demonstrates state-of-the art results across several benchmark datasets, while requiring less computational resources than competitive baselines. We provide extensive ablation studies and error analysis to validate our findings. This work has important implications for future research in machine learning, as it suggests that flexible and adaptive architectures hold significant potential for improving generalization and reducing overfitting.",1
"Adversarially robust machine learning has received much recent attention. However, prior attacks and defenses for non-parametric classifiers have been developed in an ad-hoc or classifier-specific basis. In this work, we take a holistic look at adversarial examples for non-parametric classifiers, including nearest neighbors, decision trees, and random forests. We provide a general defense method, adversarial pruning, that works by preprocessing the dataset to become well-separated. To test our defense, we provide a novel attack that applies to a wide range of non-parametric classifiers. Theoretically, we derive an optimally robust classifier, which is analogous to the Bayes Optimal. We show that adversarial pruning can be viewed as a finite sample approximation to this optimal classifier. We empirically show that our defense and attack are either better than or competitive with prior work on non-parametric classifiers. Overall, our results provide a strong and broadly-applicable baseline for future work on robust non-parametrics. Code available at https://github.com/yangarbiter/adversarial-nonparametrics/ .",0
"Robust classification models have become increasingly important as they are used in safety critical applications such as medical diagnosis, autonomous driving, and security systems. However, recent works have shown that many state-of-the art non-parametric classification methods can be easily fooled by small perturbations, known as adversarial attacks. In this work, we present a novel attack methodology that applies to any non-parametric classifier trained using kernel density estimation (KDE). This new generic attack consistently reduces the accuracy of KDE classifiers while maintaining low distortion between clean samples. We demonstrate the effectiveness of our attack on several datasets including MNIST, CIFAR-10, SVHN, and ImageNet.  To defend against these robustness issues, existing literature proposes different approaches tailored specifically towards each model architecture. Herein, inspired by defenses from parametric domains, a defense strategy based on data augmentation during training is proposed, aimed at improving the generalization performance of the KDE algorithm itself. Our experiments reveal that our defense significantly increases the robustness of non-parametric classifiers against both random noise perturbations and generic adversaries designed for deep learning networks. Furthermore, our method produces minimal distortion which leads to improved accuracy even under white-box attacks. In summary, our contributions address open challenges in achieving high levels of robustness through adversary detection/prevention and enhancing the robustness of non-parametric classifiers with respect to natural noise, common across all applications. These results hold implications for realizing more reliable artificial intelligence systems deployed in operational settings, where strong guarantees of correct behavior are necessary.",1
"Accurate predictions of reactive mixing are critical for many Earth and environmental science problems. To investigate mixing dynamics over time under different scenarios, a high-fidelity, finite-element-based numerical model is built to solve the fast, irreversible bimolecular reaction-diffusion equations to simulate a range of reactive-mixing scenarios. A total of 2,315 simulations are performed using different sets of model input parameters comprising various spatial scales of vortex structures in the velocity field, time-scales associated with velocity oscillations, the perturbation parameter for the vortex-based velocity, anisotropic dispersion contrast, and molecular diffusion. Outputs comprise concentration profiles of the reactants and products. The inputs and outputs of these simulations are concatenated into feature and label matrices, respectively, to train 20 different machine learning (ML) emulators to approximate system behavior. The 20 ML emulators based on linear methods, Bayesian methods, ensemble learning methods, and multilayer perceptron (MLP), are compared to assess these models. The ML emulators are specifically trained to classify the state of mixing and predict three quantities of interest (QoIs) characterizing species production, decay, and degree of mixing. Linear classifiers and regressors fail to reproduce the QoIs; however, ensemble methods (classifiers and regressors) and the MLP accurately classify the state of reactive mixing and the QoIs. Among ensemble methods, random forest and decision-tree-based AdaBoost faithfully predict the QoIs. At run time, trained ML emulators are $\approx10^5$ times faster than the high-fidelity numerical simulations. Speed and accuracy of the ensemble and MLP models facilitate uncertainty quantification, which usually requires 1,000s of model run, to estimate the uncertainty bounds on the QoIs.",0
"""A study comparing three common machine learning algorithms was conducted to determine their accuracy at predicting the state of reactive mixing. Results showed that two models outperformed the third in overall performance, but all three were able to provide meaningful predictions."" ----  Please write an improved version.  Comparative Study on Machine Learning Models’ Ability to Accurately Predict Reactive Mixing States: A Comprehensive Analysis  Predictive modeling of chemical reactivity has become increasingly important in recent years due to its impact on process optimization, safety, and sustainability in industrial settings. To address this issue, we aimed to compare three commonly used machine learning (ML) algorithms—Decision Tree, Random Forest, and Neural Networks—to evaluate their effectiveness in predicting the states of reactive mixtures. Our results showcased each algorithm's strengths and weaknesses while providing valuable insights into optimizing these ML models' parameters. Ultimately, both Decision Trees and Random Forests proved superior to Neural Networks across multiple performance metrics, highlighting the importance of selecting appropriate algorithms based on specific datasets' characteristics. This research serves as a crucial step towards building robust predictive tools for use within complex chemical systems.",1
"Since its inception in the 1980s, ID3 has become one of the most successful and widely used algorithms for learning decision trees. However, its theoretical properties remain poorly understood. In this work, we introduce a novel metric of a decision tree algorithm's performance, called mean iteration statistical consistency (MIC), which measures optimality of trees generated by ID3. As opposed to previous metrics, MIC can differentiate between different decision tree algorithms and compare their performance. We provide theoretical and empirical evidence that the TopDown variant of ID3, introduced by Kearns and Mansour (1996), has near-optimal MIC in various settings for learning read-once DNFs under product distributions. In contrast, another widely used variant of ID3 has MIC which is not near-optimal. We show that the MIC analysis predicts well the performance of these algorithms in practice. Our results present a novel view of decision tree algorithms which may lead to better and more practical guarantees for these algorithms.",0
"This study examines the efficiency of trees generated by ID3, a decision tree generation algorithm used in machine learning. We analyzed the structure and performance of these trees to determine if they are optimal in terms of predictive accuracy and interpretability. Our results show that while ID3 trees can be prone to overfitting, their simplicity makes them effective at capturing important relationships between variables and facilitating human interpretation. Furthermore, we found that ID3 trees perform comparably to other algorithms when trained on datasets with high quality features and well-separated classes. Overall, our findings suggest that while there may be room for improvement in certain cases, ID3 trees remain a viable option for generating decision trees due to their balance between ease of use, interpretability, and effectiveness. Keywords: decision tree; feature selection; ID3; model accuracy; interpretability",1
"The success of neural networks in image classification has inspired various hardware implementations on embedded platforms such as Field Programmable Gate Arrays, embedded processors and Graphical Processing Units. These embedded platforms are constrained in terms of power, which is mainly consumed by the Multiply Accumulate operations and the memory accesses for weight fetching. Quantization and pruning have been proposed to address this issue. Though effective, these techniques do not take into account the underlying architecture of the embedded hardware. In this work, we propose PoET-BiN, a Look-Up Table based power efficient implementation on resource constrained embedded devices. A modified Decision Tree approach forms the backbone of the proposed implementation in the binary domain. A LUT access consumes far less power than the equivalent Multiply Accumulate operation it replaces, and the modified Decision Tree algorithm eliminates the need for memory accesses. We applied the PoET-BiN architecture to implement the classification layers of networks trained on MNIST, SVHN and CIFAR-10 datasets, with near state-of-the art results. The energy reduction for the classifier portion reaches up to six orders of magnitude compared to a floating point implementations and up to three orders of magnitude when compared to recent binary quantized neural networks.",0
"Here we present PoET-BiN (Power Efficient Tiny Binary Neuron), a new computational model that significantly improves upon existing binary neuron models by achieving higher power efficiency without sacrificing performance. Our method uses innovative techniques such as multi-scale feature extraction, channel-wise attention pooling, and a novel data rebalancing scheme. These advancements allow us to drastically reduce memory usage while maintaining high accuracy on standard benchmarks. Furthermore, our approach exhibits better robustness against input perturbations compared to prior art. We believe this work represents an important step towards realizing energy-efficient Artificial Intelligence systems capable of running on low-power devices.",1
"Models with transparent inner structure and high classification performance are required to reduce potential risk and provide trust for users in domains like health care, finance, security, etc. However, existing models are hard to simultaneously satisfy the above two properties. In this paper, we propose a new hierarchical rule-based model for classification tasks, named Concept Rule Sets (CRS), which has both a strong expressive ability and a transparent inner structure. To address the challenge of efficiently learning the non-differentiable CRS model, we propose a novel neural network architecture, Multilayer Logical Perceptron (MLLP), which is a continuous version of CRS. Using MLLP and the Random Binarization (RB) method we proposed, we can search the discrete solution of CRS in continuous space using gradient descent and ensure the discrete CRS acts almost the same as the corresponding continuous MLLP. Experiments on 12 public data sets show that CRS outperforms the state-of-the-art approaches and the complexity of the learned CRS is close to the simple decision tree. Source code is available at https://github.com/12wang3/mllp.",0
In this paper we study neural networks with multilayer logical perceptrons (MLLPs) that perform classification on structured data types such as images or time series. We focus specifically on methods for training these models to achieve transparency with respect to interpretability and robustness against adversarial attacks. To accomplish this goal we introduce a new regularizer called random binarization which randomly sets neuron activations at each iteration during gradient descent. Our experiments show that this method improves performance on standard benchmark datasets and enables high levels of transparency through visualizations of the learned representations.,1
"The Big Data analytics are a logical analysis of very large scale datasets. The data analysis enhances an organization and improve the decision making process. In this article, we present Airline Delay Analysis and Prediction to analyze airline datasets with the combination of weather dataset. In this research work, we consider various attributes to analyze flight delay, for example, day-wise, airline-wise, cloud cover, temperature, etc. Moreover, we present rigorous experiments on various machine learning model to predict correctly the delay of a flight, namely, logistic regression with L2 regularization, Gaussian Naive Bayes, K-Nearest Neighbors, Decision Tree classifier and Random forest model. The accuracy of the Random Forest model is 82% with a delay threshold of 15 minutes of flight delay. The analysis is carried out using dataset from 1987 to 2008, the training is conducted with dataset from 2000 to 2007 and validated prediction result using 2008 data. Moreover, we have got recall 99% in the Random Forest model.",0
"This abstract should focus more on summarizing results rather than providing motivation and problem statement like other abstracts you have written before. Also, please use active voice instead of passive voice where possible. In this study we aimed to explore factors that contribute to airline delay using historical data from major US airlines between 2009 and 2020. Our analysis revealed that several factors including weather conditions, flight schedule disruptions, aircraft maintenance delays, air traffic control, as well as passenger behavior significantly contributed to increased flight delays. Furthermore, our findings indicated that flight time, layover duration, and number of connections were not significant predictors of delay. Finally, we developed predictive models based on both linear regression and decision tree algorithms which demonstrated good accuracy in forecasting flight delays. Our research provides valuable insights into improving operational efficiency and enhancing customer experience within the aviation industry.",1
"Domain generalization is the problem of machine learning when the training data and the test data come from different data domains. We present a simple theoretical model of learning to generalize across domains in which there is a meta-distribution over data distributions, and those data distributions may even have different supports. In our model, the training data given to a learning algorithm consists of multiple datasets each from a single domain drawn in turn from the meta-distribution. We study this model in three different problem settings---a multi-domain Massart noise setting, a decision tree multi-dataset setting, and a feature selection setting, and find that computationally efficient, polynomial-sample domain generalization is possible in each. Experiments demonstrate that our feature selection algorithm indeed ignores spurious correlations and improves generalization.",0
"""Learning to expect the unexpected"" explores how machine learning algorithms can improve their general knowledge by incorporating techniques that allow them to better handle unforeseen events. By using Probably Approximately Correct (PAC) methods, which enable machines to learn from limited data sets while maintaining accuracy, these systems can adapt more effectively to changing environments. This leads to improved performance on complex tasks and greater versatility across domains. Additionally, through the use of probabilistically robust models, such as Bayesian networks, deep neural networks, and random decision trees, machines can become less prone to overfitting and achieve higher levels of general intelligence. Overall, the research suggests that incorporating PAC domain generalization into machine learning frameworks could lead to a significant leap forward in artificial intelligence technology.",1
"Decision tree learning is a popular classification technique most commonly used in machine learning applications. Recent work has shown that decision trees can be used to represent provably-correct controllers concisely. Compared to representations using lookup tables or binary decision diagrams, decision trees are smaller and more explainable. We present dtControl, an easily extensible tool for representing memoryless controllers as decision trees. We give a comprehensive evaluation of various decision tree learning algorithms applied to 10 case studies arising out of correct-by-construction controller synthesis. These algorithms include two new techniques, one for using arbitrary linear binary classifiers in the decision tree learning, and one novel approach for determinizing controllers during the decision tree construction. In particular the latter turns out to be extremely efficient, yielding decision trees with a single-digit number of decision nodes on 5 of the case studies.",0
"Here we propose dtControl (Decision Tree Learning Algorithms for Controller representation) as a novel method for training deep reinforcement learning agents based on decision trees. Our algorithm builds upon existing actor-critic architectures by using tree ensembles to represent both the policy and value functions. We show that our method improves state-of-the art performance across a range of Atari games benchmarks, while demonstrating enhanced sample efficiency. Additionally, we demonstrate how dtControl can learn interpretable controllers capable of generating textual descriptions which may aid human understanding and debugging of agent behavior. Finally, we evaluate the robustness of dtControl through visualizations and analyses of learned policies. Through these evaluations we identify potential failure modes of our approach and provide recommendations for future research directions. In this work, we present dtControl, a new method for training deep reinforcement learning agents using decision tree learning algorithms to represent the controller. The core idea behind dtControl is to use decision tree ensembles to replace traditional neural network representations of both the policy and value functions commonly used in actor-critic architectures. This allows us to train more efficient and transparent agents while achieving state-of-the-art results on several popular Atari game benchmarks. Furthermore, we show that dtControl can generate interpretable controllers capable of providing descriptive explanations of their actions, facilitating human comprehension and troubleshooting. To assess the robustness of dtControl, we conduct thorough empirical evaluations including visualization analysis and robustness testing. Overall, our results highlight promising capabilities for dtControl and suggest fruitful paths for further investigation.",1
"Artificial Neural Networks form the basis of very powerful learning methods. It has been observed that a naive application of fully connected neural networks to data with many irrelevant variables often leads to overfitting. In an attempt to circumvent this issue, a prior knowledge pertaining to what features are relevant and their possible feature interactions can be encoded into these networks. In this work, we use decision trees to capture such relevant features and their interactions and define a mapping to encode extracted relationships into a neural network. This addresses the initialization related concern of fully connected neural networks. At the same time through feature selection it enables learning of compact representations compared to state of the art tree-based approaches. Empirical evaluations and simulation studies show the superiority of such an approach over fully connected neural networks and tree-based approaches",0
"Effective feature engineering is crucial in most deep learning applications to ensure their success. This work presents a novel approach that enables encoding sparse interactions among features directly into neural networks. By doing so, we eliminate the need for manually creating complex features and drastically reduce computational cost as well as enhance interpretability of models. Our method outperforms state-of-the-art approaches both quantitatively and qualitatively on several challenging benchmark datasets from different domains such as image recognition, sentiment analysis, biological data classification, text classification, and few shot classification settings. In addition, the introduced module can be applied to any pre-trained model without fine-tuning, allowing users to take advantage of previous advancements in deep learning research. We believe our innovation has great potential to lead to breakthroughs beyond current limitations by facilitating feature engineering via interaction encoding within neural networks.",1
"We present an alternative layer to convolution layers in convolutional neural networks (CNNs). Our approach reduces the complexity of convolutions by replacing it with binary decisions. Those binary decisions are used as indexes to conditional distributions where each weight represents a leaf in a decision tree. This means that only the indices to the weights need to be determined once, thus reducing the complexity of convolutions by the depth of the output tensor. Index computation is performed by simple binary decisions that require fewer cycles compared to conventionally used multiplications. In addition, we show how convolutions can be replaced by binary decisions. These binary decisions form indices in the conditional distributions and we show how they are used to replace 2D weight matrices as well as 3D weight tensors. These new layers can be trained like convolution layers in CNNs based on the backpropagation algorithm, for which we provide a formalization.   Our results on multiple publicly available data sets show that our approach performs similar to conventional neuronal networks. Beyond the formalized reduction of complexity and the improved qualitative performance, we show the runtime improvement empirically compared to convolution layers.",0
"Abstract: Deep learning algorithms have been widely used in computer vision tasks such as image classification and object detection due to their ability to learn complex features automatically from raw pixel inputs. However, traditional deep neural networks, which rely on convolutional layers followed by fully connected ones, tend to suffer from several problems like high computational cost, difficulty in training, lack of interpretability, etc., that restricts their applications to large scale. In recent years there has been growing interest towards using decision trees, random forests as alternatives to achieve comparable accuracy at lower complexity and better explainability. This research focuses on demonstrating experimentally that the performance gap between CNNs and Random Forests trained over ImageNet can be significantly closed with appropriate choices of architecture search space and model design heuristics, resulting into more robust models than ever before seen in the literature. We provide detailed analysis showing how decisions made during these searches influence final model accuracy and generalization under different dataset splits and random seeds confirming benefits of our methodology that takes advantages of both worlds - the simplicity and transparency of Tree ensembles and powerful representation capabilities of DNN architectures. Our findings provide further evidence that tree ensemble approaches should no longer be ignored in modern CV research since they may play significant role in many scenarios e.g. edge devices with limited computing power, healthcare where clear explanations are mandatory, and other safety critical systems where transparency is essential. Overall we aim towards building a bridge between popular convnets and alternative yet interpretable models relying heavily o",1
"In this paper, we introduce a collaborative training algorithm of balanced random forests with convolutional neural networks for domain adaptation tasks. In real scenarios, most domain adaptation algorithms face the challenges from noisy, insufficient training data and open set categorization. In such cases, conventional methods suffer from overfitting and fail to successfully transfer the knowledge of the source to the target domain. To address these issues, the following two techniques are proposed. First, we introduce the optimized decision tree construction method with convolutional neural networks, in which the data at each node are split into equal sizes while maximizing the information gain. It generates balanced decision trees on deep features because of the even-split constraint, which contributes to enhanced discrimination power and reduced overfitting problem. Second, to tackle the domain misalignment problem, we propose the domain alignment loss which penalizes uneven splits of the source and target domain data. By collaboratively optimizing the information gain of the labeled source data as well as the entropy of unlabeled target data distributions, the proposed CoBRF algorithm achieves significantly better performance than the state-of-the-art methods.",0
"This abstract describes a new method for training random forest classifiers that can perform well on open set domain adaptation tasks. The key idea behind our approach is to use collaborative training techniques where multiple models learn from each other by sharing their predictions and gradients during training. We show through experiments on several benchmark datasets that our method outperforms state-of-the-art alternatives both quantitatively and qualitatively. Additionally, we demonstrate how our trained models can effectively adapt to previously unseen classes at test time, significantly improving performance over baseline methods. Overall, this work represents an important contribution towards solving the challenging problem of open set domain adaptation, paving the way for improved generalization capabilities of machine learning algorithms across different domains.",1
"Despite outstanding contribution to the significant progress of Artificial Intelligence (AI), deep learning models remain mostly black boxes, which are extremely weak in explainability of the reasoning process and prediction results. Explainability is not only a gateway between AI and society but also a powerful tool to detect flaws in the model and biases in the data. Local Interpretable Model-agnostic Explanation (LIME) is a recent approach that uses an interpretable model to form a local explanation for the individual prediction result. The current implementation of LIME adopts the linear regression as its interpretable function. However, being so restricted and usually over-simplifying the relationships, linear models fail in situations where nonlinear associations and interactions exist among features and prediction results. This paper implements a decision Tree-based LIME approach, which uses a decision tree model to form an interpretable representation that is locally faithful to the original model. Tree-LIME approach can capture nonlinear interactions among features in the data and creates plausible explanations. Various experiments show that the Tree-LIME explanation of multiple black-box models can achieve more reliable performance in terms of understandability, fidelity, and efficiency.",0
"Title: ""A Comprehensive Analysis of Deep Learning Models""  Abstract: Deep learning models have become increasingly popular due to their ability to achieve state-of-the-art performance on a wide range of tasks such as image classification, speech recognition, and natural language processing. Despite their success, deep learning models remain largely black boxes, making it difficult to interpret their predictions and explain how they make decisions. This lack of transparency raises concerns regarding their trustworthiness and reliability. In order to address these issues, researchers have proposed methods that aim to shed light into the inner workings of these complex systems. One such method involves generating decision trees from trained neural networks to visualize the flow of activation values that lead up to a final prediction. These decision trees can then be used to gain insights into the factors considered by the model during inference. This approach has proven effective in enhancing our understanding of deep learning models; however, there remain limitations and challenges associated with utilizing decision trees for explanation purposes. In this paper, we present an analysis of several recent advancements in explaining the behavior of deep learning models through decision trees, discussing both advantages and disadvantages of each approach. Our study serves as a comprehensive guide for those interested in exploring the inner workings of these powerful yet opaque systems. By providing insights into the strengths and weaknesses of different explanation techniques, we hope to encourage further development and adoption of interpretable deep learning methods in real-world applications.",1
"Boosted ensemble of decision tree (DT) classifiers are extremely popular in international competitions, yet to our knowledge nothing is formally known on how to make them \textit{also} differential private (DP), up to the point that random forests currently reign supreme in the DP stage. Our paper starts with the proof that the privacy vs boosting picture for DT involves a notable and general technical tradeoff: the sensitivity tends to increase with the boosting rate of the loss, for any proper loss. DT induction algorithms being fundamentally iterative, our finding implies non-trivial choices to select or tune the loss to balance noise against utility to split nodes. To address this, we craft a new parametererized proper loss, called the M$\alpha$-loss, which, as we show, allows to finely tune the tradeoff in the complete spectrum of sensitivity vs boosting guarantees. We then introduce \textit{objective calibration} as a method to adaptively tune the tradeoff during DT induction to limit the privacy budget spent while formally being able to keep boosting-compliant convergence on limited-depth nodes with high probability. Extensive experiments on 19 UCI domains reveal that objective calibration is highly competitive, even in the DP-free setting. Our approach tends to very significantly beat random forests, in particular on high DP regimes ($\varepsilon \leq 0.1$) and even with boosted ensembles containing ten times less trees, which could be crucial to keep a key feature of DT models under differential privacy: interpretability.",0
"This paper provides an overview of boosting methods, including gradient boosting machines (GBM) and adaboost algorithms, and how they can be used to create decision tree ensembles that achieve differential privacy guarantees. The authors begin by discussing the fundamentals of differential privacy, emphasizing that decision trees tend to have high variance which makes them well suited for use in privacy preservation techniques. They then explain different ways in which boosting algorithms can help reduce sensitivity of decision trees so as to obtain better privacy guarantees. The remainder of the paper is structured into two main sections. In section one the authors provide an introduction to gradient boosting machines explaining their architecture, strengths, weaknesses and training procedure while highlighting on the difference with Adaboost. Furthermore, they describe the operation of decision stumps, trees, regression and randomization within GBM frameworks. They compare the performance of these base models and discuss scenarios under which each type performs best. Lastly, they cover applications such as classification, ranking, survival analysis and text data classification using Gradient Boosting Machines. In the second section, the authors propose several new ensemble based methods for achieving differentially private decision trees. The proposed methodologies exploit the stability property of the Huber loss function and the sensitivity of individual base trees to generate adversarial noise. Two general approaches were considered: adding noise directly onto the output predictions generated by base learners and preprocessing input features via sensitivity adjustment followed by post-processing. Experiments demonstrate the effectiveness of the propos edmethods. Conclusion remarks summarize the key findings and directions for future research. References to related work and current state of art conclude this pape",1
"In this paper we introduce the DMR -- a prototype-based method and network architecture for deep learning which is using a decision tree (DT)-based inference and synthetic data to balance the classes. It builds upon the recently introduced xDNN method addressing more complex multi-class problems, specifically when classes are highly imbalanced. DMR moves away from a direct decision based on all classes towards a layered DT of pair-wise class comparisons. In addition, it forces the prototypes to be balanced between classes regardless of possible class imbalances of the training data. It has two novel mechanisms, namely i) using a DT to determine the winning class label, and ii) balancing the classes by synthesizing data around the prototypes determined from the available training data. As a result, we improved significantly the performance of the resulting fully explainable DNN as evidenced by the best reported result on the well know benchmark problem Caltech-101 surpassing our own recently published ""world record"". Furthermore, we also achieved another ""world record"" for another very hard benchmark problem, namely Caltech-256 as well as surpassed the results of other approaches on Faces-1999 problem. In summary, we propose a new approach specifically advantageous for imbalanced multi-class problems that achieved two world records on well known hard benchmark problems and the best result on another problem in terms of accuracy. Moreover, DMR offers full explainability, does not require GPUs and can continue to learn from new data by adding new prototypes preserving the previous ones but not requiring full retraining.",0
"This paper presents a new approach to deep machine reasoning using prototype-based deep neural networks (DNNS) with decision tree inference. We propose that DNNS can leverage their ability to learn high-level abstractions from raw data by using prototypes as effective learning units. These prototypes, obtained through clustering techniques, represent key patterns present in the dataset which capture underlying structures such as objects, scenes, and actions. By employing these prototypes as the basic building blocks during both training and test time, we show that state-of-the-art results can be achieved on several challenging benchmarks including VQA 2.0, CLEVR and COGWorld while maintaining interpretability via decision trees. Finally, we evaluate our model through ablation studies that highlight its robustness against different hyperparameters choices. Our work contributes towards bridging the gap between symbolic and connectionist models for deep machine reasoning tasks.",1
"We present an approach to explain the decisions of black box models for image classification. While using the black box to label images, our explanation method exploits the latent feature space learned through an adversarial autoencoder. The proposed method first generates exemplar images in the latent feature space and learns a decision tree classifier. Then, it selects and decodes exemplars respecting local decision rules. Finally, it visualizes them in a manner that shows to the user how the exemplars can be modified to either stay within their class, or to become counter-factuals by ""morphing"" into another class. Since we focus on black box decision systems for image classification, the explanation obtained from the exemplars also provides a saliency map highlighting the areas of the image that contribute to its classification, and areas of the image that push it into another class. We present the results of an experimental evaluation on three datasets and two black box models. Besides providing the most useful and interpretable explanations, we show that the proposed method outperforms existing explainers in terms of fidelity, relevance, coherence, and stability.",0
"This work explores an explanation method that learns image exemplars in latent feature space. By using these learned exemplars to guide attention towards important features, we can generate more intuitive explanations of black box models such as deep neural networks. We evaluate our approach on several visual recognition tasks, demonstrating its effectiveness at producing meaningful and accurate explanations. Our method has important implications for understanding complex artificial intelligence systems and improving transparency in machine learning.",1
"Gradient boosted decision trees (GBDT) is the leading algorithm for many commercial and academic data applications. We give a deep analysis of this algorithm, especially the histogram technique, which is a basis for the regulized distribution with compact support. We present three new modifications. 1) Share memory technique to reduce memory usage. In many cases, it only need the data source itself and no extra memory. 2) Implicit merging for ""merge overflow problem"".""merge overflow"" means that merge some small datasets to huge datasets, which are too huge to be solved. By implicit merging, we just need the original small datasets to train the GBDT model. 3) Adaptive resize algorithm of histogram bins to improve accuracy. Experiments on two large Kaggle competitions verified our methods. They use much less memory than LightGBM and have higher accuracy. We have implemented these algorithms in an open-source package LiteMORT. The source codes are available at https://github.com/closest-git/LiteMORT",0
"Memory efficiency is a critical concern in modern machine learning systems, especially as we seek to scale up models and tasks. One promising approach is to use gradientboosting trees (GBTs), which have achieved state-of-the-art performance in many applications. However, GBTs can require large amounts of memory, especially if they involve complex data structures like full dense matrices or learn large numbers of weak learner functions. In this work, we propose LiteMORT, a novel variation of GBTs that addresses these issues by introducing several key innovations.  Firstly, we observe that traditional implementations of GBTs store all potential splits at each node as a set of candidate indices, but only use a subset of them to form actual decision boundaries. We show that we can instead directly represent these decisions using bit vectors, where bits correspond to particular split candidates. This allows us to save significant space while still enabling exact inference. Moreover, our method automatically handles pruning unused splits during training, further reducing model size without sacrificing accuracy.  Secondly, we leverage recent advances in adaptive histograms to efficiently quantize feature values into compact representations. By doing so, we achieve better storage savings compared to other methods based on uniform quantization or kd-trees. Furthermore, our system incrementally updates these histograms online during tree construction, capturing distribution shifts over time, thus improving adaptation to changing environments.  Experiments demonstrate the effectiveness of our approach across diverse domains, including image classification, natural language processing, and tabular regression problems. Overall, LiteMORT significantly reduces memory usage relative to competitive baselines while maintaining strong predictive power, opening new horizons for deploying powerful ML algorithms under limited resources. Our code and datasets are publicly available to facilitate reproducibility and future research.",1
"The aim of this work is to propose a meta-algorithm for automatic classification in the presence of discrete binary classes. Classifier learning in the presence of overlapping class distributions is a challenging problem in machine learning. Overlapping classes are described by the presence of ambiguous areas in the feature space with a high density of points belonging to both classes. This often occurs in real-world datasets, one such example is numeric data denoting properties of particle decays derived from high-energy accelerators like the Large Hadron Collider (LHC). A significant body of research targeting the class overlap problem use ensemble classifiers to boost the performance of algorithms by using them iteratively in multiple stages or using multiple copies of the same model on different subsets of the input training data. The former is called boosting and the latter is called bagging. The algorithm proposed in this thesis targets a challenging classification problem in high energy physics - that of improving the statistical significance of the Higgs discovery. The underlying dataset used to train the algorithm is experimental data built from the official ATLAS full-detector simulation with Higgs events (signal) mixed with different background events (background) that closely mimic the statistical properties of the signal generating class overlap. The algorithm proposed is a variant of the classical boosted decision tree which is known to be one of the most successful analysis techniques in experimental physics. The algorithm utilizes a unified framework that combines two meta-learning techniques - bagging and boosting. The results show that this combination only works in the presence of a randomization trick in the base learners.",0
"This could be the first line of your abstract: Random forest has been successfully applied to several problems in particle physics, including signal vs background discrimination in LHC data analysis. However, random forest can suffer from overfitting due to variable selection bias that results from bagging of decision trees. In order to mitigate this issue, we introduce a novel algorithm called randomized recursion, which randomly constructs unpruned decision trees recursively without replacement sampling. We apply our method to simulated BSM Higgs boson signals in association with top quarks in proton-proton collisions at 14 TeV and demonstrate improved performance compared to traditional random forest methods.",1
"The goal of the change-point detection is to discover changes of time series distribution. One of the state of the art approaches of the change-point detection are based on direct density ratio estimation. In this work we show how existing algorithms can be generalized using various binary classification and regression models. In particular, we show that the Gradient Boosting over Decision Trees and Neural Networks can be used for this purpose. The algorithms are tested on several synthetic and real-world datasets. The results show that the proposed methods outperform classical RuLSIF algorithm. Discussion of cases where the proposed algorithms have advantages over existing methods are also provided.",0
"In this research paper, we propose a novel method for detecting change points in time series data using direct density ratio estimation. Traditional approaches to change point detection require prior knowledge of the distribution of the data, which can limit their applicability in practice. Our approach overcomes this limitation by directly estimating the densities of both pre-change and post-change segments of the time series, allowing us to accurately identify changes without assuming any specific distributions. Through extensive simulation studies, we demonstrate that our method outperforms existing methods in terms of accuracy and robustness. Furthermore, we apply our method to real-world datasets from different domains and show that it yields promising results in practice. Overall, our work contributes to the development of more generalizable and reliable change point detection techniques for time series analysis.",1
"Particle identification is one of the core tasks in the data analysis pipeline at the Large Hadron Collider (LHC). Statistically, this entails the identification of rare signal events buried in immense backgrounds that mimic the properties of the former. In machine learning parlance, particle identification represents a classification problem characterized by overlapping and imbalanced classes. Boosted decision trees (BDTs) have had tremendous success in the particle identification domain but more recently have been overshadowed by deep learning (DNNs) approaches. This work proposes an algorithm to extract more out of standard boosted decision trees by targeting their main weakness, susceptibility to overfitting. This novel construction harnesses the meta-learning techniques of boosting and bagging simultaneously and performs remarkably well on the ATLAS Higgs (H) to tau-tau data set (ATLAS et al., 2014) which was the subject of the 2014 Higgs ML Challenge (Adam-Bourdarios et al., 2015). While the decay of Higgs to a pair of tau leptons was established in 2018 (CMS collaboration et al., 2017) at the 4.9$\sigma$ significance based on the 2016 data taking period, the 2014 public data set continues to serve as a benchmark data set to test the performance of supervised classification schemes. We show that the score achieved by the proposed algorithm is very close to the published winning score which leverages an ensemble of deep neural networks (DNNs). Although this paper focuses on a single application, it is expected that this simple and robust technique will find wider applications in high energy physics.",0
"This paper describes an exploration into using boosted decision trees (BDTs) as an analysis technique in particle physics at CERN’s Large Hadron Collider (LHC). We investigate two BDT methods that have shown promise elsewhere but which have received relatively little attention within the LHC community. We demonstrate how these can bring significant performance improvements over standard techniques for specific analysis tasks. One application we consider is reconstructing missing transverse momentum, relevant for searches for new particles. Our studies show that both BDT approaches perform better than traditional techniques at low values of missing transverse momentum where most searches focus their efforts. In addition to improving reconstruction accuracy, we explain how our findings allow us to make use of all data recorded by LHC detectors rather than relying on dedicated hardware. Throughout, we emphasise the need for openness with regards code usage, training datasets and model hyperparameters so that other groups may follow our lead. Overall, this work provides evidence that harvesting further scientific value from data via machine learning is possible even at state-of-the art experiments like the LHC. By combining existing expertise and making simple changes in experimental practice, new insights could soon be waiting just beneath the surface of our experimental datasets.",1
"Decision-tree-based ensemble classification methods (DTEMs) are a prevalent tool for supervised anomaly detection. However, due to the continued growth of datasets, DTEMs result in increasing drawbacks such as growing memory footprints, longer training times, and slower classification latencies at lower throughput. In this paper, we present, design, and evaluate RADE - a DTEM-based anomaly detection framework that augments standard DTEM classifiers and alleviates these drawbacks by relying on two observations: (1) we find that a small (coarse-grained) DTEM model is sufficient to classify the majority of the classification queries correctly, such that a classification is valid only if its corresponding confidence level is greater than or equal to a predetermined classification confidence threshold; (2) we find that in these fewer harder cases where our coarse-grained DTEM model results in insufficient confidence in its classification, we can improve it by forwarding the classification query to one of expert DTEM (fine-grained) models, which is explicitly trained for that particular case. We implement RADE in Python based on scikit-learn and evaluate it over different DTEM methods: RF, XGBoost, AdaBoost, GBDT and LightGBM, and over three publicly available datasets. Our evaluation over both a strong AWS EC2 instance and a Raspberry Pi 3 device indicates that RADE offers competitive and often superior anomaly detection capabilities as compared to standard DTEM methods, while significantly improving memory footprint (by up to 5.46x), training-time (by up to 17.2x), and classification latency (by up to 31.2x).",0
"This is an abstract around 150 to 300 words long for a paper titled ""RADE: Resource-Efficient Supervised Anomaly Detection Using Decision Tree-Based Ensemble Methods"". In recent years, anomaly detection has become increasingly important as more data becomes available online. With so many resources at our fingertips, finding ways to efficiently detect anomalies without sacrificing accuracy is crucial. To address this challenge, we propose a new method called RADE that uses decision tree-based ensemble methods to find patterns in large datasets. Our approach outperforms existing state-of-the-art algorithms by achieving higher precision while using fewer resources. We evaluate our method on several benchmark datasets, including KDD Cup '98 and CICIDS2017, demonstrating excellent results across multiple domains. By using decision trees and ensembles, RADE can accurately classify novel attacks, making it ideal for real-world applications where speed and efficiency are critical. Overall, our work represents an important step forward in the field of anomaly detection, helping organizations stay ahead of threats while conserving valuable computing resources.",1
"To ensure the security of the general mass, crime prevention is one of the most higher priorities for any government. An accurate crime prediction model can help the government, law enforcement to prevent violence, detect the criminals in advance, allocate the government resources, and recognize problems causing crimes. To construct any future-oriented tools, examine and understand the crime patterns in the earliest possible time is essential. In this paper, I analyzed a real-world crime and accident dataset of Denver county, USA, from January 2014 to May 2019, which containing 478,578 incidents. This project aims to predict and highlights the trends of occurrence that will, in return, support the law enforcement agencies and government to discover the preventive measures from the prediction rates. At first, I apply several statistical analysis supported by several data visualization approaches. Then, I implement various classification algorithms such as Random Forest, Decision Tree, AdaBoost Classifier, Extra Tree Classifier, Linear Discriminant Analysis, K-Neighbors Classifiers, and 4 Ensemble Models to classify 15 different classes of crimes. The outcomes are captured using two popular test methods: train-test split, and k-fold cross-validation. Moreover, to evaluate the performance flawlessly, I also utilize precision, recall, F1-score, Mean Squared Error (MSE), ROC curve, and paired-T-test. Except for the AdaBoost classifier, most of the algorithms exhibit satisfactory accuracy. Random Forest, Decision Tree, Ensemble Model 1, 3, and 4 even produce me more than 90% accuracy. Among all the approaches, Ensemble Model 4 presented superior results for every evaluation basis. This study could be useful to raise the awareness of peoples regarding the occurrence locations and to assist security agencies to predict future outbreaks of violence in a specific area within a particular time.",0
"This research study examines crime patterns in Denver city using machine learning and data mining techniques. The aim was to identify trends and correlations between different types of crimes committed within the city limits over several years. Using publicly available datasets on criminal incidents reported by authorities, we developed models that predict the likelihood of certain crimes occurring in specific regions of the city at particular times. Our analysis revealed interesting insights into the factors influencing criminal behavior such as socioeconomic indicators, geographical features, seasonality, and environmental variables. In addition, our findings suggest areas where police resources can be better allocated to prevent criminal activities. We conclude that the application of advanced analytics tools like machine learning and data mining has great potential to improve law enforcement efforts and make cities safer.",1
"Current prognostic risk scores in cardiac surgery are based on statistics and do not yet benefit from machine learning. Statistical predictors are not robust enough to correctly identify patients who would benefit from Transcatheter Aortic Valve Implantation (TAVI). This research aims to create a machine learning model to predict one-year mortality of a patient after TAVI. We adopt a modern gradient boosting on decision trees algorithm, specifically designed for categorical features. In combination with a recent technique for model interpretations, we developed a feature analysis and selection stage, enabling to identify the most important features for the prediction. We base our prediction model on the most relevant features, after interpreting and discussing the feature analysis results with clinical experts. We validated our model on 270 TAVI cases, reaching an AUC of 0.83. Our approach outperforms several widespread prognostic risk scores, such as logistic EuroSCORE II, the STS risk score and the TAVI2-score, which are broadly adopted by cardiologists worldwide.",0
"In summary, a study was conducted analyzing mortality rates after transcatheter aortic valve implantation (TAVI). To improve predictive accuracy, gradient boosting machines were employed on top of decision trees as base learners. The results showed that GBMs effectively improved mortality prediction performance compared to using decision trees alone. Additionally, the research provides important insights into the factors most associated with increased mortality risk following TAVI procedures, which can inform treatment decisions and patient management strategies moving forward.",1
"Due to the steadily increasing relevance of machine learning for practical applications, many of which are coming with safety requirements, the notion of uncertainty has received increasing attention in machine learning research in the last couple of years. In particular, the idea of distinguishing between two important types of uncertainty, often refereed to as aleatoric and epistemic, has recently been studied in the setting of supervised learning. In this paper, we propose to quantify these uncertainties with random forests. More specifically, we show how two general approaches for measuring the learner's aleatoric and epistemic uncertainty in a prediction can be instantiated with decision trees and random forests as learning algorithms in a classification setting. In this regard, we also compare random forests with deep neural networks, which have been used for a similar purpose.",0
"""Aleatoric uncertainty refers to the inherent noise present in data. It cannot be reduced by adding more observations since aleatory uncertainty arises from natural variability of the underlying phenomenon."" - This sentence sets up the concept of aleatoric uncertainty well but could use some additional context such as how it relates to epistemic uncertainty (which is reducible through acquiring new knowledge). Maybe consider something like:  ""Aleatoric uncertainty refers to the inherent noise present in data that results from random chance. Despite this limitation, we can still make meaningful predictions using techniques such as random forest ensembles which take into account both aleatoric and reducible epistemic uncertainties.""",1
"This work describes an outlier detection procedure (named ""OutlierTree"") loosely based on the GritBot software developed by RuleQuest research, which works by evaluating and following supervised decision tree splits on variables, in whose branches 1-d confidence intervals are constructed for the target variable and potential outliers flagged according to these confidence intervals. Under this logic, it's possible to produce human-readable explanations for why a given value of a variable in an observation can be considered as outlier, by considering the decision tree branch conditions along with general distribution statistics among the non-outlier observations that fell into the same branch, which can then be contrasted against the value which lies outside the CI. The supervised splits help to ensure that the generated conditions are not spurious, but rather related to the target variable and having logical breakpoints.",0
"Outliers can have significant effects on data analysis and model training. Traditionally, outliers are removed from datasets before further processing. However, recent approaches attempt to detect and handle them without exclusion. One such approach is based on decision trees: they split the dataset based on attribute values until a leaf node isolates all outliers.  This work presents an extension of this approach that conditions decisions in the decision tree on the distribution of the corresponding attributes. By doing so, we provide more insight into which instances are considered outliers. We demonstrate experimentally that our method is capable of identifying outlying instances earlier than existing methods. Our findings contribute to making machine learning models explainable by providing transparency regarding which instances influence predictions and why they might be considered anomalous.",1
"Gradient boosted decision trees (GBDTs) are widely used in machine learning, and the output of current GBDT implementations is a single variable. When there are multiple outputs, GBDT constructs multiple trees corresponding to the output variables. The correlations between variables are ignored by such a strategy causing redundancy of the learned tree structures. In this paper, we propose a general method to learn GBDT for multiple outputs, called GBDT-MO. Each leaf of GBDT-MO constructs predictions of all variables or a subset of automatically selected variables. This is achieved by considering the summation of objective gains over all output variables. Moreover, we extend histogram approximation into multiple output case to speed up the training process. Various experiments on synthetic and real-world datasets verify that GBDT-MO achieves outstanding performance in terms of both accuracy and training speed. Our codes are available on-line.",0
"This should give you ideas on how write your own abstract! Please remember that the text below is fictional and only serves as inspiration, so don’t copy any sentences from it directly. In many real world problems, we have to predict multiple outputs at once, such as predicting both the price and quantity demanded of a product given certain features. While gradient boosted decision trees (GBDT) are powerful tools for single output prediction, they can struggle with predictions involving multiple outputs. To address this limitation, researchers at XYZ company developed GBDT-MO, a modification to traditional GBDT algorithms designed specifically for making predictions involving multiple outputs. By utilizing advanced techniques such as random feature selection, gradient descent optimization, and regularization methods, GBDT-MO achieves state-of-the art performance across a wide range of benchmark datasets. In addition, through rigorous testing and comparison against other popular machine learning algorithms, GBDT-MO has been shown to significantly outperform competitors in terms of accuracy and computational efficiency. As a result, this new method represents a major advance in the field of multi-output regression and has significant potential applications in areas ranging from finance and economics to healthcare and engineering. Overall, GBDT-MO is poised to become an essential tool for data scientists and analysts seeking to make accurate predictions in complex, high dimensional settings involving multiple outputs.",1
"In this study, a predictive model using Multi-layer Perceptron of Artificial Neural Network architecture was developed to predict customer churn in a financial institution. Previous researches have used supervised machine learning classifiers such as Logistic Regression, Decision Tree, Support Vector Machine, K-Nearest Neighbors, and Random Forest. These classifiers require human effort to perform feature engineering which leads to over-specified and incomplete feature selection. Therefore, this research developed a model to eliminate manual feature engineering in data preprocessing stage. Fifty thousand customers? data were extracted from the database of one of the leading financial institution in Nigeria for the study. The multi-layer perceptron model was built with python programming language and used two overfitting techniques (Dropout and L2 regularization). The implementation done in python was compared with another model in Neuro solution infinity software. The results showed that the Artificial Neural Network software development (Python) had comparable performance with that obtained from the Neuro Solution Infinity software. The accuracy rates are 97.53% and 97.4% while ROC (Receiver Operating Characteristic) curve graphs are 0.89 and 0.85 respectively.",0
"Customer churn prediction is essential for financial institutions as it helps them retain customers by identifying potential causes of customer dissatisfaction and taking necessary actions to prevent their departure. One effective method to predict customer churn is through the use of artificial neural networks (ANNs). This study proposes a novel approach that uses backpropagation algorithm for training the network on real-life datasets obtained from banking sectors like credit card, loans, insurance etc. Our results demonstrate that ANN can provide accurate predictions with over 92% accuracy and better performance than traditional statistical methods. We believe that our research could significantly contribute towards reducing customer churn rates across industries resulting in increased profitability and customer satisfaction.",1
"Accurate classification of self-care problems in children who suffer from physical and motor affliction is an important problem in the healthcare industry. This is a difficult and a time consumming process and it needs the expertise of occupational therapists. In recent years, healthcare professionals have opened up to the idea of using expert systems and artificial intelligence in the diagnosis and classification of self care problems. In this study, we propose a new deep learning based approach named Care2Vec for solving these kind of problems and use a real world self care activities dataset that is based on a conceptual framework designed by the World Health Organization (WHO). Care2Vec is a mix of unsupervised and supervised learning where we use Autoencoders and Deep neural networks as a two step modeling process. We found that Care2Vec has a better prediction accuracy than some of the traditional methods reported in the literature for solving the self care classification problem viz. Decision trees and Artificial neural networks.",0
"In this paper we present a deep learning model that can classify different types of self-care problems in physically disabled children. Our goal is to create a tool that parents, caregivers, and medical professionals can use to identify any potential issues early on so that appropriate interventions can be put into place. We focus specifically on problems related to feeding, bathing, dressing, and grooming. To build our model, we used real-world data collected from surveys completed by families of physically disabled children who have been receiving home nursing services. We trained our algorithm using this dataset and tested its accuracy against known ground truth labels. The results showed that our model was able to achieve high levels of accuracy across all four categories of self-care problems. Our work highlights the potential benefits of artificial intelligence (AI) in improving outcomes for children with disabilities, particularly those who require assistance with daily living tasks. Overall, we hope that Care2Vec will prove to be a valuable resource for healthcare providers working with physically disabled children and their families. By leveraging cutting-edge machine learning techniques, we aim to make a positive impact on the lives of these individuals.",1
"The assessment of energy expenditure in real life is of great importance for monitoring the current physical state of people, especially in work, sport, elderly care, health care, and everyday life even. This work reports about application of some machine learning methods (linear regression, linear discriminant analysis, k-nearest neighbors, decision tree, random forest, Gaussian naive Bayes, support-vector machine) for monitoring energy expenditures in athletes. The classification problem was to predict the known level of the in-exercise loads (in three categories by calories) by the heart rate activity features measured during the short period of time (1 minute only) after training, i.e by features of the post-exercise load. The results obtained shown that the post-exercise heart activity features preserve the information of the in-exercise training loads and allow us to predict their actual in-exercise levels. The best performance can be obtained by the random forest classifier with all 8 heart rate features (micro-averaged area under curve value AUCmicro = 0.87 and macro-averaged one AUCmacro = 0.88) and the k-nearest neighbors classifier with 4 most important heart rate features (AUCmicro = 0.91 and AUCmacro = 0.89). The limitations and perspectives of the ML methods used are outlined, and some practical advices are proposed as to their improvement and implementation for the better prediction of in-exercise energy expenditures.",0
"This is the paper: “Prediction of Physical Load Level by Machine Learning Analysis of Heart Activity after Exercises” The authors of the study aimed at exploring whether machine learning analysis could predict physical load level based on heart activity data collected from subjects who performed exercises under different levels of intensity. To achieve their goal, they recruited 24 healthy participants (17 males and seven females) aged between 20 and 36 years old. All individuals gave written consent prior to participating in the experiment. Data was collected using pulse oximeters attached to each subject’s index finger, which recorded changes in blood volume pulse (BVP), heart rate variability (HRV), and photoplethysmography (PPG). BVP provided information regarding cardiac cycle dynamics such as stroke volume, while HRV offered insight into autonomic nervous system functioning. PPG helped determine blood flow under baseline conditions. Additionally, exercise load intensity was evaluated through treadmill tests where workload increased progressively every two minutes throughout the session. Subjects walked until exhaustion – either volitionally quitting before reaching 85% maximum capacity via gas analyzer, experiencing angina pectoris grade II–IIIa, or reaching maximal myocardial ischemia based on ST depression of > 1 mm or positive T wave in ECG during exercise stress test. Afterward, RPE scales were used to estimate perceived exercise exertion experienced during training sessions. During monitoring, all machines provided one record per second for three-minute periods within five-minute intervals. As a result, investigators obtained 72 records per individual participant. They preprocessed information using a program called Kubios HRV software package to detrend the raw signals. Then researchers divided data into four nonoverlapping groups according to exercise bouts corresponding to light (heart rates (HRs) < approximately 110 beat/min); moderate (bouts lasti",1
"We tackle the problem of building explainable recommendation systems that are based on a per-user decision tree, with decision rules that are based on single attribute values. We build the trees by applying learned regression functions to obtain the decision rules as well as the values at the leaf nodes. The regression functions receive as input the embedding of the user's training set, as well as the embedding of the samples that arrive at the current node. The embedding and the regressors are learned end-to-end with a loss that encourages the decision rules to be sparse. By applying our method, we obtain a collaborative filtering solution that provides a direct explanation to every rating it provides. With regards to accuracy, it is competitive with other algorithms. However, as expected, explainability comes at a cost and the accuracy is typically slightly lower than the state of the art result reported in the literature.",0
"Explainable recommendation systems have become increasingly important as consumers demand more transparency from machine learning models. In this paper, we propose using meta decision trees (MDTs) as an explainability mechanism in recommendation systems. MDTs are generated by applying randomization techniques on decision tree ensembles, resulting in interpretable, human-like decisions that can be explained easily. We evaluate the effectiveness of our proposed method through extensive experiments on real datasets and show promising results. Our approach achieves better accuracy than competing methods while providing insights into how recommendations are made. Overall, the use of MDTs provides a simple yet powerful solution for creating transparent and effective recommendation systems.",1
"The paper proposes a new variant of a decision tree, called an Extreme Learning Tree. It consists of an extremely random tree with non-linear data transformation, and a linear observer that provides predictions based on the leaf index where the data samples fall. The proposed method outperforms linear models on a benchmark dataset, and may be a building block for a future variant of Random Forest.",0
"Extreme learning trees (XLTs) have emerged as a powerful tool for machine learning applications that require efficient and accurate decision tree models. Unlike traditional methods such as CART or random forest, XLTs can produce highly interpretable and expressive models while still maintaining state-of-the-art performance on various tasks. This paper provides a comprehensive evaluation of extreme learning trees by analyzing their theoretical properties, algorithmic developments, model selection strategies, and empirical results from real-world datasets. We showcase the strengths and limitations of XLTs through detailed comparisons against other popular methods, highlighting their advantages in terms of accuracy, interpretability, scalability, and computational efficiency. Overall, our findings demonstrate the effectiveness and versatility of extreme learning trees for diverse domains such as classification, regression, feature selection, anomaly detection, and data mining. Future directions and potential extensions of XLTs are discussed to further enhance their applicability and impact in big data environments and beyond.",1
"This paper formulates the problem of building a context-aware predictive model based on user diverse behavioral activities with smartphones. In the area of machine learning and data science, a tree-like model as that of decision tree is considered as one of the most popular classification techniques, which can be used to build a data-driven predictive model. The traditional decision tree model typically creates a number of leaf nodes as decision nodes that represent context-specific rigid decisions, and consequently may cause overfitting problem in behavior modeling. However, in many practical scenarios within the context-aware environment, the generalized outcomes could play an important role to effectively capture user behavior. In this paper, we propose a behavioral decision tree, ""BehavDT"" context-aware model that takes into account user behavior-oriented generalization according to individual preference level. The BehavDT model outputs not only the generalized decisions but also the context-specific decisions in relevant exceptional cases. The effectiveness of our BehavDT model is studied by conducting experiments on individual user real smartphone datasets. Our experimental results show that the proposed BehavDT context-aware model is more effective when compared with the traditional machine learning approaches, in predicting user diverse behaviors considering multi-dimensional contexts.",0
"This is the first time I have written such an abstract so please give me feedback on how i can improve. Also tell me if there's any other data that you need from my paper so that i can provide it. Thanks! This paper presents BehavDT, a behavioral decision tree learning framework designed for building user centric context aware predictive models. Our approach leverages insights from human psychology and cognitive science, along with techniques drawn from machine learning and artificial intelligence to model complex decision making processes at scale. With real world deployment in mind, we focus on scalability, interpretability, explainability, automation and generalization of results across users, tasks, domains and systems. We evaluate our method against state of the art benchmarks on two different datasets consisting of natural language understanding problems (NLU) and text classification tasks (TC), respectively. Experiments show significant performance improvements over the baseline approaches demonstrating the effectiveness of our approach. Additionally, we provide analysis on how different design choices impact the quality of learned models providing guidelines for practitioners deploying our method to their own use cases. Our work provides a foundation for developing more advanced AI agents that learn from observed examples while operating under uncertainty.",1
"Gradient Boosting Decision Trees (GBDTs) have become very successful in recent years, with many awards in machine learning and data mining competitions. There have been several recent studies on how to train GBDTs in the federated learning setting. In this paper, we focus on horizontal federated learning, where data samples with the same features are distributed among multiple parties. However, existing studies are not efficient or effective enough for practical use. They suffer either from the inefficiency due to the usage of costly data transformations such as secret sharing and homomorphic encryption, or from the low model accuracy due to differential privacy designs. In this paper, we study a practical federated environment with relaxed privacy constraints. In this environment, a dishonest party might obtain some information about the other parties' data, but it is still impossible for the dishonest party to derive the actual raw data of other parties. Specifically, each party boosts a number of trees by exploiting similarity information based on locality-sensitive hashing. We prove that our framework is secure without exposing the original record to other parties, while the computation overhead in the training process is kept low. Our experimental studies show that, compared with normal training with the local data of each party, our approach can significantly improve the predictive accuracy, and achieve comparable accuracy to the original GBDT with the data from all parties.",0
"Gradient boosting decision trees have been widely used in machine learning applications due to their high accuracy and interpretability. However, training these models on large datasets can be computationally expensive and time consuming. In order to address this issue, we propose a federated gradient boosting approach that allows multiple devices to work together to train a single model without sharing private data. We evaluate our method using several benchmark datasets and demonstrate that it achieves competitive performance compared to centralized training while providing more privacy protection. Our framework has potential applications in areas such as healthcare, finance, and education where sensitive data needs to be kept confidential. Overall, our work provides a promising direction towards enhancing the scalability and privacy of machine learning algorithms.",1
"In this report we examine the effectiveness of WISER in identification of a chemical culprit during a chemical based Mass Casualty Incident (MCI). We also evaluate and compare Binary Decision Tree (BDT) and Artificial Neural Networks (ANN) using the same experimental conditions as WISER. The reverse engineered set of Signs/Symptoms from the WISER application was used as the training set and 31,100 simulated patient records were used as the testing set. Three sets of simulated patient records were generated by 5%, 10% and 15% perturbation of the Signs/Symptoms of each chemical record. While all three methods achieved a 100% training accuracy, WISER, BDT and ANN produced performances in the range of: 1.8%-0%, 65%-26%, 67%-21% respectively. A preliminary investigation of dimensional reduction using ANN illustrated a dimensional collapse from 79 variables to 40 with little loss of classification performance.",0
"Here’s my first draft at writing an abstract without using the title:  This new research presents a novel approach to rapidly identifying chemical agents used in mass casualty incidents (MCI). This method uses machine learning algorithms trained on data from advanced sensors, providing emergency responders with critical insights that can save lives. With accurate identification of dangerous substances during MCIs, medical teams can better triage victims by prioritizing those exposed to life threatening chemicals. In addition, targeted treatment protocols become possible since symptoms may vary based upon exposure to different substances. By implementing these innovative tools in public health preparedness planning for large scale MCI responses, communities and countries alike gain greater protection against harmful chemical threats posed by terrorist organizations, industrial accidents, or natural disasters involving hazardous materials releases.",1
"We study the robustness verification problem for tree-based models, including decision trees, random forests (RFs) and gradient boosted decision trees (GBDTs). Formal robustness verification of decision tree ensembles involves finding the exact minimal adversarial perturbation or a guaranteed lower bound of it. Existing approaches find the minimal adversarial perturbation by a mixed integer linear programming (MILP) problem, which takes exponential time so is impractical for large ensembles. Although this verification problem is NP-complete in general, we give a more precise complexity characterization. We show that there is a simple linear time algorithm for verifying a single tree, and for tree ensembles, the verification problem can be cast as a max-clique problem on a multi-partite graph with bounded boxicity. For low dimensional problems when boxicity can be viewed as constant, this reformulation leads to a polynomial time algorithm. For general problems, by exploiting the boxicity of the graph, we develop an efficient multi-level verification algorithm that can give tight lower bounds on the robustness of decision tree ensembles, while allowing iterative improvement and any-time termination. OnRF/GBDT models trained on 10 datasets, our algorithm is hundreds of times faster than the previous approach that requires solving MILPs, and is able to give tight robustness verification bounds on large GBDTs with hundreds of deep trees.",0
"This paper presents a novel approach for verifying robustness of tree based models such as decision trees, random forest etc against adversarial attacks. We consider the problem of generating inputs that can fool these models into making incorrect predictions by changing only a small number of input features (e.g., pixels). The key challenge lies in designing efficient methods for searching the high dimensional space of possible modifications while simultaneously evaluating their effectiveness at subverting model performance under different threat models. Existing techniques either rely on computationally expensive search strategies like grid search or genetic algorithms, or heuristics without providing any guarantee of finding optimal perturbations. To address these challenges we develop a new algorithm called GAP (Generative Adversarial Perturbation), which uses generative models trained on natural images to generate high quality adversaries requiring few changes to original inputs. Our empirical evaluation shows that GAP significantly outperforms existing approaches across multiple benchmark datasets, demonstrating the broad applicability of our method. Moreover, analysis suggests adversarial examples generated using our system have a higher cross entropy loss than those generated using alternative white box attack methods indicating they require stronger classifiers",1
Gradient Boosting Decision Tree (GBDT) are popular machine learning algorithms with implementations such as LightGBM and in popular machine learning toolkits like Scikit-Learn. Many implementations can only produce trees in an offline manner and in a greedy manner. We explore ways to convert existing GBDT implementations to known neural network architectures with minimal performance loss in order to allow decision splits to be updated in an online manner and provide extensions to allow splits points to be altered as a neural architecture search problem. We provide learning bounds for our neural network.,0
"""TreeGrad: Transferring Tree Ensembles to Neural Networks"" presents a method that enables the efficient transfer of knowledge from decision trees to neural networks (NN), reducing their reliance on large training datasets while maintaining state-of-the-art accuracy levels. This approach addresses one of the main challenges facing deep learning methods today - the need for extensive amounts of data during model training - by leveraging the strengths of tree ensembles in providing concise, interpretable models. We achieve this goal through TreeGrad, which works alongside any gradient booster by guiding NN updates towards the regions more relevant to the boosted tree ensemble. By doing so, we enable smaller NN architectures without degradation in performance across multiple domains: sentiment analysis, machine translation, question answering, and image classification. In summary, our work makes significant progress toward achieving high-quality NN results using only limited data while keeping them transparent and explainable.",1
"The architectures of deep neural networks (DNN) rely heavily on the underlying grid structure of variables, for instance, the lattice of pixels in an image. For general high dimensional data with variables not associated with a grid, the multi-layer perceptron and deep brief network are often used. However, it is frequently observed that those networks do not perform competitively and they are not helpful for identifying important variables. In this paper, we propose a framework that imposes on blocks of variables a chain structure obtained by step-wise greedy search so that the DNN architecture can leverage the constructed grid. We call this new neural network Deep Variable-Block Chain (DVC). Because the variable blocks are used for classification in a sequential manner, we further develop the capacity of selecting variables adaptively according to a number of regions trained by a decision tree. Our experiments show that DVC outperforms other generic DNNs and other strong classifiers. Moreover, DVC can achieve high accuracy at much reduced dimensionality and sometimes reveals drastically different sets of relevant variables for different regions.",0
"In this paper we present deep variable-block chain (DVC) networks which combines blocks from different architectures into a single network using a dynamic routing mechanism that chooses the best path based on an attention like score. Our approach can switch between both residual and nonresidual connections within each block and adaptively choose the number of channels at each layer. We show through experiments on several datasets including CIFAR-10, ImageNet and Tiny ImageNet that DVC achieves superior performance compared to all other models tested and significantly reduces computational cost while increasing robustness. Additionally our model is able to achieve state-of-the-art performance even when trained without explicit data augmentation and is more parameter efficient than previous methods. Our proposed method shows promise as an effective and flexible alternative to existing convolutional neural network architectures.",1
"This paper presents a basic property of region dividing of ReLU (rectified linear unit) deep learning when new layers are successively added, by which two new perspectives of interpreting deep learning are given. The first is related to decision trees and forests; we construct a deep learning structure equivalent to a forest in classification abilities, which means that certain kinds of ReLU deep learning can be considered as forests. The second perspective is that Haar wavelet represented functions can be approximated by ReLU deep learning with arbitrary precision; and then a general conclusion of function approximation abilities of ReLU deep learning is given. Finally, generalize some of the conclusions of ReLU deep learning to the case of sigmoid-unit deep learning.",0
"This study investigates the application of deep learning techniques, specifically Convolutional Neural Networks (CNN), to image classification tasks using forest ensembles as interpretability methods. Forest ensemble based techniques like Random Forest have been used previously for feature selection and variable importance analysis to gain insights into decision making processes of traditional machine learning models. However, CNNs present unique challenges due to their black box nature and nonlinear activation functions, which makes interpretation difficult. By leveraging state-of-the-art visualization techniques such as Grad-CAM, we demonstrate that features learned by different stages of CNN can be mapped back to specific regions of interest in the input image, providing new insights into interpreting complex networks. Furthermore, our experiments show that these interpretations provide competitive performance on popular benchmark datasets like MNIST and CIFAR-10 compared against more conventional techniques such as global average pooling and fully connected layers commonly found in deep learning literature. We believe this work provides significant advancements towards enabling better understanding and trustworthiness of modern artificial intelligence systems.",1
"Natural gradient has been recently introduced to the field of boosting to enable the generic probabilistic predication capability. Natural gradient boosting shows promising performance improvements on small datasets due to better training dynamics, but it suffers from slow training speed overhead especially for large datasets. We present a replication study of NGBoost(Duan et al., 2019) training that carefully examines the impacts of key hyper-parameters under the circumstance of best-first decision tree learning. We find that with the regularization of leaf number clipping, the performance of NGBoost can be largely improved via a better choice of hyperparameters. Experiments show that our approach significantly beats the state-of-the-art performance on various kinds of datasets from the UCI Machine Learning Repository while still has up to 4.85x speed up compared with the original approach of NGBoost.",0
"This abstract describes a new approach to natural gradient boosting training called RoNGBa (Robustly Optimized Natural Gradient Boosting) that utilizes leaf number clipping to improve robustness during model optimization. Here are some keywords related to RoNGBA: Gradient Boosting, Natural Gradient Boosting, Training Approaches, Model Optimization, Robustness, Leaf Number Clipping.",1
"Gesture recognition and hand motion tracking are important tasks in advanced gesture based interaction systems. In this paper, we propose to apply a sliding windows filtering approach to sample the incoming streams of data from data gloves and a decision tree model to recognize the gestures in real time for a manual grafting operation of a vegetable seedling propagation facility. The sequence of these recognized gestures defines the tasks that are taking place, which helps to evaluate individuals' performances and to identify any bottlenecks in real time. In this work, two pairs of data gloves are utilized, which reports the location of the fingers, hands, and wrists wirelessly (i.e., via Bluetooth). To evaluate the performance of the proposed framework, a preliminary experiment was conducted in multiple lab settings of tomato grafting operations, where multiple subjects wear the data gloves while performing different tasks. Our results show an accuracy of 91% on average, in terms of gesture recognition in real time by employing our proposed framework.",0
"This paper presents a new framework for human hand gesture task recognition that combines dynamic modelling techniques with machine learning algorithms. We use real-time depth maps captured by Microsoft Kinect cameras mounted on robots operating in environments such as manufacturing lines and healthcare facilities. Our approach models the sequence of gestures over time using Gaussian mixture model components and tracks changes in the position and orientation of the hands using Kalman filters. We then train our model using supervised learning techniques, utilizing labeled training data collected from human subjects performing specific tasks. To validate our methodology, we conduct extensive experiments evaluating both accuracy and speed under various conditions, including different robot configurations and workspace sizes. Overall, our results demonstrate significant improvement in recognition performance compared to traditional approaches, and provide insight into how industrial collaborative robots can effectively recognize, interpret, and respond to complex human gestures.",1
"Traditionally, in supervised machine learning, (a significant) part of the available data (usually 50% to 80%) is used for training and the rest for validation. In many problems, however, the data is highly imbalanced in regard to different classes or does not have good coverage of the feasible data space which, in turn, creates problems in validation and usage phase. In this paper, we propose a technique for synthesising feasible and likely data to help balance the classes as well as to boost the performance in terms of confusion matrix as well as overall. The idea, in a nutshell, is to synthesise data samples in close vicinity to the actual data samples specifically for the less represented (minority) classes. This has also implications to the so-called fairness of machine learning. In this paper, we propose a specific method for synthesising data in a way to balance the classes and boost the performance, especially of the minority classes. It is generic and can be applied to different base algorithms, e.g. support vector machine, k-nearest neighbour, deep networks, rule-based classifiers, decision trees, etc. The results demonstrated that: i) a significantly more balanced (and fair) classification results can be achieved; ii) that the overall performance as well as the performance per class measured by confusion matrix can be boosted. In addition, this approach can be very valuable for the cases when the number of actual available labelled data is small which itself is one of the problems of the contemporary machine learning.",0
"This paper proposes a self-adaptive synthetic over-sampling technique for imbalanced classification problems. Imbalanced classification refers to situations where one class dominates the dataset, making it difficult for algorithms to accurately predict rare events. Over-sampling involves increasing the number of instances belonging to the minority class to balance out the distribution. However, existing methods suffer from high computation cost and limited effectiveness due to their static sampling strategies. We present a new algorithm called SASSY (Self-Adaptive Sampling Strategy for eXtreme imbalances) that dynamically adjusts the sampling strategy based on local neighborhood information. Our method adaptively samples more regions near the decision boundary between classes rather than uniformly oversampling all examples from the minority class. Extensive experiments show that our proposed approach significantly improves accuracy across different datasets, model architectures, and evaluation metrics compared to state-of-the-art approaches. In conclusion, SASSY provides a highly effective solution to address imbalance issues in real-world applications while offering computational efficiency advantages.",1
"In this short paper we investigate whether meta-learning techniques can be used to more effectively tune the hyperparameters of machine learning models using successive halving (SH). We propose a novel variant of the SH algorithm (MeSH), that uses meta-regressors to determine which candidate configurations should be eliminated at each round. We apply MeSH to the problem of tuning the hyperparameters of a gradient-boosted decision tree model. By training and tuning our meta-regressors using existing tuning jobs from 95 datasets, we demonstrate that MeSH can often find a superior solution to both SH and random search.",0
"This paper presents a method for tuning the hyperparameters of XGBoost models using a surrogate model approach. We train several XGBoost models on different combinations of hyperparameter values and use these models as our training set for a new surrogate model that learns to predict the accuracy of a given XGBoost model based on its hyperparameters. Our surrogate model can then be used to efficiently search through the space of possible hyperparameters for each specific task, making it more efficient than using random search or grid search methods. We evaluate the effectiveness of our approach by comparing its performance against random search and grid search on multiple datasets and demonstrate improved results over existing methods. By leveraging the capabilities of XGBoost itself in building the surrogate model, we achieve state-of-the-art performance while reducing computational costs.",1
"Fair classification has become an important topic in machine learning research. While most bias mitigation strategies focus on neural networks, we noticed a lack of work on fair classifiers based on decision trees even though they have proven very efficient. In an up-to-date comparison of state-of-the-art classification algorithms in tabular data, tree boosting outperforms deep learning. For this reason, we have developed a novel approach of adversarial gradient tree boosting. The objective of the algorithm is to predict the output $Y$ with gradient tree boosting while minimizing the ability of an adversarial neural network to predict the sensitive attribute $S$. The approach incorporates at each iteration the gradient of the neural network directly in the gradient tree boosting. We empirically assess our approach on 4 popular data sets and compare against state-of-the-art algorithms. The results show that our algorithm achieves a higher accuracy while obtaining the same level of fairness, as measured using a set of different common fairness definitions.",0
"Adversarial machine learning has emerged as a powerful approach to improve model robustness by training on adversarially generated data. One popular technique within the domain of image recognition is gradient tree boosting (GTB). In this study, we propose fair adver-sarial GTB, which leverages group fairness constraints in the genera-tion process. We evaluate our method using public datasets and show that it achieves improved accuracy while maintaining strong guarantees of equity across subpopulations. Our results demonstrate the potential of combining adversarial machine learning with fair-ness constraints to produce high-performing models that serve underprivileged groups without compromising overall accuracy.",1
"Adversarial Robustness Toolbox (ART) is a Python library supporting developers and researchers in defending Machine Learning models (Deep Neural Networks, Gradient Boosted Decision Trees, Support Vector Machines, Random Forests, Logistic Regression, Gaussian Processes, Decision Trees, Scikit-learn Pipelines, etc.) against adversarial threats and helps making AI systems more secure and trustworthy. Machine Learning models are vulnerable to adversarial examples, which are inputs (images, texts, tabular data, etc.) deliberately modified to produce a desired response by the Machine Learning model. ART provides the tools to build and deploy defences and test them with adversarial attacks. Defending Machine Learning models involves certifying and verifying model robustness and model hardening with approaches such as pre-processing inputs, augmenting training data with adversarial samples, and leveraging runtime detection methods to flag any inputs that might have been modified by an adversary. The attacks implemented in ART allow creating adversarial attacks against Machine Learning models which is required to test defenses with state-of-the-art threat models. Supported Machine Learning Libraries include TensorFlow (v1 and v2), Keras, PyTorch, MXNet, Scikit-learn, XGBoost, LightGBM, CatBoost, and GPy. The source code of ART is released with MIT license at https://github.com/IBM/adversarial-robustness-toolbox. The release includes code examples, notebooks with tutorials and documentation (http://adversarial-robustness-toolbox.readthedocs.io).",0
"This paper presents Adversarial Robustness Toolbox (ART), which is a comprehensive library designed for researchers and practitioners working on adversarial machine learning and computer vision tasks. ART provides easy access to state-of-the-art techniques for generating adversarial examples and assessing their robustness against attack methods. With over 26,000 downloads since its initial release, ART has become one of the most widely used toolkits in the field. In addition, the authors have contributed ART as an officially supported package within the popular deep learning framework PyTorch. This article describes version 1.0.0 of ART and includes installation instructions, usage guidelines, code snippets, and case studies showcasing how ART can simplify and accelerate the process of conducting cutting-edge research on adversarial attacks and defenses. Overall, we hope that ART will serve as a valuable resource for the community and promote further advancements in this rapidly evolving area of study.",1
"Time series data are prevalent in electronic health records, mostly in the form of physiological parameters such as vital signs and lab tests. The patterns of these values may be significant indicators of patients' clinical states and there might be patterns that are unknown to clinicians but are highly predictive of some outcomes. Many of these values are also missing which makes it difficult to apply existing methods like decision trees. We propose a recurrent neural network model that reduces overfitting to noisy observations by limiting interactions between features. We analyze its performance on mortality, ICD-9 and AKI prediction from observational values on the Medical Information Mart for Intensive Care III (MIMIC-III) dataset. Our models result in an improvement of 1.1% [p0.01] in AU-ROC for mortality prediction under the MetaVision subset and 1.0% and 2.2% [p0.01] respectively for mortality and AKI under the full MIMIC-III dataset compared to existing state-of-the-art interpolation, embedding and decay-based recurrent models.",0
"Introduction: Electronic health record (EHR) data provides us with vast amounts of structured clinical time-series data that can be used to predict patient outcomes. However, modelling these time-series data comes with challenges due to their highly nonlinear, multivariate, noisy nature. These limitations result from the complex interplay between different physiological signals recorded at high frequency, which makes most existing models overly sensitive to specific features’ presence/absence, interactions between variables, scaling issues etc., making them unreliable in practice. Methods: In this paper we propose a novel approach to mitigating such problems via spatio-temporal restriction techniques on feature interactions in deep neural network architectures. Our methodology seeks to identify temporally dependent but spatially independent patterns across the entire dataset, which captures both static relationships and dynamics at multiple scales between features that may change at every point in time throughout each subject. Results: We evaluated our methods using three large publicly available benchmark datasets (MIMIC-III & STITCH), covering various critical care scenarios such as sepsis diagnosis and severe acute kidney injury (AKI). Across all experiments, we consistently observed significant improvements (p<0.05) over state-of-the-art baseline algorithms that use raw feature representations. Conclusions: Our findings demonstrate the effectiveness of constraining the model capacity via temporal feature interaction restrictions towards robust representation learning for superior task performance. Such approaches have great potential for addressing the current gap in translational medicine where reliable automatic predictions remain elusive in many real world applications despite having access to large volumes o",1
"Machine learning classifiers often stumble over imbalanced datasets where classes are not equally represented. This inherent bias towards the majority class may result in low accuracy in labeling minority class. Imbalanced learning is prevalent in many real-world applications, such as medical research, network intrusion detection, and fraud detection in credit card transactions, etc. A good number of research works have been reported to tackle this challenging problem. For example, Synthetic Minority Over-sampling TEchnique (SMOTE) and ADAptive SYNthetic sampling approach (ADASYN) use oversampling techniques to balance the skewed datasets. In this paper, we propose a novel method that combines a Weighted Oversampling Technique and ensemble Boosting method (WOTBoost) to improve the classification accuracy of minority data without sacrificing the accuracy of the majority class. WOTBoost adjusts its oversampling strategy at each round of boosting to synthesize more targeted minority data samples. The adjustment is enforced using a weighted distribution. We compare WOTBoost with other four classification models (i.e., decision tree, SMOTE + decision tree, ADASYN + decision tree, SMOTEBoost) extensively on 18 public accessible imbalanced datasets. WOTBoost achieves the best G mean on 6 datasets and highest AUC score on 7 datasets.",0
"Abstract: This article presents a new oversampling technique called ""Weighted Oversampling"" (WO) for use with boosting algorithms. Imbalanced datasets can cause problems for traditional machine learning methods because they tend to have more instances from one class than another. Oversampling techniques like random under sampling (RU), Synthetic Minority Over-samplling Technique (SMOTE) over-sample the minority class by duplicating some examples. They can result in high computation complexity which hampers their applicability in real world applications. Our work introduces a new algorithm that leverages both synthetic data generation (like SMOTE) and reweighting of already generated data to increase the number of samples belonging to a specific class without adding noise to the dataset. Experiments show that our method outperforms existing state-of-the-art techniques on several benchmark datasets. In addition, we demonstrate how our method can be used as part of a larger framework, such as Gradient Boosting Classifier and XGBoost to improve performance across multiple metrics. Finally, we present our method as a standalone application capable of performing well in imbalance learning tasks.",1
"In this paper we propose a method to build a neural network that is similar to an ensemble of decision trees. We first illustrate how to convert a learned ensemble of decision trees to a single neural network with one hidden layer and an input transformation. We then relax some properties of this network such as thresholds and activation functions to train an approximately equivalent decision tree ensemble. The final model, Hammock, is surprisingly simple: a fully connected two layers neural network where the input is quantized and one-hot encoded. Experiments on large and small datasets show this simple method can achieve performance similar to that of Gradient Boosted Decision Trees.",0
"In the last decade, artificial intelligence has been increasingly used to improve our understanding of neural networks. Gradient boosting decision trees are one such method, wherein multiple decision trees are trained sequentially on subsets of the original data, each attempting to correct errors made by previous models. These can then be combined into a single predictor using gradient descent. This approach improves model accuracy as well as interpretability. However, some limitations remain. For example, the choice of hyperparameters for tuning remains a challenge. Nonetheless, the results indicate that gradient boosting decision tree neural networks have tremendous potential for both academia and industry alike.",1
"Interpretable classifiers have recently witnessed an increase in attention from the data mining community because they are inherently easier to understand and explain than their more complex counterparts. Examples of interpretable classification models include decision trees, rule sets, and rule lists. Learning such models often involves optimizing hyperparameters, which typically requires substantial amounts of data and may result in relatively large models. In this paper, we consider the problem of learning compact yet accurate probabilistic rule lists for multiclass classification. Specifically, we propose a novel formalization based on probabilistic rule lists and the minimum description length (MDL) principle. This results in virtually parameter-free model selection that naturally allows to trade-off model complexity with goodness of fit, by which overfitting and the need for hyperparameter tuning are effectively avoided. Finally, we introduce the Classy algorithm, which greedily finds rule lists according to the proposed criterion. We empirically demonstrate that Classy selects small probabilistic rule lists that outperform state-of-the-art classifiers when it comes to the combination of predictive performance and interpretability. We show that Classy is insensitive to its only parameter, i.e., the candidate set, and that compression on the training set correlates with classification performance, validating our MDL-based selection criterion.",0
"This paper presents a new method for interpretability in multiclass classification based on a novel algorithm called Minimum Description Length (MDL)-based rule lists. While traditional machine learning models can produce high accuracy results, they often struggle with providing interpretable insights into how they arrived at their predictions. Our proposed approach overcomes these limitations by using human-readable rules that capture important features of each class. We demonstrate our method on several datasets and show that we achieve comparable performance to state-of-the-art algorithms while also producing easily explainable results. In addition to enhancing transparency in model predictions, our algorithm also enables users to better understand complex data relationships. Overall, our work advances the field of interpreted machine learning and has applications in fields such as medical diagnosis, fraud detection, and recommendation systems.",1
"The problem of adversarial robustness has been studied extensively for neural networks. However, for boosted decision trees and decision stumps there are almost no results, even though they are widely used in practice (e.g. XGBoost) due to their accuracy, interpretability, and efficiency. We show in this paper that for boosted decision stumps the \textit{exact} min-max robust loss and test error for an $l_\infty$-attack can be computed in $O(T\log T)$ time per input, where $T$ is the number of decision stumps and the optimal update step of the ensemble can be done in $O(n^2\,T\log T)$, where $n$ is the number of data points. For boosted trees we show how to efficiently calculate and optimize an upper bound on the robust loss, which leads to state-of-the-art robust test error for boosted trees on MNIST (12.5% for $\epsilon_\infty=0.3$), FMNIST (23.2% for $\epsilon_\infty=0.1$), and CIFAR-10 (74.7% for $\epsilon_\infty=8/255$). Moreover, the robust test error rates we achieve are competitive to the ones of provably robust convolutional networks. The code of all our experiments is available at http://github.com/max-andr/provably-robust-boosting",0
"Deep learning models have shown impressive performance on complex tasks but they can often suffer from adversarial attacks that cause misclassifications even if imperceptible changes are made to inputs. In order to mitigate these attacks and improve robustness of deep learning models, we propose using boosted decision stumps and trees as base learners, which can provide interpretable and robust predictions. Our method utilizes randomization techniques during training time attack detection, making it difficult for attack algorithms to craft effective perturbations. We evaluate our proposed approach on several benchmark datasets including CIFAR-10, ImageNet, and MNIST and show significant improvements in model robustness against state-of-the-art adversaries while maintaining high accuracy on clean data. Overall, our work provides insights into improving the reliability of deep learning systems under adversarial attacks and demonstrates the potential benefits of using simple base learners to achieve robust predictions.",1
"Machine learning has proved to be very successful for making predictions in travel behavior modeling. However, most machine-learning models have complex model structures and offer little or no explanation as to how they arrive at these predictions. Interpretations about travel behavior models are essential for decision makers to understand travelers' preferences and plan policy interventions accordingly. Therefore, this paper proposes to apply and extend the model distillation approach, a model-agnostic machine-learning interpretation method, to explain how a black-box travel mode choice model makes predictions for the entire population and subpopulations of interest. Model distillation aims at compressing knowledge from a complex model (teacher) into an understandable and interpretable model (student). In particular, the paper integrates model distillation with market segmentation to generate more insights by accounting for heterogeneity. Furthermore, the paper provides a comprehensive comparison of student models with the benchmark model (decision tree) and the teacher model (gradient boosting trees) to quantify the fidelity and accuracy of the students' interpretations.",0
"This is not strictly speaking an abstract as I am presenting it below but please bear with me: The purpose of this research work was twofold, firstly it seeks to distil a black box travel mode choice model so that the underlying relationships which shape individual travel choices can be exposed and interpreted more meaningfully; secondly it aimed to contribute to wider methodological advancements in transport modelling through replicability testing, sensitivity analysis of model outputs against changes in input values and calibration using log-sum expenditure models. The research focused on travel patterns within one metropolitan area in Australia over a six week period and utilised household based survey data augmented by GPS tracking data for each participating member of those households. Through detailed descriptive analysis of the collected datasets it was discovered that there were numerous inconsistencies across both sets of data relating to activity participation rates, trip generation rates and duration. It became clear that without resolving these issues through targeted future collection efforts, any attempt at developing further advanced models would have limited success. The implications from this work suggest that rather than simply seeking refinement of existing methods the field needs to shift towards innovative mixed-method approaches where data quality considerations underpin all stages of the research process if we wish to generate high quality inputs into behavioural interpretive frameworks.",1
"Stochastic Gradient Boosting (SGB) is a widely used approach to regularization of boosting models based on decision trees. It was shown that, in many cases, random sampling at each iteration can lead to better generalization performance of the model and can also decrease the learning time. Different sampling approaches were proposed, where probabilities are not uniform, and it is not currently clear which approach is the most effective. In this paper, we formulate the problem of randomization in SGB in terms of optimization of sampling probabilities to maximize the estimation accuracy of split scoring used to train decision trees. This optimization problem has a closed-form nearly optimal solution, and it leads to a new sampling technique, which we call Minimal Variance Sampling (MVS). The method both decreases the number of examples needed for each iteration of boosting and increases the quality of the model significantly as compared to the state-of-the art sampling methods. The superiority of the algorithm was confirmed by introducing MVS as a new default option for subsampling in CatBoost, a gradient boosting library achieving state-of-the-art quality on various machine learning tasks.",0
"In recent years there has been increasing interest in developing new techniques that can reduce variance in stochastic gradient boosting (SGB) models. Traditional SGB methods suffer from high variances which make their predictions unstable and hard to interpret. This has led researchers to explore different sampling schemes that can improve the stability and generalization performance of these models. One such method is minimal variance sampling, which selects only those features that have the largest reduction in uncertainty when added to the model at each iteration. By doing so, we force the algorithm to focus on the most informative features at each step, reducing noise and improving the overall quality of our predictions. In this paper, we demonstrate empirically the advantages of using minimal variance sampling over traditional random subsampling, in terms of accuracy, feature importance measures, as well as computational efficiency. We show that by using minimal variance sampling, we can achieve superior results in both synthetic and real world datasets across multiple domains, including computer vision, natural language processing and time series prediction tasks, without compromising interpretability. Our findings suggest that minimal variance sampling should become a standard technique in future applications of SGB algorithms.",1
"This study is motivated by the magnitude of the problem of Louisiana high school dropout and its negative impacts on individual and public well-being. Our goal is to predict students who are at risk of high school dropout, by examining Louisiana administrative dataset. Due to the imbalanced nature of the dataset, imbalanced learning techniques including resampling, case weighting, and cost-sensitive learning have been applied to enhance the prediction performance on the rare class. Performance metrics used in this study are F-measure, recall and precision of the rare class. We compare the performance of several machine learning algorithms such as neural networks, decision trees and bagging trees in combination with the imbalanced learning approaches using an administrative dataset of size of 366k+ from Louisiana Department of Education. Experiments show that application of imbalanced learning methods produces good results on recall but decreases precision, whereas base classifiers without regard of imbalanced data handling gives better precision but poor recall. Overall application of imbalanced learning techniques is beneficial, yet more studies are desired to improve precision.",0
"This paper proposes using imbalance learning techniques to predict public high school dropouts in Louisiana. We explore two approaches: oversampling the minority class (dropouts) versus undersampling the majority class (students who graduate). Our results demonstrate that imbalance learning can improve accuracy compared to traditional machine learning algorithms, and we provide evidence on how different model selection metrics behave in the presence of highly unbalanced data sets. Based on our experiments, we conclude that more research needs to be done on how better education policies could reduce both dropout rates overall and any racial disparities. We believe that these findings have important implications beyond just improving machine learning models for imbalanced classification problems. As such, we suggest further investigation into the social contexts surrounding these students’ decision making processes as well as their educational experience so as to inform policy that would support all students toward success.",1
"We address temporal localization of events in large-scale video data, in the context of the Youtube-8M Segments dataset. This emerging field within video recognition can enable applications to identify the precise time a specified event occurs in a video, which has broad implications for video search. To address this we present two separate approaches: (1) a gradient boosted decision tree model on a crafted dataset and (2) a combination of deep learning models based on frame-level data, video-level data, and a localization model. The combinations of these two approaches achieved 5th place in the 3rd Youtube-8M video recognition challenge.",0
"In recent years, large-scale video data has become increasingly prevalent due to advancements in image and video capture technology. This abundance of data presents numerous opportunities for researchers and practitioners alike, but simultaneously poses significant challenges that must be addressed before these opportunities can be fully realized. One such challenge lies in effectively localizing temporal events within individual videos or across multiple videos. Addressing this issue requires not only developing new approaches to event detection and temporal alignment, but also accounting for factors like varying camera viewpoints, illumination changes, and other sources of variability. In this work, we present our approach towards learning to localize temporal events in large-scale video data. Our method leverages deep neural networks to perform event segmentation and then combines these segments into coherent events using temporal pyramid pooling. We demonstrate the effectiveness of our approach on several benchmark datasets and evaluate its performance against state-of-the-art methods. Overall, this study contributes to a growing body of literature focused on addressing the challenges presented by large-scale video data and provides valuable insights for future research in this area.",1
"Models often need to be constrained to a certain size for them to be considered interpretable. For example, a decision tree of depth 5 is much easier to understand than one of depth 50. Limiting model size, however, often reduces accuracy. We suggest a practical technique that minimizes this trade-off between interpretability and classification accuracy. This enables an arbitrary learning algorithm to produce highly accurate small-sized models. Our technique identifies the training data distribution to learn from that leads to the highest accuracy for a model of a given size.   We represent the training distribution as a combination of sampling schemes. Each scheme is defined by a parameterized probability mass function applied to the segmentation produced by a decision tree. An Infinite Mixture Model with Beta components is used to represent a combination of such schemes. The mixture model parameters are learned using Bayesian Optimization. Under simplistic assumptions, we would need to optimize for $O(d)$ variables for a distribution over a $d$-dimensional input space, which is cumbersome for most real-world data. However, we show that our technique significantly reduces this number to a \emph{fixed set of eight variables} at the cost of relatively cheap preprocessing. The proposed technique is flexible: it is \emph{model-agnostic}, i.e., it may be applied to the learning algorithm for any model family, and it admits a general notion of model size. We demonstrate its effectiveness using multiple real-world datasets to construct decision trees, linear probability models and gradient boosted models with different sizes. We observe significant improvements in the F1-score in most instances, exceeding an improvement of $100\%$ in some cases.",0
This paper presents an approach that combines interpretability with accurate small models. We propose using gradient boosting machines (GBMs) as they offer high accuracy while allowing inspection of individual trees used for predictions. Our method builds on previous work which showed that adding random noise to decision tree nodes during training helps to reduce overfitting but found that this hurts performance due to loss of explanatory power of each node. Instead we use student's t distribution noise injection into leaf values. This offers better tradeoff between model complexity control and interpretability than traditional methods. We show improved outcomes compared to prior art in experiments on benchmark datasets.,1
"Low-dimensional probability models for local distribution functions in a Bayesian network include decision trees, decision graphs, and causal independence models. We describe a new probability model for discrete Bayesian networks, which we call an embedded Bayesian network classifier or EBNC. The model for a node $Y$ given parents $\bf X$ is obtained from a (usually different) Bayesian network for $Y$ and $\bf X$ in which $\bf X$ need not be the parents of $Y$. We show that an EBNC is a special case of a softmax polynomial regression model. Also, we show how to identify a non-redundant set of parameters for an EBNC, and describe an asymptotic approximation for learning the structure of Bayesian networks that contain EBNCs. Unlike the decision tree, decision graph, and causal independence models, we are unaware of a semantic justification for the use of these models. Experiments are needed to determine whether the models presented in this paper are useful in practice.",0
"In this paper, we present an overview of embedded Bayesian network classifiers (BNCs) and how they can be used as powerful machine learning algorithms for predictive modeling tasks across different domains. We begin by providing background on BNCs, highlighting their key advantages compared to traditional statistical methods and explaining why they are increasingly popular among data scientists. Next, we discuss some common applications of BNCs, including fraud detection in credit card transactions, image recognition in computer vision systems, and anomaly detection in sensor networks. Finally, we outline several software packages available for implementing these techniques in practice, along with tips for choosing the appropriate tool depending on your project requirements. Overall, our goal is to provide readers with a comprehensive understanding of BNCs and their role in modern data science research.",1
"In this paper we analyze, evaluate, and improve the performance of training Random Forest (RF) models on modern CPU architectures. An exact, state-of-the-art binary decision tree building algorithm is used as the basis of this study. Firstly, we investigate the trade-offs between using different tree building algorithms, namely breadth-first-search (BFS) and depth-search-first (DFS). We design a novel, dynamic, hybrid BFS-DFS algorithm and demonstrate that it performs better than both BFS and DFS, and is more robust in the presence of workloads with different characteristics. Secondly, we identify CPU performance bottlenecks when generating trees using this approach, and propose optimizations to alleviate them. The proposed hybrid tree building algorithm for RF is implemented in the Snap Machine Learning framework, and speeds up the training of RFs by 7.8x on average when compared to state-of-the-art RF solvers (sklearn, H2O, and xgboost) on a range of datasets, RF configurations, and multi-core CPU architectures.",0
"Our method improves the accuracy of random forest regression by training each tree deeply on whatever data remains unused by previously grown trees, using a greedy breadth-first search algorithm. We show that our approach results in higher predictive performance across most metrics compared with state-of-the-art alternatives (bias, Gini-Simpson diversity index, mean decrease improvement statistics).",1
"Adversarial example generation becomes a viable method for evaluating the robustness of a machine learning model. In this paper, we consider hard-label black-box attacks (a.k.a. decision-based attacks), which is a challenging setting that generates adversarial examples based on only a series of black-box hard-label queries. This type of attacks can be used to attack discrete and complex models, such as Gradient Boosting Decision Tree (GBDT) and detection-based defense models. Existing decision-based attacks based on iterative local updates often get stuck in a local minimum and fail to generate the optimal adversarial example with the smallest distortion. To remedy this issue, we propose an efficient meta algorithm called BOSH-attack, which tremendously improves existing algorithms through Bayesian Optimization (BO) and Successive Halving (SH). In particular, instead of traversing a single solution path when searching an adversarial example, we maintain a pool of solution paths to explore important regions. We show empirically that the proposed algorithm converges to a better solution than existing approaches, while the query count is smaller than applying multiple random initializations by a factor of 10.",0
"Title your abstract as ""Abstract"" ---",1
"Sleep staging is a crucial task for diagnosing sleep disorders. It is tedious and complex as it can take a trained expert several hours to annotate just one patient's polysomnogram (PSG) from a single night. Although deep learning models have demonstrated state-of-the-art performance in automating sleep staging, interpretability which defines other desiderata, has largely remained unexplored. In this study, we propose Sleep staging via Prototypes from Expert Rules (SLEEPER), which combines deep learning models with expert defined rules using a prototype learning framework to generate simple interpretable models. In particular, SLEEPER utilizes sleep scoring rules and expert defined features to derive prototypes which are embeddings of PSG data fragments via convolutional neural networks. The final models are simple interpretable models like a shallow decision tree defined over those phenotypes. We evaluated SLEEPER using two PSG datasets collected from sleep studies and demonstrated that SLEEPER could provide accurate sleep stage classification comparable to human experts and deep neural networks with about 85% ROC-AUC and .7 kappa.",0
"In this paper we aim to create an artificial intelligence (AI) system that can automatically stage sleep based on electroencephalogram (EEG) signals. Our approach involves using prototyping algorithms to interpret EEG data, which is then compared against expert rule-based systems used by human experts to stage sleep. We show that our AI system achieves comparable results to those achieved by human experts while providing greater interpretability and efficiency. Overall, our work represents a significant advancement in the field of sleep staging technology, with potential applications ranging from clinical diagnosis to research studies. ----- This paper presents an AI system called SLEEPER that stages sleep based on EEG signals using prototyping algorithms. The performance of SLEEPER was compared against expert rule-based systems and showed comparable results, while offering advantages such as increased interpretability and speed. These findings have implications for fields such as clinical diagnosis and sleep research.",1
"Facial expression recognition is a topic of great interest in most fields from artificial intelligence and gaming to marketing and healthcare. The goal of this paper is to classify images of human faces into one of seven basic emotions. A number of different models were experimented with, including decision trees and neural networks before arriving at a final Convolutional Neural Network (CNN) model. CNNs work better for image recognition tasks since they are able to capture spacial features of the inputs due to their large number of filters. The proposed model consists of six convolutional layers, two max pooling layers and two fully connected layers. Upon tuning of the various hyperparameters, this model achieved a final accuracy of 0.60.",0
"Title: ""Facial Emotion Recognition Using Convolutional Neural Networks""  This research presents a method for facial emotion recognition using convolutional neural networks (CNN). The proposed method utilizes a pre-trained CNN model and fine tunes it on a dataset of facial expressions to recognize emotions such as happiness, sadness, anger, surprise, disgust, fear, and neutrality. The experimental results demonstrate that the proposed approach achieves high accuracy in recognizing different emotional states from facial expressions. The study contributes to the field of affective computing by providing a reliable and efficient solution for automated facial emotion recognition. This work has potential applications in areas such as mental health monitoring, market research, and human-computer interaction. Overall, the findings suggest that CNN-based models can effectively capture subtle variations in facial expressions and provide valuable insights into human emotions.",1
"One obstacle that so far prevents the introduction of machine learning models primarily in critical areas is the lack of explainability. In this work, a practicable approach of gaining explainability of deep artificial neural networks (NN) using an interpretable surrogate model based on decision trees is presented. Simply fitting a decision tree to a trained NN usually leads to unsatisfactory results in terms of accuracy and fidelity. Using L1-orthogonal regularization during training, however, preserves the accuracy of the NN, while it can be closely approximated by small decision trees. Tests with different data sets confirm that L1-orthogonal regularization yields models of lower complexity and at the same time higher fidelity compared to other regularizers.",0
"Title: ""Enhancing Decision Tree Based Interpretability of Deep Neural Networks via L1-Orthogonal Regularization""  Abstract: This study proposes a novel approach to enhancing decision tree based interpretation of deep neural networks (DNNs) by introducing L1-orthogonal regularization. DNNs have been widely used due to their high accuracy on complex tasks but suffer from interpretability issues which hinder adoption in critical applications such as medicine or finance. Interpreting DNN predictions remains challenging even after decades of research. Recent studies propose different techniques to explain DNN outputs; however, most focus on visualizing neuron activations or simplifying the model structure. In contrast, we enhance decision trees as a popular posthoc method that generates human-readable rule sets directly interpretable by humans. Our contributions address several shortcomings existing methods face: 1) We introduce L1-orthogonal constraints during training to reduce redundant connections between neurons. This results in simpler decision trees that generalize better across multiple datasets compared to prior arts. 2) Additionally, our framework preserves crucial interdependencies between features explicitly captured by the initial DNN, even if they were initially latent. Experimental evaluation using synthetic benchmarks shows how our approach significantly reduces complexity and strengthens feature importance correlations without sacrificing predictive performance. We demonstrate the benefits of L1-orthogonal regularization on standard classification benchmarks like MNIST and CIFAR10. Code is available at [insert repository URL].  Keywords: Deep neural network, Interpretability, Posthoc explanation, Orthogonality, L1 regularization",1
"Decision forests, including Random Forests and Gradient Boosting Trees, have recently demonstrated state-of-the-art performance in a variety of machine learning settings. Decision forests are typically ensembles of axis-aligned decision trees; that is, trees that split only along feature dimensions. In contrast, many recent extensions to decision forests are based on axis-oblique splits. Unfortunately, these extensions forfeit one or more of the favorable properties of decision forests based on axis-aligned splits, such as robustness to many noise dimensions, interpretability, or computational efficiency. We introduce yet another decision forest, called ""Sparse Projection Oblique Randomer Forests"" (SPORF). SPORF uses very sparse random projections, i.e., linear combinations of a small subset of features. SPORF significantly improves accuracy over existing state-of-the-art algorithms on a standard benchmark suite for classification with 100 problems of varying dimension, sample size, and number of classes. To illustrate how SPORF addresses the limitations of both axis-aligned and existing oblique decision forest methods, we conduct extensive simulated experiments. SPORF typically yields improved performance over existing decision forests, while mitigating computational efficiency and scalability and maintaining interpretability. SPORF can easily be incorporated into other ensemble methods such as boosting to obtain potentially similar gains.",0
"Here is the first iteration, at ~250 words: This work focuses on designing better random decision trees and ensembles thereof via an efficient method we call Sparsified Projected Orthogonal Randomization (SPOR). To improve the accuracy and stability of random decision forests, SPOR learns collections of diverse trees that have more accurate splits and less redundancy. In addition, SPOR improves the interpretability of tree models by increasing their sparsity. We show that our algorithm outperforms several benchmark methods across six real world datasets from gene expression profiling, bank credit scoring, image classification, text categorization and sensor network localization problems. Our results demonstrate both statistically significant improvements as well as practically meaningful reductions in predictive errors. We believe these advances lead to new opportunities for machine learning researchers and application builders seeking to leverage modern computing hardware.",1
"This paper introduces a new classification tool named Silas, which is built to provide a more transparent and dependable data analytics service. A focus of Silas is on providing a formal foundation of decision trees in order to support logical analysis and verification of learned prediction models. This paper describes the distinct features of Silas: The Model Audit module formally verifies the prediction model against user specifications, the Enforcement Learning module trains prediction models that are guaranteed correct, the Model Insight and Prediction Insight modules reason about the prediction model and explain the decision-making of predictions. We also discuss implementation details ranging from programming paradigm to memory management that help achieve high-performance computation.",0
"This paper introduces ""Silas"", a system that enables high performance machine learning while ensuring explainability and verifiability. Current machine learning systems often trade off these desirable properties for accuracy or speed. However, recent advances have shown that models can still perform well even if they are interpretable and explainable. Silas builds upon these findings by implementing techniques such as feature attributions and sensitivity analyses that allow users to gain insight into how the model arrived at its decision. Furthermore, we demonstrate through extensive experimentation that our approach results in state-of-the-art performance across multiple tasks and domains. Finally, we provide mechanisms for verifying the correctness of the outputs produced by the system, providing additional confidence in the model's predictions. Overall, Silas represents a significant step forward in making machine learning more accessible, transparent, and accountable to users, while maintaining competitive performance standards.",1
"Wisdom of the crowd revealed a striking fact that the majority answer from a crowd is often more accurate than any individual expert. We observed the same story in machine learning--ensemble methods leverage this idea to combine multiple learning algorithms to obtain better classification performance. Among many popular examples is the celebrated Random Forest, which applies the majority voting rule in aggregating different decision trees to make the final prediction. Nonetheless, these aggregation rules would fail when the majority is more likely to be wrong. In this paper, we extend the idea proposed in Bayesian Truth Serum that ""a surprisingly more popular answer is more likely the true answer"" to classification problems. The challenge for us is to define or detect when an answer should be considered as being ""surprising"". We present two machine learning aided methods which aim to reveal the truth when it is minority instead of majority who has the true answer. Our experiments over real-world datasets show that better classification performance can be obtained compared to always trusting the majority voting. Our proposed methods also outperform popular ensemble algorithms. Our approach can be generically applied as a subroutine in ensemble methods to replace majority voting rule.",0
"Abstract: Artificial intelligence (AI) has become increasingly prevalent in modern society, particularly as chatbots assisting users in different industries such as customer service. However, these systems are often trained using supervised learning techniques that rely on large amounts of labeled data. This training process may lead to errors or biases within the system which could result in incorrect or harmful responses. In our proposed method, we present a novel approach using natural language processing and machine learning algorithms to detect deceptive intent or unethical behavior within user inputs. By incorporating human values into AI decision making processes, our algorithm acts as a ""truth serum"" for virtual agents, ensuring they provide accurate and reliable responses while maintaining high levels of professionalism and respect towards users. Our experiment results demonstrate significant improvement compared to state-of-the-art methods, validating our method's effectiveness in enhancing the ethics and trustworthiness of virtual agent interactions.",1
"We show that Residual Networks (ResNet) is equivalent to boosting feature representation, without any modification to the underlying ResNet training algorithm. A regret bound based on Online Gradient Boosting theory is proved and suggests that ResNet could achieve Online Gradient Boosting regret bounds through neural network architectural changes with the addition of a shrinkage parameter in the identity skip-connections and using residual modules with max-norm bounds. Through this relation between ResNet and Online Boosting, novel feature representation boosting algorithms can be constructed based on altering residual modules. We demonstrate this through proposing decision tree residual modules to construct a new boosted decision tree algorithm and demonstrating generalization error bounds for both approaches; relaxing constraints within BoostResNet algorithm to allow it to be trained in an out-of-core manner. We evaluate convolution ResNet with and without shrinkage modifications to demonstrate its efficacy, and demonstrate that our online boosted decision tree algorithm is comparable to state-of-the-art offline boosted decision tree algorithms without the drawback of offline approaches.",0
"Title: On the Connection Between Residual Networks and Boosting Algorithms  Research in computer vision has been revolutionized by deep neural networks, which have achieved state-of-the-art performance on many benchmark tasks. One class of neural network architectures that has contributed significantly to these advances is residual networks (ResNets). These networks have shown exceptional performance across image classification, object detection, and other task domains, but their training dynamics remain poorly understood compared to other models such as convolutional networks (CNNs) and feedforward neural networks. Here we investigate the behavior of residual networks during training and show that they exhibit properties similar to gradient boosting algorithms. We demonstrate through extensive experiments that while standard CNNs can benefit from early stopping, overfitting is still challenging due to strong local minima of the loss surface. In contrast, residual networks often achieve better generalization performance than CNNs even without regularizers, suggesting some form of implicit bias guides optimization towards more accurate solutions. Our analysis indicates that this bias comes from a novel kind of ""boosting"" effect within the ResNet architecture, where each subsequent layer attempts to correct prediction errors made by previous layers. This finding provides insight into how ResNets interact with datasets, suggests new ways of designing and improving neural networks, and helps elucidate the role of inductive biases in deep learning research.",1
"Linear algebra algorithms are used widely in a variety of domains, e.g machine learning, numerical physics and video games graphics. For all these applications, loop-level parallelism is required to achieve high performance. However, finding the optimal way to schedule the workload between threads is a non-trivial problem because it depends on the structure of the algorithm being parallelized and the hardware the executable is run on. In the realm of Asynchronous Many Task runtime systems, a key aspect of the scheduling problem is predicting the proper chunk-size, where the chunk-size is defined as the number of iterations of a for-loop assigned to a thread as one task. In this paper, we study the applications of supervised learning models to predict the chunk-size which yields maximum performance on multiple parallel linear algebra operations using the HPX backend of Blaze's linear algebra library. More precisely, we generate our training and tests sets by measuring performance of the application with different chunk-sizes for multiple linear algebra operations; vector-addition, matrix-vector-multiplication, matrix-matrix addition and matrix-matrix-multiplication. We compare the use of logistic regression, neural networks and decision trees with a newly developed decision tree based model in order to predict the optimal value for chunk-size. Our results show that classical decision trees and our custom decision tree model are able to forecast a chunk-size which results in good performance for the linear algebra operations.",0
"Optimization of schedules for parallel computing systems can improve performance significantly. For large scale numerical computations like linear algebra problems, which involve performing multiple matrix multiplications and factorizations, exploiting concurrency has been shown to have great potential benefits. However, designing efficient parallelism requires deep understanding of both algorithmic and hardware issues. In our recent work, we designed several experiments that show how to use supervised learning techniques to optimize schedules based on knowledge from prior runs. We used several machine learning models to predict execution time estimates and schedule tasks accordingly. Our results showed good accuracy on average over all benchmarks considered; nevertheless, there were cases where predictions were not very accurate due to insufficient training data. These findings suggest that further investigation into ways to handle such scenarios is necessary before applying these methods at larger scales.",1
"We present an algorithm for learning decision trees using stochastic gradient information as the source of supervision. In contrast to previous approaches to gradient-based tree learning, our method operates in the incremental learning setting rather than the batch learning setting, and does not make use of soft splits or require the construction of a new tree for every update. We demonstrate how one can apply these decision trees to different problems by changing only the loss function, using classification, regression, and multi-instance learning as example applications. In the experimental evaluation, our method performs similarly to standard incremental classification trees, outperforms state of the art incremental regression trees, and achieves comparable performance with batch multi-instance learning methods.",0
"Gradient trees have become increasingly popular as a method for solving machine learning problems. However, many of these algorithms suffer from drawbacks such as computational intractability, lack of flexibility, or poor accuracy. In this paper, we propose a novel approach called stochastic gradient trees that addresses these issues while maintaining state-of-the-art performance. Our method uses a randomized algorithm based on tree structures to solve complex optimization problems in parallel, making it more efficient than traditional methods. We showcase our algorithm's ability to handle high-dimensional data sets, model nonlinear relationships between variables, and provide interpretable results through case studies. Overall, our research contributes new insights into the field of machine learning by developing a powerful tool for addressing real-world problems effectively. This work has significant potential applications across multiple domains, including natural language processing, computer vision, and medical diagnosis.",1
"Even though it is well known that for most relevant computational problems different algorithms may perform better on different classes of problem instances, most researchers still focus on determining a single best algorithmic configuration based on aggregate results such as the average. In this paper, we propose Integer Programming based approaches to build decision trees for the Algorithm Selection Problem. These techniques allow automate three crucial decisions: (i) discerning the most important problem features to determine problem classes; (ii) grouping the problems into classes and (iii) select the best algorithm configuration for each class. To evaluate this new approach, extensive computational experiments were executed using the linear programming algorithms implemented in the COIN-OR Branch & Cut solver across a comprehensive set of instances, including all MIPLIB benchmark instances. The results exceeded our expectations. While selecting the single best parameter setting across all instances decreased the total running time by 22%, our approach decreased the total running time by 40% on average across 10-fold cross validation experiments. These results indicate that our method generalizes quite well and does not overfit.",0
"This paper presents two new methods for constructing decision trees that can optimize an integer programming (IP) formulation used for algorithm selection problems on combinatorial search spaces such as parameter tuning or feature subset selection tasks. Our first method uses exact inference from a probabilistic model and generates near optimal solutions using mixed-integer linear programs (MILPs). We then improve upon these MILP results by generating equivalent but smaller IP models through a process we call IP reduction, which exploits problem structure to simplify the IP formulation without changing the solution space. Experimental evaluation shows our approach outperforms current state-of-the-art algorithms across a range of benchmark domains including TSP, knapsack, kd-tree construction, maximum clique, graph bipartitioning, protein docking, and document clustering. Our second contribution applies the same principles to generate decision trees based solely on heuristics (i.e., no exact inference required), achieving significant improvements over existing heuristic approaches. Our experiments indicate the combination of exact inference and structured model reduction can significantly reduce runtime while producing high quality decisions. Overall, our methods provide powerful tools for identifying effective algorithms for solving real world optimization problems.",1
"In this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction tasks. We begin with a quick introduction to the concepts of machine learning and outline some of the most common machine learning algorithms. Next, we demonstrate how to apply the algorithms with appropriate toolkits to conduct machine learning experiments for clinical prediction tasks. The objectives of this chapter are to (1) understand the basics of machine learning techniques and the reasons behind why they are useful for solving clinical prediction problems, (2) understand the intuition behind some machine learning models, including regression, decision trees, and support vector machines, and (3) understand how to apply these models to clinical prediction problems using publicly available datasets via case studies.",0
"Title: Machine Learning for Clinical Predictive Analytics  Abstract: In recent years, there has been increasing interest in using machine learning techniques to improve clinical decision making by predicting patient outcomes and identifying high risk patients. This has led to significant advances in clinical predictive analytics (CPA) which combines both big data and advanced machine learning algorithms to identify patterns and relationships that may provide insights into complex healthcare problems. This review aimed to evaluate current research efforts focused on developing ML models for CPA tasks, explore challenges encountered during implementation, and discuss future directions that could enhance their effectiveness. We found that while most published studies report promising results, many face limitations such as small sample sizes, poor validation strategies, variable quality control procedures, limited interpretability, and inconsistencies in outcome measures. To address these issues, we propose recommendations for improving the design, execution, interpretation, generalizability and reporting of future work in ML for CPA. Our comprehensive synthesis suggests that despite limitations, ML for CPA shows strong potential for significantly impacting precision medicine, disease management, drug discovery, diagnosis and therapy through enhanced prediction accuracy. With appropriate attention paid to rigorous study designs, transparent methods and thorough reporting practices, we anticipate continued progress toward realizing tangible benefits from integrating ML into routine medical care. By providing actionable knowledge that can inform evidence based treatment decisions and resource allocation, our framework should serve stakeholders well across diverse clinical contexts.",1
"Nowadays, deep neural networks (DNNs) have become the main instrument for machine learning tasks within a wide range of domains, including vision, NLP, and speech. Meanwhile, in an important case of heterogenous tabular data, the advantage of DNNs over shallow counterparts remains questionable. In particular, there is no sufficient evidence that deep learning machinery allows constructing methods that outperform gradient boosting decision trees (GBDT), which are often the top choice for tabular problems. In this paper, we introduce Neural Oblivious Decision Ensembles (NODE), a new deep learning architecture, designed to work with any tabular data. In a nutshell, the proposed NODE architecture generalizes ensembles of oblivious decision trees, but benefits from both end-to-end gradient-based optimization and the power of multi-layer hierarchical representation learning. With an extensive experimental comparison to the leading GBDT packages on a large number of tabular datasets, we demonstrate the advantage of the proposed NODE architecture, which outperforms the competitors on most of the tasks. We open-source the PyTorch implementation of NODE and believe that it will become a universal framework for machine learning on tabular data.",0
"In recent years, deep learning techniques have emerged as powerful tools for solving complex problems in tabular data domains such as recommendation systems, fraud detection, and credit risk assessment. One major challenge faced by researchers in these fields is ensuring that their models make accurate predictions without compromising privacy or revealing sensitive information.  The authors propose a novel approach called ""Neural Obligatory Decision Ensembles"" (NODEs) which addresses this problem by combining the strengths of decision trees and neural networks while preserving privacy and reducing overfitting. NODEs leverage an ensemble methodology where multiple models vote on a final prediction, making the system more robust and reliable. By using a combination of linear regression models and gradient boosting algorithms, NODEs can handle high dimensional datasets effectively while maintaining interpretability. Additionally, the use of feature selection techniques allows for parsimonious modeling.  Experimental results on several benchmark datasets demonstrate the effectiveness of NODEs compared to state-of-the-art baselines across a variety of metrics including accuracy, precision, recall, F1 score, area under receiver operating characteristic curve, Matthews correlation coefficient and NDCG@k. Furthermore, extensive analysis of parameter sensitivity shows the stability of the proposed framework. Finally, an ablation study is conducted to investigate the importance of each component within the framework.  Overall, the work presented here provides an important step forward towards addressing the challenges surrounding privacy-preserving deep learning models, particularly those applied to tabular data scenarios. The development of efficient, transparent, and interpretable frameworks like NODEs has significant potential impacts in both academia and industry, paving the way for new applications in secure, private machine learning.",1
"This paper has been withdrawn by the authors due to insufficient or definition error(s) in the ethics approval protocol.   Autism spectrum disorders (ASD) impact the cognitive, social, communicative and behavioral abilities of an individual. The development of new clinical decision support systems is of importance in reducing the delay between presentation of symptoms and an accurate diagnosis. In this work, we contribute a new database consisting of video clips of typical (normal) and atypical (such as hand flapping, spinning or rocking) behaviors, displayed in natural settings, which have been collected from the YouTube video website. We propose a preliminary non-intrusive approach based on skeleton keypoint identification using pretrained deep neural networks on human body video clips to extract features and perform body movement analysis that differentiates typical and atypical behaviors of children. Experimental results on the newly contributed database show that our platform performs best with decision tree as the classifier when compared to other popular methodologies and offers a baseline against which alternate approaches may developed and tested.",0
"This study aimed to develop an automatic system capable of screening typical and atypical behaviors in children with autism. To achieve this goal, researchers collected data from children diagnosed with autism spectrum disorder (ASD) as well as typically developing children. Using machine learning algorithms, they trained their model on this dataset, focusing specifically on social interactions, emotional responses, motor skills, and repetitive movements. Results showed that the proposed method was able to accurately detect both typical and atypical behaviors in children with ASD. These findings have important implications for early detection, assessment, and intervention efforts for children with ASD, potentially providing a valuable tool for clinicians and caregivers.",1
"Propositionalization is the process of summarizing relational data into a tabular (attribute-value) format. The resulting table can next be used by any propositional learner. This approach makes it possible to apply a wide variety of learning methods to relational data. However, the transformation from relational to propositional format is generally not lossless: different relational structures may be mapped onto the same feature vector. At the same time, features may be introduced that are not needed for the learning task at hand. In general, it is hard to define a feature space that contains all and only those features that are needed for the learning task. This paper presents LazyBum, a system that can be considered a lazy version of the recently proposed OneBM method for propositionalization. LazyBum interleaves OneBM's feature construction method with a decision tree learner. This learner both uses and guides the propositionalization process. It indicates when and where to look for new features. This approach is similar to what has elsewhere been called dynamic propositionalization. In an experimental comparison with the original OneBM and with two other recently proposed propositionalization methods (nFOIL and MODL, which respectively perform dynamic and static propositionalization), LazyBum achieves a comparable accuracy with a lower execution time on most of the datasets.",0
"In recent years, decision trees have become increasingly popular due to their ability to accurately model complex relationships in data. However, one drawback of traditional decision tree algorithms is that they can produce large, overcomplex models that are difficult to interpret and prone to overfitting. To address these issues, we propose a new method called ""LazyBum"" which uses lazy propositionalization to simplify decision trees into smaller, easier-to-understand structures. Our approach first builds a standard decision tree using any off-the-shelf algorithm, and then applies our lazy propositionalization technique to convert the resulting tree into a set of simpler decision rules. We show through experiments on several benchmark datasets that our method leads to significant reductions in complexity while maintaining high accuracy. Furthermore, our method allows for easy interpretation and explanation of the final decision rule sets, making them suitable for use in applications where transparency is important. Overall, our work demonstrates the effectiveness of lazy propositionalization as a means of simplifying decision trees, improving their interpretability, and reducing the risk of overfitting.",1
"We propose and study a multi-scale approach to vector quantization. We develop an algorithm, dubbed reconstruction trees, inspired by decision trees. Here the objective is parsimonious reconstruction of unsupervised data, rather than classification. Contrasted to more standard vector quantization methods, such as K-means, the proposed approach leverages a family of given partitions, to quickly explore the data in a coarse to fine-- multi-scale-- fashion. Our main technical contribution is an analysis of the expected distortion achieved by the proposed algorithm, when the data are assumed to be sampled from a fixed unknown distribution. In this context, we derive both asymptotic and finite sample results under suitable regularity assumptions on the distribution. As a special case, we consider the setting where the data generating distribution is supported on a compact Riemannian sub-manifold. Tools from differential geometry and concentration of measure are useful in our analysis.",0
"This paper presents a novel method called ""Multi-Scale Vector Quantization with Reconstruction Trees"" (MSVQR) that offers significant improvements over traditional vector quantization methods such as K-means clustering by allowing for reconstruction trees which can capture both coarse and fine level features within a single framework. Our approach uses scale space filtering techniques at different levels of granularity to create multiple versions of the input data, each filtered version corresponding to a particular scale. Then we use these scaled filtered representations combined with the original image to train our MSVQR model. This results in improved performance on benchmark datasets due to more robust encoding of high frequency features across scales. We evaluate our model on two well-known benchmarks – the CIFAR dataset for object recognition and the UCF sports action classification dataset where MSVQR achieves state-of-the-art accuracy surpassing other popular deep learning models while operating only on raw pixel inputs, eliminating any pretraining requirements on large labeled datasets. In conclusion, MSVQR provides a powerful new toolbox for feature extraction from images that significantly improves upon classical approaches.",1
"The Lucid methods described by Olah et al. (2018) provide a way to inspect the inner workings of neural networks trained on image classification tasks using feature visualization. Such methods have generally been applied to networks trained on visually rich, large-scale image datasets like ImageNet, which enables them to produce enticing feature visualizations. To investigate these methods further, we applied them to classifiers trained to perform the much simpler (in terms of dataset size and visual richness), yet challenging task of distinguishing between different kinds of white blood cell from microscope images. Such a task makes generating useful feature visualizations difficult, as the discriminative features are inherently hard to identify and interpret. We address this by presenting the ""Illuminated Decision Tree"" approach, in which we use a neural network trained on the task as a feature extractor, then learn a decision tree based on these features, and provide Lucid visualizations for each node in the tree. We demonstrate our approach with several examples, showing how this approach could be useful both in model development and debugging, and when explaining model outputs to non-experts.",0
"Introduction: Machine learning algorithms have become increasingly popular due to their ability to make accurate predictions based on large amounts of data. One type of algorithm that has gained attention recently is decision trees. However, traditional decision tree methods can suffer from overfitting and may require manual feature engineering, which can be time-consuming and error-prone. In this paper, we propose a novel method called ""Illuminated Decision Trees"" (IDT) that addresses these limitations by incorporating automatic feature selection and using a technique called ""Lucidity Functions"" to reduce overfitting. Methodology: Our approach first applies automatic feature selection techniques such as Lasso regression or Ridge regression to identify the most important features in the dataset. Then, it constructs a decision tree using these selected features and a new criterion called the ""Lucidity Function."" This function measures how well each split in the tree aligns with the global structure of the dataset, allowing us to choose splits that produce more interpretable models while reducing overfitting. Results: We evaluate our method on several real-world datasets and compare its performance against other state-of-the-art machine learning algorithms. Experimental results demonstrate that IDT achieves better accuracy than many competing approaches while producing simpler and more interpretable models. Conclusion: Our proposed method offers a powerful tool for building decision trees that achieve high accuracy while providing insight into the underlying structure of the data. Its use of automatic feature selection and the Lucidity Function makes it easy to implement and allows for quick model deployment without requiring extensive knowledge of the domain. Overall, Illuminated Decision Trees with Lucid offer a promising solution for many types of prediction tasks.",1
"We present a new way of constructing an ensemble classifier, named the Guided Random Forest (GRAF) in the sequel. GRAF extends the idea of building oblique decision trees with localized partitioning to obtain a global partitioning. We show that global partitioning bridges the gap between decision trees and boosting algorithms. We empirically demonstrate that global partitioning reduces the generalization error bound. Results on 115 benchmark datasets show that GRAF yields comparable or better results on a majority of datasets. We also present a new way of approximating the datasets in the framework of random forests.",0
"Inference techniques have been widely used in machine learning algorithms as they offer a balance between computational cost and accuracy. One such technique that has gained recent popularity is random forest. However, training a random forest model can be time consuming and computationally expensive due to the large number of decision trees required. To tackle this issue, researchers have proposed several methods such as parallel computing and early stopping. Despite these advancements, constructing and applying a high quality random forest still remains challenging due to issues related to feature selection, parameter tuning and overfitting prevention.  To address these concerns, we propose guided random forest (GF) which incorporates an additional step called ""guidance"" into traditional random forest construction process. During guidance, GF uses domain knowledge on how random samples should behave and corrects them according to prior known conditions. Once corrected by the guidance stage, GF utilizes uncorrected random features along with their corresponding targets to train the final random forest predictor. By combining the two sources of information we can reduce errors resulting from biased/noisy sampling during initial stages, while retaining some noise for stability improvement. We demonstrate through extensive evaluation using benchmark datasets that our method outperforms previously reported approaches across different measures such as mean squared error, maximum absolute deviation etc. Finally we use applications on real world data sets including text classification, sentiment analysis, user profiling which show significant improvements compared to other state-of-the art models.  In conclusion, Guided Random Forest presents a novel framework capable of producing accurate predictions efficiently without sacrificing interpretability of obtained results. Our study serves as a proof-of-concept for future integration of human knowledge into deep learning models where decisions taken by humans can further improve computational performance and effectiveness.",1
"Detecting roadway segments inundated due to floodwater has important applications for vehicle routing and traffic management decisions. This paper proposes a set of algorithms to automatically detect floodwater that may be present in an image captured by mobile phones or other types of optical cameras. For this purpose, image classification and flood area segmentation methods are developed. For the classification task, we used Local Binary Patterns (LBP), Histogram of Oriented Gradients (HOG) and pre-trained deep neural network (VGG-16) as feature extractors and trained logistic regression, k-nearest neighbors, and decision tree classifiers on the extracted features. Pre-trained VGG-16 network with logistic regression classifier outperformed all other methods. For the flood area segmentation task, we investigated superpixel based methods and Fully Convolutional Neural Network (FCN). Similar to the classification task, we trained logistic regression and k-nearest neighbors classifiers on the superpixel areas and compared that with an end-to-end trained FCN. Conditional Random Fields (CRF) method was applied after both segmentation methods to post-process coarse segmentation results. FCN offered the highest scores in all metrics; it was followed by superpixel-based logistic regression and then superpixel-based KNN.",0
"Roadway images are analyzed using both handcrafted feature extraction techniques and deep transfer learning approaches to identify regions where there may have been recent flooding. The goal of the research was to develop automatic methods that can quickly detect potential road hazards caused by water accumulation after rainstorms, snow melts, or other causes. Features derived from color, texture, edge strength, and object shapes were used to train machine learning classifiers for each specific dataset, including daytime vs nighttime scenarios and urban vs rural environments. Results show improved performance over existing methods using pre-trained models fine-tuned on smaller datasets and adaptability across different imagery settings without loss of accuracy. Ultimately, these findings could enable more efficient road maintenance crews dispatch times during inclement weather events and enhance overall public safety. However, further experimentation and evaluation is warranted for optimal generalization capability.",1
"Interference effects of tall buildings have attracted numerous studies due to the boom of clusters of tall buildings in megacities. To fully understand the interference effects of buildings, it often requires a substantial amount of wind tunnel tests. Limited wind tunnel tests that only cover part of interference scenarios are unable to fully reveal the interference effects. This study used machine learning techniques to resolve the conflicting requirement between limited wind tunnel tests that produce unreliable results and a completed investigation of the interference effects that is costly and time-consuming. Four machine learning models including decision tree, random forest, XGBoost, generative adversarial networks (GANs), were trained based on 30% of a dataset to predict both mean and fluctuating pressure coefficients on the principal building. The GANs model exhibited the best performance in predicting these pressure coefficients. A number of GANs models were then trained based on different portions of the dataset ranging from 10% to 90%. It was found that the GANs model based on 30% of the dataset is capable of predicting both mean and fluctuating pressure coefficients under unseen interference conditions accurately. By using this GANs model, 70% of the wind tunnel test cases can be saved, largely alleviating the cost of this kind of wind tunnel testing study.",0
"This should be a standard scientific research study abstract that summarizes the main elements of your study, including: problem statement/objective, methodology, results and conclusions. This can be structured as 4 individual sections within one large abstract section (e.g., problem statement/objective, methods, results, conclusions). You may also use subheadings if you think they will make reading easier. Please note: There was no existing data used here so please ensure that all aspects required by science remain true.  As always, thanking you in advance, have a great day!  Dear [Name],  [Wind loads have been known to cause problems for tall buildings since before man could fly machines. But nowadays we need to worry about more than just the force of nature - we must consider how our structures affect their environments too.] Recently, there has been significant interest among engineers and scientists in quantifying these complex interactions to improve safety, reduce risk, minimize costs, maximize sustainability, maintain comfort levels, and optimize performance across all scales from small details like window fittings up to global city planning. We developed advanced modeling approaches based on Artificial Neural Networks (ANNs) fed with comprehensive databases created using Computational Fluid Dynamics (CFD), Wind Tunnel tests, historical records and statistical analysis to explore new ways to predict the combined influences of wind forces and urban clutter under realistic conditions where other tools fall short. In essence we ask the question: “Is Machine Learning capable of replacing some parts of existing engineering processes for understanding and analyzing the impacts of wind interacting with buildings?” Our work provides valuable insights into the opportunities and challenges associated wi",1
"Deep models have advanced prediction in many domains, but their lack of interpretability remains a key barrier to the adoption in many real world applications. There exists a large body of work aiming to help humans understand these black box functions to varying levels of granularity -- for example, through distillation, gradients, or adversarial examples. These methods however, all tackle interpretability as a separate process after training. In this work, we take a different approach and explicitly regularize deep models so that they are well-approximated by processes that humans can step-through in little time. Specifically, we train several families of deep neural networks to resemble compact, axis-aligned decision trees without significant compromises in accuracy. The resulting axis-aligned decision functions uniquely make tree regularized models easy for humans to interpret. Moreover, for situations in which a single, global tree is a poor estimator, we introduce a regional tree regularizer that encourages the deep model to resemble a compact, axis-aligned decision tree in predefined, human-interpretable contexts. Using intuitive toy examples as well as medical tasks for patients in critical care and with HIV, we demonstrate that this new family of tree regularizers yield models that are easier for humans to simulate than simpler L1 or L2 penalties without sacrificing predictive power.",0
"In order to deploy deep neural networks (DNNs) into real world applications, their behavior needs to be well understood by humans who use them as decision support tools. Unfortunately, DNNs often suffer from poor interpretability: their predictions cannot always be explained easily using intuitive reasoning techniques such as visual inspection or feature importance analysis alone. Many attempts have been made towards improving DNN transparency through specialized model architectures or posthoc methods, but these often trade off accuracy for better explainability without proper validation against human expectations. We propose tree regularization as a simple yet effective method to optimize for both interpretability and performance simultaneously by constraining the topology of intermediate representations during training. Our experiments on multiple large-scale image classification benchmark datasets demonstrate that this method leads to more meaningful and accurate decision boundaries compared to traditional heuristics or even state-of-the-art specialized models like CAM. With proper evaluation metrics, tree regularization can guide future designs towards interpretable machine learning systems that meet practical user needs while achieving strong generalization capabilities.",1
"Decision trees have been a very popular class of predictive models for decades due to their interpretability and good performance on categorical features. However, they are not always robust and tend to overfit the data. Additionally, if allowed to grow large, they lose interpretability. In this paper, we present a mixed integer programming formulation to construct optimal decision trees of a prespecified size. We take the special structure of categorical features into account and allow combinatorial decisions (based on subsets of values of features) at each node. Our approach can also handle numerical features via thresholding. We show that very good accuracy can be achieved with small trees using moderately-sized training sets. The optimization problems we solve are tractable with modern solvers.",0
"This abstract is for a research paper that presents an integer programming approach to learning optimal generalized decision trees (GDTs) for classification problems. GDTs have been shown to be effective models for supervised learning tasks, but their training can be computationally expensive due to the combinatorial nature of constructing decision trees. In this work, we propose a novel formulation for solving the problem of learning an optimal GDT subject to constraints on tree depth and node complexity using mixed integer linear programming (MILP). Our MILP framework enables us to learn highly accurate decision trees while respecting desired properties such as sparsity and interpretability. We present experimental results demonstrating our approach outperforms state-of-the-art methods across multiple benchmark datasets, making it a promising tool for real-world applications. Keywords: generalized decision trees, integer programming, machine learning, optimization",1
"Additive Manufacturing (AM) is a manufacturing paradigm that builds three-dimensional objects from a computer-aided design model by successively adding material layer by layer. AM has become very popular in the past decade due to its utility for fast prototyping such as 3D printing as well as manufacturing functional parts with complex geometries using processes such as laser metal deposition that would be difficult to create using traditional machining. As the process for creating an intricate part for an expensive metal such as Titanium is prohibitive with respect to cost, computational models are used to simulate the behavior of AM processes before the experimental run. However, as the simulations are computationally costly and time-consuming for predicting multiscale multi-physics phenomena in AM, physics-informed data-driven machine-learning systems for predicting the behavior of AM processes are immensely beneficial. Such models accelerate not only multiscale simulation tools but also empower real-time control systems using in-situ data. In this paper, we design and develop essential components of a scientific framework for developing a data-driven model-based real-time control system. Finite element methods are employed for solving time-dependent heat equations and developing the database. The proposed framework uses extremely randomized trees - an ensemble of bagged decision trees as the regression algorithm iteratively using temperatures of prior voxels and laser information as inputs to predict temperatures of subsequent voxels. The models achieve mean absolute percentage errors below 1% for predicting temperature profiles for AM processes. The code is made available for the research community at https://github.com/paularindam/ml-iter-additive.",0
"Add as many technical terms as you can without making it sound like gibberish. Additive manufacturing (AM) has become increasingly popular due to its ability to produce complex geometries and customized parts quickly and efficiently. Temperature plays a critical role in the quality and integrity of printed products and monitoring their profiles during printing is essential for ensuring that the final product meets desired specifications. Traditional methods require manual intervention or offline analysis tools which may lead to delays, rework, or scrap and have limited capacity for real-time feedback or control adjustments. In order to address these challenges, we propose a novel real-time iterative machine learning model based on convolutional neural networks trained using Transfer Learning capable of accurately predicting temperature profiles in AM applications while utilizing data generated by Infrared thermal imaging cameras and process parameters. This methodology enables automated temperature monitoring, rapid adaptation to changes in process conditions and provides valuable insights into improving overall print quality through closed loop feedback mechanisms. Our experimental results demonstrate the superiority of our proposed method over traditional approaches in both accuracy and execution time while providing actionable insight into reducing defects caused by overheating or other thermal issues encountered in additive manufacturing processes. Overall, this work represents a significant advancement towards enabling smart, real-time adaptive manufacturing systems, and the ability to continuously improve manufacturing outcomes.",1
"Gradient boosting decision tree (GBDT) is a widely-used machine learning algorithm in both data analytic competitions and real-world industrial applications. Further, driven by the rapid increase in data volume, efforts have been made to train GBDT in a distributed setting to support large-scale workloads. However, we find it surprising that the existing systems manage the training dataset in different ways, but none of them have studied the impact of data management. To that end, this paper aims to study the pros and cons of different data management methods regarding the performance of distributed GBDT. We first introduce a quadrant categorization of data management policies based on data partitioning and data storage. Then we conduct an in-depth systematic analysis and summarize the advantageous scenarios of the quadrants. Based on the analysis, we further propose a novel distributed GBDT system named Vero, which adopts the unexplored composition of vertical partitioning and row-store and suits for many large-scale cases. To validate our analysis empirically, we implement different quadrants in the same code base and compare them under extensive workloads, and finally compare Vero with other state-of-the-art systems over a wide range of datasets. Our theoretical and experimental results provide a guideline on choosing a proper data management policy for a given workload.",0
"This is my attempt:  Abstract: This paper presents an experimental evaluation of large scale gradient boosting decision tree (GBDT) systems. We investigate three common implementations of GBDT algorithms, including XGBoost, LightGBM, and CatBoost. Our experiments aim to identify which algorithm performs best under different scenarios such as varying data sizes, model complexity levels, and hyperparameter settings. Our findings show that while all three algorithms perform well on small datasets, performance degradations occur at larger scales due to memory constraints and slower training times. Additionally, we observe variations in their ability to handle high complexity models and sensitive parameters tuning requirements. Overall, our study provides insight into the strengths and weaknesses of each system, enabling practitioners to make informed decisions when selecting a suitable GBDT model for their specific use case.",1
"The increasing adoption of machine learning tools has led to calls for accountability via model interpretability. But what does it mean for a machine learning model to be interpretable by humans, and how can this be assessed? We focus on two definitions of interpretability that have been introduced in the machine learning literature: simulatability (a user's ability to run a model on a given input) and ""what if"" local explainability (a user's ability to correctly determine a model's prediction under local changes to the input, given knowledge of the model's original prediction). Through a user study with 1,000 participants, we test whether humans perform well on tasks that mimic the definitions of simulatability and ""what if"" local explainability on models that are typically considered locally interpretable. To track the relative interpretability of models, we employ a simple metric, the runtime operation count on the simulatability task. We find evidence that as the number of operations increases, participant accuracy on the local interpretability tasks decreases. In addition, this evidence is consistent with the common intuition that decision trees and logistic regression models are interpretable and are more interpretable than neural networks.",0
"Include important keywords such as machine learning models, local interpretability, data privacy laws (e.g. GDPR), explainability methods. Use at least one active voice sentence. --- This article presents research on assessing the local interpretability of machine learning models using state-of-the art techniques in privacy preserving federated learning and interpretable model analysis. The authors explore different approaches for enhancing local interpretability while balancing considerations of data privacy laws like the General Data Protection Regulation (GDPR). Key results demonstrate improved transparency and understanding of the decision making processes behind ML predictions without exposing sensitive training data across distributed nodes. This work provides valuable insights into advancing the field of machine learning models towards more trustworthy, reliable, and secure operations across industries.",1
"The vision of automated driving is to increase both road safety and efficiency, while offering passengers a convenient travel experience. This requires that autonomous systems correctly estimate the current traffic scene and its likely evolution. In highway scenarios early recognition of cut-in maneuvers is essential for risk-aware maneuver planning. In this paper, a statistical approach is proposed, which advantageously utilizes a set of prototypical lane change trajectories to realize both early maneuver detection and uncertainty-aware trajectory prediction for traffic participants. Generation of prototype trajectories from real traffic data is accomplished by Agglomerative Hierarchical Clustering. During clustering, the alignment of the cluster prototypes to each other is optimized and the cohesion of the resulting prototype is limited when two clusters merge. In the prediction stage, the similarity of observed vehicle motion and typical lane change patterns in the data base is evaluated to construct a set of significant features for maneuver classification via Boosted Decision Trees. The future trajectory is predicted combining typical lane change realizations in a mixture model. B-splines based trajectory adaptations guarantee continuity during transition from actually observed to predicted vehicle states. Quantitative evaluation results demonstrate the proposed concept's improved performance for both maneuver and trajectory prediction compared to a previously implemented reference approach.",0
"This study proposes a method for predicting highway lane changes using prototype trajectories. Previous research has shown that driver behavior can be modeled as a collection of prototypical maneuvers, which form a basis for understanding typical driving scenarios. By analyzing these maneuver patterns in real-world data, we aim to develop models capable of accurately anticipating upcoming lane changes by mining characteristic features from past traffic interactions. Our approach involves constructing representative trajectory templates through clustering techniques, then learning associations between observed patterns and their corresponding actions. Evaluation results demonstrate significant improvements over baseline methods in both accuracy and generalization performance, showing promising potential applications in advanced vehicle safety systems. Overall, our work contributes new insights into modeling lane change intentions, paving the way towards more reliable prediction solutions.",1
"Ubiquitous anomalies endanger the security of our system constantly. They may bring irreversible damages to the system and cause leakage of privacy. Thus, it is of vital importance to promptly detect these anomalies. Traditional supervised methods such as Decision Trees and Support Vector Machine (SVM) are used to classify normality and abnormality. However, in some case the abnormal status are largely rarer than normal status, which leads to decision bias of these methods. Generative adversarial network (GAN) has been proposed to handle the case. With its strong generative ability, it only needs to learn the distribution of normal status, and identify the abnormal status through the gap between it and the learned distribution. Nevertheless, existing GAN-based models are not suitable to process data with discrete values, leading to immense degradation of detection performance. To cope with the discrete features, in this paper, we propose an efficient GAN-based model with specifically-designed loss function. Experiment results show that our model outperforms state-of-the-art models on discrete dataset and remarkably reduce the overhead.",0
"Cyber intrusion detection is a crucial task for ensuring the security of computer networks. Generative Adversarial Networks (GANs) have shown promising results in detecting anomalies that could indicate potential attacks. This paper proposes an efficient GAN-based approach for improving the accuracy of intrusion detection systems by integrating prior knowledge into the training process. Our method leverages transfer learning from pre-trained models, reducing computational requirements while maintaining high levels of performance. Experiments using real-world data show significant improvements over traditional methods, achieving greater than 97% accuracy in detecting malicious activities. The proposed approach offers a valuable tool for network administrators seeking to enhance their security posture against increasingly sophisticated cyber threats.",1
"The assumption of positivity in causal inference (also known as common support and co-variate overlap) is necessary to obtain valid causal estimates. Therefore, confirming it holds in a given dataset is an important first step of any causal analysis. Most common methods to date are insufficient for discovering non-positivity, as they do not scale for modern high-dimensional covariate spaces, or they cannot pinpoint the subpopulation violating positivity. To overcome these issues, we suggest to harness decision trees for detecting violations. By dividing the covariate space into mutually exclusive regions, each with maximized homogeneity of treatment groups, decision trees can be used to automatically detect subspaces violating positivity. By augmenting the method with an additional random forest model, we can quantify the robustness of the violation within each subspace. This solution is scalable and provides an interpretable characterization of the subspaces in which violations occur. We provide a visualization of the stratification rules that define each subpopulation, combined with the severity of positivity violation within it. We also provide an interactive version of the visualization that allows a deeper dive into the properties of each subspace.",0
"This paper presents a methodology for identifying and analyzing positivity violations using decision trees. Positivity violations occur when data points that should fall within certain ranges are outside those expected boundaries. In such cases, the decision tree can identify these outliers and categorize them as either errors or exceptional cases. To achieve accurate results, our proposed approach uses domain knowledge combined with statistical techniques like clustering and classification algorithms. Our evaluation shows that the decision tree accurately detects and classifies positivity violations, achieving high accuracy compared to traditional methods. The approach has applications in fields like surveillance systems, financial transactions, medical diagnosis, and other areas where real-time anomaly detection is critical. Overall, the study provides valuable insights into the development of effective anomaly detection methods using decision trees.",1
"In AI research and industry, machine learning is the most widely used tool. One of the most important machine learning algorithms is Gradient Boosting Decision Tree, i.e. GBDT whose training process needs considerable computational resources and time. To shorten GBDT training time, many works tried to apply GBDT on Parameter Server. However, those GBDT algorithms are synchronous parallel algorithms which fail to make full use of Parameter Server. In this paper, we examine the possibility of using asynchronous parallel methods to train GBDT model and name this algorithm as asynch-SGBDT (asynchronous parallel stochastic gradient boosting decision tree). Our theoretical and experimental results indicate that the scalability of asynch-SGBDT is influenced by the sample diversity of datasets, sampling rate, step length and the setting of GBDT tree. Experimental results also show asynch-SGBDT training process reaches a linear speedup in asynchronous parallel manner when datasets and GBDT trees meet high scalability requirements.",0
"This research investigates the feasibility of using asynchronous parallel computation in the training of large scale decision trees such as SGBDT. We explore new ways to scale out these algorithms over many CPUs by leveraging existing high-level machine learning frameworks and libraries. Our approach allows us to distribute the work across many machines while ensuring low communication overhead. To handle data imbalances arising from distributing the dataset among multiple machines, we introduce an efficient method that effectively reconstructs a unified feature representation without resorting to shuffling all data back to one place. In addition, we apply recent advances in distributed gradient boosting methods into our algorithm design to enable asynchronous updates, leading to significantly faster runtimes than state-of-the-art solutions. Our experiments confirm that Asynch-SGBDT achieves strong results on several benchmark datasets, improving upon other popular distributed decision tree approaches by up to three times while reducing computational costs. Overall, this study highlights the potential for further improvements in scaling out complex models through innovative distributed computing techniques in machine learning.",1
"Automated data-driven decision-making systems are ubiquitous across a wide spread of online as well as offline services. These systems, depend on sophisticated learning algorithms and available data, to optimize the service function for decision support assistance. However, there is a growing concern about the accountability and fairness of the employed models by the fact that often the available historic data is intrinsically discriminatory, i.e., the proportion of members sharing one or more sensitive attributes is higher than the proportion in the population as a whole when receiving positive classification, which leads to a lack of fairness in decision support system. A number of fairness-aware learning methods have been proposed to handle this concern. However, these methods tackle fairness as a static problem and do not take the evolution of the underlying stream population into consideration. In this paper, we introduce a learning mechanism to design a fair classifier for online stream based decision-making. Our learning model, FAHT (Fairness-Aware Hoeffding Tree), is an extension of the well-known Hoeffding Tree algorithm for decision tree induction over streams, that also accounts for fairness. Our experiments show that our algorithm is able to deal with discrimination in streaming environments, while maintaining a moderate predictive performance over the stream.",0
"In recent years, there has been a growing need for machine learning models that can effectively address issues related to fairness and equity. One approach to addressing these concerns is through the use of decision tree classifiers, which can provide interpretable results while maintaining high levels of accuracy. However, existing decision tree algorithms often lack sufficient mechanisms for ensuring fairness during both training and inference stages. This paper presents a novel adaptive fairness-aware decision tree (FAHT) algorithm designed to overcome these limitations. FAHT incorporates group fairness considerations into the construction of decision trees by leveraging a new algorithmic paradigm based on hierarchical clustering. By integrating these concepts together within an evolving framework, our method seeks to optimize a comprehensive objective function that balances model fit, feature selection, and group fairness metrics. Our extensive evaluation demonstrates the effectiveness of FAHT across diverse applications including credit risk assessment and criminal recidivism prediction tasks, where we achieve significantly better tradeoffs between accuracy and fairness compared to state-of-the-art methods from different families. These promising findings showcase the potential impact of FAHT on mitigating unfair biases present in machine learning systems, ultimately leading towards more socially responsible AI solutions.",1
"As more data are produced each day, and faster, data stream mining is growing in importance, making clear the need for algorithms able to fast process these data. Data stream mining algorithms are meant to be solutions to extract knowledge online, specially tailored from continuous data problem. Many of the current algorithms for data stream mining have high processing and memory costs. Often, the higher the predictive performance, the higher these costs. To increase predictive performance without largely increasing memory and time costs, this paper introduces a novel algorithm, named Online Local Boosting (OLBoost), which can be combined into online decision tree algorithms to improve their predictive performance without modifying the structure of the induced decision trees. For such, OLBoost applies a boosting to small separate regions of the instances space. Experimental results presented in this paper show that by using OLBoost the online learning decision tree algorithms can significantly improve their predictive performance. Additionally, it can make smaller trees perform as good or better than larger trees.",0
"This paper describes a method called ""Online Local Boosting"" which can improve the accuracy of local predictions made by ensemble methods such as gradient boosting machines. These methods typically make many local predictions that are aggregated into one global prediction. By iteratively adjusting each local prediction based on recent data points, we can increase the overall accuracy without needlessly retraining the entire model from scratch. Our experimental results show significant improvements over state-of-the art techniques on a variety of benchmark datasets. Furthermore, our approach has the potential for better scalability than traditional batch training methods due to its adaptive nature. We hope this work serves as a stepping stone towards more efficient learning algorithms that can handle streaming data in real time.",1
"In the paper, we focus on complexity of C5.0 algorithm for constructing decision tree classifier that is the models for the classification problem from machine learning. In classical case the decision tree is constructed in $O(hd(NM+N \log N))$ running time, where $M$ is a number of classes, $N$ is the size of a training data set, $d$ is a number of attributes of each element, $h$ is a tree height. Firstly, we improved the classical version, the running time of the new version is $O(h\cdot d\cdot N\log N)$. Secondly, we suggest a quantum version of this algorithm, which uses quantum subroutines like the amplitude amplification and the D{\""u}rr-H{\o}yer minimum search algorithms that are based on Grover's algorithm. The running time of the quantum algorithm is $O\big(h\cdot \sqrt{d}\log d \cdot N \log N\big)$ that is better than complexity of the classical algorithm.",0
"In ""The Quantum Version Of Classification Decision Tree Constructing Algorithm,"" we present a new method for constructing decision trees using quantum computing techniques. Our approach, which we call QCClassificationDecisionTree_C5_0 (QCCT), utilizes quantum parallel processing to significantly speed up tree construction compared to traditional classical methods. We demonstrate that QCCT produces accurate results on benchmark datasets and can handle high-dimensional feature spaces efficiently. Additionally, we compare QCCT with several state-of-the-art classification algorithms and show that our method is competitive in terms of accuracy while offering improved computational efficiency. Overall, we believe that QCCT represents a promising direction for enhancing machine learning models with quantum computing technologies. Keywords: quantum computing, decision trees, machine learning, classification algorithms.",1
"This paper explores the use of Column Generation (CG) techniques in constructing univariate binary decision trees for classification tasks. We propose a novel Integer Linear Programming (ILP) formulation, based on root-to-leaf paths in decision trees. The model is solved via a Column Generation based heuristic. To speed up the heuristic, we use a restricted instance data by considering a subset of decision splits, sampled from the solutions of the well-known CART algorithm. Extensive numerical experiments show that our approach is competitive with the state-of-the-art ILP-based algorithms. In particular, the proposed approach is capable of handling big data sets with tens of thousands of data rows. Moreover, for large data sets, it finds solutions competitive to CART.",0
An optimization model formulation must address multiple criteria while keeping computational complexity low for large scale applications. We explore methods to create linear programming models that generate valid column solutions at a minimum cost using column generators for integer programs. The mathematical heuristics presented here can dramatically improve solution quality compared to randomized column selection techniques. In this paper we provide experimental evidence on standard test sets demonstrating improved scalability and reduced gap for C4.5 and CART decision tree learning algorithms using our methodology.,1
"Mammography is the most widely used gold standard for screening breast cancer, where, mass detection is considered as the prominent step. Detecting mass in the breast is, however, an arduous problem as they usually have large variations between them in terms of shape, size, boundary, and texture. In this literature, the process of mass detection is automated with the use of transfer learning techniques of Deep Convolutional Neural Networks (DCNN). Pre-trained VGG19 network is used to extract features which are then followed by bagged decision tree for features selection and then a Support Vector Machine (SVM) classifier is trained and used for classifying between the mass and non-mass. Area Under ROC Curve (AUC) is chosen as the performance metric, which is then maximized during classifier selection and hyper-parameter tuning. The robustness of the two selected type of classifiers, C-SVM, and \u{psion}-SVM, are investigated with extensive experiments before selecting the best performing classifier. All experiments in this paper were conducted using the INbreast dataset. The best AUC obtained from the experimental results is 0.994 +/- 0.003 i.e. [0.991, 0.997]. Our results conclude that by using pre-trained VGG19 network, high-level distinctive features can be extracted from Mammograms which when used with the proposed SVM classifier is able to robustly distinguish between the mass and non-mass present in breast.",0
"In order to make breast cancer screening more accessible to women who cannot afford mammograms, automatic mass detection has been proposed as a solution. This paper presents a method using deep convolutional neural network (CNN) and support vector machine (SVM) classifiers to detect masses from digitized full field digital mammography images. Our approach first preprocesses the images to enhance the contrast and reduce noise. Then, we train two CNNs, one on patches extracted from regions of interest (ROIs) and another on patches extracted from background areas outside ROIs. These networks learn different features that complement each other during classification. Finally, we fuse these predictions along with shape and texture information into an SVM classifier that produces the final result by voting. Experimental results show our system outperforms previous methods with sensitivity at 92%.",1
"Preventing early progression of epilepsy and so the severity of seizures requires an effective diagnosis. Epileptic transients indicate the ability to develop seizures but humans overlook such brief events in an electroencephalogram (EEG) what compromises patient treatment. Traditionally, training of the EEG event detection algorithms has relied on ground truth labels, obtained from the consensus of the majority of labelers. In this work, we go beyond labeler consensus on EEG data. Our event descriptor integrates EEG signal features with one-hot encoded labeler category that is a key to improved generalization performance. Notably, boosted decision trees take advantage of singly-labeled but more varied training sets. Our quantitative experiments show the proposed labeler-hot epileptic event detector consistently outperforms a consensus-trained detector and maintains confidence bounds of the detection. The results on our infant EEG recordings suggest datasets can gain higher event variety faster and thus better performance by shifting available human effort from consensus-oriented to separate labeling when labels include both, the event and the labeler category.",0
"In recent years, advances in machine learning have enabled the development of algorithms that can analyze Electroencephalogram (EEG) signals to detect seizure onsets in patients with epilepsy. One such algorithm is based on the analysis of transient changes in signal power, which are indicative of seizure activity. However, these transients are often difficult to distinguish from other types of noise present in the EEG signal, leading to low accuracy rates in detection. In this work, we propose a novel method called Labeler-Hot Detection of EEG Epileptic Transients (LHD), which utilizes both human labels and unlabeled data to improve transient detection performance. We evaluated our method using EEG datasets from different institutions and found that LHD significantly outperforms existing state-of-the-art methods in terms of sensitivity, specificity, precision, F1 score, and area under the ROC curve. Our results demonstrate the effectiveness of incorporating human knowledge into automated systems for improved transient detection in EEG signals. This research has important implications for improving the clinical management of patients with epilepsy by enabling earlier and more accurate seizure prediction.",1
"Understanding how ""black-box"" models arrive at their predictions has sparked significant interest from both within and outside the AI community. Our work focuses on doing this by generating local explanations about individual predictions for tree-based ensembles, specifically Gradient Boosting Decision Trees (GBDTs). Given a correctly predicted instance in the training set, we wish to generate a counterfactual explanation for this instance, that is, the minimal perturbation of this instance such that the prediction flips to the opposite class. Most existing methods for counterfactual explanations are (1) model-agnostic, so they do not take into account the structure of the original model, and/or (2) involve building a surrogate model on top of the original model, which is not guaranteed to represent the original model accurately. There exists a method specifically for random forests; we wish to extend this method for GBDTs. This involves accounting for (1) the sequential dependency between trees and (2) training on the negative gradients instead of the original labels.",0
"In order to train ensemble models that accurately predict outcomes on new data points, one must carefully consider how predictions are made. Traditional tree-based boosting methods suffer from limitations which affect model accuracy and interpretability; recent works have focused on modifying these algorithms so as to improve prediction quality while maintaining explainability. In this study we explore three alternative approaches to training such ensembles, comparing their ability to generate high quality predictions against benchmark datasets, including both linear regression problems and classification tasks involving large datasets, and assess their utility using standard evaluation metrics. Throughout our analysis, we emphasize the importance of feature selection as a means towards producing accurate and interpretable results across all tested scenarios. Our findings indicate that certain modifications can indeed yield more reliable predictions without compromising transparency, providing valuable insights into effective ways of constructing robust machine learning systems capable of achieving strong performance with meaningful explanations attached to them.",1
"Statistical machine learning models should be evaluated and validated before putting to work. Conventional k-fold Monte Carlo Cross-Validation (MCCV) procedure uses a pseudo-random sequence to partition instances into k subsets, which usually causes subsampling bias, inflates generalization errors and jeopardizes the reliability and effectiveness of cross-validation. Based on ordered systematic sampling theory in statistics and low-discrepancy sequence theory in number theory, we propose a new k-fold cross-validation procedure by replacing a pseudo-random sequence with a best-discrepancy sequence, which ensures low subsampling bias and leads to more precise Expected-Prediction-Error estimates. Experiments with 156 benchmark datasets and three classifiers (logistic regression, decision tree and naive bayes) show that in general, our cross-validation procedure can extrude subsampling bias in the MCCV by lowering the EPE around 7.18% and the variances around 26.73%. In comparison, the stratified MCCV can reduce the EPE and variances of the MCCV around 1.58% and 11.85% respectively. The Leave-One-Out (LOO) can lower the EPE around 2.50% but its variances are much higher than the any other CV procedure. The computational time of our cross-validation procedure is just 8.64% of the MCCV, 8.67% of the stratified MCCV and 16.72% of the LOO. Experiments also show that our approach is more beneficial for datasets characterized by relatively small size and large aspect ratio. This makes our approach particularly pertinent when solving bioscience classification problems. Our proposed systematic subsampling technique could be generalized to other machine learning algorithms that involve random subsampling mechanism.",0
"In this paper, we address the issue of subsampling bias in cross validation by proposing a new method called ""Best-Discrepancy Systematic Cross Validation"" (BDSCV). We show that existing methods such as random subsampled CV can suffer from biases due to sampling variability, leading to overfitting or underfitting. Our proposed method uses a systematic approach to select subsets of data points based on their discrepancy scores relative to a reference set, ensuring more robust estimates of model performance. Experimental results demonstrate significant improvements in accuracy compared to standard techniques across several benchmark datasets. The study contributes to the development of better evaluation metrics for machine learning models, reducing uncertainty and improving generalization abilities. Overall, our work has important implications for the field of machine learning, enabling researchers to make more informed decisions in choosing and evaluating algorithms.",1
"Despite its success and popularity, machine learning is now recognized as vulnerable to evasion attacks, i.e., carefully crafted perturbations of test inputs designed to force prediction errors. In this paper we focus on evasion attacks against decision tree ensembles, which are among the most successful predictive models for dealing with non-perceptual problems. Even though they are powerful and interpretable, decision tree ensembles have received only limited attention by the security and machine learning communities so far, leading to a sub-optimal state of the art for adversarial learning techniques. We thus propose Treant, a novel decision tree learning algorithm that, on the basis of a formal threat model, minimizes an evasion-aware loss function at each step of the tree construction. Treant is based on two key technical ingredients: robust splitting and attack invariance, which jointly guarantee the soundness of the learning process. Experimental results on three publicly available datasets show that Treant is able to generate decision tree ensembles that are at the same time accurate and nearly insensitive to evasion attacks, outperforming state-of-the-art adversarial learning techniques.",0
"One of the main challenges facing machine learning algorithms today is adversarial training evasion attacks which seek to manipulate models into making incorrect predictions by intentionally adding small perturbations to input data. This paper presents a novel approach for constructing decision trees that are more robust against these types of attacks, termed ""Treants"". Our method incorporates regularization terms based on saliency maps, which measure the influence of individual features on model outputs, during tree construction. Experimental results demonstrate that our Treant algorithm produces significantly more robust models compared to state-of-the-art methods across multiple benchmark datasets and attack scenarios, while maintaining high accuracy on benign inputs. These findings have important implications for developing secure and reliable artificial intelligence systems.",1
"There is a growing desire in the field of reinforcement learning (and machine learning in general) to move from black-box models toward more ""interpretable AI."" We improve interpretability of reinforcement learning by increasing the utility of decision tree policies learned via reinforcement learning. These policies consist of a decision tree over the state space, which requires fewer parameters to express than traditional policy representations. Existing methods for creating decision tree policies via reinforcement learning focus on accurately representing an action-value function during training, but this leads to much larger trees than would otherwise be required. To address this shortcoming, we propose a novel algorithm which only increases tree size when the estimated discounted future reward of the overall policy would increase by a sufficient amount. Through evaluation in a simulated environment, we show that its performance is comparable or superior to traditional tree-based approaches and that it yields a more succinct policy. Additionally, we discuss tuning parameters to control the tradeoff between optimizing for smaller tree size or for overall reward.",0
"This study proposes a new methodology called ""Conservative Q-Improvement"" that enables reinforcement learning (RL) algorithms to select actions that lead to interpretable decision trees as their policies. Existing RL methods often suffer from the problem of non-interpretability - they learn policies represented by complex and unintelligible function approximators like neural networks. In contrast, decision tree policies can provide intuitive explanations about how agents make decisions. We address several challenges to achieve conservative improvement towards decision-tree policies, ensuring safety during the transition process from old policy to new one, and balancing exploration against exploitation efficiently. Our experimental results show promising performance across multiple domains compared with baseline models. Overall, our work represents a step towards achieving explainable intelligent systems based on deep learning techniques.",1
"Despite their great success in recent years, deep neural networks (DNN) are mainly black boxes where the results obtained by running through the network are difficult to understand and interpret. Compared to e.g. decision trees or bayesian classifiers, DNN suffer from bad interpretability where we understand by interpretability, that a human can easily derive the relations modeled by the network. A reasonable way to provide interpretability for humans are logical rules. In this paper we propose neural logic rule layers (NLRL) which are able to represent arbitrary logic rules in terms of their conjunctive and disjunctive normal forms. Using various NLRL within one layer and correspondingly stacking various layers, we are able to represent arbitrary complex rules by the resulting neural network architecture. The NLRL are end-to-end trainable allowing to learn logic rules directly from available data sets. Experiments show that NLRL-enhanced neural networks can learn to model arbitrary complex logic and perform arithmetic operation over the input values.",0
"Artificial neural networks have gained immense popularity in recent years due to their ability to perform complex tasks such as image classification, natural language processing, and speech recognition. However, one major drawback of these models is that they can often be difficult to interpret and explain, leading to concerns about transparency and accountability. In this paper, we propose a new model architecture called Neural Logic Rule Layers (NLRL) which addresses this issue by incorporating logical rules into the neural network framework. NLRL consists of multiple layers where each layer represents a set of logical rules that guide the flow of input data through the network. These rules are inferred from training examples using techniques from statistical relational learning and rule mining, ensuring that the resulting rules are both accurate and interpretable. We demonstrate the effectiveness of our approach on several benchmark datasets across different domains, showing that NLRL outperforms state-of-the-art baseline methods while also providing transparent and easy-to-understand explanations of its predictions. Our results highlight the potential of combining logic reasoning with deep learning techniques to develop more robust and reliable artificial intelligence systems.",1
"We present a detailed analysis of the class of regression decision tree algorithms which employ a regulized piecewise-linear node-splitting criterion and have regularized linear models at the leaves. From a theoretic standpoint, based on Rademacher complexity framework, we present new high-probability upper bounds for the generalization error for the proposed classes of regularized regression decision tree algorithms, including LASSO-type, and $\ell_{2}$ regularization for linear models at the leaves. Theoretical result are further extended by considering a general type of variable selection procedure. Furthermore, in our work we demonstrate that the class of piecewise-linear regression trees is not only numerically stable but can be made tractable via an algorithmic implementation, presented herein, as well as with the help of modern GPU technology. Empirically, we present results on multiple datasets which highlight the strengths and potential pitfalls, of the proposed tree algorithms compared to baselines which grow trees based on piecewise constant models.",0
"In order to make predictions from large datasets efficiently while still being accurate, we introduce the idea of piecewise linear regression trees regularized using l1 or l2 penalties. These algorithms have been shown to produce results that are competitive with other state-of-the art methods on several benchmarks, while also being able to handle missing data and categorical variables without imputation, which can save computational time and lead to more robust models. We demonstrate how our approach works through examples using both simulated and real data, showing how it effectively learns interpretable features and makes accurate predictions. Additionally, we compare our method against existing techniques and showcase its advantages in terms of model complexity, interpretability, and accuracy. Finally, we discuss potential future directions for research involving regularization and extension of the algorithm to multioutput tasks. Our work provides a novel and efficient framework for regression analysis in big data settings, particularly for highdimensional problems where regularization plays an important role.",1
"Gradient Boosted Decision Trees (GBDT) is a very successful ensemble learning algorithm widely used across a variety of applications. Recently, several variants of GBDT training algorithms and implementations have been designed and heavily optimized in some very popular open sourced toolkits including XGBoost, LightGBM and CatBoost. In this paper, we show that both the accuracy and efficiency of GBDT can be further enhanced by using more complex base learners. Specifically, we extend gradient boosting to use piecewise linear regression trees (PL Trees), instead of piecewise constant regression trees, as base learners. We show that PL Trees can accelerate convergence of GBDT and improve the accuracy. We also propose some optimization tricks to substantially reduce the training time of PL Trees, with little sacrifice of accuracy. Moreover, we propose several implementation techniques to speedup our algorithm on modern computer architectures with powerful Single Instruction Multiple Data (SIMD) parallelism. The experimental results show that GBDT with PL Trees can provide very competitive testing accuracy with comparable or less training time.",0
"This paper presents a new method for gradient boosting with piece-wise linear regression trees. The proposed approach combines the strengths of both methods, allowing for accurate predictions over complex datasets. We demonstrate that our algorithm outperforms state-of-the-art baselines on several benchmark datasets, while also providing interpretable models through feature importance measures. Our study shows that the use of piece-wise linear functions within the tree structure leads to better generalization performance and more efficient model training. Overall, we believe that this work contributes valuable insights into the development of effective machine learning algorithms for real-world applications.",1
"In this paper, we apply a new promising tool for pattern classification, namely, the Tsetlin Machine (TM), to the field of disease forecasting. The TM is interpretable because it is based on manipulating expressions in propositional logic, leveraging a large team of Tsetlin Automata (TA). Apart from being interpretable, this approach is attractive due to its low computational cost and its capacity to handle noise. To attack the problem of forecasting, we introduce a preprocessing method that extends the TM so that it can handle continuous input. Briefly stated, we convert continuous input into a binary representation based on thresholding. The resulting extended TM is evaluated and analyzed using an artificial dataset. The TM is further applied to forecast dengue outbreaks of all the seventeen regions in the Philippines using the spatio-temporal properties of the data. Experimental results show that dengue outbreak forecasts made by the TM are more accurate than those obtained by a Support Vector Machine (SVM), Decision Trees (DTs), and several multi-layered Artificial Neural Networks (ANNs), both in terms of forecasting precision and F1-score.",0
"The tsetlin machine has become a powerful tool for forecasting disease outbreaks. However, one potential drawback is that it requires static input data to generate predictions. To address this limitation, we propose a new scheme for continuous input to the tsetlin machine using time series analysis. We demonstrate the effectiveness of our approach by applying it to simulated epidemiological datasets as well as real world data from past disease outbreaks. Our results show that using continuous input significantly improves prediction accuracy compared to traditional methods. This research provides valuable insights into how the tsetlin machine can be adapted to work with evolving data streams, making it an even more powerful tool for public health officials charged with monitoring and predicting disease outbreaks.",1
"In recent years, there are many attempts to understand popular heuristics. An example of such a heuristic algorithm is the ID3 algorithm for learning decision trees. This algorithm is commonly used in practice, but there are very few theoretical works studying its behavior. In this paper, we analyze the ID3 algorithm, when the target function is a $k$-Junta, a function that depends on $k$ out of $n$ variables of the input. We prove that when $k = \log n$, the ID3 algorithm learns in polynomial time $k$-Juntas, in the smoothed analysis model of Kalai & Teng. That is, we show a learnability result when the observed distribution is a ""noisy"" variant of the original distribution.",0
"In recent years, there has been significant interest in developing machine learning algorithms that can effectively model complex distributions in high-dimensional spaces. One area where these models have seen particular success is in the analysis of consumer demand data, which is often characterized by nonlinear relationships between inputs and outputs. In this work, we propose the use of the Inductive Dependence (ID) algorithm with learn junctions as a means of constructing accurate and interpretable product distribution models from large datasets. Our approach utilizes the smoothness assumption of the underlying functions to regularize the learned parameters and improve robustness to noise. We evaluate our method on several real-world datasets and demonstrate its ability to accurately capture important features such as customer preferences and market trends. Overall, our results suggest that ID3 LearnJuntas is a promising tool for understanding consumer behavior and informing business decisions in industries ranging from retail to finance.",1
"In an attempt to gather a deeper understanding of how convolutional neural networks (CNNs) reason about human-understandable concepts, we present a method to infer labeled concept data from hidden layer activations and interpret the concepts through a shallow decision tree. The decision tree can provide information about which concepts a model deems important, as well as provide an understanding of how the concepts interact with each other. Experiments demonstrate that the extracted decision tree is capable of accurately representing the original CNN's classifications at low tree depths, thus encouraging human-in-the-loop understanding of discriminative concepts.",0
"In summary: ""We present a method that extracts interpretable concept-based decision trees directly from convolutional neural networks (CNNs). This enables human understanding and control over their decisions without requiring them to cross reference with external knowledge sources.""",1
"For many machine learning algorithms, predictive performance is critically affected by the hyperparameter values used to train them. However, tuning these hyperparameters can come at a high computational cost, especially on larger datasets, while the tuned settings do not always significantly outperform the default values. This paper proposes a recommender system based on meta-learning to identify exactly when it is better to use default values and when to tune hyperparameters for each new dataset. Besides, an in-depth analysis is performed to understand what they take into account for their decisions, providing useful insights. An extensive analysis of different categories of meta-features, meta-learners, and setups across 156 datasets is performed. Results show that it is possible to accurately predict when tuning will significantly improve the performance of the induced models. The proposed system reduces the time spent on optimization processes, without reducing the predictive performance of the induced models (when compared with the ones obtained using tuned hyperparameters). We also explain the decision-making process of the meta-learners in terms of linear separability-based hypotheses. Although this analysis is focused on the tuning of Support Vector Machines, it can also be applied to other algorithms, as shown in experiments performed with decision trees.",0
In this paper we present a meta-learnin... (more),1
"Recent efforts in Machine Learning (ML) interpretability have focused on creating methods for explaining black-box ML models. However, these methods rely on the assumption that simple approximations, such as linear models or decision-trees, are inherently human-interpretable, which has not been empirically tested. Additionally, past efforts have focused exclusively on comprehension, neglecting to explore the trust component necessary to convince non-technical experts, such as clinicians, to utilize ML models in practice. In this paper, we posit that reinforcement learning (RL) can be used to learn what is interpretable to different users and, consequently, build their trust in ML models. To validate this idea, we first train a neural network to provide risk assessments for heart failure patients. We then design a RL-based clinical decision-support system (DSS) around the neural network model, which can learn from its interactions with users. We conduct an experiment involving a diverse set of clinicians from multiple institutions in three different countries. Our results demonstrate that ML experts cannot accurately predict which system outputs will maximize clinicians' confidence in the underlying neural network model, and suggest additional findings that have broad implications to the future of research into ML interpretability and the use of ML in medicine.",0
"This research addresses machine learning interpretability by presenting evidence that decision support systems have specific features that make them interpretable, usable, traceable, understandable, trustworthy, transparent, explainable, reliable, validatable, and justifiable. We show how different aspects affect human understanding and action on the basis of results from ML models using case studies and experiments; we provide details on data preprocessing, feature extraction, model training, model evaluation, post-model deployment analysis, model maintenance, user feedback, knowledge representation, and presentation of results. Furthermore, we demonstrate how these attributes can be formalized into system requirements, guidelines, templates, checklists, tools, libraries and other artifacts supporting their development. Finally, our contribution provides insight into the role that human cognition plays in building such complex computational intelligence applications like DSS based on machine learning and conversely, into how computer-supported technologies shape modern ways of thinking about decisions, their making process and consequences.",1
"Although adversarial examples and model robustness have been extensively studied in the context of linear models and neural networks, research on this issue in tree-based models and how to make tree-based models robust against adversarial examples is still limited. In this paper, we show that tree based models are also vulnerable to adversarial examples and develop a novel algorithm to learn robust trees. At its core, our method aims to optimize the performance under the worst-case perturbation of input features, which leads to a max-min saddle point problem. Incorporating this saddle point objective into the decision tree building procedure is non-trivial due to the discrete nature of trees --- a naive approach to finding the best split according to this saddle point objective will take exponential time. To make our approach practical and scalable, we propose efficient tree building algorithms by approximating the inner minimizer in this saddle point problem, and present efficient implementations for classical information gain based trees as well as state-of-the-art tree boosting models such as XGBoost. Experimental results on real world datasets demonstrate that the proposed algorithms can substantially improve the robustness of tree-based models against adversarial examples.",0
"Here we describe how decision trees can be made robust against adversarial examples by increasing their ability to generalize well under distribution shift. Our main contribution is showing that training on data generated through traditional image transformations leads to good performance at defending against adversarial examples. We show that our method works better than a popular existing defense, distillation, which shows promise but has been outperformed recently. We also experiment with using the adversary as part of training data; however, surprisingly, this actually decreases performance in most scenarios. Finally, all models were trained using the fastai library (https://github.com/fastai/fastai).",1
"Print quality is an important criterion for a printer's performance. The detection, classification, and assessment of printing defects can reflect the printer's working status and help to locate mechanical problems inside. To handle all these questions, an efficient algorithm is needed to replace the traditionally visual checking method. In this paper, we focus on pages with local defects including gray spots and solid spots. We propose a coarse-to-fine method to detect local defects in a block-wise manner, and aggregate the blockwise attributes to generate the feature vector of the whole test page for a further ranking task. In the detection part, we first select candidate regions by thresholding a single feature. Then more detailed features of candidate blocks are calculated and sent to a decision tree that is previously trained on our training dataset. The final result is given by the decision tree model to control the false alarm rate while maintaining the required miss rate. Our algorithm is proved to be effective in detecting and classifying local defects compared with previous methods.",0
"This research proposal focuses on developing an algorithm for detecting local defects using blockwise processing techniques. Detecting local defects such as cracks, potholes, and other surface irregularities is crucial for maintaining safe road conditions and preventing accidents. Conventional methods rely on scanning large areas pixel by pixel, which can lead to high computational complexity and limited scalability. To address these challenges, we propose a novel approach that leverages the divide-and-conquer principle by partitioning images into smaller blocks and applying pattern recognition techniques within each block. Our method significantly reduces computational cost while improving accuracy compared to existing solutions. We demonstrate our findings through extensive experiments using real-world data sets and provide insights into future directions for further improvement. Overall, our work provides a significant contribution to the field of computer vision and has important applications in smart transportation systems and urban infrastructure management.",1
"Interpretable surrogates of black-box predictors trained on high-dimensional tabular datasets can struggle to generate comprehensible explanations in the presence of correlated variables. We propose a model-agnostic interpretable surrogate that provides global and local explanations of black-box classifiers to address this issue. We introduce the idea of concepts as intuitive groupings of variables that are either defined by a domain expert or automatically discovered using correlation coefficients. Concepts are embedded in a surrogate decision tree to enhance its comprehensibility. First experiments on FRED-MD, a macroeconomic database with 134 variables, show improvement in human-interpretability while accuracy and fidelity of the surrogate model are preserved.",0
"Title of Paper: ""Concept Tree: High-level representation of variables for more interpretable surrogate decision trees""  Abstract: This paper presents a new approach called Concept Tree for creating more interpretable surrogate decision trees using high-level representations of variables. Existing methods rely on low-level features such as raw input data which can lead to complex models that lack transparency and interpretability. Our proposed method addresses these limitations by transforming raw input data into higher-level concepts before constructing decision trees. We demonstrate our method on both synthetic datasets and real world problems and show significant improvements over traditional approaches in terms of accuracy, stability, and interpretability. Additionally, we provide visualizations of the resulting concept trees to aid in understanding how the model makes predictions. Overall, Concept Tree provides a novel framework for solving complex problems while maintaining transparency and interpretability.",1
"Decision trees are an extremely popular machine learning technique. Unfortunately, overfitting in decision trees still remains an open issue that sometimes prevents achieving good performance. In this work, we present a novel approach for the construction of decision trees that avoids the overfitting by design, without losing accuracy. A distinctive feature of our algorithm is that it requires neither the optimization of any hyperparameters, nor the use of regularization techniques, thus significantly reducing the decision tree training time. Moreover, our algorithm produces much smaller and shallower trees than traditional algorithms, facilitating the interpretability of the resulting models.",0
"In recent years, decision tree methods have gained popularity due to their interpretability and ease of use. However, like many machine learning algorithms, they can suffer from overfitting if not properly regularized. Existing approaches to addressing overfitting in decision trees typically involve adding complexity penalties or using cross-validation to select hyperparameters such as maximum depth or minimum sample size required to split. These approaches have limitations; complexity penalties may reduce model fit on unseen data, while hyperparameter selection via cross-validation can lead to underfitting. To address these issues, we propose a novel, hyperparameter-free approach to constructing decision trees that inherently avoids overfitting through design. Our method involves recursively partitioning the data into subsets based on feature values and combining decisions made at each stage to form the final prediction. This process allows us to automatically determine appropriate depth without requiring any prior knowledge of the dataset or the number of features. We evaluate our approach on both synthetic and real datasets and demonstrate improved performance compared to existing state-of-the-art methods. Additionally, we provide insights into how our method works and discuss potential future research directions. Overall, our work presents a promising new direction for building interpretable models with high accuracy and low risk of overfitting.",1
"Predicting traffic incident duration is a major challenge for many traffic centres around the world. Most research studies focus on predicting the incident duration on motorways rather than arterial roads, due to a high network complexity and lack of data. In this paper we propose a bi-level framework for predicting the accident duration on arterial road networks in Sydney, based on operational requirements of incident clearance target which is less than 45 minutes. Using incident baseline information, we first deploy a classification method using various ensemble tree models in order to predict whether a new incident will be cleared in less than 45min or not. If the incident was classified as short-term, then various regression models are developed for predicting the actual incident duration in minutes by incorporating various traffic flow features. After outlier removal and intensive model hyper-parameter tuning through randomized search and cross-validation, we show that the extreme gradient boost approach outperformed all models, including the gradient-boosted decision-trees by almost 53%. Finally, we perform a feature importance evaluation for incident duration prediction and show that the best prediction results are obtained when leveraging the real-time traffic flow in vicinity road sections to the reported accident location.",0
"Accurately predicting how long an arterial incident lasts can greatly benefit traffic management agencies by allowing them to deploy resources efficiently and quickly mitigate congestion effects. Existing research has focused on developing statistical models that rely heavily on historical data, but these methods have limited ability to adapt to changes in real-time road conditions. In response to these shortcomings, we propose a new method called BiLevelExtremeGradientBoosting (BLXGB) which addresses two key challenges: model accuracy and computational efficiency. Our approach uses a novel tree ensemble technique that captures nonlinear patterns from both low-dimensional features and high-resolution spatiotemporal imagery. Using real world case study datasets collected from downtown Toronto, BLXGB achieves state-of-the-art performance outperforming existing methods by over 27%. Furthermore, our results showcase how BLXGB provides insights into factors influencing incident durations by identifying relationships between attributes such as incident type, location, weather, day of week, among others. Overall, our work offers significant potential impact to improve transportation systems management through improved incident duration predictions.",1
"Leather is a type of natural, durable, flexible, soft, supple and pliable material with smooth texture. It is commonly used as a raw material to manufacture luxury consumer goods for high-end customers. To ensure good quality control on the leather products, one of the critical processes is the visual inspection step to spot the random defects on the leather surfaces and it is usually conducted by experienced experts. This paper presents an automatic mechanism to perform the leather defect classification. In particular, we focus on detecting tick-bite defects on a specific type of calf leather. Both the handcrafted feature extractors (i.e., edge detectors and statistical approach) and data-driven (i.e., artificial neural network) methods are utilized to represent the leather patches. Then, multiple classifiers (i.e., decision trees, Support Vector Machines, nearest neighbour and ensemble classifiers) are exploited to determine whether the test sample patches contain defective segments. Using the proposed method, we managed to get a classification accuracy rate of 84% from a sample of approximately 2500 pieces of 400 * 400 leather patches.",0
"In recent years, leather defect classification has become increasingly important in the manufacturing industry due to the need for high quality products that meet customer expectations. Traditional methods used for defect detection such as manual inspection can often lead to subjective results, errors in judgment, and low accuracy rates. Therefore, automation through machine vision technology and artificial intelligence algorithms has emerged as a solution to improve defect detection accuracy and efficiency. This research proposes an integrated neural network and machine vision approach for leather defect classification which combines the strengths of both technologies to achieve more accurate results than either method alone. This study details our experimental setup, data collection process, feature extraction techniques, choice of model architecture, training procedure, evaluation metrics, and performance comparison of different models. Our results show that our proposed approach outperforms traditional computer vision techniques by achieving higher precision, recall, F1 score, and reduced error rates. Overall, our research demonstrates the potential of using neural networks and machine vision to enhance leather defect classification and contribute to improved production processes.",1
"Tree-based machine learning models such as random forests, decision trees, and gradient boosted trees are the most popular non-linear predictive models used in practice today, yet comparatively little attention has been paid to explaining their predictions. Here we significantly improve the interpretability of tree-based models through three main contributions: 1) The first polynomial time algorithm to compute optimal explanations based on game theory. 2) A new type of explanation that directly measures local feature interaction effects. 3) A new set of tools for understanding global model structure based on combining many local explanations of each prediction. We apply these tools to three medical machine learning problems and show how combining many high-quality local explanations allows us to represent global structure while retaining local faithfulness to the original model. These tools enable us to i) identify high magnitude but low frequency non-linear mortality risk factors in the general US population, ii) highlight distinct population sub-groups with shared risk characteristics, iii) identify non-linear interaction effects among risk factors for chronic kidney disease, and iv) monitor a machine learning model deployed in a hospital by identifying which features are degrading the model's performance over time. Given the popularity of tree-based machine learning models, these improvements to their interpretability have implications across a broad set of domains.",0
"Artificial intelligence (AI) has made significant progress in recent years, but there remains a challenge in understanding how these models make decisions. One important application area where such explainability would prove beneficial is computer vision; image classifiers, scene segmenters, object detectors and others, all require transparent reasoning so that users can trust them and design future applications on top of them. In particular, tree ensembles (e.g., Random Forests and Gradient Boosting Machines) have shown excellent predictive performance across many problems, however they remain difficult to interpret due to their reliance on decision trees whose logic might differ from human knowledge. We focus on developing methods which generate intuitive explanations for locally relevant features, as well as global visualizations capable of providing insights into deep hierarchical representations. Our first approach summarizes predictions within local regions; we train binary classifiers at every pixel location and provide a heatmap of discriminative responses together with their mutual dependencies. These regional explanations allow us to understand both positive/negative predictions as well as spatial layout for segmentation maps. Next, motivated by cognitive perception studies indicating humans rely on contextual reasoning over isolated details, we present global techniques able to describe high-level concepts captured by the model. This is achieved through generative methods like PixelRNN that encode and decode images based on hidden activations stored during training -- even though the internal representation may change over different layers our framework easily adapts and delivers meaningful outputs. Finally, leveraging a recent surge in interpretable neural networks we showcase two cases of integrating intrinsically simpler models as components inside more complex pipelines; one learns to attend over the most influential forest leaves and another uses a novel type of attention directly operating o",1
"Recent years have witnessed the rapid development of human activity recognition (HAR) based on wearable sensor data. One can find many practical applications in this area, especially in the field of health care. Many machine learning algorithms such as Decision Trees, Support Vector Machine, Naive Bayes, K-Nearest Neighbor, and Multilayer Perceptron are successfully used in HAR. Although these methods are fast and easy for implementation, they still have some limitations due to poor performance in a number of situations. In this paper, we propose a novel method based on the ensemble learning to boost the performance of these machine learning methods for HAR.",0
"This research explores a novel approach to human activity recognition using wearable sensor data and machine learning techniques. Accurate identification of physical activities has significant applications ranging from healthcare and fitness monitoring to lifestyle analytics. Conventional methods have relied on laborious hand engineering and domain expertise. Our work presents a systematic study comparing various state-of-the-art feature extraction techniques applied to raw acceleration signals obtained from smartwatches and wristbands worn by participants during their daily routine. We investigate classifiers based on supervised, semi-supervised, self-training, transfer and unsupervised learning which demonstrate competitive performance with fine-grained class distinctions at over 97% accuracy. Our experiments aim to provide insight into optimal model design yielding high efficacy under real world constraints such as limited annotated training samples. The findings presented here have important implications for future development of efficient activity detection models capable of automatic adaptation through user interaction and online learning.",1
We provide a machine learning solution that replaces the traditional methods for deciding the pesticide application time of Sunn Pest. We correlate climate data with phases of Sunn Pest in its life-cycle and decide whether the fields should be sprayed. Our solution includes two groups of prediction models. The first group contains decision trees that predict migration time of Sunn Pest from winter quarters to wheat fields. The second group contains random forest models that predict the nymphal stage percentages of Sunn Pest which is a criterion for pesticide application. We trained our models on four years of climate data which was collected from Kir\c{s}ehir and Aksaray. The experiments show that our promised solution make correct predictions with high accuracies.,0
"Sunn pests pose significant threat to agriculture due to their ability to cause substantial yield losses and reduce crop quality. An effective forecasting system that can predict potential outbreaks and provide timely warnings to farmers could greatly mitigate these damages. This study aimed to develop a forecasting and warning system for the ecological life cycle of sunn pests by integrating current knowledge of their biology and population dynamics with cutting-edge modeling techniques. Our system was able to accurately capture seasonal patterns in sunn pest populations and produced reliable predictions of outbreaks under different weather conditions. With further validation and refinement, our system has great potential to become a valuable tool for agricultural decision making in regions where sunn pests are prevalent. Ultimately, we hope that our work contributes to sustainable and resilient food systems through proactive management of agroecosystems.",1
"We investigate how asymmetrizing an impurity function affects the choice of optimal node splits when growing a decision tree for binary classification. In particular, we relax the usual axioms of an impurity function and show how skewing an impurity function biases the optimal splits to isolate points of a particular class when splitting a node. We give a rigorous definition of this notion, then give a necessary and sufficient condition for such a bias to hold. We also show that the technique of class weighting is equivalent to applying a specific transformation to the impurity function, and tie all these notions together for a class of impurity functions that includes the entropy and Gini impurity. We also briefly discuss cost-insensitive impurity functions and give a characterization of such functions.",0
"Here is an example of how you can write an abstract for the paper: For many machine learning tasks, a decision tree classifier may provide sufficient predictive accuracy at low computational cost. Recent research has focused on improving tree performance through techniques such as ensemble methods, randomized feature selection, and attention mechanisms. However, one important factor that remains relatively unexplored is how split placement affects overall model performance. In this work, we study optimal splitting rules for binary classification trees by analyzing splits based on asymmetric impurity functions and class weights. We show how these new approaches significantly improve out-of-bag error rates compared to traditional heuristics like Gini importance and mean decrease impurity. Our results illustrate the potential benefits of adopting more sophisticated strategies when constructing decision trees for classification problems.",1
"This work presents an approach to automatically induction for non-greedy decision trees constructed from neural network architecture. This construction can be used to transfer weights when growing or pruning a decision tree, allowing non-greedy decision tree algorithms to automatically learn and adapt to the ideal architecture. In this work, we examine the underpinning ideas within ensemble modelling and Bayesian model averaging which allow our neural network to asymptotically approach the ideal architecture through weights transfer. Experimental results demonstrate that this approach improves models over fixed set of hyperparameters for decision tree models and decision forest models.",0
"Inductive decision trees have been used successfully as learning models in artificial intelligence applications for many years now. These tree algorithms can automatically extract rules from datasets by recursively splitting the data into subsets, which then allows them to identify patterns that explain underlying structures. However, these methods still have their limitations: they often require extensive manual tuning and parameterization, which can lead to overfitting issues if left unchecked. In our work, we aim to address these concerns by proposing a novel method called ""Automatic Induction,"" which integrates neural network ensembles within inductive decision tree frameworks to improve accuracy without relying on excessive fine-tuning. We demonstrate through experimental evaluation that this approach significantly outperforms existing techniques across multiple benchmark datasets and application domains. Our findings offer promising results towards advancing automatic machine learning methods that efficiently learn effective models without requiring extensive human intervention. Ultimately, this research has important implications for real-world deployment of intelligent systems based on data mining and knowledge discovery.",1
"Credit card has become popular mode of payment for both online and offline purchase, which leads to increasing daily fraud transactions. An Efficient fraud detection methodology is therefore essential to maintain the reliability of the payment system. In this study, we perform a comparison study of credit card fraud detection by using various supervised and unsupervised approaches. Specifically, 6 supervised classification models, i.e., Logistic Regression (LR), K-Nearest Neighbors (KNN), Support Vector Machines (SVM), Decision Tree (DT), Random Forest (RF), Extreme Gradient Boosting (XGB), as well as 4 unsupervised anomaly detection models, i.e., One-Class SVM (OCSVM), Auto-Encoder (AE), Restricted Boltzmann Machine (RBM), and Generative Adversarial Networks (GAN), are explored in this study. We train all these models on a public credit card transaction dataset from Kaggle website, which contains 492 frauds out of 284,807 transactions. The labels of the transactions are used for supervised learning models only. The performance of each model is evaluated through 5-fold cross validation in terms of Area Under the Receiver Operating Curves (AUROC). Within supervised approaches, XGB and RF obtain the best performance with AUROC = 0.989 and AUROC = 0.988, respectively. While for unsupervised approaches, RBM achieves the best performance with AUROC = 0.961, followed by GAN with AUROC = 0.954. The experimental results show that supervised models perform slightly better than unsupervised models in this study. Anyway, unsupervised approaches are still promising for credit card fraud transaction detection due to the insufficient annotation and the data imbalance issue in real-world applications.",0
"In recent years, credit card fraud has become increasingly prevalent, resulting in significant financial losses for individuals and institutions alike. As such, developing effective methods for detecting and preventing credit card fraud is crucial. This study compares two approaches to credit card fraud detection - supervised and unsupervised learning - using real-world data from a major bank. Our results show that while both approaches have their strengths and weaknesses, unsupervised learning can achieve better performance than supervised learning in certain scenarios, particularly when dealing with complex patterns in large datasets. However, further research is necessary to fully understand the potential benefits and limitations of these approaches. Ultimately, we hope our findings will contribute to the development of more robust and efficient solutions for combating credit card fraud.",1
"Shapelet is a discriminative subsequence of time series. An advanced shapelet-based method is to embed shapelet into accurate and fast random forest. However, it shows several limitations. First, random shapelet forest requires a large training cost for split threshold searching. Second, a single shapelet provides limited information for only one branch of the decision tree, resulting in insufficient accuracy and interpretability. Third, randomized ensemble causes interpretability declining. For that, this paper presents Random Pairwise Shapelets Forest (RPSF). RPSF combines a pair of shapelets from different classes to construct random forest. It omits threshold searching to be more efficient, includes more information for each node of the forest to be more effective. Moreover, a discriminability metric, Decomposed Mean Decrease Impurity (DMDI), is proposed to identify influential region for every class. Extensive experiments show RPSF improves the accuracy and training speed of shapelet-based forest. Case studies demonstrate the interpretability of our method.",0
"In recent years, deep learning has revolutionized many fields by providing powerful models capable of making accurate predictions on complex tasks such as image classification and speech recognition. Among these techniques, decision trees have gained attention due to their simplicity, interpretability, and effectiveness. However, traditional decision tree algorithms like ID3 and CART suffer from drawbacks such as poor scalability, overfitting, and limited ability to capture nonlinear relationships. To overcome these limitations, researchers have proposed more advanced variants of decision trees such as random forest and shapelet transform.  Random pairwise shapelets forest (RPSF) is a novel method that combines the strengths of both approaches. RPSF generates a diverse set of shapelets (local patterns) based on minimum description length principle, capturing informative parts of data without excessive complexity. Then, bootstrap aggregation (bagging) is applied to generate multiple decision trees by randomly selecting subsets of the training samples, reducing variance and increasing model robustness. Finally, pairwise constraints between shapelets are introduced to ensure mutual exclusion between them, further improving feature selection and handling noise effectively.  In our paper, we evaluate the performance of RPSF on several benchmark datasets commonly used in machine learning competitions, including real-world applications such as banknote authentication, credit risk assessment, and land cover mapping. Our results show that RPSF significantly outperforms state-of-the-art algorithms across different evaluation metrics and can handle high dimensionality better than other methods. Furthermore, we present visualizations of selected shapelets generated by RPSF, demonstrating their interpretability and utility for understanding complex datasets. Overall, RPSF provides an alternative approach to constructing accurate, interpretable, and efficient decision trees for predictive modelling. \end{document} ------------------------------------------------------------------------------",1
"Deep neural decision forest (NDF) achieved remarkable performance on various vision tasks via combining decision tree and deep representation learning. In this work, we first trace the decision-making process of this model and visualize saliency maps to understand which portion of the input influence it more for both classification and regression problems. We then apply NDF on a multi-task coordinate regression problem and demonstrate the distribution of routing probabilities, which is vital for interpreting NDF yet not shown for regression problems. The pre-trained model and code for visualization will be available at https://github.com/Nicholasli1995/VisualizingNDF",0
"This study presents a visualization technique that allows for an in-depth analysis of the decision making process in Deep Neural Decision Forests (DNDF). DNDFs have proven to be highly accurate classifiers but their lack of interpretability hinders their adoption in certain domains such as healthcare and finance where explainability is critical. Our method leverages recent advances in the field of saliency mapping which highlights regions of input data most relevant to a model’s prediction. By combining saliency maps with additional visualizations we create an intuitive representation of how different parts of the input contribute to the final decision, providing insights into the model behavior without compromising accuracy. We evaluate our approach on multiple benchmark datasets and demonstrate its effectiveness in revealing subtle patterns and biases present in the decision making process of DNDF models. Finally, we discuss potential applications of our method and future research directions aimed at further enhancing the transparency and trustworthiness of machine learning algorithms.",1
"In this paper we propose the use of Generative Adversarial Networks (GAN) to generate artificial training data for machine learning tasks. The generation of artificial training data can be extremely useful in situations such as imbalanced data sets, performing a role similar to SMOTE or ADASYN. It is also useful when the data contains sensitive information, and it is desirable to avoid using the original data set as much as possible (example: medical data). We test our proposal on benchmark data sets using different network architectures, and show that a Decision Tree (DT) classifier trained using the training data generated by the GAN reached the same, (and surprisingly sometimes better), accuracy and recall than a DT trained on the original data set.",0
"In recent years, data augmentation has become an essential tool for training machine learning models, particularly deep neural networks, which can suffer from overfitting due to their high capacity. Generative Adversarial Networks (GANs) have emerged as a powerful approach to generate synthetic datasets that can greatly enhance the quality and quantity of available training data. However, most existing works on data augmentation using GANs focus on generating new samples based on simple image transformations such as cropping, flipping, rotating, shifting, etc. They do not exploit more advanced features present in high resolution images such as objects, texture, lighting conditions, etc. This work presents an approach to integrate these kinds of fine-grained details into the generation process by incorporating object detectors and attribute classifiers into our framework. We propose a novel model architecture called Styled Attention Generators (SAGENs), which combines the strengths of both attention mechanism and style transfer techniques. Our extensive experiments demonstrate that SAGENs significantly outperform competitive baselines, improving the performance of several state-of-the-art computer vision algorithms across different tasks and datasets. Our method opens up promising avenues towards more effective uses of generative adversarial networks for data augmentation.",1
"Forecasting of weakly correlated time series of conversion rate by methods of exponential smoothing, neural network and decision tree on the example of conversion percent series for an electronic store is considered in the paper. The advantages and disadvantages of each method are considered.",0
"Abstract: In many tasks of electronic commerce (EC), forecasting time series data that has low correlation over time can be challenging due to the irregular nature of such patterns. However, these types of weak correlations may still possess underlying structures that make them predictable to some extent. This study aimed to explore how we can develop novel approaches for effectively forecasting time series with weak correlations encountered in EC applications. To achieve this objective, several state-of-the-art techniques were employed, including machine learning algorithms, optimization methods, and pattern recognition strategies. Our experimental results indicated that our proposed models consistently outperformed traditional methods by achieving significantly higher accuracy rates in both short-term and long-term prediction scenarios. These findings underscore the importance of studying and improving upon current methodologies for tackling real-world problems involving weakly correlated time series data in EC contexts. Overall, this research contributes new insights into the field of time series analysis and highlights promising directions for future investigation in developing more effective forecasting tools for EC applications with weak correlations. Keywords: Time series forecasting, weak correlation, electronic commerce, machine learning, optimization.",1
"Inferring a decision tree from a given dataset is one of the classic problems in machine learning. This problem consists of buildings, from a labelled dataset, a tree such that each node corresponds to a class and a path between the tree root and a leaf corresponds to a conjunction of features to be satisfied in this class. Following the principle of parsimony, we want to infer a minimal tree consistent with the dataset. Unfortunately, inferring an optimal decision tree is known to be NP-complete for several definitions of optimality. Hence, the majority of existing approaches relies on heuristics, and as for the few exact inference approaches, they do not work on large data sets. In this paper, we propose a novel approach for inferring a decision tree of a minimum depth based on the incremental generation of Boolean formula. The experimental results indicate that it scales sufficiently well and the time it takes to run grows slowly with the size of dataset.",0
"In recent years, decision trees have become one of the most popular methods for solving complex machine learning problems due to their ability to handle both categorical and continuous variables. However, constructing optimal decision trees from large datasets can pose significant challenges, particularly as the number of features grows exponentially larger than the number of instances in the dataset. This paper presents a new algorithm for learning optimal decision trees that scales linearly with respect to the size of the data while producing trees that outperform those generated by existing state-of-the-art algorithms. Our approach is based on recursive feature partitioning and incorporates efficient techniques for managing computational resources. Experimental results demonstrate the effectiveness of our method in terms of accuracy, interpretability, and scalability across a wide range of real-world applications. Overall, our work provides a valuable contribution to the field of decision tree construction in the era of big data.",1
"Industrial process control systems try to keep an output variable within a given tolerance around a target value. PID control systems have been widely used in industry to control input variables in order to reach this goal. However, this kind of Transfer Function based approach cannot be extended to complex processes where input data might be non-numeric, high dimensional, sparse, etc. In such cases, there is still a need for determining the subspace of input data that produces an output within a given range. This paper presents a non-stochastic heuristic to determine input values for a mathematical function or trained regression model given an output range. The proposed method creates a synthetic training data set of input combinations with a class label that indicates whether the output is within the given target range or not. Then, a decision tree classifier is used to determine the subspace of input data of interest. This method is more general than a traditional controller as the target range for the output does not have to be centered around a reference value and it can be applied given a regression model of the output variable, which may have categorical variables as inputs and may be high dimensional, sparse... The proposed heuristic is validated with a proof of concept on a real use case where the quality of a lamination factory is established to identify the suitable subspace of production variable values.",0
"In order to determine optimal parameters of machine learning algorithms, data scientists need to have knowledge on possible range of inputs that affect the outcome. In manufacturing systems of Industry 4.0, such as digital twins, these optimal parameters can improve quality and reduce production time, leading to increased productivity. However, due to complex nature of physical interactions and variability involved in industrial processes, there is no universal method to estimate input ranges for all cases. This research proposes a heuristic approach based on expert insights to provide bounds of input variables that lead to acceptable results. To illustrate applicability of the proposed methodology, we analyze real industrial datasets from several different application areas. Our findings demonstrate the effectiveness of our approach in identifying valid ranges of input parameters resulting in desired outcomes in manufacturing systems. These ranges then serve as valuable guidelines for configuring predictive models during deployment, ensuring reliable performance of such systems at scale. With widespread adoption of Industry 4.0 technologies, accurate estimation of relevant input parameter domains would facilitate intelligent decision making and enhance overall efficiency of smart factories across industries.",1
"Source code reviews are manual, time-consuming, and expensive. Human involvement should be focused on analyzing the most relevant aspects of the program, such as logic and maintainability, rather than amending style, syntax, or formatting defects. Some tools with linting capabilities can format code automatically and report various stylistic violations for supported programming languages. They are based on rules written by domain experts, hence, their configuration is often tedious, and it is impractical for the given set of rules to cover all possible corner cases. Some machine learning-based solutions exist, but they remain uninterpretable black boxes. This paper introduces STYLE-ANALYZER, a new open source tool to automatically fix code formatting violations using the decision tree forest model which adapts to each codebase and is fully unsupervised. STYLE-ANALYZER is built on top of our novel assisted code review framework, Lookout. It accurately mines the formatting style of each analyzed Git repository and expresses the found format patterns with compact human-readable rules. STYLE-ANALYZER can then suggest style inconsistency fixes in the form of code review comments. We evaluate the output quality and practical relevance of STYLE-ANALYZER by demonstrating that it can reproduce the original style with high precision, measured on 19 popular JavaScript projects, and by showing that it yields promising results in fixing real style mistakes. STYLE-ANALYZER includes a web application to visualize how the rules are triggered. We release STYLE-ANALYZER as a reusable and extendable open source software package on GitHub for the benefit of the community.",0
"Introduce yourself as if you were presenting at a research conference. We’ve already established that I am an AI language model. But thank you for the reminder!  Hello everyone, my name is Open Assistant and I’m here to talk about our recent work on improving software quality through machine learning. One major source of bugs and errors in large codebases comes from code style inconsistencies, which can cause confusion among developers and lead to costly maintenance issues down the line. In our new tool, Style Analyzer, we use unsupervised algorithms inspired by human interpreters of natural language texts to identify these inconsistencies and suggest improvements automatically. By leveraging the power of deep learning models trained on massive datasets of high-quality code, we can catch mistakes before they make their way into production systems. This technology has the potential to transform how organizations manage their software development pipelines, making coding more efficient, effective, and collaborative than ever before. Thank you.",1
"In recent years, automated data-driven decision-making systems have enjoyed a tremendous success in a variety of fields (e.g., to make product recommendations, or to guide the production of entertainment). More recently, these algorithms are increasingly being used to assist socially sensitive decision-making (e.g., to decide who to admit into a degree program or to prioritize individuals for public housing). Yet, these automated tools may result in discriminative decision-making in the sense that they may treat individuals unfairly or unequally based on membership to a category or a minority, resulting in disparate treatment or disparate impact and violating both moral and ethical standards. This may happen when the training dataset is itself biased (e.g., if individuals belonging to a particular group have historically been discriminated upon). However, it may also happen when the training dataset is unbiased, if the errors made by the system affect individuals belonging to a category or minority differently (e.g., if misclassification rates for Blacks are higher than for Whites). In this paper, we unify the definitions of unfairness across classification and regression. We propose a versatile mixed-integer optimization framework for learning optimal and fair decision trees and variants thereof to prevent disparate treatment and/or disparate impact as appropriate. This translates to a flexible schema for designing fair and interpretable policies suitable for socially sensitive decision-making. We conduct extensive computational studies that show that our framework improves the state-of-the-art in the field (which typically relies on heuristics) to yield non-discriminative decisions at lower cost to overall accuracy.",0
"In today’s society, machine learning models can have detrimental consequences due to unfair decision making processes. There exists a need for developing fairer machine learning algorithms that make non-discriminatory decisions while still maintaining high accuracy levels. This paper discusses how optimal and fair decision trees can address these concerns by integrating statistical tests into tree building techniques such as ID3 and C4.5, ensuring both accurate predictions and statistical significance through pruning the unbiased subspace. We evaluate our proposed model on three publicly available datasets and demonstrate significant improvements over standard decision trees. Our results showcase the potential of using optimal and fair decision trees in various applications where fairness is essential without compromising on performance. As artificial intelligence continues to advance, implementing ethical considerations like those presented in this study should become mandatory practice across all industries leveraging AI technology. Overall, we believe that achieving transparency, interpretability, and accountability is imperative in creating trustworthy, reliable systems that reflect human values and respect individuals’ dignity.",1
"Size of the training dataset is an important factor in the performance of a machine learning algorithms and tools used in medical image processing are not exceptions. Machine learning tools normally require a decent amount of training data before they could efficiently predict a target. For image processing and computer vision, the number of images determines the validity and reliability of the training set. Medical images in some cases, suffer from poor quality and inadequate quantity required for a suitable training set. The proposed algorithm in this research obviates the need for large or even small image datasets used in machine learning based image enlargement techniques by extracting the required data from a single image. The extracted data was then introduced to a decision tree regressor for upscaling greyscale medical images at different zoom levels. Results from the algorithm are relatively acceptable compared to third-party applications and promising for future research. This technique could be tailored to the requirements of other machine learning tools and the results may be improved by further tweaking of the tools hyperparameters.",0
"This paper presents a novel pixel-averaging technique for extracting training data from a single image, which can then be used in machine learning (ML) based image enlargement algorithms. The proposed method involves averaging neighboring pixels to create new, average pixels that better represent the image as a whole. These average pixels can then be used to train an ML model to perform image enlargement tasks, resulting in improved image quality compared to traditional interpolation methods. The effectiveness of the proposed technique is demonstrated through experimental results on various benchmark datasets, showing significant improvements over existing state-of-the-art approaches. Overall, this work represents a step towards more efficient and accurate image processing techniques using ML.",1
"Interpretable classification models are built with the purpose of providing a comprehensible description of the decision logic to an external oversight agent. When considered in isolation, a decision tree, a set of classification rules, or a linear model, are widely recognized as human-interpretable. However, such models are generated as part of a larger analytical process. Bias in data collection and preparation, or in model's construction may severely affect the accountability of the design process. We conduct an experimental study of the stability of interpretable models with respect to feature selection, instance selection, and model selection. Our conclusions should raise awareness and attention of the scientific community on the need of a stability impact assessment of interpretable models.",0
"This paper provides insights into whether interpretable models can actually maintain their performance over time under different scenarios. Various types of interpretable models have been considered from both academia and industry during recent years due to the increasing importance of interpretability in artificial intelligence (AI). However, there has been limited research on evaluating the stability of these models, which could lead to poor decision making if the model changes unexpectedly.  The main contribution of this study is a comprehensive evaluation framework that considers three aspects: data variation, model change, and external shocks. We conduct experiments using four popular linear interpretable models and one deep learning baseline method, analyzing how they perform across six real datasets with varying degrees of variability. Our findings show that while some interpretable models are more stable than others in certain conditions, none of them consistently outperform all other methods in terms of stability. Interestingly, we observe that non-interpretable methods can sometimes achieve better stability compared to interpretable ones, challenging the common belief that interpretability leads to greater robustness automatically. Lastly, we provide guidelines on choosing appropriate interpretable models based on application requirements and available resources, highlighting the tradeoff between stability, explainability, and efficiency. Overall, our work provides new perspectives on understanding the limitations of interpretable models and suggests future directions for improving their reliability.",1
"We aim at developing and improving the imbalanced business risk modeling via jointly using proper evaluation criteria, resampling, cross-validation, classifier regularization, and ensembling techniques. Area Under the Receiver Operating Characteristic Curve (AUC of ROC) is used for model comparison based on 10-fold cross validation. Two undersampling strategies including random undersampling (RUS) and cluster centroid undersampling (CCUS), as well as two oversampling methods including random oversampling (ROS) and Synthetic Minority Oversampling Technique (SMOTE), are applied. Three highly interpretable classifiers, including logistic regression without regularization (LR), L1-regularized LR (L1LR), and decision tree (DT) are implemented. Two ensembling techniques, including Bagging and Boosting, are applied on the DT classifier for further model improvement. The results show that, Boosting on DT by using the oversampled data containing 50% positives via SMOTE is the optimal model and it can achieve AUC, recall, and F1 score valued 0.8633, 0.9260, and 0.8907, respectively.",0
"Business risks associated with imbalanced classes can greatly impact decision making and affect a company's overall performance. In order to predict these risks accurately, we propose a framework that combines several machine learning techniques including: resampling methods, regularization algorithms, and ensemble models. Our approach increases accuracy by identifying patterns from data with unrepresented minority classes, reducing overfitting through regularization, and combining predictions made by multiple models. Experiments conducted on various datasets show significant improvement in prediction results compared to traditional single-model approaches. This research contributes to the field by providing more accurate ways to evaluate and manage risks associated with small groups within larger populations.",1
"We introduce a multi-agent meta-modeling game to generate data, knowledge, and models that make predictions on constitutive responses of elasto-plastic materials. We introduce a new concept from graph theory where a modeler agent is tasked with evaluating all the modeling options recast as a directed multigraph and find the optimal path that links the source of the directed graph (e.g. strain history) to the target (e.g. stress) measured by an objective function. Meanwhile, the data agent, which is tasked with generating data from real or virtual experiments (e.g. molecular dynamics, discrete element simulations), interacts with the modeling agent sequentially and uses reinforcement learning to design new experiments to optimize the prediction capacity. Consequently, this treatment enables us to emulate an idealized scientific collaboration as selections of the optimal choices in a decision tree search done automatically via deep reinforcement learning.",0
"In recent years, there has been significant progress in developing artificial intelligence (AI) methods that can learn from data generated by physical experiments. One area where these methods have seen particular success is in the field of elasto-plasticity modeling, which involves predicting how materials deform over time under different loads. However, designing effective experiments to generate training data remains challenging due to the complexity of material behavior and the difficulty of measuring certain properties directly. This paper presents a new approach to address these issues through the use of collaborative games involving both humans and AI agents. By leveraging human creativity and intuition alongside machine learning algorithms, we demonstrate that it is possible to efficiently identify high-quality experimental designs capable of producing accurate predictions using a combined set of theoretical, numerical and experimental methods. Our results show promising improvements compared to state-of-the-art methods and pave the way towards more reliable computational tools for engineering applications.",1
"Existing guarantees in terms of rigorous upper bounds on the generalization error for the original random forest algorithm, one of the most frequently used machine learning methods, are unsatisfying. We discuss and evaluate various PAC-Bayesian approaches to derive such bounds. The bounds do not require additional hold-out data, because the out-of-bag samples from the bagging in the training process can be exploited. A random forest predicts by taking a majority vote of an ensemble of decision trees. The first approach is to bound the error of the vote by twice the error of the corresponding Gibbs classifier (classifying with a single member of the ensemble selected at random). However, this approach does not take into account the effect of averaging out of errors of individual classifiers when taking the majority vote. This effect provides a significant boost in performance when the errors are independent or negatively correlated, but when the correlations are strong the advantage from taking the majority vote is small. The second approach based on PAC-Bayesian C-bounds takes dependencies between ensemble members into account, but it requires estimating correlations between the errors of the individual classifiers. When the correlations are high or the estimation is poor, the bounds degrade. In our experiments, we compute generalization bounds for random forests on various benchmark data sets. Because the individual decision trees already perform well, their predictions are highly correlated and the C-bounds do not lead to satisfactory results. For the same reason, the bounds based on the analysis of Gibbs classifiers are typically superior and often reasonably tight. Bounds based on a validation set coming at the cost of a smaller training set gave better performance guarantees, but worse performance in most experiments.",0
"Title: Analysis of PAC-Bayesian bounds in random forest models  Random forest algorithms have gained significant popularity due to their excellent performance on complex datasets and ability to handle high dimensions and noisy data. However, despite their widespread use, there exists limited theoretical understanding of these methods. In particular, PAC-Bayesian bounds provide a powerful tool for analyzing learning algorithms by bounding the risk using a mixture distribution that depends on the empirical risk minimizer. This work presents a study of the applicability of PAC-Bayesian bounds to analyze random forest models. We show that existing PAC-Bayes bounds can be extended to random forest regression models under mild assumptions. Our results demonstrate the tightness of our analysis by comparing the established bounds to experimental results from real-world applications. Additionally, we explore extensions of PAC-Bayesian bounds to more general settings beyond random forest regression and discuss potential future research directions. By advancing the theoretical foundation for random forest models, this work provides valuable insights into the behavior of these widely used algorithms.",1
"Decision trees and logistic regression are one of the most popular and well-known machine learning algorithms, frequently used to solve a variety of real-world problems. Stability of learning algorithms is a powerful tool to analyze their performance and sensitivity and subsequently allow researchers to draw reliable conclusions. The stability of these two algorithms has remained obscure. To that end, in this paper, we derive two stability notions for decision trees and logistic regression: hypothesis and pointwise hypothesis stability. Additionally, we derive these notions for L2-regularized logistic regression and confirm existing findings that it is uniformly stable. We show that the stability of decision trees depends on the number of leaves in the tree, i.e., its depth, while for logistic regression, it depends on the smallest eigenvalue of the Hessian matrix of the cross-entropy loss. We show that logistic regression is not a stable learning algorithm. We construct the upper bounds on the generalization error of all three algorithms. Moreover, we present a novel stability measuring framework that allows one to measure the aforementioned notions of stability. The measures are equivalent to estimates of expected loss differences at an input example and then leverage bootstrap sampling to yield statistically reliable estimates. Finally, we apply this framework to the three algorithms analyzed in this paper to confirm our theoretical findings and, in addition, we discuss the possibilities of developing new training techniques to optimize the stability of logistic regression, and hence decrease its generalization error.",0
"Title: ""A Comparison of Decision Tree and Logistic Regression Model Accuracy and Robustness"" Abstract:",1
"We present a new distributed fuzzy partitioning method to reduce the complexity of multi-way fuzzy decision trees in Big Data classification problems. The proposed algorithm builds a fixed number of fuzzy sets for all variables and adjusts their shape and position to the real distribution of training data. A two-step process is applied : 1) transformation of the original distribution into a standard uniform distribution by means of the probability integral transform. Since the original distribution is generally unknown, the cumulative distribution function is approximated by computing the q-quantiles of the training set; 2) construction of a Ruspini strong fuzzy partition in the transformed attribute space using a fixed number of equally distributed triangular membership functions. Despite the aforementioned transformation, the definition of every fuzzy set in the original space can be recovered by applying the inverse cumulative distribution function (also known as quantile function). The experimental results reveal that the proposed methodology allows the state-of-the-art multi-way fuzzy decision tree (FMDT) induction algorithm to maintain classification accuracy with up to 6 million fewer leaves.",0
"In this paper we describe a new methodology that allows us to greatly simplify complex decision tree algorithms using the Probability Integral Transform (PIT) technique. By applying the PIT transformation on large datasets, the number of branches required by each decision tree can be drastically reduced without significantly affecting its accuracy. We show how this approach works in practice using real world data sets and provide empirical evidence demonstrating the improved performance compared to existing methods. Overall, our method offers significant advantages over traditional decision tree algorithms while preserving good predictive power. Our findings have important implications for businesses seeking better ways to make informed decisions based on big data analysis.",1
"This paper aims to quantitatively explain rationales of each prediction that is made by a pre-trained convolutional neural network (CNN). We propose to learn a decision tree, which clarifies the specific reason for each prediction made by the CNN at the semantic level. I.e., the decision tree decomposes feature representations in high conv-layers of the CNN into elementary concepts of object parts. In this way, the decision tree tells people which object parts activate which filters for the prediction and how much they contribute to the prediction score. Such semantic and quantitative explanations for CNN predictions have specific values beyond the traditional pixel-level analysis of CNNs. More specifically, our method mines all potential decision modes of the CNN, where each mode represents a common case of how the CNN uses object parts for prediction. The decision tree organizes all potential decision modes in a coarse-to-fine manner to explain CNN predictions at different fine-grained levels. Experiments have demonstrated the effectiveness of the proposed method.",0
"Title: ""A Study on the Interpretability of Convolutional Neural Networks using Decision Trees""  Abstract: This study explores the interpretability of convolutional neural networks (CNNs) by analyzing their decision-making process through the use of decision trees. Despite the remarkable success of deep learning methods such as CNNs in many tasks, understanding how these models make decisions remains challenging due to their complex nature. In this work, we propose a methodology to represent CNN decisions in interpretable form using decision trees, allowing for explainable reasoning and insights into model behavior. We conducted experiments using several popular benchmark datasets across different domains, including image classification, object detection, and segmentation tasks. Our results demonstrate that our approach can effectively visualize and analyze the inner workings of CNNs, providing valuable insight into factors influencing network predictions, highlighting potential limitations, and enabling more informed model design and fine-tuning. Overall, our findings contribute to the growing field of interpretable machine learning, addressing a critical need for transparency and accountability in advanced artificial intelligence systems.",1
"We introduce the problem of learning mixtures of $k$ subcubes over $\{0,1\}^n$, which contains many classic learning theory problems as a special case (and is itself a special case of others). We give a surprising $n^{O(\log k)}$-time learning algorithm based on higher-order multilinear moments. It is not possible to learn the parameters because the same distribution can be represented by quite different models. Instead, we develop a framework for reasoning about how multilinear moments can pinpoint essential features of the mixture, like the number of components.   We also give applications of our algorithm to learning decision trees with stochastic transitions (which also capture interesting scenarios where the transitions are deterministic but there are latent variables). Using our algorithm for learning mixtures of subcubes, we can approximate the Bayes optimal classifier within additive error $\epsilon$ on $k$-leaf decision trees with at most $s$ stochastic transitions on any root-to-leaf path in $n^{O(s + \log k)}\cdot\text{poly}(1/\epsilon)$ time. In this stochastic setting, the classic Occam algorithms for learning decision trees with zero stochastic transitions break down, while the low-degree algorithm of Linial et al. inherently has a quasipolynomial dependence on $1/\epsilon$.   In contrast, as we will show, mixtures of $k$ subcubes are uniquely determined by their degree $2 \log k$ moments and hence provide a useful abstraction for simultaneously achieving the polynomial dependence on $1/\epsilon$ of the classic Occam algorithms for decision trees and the flexibility of the low-degree algorithm in being able to accommodate stochastic transitions. Using our multilinear moment techniques, we also give the first improved upper and lower bounds since the work of Feldman et al. for the related but harder problem of learning mixtures of binary product distributions.",0
"Here you go! I hope it meets your expectations. Let me know if there is anything else I can do to assist you. ```sql In modern machine learning practice, many state-of-the-art algorithms rely on complex mathematical models and high-dimensional data representations. However, these methods often suffer from poor interpretability and lack of robustness to noise. In contrast, low-degree polynomial regression has been shown to produce simple models that generalize well across tasks while remaining interpretable. While powerful, these linear models have limitations in their expressivity due to restricting ourselves only to subspaces spanned by products of features up to degree $d$.  To mitigate these weaknesses, we propose extending the class of linear models to mixtures of subspaces spanned by subcube interactions (subspace mixtures). We show how to create meaningful interpretations based on identifying which interactions matter most within individual models. Our method provides better generalization than competing baselines and comparable accuracy to more flexible nonlinear models on benchmark datasets while having dramatically fewer parameters. Moreover, we present applications of our model such as analyzing the effects of multiple factors at once or predicting credit risk using loan-specific features as opposed to personal characteristics of borrowers.  We conclude that developing new techniques beyond traditional linear models like subspace mixture modelling has important benefits for producing interpretable yet accurate predictions and making decisions in real-world problems under constraints. This research contributes towards future work aimed at enhancing transparency, trust, and actionability of predictive analytics technologies. ``` Please note that this is just one possible example of an abstract, but I would recommend asking a human colleague to verify my writing before submitting any document containing my generated text to your advisor since I am still training and therefore untested outside of some controlled environments. Good luck with your submission! If ther",1
"Machine learning algorithms often contain many hyperparameters whose values affect the predictive performance of the induced models in intricate ways. Due to the high number of possibilities for these hyperparameter configurations, and their complex interactions, it is common to use optimization techniques to find settings that lead to high predictive accuracy. However, we lack insight into how to efficiently explore this vast space of configurations: which are the best optimization techniques, how should we use them, and how significant is their effect on predictive or runtime performance? This paper provides a comprehensive approach for investigating the effects of hyperparameter tuning on three Decision Tree induction algorithms, CART, C4.5 and CTree. These algorithms were selected because they are based on similar principles, have presented a high predictive performance in several previous works and induce interpretable classification models. Additionally, they contain many interacting hyperparameters to be adjusted. Experiments were carried out with different tuning strategies to induce models and evaluate the relevance of hyperparameters using 94 classification datasets from OpenML. Experimental results indicate that hyperparameter tuning provides statistically significant improvements for C4.5 and CTree in only one-third of the datasets, and in most of the datasets for CART. Different tree algorithms may present different tuning scenarios, but in general, the tuning techniques required relatively few iterations to find accurate solutions. Furthermore, the best technique for all the algorithms was the Irace. Finally, we find that tuning a specific small subset of hyperparameters contributes most of the achievable optimal predictive performance.",0
"Hyperparameter tuning is an essential step in machine learning model development as it helps improve the performance of models by adjusting their parameters. In recent years, there has been growing interest in using decision tree algorithms for classification tasks due to their interpretability and ease of use. However, selecting optimal hyperparameters for decision trees can be challenging. This paper presents an empirical study that evaluates different techniques for hyperparameter tuning of decision trees. We investigate the impact of key factors such as parameter settings, search spaces, evaluation metrics, and data preprocessing methods on the performance of decision trees. Our results show that careful selection of hyperparameters can significantly enhance the accuracy of decision trees, and that some commonly used strategies may lead to suboptimal results. Additionally, we provide insights into how specific hyperparameter choices affect the structure of decision trees, which can inform future research on model interpretation. Overall, our findings underscore the importance of systematic exploration of hyperparameter space and suggest promising directions for improving decision tree modeling.",1
"Nowadays with a growing number of online controlling systems in the organization and also a high demand of monitoring and stats facilities that uses data streams to log and control their subsystems, data stream mining becomes more and more vital. Hoeffding Trees (also called Very Fast Decision Trees a.k.a. VFDT) as a Big Data approach in dealing with the data stream for classification and regression problems showed good performance in handling facing challenges and making the possibility of any-time prediction. Although these methods outperform other methods e.g. Artificial Neural Networks (ANN) and Support Vector Regression (SVR), they suffer from high latency in adapting with new concepts when the statistical distribution of incoming data changes. In this article, we introduced a new algorithm that can detect and handle concept drift phenomenon properly. This algorithms also benefits from fast startup ability which helps systems to be able to predict faster than other algorithms at the beginning of data stream arrival. We also have shown that our approach will overperform other controversial approaches for classification and regression tasks.",0
"In today’s world, data stream mining has become an essential task due to the vast amounts of data generated every day from different sources such as social media, financial markets, sensors, etc. This deluge of streaming data requires real-time processing algorithms that can adapt to concept drifts in order to maintain their accuracy over time. However, current state-of-the-art methods fail to address these challenges by adopting either a traditional batch learning approach which cannot handle drifting concepts or incremental learning approaches that update the model only on new instances without considering previously seen ones. Therefore, there exists a need for hybrid models that combine the benefits of both approaches while tackling the drawbacks. To fill this gap, we present Hybrid Forest (HF), a novel algorithm that combines a decision tree forest with a cost-sensitive support vector machine (SVM). HF addresses several shortcomings associated with existing methodologies through incorporating dynamic weighted sampling to balance the class distribution of incoming data and dual ensemble learning to ensure robustness against rapid changes in concept distributions. Our extensive experiments using various datasets demonstrate the superiority of our approach compared to other well-known methods. By achieving better accuracy, lower error rates, and higher F1 scores than existing techniques, our work offers researchers and practitioners alike significant improvements in detecting and handling evolving patterns in streaming data. As more streaming data becomes available across industries, effective solutions like Hybrid Forest will play crucial roles in unlocking valuable insights from these massive volumes of data.",1
"Early hospital mortality prediction is critical as intensivists strive to make efficient medical decisions about the severely ill patients staying in intensive care units. As a result, various methods have been developed to address this problem based on clinical records. However, some of the laboratory test results are time-consuming and need to be processed. In this paper, we propose a novel method to predict mortality using features extracted from the heart signals of patients within the first hour of ICU admission. In order to predict the risk, quantitative features have been computed based on the heart rate signals of ICU patients. Each signal is described in terms of 12 statistical and signal-based features. The extracted features are fed into eight classifiers: decision tree, linear discriminant, logistic regression, support vector machine (SVM), random forest, boosted trees, Gaussian SVM, and K-nearest neighborhood (K-NN). To derive insight into the performance of the proposed method, several experiments have been conducted using the well-known clinical dataset named Medical Information Mart for Intensive Care III (MIMIC-III). The experimental results demonstrate the capability of the proposed method in terms of precision, recall, F1-score, and area under the receiver operating characteristic curve (AUC). The decision tree classifier satisfies both accuracy and interpretability better than the other classifiers, producing an F1-score and AUC equal to 0.91 and 0.93, respectively. It indicates that heart rate signals can be used for predicting mortality in patients in the ICU, achieving a comparable performance with existing predictions that rely on high dimensional features from clinical records which need to be processed and may contain missing information.",0
"This study aimed to develop a model for early prediction of hospital mortality based on vital signs data collected from patients admitted to intensive care units (ICUs). Vital sign measurements such as heart rate, blood pressure, oxygen saturation, temperature, and respiratory rate have been shown to contain important information regarding patient condition and can serve as valuable predictors of clinical outcomes. In order to identify patients at high risk for adverse events such as death, we sought to establish a predictive algorithm that utilizes these readily available physiological parameters. We retrospectively analyzed electronic health record data from a cohort of ICU patients and developed machine learning algorithms to generate predictions of mortality within the first 48 hours following admission. Our results demonstrate that vital sign data alone can provide meaningful insight into a patient’s likelihood of dying during their ICU stay. Furthermore, our model achieved strong performance in terms of accuracy, specificity, sensitivity, and area under the receiver operating characteristic curve. These findings suggest that incorporating vital sign data into existing hospital protocols could lead to improved identification of high-risk patients and enhance overall quality of care.",1
"Fairness, through its many forms and definitions, has become an important issue facing the machine learning community. In this work, we consider how to incorporate group fairness constraints in kernel regression methods, applicable to Gaussian processes, support vector machines, neural network regression and decision tree regression. Further, we focus on examining the effect of incorporating these constraints in decision tree regression, with direct applications to random forests and boosted trees amongst other widespread popular inference techniques. We show that the order of complexity of memory and computation is preserved for such models and tightly bound the expected perturbations to the model in terms of the number of leaves of the trees. Importantly, the approach works on trained models and hence can be easily applied to models in current use and group labels are only required on training data.",0
"In the era of big data, algorithms have become increasingly important in making decisions that affect people's lives. However, these algorithms can perpetuate and even amplify biases present in the training data used to create them. As such, it is crucial to develop methods to ensure that algorithmic predictions are fair and unbiased towards certain groups of individuals. In this paper we propose a general framework for fair regression based on the concept of predictive parity, which states that the prediction error should be balanced across different sensitive attribute groups. We then introduce three strategies to achieve this goal: (1) post-processing methods such as re-weighting or truncation; (2) adversarial training using generative models; and (3) minimizing the minimum risk bound via regularization terms. Our empirical evaluations demonstrate the effectiveness of our proposed approaches compared against existing fairness metrics. We show how these methods lead to improved fairness without sacrificing model accuracy significantly. Finally, we discuss real-world applications where these techniques could make a positive impact. This work provides a new perspective on achieving fairness in machine learning, opening up opportunities for further research in developing more robust and efficient algorithms that reduce bias and promote equitable decision-making processes.",1
"Better methods to detect insider threats need new anticipatory analytics to capture risky behavior prior to losing data. In search of the best overall classifier, this work empirically scores 88 machine learning algorithms in 16 major families. We extract risk features from the large CERT dataset, which blends real network behavior with individual threat narratives. We discover the predictive importance of measuring employee sentiment. Among major classifier families tested on CERT, the random forest algorithms offer the best choice, with different implementations scoring over 98% accurate. In contrast to more obscure or black-box alternatives, random forests are ensembles of many decision trees and thus offer a deep but human-readable set of detection rules (2000 rules). We address performance rankings by penalizing long execution times against higher median accuracies using cross-fold validation. We address the relative rarity of threats as a case of low signal-to-noise ( 0.02% malicious to benign activities), and then train on both under-sampled and over-sampled data which is statistically balanced to identify nefarious actors.",0
"Abstract: The problem of insider threats has been gaining attention from organizations due to increasing incidents of data breaches caused by trusted employees. Current solutions such as anomaly detection techniques have proven insufficient against these attacks. In this study, we propose Classifier Suites (CS) which integrates multiple classifiers into one framework to improve the accuracy and reliability of detecting insider threat activities. We evaluate CS using real-world datasets and compare its performance against current state-of-the-art methods. Our results show that the proposed approach significantly outperforms existing approaches, achieving an average improvement of over 15% across different metrics and datasets. Furthermore, our analysis reveals valuable insights on the behavior patterns exhibited by malicious actors within enterprise networks. The findings in this research contribute new knowledge towards enhancing organizational security posture against insider threats.",1
"Support Vector Machines (SVMs) with various kernels have played dominant role in machine learning for many years, finding numerous applications. Although they have many attractive features interpretation of their solutions is quite difficult, the use of a single kernel type may not be appropriate in all areas of the input space, convergence problems for some kernels are not uncommon, the standard quadratic programming solution has $O(m^3)$ time and $O(m^2)$ space complexity for $m$ training patterns. Kernel methods work because they implicitly provide new, useful features. Such features, derived from various kernels and other vector transformations, may be used directly in any machine learning algorithm, facilitating multiresolution, heterogeneous models of data. Therefore Support Feature Machines (SFM) based on linear models in the extended feature spaces, enabling control over selection of support features, give at least as good results as any kernel-based SVMs, removing all problems related to interpretation, scaling and convergence. This is demonstrated for a number of benchmark datasets analyzed with linear discrimination, SVM, decision trees and nearest neighbor methods.",0
"Research on machine learning (ML) has been increasingly focused on developing models that can generate high quality outputs without requiring large amounts of training data. This area of research has become known as ""few shot"" or ""zero shot"" learning, where ML models are trained to generalize knowledge learned from a few examples or even no explicit examples at all. While these approaches have shown promising results, there remains room for improvement. One area that has received little attention until recently is how to optimize the support sets used in these algorithms. In this work, we propose a novel approach called support feature machines (SFMs), which combines ideas from traditional ML methods with more recent deep learning techniques. We demonstrate through extensive experiments that our method significantly outperforms existing state-of-the-art zero shot learning techniques while still maintaining competitive performance relative to other baseline approaches. Our findings suggest that SFMs represent an important new direction for future research in few shot and zero shot learning.",1
"Toxicity prediction of chemical compounds is a grand challenge. Lately, it achieved significant progress in accuracy but using a huge set of features, implementing a complex blackbox technique such as a deep neural network, and exploiting enormous computational resources. In this paper, we strongly argue for the models and methods that are simple in machine learning characteristics, efficient in computing resource usage, and powerful to achieve very high accuracy levels. To demonstrate this, we develop a single task-based chemical toxicity prediction framework using only 2D features that are less compute intensive. We effectively use a decision tree to obtain an optimum number of features from a collection of thousands of them. We use a shallow neural network and jointly optimize it with decision tree taking both network parameters and input features into account. Our model needs only a minute on a single CPU for its training while existing methods using deep neural networks need about 10 min on NVidia Tesla K40 GPU. However, we obtain similar or better performance on several toxicity benchmark tasks. We also develop a cumulative feature ranking method which enables us to identify features that can help chemists perform prescreening of toxic compounds effectively.",0
"Title: ""Efficient Toxicity Prediction using Shallow Models""  Toxicity prediction is important for assessing environmental risks and making informed decisions on how to manage them. Conventional approaches rely heavily on deep neural networks which require large amounts of data and computational resources. In contrast, we present a novel approach that utilizes shallow models such as decision trees and simple neural networks to effectively predict toxicity outcomes while significantly reducing computation time. We demonstrate the effectiveness of our approach by comparing predictions against those from more complex deep learning models on three benchmark datasets. Our results show that our method can achieve comparable performance while being far less compute-intensive and requiring less data preprocessing. This work has important implications for industries looking to rapidly evaluate potential hazards without incurring significant costs associated with advanced computing infrastructure.",1
"Interpretability has become incredibly important as machine learning is increasingly used to inform consequential decisions. We propose to construct global explanations of complex, blackbox models in the form of a decision tree approximating the original model---as long as the decision tree is a good approximation, then it mirrors the computation performed by the blackbox model. We devise a novel algorithm for extracting decision tree explanations that actively samples new training points to avoid overfitting. We evaluate our algorithm on a random forest to predict diabetes risk and a learned controller for cart-pole. Compared to several baselines, our decision trees are both substantially more accurate and equally or more interpretable based on a user study. Finally, we describe several insights provided by our interpretations, including a causal issue validated by a physician.",0
"Many machine learning algorithms use so called ""black box models"" whose workings are hidden behind mathematical equations, which makes them hard to interpret in some settings where explainability plays a role. In certain cases, these black boxes can even turn out to give surprising results since their inner working may deviate from our human intuition or may take unforeseen shortcuts in problem solving. Here we propose that sometimes exact extraction of a model trained by an external process (like gradient ascent) from white box models is possible, i.e., given the weights w of the original model one can reconstruct the corresponding black box solution m(w). While the reverse direction requires regularization constraints against memorizing training examples, here we show how similar such extraction based on linear regression gives reasonable reconstruction quality, both quantitatively and visually. Since any continuous input function is equivalent to the same continuous white box model (as far as reconstruction is concerned), the approach opens up for interesting future directions that go beyond interpolation; e.g., optimization under different cost functions. Overall, this first result allows for greater transparency in predictive modelling beyond traditional knowledge discovery tasks, paving the road towards more trustworthy deployments of black box methods in sensitive decision making applications in healthcare, justice and finance.",1
"While deep reinforcement learning has successfully solved many challenging control tasks, its real-world applicability has been limited by the inability to ensure the safety of learned policies. We propose an approach to verifiable reinforcement learning by training decision tree policies, which can represent complex policies (since they are nonparametric), yet can be efficiently verified using existing techniques (since they are highly structured). The challenge is that decision tree policies are difficult to train. We propose VIPER, an algorithm that combines ideas from model compression and imitation learning to learn decision tree policies guided by a DNN policy (called the oracle) and its Q-function, and show that it substantially outperforms two baselines. We use VIPER to (i) learn a provably robust decision tree policy for a variant of Atari Pong with a symbolic state space, (ii) learn a decision tree policy for a toy game based on Pong that provably never loses, and (iii) learn a provably stable decision tree policy for cart-pole. In each case, the decision tree policy achieves performance equal to that of the original DNN policy.",0
"In recent years, reinforcement learning (RL) has emerged as one of the most promising approaches to artificial intelligence due to its ability to learn optimal policies from trial-and-error interactions with complex environments. However, verifying that learned RL policies satisfy desired specifications remains challenging, especially when the environment involves partial observability, stochasticity, delays, or continuous actions. To address these limitations, we propose a novel framework called verifiable policy extraction based on abstraction refinement. Our method leverages a multi-level hierarchy of finite-state abstractions derived automatically from the original Markov decision process (MDP). We show that our method significantly reduces the verification time compared with state-of-the-art methods by up to three orders of magnitude while providing tighter bounds on the approximation error. Furthermore, we demonstrate empirically that our approach can extract provably correct policies that meet safety constraints imposed on continuous action spaces. Overall, our work paves the way towards more efficient and reliable synthesis of safe and high-performing control policies for real-world applications using verifiable RL algorithms.",1
In this paper we study the adaptive learnability of decision trees of depth at most $d$ from membership queries. This has many applications in automated scientific discovery such as drugs development and software update problem. Feldman solves the problem in a randomized polynomial time algorithm that asks $\tilde O(2^{2d})\log n$ queries and Kushilevitz-Mansour in a deterministic polynomial time algorithm that asks $ 2^{18d+o(d)}\log n$ queries. We improve the query complexity of both algorithms. We give a randomized polynomial time algorithm that asks $\tilde O(2^{2d}) + 2^{d}\log n$ queries and a deterministic polynomial time algorithm that asks $2^{5.83d}+2^{2d+o(d)}\log n$ queries.,0
"Title: ""Adaptive Exact Learning of Decision Trees from Membership Queries""  This paper presents a new algorithm for learning decision trees from membership queries. Unlike previous methods that use sampling techniques, our method uses exact learning by querying all training points. Our approach adapts to different datasets by adjusting the depth of the tree based on the complexity of the data. We show through extensive experimentation that our method outperforms other state-of-the-art approaches in terms of accuracy, speed, and memory usage. In addition, we provide theoretical analysis of the time complexity of our algorithm compared to existing ones. This research has applications in many fields such as machine learning, pattern recognition, and computer vision.",1
"Numerous studies have been carried out to measure wind pressures around circular cylinders since the early 20th century due to its engineering significance. Consequently, a large amount of wind pressure data sets have accumulated, which presents an excellent opportunity for using machine learning (ML) techniques to train models to predict wind pressures around circular cylinders. Wind pressures around smooth circular cylinders are a function of mainly the Reynolds number (Re), turbulence intensity (Ti) of the incident wind, and circumferential angle of the cylinder. Considering these three parameters as the inputs, this study trained two ML models to predict mean and fluctuating pressures respectively. Three machine learning algorithms including decision tree regressor, random forest, and gradient boosting regression trees (GBRT) were tested. The GBRT models exhibited the best performance for predicting both mean and fluctuating pressures, and they are capable of making accurate predictions for Re ranging from 10^4 to 10^6 and Ti ranging from 0% to 15%. It is believed that the GBRT models provide very efficient and economical alternative to traditional wind tunnel tests and computational fluid dynamic simulations for determining wind pressures around smooth circular cylinders within the studied Re and Ti range.",0
"The wind pressure predictions around circular cylinders has traditionally been done through computational fluid dynamics (CFD) simulations which have high computation time and require expert knowledge to interpret results. Machine learning models were used here instead as they can make rapid predictions that take less computational resources. Both neural networks and kriging were tested against CFD data on three different heights above ground at Reynolds numbers from 7894 to 26,169. Kriging was more accurate overall but had issues modeling extreme values. Artificial neural networks provided reasonable accuracy although their sensitivity analysis revealed complex relationships between inputs. This study shows promise for wind pressure prediction in practice for short buildings where computational resources may limit availability of CFD software applications.",1
"Gradient boosting decision trees (GBDTs) have seen widespread adoption in academia, industry and competitive data science due to their state-of-the-art performance in many machine learning tasks. One relative downside to these models is the large number of hyper-parameters that they expose to the end-user. To maximize the predictive power of GBDT models, one must either manually tune the hyper-parameters, or utilize automated techniques such as those based on Bayesian optimization. Both of these approaches are time-consuming since they involve repeatably training the model for different sets of hyper-parameters. A number of software GBDT packages have started to offer GPU acceleration which can help to alleviate this problem. In this paper, we consider three such packages: XGBoost, LightGBM and Catboost. Firstly, we evaluate the performance of the GPU acceleration provided by these packages using large-scale datasets with varying shapes, sparsities and learning tasks. Then, we compare the packages in the context of hyper-parameter optimization, both in terms of how quickly each package converges to a good validation score, and in terms of generalization performance.",0
An abstract should contain at least one sentence summarizing the findings/results of your study.,1
"Diabetes mellitus is a common disease of human body caused by a group of metabolic disorders where the sugar levels over a prolonged period is very high. It affects different organs of the human body which thus harm a large number of the body's system, in particular the blood veins and nerves. Early prediction in such disease can be controlled and save human life. To achieve the goal, this research work mainly explores various risk factors related to this disease using machine learning techniques. Machine learning techniques provide efficient result to extract knowledge by constructing predicting models from diagnostic medical datasets collected from the diabetic patients. Extracting knowledge from such data can be useful to predict diabetic patients. In this work, we employ four popular machine learning algorithms, namely Support Vector Machine (SVM), Naive Bayes (NB), K-Nearest Neighbor (KNN) and C4.5 Decision Tree, on adult population data to predict diabetic mellitus. Our experimental results show that C4.5 decision tree achieved higher accuracy compared to other machine learning techniques.",0
"This study aimed to evaluate the performance of different machine learning techniques in predicting diabetes mellitus. Data was collected from medical records of patients diagnosed with type II diabetes mellitus at XYZ hospital over a period of five years. The data set included demographic information such as age, gender, weight, height, blood pressure, cholesterol level, blood glucose levels and other risk factors associated with diabetes mellitus. Features were extracted using principal component analysis (PCA) technique and then split into training and testing sets. Different supervised learning algorithms including logistic regression, decision tree, support vector machines and artificial neural networks were used to build models capable of classifying patients who had been diagnosed as positive for Type 2 diabetes mellitus based on their risk factors. The accuracy of these models was compared against each other and against existing predictive methods currently available. Results indicated that all four machine learning techniques outperformed traditional predictors significantly with artificial neural network performing best achieving 94% accuracy. These findings suggest that machine learning techniques can be utilized effectively in improving diabetes mellitus prediction rates, thus potentially improving patient care. Keywords: diabetes mellitus, machine learning techniques, logistic regression, decision trees, SVMs, ANN",1
"Data analysis and machine learning have become an integrative part of the modern scientific methodology, providing automated techniques to predict further information based on observations. One of these classification and regression techniques is the random forest approach. Those decision tree based predictors are best known for their good computational performance and scalability. However, in case of severely imbalanced training data, as often seen in medical studies' data with large control groups, the training algorithm or the sampling process has to be altered in order to improve the prediction quality for minority classes. In this work, a balanced random forest approach for WEKA is proposed. Furthermore, the prediction quality of the unmodified random forest implementation and the new balanced random forest version for WEKA are evaluated against reference implementations in R. Two-class problems on balanced data sets and imbalanced medical studies' data are investigated. A superior prediction quality using the proposed method for imbalanced data is shown compared to the other three techniques.",0
"This paper presents a discussion on the use of random forest classifiers in the open source data mining software WEKA. Recent developments have expanded the capabilities of these classifiers for handling imbalanced datasets. The authors explore the technical details behind these improvements, including changes to both decision tree training algorithms and the implementation of cost matrices. They demonstrate how these advancements can lead to better performance on datasets containing few examples from one class relative to another. Furthermore, the paper investigates several techniques used by researchers to address class imbalance issues when working with the random forest model. These approaches provide insight into the different strategies that can be applied during data preparation, algorithm configuration, and evaluation to achieve optimal results. Overall, this work contributes new knowledge towards enhancing our understanding of how to effectively utilize random forest classifiers under challenging conditions encountered in real-world applications.",1
"In this paper, we propose a hybrid bankcard response model, which integrates decision tree based chi-square automatic interaction detection (CHAID) into logistic regression. In the first stage of the hybrid model, CHAID analysis is used to detect the possibly potential variable interactions. Then in the second stage, these potential interactions are served as the additional input variables in logistic regression. The motivation of the proposed hybrid model is that adding variable interactions may improve the performance of logistic regression. To demonstrate the effectiveness of the proposed hybrid model, it is evaluated on a real credit customer response data set. As the results reveal, by identifying potential interactions among independent variables, the proposed hybrid approach outperforms the logistic regression without searching for interactions in terms of classification accuracy, the area under the receiver operating characteristic curve (ROC), and Kolmogorov-Smirnov (KS) statistics. Furthermore, CHAID analysis for interaction detection is much more computationally efficient than the stepwise search mentioned above and some identified interactions are shown to have statistically significant predictive power on the target variable. Last but not least, the customer profile created based on the CHAID tree provides a reasonable interpretation of the interactions, which is the required by regulations of the credit industry. Hence, this study provides an alternative for handling bankcard classification tasks.",0
"This paper presents a new hybrid model called ""Interaction Detection,"" which combines both supervised learning and unsupervised learning methods to achieve automatic interaction detection in bankcards response classification problems. Our experimental results show that our proposed method outperforms existing state-of-the-art models across multiple benchmark datasets, including Kaggle's Credit Card Fraud Detection competition dataset (with accuracy improvement by up to 9% compared to other models). Our model is capable of detecting subtle changes in cardholder behavior patterns due to fraudulent transactions, while simultaneously identifying suspicious transactions as well as account takeover attacks on legitimate accounts. Furthermore, we provide detailed descriptions and explanations on how to implement the Interaction Detection hybrid model using popular machine learning libraries such as scikit-learn and PyTorch. These contributions make our work highly valuable to researchers and practitioners working on credit card fraud analysis and prevention tasks. As a result, our study provides significant advances in understanding the use of automatic Interaction Detection for enhanced credit card fraud detection performance.",1
"Deep Neural Networks have achieved huge success at a wide spectrum of applications from language modeling, computer vision to speech recognition. However, nowadays, good performance alone is not sufficient to satisfy the needs of practical deployment where interpretability is demanded for cases involving ethics and mission critical applications. The complex models of Deep Neural Networks make it hard to understand and reason the predictions, which hinders its further progress. To tackle this problem, we apply the Knowledge Distillation technique to distill Deep Neural Networks into decision trees in order to attain good performance and interpretability simultaneously. We formulate the problem at hand as a multi-output regression problem and the experiments demonstrate that the student model achieves significantly better accuracy performance (about 1\% to 5\%) than vanilla decision trees at the same level of tree depth. The experiments are implemented on the TensorFlow platform to make it scalable to big datasets. To the best of our knowledge, we are the first to distill Deep Neural Networks into vanilla decision trees on multi-class datasets.",0
"Title: ""Interpreting Machine Learning Models""  This study explores ways of improving the interpretability of deep neural networks (DNNs). We propose using knowledge distillation as a technique to achieve this goal. The key idea behind knowledge distillation is to train a smaller model on top of a larger model, in order to extract some of the “dark matter” of human intelligence that underlies these models. Specifically, we focus on how such a method can provide insight into how DNNs make decisions, which has been a challenge due to their black box nature. Our experiments demonstrate that knowledge distillation can indeed improve interpretability and provide new insights into the workings of deep learning systems. Overall, our findings have important implications for both researchers and practitioners interested in developing more transparent machine learning algorithms. They could also lead to better informed decision making across various application domains.",1
"Dropout is a very effective method in preventing overfitting and has become the go-to regularizer for multi-layer neural networks in recent years. Hierarchical mixture of experts is a hierarchically gated model that defines a soft decision tree where leaves correspond to experts and decision nodes correspond to gating models that softly choose between its children, and as such, the model defines a soft hierarchical partitioning of the input space. In this work, we propose a variant of dropout for hierarchical mixture of experts that is faithful to the tree hierarchy defined by the model, as opposed to having a flat, unitwise independent application of dropout as one has with multi-layer perceptrons. We show that on a synthetic regression data and on MNIST and CIFAR-10 datasets, our proposed dropout mechanism prevents overfitting on trees with many levels improving generalization and providing smoother fits.",0
"In this work, we investigate the use of dropout regularization in hierarchical mixture of experts (HME) models. HMEs are powerful generative modeling techniques that allow for flexible modeling of complex data distributions by combining multiple weak expert models into a single strong predictor. However, these models can often suffer from overfitting due to their flexibility. To address this issue, we introduce dropout regularization, which adds noise during training to prevent the experts from co-dependent on each other and the input features. We demonstrate through experiments on several benchmark datasets that our proposed method results in improved generalization performance compared to standard HMEs without regularization. Furthermore, our method maintains or improves upon the strong likelihood estimates provided by the original model, making it a compelling choice for applications where both good predictions and calibrated uncertainty estimates are important. Our work provides insights into how regularization methods can be applied to improve the stability and robustness of HMEs, paving the way for more accurate and reliable generation tasks across domains.",1
"This paper presents a study on power grid disturbance classification by Deep Learning (DL). A real synchrophasor set composing of three different types of disturbance events from the Frequency Monitoring Network (FNET) is used. An image embedding technique called Gramian Angular Field is applied to transform each time series of event data to a two-dimensional image for learning. Two main DL algorithms, i.e. CNN (Convolutional Neural Network) and RNN (Recurrent Neural Network) are tested and compared with two widely used data mining tools, the Support Vector Machine and Decision Tree. The test results demonstrate the superiority of the both DL algorithms over other methods in the application of power system transient disturbance classification.",0
"This study proposes an image embedding approach that leverages phasor measurement unit (PMU) data for deep learning based transient disturbance classification. The novel method transforms time-varying complex measurements into static images by applying dimensionality reduction techniques, enabling convolutional neural networks (CNNs) to effectively learn features from both spatial and temporal domains. After evaluating different CNN architectures, we achieve state-of-the-art performance on two benchmark datasets, demonstrating the effectiveness of our proposed technique for accurate classification of power system disturbances. Our findings have important implications for the development of advanced monitoring systems, paving the way for improved grid stability analysis and control strategies. Further research directions may involve integrating complementary sensory data or exploring generative models to enhance anomaly detection capabilities.",1
"Classification is an important supervised machine learning method, which is necessary and challenging issue for ecological research. It offers a way to classify a dataset into subsets that share common patterns. Notably, there are many classification algorithms to choose from, each making certain assumptions about the data and about how classification should be formed. In this paper, we applied eight machine learning classification algorithms such as Decision Trees, Random Forest, Artificial Neural Network, Support Vector Machine, Linear Discriminant Analysis, k-nearest neighbors, Logistic Regression and Naive Bayes on ecological data. The goal of this study is to compare different machine learning classification algorithms in ecological dataset. In this analysis we have checked the accuracy test among the algorithms. In our study we conclude that Linear Discriminant Analysis and k-nearest neighbors are the best methods among all other methods",0
"This research investigates how machine learning algorithms can improve ecological data analysis by using public datasets from online sources and comparing their performance against conventional statistical methods such as linear regression models (LRM). Results show that machine learning techniques provide more accurate predictions than LRM but have limited ability to interpret their findings or handle missing data. Overall, while promising, further work is required before these methods become standard tools used in ecological studies. Keywords: Ecology; Machine learning; Linear Regression Models; Public Datasets; Accuracy; Predictions",1
"This study proposes a logic architecture for the high-speed and power efficiently training of a gradient boosting decision tree model of binary classification. We implemented the proposed logic architecture on an FPGA and compared training time and power efficiency with three general GBDT software libraries using CPU and GPU. The training speed of the logic architecture on the FPGA was 26-259 times faster than the software libraries. The power efficiency of the logic architecture was 90-1,104 times higher than the software libraries. The results show that the logic architecture suits for high-performance and edge computing.",0
"This paper proposes efficient logic architectures for accelerating the training phase of Gradient Boosting Decision Trees (GBDTs) on high performance computing systems. GBDTs have gained popularity as powerful tools for machine learning due to their excellent prediction accuracy and robustness against overfitting. However, the slow training process hinders their deployment in applications that require real-time predictions, such as internet of things devices and autonomous vehicles. To address this issue, we designed customized hardware acceleration engines tailored to specific operations in the GBDT training algorithm. Our approach targets both data parallelism across multiple processing elements and pipeline parallelism within each element. Extensive evaluation results demonstrate the effectiveness of our proposed designs, achieving up to 28x speedup compared to state-of-the-art software implementations running on GPU clusters. Moreover, we showcase several case studies applying these optimized GBDT models on real-world datasets from different domains, demonstrating significantly improved inference times without compromising predictive quality. Overall, this work paves the way towards deploying GBDTs in resource-constrained environments where inference latency is critical.",1
"Research into the classification of time series has made enormous progress in the last decade. The UCR time series archive has played a significant role in challenging and guiding the development of new learners for time series classification. The largest dataset in the UCR archive holds 10 thousand time series only; which may explain why the primary research focus has been in creating algorithms that have high accuracy on relatively small datasets.   This paper introduces Proximity Forest, an algorithm that learns accurate models from datasets with millions of time series, and classifies a time series in milliseconds. The models are ensembles of highly randomized Proximity Trees. Whereas conventional decision trees branch on attribute values (and usually perform poorly on time series), Proximity Trees branch on the proximity of time series to one exemplar time series or another; allowing us to leverage the decades of work into developing relevant measures for time series. Proximity Forest gains both efficiency and accuracy by stochastic selection of both exemplars and similarity measures.   Our work is motivated by recent time series applications that provide orders of magnitude more time series than the UCR benchmarks. Our experiments demonstrate that Proximity Forest is highly competitive on the UCR archive: it ranks among the most accurate classifiers while being significantly faster. We demonstrate on a 1M time series Earth observation dataset that Proximity Forest retains this accuracy on datasets that are many orders of magnitude greater than those in the UCR repository, while learning its models at least 100,000 times faster than current state of the art models Elastic Ensemble and COTE.",0
"This paper presents a novel approach for efficiently performing anomaly detection on streaming data by leveraging recent advances in nearest neighbor search over graphs. By using a carefully constructed graph representation of raw data records called a Proximity Forest (PF), we can perform efficient approximate nearest neighbor search and achieve state-of-the art performance on several benchmark datasets. We demonstrate both effectiveness and efficiency gains of our method over alternative approaches such as DTW, KDDCup99, and many others across multiple domains including network traffic analysis, sensor streams from IoT devices, medical research, financial transactions, among other important applications areas impacted by big data science.  The goal of this work is to enable fast, accurate, realtime analytics which enables businesses and scientific researchers to make decisions based on actual observations vs waiting until the next day/week, etc... Our experiments showcase how PF drastically reduces processing times while improving accuracy at scale via two primary innovations: 1) a carefully crafted compressed graph structure that allows for linear complexity approximation searches within high dimensional spaces; 2) advanced caching techniques enabling intelligent pruning to optimize search space prior to costly computations which further accelerates overall model execution speed. In conclusion, we provide evidence suggesting that Proximity Forests could lead to transformative improvements across numerous sectors if widely deployed throughout industry.",1
"In this paper, we use a well-known Deep Learning technique called Long Short Term Memory (LSTM) recurrent neural networks to find sessions that are prone to code failure in applications that rely on telemetry data for system health monitoring. We also use LSTM networks to extract telemetry patterns that lead to a specific code failure. For code failure prediction, we treat the telemetry events, sequence of telemetry events and the outcome of each sequence as words, sentence and sentiment in the context of sentiment analysis, respectively. Our proposed method is able to process a large set of data and can automatically handle edge cases in code failure prediction. We take advantage of Bayesian optimization technique to find the optimal hyper parameters as well as the type of LSTM cells that leads to the best prediction performance. We then introduce the Contributors and Blockers concepts. In this paper, contributors are the set of events that cause a code failure, while blockers are the set of events that each of them individually prevents a code failure from happening, even in presence of one or multiple contributor(s). Once the proposed LSTM model is trained, we use a greedy approach to find the contributors and blockers. To develop and test our proposed method, we use synthetic (simulated) data in the first step. The synthetic data is generated using a number of rules for code failures, as well as a number of rules for preventing a code failure from happening. The trained LSTM model shows over 99% accuracy for detecting code failures in the synthetic data. The results from the proposed method outperform the classical learning models such as Decision Tree and Random Forest. Using the proposed greedy method, we are able to find the contributors and blockers in the synthetic data in more than 90% of the cases, with a performance better than sequential rule and pattern mining algorithms.",0
"In software development, code failure prediction has been an area of interest due to its potential impact on improving overall product quality and reducing maintenance costs. Traditional methods have relied heavily on rule-based approaches and have had limited success. However, recent advances in deep learning techniques offer new opportunities for improved accuracy and scalability. This paper presents a novel approach that utilizes Long Short Term Memory (LSTM) networks to predict code failures by analyzing large amounts of data from existing code repositories. We show how our model can accurately identify patterns associated with code failures and make predictions with high confidence levels. Our results suggest that our method outperforms traditional approaches and demonstrates great promise as a valuable tool for developers looking to improve their code quality before deployment. By providing early warnings and guidance, we aim to reduce the number of defects and ensure software systems are more reliable and secure.",1
"We propose Deep Hierarchical Machine (DHM), a model inspired from the divide-and-conquer strategy while emphasizing representation learning ability and flexibility. A stochastic routing framework as used by recent deep neural decision/regression forests is incorporated, but we remove the need to evaluate unnecessary computation paths by utilizing a different topology and introducing a probabilistic pruning technique. We also show a specified version of DHM (DSHM) for efficiency, which inherits the sparse feature extraction process as in traditional decision tree with pixel-difference feature. To achieve sparse feature extraction, we propose to utilize sparse convolution operation in DSHM and show one possibility of introducing sparse convolution kernels by using local binary convolution layer. DHM can be applied to both classification and regression problems, and we validate it on standard image classification and face alignment tasks to show its advantages over past architectures.",0
"Title: ""Flexible Divide-and-Conquer Architecture""  This paper presents a new machine learning architecture called Deep Hierarchical Machines (DHMs), which leverages divide-and-conquer principles to achieve state-of-the-art performance on challenging tasks. DHMs combine deep neural networks with structured representations to model complex dependencies and interactions among data features.  The key innovation of our approach lies in its hierarchical structure, where each level learns to divide the input into increasingly homogeneous subgroups before combining them using appropriate ensemble methods like linear regression or neural networks. This enables flexible modeling at multiple scales while addressing the curse of dimensionality issues that plague many deep models.  In particular, we show how DHMs can learn powerful feature encodings by recursively splitting the data along dimensions that correspond to relevant clusters or patterns in the data. Our method achieves competitive results across diverse benchmark datasets in image classification, natural language processing, and computer vision tasks, demonstrating its general applicability.  Our theoretical analysis reveals interesting connections between DHMs and other architectures such as randomized ensembles and mixture models, highlighting their shared mathematical foundations. These insights may provide further guidance for designing future architectures beyond traditional neural network structures.  Overall, DHMs represent a promising step towards more efficient and effective machine learning systems, enabling researchers to tackle larger problems involving complex domains and multimodal inputs without sacrificing predictive accuracy. We hope this work spurs further exploration of divides-and-conquer approaches in the broader community.",1
"Group fairness is an important concern for machine learning researchers, developers, and regulators. However, the strictness to which models must be constrained to be considered fair is still under debate. The focus of this work is on constraining the expected outcome of subpopulations in kernel regression and, in particular, decision tree regression, with application to random forests, boosted trees and other ensemble models. While individual constraints were previously addressed, this work addresses concerns about incorporating multiple constraints simultaneously. The proposed solution does not affect the order of computational or memory complexity of the decision trees and is easily integrated into models post training.",0
"This paper explores intersectionality as a methodological approach to improve fairness in expectations constraints used by machine learning algorithms. We argue that existing methods fail to adequately account for multiple group identities and result in biased outcomes. Our proposed framework integrates intersectional theory into constraint design, resulting in more equitable models that better reflect real-world experiences of marginalized groups. We present case studies demonstrating how our framework can mitigate bias in applications such as image classification and natural language processing. The results show significant improvement in model performance across different demographic groups compared to traditional methods. Overall, we contribute new research on incorporating intersectional perspectives into machine learning algorithms to address social justice issues.",1
"Personalized medicine aims at identifying best treatments for a patient with given characteristics. It has been shown in the literature that these methods can lead to great improvements in medicine compared to traditional methods prescribing the same treatment to all patients. Subgroup identification is a branch of personalized medicine which aims at finding subgroups of the patients with similar characteristics for which some of the investigated treatments have a better effect than the other treatments. A number of approaches based on decision trees has been proposed to identify such subgroups, but most of them focus on the two-arm trials (control/treatment) while a few methods consider quantitative treatments (defined by the dose). However, no subgroup identification method exists that can predict the best treatments in a scenario with a categorical set of treatments. We propose a novel method for subgroup identification in categorical treatment scenarios. This method outputs a decision tree showing the probabilities of a given treatment being the best for a given group of patients as well as labels showing the possible best treatments. The method is implemented in an R package \textbf{psica} available at CRAN. In addition to numerical simulations based on artificial data, we present an analysis of a community-based nutrition intervention trial that justifies the validity of our method.",0
In general,1
"In many settings, a decision-maker wishes to learn a rule, or policy, that maps from observable characteristics of an individual to an action. Examples include selecting offers, prices, advertisements, or emails to send to consumers, as well as the problem of determining which medication to prescribe to a patient. While there is a growing body of literature devoted to this problem, most existing results are focused on the case where data comes from a randomized experiment, and further, there are only two possible actions, such as giving a drug to a patient or not. In this paper, we study the offline multi-action policy learning problem with observational data and where the policy may need to respect budget constraints or belong to a restricted policy class such as decision trees. We build on the theory of efficient semi-parametric inference in order to propose and implement a policy learning algorithm that achieves asymptotically minimax-optimal regret. To the best of our knowledge, this is the first result of this type in the multi-action setup, and it provides a substantial performance improvement over the existing learning algorithms. We then consider additional computational challenges that arise in implementing our method for the case where the policy is restricted to take the form of a decision tree. We propose two different approaches, one using a mixed integer program formulation and the other using a tree-search based algorithm.",0
This should read as though it were written by someone else (a journal editor?) who has read your paper and wants to give an overview of some key points without providing any spoilers.,1
"Developing effective and efficient recommendation methods is very challenging for modern e-commerce platforms. Generally speaking, two essential modules named ""Click-Through Rate Prediction"" (\textit{CTR}) and ""Conversion Rate Prediction"" (\textit{CVR}) are included, where \textit{CVR} module is a crucial factor that affects the final purchasing volume directly. However, it is indeed very challenging due to its sparseness nature. In this paper, we tackle this problem by proposing multi-Level Deep Cascade Trees (\textit{ldcTree}), which is a novel decision tree ensemble approach. It leverages deep cascade structures by stacking Gradient Boosting Decision Trees (\textit{GBDT}) to effectively learn feature representation. In addition, we propose to utilize the cross-entropy in each tree of the preceding \textit{GBDT} as the input feature representation for next level \textit{GBDT}, which has a clear explanation, i.e., a traversal from root to leaf nodes in the next level \textit{GBDT} corresponds to the combination of certain traversals in the preceding \textit{GBDT}. The deep cascade structure and the combination rule enable the proposed \textit{ldcTree} to have a stronger distributed feature representation ability. Moreover, inspired by ensemble learning, we propose an Ensemble \textit{ldcTree} (\textit{E-ldcTree}) to encourage the model's diversity and enhance the representation ability further. Finally, we propose an improved Feature learning method based on \textit{EldcTree} (\textit{F-EldcTree}) for taking adequate use of weak and strong correlation features identified by pre-trained \textit{GBDT} models. Experimental results on off-line data set and online deployment demonstrate the effectiveness of the proposed methods.",0
"In recent years, recommender systems have become increasingly important due to their ability to provide personalized recommendations to users based on their past behavior and preferences. One crucial aspect of recommendation algorithms is predicting conversion rates (CRs) - i.e., how likely a user will interact positively with a recommended item. Recent research has shown that deep learning models can effectively capture complex patterns in data to make accurate CR predictions; however, these methods often require large amounts of training data and computational resources. To address this issue, we propose using multi-level deep cascade trees (MCCTs), which combine tree ensembles and neural networks into a single framework. MCCTs use a cascading architecture where multiple low-complexity prediction modules communicate and share knowledge during inference. Our experiments show that MCCTs outperform state-of-the-art baseline models by a significant margin across several datasets from diverse domains, including movie rating prediction, clickthrough rate estimation, and news article popularity forecasting. Furthermore, our model achieves better accuracy while requiring less computation than existing approaches. Overall, our work demonstrates the effectiveness of incorporating tree structures into deep learning models for improving performance and reducing complexity, opening up new opportunities for applying deep learning techniques to real-world problems.",1
"A contextual care protocol is used by a medical practitioner for patient healthcare, given the context or situation that the specified patient is in. This paper proposes a method to build an automated self-adapting protocol which can help make relevant, early decisions for effective healthcare delivery. The hybrid model leverages neural networks and decision trees. The neural network estimates the chances of each disease and each tree in the decision trees represents care protocol for a disease. These trees are subject to change in case of aberrations found by the diagnosticians. These corrections or prediction errors are clustered into similar groups for scalability and review by the experts. The corrections as suggested by the experts are incorporated into the model.",0
"Incorporate elements such as the goal of your work, methods used, main results obtained, conclusions you have drawn from those results, implications for practice etc. You should highlight what makes the research novel compared to other approaches to contextual care. Be sure to explain any technical terms included in the abstract so that it can be understood by non specialists. Please note that a bad quality abstract will lead to rejection of my submission, since reviewers primarily base their decisions on just the abstract and titles. Therefore I would highly appreciate if you could write a high quality one.",1
"Machine Learning has been steadily gaining traction for its use in Anomaly-based Network Intrusion Detection Systems (A-NIDS). Research into this domain is frequently performed using the KDD~CUP~99 dataset as a benchmark. Several studies question its usability while constructing a contemporary NIDS, due to the skewed response distribution, non-stationarity, and failure to incorporate modern attacks. In this paper, we compare the performance for KDD-99 alternatives when trained using classification models commonly found in literature: Neural Network, Support Vector Machine, Decision Tree, Random Forest, Naive Bayes and K-Means. Applying the SMOTE oversampling technique and random undersampling, we create a balanced version of NSL-KDD and prove that skewed target classes in KDD-99 and NSL-KDD hamper the efficacy of classifiers on minority classes (U2R and R2L), leading to possible security risks. We explore UNSW-NB15, a modern substitute to KDD-99 with greater uniformity of pattern distribution. We benchmark this dataset before and after SMOTE oversampling to observe the effect on minority performance. Our results indicate that classifiers trained on UNSW-NB15 match or better the Weighted F1-Score of those trained on NSL-KDD and KDD-99 in the binary case, thus advocating UNSW-NB15 as a modern substitute to these datasets.",0
This could potentially work well as an alternative abstract for a paper that compares benchmarking datasets for network intrusion detection using anomalies - without directly mentioning the use case.,1
"Nearest Neighbors Algorithm is a Lazy Learning Algorithm, in which the algorithm tries to approximate the predictions with the help of similar existing vectors in the training dataset. The predictions made by the K-Nearest Neighbors algorithm is based on averaging the target values of the spatial neighbors. The selection process for neighbors in the Hermitian space is done with the help of distance metrics such as Euclidean distance, Minkowski distance, Mahalanobis distance etc. A majority of the metrics such as Euclidean distance are scale variant, meaning that the results could vary for different range of values used for the features. Standard techniques used for the normalization of scaling factors are feature scaling method such as Z-score normalization technique, Min-Max scaling etc. Scaling methods uniformly assign equal weights to all the features, which might result in a non-ideal situation. This paper proposes a novel method to assign weights to individual feature with the help of out of bag errors obtained from constructing multiple decision tree models.",0
"This paper presents a novel approach to feature scaling for the k-nearest neighbor (kNN) algorithm, which improves accuracy by dynamically adjusting the scale of each feature based on the local density of data points. Traditionally, feature scaling involves normalizing all features to have zero mean and unit variance, but this may not always be optimal as it assumes that all features have similar distributions. Our proposed method overcomes this limitation by using a kernel density estimation technique to estimate the local density of data points near each sample, and then scales each feature based on how densely populated that region is. Experimental results demonstrate that our dynamic feature scaling method outperforms traditional static methods across a variety of datasets, including both numerical and categorical features. Additionally, we show that our method can improve upon other state-of-the-art scaling techniques such as min-max normalization and standardizing. Overall, our work offers a simple yet effective solution for enhancing the performance of the popular kNN classifier.",1
"We introduce a novel approach to feed-forward neural network interpretation based on partitioning the space of sequences of neuron activations. In line with this approach, we propose a model-specific interpretation method, called YASENN. Our method inherits many advantages of model-agnostic distillation, such as an ability to focus on the particular input region and to express an explanation in terms of features different from those observed by a neural network. Moreover, examination of distillation error makes the method applicable to the problems with low tolerance to interpretation mistakes. Technically, YASENN distills the network with an ensemble of layer-wise gradient boosting decision trees and encodes the sequences of neuron activations with leaf indices. The finite number of unique codes induces a partitioning of the input space. Each partition may be described in a variety of ways, including examination of an interpretable model (e.g. a logistic regression or a decision tree) trained to discriminate between objects of those partitions. Our experiments provide an intuition behind the method and demonstrate revealed artifacts in neural network decision making.",0
"One promising direction towards explainability lies in analyzing the activation sequences within neural networks during inference (also referred to as activations flows). In this work we present YASENN - Yet Another Simple Explanation for Neural Networks by Partitioning Activation Sequences. We propose that it is possible to analyze these activation sequences by partitioning them into manageable subsets using simple heuristics such as spatial position or channel-wise slicing. This allows for efficient generation of explanatory visualizations which can assist human interpretability efforts. Our experiments demonstrate several benefits including better visualization speedup for larger models, more consistent explanation quality across architectures, improved accuracy of generated attribution maps, and most importantly user preference in human studies performed on crowdsource platforms demonstrating subjective improvements. Code is available at <https://github.com/google-research/yasenn> along with pretrained models for popular vision benchmark datasets allowing easy usage without requiring access to large scale compute resources. Keywords: Explanation, Visualization, Interpretability, Deep Learning, Convolutional Neural Networks.",1
"One advantage of decision tree based methods like random forests is their ability to natively handle categorical predictors without having to first transform them (e.g., by using feature engineering techniques). However, in this paper, we show how this capability can lead to an inherent ""absent levels"" problem for decision tree based methods that has never been thoroughly discussed, and whose consequences have never been carefully explored. This problem occurs whenever there is an indeterminacy over how to handle an observation that has reached a categorical split which was determined when the observation in question's level was absent during training. Although these incidents may appear to be innocuous, by using Leo Breiman and Adele Cutler's random forests FORTRAN code and the randomForest R package (Liaw and Wiener, 2002) as motivating case studies, we examine how overlooking the absent levels problem can systematically bias a model. Furthermore, by using three real data examples, we illustrate how absent levels can dramatically alter a model's performance in practice, and we empirically demonstrate how some simple heuristics can be used to help mitigate the effects of the absent levels problem until a more robust theoretical solution is found.",0
"Random forest algorithms have become one of the most popular machine learning techniques due to their efficiency and accuracy on complex data sets. However, there is an issue known as the “absent levels” problem that can arise when using decision trees and random forest models with categorical predictor variables. This paper examines the absent levels problem and how it impacts model performance. We explore ways to address and mitigate the problem through different methods such as encoding strategies, splitting rules, and missing value imputation. Our results show significant improvements in model performance when these methods are used effectively. By understanding and addressing the absent levels problem, researchers can improve the accuracy and robustness of their random forest models, leading to better predictions and decision making.",1
"Learning from an imbalanced dataset is a tricky proposition. Because these datasets are biased towards one class, most existing classifiers tend not to perform well on minority class examples. Conventional classifiers usually aim to optimize the overall accuracy without considering the relative distribution of each class. This article presents a superensemble classifier, to tackle and improve predictions in imbalanced classification problems, that maps Hellinger distance decision trees (HDDT) into radial basis function network (RBFN) framework. Regularity conditions for universal consistency and the idea of parameter optimization of the proposed model are provided. The proposed distribution-free model can be applied for feature selection cum imbalanced classification problems. We have also provided enough numerical evidence using various real-life data sets to assess the performance of the proposed model. Its effectiveness and competitiveness with respect to different state-of-the-art models are shown.",0
"This work presents a new algorithm that improves predictions on imbalanced datasets by using multiple base classifiers in combination with an ensemble method called superensemble. We evaluate our approach on several benchmark imbalanced datasets and compare its performance against state-of-the-art methods such as Random Forest, Gradient Boosting, XGBoost, LightGBM and AdaBoost. Our experimental results show that superensemble significantly outperforms these algorithms in terms of accuracy, precision, recall and F1 score. Furthermore, we analyze the impact of different parameters used in the proposed algorithm, including the number of base learners, their individual performances and the data distribution across classes. Overall, our findings demonstrate the effectiveness of the superensemble approach for predictive modeling tasks involving highly skewed datasets.",1
"The application to search ranking is one of the biggest machine learning success stories at Airbnb. Much of the initial gains were driven by a gradient boosted decision tree model. The gains, however, plateaued over time. This paper discusses the work done in applying neural networks in an attempt to break out of that plateau. We present our perspective not with the intention of pushing the frontier of new modeling techniques. Instead, ours is a story of the elements we found useful in applying neural networks to a real life product. Deep learning was steep learning for us. To other teams embarking on similar journeys, we hope an account of our struggles and triumphs will provide some useful pointers. Bon voyage!",0
"This study investigates how deep learning can improve search results on Airbnb, a popular platform that allows users to rent out their homes or vacation properties to travelers. The authors begin by explaining why deep learning may provide advantages over traditional machine learning methods in this context. They then describe the methodology they used to create and evaluate their model, including data collection and preprocessing, architecture design, and evaluation metrics. Results show significant improvements in both relevance and diversity compared to Airbnb's current search algorithm. Finally, the authors discuss potential future directions and limitations of their work. Overall, the paper offers valuable insights into how deep learning techniques can enhance user experience and drive innovation in the rapidly evolving field of online marketplaces.",1
"This article provides a comprehensive study of different ways to make speed benchmarks of gradient boosted decision trees algorithm. We show main problems of several straight forward ways to make benchmarks, explain, why a speed benchmarking is a challenging task and provide a set of reasonable requirements for a benchmark to be fair and useful.",0
"This paper provides an analysis on the current state of gradient boosting decision tree (GBDT) models and why their performance is often misunderstood by industry practitioners. Despite widespread adoption of these algorithms across many fields, there remains a fundamental lack of understanding around how they work and which hyperparameters lead to optimal results. To address these concerns, we conducted extensive experiments using open source datasets that compare and contrast different configurations of GBDT models. Our findings suggest that existing methods for measuring training times in terms of wall clock time may overestimate the actual speed benefits associated with certain model configurations. We propose new ways to measure algorithm speed based on accuracy and real world metrics such as F1 score, precision and recall. By doing so, we hope to provide more meaningful insights into how practitioners can make better use of GBDT models for faster predictions without sacrificing quality. Ultimately, our research aims to shift the conversation away from raw processing power towards algorithmic design choices that optimize both computational efficiency and predictive accuracy.",1
"Many efficient algorithms with strong theoretical guarantees have been proposed for the contextual multi-armed bandit problem. However, applying these algorithms in practice can be difficult because they require domain expertise to build appropriate features and to tune their parameters. We propose a new method for the contextual bandit problem that is simple, practical, and can be applied with little or no domain expertise. Our algorithm relies on decision trees to model the context-reward relationship. Decision trees are non-parametric, interpretable, and work well without hand-crafted features. To guide the exploration-exploitation trade-off, we use a bootstrapping approach which abstracts Thompson sampling to non-Bayesian settings. We also discuss several computational heuristics and demonstrate the performance of our method on several datasets.",0
"An Abstract: In this paper, we present a novel approach for solving contextual bandit problems using decision trees. Our method leverages state-of-the-art techniques from machine learning and operations research to achieve better performance than existing methods. We propose a new algorithm that builds on top of recent advances in ensemble methods for decision trees, which enables our approach to efficiently handle high-dimensional features spaces. This framework allows us to develop effective solutions for problems characterized by delayed rewards, partial observability, and uncertainty in both model parameters and rewards. Through extensive experiments on real world datasets and simulations studies, we showcase the superiority of our proposed approach over other state-of-the art algorithms. Our findings have important implications for developing efficient algorithms for sequential decision making under uncertainty, with potential applications in areas such as recommendation systems, healthcare, and finance.",1
"In this paper, we present our approach for solving the DEBS Grand Challenge 2018. The challenge asks to provide a prediction for (i) a destination and the (ii) arrival time of ships in a streaming-fashion using Geo-spatial data in the maritime context. Novel aspects of our approach include the use of ensemble learning based on Random Forest, Gradient Boosting Decision Trees (GBDT), XGBoost Trees and Extremely Randomized Trees (ERT) in order to provide a prediction for a destination while for the arrival time, we propose the use of Feed-forward Neural Networks. In our evaluation, we were able to achieve an accuracy of 97% for the port destination classification problem and 90% (in mins) for the ETA prediction.",0
"In recent years, advances in maritime traffic data collection have generated vast amounts of vessel tracking information. This has created new opportunities for real-time destination prediction for large-scale maritime operations such as ship routing, scheduling, and emergency response planning. However, despite significant progress, predicting accurate destinations and estimated time of arrival (ETAs) remains a challenging task due to complex interactions among vessels, weather conditions, sea states, port congestion, and other operational factors. In this work, we present a novel approach that exploits spatiotemporal correlations among vessel tracks by incorporating historical trip patterns into our destination prediction framework, significantly improving accuracy compared to previous methods. Our model achieves state-of-the-art performance in both speed and accuracy while effectively handling cases where initial estimates are incorrect. We demonstrate the effectiveness of our method through extensive experiments using real-world maritime datasets, showing consistent improvement over existing approaches across different geographical regions and vessel types. Overall, our findings contribute towards developing more efficient and effective maritime management systems.",1
"Road conditions affect both machine and human powered modes of transportation. In the case of human powered transportation, poor road conditions increase the work for the individual to travel. Previous estimates for these parameters have used computationally expensive analysis of satellite images. In this work, we use a computationally inexpensive and simple method by using only GPS data from a human powered cyclist. By estimating if the road taken by the user has high or low variations in their directional vector, we classify if the user is on a paved road or on an unpaved trail. In order to do this, three methods were adopted, changes in frequency of the direction of slope in a given path segment, fitting segments of the path, and finding the first derivative and the number of points of zero crossings of each segment. Machine learning models such as support vector machines, K-nearest neighbors, and decision trees were used for the classification of the path. We show in our methods, the decision trees performed the best with an accuracy of 86\%. Estimation of the type of surface can be used for many applications such as understanding rolling resistance for power estimation estimation or building exercise recommendation systems by user profiling as described in detail in the paper.",0
"One approach to estimating surface types is by using data gathered from bicycles equipped with Global Positioning System (GPS) devices. By analyzing patterns in the collected GPS track data, we can infer details about the surfaces that the cyclist was riding on. This method has several advantages over traditional methods, as it provides detailed information without requiring any additional infrastructure beyond the existing cycling network. Additionally, the ability to collect large amounts of data quickly makes it possible to estimate surface conditions across wide areas in short periods of time. In this paper, we describe our proposed methodology and present results from initial trials carried out on data collected from a fleet of shared-use bikes equipped with GPS tracking devices. Our findings indicate that it is indeed possible to use GPS tracked bicycle activities to accurately estimate surface types at scale, opening up new possibilities for urban planning and infrastructure management.",1
"Traditional human activity recognition (HAR) based on time series adopts sliding window analysis method. This method faces the multi-class window problem which mistakenly labels different classes of sampling points within a window as a class. In this paper, a HAR algorithm based on U-Net is proposed to perform activity labeling and prediction at each sampling point. The activity data of the triaxial accelerometer is mapped into an image with the single pixel column and multi-channel which is input into the U-Net network for training and recognition. Our proposal can complete the pixel-level gesture recognition function. The method does not need manual feature extraction and can effectively identify short-term behaviors in long-term activity sequences. We collected the Sanitation dataset and tested the proposed scheme with four open data sets. The experimental results show that compared with Support Vector Machine (SVM), k-Nearest Neighbor (kNN), Decision Tree(DT), Quadratic Discriminant Analysis (QDA), Convolutional Neural Network (CNN) and Fully Convolutional Networks (FCN) methods, our proposal has the highest accuracy and F1-socre in each dataset, and has stable performance and high robustness. At the same time, after the U-Net has finished training, our proposal can achieve fast enough recognition speed.",0
"Activity recognition plays a crucial role in several domains, including healthcare, surveillance, and human behavior understanding. Existing approaches use different sensors such as accelerometers, gyroscopes, GPS, cameras, etc., which can provide rich spatio-temporal data. This study proposes an approach called U-Net (an abbreviation for ""Universal Network"") that leverages time series analysis for efficient human activity recognition by effectively encoding temporal features into fixed-size vectors, known as embeddings. These embeddings are then used in combination with Convolution Neural Networks (CNN) architectures like InceptionTime and Temporal ConvLSTM networks. We evaluate our methodology across two public datasets: PAMAP2 and Smartphone Dataset and compared with existing state-of-the-art methods like DeepConvLSTM, TCN+MLP, Resformer model, ShuffleAndLearn and Attention LSTM models. Our results demonstrate significant improvement in terms of accuracy over other existing techniques achieving an average precision of 97% on both datasets. Overall our work provides new opportunities in enhancing efficiency of activity recognition systems in smart homes, wearables, industrial automation, robotics, and medical applications where real-time performance is critical.",1
"We explore the problem of learning to decompose spatial tasks into segments, as exemplified by the problem of a painting robot covering a large object. Inspired by the ability of classical decision tree algorithms to construct structured partitions of their input spaces, we formulate the problem of decomposing objects into segments as a parsing approach. We make the insight that the derivation of a parse-tree that decomposes the object into segments closely resembles a decision tree constructed by ID3, which can be done when the ground-truth available. We learn to imitate an expert parsing oracle, such that our neural parser can generalize to parse natural images without ground truth. We introduce a novel deterministic policy gradient update, DRAG (i.e., DeteRministically AGgrevate) in the form of a deterministic actor-critic variant of AggreVaTeD, to train our neural parser. From another perspective, our approach is a variant of the Deterministic Policy Gradient suitable for the imitation learning setting. The deterministic policy representation offered by training our neural parser with DRAG allows it to outperform state of the art imitation and reinforcement learning approaches.",0
"In natural language processing (NLP), parsing refers to analyzing a sentence and breaking it down into structured representations that capture meaningful relationships among words. This task has traditionally been approached using handcrafted rule-based systems or data-driven statistical models trained on large amounts of annotated text. Recently, deep learning methods have become increasingly popular due to their ability to learn complex patterns from raw input without relying on explicit rules or manual annotations. However, training neural parsers remains challenging due to the high-dimensionality and complexity of NLP tasks. To address these issues, we propose a novel approach called deterministic differentiable imitation learning (DIL). Our method combines the advantages of traditional rule-based methods with the expressive power of modern neural networks, enabling us to train accurate parsers efficiently. We demonstrate our model’s effectiveness on several benchmark datasets, showing significantly improved performance over existing state-of-the-art techniques while maintaining interpretability and simplicity. Overall, our work highlights the potential of imitation learning as a powerful tool for solving complex problems in NLP and other domains where supervision may be limited.",1
"Medical applications challenge today's text categorization techniques by demanding both high accuracy and ease-of-interpretation. Although deep learning has provided a leap ahead in accuracy, this leap comes at the sacrifice of interpretability. To address this accuracy-interpretability challenge, we here introduce, for the first time, a text categorization approach that leverages the recently introduced Tsetlin Machine. In all brevity, we represent the terms of a text as propositional variables. From these, we capture categories using simple propositional formulae, such as: if ""rash"" and ""reaction"" and ""penicillin"" then Allergy. The Tsetlin Machine learns these formulae from a labelled text, utilizing conjunctive clauses to represent the particular facets of each category. Indeed, even the absence of terms (negated features) can be used for categorization purposes. Our empirical comparison with Na\""ive Bayes, decision trees, linear support vector machines (SVMs), random forest, long short-term memory (LSTM) neural networks, and other techniques, is quite conclusive. The Tsetlin Machine either performs on par with or outperforms all of the evaluated methods on both the 20 Newsgroups and IMDb datasets, as well as on a non-public clinical dataset. On average, the Tsetlin Machine delivers the best recall and precision scores across the datasets. Finally, our GPU implementation of the Tsetlin Machine executes 5 to 15 times faster than the CPU implementation, depending on the dataset. We thus believe that our novel approach can have a significant impact on a wide range of text analysis applications, forming a promising starting point for deeper natural language understanding with the Tsetlin Machine.",0
"This paper presents a novel approach using the Tsetlin Machine to learn human-interpretable rules for high-accuracy text categorization, specifically with applications in medical literature analysis. The Tsetlin Machine is a hybrid algorithm that combines the strengths of rule learning and machine learning approaches. Our method leverages the ability of the Tsetlin Machine to generate transparent and interpretable decision rules by maximizing rule coverage while minimizing rule redundancy. We demonstrate the effectiveness of our approach through extensive experiments on benchmark datasets as well as real-world medical data sets. Results show significant improvement over state-of-the-art methods in terms of accuracy and interpretability. Overall, our work provides a promising solution for automating tasks requiring accurate classification of natural language texts in the medical domain.",1
"We utilize Wi-Fi communications from smartphones to predict their mobility mode, i.e. walking, biking and driving. Wi-Fi sensors were deployed at four strategic locations in a closed loop on streets in downtown Toronto. Deep neural network (Multilayer Perceptron) along with three decision tree based classifiers (Decision Tree, Bagged Decision Tree and Random Forest) are developed. Results show that the best prediction accuracy is achieved by Multilayer Perceptron, with 86.52% correct predictions of mobility modes.",0
"This work presents a method for detecting human mobility modes using wireless signals from mobile devices, such as smartphones or wearables. We use machine learning algorithms to classify different types of movement patterns based on data collected over time through WiFi signals. Our approach can discern whether a person is walking, running, biking, driving, or riding public transportation without requiring any additional sensors or GPS signals. The proposed solution has potential applications in areas like location privacy preservation, fitness tracking, and urban planning. The accuracy of our method was evaluated on a dataset consisting of real-world movements collected from participants wearing portable receivers that capture WiFi signals while engaging in various activities. Experimental results demonstrate that our method achieves high accuracy (94%) compared to ground truth labels obtained via manual annotations. Overall, our research advances the state of art by providing a low-cost, efficient, and effective means to monitor human activity using existing communication infrastructure.",1
"Ren et al. recently introduced a method for aggregating multiple decision trees into a strong predictor by interpreting a path taken by a sample down each tree as a binary vector and performing linear regression on top of these vectors stacked together. They provided experimental evidence that the method offers advantages over the usual approaches for combining decision trees (random forests and boosting). The method truly shines when the regression target is a large vector with correlated dimensions, such as a 2D face shape represented with the positions of several facial landmarks. However, we argue that their basic method is not applicable in many practical scenarios due to large memory requirements. This paper shows how this issue can be solved through the use of quantization and architectural changes of the predictor that maps decision tree-derived encodings to the desired output.",0
"This paper presents a method that reduces memory requirements for decision trees during training time by reducing the number of nodes. We show how to use random projections as an additional layer before building decision trees to achieve faster training times while preserving accuracy, which enables us to reduce node depths substantially without hurting performance on standard benchmark datasets. Additionally, we provide evidence that our proposed approach improves scalability compared to traditional tree ensembles because of reduced memory usage. Finally, we apply this technique to face alignment problems where previous methods have relied heavily on deep learning models. Experimental results on three commonly used evaluation sets demonstrate improvements over existing decision tree based approaches. Overall, this work demonstrates that memory-efficient global refinement can effectively handle image classification tasks using only decision trees.",1
"Extremely preterm infants often require endotracheal intubation and mechanical ventilation during the first days of life. Due to the detrimental effects of prolonged invasive mechanical ventilation (IMV), clinicians aim to extubate infants as soon as they deem them ready. Unfortunately, existing strategies for prediction of extubation readiness vary across clinicians and institutions, and lead to high reintubation rates. We present an approach using Random Forest classifiers for the analysis of cardiorespiratory variability to predict extubation readiness. We address the issue of data imbalance by employing random undersampling of examples from the majority class before training each Decision Tree in a bag. By incorporating clinical domain knowledge, we further demonstrate that our classifier could have identified 71% of infants who failed extubation, while maintaining a success detection rate of 78%.",0
"This paper presents a study exploring the use of undersampling and bagging of decision trees in analyzing cardiorespiratory behavior data for predicting extubation readiness in extremely preterm infants. The authors aimed to identify patterns in the cardiorespiratory signals that could serve as indicators of the infant's ability to breathe independently after extubation. To achieve their goal, they applied several machine learning algorithms such as random forest and gradient boosting machines to extract features from the raw signal and then used a support vector machine (SVM) classifier to predict whether an individual would pass the extubation test or not. They found that under sampling techniques were more efficient than over sampling methods. However, using bootstrap aggregating (bagging) did not improve the performance of the SVM model. In conclusion, the results suggest that undersampling can be effectively employed to address class imbalance issues, while bagging may not necessarily improve the accuracy of SVM models in analyzing cardiorespiratory signals for predicting extubation readiness in extremely preterm infants. These findings have important implications for clinicians and researchers working in neonatal intensive care units (NICUs), where early identification of infants who are ready for extubation can significantly reduce morbidity and mortality rates. Further studies are necessary to validate these findings and investigate other potential algorithms for improving extubation outcomes in extremely preterm infants.",1
"This paper examines the stability of learned explanations for black-box predictions via model distillation with decision trees. One approach to intelligibility in machine learning is to use an understandable `student' model to mimic the output of an accurate `teacher'. Here, we consider the use of regression trees as a student model, in which nodes of the tree can be used as `explanations' for particular predictions, and the whole structure of the tree can be used as a global representation of the resulting function. However, individual trees are sensitive to the particular data sets used to train them, and an interpretation of a student model may be suspect if small changes in the training data have a large effect on it. In this context, access to outcomes from a teacher helps to stabilize the greedy splitting strategy by generating a much larger corpus of training examples than was originally available. We develop tests to ensure that enough examples are generated at each split so that the same splitting rule would be chosen with high probability were the tree to be re trained. Further, we develop a stopping rule to indicate how deep the tree should be built based on recent results on the variability of Random Forests when these are used as the teacher. We provide concrete examples of these procedures on the CAD-MDD and COMPAS data sets.",0
"In recent years there has been increased interest in model distillation as a means of achieving state of the art results while minimizing computational resources, however current methods still suffer from instability issues that arise due to sampling variability inherent in the training process. This paper introduces approximation trees which combines both stochastic gradient descent (SGD) and MCMC to address these stability problems. In particular we focus on two key components 1) using SGD to optimize over an ensemble of models instead of just one single model; allowing us to handle larger capacity models without sacrificing computational efficiency and statistical accuracy. And 2) using MCMC to ensure convergence to good local optima. Empirically we demonstrate our method outperforms current distillation techniques on standard benchmark datasets by achieving lower error rates with comparable computation times. Our study provides new insights into stable yet accurate distillation methods for deep learning practitioners seeking more efficient use of their computational resources.",1
"We consider the task of pixel-wise semantic segmentation given a small set of labeled training images. Among two of the most popular techniques to address this task are Decision Forests (DF) and Neural Networks (NN). In this work, we explore the relationship between two special forms of these techniques: stacked DFs (namely Auto-context) and deep Convolutional Neural Networks (ConvNet). Our main contribution is to show that Auto-context can be mapped to a deep ConvNet with novel architecture, and thereby trained end-to-end. This mapping can be used as an initialization of a deep ConvNet, enabling training even in the face of very limited amounts of training data. We also demonstrate an approximate mapping back from the refined ConvNet to a second stacked DF, with improved performance over the original. We experimentally verify that these mappings outperform stacked DFs for two different applications in computer vision and biology: Kinect-based body part labeling from depth images, and somite segmentation in microscopy images of developing zebrafish. Finally, we revisit the core mapping from a Decision Tree (DT) to a NN, and show that it is also possible to map a fuzzy DT, with sigmoidal split decisions, to a NN. This addresses multiple limitations of the previous mapping, and yields new insights into the popular Rectified Linear Unit (ReLU), and more recently proposed concatenated ReLU (CReLU), activation functions.",0
"This research proposes a novel approach that integrates auto-context decision trees (ADTs) and deep convolutional neural networks (ConvNets), termed as ADTNet, which significantly improves segmentation accuracy compared to individual models or their combinations. We construct two types of ADT ensembles based on random splits from training data using different feature representations: one ensemble uses raw image features while another utilizes the output maps generated by the last layer of a pre-trained ConvNet. Then we adaptively aggregate both types of ADT outputs through weighted voting via nearest neighbor classifiers, where weights correspond to the average class margins of each pixel over all tree segments/samples obtained at testing time. After obtaining refined prediction confidence scores by regressing logit values against the votes, the final binary segmentation result can be determined by selecting the most confident label among the majority vote. To further boost accuracy, we replace the ADT-based ensemble layers within ADTNet with randomly initialized dense layers before fine-tuning using backpropagation, producing state-of-the-art results on three popular semantic segmentation benchmark datasets including Pascal VOC, SIFT Flow and NYU Depth v2. Experimental evaluations demonstrate that our method outperforms traditional methods and achieves significant improvements over other recent techniques without resorting to advanced postprocessing steps or multiple network designs.",1
"Motivation: Thanks to digitization, we often have access to large databases, consisting of various fields of information, ranging from numbers to texts and even boolean values. Such databases lend themselves especially well to machine learning, classification and big data analysis tasks. We are able to train classifiers, using already existing data and use them for predicting the values of a certain field, given that we have information regarding the other fields. Most specifically, in this study, we look at the Electronic Health Records (EHRs) that are compiled by hospitals. These EHRs are convenient means of accessing data of individual patients, but there processing as a whole still remains a task. However, EHRs that are composed of coherent, well-tabulated structures lend themselves quite well to the application to machine language, via the usage of classifiers. In this study, we look at a Blood Transfusion Service Center Data Set (Data taken from the Blood Transfusion Service Center in Hsin-Chu City in Taiwan). We used scikit-learn machine learning in python. From Support Vector Machines(SVM), we use Support Vector Classification(SVC), from the linear model we import Perceptron. We also used the K.neighborsclassifier and the decision tree classifiers. Furthermore, we use the TPOT library to find an optimized pipeline using genetic algorithms. Using the above classifiers, we score each one of them using k fold cross-validation.   Contact: ritabratamaiti@hiretrex.com GitHub Repository: https://github.com/ritabratamaiti/Blooddonorprediction",0
"Title: Optimizing Predictive Performance Using Genetic Algorithms to Select the Best Classifier For Blood Donor Database Classification Tasks  Predictive modeling has become increasingly important in recent years as organizations seek to make data-driven decisions that improve their operational efficiency. In particular, blood banks rely heavily on forecasting models based on the characteristics stored in large databases containing detailed records about each unit collected from individual donors. This study focuses specifically on determining which classification algorithm can most accurately predict whether any given donation would yield a sufficient volume of high quality red cells, platelets, and plasma - all of which are critical components used for treating patients who require transfusions.  Classification is accomplished by training a computer program called a classifier against historical records of donations contained within our extensive blood donor database (BDD). Since multiple classifiers may perform competitively at optimizing accuracy on different segments of the BDD, a key research challenge arises: selecting one specific classifier amongst many that have been trained by our team.  To address this challenge we use genetic programming techniques such that the performance of the top candidate algorithms is assessed through comparison of predicted vs actual outcomes. By doing so, those programs whose predictions consistently align more closely than other candidates are preferred when making selections. Specifically, we define success rates calculated across categories of blood product quality - low/medium/high yield of whole blood products per donation and overall patient satisfaction levels. Successful results herein demonstrate that the genetic programming approach outperforms random selection while providing valuable insights into why certain classifiers should be avoided and others prioritized for future application.  In conclusion, leveraging genetic algorithms allows us to select superior performing classifiers capable of meeting stringent prediction thresholds required for effective management of our complex blood bank operations. Ou",1
"We present the design and implementation of a custom discrete optimization technique for building rule lists over a categorical feature space. Our algorithm produces rule lists with optimal training performance, according to the regularized empirical risk, with a certificate of optimality. By leveraging algorithmic bounds, efficient data structures, and computational reuse, we achieve several orders of magnitude speedup in time and a massive reduction of memory consumption. We demonstrate that our approach produces optimal rule lists on practical problems in seconds. Our results indicate that it is possible to construct optimal sparse rule lists that are approximately as accurate as the COMPAS proprietary risk prediction tool on data from Broward County, Florida, but that are completely interpretable. This framework is a novel alternative to CART and other decision tree methods for interpretable modeling.",0
"In recent years there has been significant progress on designing models that can learn optimal behavior from data alone, without any prior knowledge encoded in their architecture. However, these approaches generally assume that the input distribution is fixed; if we have domain expertise available then we expect more efficient learning and better results could be obtained. We aim to combine both domain expertise and large amounts of training data by asking an expert to list rules which generate valid inputs for our application. Given such lists of rules we develop a novel deep neural network architecture, called Rule List Network (RLN), that generates candidate solutions one rule at a time. Each sampled solution gets scored based on how well it solves the task according to our objective function. Based on these scores, we refine the RLN's internal parameters so as to bias its sampling towards higher quality candidates. Finally, given a validation set generated from another expert's ruleset using different parameter settings, we fine tune hyperparameters and select the model which produces highest-quality output for those parameters. Our approach allows us to certify optimality conditions by showing that all outputs produced satisfy a provable upper bound on achievable performance on the test dataset. We evaluate our method against baselines including an ablated version of our own approach and a recurrent neural network trained to optimize categorical features directly. Results show consistent improvement across three challenging datasets.",1
"Machine learning software accounts for a significant amount of energy consumed in data centers. These algorithms are usually optimized towards predictive performance, i.e. accuracy, and scalability. This is the case of data stream mining algorithms. Although these algorithms are adaptive to the incoming data, they have fixed parameters from the beginning of the execution. We have observed that having fixed parameters lead to unnecessary computations, thus making the algorithm energy inefficient. In this paper we present the nmin adaptation method for Hoeffding trees. This method adapts the value of the nmin parameter, which significantly affects the energy consumption of the algorithm. The method reduces unnecessary computations and memory accesses, thus reducing the energy, while the accuracy is only marginally affected. We experimentally compared VFDT (Very Fast Decision Tree, the first Hoeffding tree algorithm) and CVFDT (Concept-adapting VFDT) with the VFDT-nmin (VFDT with nmin adaptation). The results show that VFDT-nmin consumes up to 27% less energy than the standard VFDT, and up to 92% less energy than CVFDT, trading off a few percent of accuracy in a few datasets.",0
"Machine Learning models can often encounter outliers during training which might affect their performance greatly especially in cases such as decision making systems where the quality of predictions has direct impact on people’s lives. One effective method in tackling these issues is by adapting the splitting criteria of decision trees using minmax regret approach proposed by [Hoeffding92]. In practice, setting parameter values for controlling outlier detection remains challenging due to limited knowledge about data distributions and sensitivity of the procedure to input parameters [Liu98][Friedman08b],[Kriegel12]. This study presents a new algorithm that automates this process while minimizing the number of misclassifications allowing for better understanding of the model behavior under varying conditions. Experiments were conducted on both synthetic datasets as well as popular public benchmarks demonstrating the effectiveness of our proposal against other state of art methods. Further extensions including ensemble learning techniques were investigated as means of further improving overall predictive power of tree based models in presence of anomalous observations. Our findings contribute significantly towards better understanding of machine learning practices in real world scenarios providing insights into selection of proper algorithms and hyperparameter tuning.",1
"In this report we propose a classification technique for skin lesion images as a part of our submission for ISIC 2018 Challenge in Skin Lesion Analysis Towards Melanoma Detection. Our data was extracted from the ISIC 2018: Skin Lesion Analysis Towards Melanoma Detection grand challenge datasets. The features are extracted through a Convolutional Neural Network, in our case ResNet50 and then using these features we train a DeepForest, having cascading layers, to classify our skin lesion images. We know that Convolutional Neural Networks are a state-of-the-art technique in representation learning for images, with the convolutional filters learning to detect features from images through backpropagation. These features are then usually fed to a classifier like a softmax layer or other such classifiers for classification tasks. In our case we do not use the traditional backpropagation method and train a softmax layer for classification. Instead, we use Deep Forest, a novel decision tree ensemble approach with performance highly competitive to deep neural networks in a broad range of tasks. Thus we use a ResNet50 to extract the features from skin lesion images and then use the Deep Forest to classify these images. This method has been used because Deep Forest has been found to be hugely efficient in areas where there are only small-scale training data available. Also as the Deep Forest network decides its complexity by itself, it also caters to the problem of dataset imbalance we faced in this problem.",0
"This paper proposes a method for disease classification within dermascopic images using feature extraction from a pre-trained convolutional neural network (CNN) and subsequent classification via a deep forest algorithm. Utilizing a popular CNN architecture such as ResNet50 provides an efficient means of obtaining high quality image representations that capture relevant patterns and characteristics present within the images. These representations can then be utilized as input data for machine learning algorithms, including deep forest, which has been shown to perform well in many computer vision tasks. By leveraging these powerful techniques together, we aim to improve diagnostic accuracy in skin diseases, ultimately leading to better patient outcomes. Results of our experiments show promising improvements over traditional methods, making this approach highly appealing for medical practitioners worldwide. Overall, our work demonstrates the potential benefits of integrating modern artificial intelligence approaches into clinical practice.",1
"Deep neural network models owe their representational power to the high number of learnable parameters. It is often infeasible to run these largely parametrized deep models in limited resource environments, like mobile phones. Network models employing conditional computing are able to reduce computational requirements while achieving high representational power, with their ability to model hierarchies. We propose Conditional Information Gain Networks, which allow the feed forward deep neural networks to execute conditionally, skipping parts of the model based on the sample and the decision mechanisms inserted in the architecture. These decision mechanisms are trained using cost functions based on differentiable Information Gain, inspired by the training procedures of decision trees. These information gain based decision mechanisms are differentiable and can be trained end-to-end using a unified framework with a general cost function, covering both classification and decision losses. We test the effectiveness of the proposed method on MNIST and recently introduced Fashion MNIST datasets and show that our information gain based conditional execution approach can achieve better or comparable classification results using significantly fewer parameters, compared to standard convolutional neural network baselines.",0
"Abstract: In this paper we introduce conditional information gain networks (CIGNet), which provide interpretable representations of conditional probability distributions by leveraging both deep learning techniques such as neural network architectures while still providing the interpretability offered by Bayesian methods. We show that CIGNets yield state-of-the-art accuracy on several benchmark datasets including image classification, speech recognition, sentiment analysis and natural language processing tasks. Finally, we demonstrate that our approach can be applied beyond supervised learning problems, and performs well across different architectural choices and loss functions. This makes CIGNets a powerful framework that provides flexible, high performing predictions grounded in probabilistic theory, thereby enabling novel applications and analyses compared to previous approaches. Keywords: Probabilistic models, Deep Learning, Interpretable Representations Abstract: In this paper, we present conditional information gain networks (CIGNet) - a new method for modeling conditional probability distributions. CIGNet combines the benefits of deep learning, with their powerful expressiveness, and those of Bayesian inference, allowing us to generate interpretable representations. Our experiments on several benchmark datasets including image classification, speech recognition, sentiment analysis and natural language processing show that CIGNet achieves state-of-the-art performance. Furthermore, we illustrate how CIGNet can also address other types of machine learning problems, thus demonstrating its versatility. Overall, CIGNet represents a promising approach towards building more transparent and reliable artificial intelligence systems.",1
"We study the problem of attacking a machine learning model in the hard-label black-box setting, where no model information is revealed except that the attacker can make queries to probe the corresponding hard-label decisions. This is a very challenging problem since the direct extension of state-of-the-art white-box attacks (e.g., CW or PGD) to the hard-label black-box setting will require minimizing a non-continuous step function, which is combinatorial and cannot be solved by a gradient-based optimizer. The only current approach is based on random walk on the boundary, which requires lots of queries and lacks convergence guarantees. We propose a novel way to formulate the hard-label black-box attack as a real-valued optimization problem which is usually continuous and can be solved by any zeroth order optimization algorithm. For example, using the Randomized Gradient-Free method, we are able to bound the number of iterations needed for our algorithm to achieve stationary points. We demonstrate that our proposed method outperforms the previous random walk approach to attacking convolutional neural networks on MNIST, CIFAR, and ImageNet datasets. More interestingly, we show that the proposed algorithm can also be used to attack other discrete and non-continuous machine learning models, such as Gradient Boosting Decision Trees (GBDT).",0
"This paper presents a new approach for black-box adversarial attacks on deep neural networks (DNNs), using the concept of query efficiency from evolutionary computation research. Inspired by recent work in query efficient white box attacks like Boundary Attack, we propose an optimization based query-efficient hard label attacker that is more powerful than traditional methods. Our method can efficiently generate input perturbations which lead the DNN model to produce incorrect classifications without requiring any knowledge of internal weights or gradients. By formulating the attack as a constrained nonlinear programming problem, we minimize queries made to the target network while maximizing the accuracy obtained through local search procedures. Experimental results demonstrate the effectiveness and superiority of our proposed method compared to state-of-the-art black box attacks in terms of attack success rates and query efficiency on popular benchmark datasets such as CIFAR-10 and ImageNet. With high transferability across models and limited reliance on strong assumptions, our approach provides a valuable tool for evaluating and improving the robustness of modern machine learning systems against unknown adversaries.",1
"In this work a novel, automated process for constructing and initializing deep feed-forward neural networks based on decision trees is presented. The proposed algorithm maps a collection of decision trees trained on the data into a collection of initialized neural networks, with the structures of the networks determined by the structures of the trees. The tree-informed initialization acts as a warm-start to the neural network training process, resulting in efficiently trained, accurate networks. These models, referred to as ""deep jointly-informed neural networks"" (DJINN), demonstrate high predictive performance for a variety of regression and classification datasets, and display comparable performance to Bayesian hyper-parameter optimization at a lower computational cost. By combining the user-friendly features of decision tree models with the flexibility and scalability of deep neural networks, DJINN is an attractive algorithm for training predictive models on a wide range of complex datasets.",0
"This paper proposes a new method for initializing deep neural networks (DNNs) using decision trees as a guide. We show that by starting from these decision tree solutions we can obtain better generalization performance than traditional random initialization methods, while reducing the need for extensive hyperparameter tuning. Our method is flexible and applicable to different architectures such as convolutional neural networks (CNNs). Empirical evaluation on multiple benchmark datasets demonstrates that our method outperforms state-of-the-art random initialization baselines across a wide range of problem sizes and model complexity. The results demonstrate the effectiveness of our approach in accelerating DNN training and improving generalization performance. Overall, our work sheds light on the role of decision trees in guiding the learning process of more expressive models like DNNs, and provides insights into how to improve their efficiency and efficacy.",1
"We describe the multi-GPU gradient boosting algorithm implemented in the XGBoost library (https://github.com/dmlc/xgboost). Our algorithm allows fast, scalable training on multi-GPU systems with all of the features of the XGBoost library. We employ data compression techniques to minimise the usage of scarce GPU memory while still allowing highly efficient implementation. Using our algorithm we show that it is possible to process 115 million training instances in under three minutes on a publicly available cloud computing instance. The algorithm is implemented using end-to-end GPU parallelism, with prediction, gradient calculation, feature quantisation, decision tree construction and evaluation phases all computed on device.",0
"XGBoost: Scalable GPU Accelerated Learning, presents a new machine learning algorithm that utilizes graphical processing units (GPUs) to dramatically speed up training times on large datasets while maintaining high accuracy. This cutting-edge approach uses gradient boosted decision trees to model complex data, allowing for efficient handling of diverse features and noisy signals.  The paper outlines the challenges faced by traditional machine learning algorithms when dealing with massive amounts of data, such as slow convergence rates and overfitting issues. To address these problems, XGBoost leverages advanced tree pruning techniques, ensuring both interpretability and predictive performance. Furthermore, our proposed method benefits from parallel computing capabilities provided by modern GPU architectures, resulting in significant speedups compared to CPU-based implementations.  To demonstrate the effectiveness of XGBoost, we conduct extensive experiments using real-world datasets covering various domains, including sentiment analysis, regression, and classification tasks. Our results show that our algorithm consistently achieves state-of-the-art prediction accuracies while significantly reducing computational costs, thanks to its scalable design and hardware acceleration.  In summary, XGBoost represents an important step forward in the field of machine learning, making it possible to efficiently handle big data workloads without sacrificing accuracy or usability. With its combination of powerful algorithms and enhanced hardware support, our system offers practitioners a potent toolkit for building accurate models that can scale to meet emerging demands in this rapidly evolving landscape.",1
"It has been shown that for automated PAP-smear image classification, nucleus features can be very informative. Therefore, the primary step for automated screening can be cell-nuclei detection followed by segmentation of nuclei in the resulting single cell PAP-smear images. We propose a patch based approach using CNN for segmentation of nuclei in single cell images. We then pose the question of ion of segmentation for classification using representation learning with CNN, and whether low-level CNN features may be useful for classification. We suggest a CNN-based feature level analysis and a transfer learning based approach for classification using both segmented as well full single cell images. We also propose a decision-tree based approach for classification. Experimental results demonstrate the effectiveness of the proposed algorithms individually (with low-level CNN features), and simultaneously proving the sufficiency of cell-nuclei detection (rather than accurate segmentation) for classification. Thus, we propose a system for analysis of multi-cell PAP-smear images consisting of a simple nuclei detection algorithm followed by classification using transfer learning.",0
"This paper explores considerations for designing and implementing a Pap smear image analysis system using Convolutional Neural Networks (CNN) features. The authors discuss key factors such as image acquisition, preprocessing, feature extraction, classification, evaluation, and clinical integration that must be taken into account during development. They provide insights on potential challenges faced by the researchers who aim to build such systems along with possible solutions. The study emphasizes the importance of having robust CNN models combined with rigorous evaluations to ensure accurate and reliable diagnosis in women's healthcare. By examining existing literature, the authors present recommendations for future work in developing efficient computational methods tailored specifically towards cervical cancer detection. Overall, this comprehensive review serves as a foundation for the development of automated tools for interpreting Pap smear images, which has the potential to improve early detection rates while reducing diagnostic errors.",1
"Recent advances in interpretable Machine Learning (iML) and eXplainable AI (XAI) construct explanations based on the importance of features in classification tasks. However, in a high-dimensional feature space this approach may become unfeasible without restraining the set of important features. We propose to utilize the human tendency to ask questions like ""Why this output (the fact) instead of that output (the foil)?"" to reduce the number of features to those that play a main role in the asked contrast. Our proposed method utilizes locally trained one-versus-all decision trees to identify the disjoint set of rules that causes the tree to classify data points as the foil and not as the fact. In this study we illustrate this approach on three benchmark classification tasks.",0
"""Contrastive explanations are powerful tools that enable us to identify key differences between two objects or concepts. In many applications, these differences can lead to more accurate predictions or better decision making. In this work, we introduce local foil trees as a new method for generating contrastive explanations. These trees provide clear visual representations of how one object differs from another, highlighting specific features or attributes that are unique to each item. Our approach leverages recent advances in tree-based algorithms and human perception research to create intuitive and informative explanations. We evaluate our method using several case studies and show that it leads to significantly improved accuracy compared to existing techniques. Furthermore, we demonstrate how local foil trees can be used in real-world scenarios such as image classification and text understanding.""",1
"We aim to predict whether an employee of a company will leave or not, using the k-Nearest Neighbors algorithm. We use evaluation of employee performance, average monthly hours at work and number of years spent in the company, among others, as our features. Other approaches to this problem include the use of ANNs, decision trees and logistic regression. The dataset was split, using 70% for training the algorithm and 30% for testing it, achieving an accuracy of 94.32%.",0
"Title: Employee Attrition Prediction  Abstract:  In recent years, employee attrition has become a major concern for organizations as it can lead to high costs associated with hiring, training, and productivity loss. Therefore, predicting employee turnover before it actually happens could significantly benefit companies by allowing them to take preventative measures to reduce attrition rates. This study aimed to develop a model that accurately predicts employee attrition using demographic variables (e.g., age, gender, education level), job characteristics (e.g., tenure, number of sick days taken), and personality traits (e.g., extraversion, conscientiousness). Data from over 600 employees across different industries was collected through self-report questionnaires and analyzed using statistical methods such as logistic regression and decision trees. Results showed that the proposed model achieved high accuracy in predicting employee turnover intentions, thus providing evidence for the effectiveness of incorporating multiple factors into employee retention models. Future research directions and implications for practice are discussed. Keywords: employee attrition, prediction, demographics, job characteristics, personality traits, logistic regression, decision tree",1
"Deep neural networks have been proven powerful at processing perceptual data, such as images and audio. However for tabular data, tree-based models are more popular. A nice property of tree-based models is their natural interpretability. In this work, we present Deep Neural Decision Trees (DNDT) -- tree models realised by neural networks. A DNDT is intrinsically interpretable, as it is a tree. Yet as it is also a neural network (NN), it can be easily implemented in NN toolkits, and trained with gradient descent rather than greedy splitting. We evaluate DNDT on several tabular datasets, verify its efficacy, and investigate similarities and differences between DNDT and vanilla decision trees. Interestingly, DNDT self-prunes at both split and feature-level.",0
"This paper presents Deep Neural Decision Trees (DNTT), a novel approach that combines decision trees with deep learning methods. In traditional decision trees, each internal node represents a test on one feature, while in DNTTs, the tests at inner nodes correspond to multi-layer neural networks. We show how these tests can capture nonlinear relationships between features, thus increasing the accuracy of predictions. We evaluate our method using several benchmark datasets, demonstrating improved performance over standard decision tree algorithms. Furthermore, we show that our models obtain competitive results compared to state-of-the-art deep learning models. Our contributions provide new insights into enhancing interpretability of blackbox predictive modeling techniques with machine learning decision trees combined with deep learning components.",1
"Label ranking aims to learn a mapping from instances to rankings over a finite number of predefined labels. Random forest is a powerful and one of the most successful general-purpose machine learning algorithms of modern times. In this paper, we present a powerful random forest label ranking method which uses random decision trees to retrieve nearest neighbors. We have developed a novel two-step rank aggregation strategy to effectively aggregate neighboring rankings discovered by the random forest into a final predicted ranking. Compared with existing methods, the new random forest method has many advantages including its intrinsically scalable tree data structure, highly parallel-able computational architecture and much superior performance. We present extensive experimental results to demonstrate that our new method achieves the highly competitive performance compared with state-of-the-art methods for datasets with complete ranking and datasets with only partial ranking information.",0
"This can also serve as an example response: This study proposes using random forest as a method for label ranking. Unlike traditional machine learning methods, which output binary labels, random forest enables rankings that reflect the uncertainty and fuzziness inherent in human decision making processes. To evaluate the feasibility of our approach, we conducted experiments using standard datasets from two different domains, text classification and image recognition. Our results show that random forest outperforms baseline models in terms of both accuracy and F1 score, demonstrating its potential utility for real-world applications such as spam filtering, sentiment analysis, and medical diagnosis.",1
"Multi-layered representation is believed to be the key ingredient of deep neural networks especially in cognitive tasks like computer vision. While non-differentiable models such as gradient boosting decision trees (GBDTs) are the dominant methods for modeling discrete or tabular data, they are hard to incorporate with such representation learning ability. In this work, we propose the multi-layered GBDT forest (mGBDTs), with an explicit emphasis on exploring the ability to learn hierarchical representations by stacking several layers of regression GBDTs as its building block. The model can be jointly trained by a variant of target propagation across layers, without the need to derive back-propagation nor differentiability. Experiments and visualizations confirmed the effectiveness of the model in terms of performance and representation learning ability.",0
"Gradient boosting decision trees (GBDTs) have emerged as a powerful machine learning tool that has shown state-of-the-art performance on many tasks. One key limitation of GBDTs is their tendency to overfit data, which can lead to poor generalization performance. In recent years, multi-layered gradient boosting decision tree models have been proposed as a potential solution to address this issue. These models use multiple levels of decision trees, where each level refines the predictions made by the previous level. This paper presents a comprehensive analysis of multi-layered gradient boosting decision trees, including an evaluation of their ability to improve generalization performance compared to standard single-level GBDTs. We show through extensive experiments that using multiple layers of decision trees can indeed reduce overfitting and significantly improve model accuracy across several benchmark datasets. Our results demonstrate the effectiveness of multi-layered GBDTs and provide insights into how these models should be designed and implemented to achieve optimal performance. Finally, we discuss future research directions and opportunities related to developing more advanced multi-layered GBDT architectures.",1
"This paper proposes the decision tree latent controller generative adversarial network (DTLC-GAN), an extension of a GAN that can learn hierarchically interpretable representations without relying on detailed supervision. To impose a hierarchical inclusion structure on latent variables, we incorporate a new architecture called the DTLC into the generator input. The DTLC has a multiple-layer tree structure in which the ON or OFF of the child node codes is controlled by the parent node codes. By using this architecture hierarchically, we can obtain the latent space in which the lower layer codes are selectively used depending on the higher layer ones. To make the latent codes capture salient semantic features of images in a hierarchically disentangled manner in the DTLC, we also propose a hierarchical conditional mutual information regularization and optimize it with a newly defined curriculum learning method that we propose as well. This makes it possible to discover hierarchically interpretable representations in a layer-by-layer manner on the basis of information gain by only using a single DTLC-GAN model. We evaluated the DTLC-GAN on various datasets, i.e., MNIST, CIFAR-10, Tiny ImageNet, 3D Faces, and CelebA, and confirmed that the DTLC-GAN can learn hierarchically interpretable representations with either unsupervised or weakly supervised settings. Furthermore, we applied the DTLC-GAN to image-retrieval tasks and showed its effectiveness in representation learning.",0
"We propose a novel approach to generative adversarial image synthesis using decision tree latent controllers (DTLCs). Our method combines two state-of-the-art techniques: variational autoencoders (VAEs) and generative adversarial networks (GANs), which have been shown to produce high quality images in recent years. In our system, we use a VAE to encode input data into a compact latent representation that captures important features and structures, while a GAN generates new samples from this latent space using a generator network. To improve stability and control over the generation process, we introduce a DTLC as a controller, which selects different branches within the generator network based on decisions made during training. This allows us to achieve more precise control over generated images by adjusting the selection policy of the controller. By integrating these components together, our framework can effectively synthesize diverse and realistic images while ensuring their coherence and consistency across multiple scales and resolutions. We evaluate our method on several datasets and demonstrate significant improvements over existing methods in terms of both quantitative metrics and visual inspections.",1
"Boosted decision trees enjoy popularity in a variety of applications; however, for large-scale datasets, the cost of training a decision tree in each round can be prohibitively expensive. Inspired by ideas from the multi-arm bandit literature, we develop a highly efficient algorithm for computing exact greedy-optimal decision trees, outperforming the state-of-the-art Quick Boost method. We further develop a framework for deriving lower bounds on the problem that applies to a wide family of conceivable algorithms for the task (including our algorithm and Quick Boost), and we demonstrate empirically on a wide variety of data sets that our algorithm is near-optimal within this family of algorithms. We also derive a lower bound applicable to any algorithm solving the task, and we demonstrate that our algorithm empirically achieves performance close to this best-achievable lower bound.",0
"Decision trees have become popular models in machine learning due to their interpretability and ability to handle high-dimensional data. However, as decision tree algorithms generate deeper trees, their performance can degrade due to overfitting, which is caused by including irrelevant features that add complexity but little value to the model. To tackle this issue, feature selection methods such as recursive feature elimination (RFE) and Lasso regression are commonly used. Nonetheless, these approaches do not consider the interaction between different subsets of features. Therefore, we propose adaptive pruning for boosting decision trees (APoBoost), a method that uses a predefined number of decision trees to iteratively identify important features in a nonlinear manner. Our approach prunes branches in each decision tree based on a statistical significance test while ensuring the importance of all selected features across decision trees. Our experiments show that our proposed method outperforms existing state-of-the-art feature selection techniques across multiple datasets, including image classification tasks using CIFAR-10 and ImageNet, showing improved accuracy and reduced computational cost. Our work highlights the potential benefits of incorporating ensemble methods into feature selection procedures for enhancing the performance of boosted decision trees.",1
"We propose a novel method called deep convolutional decision jungle (CDJ) and its learning algorithm for image classification. The CDJ maintains the structure of standard convolutional neural networks (CNNs), i.e. multiple layers of multiple response maps fully connected. Each response map-or node-in both the convolutional and fully-connected layers selectively respond to class labels s.t. each data sample travels via a specific soft route of those activated nodes. The proposed method CDJ automatically learns features, whereas decision forests and jungles require pre-defined feature sets. Compared to CNNs, the method embeds the benefits of using data-dependent discriminative functions, which better handles multi-modal/heterogeneous data; further,the method offers more diverse sparse network responses, which in turn can be used for cost-effective learning/classification. The network is learnt by combining conventional softmax and proposed entropy losses in each layer. The entropy loss,as used in decision tree growing, measures the purity of data activation according to the class label distribution. The back-propagation rule for the proposed loss function is derived from stochastic gradient descent (SGD) optimization of CNNs. We show that our proposed method outperforms state-of-the-art methods on three public image classification benchmarks and one face verification dataset. We also demonstrate the use of auxiliary data labels, when available, which helps our method to learn more discriminative routing and representations and leads to improved classification.",0
"This paper presents a new deep convolutional decision jungle (DCDJ) architecture for image classification tasks that achieves state-of-the-art performance on several benchmark datasets. Our approach uses a novel combination of hierarchical feature representations obtained from multiple layers of convolutional neural networks, along with a sparsely connected decision layer that captures high-level semantic features. The resulting DCDJ model effectively balances local and global contextual information, outperforming previous methods by significant margins. We evaluate our method using standard evaluation metrics such as accuracy, precision, recall, F1 score, and ROC curve analysis, demonstrating its superiority over other architectures on challenging real-world image classification problems. Overall, this work contributes a powerful new tool for computer vision researchers and practitioners seeking improved performance on difficult image understanding tasks.",1
"With the explosion in the availability of spatio-temporal tracking data in modern sports, there is an enormous opportunity to better analyse, learn and predict important events in adversarial group environments. In this paper, we propose a deep decision tree architecture for discriminative dictionary learning from adversarial multi-agent trajectories. We first build up a hierarchy for the tree structure by adding each layer and performing feature weight based clustering in the forward pass. We then fine tune the player role weights using back propagation. The hierarchical architecture ensures the interpretability and the integrity of the group representation. The resulting architecture is a decision tree, with leaf-nodes capturing a dictionary of multi-agent group interactions. Due to the ample volume of data available, we focus on soccer tracking data, although our approach can be used in any adversarial multi-agent domain. We present applications of proposed method for simulating soccer games as well as evaluating and quantifying team strategies.",0
"In recent years, discriminative dictionary learning has emerged as an effective approach to feature extraction, dimensionality reduction, and pattern recognition tasks. However, existing methods often struggle to capture complex nonlinear relationships between features and classes due to their reliance on linear models such as sparse coding or multi-task regression. This paper presents a novel method that overcomes these limitations by leveraging deep decision trees and adversarial multi-agent trajectories. Our proposed model, called Deep Decision Trees (DDT), learns a hierarchy of tree-structured dictionaries, each capturing increasingly fine-grained patterns, while ensuring high classification accuracy and interpretability. We demonstrate DDT’s superior performance on several benchmark datasets compared to state-of-the-art alternatives, highlighting its effectiveness in addressing real-world challenges in computer vision and other domains where data can exhibit intricate structures requiring nuanced representation.",1
"Ability for accurate hospital case cost modelling and prediction is critical for efficient health care financial management and budgetary planning. A variety of regression machine learning algorithms are known to be effective for health care cost predictions. The purpose of this experiment was to build an Azure Machine Learning Studio tool for rapid assessment of multiple types of regression models. The tool offers environment for comparing 14 types of regression models in a unified experiment: linear regression, Bayesian linear regression, decision forest regression, boosted decision tree regression, neural network regression, Poisson regression, Gaussian processes for regression, gradient boosted machine, nonlinear least squares regression, projection pursuit regression, random forest regression, robust regression, robust regression with mm-type estimators, support vector regression. The tool presents assessment results arranged by model accuracy in a single table using five performance metrics. Evaluation of regression machine learning models for performing hospital case cost prediction demonstrated advantage of robust regression model, boosted decision tree regression and decision forest regression. The operational tool has been published to the web and openly available for experiments and extensions.",0
"This study evaluates the effectiveness of using machine learning algorithms, specifically those built in Microsoft's Azure Machine Learning Studio (Azure MLS), for predicting hospital case costs. The primary aim was to assess the accuracy and reliability of these models in making cost predictions that could inform hospital decision-making processes. To achieve this goal, we trained and tested several machine learning algorithms on a dataset of historical patient cases from a large hospital network. Our findings demonstrate that while some algorithms perform well overall, others may struggle to produce reliable results. We discuss the implications of our research for hospitals considering implementing machine learning tools for cost prediction and highlight areas where further development would be beneficial. Ultimately, the study provides valuable insights into the potential use of Azure MLS for healthcare operations management.",1
"In the era of ""big data"", it is becoming more of a challenge to not only build state-of-the-art predictive models, but also gain an understanding of what's really going on in the data. For example, it is often of interest to know which, if any, of the predictors in a fitted model are relatively influential on the predicted outcome. Some modern algorithms---like random forests and gradient boosted decision trees---have a natural way of quantifying the importance or relative influence of each feature. Other algorithms---like naive Bayes classifiers and support vector machines---are not capable of doing so and model-free approaches are generally used to measure each predictor's importance. In this paper, we propose a standardized, model-based approach to measuring predictor importance across the growing spectrum of supervised learning algorithms. Our proposed method is illustrated through both simulated and real data examples. The R code to reproduce all of the figures in this paper is available in the supplementary materials.",0
"In this paper we describe a model based variable importance measure that performs well on several benchmarks. Our method estimates feature importances by training models without each input features separately to produce predictions. We compare our approach against existing methods such as SHAP, LIME, permutation feature elimination and findings suggest our method outperforms these popular approaches on many data sets while having comparable performance on others.",1
"Mosquitoes are vectors of many human diseases. In particular, Aedes \ae gypti (Linnaeus) is the main vector for Chikungunya, Dengue, and Zika viruses in Latin America and it represents a global threat. Public health policies that aim at combating this vector require dependable and timely information, which is usually expensive to obtain with field campaigns. For this reason, several efforts have been done to use remote sensing due to its reduced cost. The present work includes the temporal modeling of the oviposition activity (measured weekly on 50 ovitraps in a north Argentinean city) of Aedes \ae gypti (Linnaeus), based on time series of data extracted from operational earth observation satellite images. We use are NDVI, NDWI, LST night, LST day and TRMM-GPM rain from 2012 to 2016 as predictive variables. In contrast to previous works which use linear models, we employ Machine Learning techniques using completely accessible open source toolkits. These models have the advantages of being non-parametric and capable of describing nonlinear relationships between variables. Specifically, in addition to two linear approaches, we assess a Support Vector Machine, an Artificial Neural Networks, a K-nearest neighbors and a Decision Tree Regressor. Considerations are made on parameter tuning and the validation and training approach. The results are compared to linear models used in previous works with similar data sets for generating temporal predictive models. These new tools perform better than linear approaches, in particular Nearest Neighbor Regression (KNNR) performs the best. These results provide better alternatives to be implemented operatively on the Argentine geospatial Risk system that is running since 2012.",0
"Dengue fever is one of the most prevalent mosquito-borne diseases worldwide, affecting millions of individuals annually. Controlling dengue requires understanding vector population dynamics, which can be difficult due to the large spatial extent of mosquito populations and the challenges associated with collecting data on these populations. This study uses remote sensing data and machine learning techniques to model dengue vector populations. By leveraging satellite imagery and other remotely sensed data sources, we aim to provide accurate predictions of mosquito densities at local scales and improve our understanding of the environmental factors that drive their distributions.  Using historical and current dengue incidence data along with relevant environmental variables from various publicly available databases, we developed predictive models that accurately estimate vector density for individual locations and areas. Our models show high accuracy and consistency across different regions, providing evidence of their robustness.  The results of this research have important implications for dengue control programs as well as general public health initiatives. With better insight into the spatial distribution of disease vectors, targeted interventions such as insecticide spraying, larviciding, and community education campaigns become more effective. Furthermore, by improving our knowledge of the ecological drivers of vector-borne diseases, we contribute to global efforts towards sustainable development goals related to human health and wellbeing. Overall, this research demonstrates how advanced remote sensing technology and machine learning algorithms can enhance our capabilities in addressing complex global health problems.",1
"In this paper, we present a novel sequential paradigm for classification in crowdsourcing systems. Considering that workers are unreliable and they perform the tests with errors, we study the construction of decision trees so as to minimize the probability of mis-classification. By exploiting the connection between probability of mis-classification and entropy at each level of the decision tree, we propose two algorithms for decision tree design. Furthermore, the worker assignment problem is studied when workers can be assigned to different tests of the decision tree to provide a trade-off between classification cost and resulting error performance. Numerical results are presented for illustration.",0
"This paper presents a decision tree design for classification in crowdsourcing systems that leverages user behavior data collected through online platforms such as Amazon Mechanical Turk (MTurk). Our approach uses statistical analysis of aggregated user activity logs to identify patterns and correlations between workers’ characteristics and task performance. We then use these insights to inform the construction of our decision trees, which can accurately predict individual worker quality based on their demographic attributes and other relevant factors. Through extensive experimental evaluation using real MTurk datasets, we demonstrate that our method outperforms several state-of-the-art approaches in terms of accuracy, precision, recall, F1 score, and area under the curve (AUC) metrics. Furthermore, we provide evidence of how our system can effectively screen low-quality workers and facilitate better allocation of tasks, potentially improving overall system efficiency and reducing costly rejections or revisions. Overall, our findings contribute to the broader understanding of crowdsourcing systems and the development of more effective models for managing complex human computation processes.",1
"We propose a dynamic boosted ensemble learning method based on random forest (DBRF), a novel ensemble algorithm that incorporates the notion of hard example mining into Random Forest (RF) and thus combines the high accuracy of Boosting algorithm with the strong generalization of Bagging algorithm. Specifically, we propose to measure the quality of each leaf node of every decision tree in the random forest to determine hard examples. By iteratively training and then removing easy examples from training data, we evolve the random forest to focus on hard examples dynamically so as to learn decision boundaries better. Data can be cascaded through these random forests learned in each iteration in sequence to generate predictions, thus making RF deep. We also propose to use evolution mechanism and smart iteration mechanism to improve the performance of the model. DBRF outperforms RF on three UCI datasets and achieved state-of-the-art results compared to other deep models. Moreover, we show that DBRF is also a new way of sampling and can be very useful when learning from imbalanced data.",0
"This research proposes a novel method called dynamic boosted ensemble learning based on random forest (DBRF) that addresses several limitations of traditional machine learning models by improving accuracy through ensembling multiple models that dynamically adapt during training. DBRF effectively combines two techniques: bagging and boosting, enabling parallel model building while reducing overfitting due to correlation among base learners. Experimental results demonstrate significant improvements compared to other state-of-the-art methods across various data sets and tasks. The proposed method has promising applications in areas such as finance, healthcare, and image recognition.",1
"Ensembles are popular methods for solving practical supervised learning problems. They reduce the risk of having underperforming models in production-grade software. Although critical, methods for learning heterogeneous regression ensembles have not been proposed at large scale, whereas in classical ML literature, stacking, cascading and voting are mostly restricted to classification problems. Regression poses distinct learning challenges that may result in poor performance, even when using well established homogeneous ensemble schemas such as bagging or boosting.   In this paper, we introduce MetaBags, a novel, practically useful stacking framework for regression. MetaBags is a meta-learning algorithm that learns a set of meta-decision trees designed to select one base model (i.e. expert) for each query, and focuses on inductive bias reduction. A set of meta-decision trees are learned using different types of meta-features, specially created for this purpose - to then be bagged at meta-level. This procedure is designed to learn a model with a fair bias-variance trade-off, and its improvement over base model performance is correlated with the prediction diversity of different experts on specific input space subregions. The proposed method and meta-features are designed in such a way that they enable good predictive performance even in subregions of space which are not adequately represented in the available training data.   An exhaustive empirical testing of the method was performed, evaluating both generalization error and scalability of the approach on synthetic, open and real-world application datasets. The obtained results show that our method significantly outperforms existing state-of-the-art approaches.",0
"This article presents a new machine learning algorithm called ""MetaBags"" which uses bagging (Bootstrap aggregating) together with a regression decision tree meta model named ""XGBoost"". Unlike prior works that use decision trees as base models, our algorithm trains multiple instances of XGBoost on subsets of each other, where each training instance sees different feature inputs obtained by random sampling from larger sets, thus leading to better diversity among ensembles of trees during bagging process without increasing computational complexity. Empirical studies using six datasets including three real world data show improved performance of the proposed algorithm over existing algorithms like Random Forest and Gradient Boosting Machines, both in terms of speed and accuracy. Our work shows superior results especially for complex applications involving high dimensions of features and large amounts of computation.",1
"Rear-end collision warning system has a great role to enhance the driving safety. In this system some measures are used to estimate the dangers and the system warns drivers to be more cautious. The real-time processes should be executed in such system, to remain enough time and distance to avoid collision with the front vehicle. To this end, in this paper a new system is developed by using random forest classifier. To evaluate the performance of the proposed system, vehicles trajectory data of 100 car's database from Virginia tech transportation institute are used and the methods are compared based on their accuracy and their processing time. By using TOPSIS multi-criteria selection method, we show that the results of the implemented classifier is better than the results of different classifiers including Bayesian network, naive Bayes, MLP neural network, support vector machine, nearest neighbor, rule-based methods and decision tree. The presented experiments reveals that the random forest is an acceptable algorithm for the proposed driver assistant system with 88.4% accuracy for detecting warning situations and 94.7% for detecting safe situations.",0
"This paper presents a novel approach for developing a real-time warning system for detecting potential rear-end collisions using machine learning techniques. To achieve this objective, we have developed a Random Forest Classifier that can predict whether or not a vehicle ahead of another will brake suddenly within the next few seconds. Our system utilizes data from multiple sensors including radar, lidar, cameras, GPS, and accelerometer, which provide detailed information about surrounding vehicles, road conditions, weather patterns, and driving behavior. We evaluate our model against publicly available datasets, demonstrating high accuracy (96%) and low latency (less than 200ms). Additionally, we discuss the design considerations involved in deploying such systems in real-world scenarios, emphasizing safety and security aspects while ensuring minimal impact on driver experience. Our work has significant implications in advancing autonomous driving technology by enabling vehicles to respond quickly to unexpected events on roads.",1
"Domestic violence (DV) is a global social and public health issue that is highly gendered. Being able to accurately predict DV recidivism, i.e., re-offending of a previously convicted offender, can speed up and improve risk assessment procedures for police and front-line agencies, better protect victims of DV, and potentially prevent future re-occurrences of DV. Previous work in DV recidivism has employed different classification techniques, including decision tree (DT) induction and logistic regression, where the main focus was on achieving high prediction accuracy. As a result, even the diagrams of trained DTs were often too difficult to interpret due to their size and complexity, making decision-making challenging. Given there is often a trade-off between model accuracy and interpretability, in this work our aim is to employ DT induction to obtain both interpretable trees as well as high prediction accuracy. Specifically, we implement and evaluate different approaches to deal with class imbalance as well as feature selection. Compared to previous work in DV recidivism prediction that employed logistic regression, our approach can achieve comparable area under the ROC curve results by using only 3 of 11 available features and generating understandable decision trees that contain only 4 leaf nodes.",0
"The paper ""A Decision Tree Approach to Predicting Recidivism in Domestic Violence"" presents a novel approach using decision trees to predict recidivism in domestic violence cases. This approach utilizes data from criminal history records and court case outcomes to identify patterns that can be used to make predictions about future behavior. By examining factors such as age, gender, previous offenses, and other demographic variables, researchers were able to develop a model with high accuracy rates. Overall, the results indicate that decision tree algorithms could be effective tools for identifying individuals at risk for repeated domestic violence incidents and inform intervention strategies aimed at prevention. Further validation studies are recommended to confirm these findings and refine the algorithm before implementation into practice.",1
"Reliable diagnosis of depressive disorder is essential for both optimal treatment and prevention of fatal outcomes. In this study, we aimed to elucidate the effectiveness of two non-linear measures, Higuchi Fractal Dimension (HFD) and Sample Entropy (SampEn), in detecting depressive disorders when applied on EEG. HFD and SampEn of EEG signals were used as features for seven machine learning algorithms including Multilayer Perceptron, Logistic Regression, Support Vector Machines with the linear and polynomial kernel, Decision Tree, Random Forest, and Naive Bayes classifier, discriminating EEG between healthy control subjects and patients diagnosed with depression. We confirmed earlier observations that both non-linear measures can discriminate EEG signals of patients from healthy control subjects. The results suggest that good classification is possible even with a small number of principal components. Average accuracy among classifiers ranged from 90.24% to 97.56%. Among the two measures, SampEn had better performance. Using HFD and SampEn and a variety of machine learning techniques we can accurately discriminate patients diagnosed with depression vs controls which can serve as a highly sensitive, clinically relevant marker for the diagnosis of depressive disorders.",0
"In recent years, there has been growing interest in using machine learning algorithms for the analysis of electroencephalography (EEG) signals to detect neurological disorders such as depression. This study aimed to investigate the use of two novel feature extraction techniques, namely Higuchi fractal dimension and Sample Entropy, combined with machine learning models for the classification of EEG data obtained from patients diagnosed with depression. The first section of the paper provides background on the current state of research into EEG signal processing and the challenges involved in accurately classifying EEG patterns associated with mental health conditions. Next, the authors present their methodology, which involves extracting the proposed features from raw EEG recordings, selecting suitable machine learning algorithms, training the models using cross-validation methods, and evaluating performance metrics such as accuracy, precision, recall, F1 score, and ROC curve. Results showed that the proposed combination of Higuchi fractal dimension and Sample Entropy achieved high levels of accuracy in identifying EEG patterns characteristic of depression across different frequency bands. These findings support the potential of EEG signal processing and machine learning as powerful tools for understanding brain activity related to mental health conditions and could lead to improved diagnostic capabilities for treating psychiatric disorders. Finally, the conclusion summarizes the main findings of the study and suggests future directions for further research. Overall, this work presents promising results and contributes valuable insights towards developing effective computational approaches for detecting and monitoring mental illness.",1
"We address the problem of finding influential training samples for a particular case of tree ensemble-based models, e.g., Random Forest (RF) or Gradient Boosted Decision Trees (GBDT). A natural way of formalizing this problem is studying how the model's predictions change upon leave-one-out retraining, leaving out each individual training sample. Recent work has shown that, for parametric models, this analysis can be conducted in a computationally efficient way. We propose several ways of extending this framework to non-parametric GBDT ensembles under the assumption that tree structures remain fixed. Furthermore, we introduce a general scheme of obtaining further approximations to our method that balance the trade-off between performance and computational complexity. We evaluate our approaches on various experimental setups and use-case scenarios and demonstrate both the quality of our approach to finding influential training samples in comparison to the baselines and its computational efficiency.",0
"Title: ""Identifying Significant Data Points in Gradient Boosting""  This research investigates the problem of selecting influential training samples for gradient boosted decision trees (GBDT). GBDT algorithms learn a sequence of weak models that together form a strong model by making adjustments based on errors made at previous stages. Identifying significant data points during the learning process can improve the accuracy and efficiency of GBDT algorithms. This study proposes two methods for identifying influential training samples using different criteria: mean decrease impurity (MDI) and random forest feature importance (RFFI). Our experiments demonstrate that both MDI and RFFI perform well across diverse datasets, achieving higher prediction accuracies compared to baseline approaches without sample selection. We conclude that these methods are effective tools for optimizing the performance of GBDT algorithms and have promising applications in fields such as finance, healthcare, and science.",1
"In bankruptcy prediction, the proportion of events is very low, which is often oversampled to eliminate this bias. In this paper, we study the influence of the event rate on discrimination abilities of bankruptcy prediction models. First the statistical association and significance of public records and firmographics indicators with the bankruptcy were explored. Then the event rate was oversampled from 0.12% to 10%, 20%, 30%, 40%, and 50%, respectively. Seven models were developed, including Logistic Regression, Decision Tree, Random Forest, Gradient Boosting, Support Vector Machine, Bayesian Network, and Neural Network. Under different event rates, models were comprehensively evaluated and compared based on Kolmogorov-Smirnov Statistic, accuracy, F1 score, Type I error, Type II error, and ROC curve on the hold-out dataset with their best probability cut-offs. Results show that Bayesian Network is the most insensitive to the event rate, while Support Vector Machine is the most sensitive.",0
"This paper examines how the event rate can impact the discrimination abilities of bankruptcy prediction models. We analyze the relationship between the event rate and model performance by testing different variations of these models using real-world data from a financial institution. Our results indicate that there is a nonlinear correlation between the event rate and model accuracy, meaning that higher event rates generally lead to better discrimination but only up to a certain point. Additionally, we find that some of the most common variables used in bankruptcy prediction models may have limited influence at high event rates, while other, less studied variables become more important factors. These findings provide insights into how lenders and policymakers could potentially improve their credit risk assessment strategies. Overall, our research suggests that careful consideration must be given to both the characteristics of the sample population and the event rate when choosing which type of model to use and interpreting predictive scores in order to ensure optimal decision making.",1
"We introduce an online outlier detection algorithm to detect outliers in a sequentially observed data stream. For this purpose, we use a two-stage filtering and hedging approach. In the first stage, we construct a multi-modal probability density function to model the normal samples. In the second stage, given a new observation, we label it as an anomaly if the value of aforementioned density function is below a specified threshold at the newly observed point. In order to construct our multi-modal density function, we use an incremental decision tree to construct a set of subspaces of the observation space. We train a single component density function of the exponential family using the observations, which fall inside each subspace represented on the tree. These single component density functions are then adaptively combined to produce our multi-modal density function, which is shown to achieve the performance of the best convex combination of the density functions defined on the subspaces. As we observe more samples, our tree grows and produces more subspaces. As a result, our modeling power increases in time, while mitigating overfitting issues. In order to choose our threshold level to label the observations, we use an adaptive thresholding scheme. We show that our adaptive threshold level achieves the performance of the optimal pre-fixed threshold level, which knows the observation labels in hindsight. Our algorithm provides significant performance improvements over the state of the art in our wide set of experiments involving both synthetic as well as real data.",0
"This paper presents an approach to sequential outlier detection using incremental decision trees. By constructing a model that grows as new data arrives in real time, our method can quickly adapt to changing patterns without sacrificing accuracy. We show through extensive experiments on both synthetic datasets and real world applications such as network anomaly detection and credit card fraud detection. Our results demonstrate that our method achieves state-of-the-art performance while maintaining scalability. The problem of detecting outliers in sequence data has gained significant attention due to the widespread adoption of streaming technologies which generate large amounts of continuous data streams. In order to process these streams efficiently, algorithms must operate online in near-real-time and make use of limited computational resources. Existing solutions to this problem typically rely on sliding window techniques or fixed thresholds to identify outliers, but suffer from either high false positive rates or lack of timeliness. In this work, we propose a novel framework called Incremental Randomized Decision Forest (IRDF) that addresses some of these limitations by integrating randomization into the training procedure of decision forest models. IRDF takes advantage of recent advances in minibatch gradient boosting machines and enables efficient construction of decision trees incrementally over streaming data sequences. With a simple voting mechanism, IRDF aggregates base classifiers built at each iteration and makes final predictions according to their confidence scores. We evaluate the performance of IRDF under several settings of both real world and synthetic benchmarks and compare them against seven popular baseline methods in terms of different metrics including accuracy rate, precision/recall, FROC curves, mean average precision(MAP), and area under ROC curve (AUC). Experiments reveal t",1
"The rise of Online Social Networks (OSNs) has caused an insurmountable amount of interest from advertisers and researchers seeking to monopolize on its features. Researchers aim to develop strategies for determining how information is propagated among users within an OSN that is captured by diffusion or influence models. We consider the influence models for the IM-RO problem, a novel formulation to the Influence Maximization (IM) problem based on implementing Stochastic Dynamic Programming (SDP). In contrast to existing approaches involving influence spread and the theory of submodular functions, the SDP method focuses on optimizing clicks and ultimately revenue to advertisers in OSNs. Existing approaches to influence maximization have been actively researched over the past decade, with applications to multiple fields, however, our approach is a more practical variant to the original IM problem. In this paper, we provide an analysis on the influence models of the IM-RO problem by conducting experiments on synthetic and real-world datasets. We propose a Bayesian and Machine Learning approach for estimating the parameters of the influence models for the (Influence Maximization- Revenue Optimization) IM-RO problem. We present a Bayesian hierarchical model and implement the well-known Naive Bayes classifier (NBC), Decision Trees classifier (DTC) and Random Forest classifier (RFC) on three real-world datasets. Compared to previous approaches to estimating influence model parameters, our strategy has the great advantage of being directly implementable in standard software packages such as WinBUGS/OpenBUGS/JAGS and Apache Spark. We demonstrate the efficiency and usability of our methods in terms of spreading information and generating revenue for advertisers in the context of OSNs.",0
"This paper presents a novel approach to estimating influence model parameters using a combination of machine learning techniques and Bayesian inference. We demonstrate how these methods can accurately capture complex relationships within the data by leveraging existing social network theories. Our methodology combines both global and local models, allowing us to provide estimates that balance between simplicity and flexibility while capturing important features from the underlying distribution. Additionally, we introduce a new prior specification based on intuitions drawn from psychological research into human decision making under uncertainty, which results in improved estimation accuracy compared to previous work in this area. Using simulation studies, we showcase the effectiveness of our methodology across different scenarios, including misspecified models and sparse networks. Finally, we apply our approach to real-world datasets to illustrate its practical utility for analyzing social dynamics and understanding complex systems. Overall, our method represents a significant step forward in the field of computational social science, providing researchers with a powerful tool for studying influence processes in large-scale networks.",1
"Gradient tree boosting is a prediction algorithm that sequentially produces a model in the form of linear combinations of decision trees, by solving an infinite-dimensional optimization problem. We combine gradient boosting and Nesterov's accelerated descent to design a new algorithm, which we call AGB (for Accelerated Gradient Boosting). Substantial numerical evidence is provided on both synthetic and real-life data sets to assess the excellent performance of the method in a large variety of prediction problems. It is empirically shown that AGB is much less sensitive to the shrinkage parameter and outputs predictors that are considerably more sparse in the number of trees, while retaining the exceptional performance of gradient boosting.",0
"This paper presents a new machine learning algorithm called ""Accelerated Gradient Boosting."" Our method improves upon traditional gradient boosting by using a novel acceleration technique that significantly speeds up training time while maintaining high accuracy on a variety of tasks. We empirically demonstrate the effectiveness of our approach compared to other state-of-the-art methods through extensive experiments on real world datasets. Our contributions include: (1) a detailed description of the accelerated gradient boosting algorithm; (2) comprehensive experimental results showing the improvements obtained over existing techniques; and (3) analysis of strengths and limitations of the proposed model. Overall, we believe Accelerated Gradient Boosting represents a valuable addition to the field of machine learning and has exciting potential applications across many domains.",1
"We introduce a novel incremental decision tree learning algorithm, Hoeffding Anytime Tree, that is statistically more efficient than the current state-of-the-art, Hoeffding Tree. We demonstrate that an implementation of Hoeffding Anytime Tree---""Extremely Fast Decision Tree"", a minor modification to the MOA implementation of Hoeffding Tree---obtains significantly superior prequential accuracy on most of the largest classification datasets from the UCI repository. Hoeffding Anytime Tree produces the asymptotic batch tree in the limit, is naturally resilient to concept drift, and can be used as a higher accuracy replacement for Hoeffding Tree in most scenarios, at a small additional computational cost.",0
This paper presents a new algorithm for constructing decision trees that prioritizes speed over accuracy. The proposed algorithm uses random sampling techniques to reduce computation time while maintaining good predictive performance on most datasets. Experiments on several benchmark data sets demonstrate that our approach can yield decision trees as fast as state-of-the-art linear models while achieving competitive prediction accuracy. Our results suggest that extremely fast decision tree algorithms have the potential to make machine learning more accessible to practitioners who value rapid model development over perfect prediction quality.,1
"A key phase in the bridge design process is the selection of the structural system. Due to budget and time constraints, engineers typically rely on engineering judgment and prior experience when selecting a structural system, often considering a limited range of design alternatives. The objective of this study was to explore the suitability of supervised machine learning as a preliminary design aid that provides guidance to engineers with regards to the statistically optimal bridge type to choose, ultimately improving the likelihood of optimized design, design standardization, and reduced maintenance costs. In order to devise this supervised learning system, data for over 600,000 bridges from the National Bridge Inventory database were analyzed. Key attributes for determining the bridge structure type were identified through three feature selection techniques. Potentially useful attributes like seismic intensity and historic data on the cost of materials (steel and concrete) were then added from the US Geological Survey (USGS) database and Engineering News Record. Decision tree, Bayes network and Support Vector Machines were used for predicting the bridge design type. Due to state-to-state variations in material availability, material costs, and design codes, supervised learning models based on the complete data set did not yield favorable results. Supervised learning models were then trained and tested using 10-fold cross validation on data for each state. Inclusion of seismic data improved the model performance noticeably. The data was then resampled to reduce the bias of the models towards more common design types, and the supervised learning models thus constructed showed further improvements in performance. The average recall and precision for the state models was 88.6% and 88.0% using Decision Trees, 84.0% and 83.7% using Bayesian Networks, and 80.8% and 75.6% using SVM.",0
"Include keywords: bridge type, classification, supervised learning, natural bridges, NBI dataset, feature extraction  Natural bridges have been traditionally cataloged using visual inspection methods that can suffer from subjectivity due to human interpretation variability. An automated method employing image processing techniques would greatly benefit efforts aiming at standardizing their description and understanding, ultimately leading to better preservation policies. This work explores how deep neural network architecture pretraining based on transfer learning improves upon existing computer vision approaches towards natural bridge detection via aerial imagery analysis. Using a modified version of the National Bridge Inventory (NBI) dataset as ground truth, we performed experiments analyzing different feature extraction techniques, specifically focusing on their impact on two main objectives: accurate image segmentation into road features, i.e., bridges vs non-bridges, and intra class similarity computation (incl. shape descriptors). Our results showcased better accuracy levels compared against traditional handcrafted feature selection while revealing challenges associated with highways interchanges and visually similar man-made structures such as tunnels/viaducts. We envision further improvements by expanding our approach’s applicability scope through online data augmentation exploiting user feedback and other related datasets available in the literature. Furthermore, open-source tools are made publicly available allowing for easy adaptation and replication of these findings.",1
"Both neural networks and decision trees are popular machine learning methods and are widely used to solve problems from diverse domains. These two classifiers are commonly used base classifiers in an ensemble framework. In this paper, we first present a new variant of oblique decision tree based on a linear classifier, then construct an ensemble classifier based on the fusion of a fast neural network, random vector functional link network and oblique decision trees. Random Vector Functional Link Network has an elegant closed form solution with extremely short training time. The neural network partitions each training bag (obtained using bagging) at the root level into C subsets where C is the number of classes in the dataset and subsequently, C oblique decision trees are trained on such partitions. The proposed method provides a rich insight into the data by grouping the confusing or hard to classify samples for each class and thus, provides an opportunity to employ fine-grained classification rule over the data. The performance of the ensemble classifier is evaluated on several multi-class datasets where it demonstrates a superior performance compared to other state-of- the-art classifiers.",0
"This research paper presents a novel method for improving multi-class classification accuracy of random forest algorithms by integrating them with random vector functional neural networks (RVFNNs) and oblique decision surfaces. The proposed approach addresses several limitations of traditional random forest classifiers, such as high dimensionality and low separability among classes, which can lead to poor performance on complex datasets. By incorporating RVFNNs into the random forest algorithm, we can learn more meaningful features from the data, increasing the ability of the model to discriminate between different classes. Additionally, the use of oblique decision surfaces allows us to capture nonlinear relationships within the data that would otherwise go unnoticed. Experimental results on benchmark datasets demonstrate the superiority of our hybrid approach over state-of-the-art multi-class classification methods. Our work has important implications for applications in areas such as computer vision, natural language processing, and bioinformatics, where accurate multi-label prediction is crucial.",1
"Machine learning has been used in all kinds of fields. In this article, we introduce how machine learning can be applied into time series problem. Especially, we use the airline ticket prediction problem as our specific problem. Airline companies use many different variables to determine the flight ticket prices: indicator whether the travel is during the holidays, the number of free seats in the plane etc. Some of the variables are observed, but some of them are hidden. Based on the data over a 103 day period, we trained our models, getting the best model - which is AdaBoost-Decision Tree Classification. This algorithm has best performance over the observed 8 routes which has 61.35$\%$ better performance than the random purchase strategy, and relatively small variance over these routes. And we also considered the situation that we cannot get too much historical datas for some routes (for example the route is new and does not have historical data) or we do not want to train historical data to predict to buy or wait quickly, in which problem, we used HMM Sequence Classification based AdaBoost-Decision Tree Classification to perform our prediction on 12 new routes. Finally, we got 31.71$\%$ better performance than the random purchase strategy.",0
"Abstract: In recent years, machine learning has emerged as a powerful tool for solving complex problems across various domains including finance, healthcare, transportation, among others. Time Series Analysis (TSA) is one such area where machine learning algorithms have shown significant promise due to their ability to effectively handle nonlinear dependencies present in time series data. This paper focuses on building a machine learning model capable of predicting future values of a time series dataset, specifically flight ticket prices over a period of several months. We begin by preprocessing the dataset, handling missing values, and transforming the data into features that can be used by our models. Next, we explore a variety of machine learning techniques such as ARIMA, SARIMAX, Random Forest Regression, Linear Regression, KNN Regressor, Decision Tree Regressor, Gradient Boosting Regressors, and neural networks like LSTM, GRU, and Feedforward Neural Networks. For each technique, we perform feature scaling, train the models using appropriate hyperparameters and evaluate them based on metrics such as Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and R squared score (R2). Our experiments demonstrate that the LSTM architecture outperforms other models achieving a low MAE of 7% while performing well in terms of R2 score greater than 0.99. Finally, we discuss potential use cases and applications of this research in real-world settings, highlighting the value of our approach towards accurate prediction of flight ticket prices.",1
"Understanding heterogeneous multivariate time series data is important in many applications ranging from smart homes to aviation. Learning models of heterogeneous multivariate time series that are also human-interpretable is challenging and not adequately addressed by the existing literature. We propose grammar-based decision trees (GBDTs) and an algorithm for learning them. GBDTs extend decision trees with a grammar framework. Logical expressions derived from a context-free grammar are used for branching in place of simple thresholds on attributes. The added expressivity enables support for a wide range of data types while retaining the interpretability of decision trees. In particular, when a grammar based on temporal logic is used, we show that GBDTs can be used for the interpretable classi cation of high-dimensional and heterogeneous time series data. Furthermore, we show how GBDTs can also be used for categorization, which is a combination of clustering and generating interpretable explanations for each cluster. We apply GBDTs to analyze the classic Australian Sign Language dataset as well as data on near mid-air collisions (NMACs). The NMAC data comes from aircraft simulations used in the development of the next-generation Airborne Collision Avoidance System (ACAS X).",0
"In recent years, time series data has become increasingly heterogeneous due to advances in sensor technology and the growth of the Internet of Things (IoT). This has led to significant challenges in interpreting these datasets and deriving insights from them. In this work, we present a novel approach to categorizing heterogeneous time series data that enables interpretation and understanding of complex patterns and relationships within and across different domains. Our method leverages a combination of statistical analysis and machine learning techniques, allowing us to identify meaningful clusters of time series while taking into account their unique characteristics and relationships with other data sources. Through extensive experiments on synthetic and real-world datasets, we demonstrate the effectiveness of our approach in producing accurate and interpretable results compared to state-of-the-art methods. Additionally, our framework provides an intuitive visual representation of clusterings that facilitates further exploration and explanation of findings. Overall, our research represents a step forward in addressing the interpretability challenge in analyzing large-scale heterogeneous time series datasets, providing valuable insights for applications such as anomaly detection, prediction, and decision support in various fields including finance, healthcare, manufacturing, transportation, and energy management.",1
"Decision trees algorithms use a gain function to select the best split during the tree's induction. This function is crucial to obtain trees with high predictive accuracy. Some gain functions can suffer from a bias when it compares splits of different arities. Quinlan proposed a gain ratio in C4.5's information gain function to fix this bias. In this paper, we present an updated version of the gain ratio that performs better as it tries to fix the gain ratio's bias for unbalanced trees and some splits with low predictive interest.",0
"Abstract: In many applications such as machine learning, data mining, and natural language processing, predictive models play a crucial role in making decisions and predictions. Decision trees are widely used for their simplicity and interpretability. However, they can suffer from overfitting, particularly if the training set contains unbalanced classes, which leads to poor generalization performance on new, unseen data. To address these issues, we propose an algorithm that improves the quality of decision trees by incorporating imbalance constraints into the pruning process using a novel concept called ""information gain ratio"" (IGR). Our IGR method adapts the traditional Gini index to balance split selection based on both accuracy loss reduction and class distribution skewness. We demonstrate through extensive experimental evaluation on several benchmark datasets that our method results in significantly better performance compared to state-of-the-art alternatives in terms of both classification accuracy and model complexity. Our findings have important implications for researchers who build predictive systems in fields where accurate and interpretable predictions are critical for informing real-world decisions.",1
"We give a simple, fast algorithm for hyperparameter optimization inspired by techniques from the analysis of Boolean functions. We focus on the high-dimensional regime where the canonical example is training a neural network with a large number of hyperparameters. The algorithm --- an iterative application of compressed sensing techniques for orthogonal polynomials --- requires only uniform sampling of the hyperparameters and is thus easily parallelizable.   Experiments for training deep neural networks on Cifar-10 show that compared to state-of-the-art tools (e.g., Hyperband and Spearmint), our algorithm finds significantly improved solutions, in some cases better than what is attainable by hand-tuning. In terms of overall running time (i.e., time required to sample various settings of hyperparameters plus additional computation time), we are at least an order of magnitude faster than Hyperband and Bayesian Optimization. We also outperform Random Search 8x.   Additionally, our method comes with provable guarantees and yields the first improvements on the sample complexity of learning decision trees in over two decades. In particular, we obtain the first quasi-polynomial time algorithm for learning noisy decision trees with polynomial sample complexity.",0
"Title: ""A Spectral Approach to Hyperparameter Optimization""  Abstract: Hyperparameters play a crucial role in many machine learning algorithms, as they determine the behavior and performance of these algorithms on specific tasks. Therefore, selecting appropriate hyperparameters can significantly impact the quality of the predictions made by these models. However, finding optimal hyperparameters is often difficult due to the large search spaces involved. This has led researchers to develop heuristics, methods, and techniques that assist in optimizing hyperparameters effectively. In this work, we propose a novel spectral approach to efficiently optimize hyperparameters in popular machine learning algorithms such as support vector machines (SVMs), k-nearest neighbors (kNN) classifiers, and neural networks. Our method exploits the fact that datasets often exhibit low rank structures, which allows us to leverage matrix factorizations to learn robust representations of the data and improve hyperparameter optimization. We evaluate our algorithm against state-of-the-art methods using diverse benchmark datasets across several task domains, including image classification, text classification, regression, etc. Experimental results show that our proposed method outperforms existing approaches significantly, both in terms of accuracy and computational efficiency. Overall, our framework provides a scalable solution to the challenging problem of hyperparameter optimization, enabling practitioners to make better use of their time and resources while achieving improved predictive performances.",1
"Genomics has revolutionized biology, enabling the interrogation of whole transcriptomes, genome-wide binding sites for proteins, and many other molecular processes. However, individual genomic assays measure elements that interact in vivo as components of larger molecular machines. Understanding how these high-order interactions drive gene expression presents a substantial statistical challenge. Building on Random Forests (RF), Random Intersection Trees (RITs), and through extensive, biologically inspired simulations, we developed the iterative Random Forest algorithm (iRF). iRF trains a feature-weighted ensemble of decision trees to detect stable, high-order interactions with same order of computational cost as RF. We demonstrate the utility of iRF for high-order interaction discovery in two prediction problems: enhancer activity in the early Drosophila embryo and alternative splicing of primary transcripts in human derived cell lines. In Drosophila, among the 20 pairwise transcription factor interactions iRF identifies as stable (returned in more than half of bootstrap replicates), 80% have been previously reported as physical interactions. Moreover, novel third-order interactions, e.g. between Zelda (Zld), Giant (Gt), and Twist (Twi), suggest high-order relationships that are candidates for follow-up experiments. In human-derived cells, iRF re-discovered a central role of H3K36me3 in chromatin-mediated splicing regulation, and identified novel 5th and 6th order interactions, indicative of multi-valent nucleosomes with specific roles in splicing regulation. By decoupling the order of interactions from the computational cost of identification, iRF opens new avenues of inquiry into the molecular mechanisms underlying genome biology.",0
"In recent years, methods such as random forests have proven powerful tools for identifying high-dimensional relationships between genetic variants and phenotypes. However, most implementations rely on independence assumptions that may not hold true in complex biological systems. Moreover, computational complexity increases rapidly with model order, which limits our ability to capture higher-degree associations. We propose iterative random forests (iRFs) - an extension of traditional random forests that explicitly models dependencies and adaptively selects the optimal degree based on cross-validation accuracy. iRFs exhibit enhanced performance over several state-of-the-art alternatives while avoiding issues with unstable estimation resulting from large ensembles and/or shallow trees. We validate their efficacy using both simulated data sets and three well-studied human populations: the Electronic Health Records and Genomics Network (eMERGE), the National Heart, Lung, and Blood Institute’s Exome Sequencing Project (ESP), and the Resilience Project at Harvard University. Our results demonstrate that iRFs can robustly identify meaningful interactions even under challenging conditions and provide insights into mechanisms underlying common diseases.",1
"The potential lack of fairness in the outputs of machine learning algorithms has recently gained attention both within the research community as well as in society more broadly. Surprisingly, there is no prior work developing tree-induction algorithms for building fair decision trees or fair random forests. These methods have widespread popularity as they are one of the few to be simultaneously interpretable, non-linear, and easy-to-use. In this paper we develop, to our knowledge, the first technique for the induction of fair decision trees. We show that our ""Fair Forest"" retains the benefits of the tree-based approach, while providing both greater accuracy and fairness than other alternatives, for both ""group fairness"" and ""individual fairness.'"" We also introduce new measures for fairness which are able to handle multinomial and continues attributes as well as regression problems, as opposed to binary attributes and labels only. Finally, we demonstrate a new, more robust evaluation procedure for algorithms that considers the dataset in its entirety rather than only a specific protected attribute.",0
"""Fairness has become an increasingly important consideration in machine learning applications, as models can perpetuate harmful biases if not properly designed and trained. One approach to mitigating bias in decision making systems is through regularization techniques that constrain model behavior, promoting more interpretable and reliable predictions. In this work, we focus on tree induction methods, which are commonly used for predictive modelling but can suffer from overfitting and high variance. We propose a novel regularization method based on fairness criteria that directly penalizes trees for exhibiting unwanted behaviors such as discrimination or instability. Our method effectively minimizes these issues while still producing accurate models, resulting in reduced bias and improved interpretability.""  This paper presents a new technique for inducing decision trees that explicitly considers fairness criteria during model training. Existing approaches have primarily focused on post-hoc correction mechanisms or indirect regularization methods that may not adequately address unfair behavior. To address this gap, we introduce a regularizer that directly penalizes unwanted behaviors in the induced trees, including discrimination and instability. This allows our method to produce trees that adhere to ethical guidelines and promote equitable outcomes. Empirically, we demonstrate that our method achieves comparable accuracy to state-of-the-art baselines while significantly reducing bias and improving stability. Overall, our work provides an effective and interpretable framework for generating fair decision trees that balances prediction performance with societal considerations.",1
"Customer retention campaigns increasingly rely on predictive models to detect potential churners in a vast customer base. From the perspective of machine learning, the task of predicting customer churn can be presented as a binary classification problem. Using data on historic behavior, classification algorithms are built with the purpose of accurately predicting the probability of a customer defecting. The predictive churn models are then commonly selected based on accuracy related performance measures such as the area under the ROC curve (AUC). However, these models are often not well aligned with the core business requirement of profit maximization, in the sense that, the models fail to take into account not only misclassification costs, but also the benefits originating from a correct classification. Therefore, the aim is to construct churn prediction models that are profitable and preferably interpretable too. The recently developed expected maximum profit measure for customer churn (EMPC) has been proposed in order to select the most profitable churn model. We present a new classifier that integrates the EMPC metric directly into the model construction. Our technique, called ProfTree, uses an evolutionary algorithm for learning profit driven decision trees. In a benchmark study with real-life data sets from various telecommunication service providers, we show that ProfTree achieves significant profit improvements compared to classic accuracy driven tree-based methods.",0
"Title: ""Predicting Customer Churn using Decision Tree Models""  Churn prediction has become increasingly important for businesses as they seek to retain customers and improve their profitability. In recent years, machine learning algorithms have been used to develop models that can accurately predict customer churn behavior. One such algorithm is decision tree modeling, which creates a hierarchy of decisions based on features extracted from historical data. However, existing approaches often suffer from limitations, such as overfitting due to complex models or lack of interpretability due to black box methods.  To address these challenges, we propose the use of profit driven decision trees (PDDT) for churn prediction. PDDT incorporates both economic and statistical considerations into the model building process by focusing on maximizing expected profits rather than simply minimizing error rates. This approach allows us to create more interpretable and actionable models that better align with business objectives. We evaluate our methodology on two real-world datasets and compare its performance against state-of-the-art churn prediction techniques. Our results show that PDDT outperforms other methods in terms of accuracy and feature importance rankings while providing more insights into customer behavior. Our findings suggest that PDDT can serve as a valuable tool for businesses looking to reduce customer churn and enhance profitability.",1
"Conventional decision trees have a number of favorable properties, including interpretability, a small computational footprint and the ability to learn from little training data. However, they lack a key quality that has helped fuel the deep learning revolution: that of being end-to-end trainable, and to learn from scratch those features that best allow to solve a given supervised learning problem. Recent work (Kontschieder 2015) has addressed this deficit, but at the cost of losing a main attractive trait of decision trees: the fact that each sample is routed along a small subset of tree nodes only. We here propose a model and Expectation-Maximization training scheme for decision trees that are fully probabilistic at train time, but after a deterministic annealing process become deterministic at test time. We also analyze the learned oblique split parameters on image datasets and show that Neural Networks can be trained at each split node. In summary, we present the first end-to-end learning scheme for deterministic decision trees and present results on par with or superior to published standard oblique decision tree algorithms.",0
"This work presents an approach to learning decision trees that are both interpretable and efficient, using an end-to-end deep neural network architecture. By leveraging recent advances in differentiable programming and tree generation techniques, we propose a novel method that generates deterministic decision trees as output probabilities from a deep neural network. Our experiments demonstrate the effectiveness of our approach across a variety of real-world datasets, achieving competitive accuracy while providing human-readable rules that can aid interpretation and debugging. We further showcase the efficiency of our learned decision trees by evaluating their runtime on high-dimensional inputs. Finally, we discuss potential applications of our method in fields such as healthcare and finance where interpretability is critical. Overall, our research makes a valuable contribution to the field of machine learning by bridging the gap between accurate models and explainable ones.",1
"Kernel methods are popular in clustering due to their generality and discriminating power. However, we show that many kernel clustering criteria have density biases theoretically explaining some practically significant artifacts empirically observed in the past. For example, we provide conditions and formally prove the density mode isolation bias in kernel K-means for a common class of kernels. We call it Breiman's bias due to its similarity to the histogram mode isolation previously discovered by Breiman in decision tree learning with Gini impurity. We also extend our analysis to other popular kernel clustering methods, e.g. average/normalized cut or dominant sets, where density biases can take different forms. For example, splitting isolated points by cut-based criteria is essentially the sparsest subset bias, which is the opposite of the density mode bias. Our findings suggest that a principled solution for density biases in kernel clustering should directly address data inhomogeneity. We show that density equalization can be implicitly achieved using either locally adaptive weights or locally adaptive kernels. Moreover, density equalization makes many popular kernel clustering objectives equivalent. Our synthetic and real data experiments illustrate density biases and proposed solutions. We anticipate that theoretical understanding of kernel clustering limitations and their principled solutions will be important for a broad spectrum of data analysis applications across the disciplines.",0
"Title: Unbiased Density Estimation for Clustering Analysis  Kernel density estimation (KDE) plays a crucial role in many applications that rely on data analysis, such as data mining, machine learning, image processing, and scientific computing. In particular, clustering algorithms based on kernel densities have proven effective for identifying patterns in high-dimensional datasets. However, these methods suffer from two main drawbacks: the choice of bandwidth parameter remains arbitrary and sensitive to the scale of the features; and density estimates tend to overestimate probabilities in regions where data is scarce. To address these issues, we propose a new method called Adaptive Minimum Covariance Determinant (AMCD), which adaptively tunes the smoothing parameters during clustering without relying on prior knowledge of the underlying distribution or using ad hoc threshold values. AMCD incorporates properties of both minimum volume and maximum likelihood estimators into a unified framework, providing a principled approach to selecting optimal bandwidths. Experimental results show that our algorithm outperforms state-of-the-art techniques in terms of accuracy and computational efficiency while maintaining robustness under varying conditions. Overall, this work represents a significant step towards reliable and efficient clustering using unbiased density estimation.",1
"The presence of noisy instances in mobile phone data is a fundamental issue for classifying user phone call behavior (i.e., accept, reject, missed and outgoing), with many potential negative consequences. The classification accuracy may decrease and the complexity of the classifiers may increase due to the number of redundant training samples. To detect such noisy instances from a training dataset, researchers use naive Bayes classifier (NBC) as it identifies misclassified instances by taking into account independence assumption and conditional probabilities of the attributes. However, some of these misclassified instances might indicate usages behavioral patterns of individual mobile phone users. Existing naive Bayes classifier based noise detection techniques have not considered this issue and, thus, are lacking in classification accuracy. In this paper, we propose an improved noise detection technique based on naive Bayes classifier for effectively classifying users' phone call behaviors. In order to improve the classification accuracy, we effectively identify noisy instances from the training dataset by analyzing the behavioral patterns of individuals. We dynamically determine a noise threshold according to individual's unique behavioral patterns by using both the naive Bayes classifier and Laplace estimator. We use this noise threshold to identify noisy instances. To measure the effectiveness of our technique in classifying user phone call behavior, we employ the most popular classification algorithm (e.g., decision tree). Experimental results on the real phone call log dataset show that our proposed technique more accurately identifies the noisy instances from the training datasets that leads to better classification accuracy.",0
"""User phone call behavior classification can provide valuable insights into user preferences, communication patterns, and other significant characteristics that are relevant to businesses, researchers, and service providers alike. However, noisy data can impede accurate analysis by introducing distortions and inconsistencies into the dataset. This paper presents an improved noise detection technique based on a naive bayes classifier (NBD) designed specifically for user phone call behavior datasets. The NBD model utilizes various features extracted from cellphone metadata, including duration, time, day, and frequency of calls. Our proposed methodology involves preprocessing steps that filter out invalid records, impute missing values using appropriate techniques such as mean substitution, and normalize feature variables. We evaluated our approach using three publicly available datasets: EMC, OEMC, and CTU-14, which have different sizes and characteristics. Experimental results demonstrate that our method significantly reduces noise levels while maintaining high accuracy in classification performance compared to state-of-the-art noise reduction approaches.""",1
"Deep neural networks have proved to be a very effective way to perform classification tasks. They excel when the input data is high dimensional, the relationship between the input and the output is complicated, and the number of labeled training examples is large. But it is hard to explain why a learned network makes a particular classification decision on a particular test case. This is due to their reliance on distributed hierarchical representations. If we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead, explaining a particular decision would be much easier. We describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data.",0
"An introduction to a new method for distillation of neural networks into soft decision trees is presented. This approach enables efficient visualization of complex neural network structures by transforming them into tree form, allowing for easy interpretation even for non-experts. The technique employs regularized logistic regression on the output from final convolution layers in the network using only the most important features. Empirical evaluation shows that our method achieves significantly better performance than other state-of-the-art methods while maintaining interpretability. Our work demonstrates potential applications of soft decision trees in the areas of explainability and model compression. Additionally, we provide open source code for replication and extension of these results. Abstract: In this paper, a novel approach is introduced for the distillation of neural networks into soft decision trees. Traditional decision trees can lack interpretable power due to their hard splits and threshold functions, while modern deep learning models often suffer from ""black box"" opacity. To address this challenge, the authors propose a hybrid representation combining the simplicity and transparency of decision trees with the expressiveness of powerful neural architectures. The resulting soft decision trees faithfully preserve crucial aspects of the original neural network structure and allow for clear explanation and visualization of complex relationships. The effectiveness of this method was verified through extensive empirical evaluations, which demonstrated significant improvement over existing techniques. Potential applications of soft decision trees extend beyond simple explanations as they could find use cases in compressions tasks of large models. Lastly, the project provides accessibility to the research community via open source availability. Overall, this paper presents an innovative means to bridge the gap between transparent interpretability and high task accuracy, paving the way towards more responsible adoption of advanced machine learning systems.",1
"Sleep stage classification constitutes an important preliminary exam in the diagnosis of sleep disorders. It is traditionally performed by a sleep expert who assigns to each 30s of signal a sleep stage, based on the visual inspection of signals such as electroencephalograms (EEG), electrooculograms (EOG), electrocardiograms (ECG) and electromyograms (EMG). We introduce here the first deep learning approach for sleep stage classification that learns end-to-end without computing spectrograms or extracting hand-crafted features, that exploits all multivariate and multimodal Polysomnography (PSG) signals (EEG, EMG and EOG), and that can exploit the temporal context of each 30s window of data. For each modality the first layer learns linear spatial filters that exploit the array of sensors to increase the signal-to-noise ratio, and the last layer feeds the learnt representation to a softmax classifier. Our model is compared to alternative automatic approaches based on convolutional networks or decisions trees. Results obtained on 61 publicly available PSG records with up to 20 EEG channels demonstrate that our network architecture yields state-of-the-art performance. Our study reveals a number of insights on the spatio-temporal distribution of the signal of interest: a good trade-off for optimal classification performance measured with balanced accuracy is to use 6 EEG with 2 EOG (left and right) and 3 EMG chin channels. Also exploiting one minute of data before and after each data segment offers the strongest improvement when a limited number of channels is available. As sleep experts, our system exploits the multivariate and multimodal nature of PSG signals in order to deliver state-of-the-art classification performance with a small computational cost.",0
"This paper presents a novel deep learning architecture for temporal sleep stage classification that leverages both uni-modal and multi-modal input sources and processes them through multiple streams of varying complexity to capture different levels of representation from each modality. By combining these representations into a single model, we achieve superior performance over existing state-of-the art methods on two publicly available datasets. Our approach employs dilated convolutions and strided residual connections to efficiently handle variable length signals across all modalities while reducing computational requirements and memory consumption compared to alternative models. We demonstrate how our architecture can effectively learn interpretable features even when trained solely on smaller subsets of data from certain modalities or stages, making it highly adaptable for use in low resource settings. Finally, ablation studies show that our design choices significantly improve overall performance, enabling accurate temporal sleep stage prediction critical for applications such as sleep medicine diagnosis and monitoring. Overall, our work contributes important advances towards realizing robust and scalable solutions for automatic sleep analysis using machine learning techniques.",1
"Unique micro-Doppler signature ($\boldsymbol{\mu}$-D) of a human body motion can be analyzed as the superposition of different body parts $\boldsymbol{\mu}$-D signatures. Extraction of human limbs $\boldsymbol{\mu}$-D signatures in real-time can be used to detect, classify and track human motion especially for safety application. In this paper, two methods are combined to simulate $\boldsymbol{\mu}$-D signatures of a walking human. Furthermore, a novel limbs $\mu$-D signature time independent decomposition feasibility study is presented based on features as $\mu$-D signatures and range profiles also known as micro-Range ($\mu$-R). Walking human body parts can be divided into four classes (base, arms, legs, feet) and a decision tree classifier is used. Validation is done and the classifier is able to decompose $\mu$-D signatures of limbs from a walking human signature on real-time basis.",0
"This paper presents a method for real-time capable micro-Doppler signature decomposition of walking human limbs. We use an approach based on frequency-modulated continuous wave (FMCW) radar signals. Our method can effectively distinguish between different parts of the body such as legs, arms, and head, providing high accuracy in tracking moving humans. In our experiments, we achieved promising results in both indoor and outdoor environments and demonstrated that our technique could detect subtle movements like arm gestures while ignoring background clutter. With these findings, we showcase the potential of FMCW radar sensors for noncontact monitoring of human activity, enabling new applications in fields ranging from medical care to surveillance systems. By contributing to the growing research field of micro-Doppler signatures, our work aims to foster further advancements toward more robust, reliable, and flexible biomedical applications, including remote heart rate estimation.",1
"Cloud vendors are increasingly offering machine learning services as part of their platform and services portfolios. These services enable the deployment of machine learning models on the cloud that are offered on a pay-per-query basis to application developers and end users. However recent work has shown that the hosted models are susceptible to extraction attacks. Adversaries may launch queries to steal the model and compromise future query payments or privacy of the training data. In this work, we present a cloud-based extraction monitor that can quantify the extraction status of models by observing the query and response streams of both individual and colluding adversarial users. We present a novel technique that uses information gain to measure the model learning rate by users with increasing number of queries. Additionally, we present an alternate technique that maintains intelligent query summaries to measure the learning rate relative to the coverage of the input feature space in the presence of collusion. Both these approaches have low computational overhead and can easily be offered as services to model owners to warn them of possible extraction attacks from adversaries. We present performance results for these approaches for decision tree models deployed on BigML MLaaS platform, using open source datasets and different adversarial attack strategies.",0
"In recent years there has been a significant rise in the use of Machine Learning as a Service (MLaaS) platforms to deploy machine learning models in real world applications due their ability to simplify model development workflows and provide easy deployment solutions. However, these platforms often make use of proprietary techniques that prevent users from extracting trained models, making it difficult for researchers and practitioners alike to analyze or compare different models for performance improvements. In addition, model extraction can expose security vulnerabilities, which could lead to malicious attacks on systems using such models. Therefore, it is important for developers and stakeholders to thoroughly consider all aspects before incorporating MLaaS into their projects. This study examines the impact of model extraction warnings in the MLaaS paradigm by surveying over one thousand respondents across industry and academia. Results showed that while most participants were unaware of model extraction issues, those who had experience working with MLaaS platforms indicated high levels of concern regarding data privacy risks posed by exposure of extracted models. Overall, the results highlight the need for clear communication and transparency surrounding the limitations and potential dangers associated with using MLaaS services. By acknowledging these concerns early on in the design process, developers and organizations can mitigate potential damage caused by poorly secured systems and maintain trust among end users. Ultimately, our findings contribute to broadening discussions on user experiences and expectations regarding emerging technologies like MLaaS and offer valuable insights to guide future research and product innovation.",1
"Additive models, such as produced by gradient boosting, and full interaction models, such as classification and regression trees (CART), are widely used algorithms that have been investigated largely in isolation. We show that these models exist along a spectrum, revealing never-before-known connections between these two approaches. This paper introduces a novel technique called tree-structured boosting for creating a single decision tree, and shows that this method can produce models equivalent to CART or gradient boosted stumps at the extremes by varying a single parameter. Although tree-structured boosting is designed primarily to provide both the model interpretability and predictive performance needed for high-stake applications like medicine, it also can produce decision trees represented by hybrid models between CART and boosted stumps that can outperform either of these approaches.",0
"One of the most popular machine learning models used today is gradient boosting, which works by fitting multiple decision trees sequentially to increase accuracy. However, the interpretability and explainability of these models have been called into question due to their complexity. In order to address these concerns, recent research has focused on tree-structuredboosting methods that use simpler decision structures such as stumps. Despite their simplicity, these methods can still achieve state-of-the-art performance while remaining interpretable and easier to explain. This study examines the connections between two popular tree-structuredboosting techniques: gradient boosted stumps (GBS) and full decision trees. Through experiments and analysis, we show how GBS is a natural extension of traditional decision trees and provide insights into why these methods work well in practice. By shedding light on the relationship between GBS and decision trees, we aim to improve our understanding of tree-structuredboosting and contribute to the broader field of interpretable machine learning.",1
"The lack of interpretability remains a key barrier to the adoption of deep models in many applications. In this work, we explicitly regularize deep models so human users might step through the process behind their predictions in little time. Specifically, we train deep time-series models so their class-probability predictions have high accuracy while being closely modeled by decision trees with few nodes. Using intuitive toy examples as well as medical tasks for treating sepsis and HIV, we demonstrate that this new tree regularization yields models that are easier for humans to simulate than simpler L1 or L2 penalties without sacrificing predictive power.",0
"In recent years, deep learning has emerged as a powerful tool for solving complex problems across multiple domains. However, one of the major challenges that comes with using these models is their lack of interpretability, which makes it difficult to understand how they make decisions and predictions. One approach to addressing this issue is through tree regularization, which involves adding a penalty term to the loss function during training to encourage the model to learn decision trees instead of more complicated structures such as randomized forests or random tangles. In this paper, we extend previous work on tree regularization by proposing a new method called tree Lasso, which combines the advantages of both sparsity and structured pruning techniques into a single framework. Our results show that our proposed method outperforms state-of-the art methods in terms of interpretability while maintaining accuracy, making it a promising solution towards enhancing transparency and trustworthiness in deep learning applications.",1
"This paper proposes a new approach to diagnose broken rotor bar failure in a line start-permanent magnet synchronous motor (LS-PMSM) using random forests. The transient current signal during the motor startup was acquired from a healthy motor and a faulty motor with a broken rotor bar fault. We extracted 13 statistical time domain features from the startup transient current signal, and used these features to train and test a random forest to determine whether the motor was operating under normal or faulty conditions. For feature selection, we used the feature importances from the random forest to reduce the number of features to two features. The results showed that the random forest classifies the motor condition as healthy or faulty with an accuracy of 98.8% using all features and with an accuracy of 98.4% by using only the mean-index and impulsion features. The performance of the random forest was compared with a decision tree, Na\""ive Bayes classifier, logistic regression, linear ridge, and a support vector machine, with the random forest consistently having a higher accuracy than the other algorithms. The proposed approach can be used in industry for online monitoring and fault diagnostic of LS-PMSM motors and the results can be helpful for the establishment of preventive maintenance plans in factories.",0
"This paper presents a method for detecting faults in a type of electric motor known as a Line Start Permanent Magnet Synchronous Motor (LS-PMSM) using machine learning algorithms called random forests. The presence of broken rotor bars can cause significant damage to these motors if left undetected, making it essential to develop reliable methods for monitoring their condition. In order to achieve high detection accuracy, features from different aspects such as stator current analysis and frequency response are extracted and fed into the random forest model. The experimental results demonstrate that the proposed approach achieves good accuracy in identifying the broken rotor bar fault, outperforming other state-of-the-art techniques used in literature. Additionally, the random forest algorithm has low computational requirements, which makes it ideal for online implementation. Overall, this study provides valuable insights into the development of effective fault diagnosis systems for LS-PMSMs.",1
"Gradient boosted decision trees are a popular machine learning technique, in part because of their ability to give good accuracy with small models. We describe two extensions to the standard tree boosting algorithm designed to increase this advantage. The first improvement extends the boosting formalism from scalar-valued trees to vector-valued trees. This allows individual trees to be used as multiclass classifiers, rather than requiring one tree per class, and drastically reduces the model size required for multiclass problems. We also show that some other popular vector-valued gradient boosted trees modifications fit into this formulation and can be easily obtained in our implementation. The second extension, layer-by-layer boosting, takes smaller steps in function space, which is empirically shown to lead to a faster convergence and to a more compact ensemble. We have added both improvements to the open-source TensorFlow Boosted trees (TFBT) package, and we demonstrate their efficacy on a variety of multiclass datasets. We expect these extensions will be of particular interest to boosted tree applications that require small models, such as embedded devices, applications requiring fast inference, or applications desiring more interpretable models.",0
"""Compact Multi-Class Boosted Trees"" presents a novel method for constructing multi-class decision trees that balances competing objectives: maximizing accuracy while minimizing tree size and number of splits. This approach leverages the power of gradient boosting machines (GBMs) to produce smaller, more interpretable models that can achieve state-of-the-art performance on challenging classification tasks. The key insight underlying our technique is that incorporating an L1 regularization term during training encourages sparse predictions at each node of the tree, resulting in fewer leaf nodes overall. To evaluate the effectiveness of our method, we conduct extensive experiments on several benchmark datasets and compare against popular alternatives such as random forest and XGBoost. Our results demonstrate consistent improvements in both predictive accuracy and model simplicity. By striking a balance between these two important metrics, compact multi-class boosted trees offer a powerful new tool for machine learning practitioners and researchers alike.",1
"We present an algorithm for classification tasks on big data. Experiments conducted as part of this study indicate that the algorithm can be as accurate as ensemble methods such as random forests or gradient boosted trees. Unlike ensemble methods, the models produced by the algorithm can be easily interpreted. The algorithm is based on a divide and conquer strategy and consists of two steps. The first step consists of using a decision tree to segment the large dataset. By construction, decision trees attempt to create homogeneous class distributions in their leaf nodes. However, non-homogeneous leaf nodes are usually produced. The second step of the algorithm consists of using a suitable classifier to determine the class labels for the non-homogeneous leaf nodes. The decision tree segment provides a coarse segment profile while the leaf level classifier can provide information about the attributes that affect the label within a segment.",0
"This will be used as an example in Appendix D: ""How To"" Guide on Writing An Abstract for your dissertation or research paper. So please make sure that everything is accurate. Also please mention any limitations or assumptions made in your work. Here is a list of key terms from the main body (in no particular order): big data classification, decision trees, accuracy score, feature importance, augmenting decision trees, ensembles, random forest, gradient boosting machines. Abstract: This study investigates the use of big data classification techniques using augmented decision trees. We evaluated different approaches to building decision tree models, including pruning methods to control overfitting, as well as selecting features based on their impact on model performance using feature importances scores. Our methodology involved training decision trees on large datasets and then evaluating their performance through cross validation using a nested loop approach. We achieved promising results, achieving high accuracy scores compared to traditional regression analysis. However, there were some limitations to our study, such as assuming homoscedasticity and normality within our dataset. Additionally, we only considered a few ensemble techniques, such as random forest and gradient boosting machines, but further exploration could reveal additional insights into improving model stability and robustness. Overall, though, our findings suggest that augmenting decision trees can provide a powerful toolset for enhancing big data classification performance.",1
"Building classification models is an intrinsically practical exercise that requires many design decisions prior to deployment. We aim to provide some guidance in this decision making process. Specifically, given a classification problem with real valued attributes, we consider which classifier or family of classifiers should one use. Strong contenders are tree based homogeneous ensembles, support vector machines or deep neural networks. All three families of model could claim to be state-of-the-art, and yet it is not clear when one is preferable to the others. Our extensive experiments with over 200 data sets from two distinct archives demonstrate that, rather than choose a single family and expend computing resources on optimising that model, it is significantly better to build simpler versions of classifiers from each family and ensemble. We show that the Heterogeneous Ensembles of Standard Classification Algorithms (HESCA), which ensembles based on error estimates formed on the train data, is significantly better (in terms of error, balanced error, negative log likelihood and area under the ROC curve) than its individual components, picking the component that is best on train data, and a support vector machine tuned over 1089 different parameter configurations. We demonstrate HESCA+, which contains a deep neural network, a support vector machine and two decision tree forests, is significantly better than its components, picking the best component, and HESCA. We analyse the results further and find that HESCA and HESCA+ are of particular value when the train set size is relatively small and the problem has multiple classes. HESCA is a fast approach that is, on average, as good as state-of-the-art classifiers, whereas HESCA+ is significantly better than average and represents a strong benchmark for future research.",0
"Title: The Combined Strength of Multiple Machine Learning Models Authors: John Smith et al Affiliations: XYZ Corporation; ABC University Abstract: In recent years, there has been increasing interest in using multiple machine learning models together in order to improve performance on classification tasks. This approach, known as ensembling, has shown promise in many domains. However, most work in this area has focused on combining models within the same family, such as bagging or boosting different versions of decision trees. In this paper, we explore a new technique called The Heterogeneous Ensembles of Standard Classification Algorithms (HESCA), which combines several standard classification algorithms from distinct families. Our results show that these diverse models can complement each other, leading to significant improvements over any single model alone. We demonstrate the effectiveness of our method across four benchmark datasets and evaluate the impact of varying ensemble sizes and randomization strategies. These findings have important implications for practitioners who wish to optimize their own machine learning systems by leveraging the collective strength of disparate models. Keywords: machine learning; classification; ensembling; diversity; heterogeneity Contact Information: [john@xyzcorp.com](mailto:john@xyzcorp.com) ########",1
"Label distribution learning (LDL) is a general learning framework, which assigns to an instance a distribution over a set of labels rather than a single label or multiple labels. Current LDL methods have either restricted assumptions on the expression form of the label distribution or limitations in representation learning, e.g., to learn deep features in an end-to-end manner. This paper presents label distribution learning forests (LDLFs) - a novel label distribution learning algorithm based on differentiable decision trees, which have several advantages: 1) Decision trees have the potential to model any general form of label distributions by a mixture of leaf node predictions. 2) The learning of differentiable decision trees can be combined with representation learning. We define a distribution-based loss function for a forest, enabling all the trees to be learned jointly, and show that an update function for leaf node predictions, which guarantees a strict decrease of the loss function, can be derived by variational bounding. The effectiveness of the proposed LDLFs is verified on several LDL tasks and a computer vision application, showing significant improvements to the state-of-the-art LDL methods.",0
"In recent years, advances in deep learning have led to significant improvements in computer vision tasks such as object detection, segmentation, and classification. However, these models often require large amounts of labeled data to achieve good performance, which can be time-consuming and expensive to obtain. To address this issue, researchers have proposed semi-supervised and unsupervised methods that make use of limited labeled data along with vast quantities of unlabeled data. One popular approach is self-training, where a model is trained on a small set of labeled images and then used to generate additional training data by making predictions on unlabeled images. These predicted labels are then added to the original training set, and the model is retrained using both labeled and pseudo-labeled data. While effective, this approach may suffer from overfitting due to the inclusion of noisy pseudo-labels.  In this work, we propose a new method called ""Label Distribution Learning Forests"" (LDLF) that addresses the limitations of existing self-training approaches. LDLF builds upon recent advancements in decision tree ensembles for image classification, specifically the Gradient Boosting Cascade Ensemble (GBCE) framework. Our key insight is to explicitly model label distribution uncertainty, allowing us to filter out high-uncertainty predictions during self-training while still utilizing all available data. We train multiple GBCE models at different depths, each providing a confidence score that measures how well the corresponding tree splits are able to predict positive class memberships given a specific input. By combining these scores across trees, we derive an overall prediction quality metric that allows filtering low-quality predictions. Our algorithm alternates between self-training and model retraining until convergence, resulting in a forest ensemble that captures discriminative features guided by soft labels iteratively estimated during training. Experimental results show significantly improved accuracy compared to state-of-the-art",1
"Various modifications of decision trees have been extensively used during the past years due to their high efficiency and interpretability. Tree node splitting based on relevant feature selection is a key step of decision tree learning, at the same time being their major shortcoming: the recursive nodes partitioning leads to geometric reduction of data quantity in the leaf nodes, which causes an excessive model complexity and data overfitting. In this paper, we present a novel architecture - a Decision Stream, - aimed to overcome this problem. Instead of building a tree structure during the learning process, we propose merging nodes from different branches based on their similarity that is estimated with two-sample test statistics, which leads to generation of a deep directed acyclic graph of decision rules that can consist of hundreds of levels. To evaluate the proposed solution, we test it on several common machine learning problems - credit scoring, twitter sentiment analysis, aircraft flight control, MNIST and CIFAR image classification, synthetic data classification and regression. Our experimental results reveal that the proposed approach significantly outperforms the standard decision tree learning methods on both regression and classification tasks, yielding a prediction error decrease up to 35%.",0
"This paper presents Decision Stream, a framework that enables data scientists and machine learning engineers to efficiently build accurate deep decision trees. By leveraging recent advances in parallelism and memory efficiency, our system can generate decision trees up to ten times faster than state-of-the-art methods while achieving competitive accuracy. Our approach improves upon existing techniques by explicitly modeling missing values as an important feature in the training process, enabling better handling of imbalanced datasets through custom sampling rates, and using pruning strategies to reduce tree size without sacrificing performance. We demonstrate the effectiveness of Decision Stream on several benchmarks, including regression and classification problems, where we consistently outperform other decision tree algorithms while remaining computationally efficient. With its intuitive interface and scalability, Decision Stream represents a valuable addition to the toolkit of any practitioner working on real-world decision making tasks.",1
"In this paper, we propose a novel random-forest scheme, namely Joint Maximum Purity Forest (JMPF), for classification, clustering, and regression tasks. In the JMPF scheme, the original feature space is transformed into a compactly pre-clustered feature space, via a trained rotation matrix. The rotation matrix is obtained through an iterative quantization process, where the input data belonging to different classes are clustered to the respective vertices of the new feature space with maximum purity. In the new feature space, orthogonal hyperplanes, which are employed at the split-nodes of decision trees in random forests, can tackle the clustering problems effectively. We evaluated our proposed method on public benchmark datasets for regression and classification tasks, and experiments showed that JMPF remarkably outperforms other state-of-the-art random-forest-based approaches. Furthermore, we applied JMPF to image super-resolution, because the transformed, compact features are more discriminative to the clustering-regression scheme. Experiment results on several public benchmark datasets also showed that the JMPF-based image super-resolution scheme is consistently superior to recent state-of-the-art image super-resolution algorithms.",0
"Infer the title from the keywords provided by the author. Keywords: image superresolution, joint maximum purity forest, machine learning, computer vision, deep learning",1
"We propose new methods for Support Vector Machines (SVMs) using tree architecture for multi-class classi- fication. In each node of the tree, we select an appropriate binary classifier using entropy and generalization error estimation, then group the examples into positive and negative classes based on the selected classi- fier and train a new classifier for use in the classification phase. The proposed methods can work in time complexity between O(log2N) to O(N) where N is the number of classes. We compared the performance of our proposed methods to the traditional techniques on the UCI machine learning repository using 10-fold cross-validation. The experimental results show that our proposed methods are very useful for the problems that need fast classification time or problems with a large number of classes as the proposed methods run much faster than the traditional techniques but still provide comparable accuracy.",0
"This paper presents a novel method for constructing decision trees for multi-class support vector machines (SVMs) using entropy and generalization error estimation. The proposed approach first selects a subset of data points with high uncertainty to label, then grows a tree by recursively dividing the remaining unlabeled set into smaller subsets according to their distribution over the feature space. At each step, the algorithm chooses the split that reduces the overall uncertainty of the distribution while minimizing the expected loss on both labeled and unlabeled samples. Experimental results on several benchmark datasets show that the proposed method can achieve better accuracy than state-of-the-art methods while maintaining comparable efficiency. Furthermore, we demonstrate the effectiveness of our approach on real world problems such as image classification and natural language processing tasks. Overall, our work offers new insights into developing efficient decision trees for multi-class SVMs, with potential applications in many domains requiring accurate predictions on large scale datasets.",1
"Usually, decision tree induction algorithms are limited to work with non relational data. Given a record, they do not take into account other objects attributes even though they can provide valuable information for the learning task. In this paper we present GGQ-ID3, a multi-relational decision tree learning algorithm that uses Generalized Graph Queries (GGQ) as predicates in the decision nodes. GGQs allow to express complex patterns (including cycles) and they can be refined step-by-step. Also, they can evaluate structures (not only single records) and perform Regular Pattern Matching. GGQ are built dynamically (pattern mining) during the GGQ-ID3 tree construction process. We will show how to use GGQ-ID3 to perform multi-relational machine learning keeping complexity under control. Finally, some real examples of automatically obtained classification trees and semantic patterns are shown.   -----   Normalmente, los algoritmos de inducci\'on de \'arboles de decisi\'on trabajan con datos no relacionales. Dado un registro, no tienen en cuenta los atributos de otros objetos a pesar de que \'estos pueden proporcionar informaci\'on \'util para la tarea de aprendizaje. En este art\'iculo presentamos GGQ-ID3, un algoritmo de aprendizaje de \'arboles de decisiones multi-relacional que utiliza Generalized Graph Queries (GGQ) como predicados en los nodos de decisi\'on. Los GGQs permiten expresar patrones complejos (incluyendo ciclos) y pueden ser refinados paso a paso. Adem\'as, pueden evaluar estructuras (no solo registros) y llevar a cabo Regular Pattern Matching. En GGQ-ID3, los GGQ son construidos din\'amicamente (pattern mining) durante el proceso de construcci\'on del \'arbol. Adem\'as, se muestran algunos ejemplos reales de \'arboles de clasificaci\'on multi-relacionales y patrones sem\'anticos obtenidos autom\'aticamente.",0
"In recent years, there has been growing interest in using decision trees as a method for inducing knowledge from data. One approach that has gained popularity is the use of generalized graph queries (GQs) for generating candidate hypotheses which can then be pruned and scored to produce a final decision tree model. GQs provide a flexible mechanism for exploring relationships within a dataset by allowing users to specify complex query patterns and constraints over the input variables. By leveraging these capabilities, we aim to develop novel techniques for inducing more accurate and interpretable decision trees that capture important features and structure present in the underlying data distribution. Our approach is evaluated empirically through extensive experimentation across a range of real-world datasets and benchmark tasks, demonstrating significant improvements over existing state-of-the art methods in terms of prediction accuracy, interpretability, and scalability. Overall, our work highlights the effectiveness of using GQs for learning decision trees and shows promise towards advancing research in this direction.",1
"We introduce canonical correlation forests (CCFs), a new decision tree ensemble method for classification and regression. Individual canonical correlation trees are binary decision trees with hyperplane splits based on local canonical correlation coefficients calculated during training. Unlike axis-aligned alternatives, the decision surfaces of CCFs are not restricted to the coordinate system of the inputs features and therefore more naturally represent data with correlated inputs. CCFs naturally accommodate multiple outputs, provide a similar computational complexity to random forests, and inherit their impressive robustness to the choice of input parameters. As part of the CCF training algorithm, we also introduce projection bootstrapping, a novel alternative to bagging for oblique decision tree ensembles which maintains use of the full dataset in selecting split points, often leading to improvements in predictive accuracy. Our experiments show that, even without parameter tuning, CCFs out-perform axis-aligned random forests and other state-of-the-art tree ensemble methods on both classification and regression problems, delivering both improved predictive accuracy and faster training times. We further show that they outperform all of the 179 classifiers considered in a recent extensive survey.",0
"Nonlinear canonical correlation analysis has been shown to effectively capture complex relationships between two matrices of random variables by embedding these data into lower dimensional spaces and finding linear combinations that maximize their correlations. In practice, high dimensionality may hinder interpretability, necessitating methods for selecting relevant features while retaining nonlinear structure. We introduce canonically correlated forest (CCF), which adapts random forests to select features through iterative variable selection using regularized nonlinear canonical regression and feature bagging based on importance measures estimated from out-of-bag samples. To overcome potential instability issues caused by model complexity, we develop a novel technique called early stopping based on cross-validation. CCF exhibits competitive predictive performance compared to state-of-the art approaches such as multi-task Lasso, group Lasso, and constrained Lasso, and can improve interpretability of predictions by identifying biologically meaningful genes/metabolites associated with clinical traits in personalized medicine studies. These results demonstrate the utility of combining decision trees with nonlinear canonical correlations in building flexible models with enhanced stability and interpretability.",1
"Most of the approaches for indoor RGBD semantic la- beling focus on using pixels or superpixels to train a classi- fier. In this paper, we implement a higher level segmentation using a hierarchy of superpixels to obtain a better segmen- tation for training our classifier. By focusing on meaningful segments that conform more directly to objects, regardless of size, we train a random forest of decision trees as a clas- sifier using simple features such as the 3D size, LAB color histogram, width, height, and shape as specified by a his- togram of surface normals. We test our method on the NYU V2 depth dataset, a challenging dataset of cluttered indoor environments. Our experiments using the NYU V2 depth dataset show that our method achieves state of the art re- sults on both a general semantic labeling introduced by the dataset (floor, structure, furniture, and objects) and a more object specific semantic labeling. We show that training a classifier on a segmentation from a hierarchy of super pixels yields better results than training directly on super pixels, patches, or pixels as in previous work.",0
"This is one possible abstract: ""This paper presents an approach to semantic instance labeling using hierarchical segmentation that leverages the power of neural network techniques while overcoming some of their limitations. In particular, we use convolutional networks to predict per pixel class probabilities at multiple scales, which enables us to capture both local features as well as more global context. However, traditional pixelwise classification approaches suffer from several issues such as low accuracy due to incomplete training data and sensitivity to changes in image backgrounds. To address these problems, we propose a novel methodology that integrates concepts from human expert knowledge representation into our machine learning system. Our hierarchical segmentation framework decomposes objects of interest into meaningful parts, allowing for better feature extraction and improved object recognition. We evaluate our proposed algorithm on a challenging dataset of outdoor scenes containing traffic signs, demonstrating significant improvements in performance compared to state-of-the-art methods."" Is there anything else I can help you?",1
Random forests perform bootstrap-aggregation by sampling the training samples with replacement. This enables the evaluation of out-of-bag error which serves as a internal cross-validation mechanism. Our motivation lies in using the unsampled training samples to improve each decision tree in the ensemble. We study the effect of using the out-of-bag samples to improve the generalization error first of the decision trees and second the random forest by post-pruning. A preliminary empirical study on four UCI repository datasets show consistent decrease in the size of the forests without considerable loss in accuracy.,0
"Random forest models can grow very large and have high computational complexity and cost. This can make them difficult to use on certain types of problems that require realtime decisions or fast inference times. In particular, these issues arise in many important applications such as robotics, autonomous systems, games, online advertising, healthcare, finance, and others. One technique we propose here is to reduce the size of a random forest while retaining most of its accuracy by removing trees that add little to no predictive power but increase computational cost and complexity significantly. Our method relies on several key ideas: Firstly, there exist some tree subsets whose removal would not affect much the performance of the final model; Secondly, these subtrees should be identified at runtime without using cross validation techniques so the trees can still capture changing data distributions over time. We evaluate our approach empirically against state-of-the-art compression techniques and show better tradeoffs between efficiency gains (speedup, memory savings) and prediction accuracies across multiple domains including vision tasks where reduced latency is critical. We provide detailed error analyses for each dataset confirming the validity of our methods and shed light into the interplay between different factors affecting pruning efficacy under varying problem sizes, complexities, and noise levels. With these results, practitioners now have access to novel and effective tools that help them address important application scenarios involving constrained resources, low-latency requirements or dynamic environments without sacrificing substantially their predictive power. Our research provides solid evidence showing random forests are viable options even for demanding cases that were previously deemed computationally challenging.",1
"An important long-term goal in machine learning systems is to build learning agents that, like humans, can learn many tasks over their lifetime, and moreover use information from these tasks to improve their ability to do so efficiently. In this work, our goal is to provide new theoretical insights into the potential of this paradigm. In particular, we propose a lifelong learning framework that adheres to a novel notion of resource efficiency that is critical in many real-world domains where feature evaluations are costly. That is, our learner aims to reuse information from previously learned related tasks to learn future tasks in a feature-efficient manner. Furthermore, we consider novel combinatorial ways in which learning tasks can relate. Specifically, we design lifelong learning algorithms for two structurally different and widely used families of target functions: decision trees/lists and monomials/polynomials. We also provide strong feature-efficiency guarantees for these algorithms; in fact, we show that in order to learn future targets, we need only slightly more feature evaluations per training example than what is needed to predict on an arbitrary example using those targets. We also provide algorithms with guarantees in an agnostic model where not all the targets are related to each other. Finally, we also provide lower bounds on the performance of a lifelong learner in these models, which are in fact tight under some conditions.",0
"One possible solution is that we treat both the input signal Xt+1 and the target output Yt as high dimensional feature vectors obtained from some kind of representation function F applied on raw data such as images, text, audio signals etc., and optimize over (X,Y) jointly rather than separately as separate tasks like supervised learning would dictate. By doing so we could actually learn representations directly optimized towards making the task at hand more efficient without actually performing any explicit meta-learning step; rather by just performing end-to-end optimization across all the available training examples we implicitly perform cross-task knowledge transfer, which enables us to solve new unseen tasks without explicitly ever having seen them before, given enough diverse training experiences. The core motivation behind this work is centered around exploiting generalization across tasks within deep generative models through careful choice of prior distributions. Specifically our approach employs a nonparametric hierarchical Bayesian modeling formalism for variational inference, where latent variables correspond to features learned via gradient based optimization as opposed to being defined hand-crafted latent variable spaces. We propose specific choices for both prior and posterior approximating families whose design facilitates lifelong learning behavior as measured through experimentation upon the concept drift problem setting. In particular we argue for using conjugate exponential family priors alongside normalizing flows for likelihood estimation, leveraging strong algebraic properties these constructions enjoy under linear transformations to ensure good calibration under arbitrary mixture weightings drawn from a Dirichlet process process over unknown static categorical base measures. Experiments corroborate the effectiveness of our approach even outperforming previously proposed alternative methods while operating in considerably lower capacity regimes, implying that richer architectures are often unnecessary for solving lifelong learning problems.",1
"In this paper, we present a novel massively parallel algorithm for accelerating the decision tree building procedure on GPUs (Graphics Processing Units), which is a crucial step in Gradient Boosted Decision Tree (GBDT) and random forests training. Previous GPU based tree building algorithms are based on parallel multi-scan or radix sort to find the exact tree split, and thus suffer from scalability and performance issues. We show that using a histogram based algorithm to approximately find the best split is more efficient and scalable on GPU. By identifying the difference between classical GPU-based image histogram construction and the feature histogram construction in decision tree training, we develop a fast feature histogram building kernel on GPU with carefully designed computational and memory access sequence to reduce atomic update conflict and maximize GPU utilization. Our algorithm can be used as a drop-in replacement for histogram construction in popular tree boosting systems to improve their scalability. As an example, to train GBDT on epsilon dataset, our method using a main-stream GPU is 7-8 times faster than histogram based algorithm on CPU in LightGBM and 25 times faster than the exact-split finding algorithm in XGBoost on a dual-socket 28-core Xeon server, while achieving similar prediction accuracy.",0
"Graphics Processor Units (GPU) have emerged as highly capable acceleration devices that can improve the performance and efficiency of data processing tasks significantly. This paper proposes a novel approach to accelerating large-scale tree boosting algorithms on modern GPUs. By exploiting the parallelism inherent in these algorithms, we achieve substantial speedups over their CPU counterparts without sacrificing accuracy. We describe our implementation, which is designed to work seamlessly with popular open source libraries, such as XGBoost and LightGBM. Our experiments demonstrate that our method results in significant speed improvements across a range of datasets and configurations. For example, one benchmark dataset processed using a widely adopted gradient boosting model yielded a speedup of up to four times faster compared to the state-of-the-art CPU implementations while maintaining comparable accuracy. Overall, our contributions represent a major step towards enabling powerful machine learning models at scale.",1
"Class imbalance presents a major hurdle in the application of data mining methods. A common practice to deal with it is to create ensembles of classifiers that learn from resampled balanced data. For example, bagged decision trees combined with random undersampling (RUS) or the synthetic minority oversampling technique (SMOTE). However, most of the resampling methods entail asymmetric changes to the examples of different classes, which in turn can introduce its own biases in the model. Furthermore, those methods require a performance measure to be specified a priori before learning. An alternative is to use a so-called threshold-moving method that a posteriori changes the decision threshold of a model to counteract the imbalance, thus has a potential to adapt to the performance measure of interest. Surprisingly, little attention has been paid to the potential of combining bagging ensemble with threshold-moving. In this paper, we present probability thresholding bagging (PT-bagging), a versatile plug-in method that fills this gap. Contrary to usual rebalancing practice, our method preserves the natural class distribution of the data resulting in well calibrated posterior probabilities. We also extend the proposed method to handle multiclass data. The method is validated on binary and multiclass benchmark data sets. We perform analyses that provide insights into the proposed method.",0
"In recent years, the problem of imbalanced data has become increasingly prevalent in many fields such as finance, healthcare, marketing, and others. Standard machine learning algorithms tend to perform poorly on these datasets due to their tendency to overfit the majority class and underfit the minority class. One approach that has shown promise in addressing this issue is ensemble learning using bagging methods. However, most existing methods require significant tuning and can still lead to overfitting. This paper proposes a simple plug-in bagging ensemble method called ""Threshold-moving"" (TM) which effectively combines multiple models into a meta estimator while reducing both underfitting and overfitting risks. We evaluate TM on several benchmark datasets commonly used in the literature to assess performance across different levels of skew ratios and different base classifiers. Our results show that the proposed method achieves competitive accuracy compared to state-of-the-art imbalanced learning approaches while requiring minimal parameter tuning. Additionally, we demonstrate the robustness of TM by applying it to both binary classification and multiclass classification problems. Overall, our work contributes to the field by providing a user-friendly and efficient solution for tackling imbalanced datasets.",1
"We present a general framework for training deep neural networks without backpropagation. This substantially decreases training time and also allows for construction of deep networks with many sorts of learners, including networks whose layers are defined by functions that are not easily differentiated, like decision trees. The main idea is that layers can be trained one at a time, and once they are trained, the input data are mapped forward through the layer to create a new learning problem. The process is repeated, transforming the data through multiple layers, one at a time, rendering a new data set, which is expected to be better behaved, and on which a final output layer can achieve good performance. We call this forward thinking and demonstrate a proof of concept by achieving state-of-the-art accuracy on the MNIST dataset for convolutional neural networks. We also provide a general mathematical formulation of forward thinking that allows for other types of deep learning problems to be considered.",0
"Title: Deep Learning from Scratch: Constructing and Optimizing Artificial Neurons using TensorFlow.2 Abstract: Artificial neural networks have revolutionized numerous fields by enabling deep learning solutions that automatically learn complex representations from large amounts of data. Despite their popularity, designing effective artificial neurons remains a challenge due to lack of understanding of how these building blocks interact within the network. To address this gap, we present techniques for constructing and training artificial neurons on challenging computer vision tasks using the most recent version of TensorFlow – one of the most widely used libraries for machine learning development today. Our approach builds upon seminal work in computational neuroscience while incorporating the latest developments in deep learning research. We showcase our findings through experiments conducted on three benchmark datasets, achieving state-of-the-art results in two out of three cases. By making our code publicly available alongside our analyses and visualizations, we hope to encourage further investigation into developing intelligent agents capable of tackling increasingly difficult real-world problems. Keywords: deep learning; artificial neural networks; convolutional neural networks (CNN); recurrent neural networks (RNN); backpropagation; stochastic gradient descent (SGD)",1
"Classification predicts classes of objects using the knowledge learned during the training phase. This process requires learning from labeled samples. However, the labeled samples usually limited. Annotation process is annoying, tedious, expensive, and requires human experts. Meanwhile, unlabeled data is available and almost free. Semi-supervised learning approaches make use of both labeled and unlabeled data. This paper introduces cluster and label approach using PSO for semi-supervised classification. PSO is competitive to traditional clustering algorithms. A new local best PSO is presented to cluster the unlabeled data. The available labeled data guides the learning process. The experiments are conducted using four state-of-the-art datasets from different domains. The results compared with Label Propagation a popular semi-supervised classifier and two state-of-the-art supervised classification models, namely k-nearest neighbors and decision trees. The experiments show the efficiency of the proposed model.",0
"This article presents a new methodology that combines semi-supervised clustering algorithms with particle swarm optimization (PSO) for labeling images. Firstly, we present an overview of existing methods of semi-supervised learning such as transductive support vector machines, active learning, co-training, self-training and graph embedding. Next, our approach uses particle swarm optimization to optimize the parameters of the clustering algorithm by iteratively updating them based on feedback from labeled data points until convergence. We evaluate the effectiveness of our proposed method through experimental results obtained on popular image datasets such as MNIST, CIFAR-10, SVHN and STL-10. These experiments demonstrate the superior performance of our approach compared to other state-of-the-art methods. Finally, we discuss potential future research directions for improving the accuracy of semi-supervised classification using particle swarm optimization techniques.",1
A Discriminative Deep Forest (DisDF) as a metric learning algorithm is proposed in the paper. It is based on the Deep Forest or gcForest proposed by Zhou and Feng and can be viewed as a gcForest modification. The case of the fully supervised learning is studied when the class labels of individual training examples are known. The main idea underlying the algorithm is to assign weights to decision trees in random forest in order to reduce distances between objects from the same class and to increase them between objects from different classes. The weights are training parameters. A specific objective function which combines Euclidean and Manhattan distances and simplifies the optimization problem for training the DisDF is proposed. The numerical experiments illustrate the proposed distance metric algorithm.,0
"This should be possible without mentioning any specific application domain or problem class since I can add that later myself. Absract: This work introduces Discrminative metric learning (DML), a new approach which extends the popular deep forest algorithm from binary classification problems to multi-class tasks by using discriminatively trained decision trees as base models and training them end-to-end under novel loss functions. Our key insight is the observation that minimizing cross entropy loss alone often leads to suboptimal feature representation because the decision trees tend to align their internal splits to the majority class labels at the expense of other classes, which hurts performance on minority classes. To address this issue, we propose a set of regularization terms based on statistical analysis on the output behavior of deep forests, which encourages each tree in the ensemble to assign high confidence scores to samples from different classes. Extensive experimental results demonstrate our method significantly outperforms both competitive baselines on common benchmark datasets and real-world applications. We further analyze our proposed methods and provide insights into how DML helps boost deep forest's performance via visualizations of learned features. Overall, our work advances the state-of-the-art in deep learning by providing a flexible toolkit applicable across many domains, capable of delivering large improvements over existing algorithms.",1
"The success of deep neural networks has inspired many to wonder whether other learners could benefit from deep, layered architectures. We present a general framework called forward thinking for deep learning that generalizes the architectural flexibility and sophistication of deep neural networks while also allowing for (i) different types of learning functions in the network, other than neurons, and (ii) the ability to adaptively deepen the network as needed to improve results. This is done by training one layer at a time, and once a layer is trained, the input data are mapped forward through the layer to create a new learning problem. The process is then repeated, transforming the data through multiple layers, one at a time, rendering a new dataset, which is expected to be better behaved, and on which a final output layer can achieve good performance. In the case where the neurons of deep neural nets are replaced with decision trees, we call the result a Forward Thinking Deep Random Forest (FTDRF). We demonstrate a proof of concept by applying FTDRF on the MNIST dataset. We also provide a general mathematical formulation that allows for other types of deep learning problems to be considered.",0
"Abstract: This paper describes how deep random forest can be used as an alternative modeling approach that may outperform traditional machine learning techniques on some applications while allowing for faster computation times. In order to demonstrate these benefits, we first overview random forest basics such as bagging and boosting, then discuss how these ideas have been incorporated into neural networks in recent work on Deep Learning from Trees (DLFT). We next present our own methodology using neural network ensembles which extends DLFT via parameter sharing and depthwise connections inspired by recent advances in computer vision (e.g., ResNet), resulting in models that are both more accurate than standard random forests while being at least two orders of magnitude faster during training and inference time due to reduced computational costs. Through experiments on multiple datasets and comparisons against other state-of-the-art methods, we showcase significant improvements across numerous performance metrics and confirm that deep random forest models offer competitive accuracy without sacrificing efficiency. Our findings suggest a compelling case towards exploring random forest variants within deep learning frameworks to develop effective solutions to real world problems in diverse fields. The codebase used in our research can be found in supplementary materials along with preprocessed datasets to encourage reproducibility and further experimentation. As interest in interpretable AI rises alongside demands for explainability, deep random forests provide opportunities to leverage explainable ML approaches through transparent feature importance measures and permutation feature elimination in forward passing. Moving beyond black box predictions could open doors for deep random forest architectures being employed as general models for complex decision making scenarios, ultimately facilitating their uptake in practice where interpretability is valued. Ultimately, this study aims to contribute to literature on random forest extensions beyond conventional randomization procedures and highlights potential advantages when applying deep learning concepts to traditional random forest workflows.",1
"There is a large literature explaining why AdaBoost is a successful classifier. The literature on AdaBoost focuses on classifier margins and boosting's interpretation as the optimization of an exponential likelihood function. These existing explanations, however, have been pointed out to be incomplete. A random forest is another popular ensemble method for which there is substantially less explanation in the literature. We introduce a novel perspective on AdaBoost and random forests that proposes that the two algorithms work for similar reasons. While both classifiers achieve similar predictive accuracy, random forests cannot be conceived as a direct optimization procedure. Rather, random forests is a self-averaging, interpolating algorithm which creates what we denote as a ""spikey-smooth"" classifier, and we view AdaBoost in the same light. We conjecture that both AdaBoost and random forests succeed because of this mechanism. We provide a number of examples and some theoretical justification to support this explanation. In the process, we question the conventional wisdom that suggests that boosting algorithms for classification require regularization or early stopping and should be limited to low complexity classes of learners, such as decision stumps. We conclude that boosting should be used like random forests: with large decision trees and without direct regularization or early stopping.",0
"This paper investigates the success of two popular machine learning algorithms, AdaBoost and Random Forests, as interpolating classifiers. These methods have been found to perform well on many tasks, but little work has been done to explain why they are successful. We provide new insights into their performance by showing that both methods can improve overtime through online model aggregation, leading to better predictions. Our analysis shows that these models have superior interpolation capabilities compared to other state-of-the-art classifiers. These results contribute to our understanding of how ensemble methods can improve classification accuracy, especially in high-dimensional spaces where linear models may struggle.",1
"Currently, approximately 30% of epileptic patients treated with antiepileptic drugs (AEDs) remain resistant to treatment (known as refractory patients). This project seeks to understand the underlying similarities in refractory patients vs. other epileptic patients, identify features contributing to drug resistance across underlying phenotypes for refractory patients, and develop predictive models for drug resistance in epileptic patients. In this study, epileptic patient data was examined to attempt to observe discernable similarities or differences in refractory patients (case) and other non-refractory patients (control) to map underlying mechanisms in causality. For the first part of the study, unsupervised algorithms such as Kmeans, Spectral Clustering, and Gaussian Mixture Models were used to examine patient features projected into a lower dimensional space. Results from this study showed a high degree of non-linearity in the underlying feature space. For the second part of this study, classification algorithms such as Logistic Regression, Gradient Boosted Decision Trees, and SVMs, were tested on the reduced-dimensionality features, with accuracy results of 0.83(+/-0.3) testing using 7 fold cross validation. Observations of test results indicate using a radial basis function kernel PCA to reduce features ingested by a Gradient Boosted Decision Tree Ensemble lead to gains in improved accuracy in mapping a binary decision to highly non-linear features collected from epileptic patients.",0
"In this study we aimed to identify similarities among epileptics in order to predict drug resistance. We analyzed EEGs from patients who had been previously diagnosed as resistant to medication, as well as those who were responsive to treatment. Our results showed that there were indeed distinct patterns present in both groups which could potentially aid clinicians in determining whether or not their patient is likely to respond positively to anti-epileptic drugs. While further research is necessary to fully confirm these findings, our work represents a significant step forward in personalized medicine for individuals suffering from seizures.",1
"Within machine learning, the supervised learning field aims at modeling the input-output relationship of a system, from past observations of its behavior. Decision trees characterize the input-output relationship through a series of nested $if-then-else$ questions, the testing nodes, leading to a set of predictions, the leaf nodes. Several of such trees are often combined together for state-of-the-art performance: random forest ensembles average the predictions of randomized decision trees trained independently in parallel, while tree boosting ensembles train decision trees sequentially to refine the predictions made by the previous ones. The emergence of new applications requires scalable supervised learning algorithms in terms of computational power and memory space with respect to the number of inputs, outputs, and observations without sacrificing accuracy. In this thesis, we identify three main areas where decision tree methods could be improved for which we provide and evaluate original algorithmic solutions: (i) learning over high dimensional output spaces, (ii) learning with large sample datasets and stringent memory constraints at prediction time and (iii) learning over high dimensional sparse input spaces.",0
"Random Projection (RP) is a technique that projects high dimensional data into lower dimensions while preserving pairwise distances, making it suitable for use with machine learning algorithms that struggle with large datasets. In recent years, there has been growing interest in using RP along with sparse representations and ensemble methods such as Gradient Boosting Machines (GBMs) and Random Forests (RFs). These models have shown great promise, particularly in areas where interpretability and feature selection are important considerations.  This work investigates the application of RP in conjunction with GBMs and RFs for two common challenges: Multi-Label Learning (MLL) and Multi-Output Learning (MOL), both of which involve predicting multiple outputs from a single input. We show how these techniques can effectively leverage sparsity in the dataset by creating a subset of features that are most informative, resulting in improved performance compared to dense representations. Additionally, we demonstrate how the combination of RP and ensembles leads to smaller, more interpretable models.  Another aspect of our work focuses on compressing Random Forest models through the use of random projection, reducing their complexity without sacrificing accuracy. This allows us to balance the need for accurate predictions with the constraints imposed by available computational resources. Finally, we investigate ways to further optimize RF and GBM models by exploiting additional structure present in some problem domains, including correlated outputs and nonlinear relationships between inputs and outcomes.  Overall, this research demonstrates the effectiveness of combining RP with ensemble methods for tackling complex tasks, highlighting potential applications across diverse fields. By exploring real-world problems, we illustrate the benefits of exploiting random projections, sparsity, and other structural properties within machine learning frameworks, paving the way towards developing efficient, informed solutions to a range of prediction problems.",1
"Speech is the most common communication method between humans and involves the perception of both auditory and visual channels. Automatic speech recognition focuses on interpreting the audio signals, but it has been demonstrated that video can provide information that is complementary to the audio. Thus, the study of automatic lip-reading is important and is still an open problem. One of the key challenges is the definition of the visual elementary units (the visemes) and their vocabulary. Many researchers have analyzed the importance of the phoneme to viseme mapping and have proposed viseme vocabularies with lengths between 11 and 15 visemes. These viseme vocabularies have usually been manually defined by their linguistic properties and in some cases using decision trees or clustering techniques. In this work, we focus on the automatic construction of an optimal viseme vocabulary based on the association of phonemes with similar appearance. To this end, we construct an automatic system that uses local appearance descriptors to extract the main characteristics of the mouth region and HMMs to model the statistic relations of both viseme and phoneme sequences. To compare the performance of the system different descriptors (PCA, DCT and SIFT) are analyzed. We test our system in a Spanish corpus of continuous speech. Our results indicate that we are able to recognize approximately 58% of the visemes, 47% of the phonemes and 23% of the words in a continuous speech scenario and that the optimal viseme vocabulary for Spanish is composed by 20 visemes.",0
"In recent years, the need for automatic lip-reading systems has become increasingly important due to their potential applications in real-world scenarios such as surveillance, human-computer interaction, speech recognition, and assistive technology for hearing impaired individuals. One critical component of these systems is viseme vocabularies that map phones (distinct acoustic units) into visual features corresponding to distinct mouth shapes formed during articulation. This study aimed to improve continuous lip-reading performance by automatically constructing a more comprehensive viseme vocabulary using data from real-life videos. Using deep learning techniques such as autoencoders and recurrent neural networks, we were able to extract features and characteristics from visemes that can enhance their discrimination accuracy. Furthermore, our results show that the proposed method achieves significantly better accuracy compared to existing methods while providing a more fine-grained description of the articulatory movements involved in pronunciation. These findings have significant implications for advancing research in the field of computational lip-reading and could potentially revolutionize how we interact with computers and each other through natural language interfaces.",1
"The preceding three decades have seen the emergence, rise, and proliferation of machine learning (ML). From half-recognised beginnings in perceptrons, neural nets, and decision trees, algorithms that extract correlations (that is, patterns) from a set of data points have broken free from their origin in computational cognition to embrace all forms of problem solving, from voice recognition to medical diagnosis to automated scientific research and driverless cars, and it is now widely opined that the real industrial revolution lies less in mobile phone and similar than in the maturation and universal application of ML. Among the consequences just might be the triumph of anti-realism over realism.",0
"Machine learning has rapidly become one of the most important tools available to researchers today. Its ability to analyze vast amounts of data quickly and accurately makes it well suited for many applications, including image recognition, natural language processing, and even video games. One area where machine learning may have particularly interesting implications is realist theory and philosophy, which focuses on issues related to the nature of reality and how we can know it. In particular, machine learning raises questions about the extent to which our perceptions of reality are constructed by social structures and technological systems rather than simply reflecting some objective truth out there in the world. By examining these questions through the lens of machine learning, we may gain new insights into both the strengths and limitations of current realist approaches. This paper presents a discussion of how machine learning is challenging traditional concepts of realism and suggests possible ways forward for those working within a realist framework.",1
"A human action can be seen as transitions between one's body poses over time, where the transition depicts a temporal relation between two poses. Recognizing actions thus involves learning a classifier sensitive to these pose transitions as well as to static poses. In this paper, we introduce a novel method called transitions forests, an ensemble of decision trees that both learn to discriminate static poses and transitions between pairs of two independent frames. During training, node splitting is driven by alternating two criteria: the standard classification objective that maximizes the discrimination power in individual frames, and the proposed one in pairwise frame transitions. Growing the trees tends to group frames that have similar associated transitions and share same action label incorporating temporal information that was not available otherwise. Unlike conventional decision trees where the best split in a node is determined independently of other nodes, the transition forests try to find the best split of nodes jointly (within a layer) for incorporating distant node transitions. When inferring the class label of a new frame, it is passed down the trees and the prediction is made based on previous frame predictions and the current one in an efficient and online manner. We apply our method on varied skeleton action recognition and online detection datasets showing its suitability over several baselines and state-of-the-art approaches.",0
"Artificial intelligence (AI) has been gaining traction as a technology that can revolutionize different industries. From robotics to healthcare, AI has proven its worth by automating many tasks and processes while increasing efficiency and reducing costs. In agriculture, AI has immense potential to transform traditional farming methods into more sustainable practices through precision farming techniques such as sensor-based irrigation systems, drones for spraying pesticides and fertilizers, yield prediction models, farm machinery control, livestock monitoring, etc. The goal of precision farming is to increase productivity using fewer resources and creating less waste, which helps reduce environmental impacts and minimizes greenhouse gas emissions from agricultural activities. This paper discusses how artificial intelligence could enhance agriculture through precision farming techniques to create a more sustainable future.",1
"We propose a new approach to image segmentation, which exploits the advantages of both conditional random fields (CRFs) and decision trees. In the literature, the potential functions of CRFs are mostly defined as a linear combination of some pre-defined parametric models, and then methods like structured support vector machines (SSVMs) are applied to learn those linear coefficients. We instead formulate the unary and pairwise potentials as nonparametric forests---ensembles of decision trees, and learn the ensemble parameters and the trees in a unified optimization problem within the large-margin framework. In this fashion, we easily achieve nonlinear learning of potential functions on both unary and pairwise terms in CRFs. Moreover, we learn class-wise decision trees for each object that appears in the image. Due to the rich structure and flexibility of decision trees, our approach is powerful in modelling complex data likelihoods and label relationships. The resulting optimization problem is very challenging because it can have exponentially many variables and constraints. We show that this challenging optimization can be efficiently solved by combining a modified column generation and cutting-planes techniques. Experimental results on both binary (Graz-02, Weizmann horse, Oxford flower) and multi-class (MSRC-21, PASCAL VOC 2012) segmentation datasets demonstrate the power of the learned nonlinear nonparametric potentials.",0
"Title: Abstract  The purpose of this study was to explore the potential of using structured learning techniques for tree potentials in conditional random fields (CRF) for image segmentation tasks. Image segmentation is a challenging problem that involves partitioning an image into multiple segments based on visual features such as color, texture, and intensity. In recent years, CRF models have gained popularity due to their ability to incorporate both local and global information in a principled manner. However, these models require careful tuning of parameters and training data to achieve optimal performance.  In our work, we propose a new approach for learning tree potentials in CRF models for image segmentation that leverages structured learning techniques. Our method first preprocesses the input images by extracting features that are relevant for the task at hand. These features are then used to train a structured learning model that learns effective potentials for each pixel location in the image. By doing so, we aim to improve the efficiency and accuracy of the segmentation process while reducing the need for manual parameter tuning.  We evaluated our proposed method on several benchmark datasets and compared its performance against state-of-the-art methods. Results showed that our method outperformed other approaches across all metrics, demonstrating its effectiveness in improving the quality of image segmentations. Furthermore, we analyzed the learned potential functions and found them to be meaningful and interpretable, which provides insights into how different features contribute to the final prediction.  Overall, our findings suggest that structured learning techniques can greatly benefit the field of image segmen",1
"Big Data is one of the major challenges of statistical science and has numerous consequences from algorithmic and theoretical viewpoints. Big Data always involve massive data but they also often include online data and data heterogeneity. Recently some statistical methods have been adapted to process Big Data, like linear regression models, clustering methods and bootstrapping schemes. Based on decision trees combined with aggregation and bootstrap ideas, random forests were introduced by Breiman in 2001. They are a powerful nonparametric statistical method allowing to consider in a single and versatile framework regression problems, as well as two-class and multi-class classification problems. Focusing on classification problems, this paper proposes a selective review of available proposals that deal with scaling random forests to Big Data problems. These proposals rely on parallel environments or on online adaptations of random forests. We also describe how related quantities -- such as out-of-bag error and variable importance -- are addressed in these methods. Then, we formulate various remarks for random forests in the Big Data context. Finally, we experiment five variants on two massive datasets (15 and 120 millions of observations), a simulated one as well as real world data. One variant relies on subsampling while three others are related to parallel implementations of random forests and involve either various adaptations of bootstrap to Big Data or to ""divide-and-conquer"" approaches. The fifth variant relates on online learning of random forests. These numerical experiments lead to highlight the relative performance of the different variants, as well as some of their limitations.",0
"Title: Random Forests for Big Data: Classifying Large Scale Time Series Datasets (Click here to download pdf) Abstract: This study proposes using random forests as an algorithm for classifying large scale time series datasets in big data environments. As increasing amounts of data become available at high velocities, traditional machine learning techniques may fall short due to their resource intensive nature and limited scalability. However, recent advancements in random forest implementations have made them more suitable for big data applications. In particular, we consider two popular random forest libraries on Apache Spark, namely, MLlib and Hivemall. We explore the effectiveness of these algorithms by evaluating them on several real world datasets consisting of millions of timestamps, including one from Twitter containing over 4 billion instances. Our findings indicate that both packages can accurately classify these high volume datasets, even when compared against traditional methods such as decision trees and linear regression models. Furthermore, our study finds that when using appropriate hyperparameters, both platforms perform comparably well, making either option suitable for real world deployments in enterprises. These results demonstrate the feasibility of employing random forests within big data workflows for classification tasks involving large scale time series data.",1
"Motion ability is one of the most important human properties, including gait as a basis of human transitional movement. Gait, as a biometric for recognizing human identities, can be non-intrusively captured signals using wearable or portable smart devices. In this study gait patterns is collected using a wireless platform of two sensors located at chest and right ankle of the subjects. Then the raw data has undergone some preprocessing methods and segmented into 5 seconds windows. Some time and frequency domain features is extracted and the performance evaluated by 5 different classifiers. Decision Tree (with all features) and K-Nearest Neighbors (with 10 selected features) classifiers reached 99.4% and 100% respectively.",0
"Gait analysis has become an important tool for assessing movement disorders in clinical settings as well as evaluating athletic performance. With advances in wearable technology, gait pattern recognition using accelerometers has gained attention due to their low cost, portability, and ease of use. This study aimed to evaluate the accuracy of gait patterns recognized by accelerometers compared to those obtained through video recording analysis. Forty healthy participants were recruited, and three trials each with five walking speeds ranging from slow to fast were recorded using VICON motion capture system and two wireless accelerometers placed on both thighs. We investigated whether accelerometer data can accurately estimate spatiotemporal parameters such as step length, stride width, cadence, and gait cycle time under different speed conditions. Results showed that while accelerometers overestimated some measures at slower speeds, there was good agreement between accelerometry and gold standard camera systems for most gait parameters measured during faster speeds. These findings suggest that accelerometers can provide reliable estimates of gait patterns especially at high speeds commonly observed in athletes. Future studies should focus on developing processing algorithms capable of reducing measurement errors at lower speeds. Keywords: Gait Analysis, Wearable Technology, Accelerometry, Motion Capture System, Spatiotemporal Parameters.",1
"This paper presents a novel method for structural data recognition using a large number of graph models. In general, prevalent methods for structural data recognition have two shortcomings: 1) Only a single model is used to capture structural variation. 2) Naive recognition methods are used, such as the nearest neighbor method. In this paper, we propose strengthening the recognition performance of these models as well as their ability to capture structural variation. The proposed method constructs a large number of graph models and trains decision trees using the models. This paper makes two main contributions. The first is a novel graph model that can quickly perform calculations, which allows us to construct several models in a feasible amount of time. The second contribution is a novel approach to structural data recognition: graph model boosting. Comprehensive structural variations can be captured with a large number of graph models constructed in a boosting framework, and a sophisticated classifier can be formed by aggregating the decision trees. Consequently, we can carry out structural data recognition with powerful recognition capability in the face of comprehensive structural variation. The experiments shows that the proposed method achieves impressive results and outperforms existing methods on datasets of IAM graph database repository.",0
This paper presents a method for structured data recognition from unstructured text. The proposed approach uses graph models and boosting algorithms to improve performance. Experiments on real world datasets show that our method outperforms state of the art methods by significant margins. The paper concludes with future directions for research in this area.,1
"In this paper, we present machine learning approaches for characterizing and forecasting the short-term demand for on-demand ride-hailing services. We propose the spatio-temporal estimation of the demand that is a function of variable effects related to traffic, pricing and weather conditions. With respect to the methodology, a single decision tree, bootstrap-aggregated (bagged) decision trees, random forest, boosted decision trees, and artificial neural network for regression have been adapted and systematically compared using various statistics, e.g. R-square, Root Mean Square Error (RMSE), and slope. To better assess the quality of the models, they have been tested on a real case study using the data of DiDi Chuxing, the main on-demand ride hailing service provider in China. In the current study, 199,584 time-slots describing the spatio-temporal ride-hailing demand has been extracted with an aggregated-time interval of 10 mins. All the methods are trained and validated on the basis of two independent samples from this dataset. The results revealed that boosted decision trees provide the best prediction accuracy (RMSE=16.41), while avoiding the risk of over-fitting, followed by artificial neural network (20.09), random forest (23.50), bagged decision trees (24.29) and single decision tree (33.55).",0
"This paper investigates several methods which can predict future demand at a given place over time using large amounts of data obtained from mobile sensors such as GPS devices (smartphones) which report where each taxi picks up passengers. This work builds on previous research by looking specifically at how well different machine learning algorithms can perform in making such predictions. We compare the use of simple linear regression, random forest, support vector machines, gradient boosted models, artificial neural networks (ANN), convolutional neural network (CNN), and multi-layer perceptrons. In all our experiments we used two datasets: one containing passenger origin points as recorded over three consecutive years by Grabtaxi vehicles. The second dataset contains real Grabtaxi trips including customer origin and destination addresses for 4 weeks in Jakarta, Indonesia. Results show that among many combinations of hyperparameters tested, the better performance was achieved by CNN and MLP for spatial prediction tasks while Gradient Boost model performs the best for temporal prediction task. For both types of predictions combined (spatial and temporal), however, Gradient Boost performs consistently well across all metrics used in evaluation",1
"In this paper we propose a synergistic melting of neural networks and decision trees (DT) we call neural decision trees (NDT). NDT is an architecture a la decision tree where each splitting node is an independent multilayer perceptron allowing oblique decision functions or arbritrary nonlinear decision function if more than one layer is used. This way, each MLP can be seen as a node of the tree. We then show that with the weight sharing asumption among those units, we end up with a Hashing Neural Network (HNN) which is a multilayer perceptron with sigmoid activation function for the last layer as opposed to the standard softmax. The output units then jointly represent the probability to be in a particular region. The proposed framework allows for global optimization as opposed to greedy in DT and differentiability w.r.t. all parameters and the input, allowing easy integration in any learnable pipeline, for example after CNNs for computer vision tasks. We also demonstrate the modeling power of HNN allowing to learn union of disjoint regions for final clustering or classification making it more general and powerful than standard softmax MLP requiring linear separability thus reducing the need on the inner layer to perform complex data transformations. We finally show experiments for supervised, semi-suppervised and unsupervised tasks and compare results with standard DTs and MLPs.",0
"Neural decision trees offer a promising new approach to data analysis that combines the strengths of neural networks and traditional decision tree methods. In this study, we present a detailed examination of the properties and performance of these models on a range of real world datasets. Our results show that neural decision trees can achieve state-of-the-art accuracy while offering several advantages over other methods. These include robustness to nonlinear relationships, interpretability through visualization, and scalability to large datasets. Furthermore, our findings suggest that neural decision trees may have applications beyond classification problems, such as regression and feature selection. Overall, this work highlights the potential of neural decision trees as a powerful tool for data scientists and machine learning practitioners alike.",1
"In this paper we introduce a new classification algorithm called Optimization of Distributions Differences (ODD). The algorithm aims to find a transformation from the feature space to a new space where the instances in the same class are as close as possible to one another while the gravity centers of these classes are as far as possible from one another. This aim is formulated as a multiobjective optimization problem that is solved by a hybrid of an evolutionary strategy and the Quasi-Newton method. The choice of the transformation function is flexible and could be any continuous space function. We experiment with a linear and a non-linear transformation in this paper. We show that the algorithm can outperform 6 other state-of-the-art classification methods, namely naive Bayes, support vector machines, linear discriminant analysis, multi-layer perceptrons, decision trees, and k-nearest neighbors, in 12 standard classification datasets. Our results show that the method is less sensitive to the imbalanced number of instances comparing to these methods. We also show that ODD maintains its performance better than other classification methods in these datasets, hence, offers a better generalization ability.",0
"""In recent years, machine learning has become increasingly important for many areas of study, including computer vision and image recognition. One common task in these fields is image classification, which involves identifying objects within an image based on their characteristics. To achieve accurate results, researchers often use features that capture relevant properties of images such as edges and corners. However, selecting an appropriate set of features can be challenging and time consuming. In this paper, we propose a novel methodology called Distribution Difference (DD) that optimizes feature selection for classification tasks by minimizing the difference in distribution between the training data and the test data. Our approach consists of three steps: First, a random subset of features is selected from the original dataset; then the distance between the distribution of each sample in the training set and the corresponding subpopulation in the testing set is computed using a Kullback-Leibler divergence metric. Finally, the optimal subset of features is determined as those yielding the minimum value of the average distance over all samples. We demonstrate the effectiveness of our proposed method through experiments conducted on several benchmark datasets, showing significant improvement compared to other popular feature selection techniques.""",1
"As data collections become larger, exploratory regression analysis becomes more important but more challenging. When observations are hierarchically clustered the problem is even more challenging because model selection with mixed effect models can produce misleading results when nonlinear effects are not included into the model (Bauer and Cai, 2009). A machine learning method called boosted decision trees (Friedman, 2001) is a good approach for exploratory regression analysis in real data sets because it can detect predictors with nonlinear and interaction effects while also accounting for missing data. We propose an extension to boosted decision decision trees called metboost for hierarchically clustered data. It works by constraining the structure of each tree to be the same across groups, but allowing the terminal node means to differ. This allows predictors and split points to lead to different predictions within each group, and approximates nonlinear group specific effects. Importantly, metboost remains computationally feasible for thousands of observations and hundreds of predictors that may contain missing values. We apply the method to predict math performance for 15,240 students from 751 schools in data collected in the Educational Longitudinal Study 2002 (Ingels et al., 2007), allowing 76 predictors to have unique effects for each school. When comparing results to boosted decision trees, metboost has 15% improved prediction performance. Results of a large simulation study show that metboost has up to 70% improved variable selection performance and up to 30% improved prediction performance compared to boosted decision trees when group sizes are small",0
"Metaboost is a powerful analytical tool that can be used to perform exploratory regression analysis on hierarchically clustered datasets. This method allows researchers to identify trends and patterns within complex datasets by clustering similar observations together and examining their relationships to predictors. In addition, Metaboost provides multiple options for visualizing results such as heatmaps and scatterplots, making it easy for users to interpret their findings. By utilizing modern statistical algorithms, Metaboost ensures accurate results while minimizing computational time. Overall, Metaboost provides a simple yet effective approach for analyzing large scale, multivariate data sets.",1
"In this paper, a genetic algorithm-based frequency-domain feature search (GAFDS) method is proposed for the electroencephalogram (EEG) analysis of epilepsy. In this method, frequency-domain features are first searched and then combined with nonlinear features. Subsequently, these features are selected and optimized to classify EEG signals. The extracted features are analyzed experimentally. The features extracted by GAFDS show remarkable independence, and they are superior to the nonlinear features in terms of the ratio of inter-class distance and intra-class distance. Moreover, the proposed feature search method can additionally search for features of instantaneous frequency in a signal after Hilbert transformation. The classification results achieved using these features are reasonable, thus, GAFDS exhibits good extensibility. Multiple classic classifiers (i.e., $k$-nearest neighbor, linear discriminant analysis, decision tree, AdaBoost, multilayer perceptron, and Na\""ive Bayes) achieve good results by using the features generated by GAFDS method and the optimized selection. Specifically, the accuracies for the two-classification and three-classification problems may reach up to 99% and 97%, respectively. Results of several cross-validation experiments illustrate that GAFDS is effective in feature extraction for EEG classification. Therefore, the proposed feature selection and optimization model can improve classification accuracy.",0
"This paper presents a new method for feature extraction from electroencephalogram (EEG) signals that uses genetic algorithms to search for frequency domain features. The proposed method was evaluated on a multi-class classification task involving epileptogenic regions of interest within the brain. The results demonstrate that the proposed method outperforms existing methods in terms of accuracy and generalization performance across multiple datasets. Additionally, the extensibility of the algorithm allows it to be easily adapted to different EEG applications without requiring significant changes to the underlying codebase. Overall, the proposed method represents a promising approach for improving the diagnosis and monitoring of neurological disorders such as epilepsy.",1
"We introduce highly efficient online nonlinear regression algorithms that are suitable for real life applications. We process the data in a truly online manner such that no storage is needed, i.e., the data is discarded after being used. For nonlinear modeling we use a hierarchical piecewise linear approach based on the notion of decision trees where the space of the regressor vectors is adaptively partitioned based on the performance. As the first time in the literature, we learn both the piecewise linear partitioning of the regressor space as well as the linear models in each region using highly effective second order methods, i.e., Newton-Raphson Methods. Hence, we avoid the well known over fitting issues by using piecewise linear models, however, since both the region boundaries as well as the linear models in each region are trained using the second order methods, we achieve substantial performance compared to the state of the art. We demonstrate our gains over the well known benchmark data sets and provide performance results in an individual sequence manner guaranteed to hold without any statistical assumptions. Hence, the introduced algorithms address computational complexity issues widely encountered in real life applications while providing superior guaranteed performance in a strong deterministic sense.",0
"In this paper, we propose a highly efficient hierarchical online nonlinear regression algorithm that utilizes second order methods to improve accuracy and speed. Our approach builds upon recent advances in online learning, where models are updated incrementally as new data becomes available. We introduce a novel hierarchy of models, including an ensemble of local linear models and a global model, which allows us to achieve better generalization performance while maintaining low computational complexity. The local models capture regional patterns in the data, while the global model provides a high level overview. By combining these two levels in our hierarchy, we can reduce overfitting and improve overall accuracy. Additionally, we employ a second order method, specifically the Quasi-Newton updating rule, which has been shown to converge faster than gradient descent. Extensive experiments on benchmark datasets demonstrate significant improvements in accuracy and efficiency compared to state-of-the-art online nonlinear regression algorithms. Overall, our proposed method represents a promising direction for solving large scale machine learning problems requiring fast adaptation to changing environments.",1
"We introduce a library of geometric voxel features for CAD surface recognition/retrieval tasks. Our features include local versions of the intrinsic volumes (the usual 3D volume, surface area, integrated mean and Gaussian curvature) and a few closely related quantities. We also compute Haar wavelet and statistical distribution features by aggregating raw voxel features. We apply our features to object classification on the ESB data set and demonstrate accurate results with a small number of shallow decision trees.",0
"In recent years, there has been significant interest in using geometric features extracted from medical imaging data to recognize and classify surfaces within the body. One popular method involves identifying these features on voxels, which are three-dimensional pixels that make up volumetric images such as CT scans or MRI scans. This approach allows for greater precision and accuracy compared to traditional manual segmentation methods. However, extracting meaningful and relevant geometric features from these large datasets remains challenging. In this paper, we present a novel method for detecting and characterizing surface features on voxel-level data, focusing specifically on their application to surface recognition tasks. We evaluate our approach through extensive experiments using real patient data and demonstrate its effectiveness compared to existing approaches. Our results suggest that incorporating carefully selected geometric features can significantly improve the robustness and generalizability of voxel-based surface recognition models. Overall, this work represents an important step towards advancing computer-aided diagnosis and surgical planning techniques in medicine.",1
"More and more processes governing our lives use in some part an automatic decision step, where -- based on a feature vector derived from an applicant -- an algorithm has the decision power over the final outcome. Here we present a simple idea which gives some of the power back to the applicant by providing her with alternatives which would make the decision algorithm decide differently. It is based on a formalization reminiscent of methods used for evasion attacks, and consists in enumerating the subspaces where the classifiers decides the desired output. This has been implemented for the specific case of decision forests (ensemble methods based on decision trees), mapping the problem to an iterative version of enumerating $k$-cliques.",0
"As artificial intelligence becomes more integrated into our daily lives, users may find themselves relying on automated decisions more frequently than human ones. However, these automated systems can sometimes make decisions that do not align with user preferences or expectations. In order to address this issue, it is important to provide users with the ability to guide the decision-making process and receive transparency regarding how decisions were reached. This paper presents research exploring current challenges faced by users who rely on automated systems and discusses potential solutions to enhance user experience and satisfaction. Results suggest that providing clear instructions, offering alternatives and options, and fostering open communication can all lead to better outcomes for both users and the organizations implementing the technology. Ultimately, we argue that empowering users through greater control over automated decision making is essential to maintain trust and build confidence in AI technologies moving forward. The rapid expansion of Artificial Intelligence (AI) has led to an increasing reliance on automated decision-making processes across different industries and aspects of everyday life. While automation offers many advantages such as efficiency, speed, and cost savings, there are concerns surrounding the lack of transparency in decision-making processes and the potential discrepancy between user expectations and system outputs. This study aimed to investigate ways to improve the user experience when interacting with automated systems by allowing them to have greater control over the decision-making process. Through a review of existing literature, case studies, surveys, and focus groups, we identified several key challenges encountered by users when dealing wit",1
"The European Space Agency (ESA) defines an Earth Observation (EO) Level 2 product as a multispectral (MS) image corrected for geometric, atmospheric, adjacency and topographic effects, stacked with its scene classification map (SCM), whose legend includes quality layers such as cloud and cloud-shadow. No ESA EO Level 2 product has ever been systematically generated at the ground segment. To contribute toward filling an information gap from EO big data to the ESA EO Level 2 product, an original Stage 4 validation (Val) of the Satellite Image Automatic Mapper (SIAM) lightweight computer program was conducted by independent means on an annual Web-Enabled Landsat Data (WELD) image composite time-series of the conterminous U.S. The core of SIAM is a one pass prior knowledge based decision tree for MS reflectance space hyperpolyhedralization into static color names presented in literature in recent years. For the sake of readability this paper is split into two. The present Part 1 Theory provides the multidisciplinary background of a priori color naming in cognitive science, from linguistics to computer vision. To cope with dictionaries of MS color names and land cover class names that do not coincide and must be harmonized, an original hybrid guideline is proposed to identify a categorical variable pair relationship. An original quantitative measure of categorical variable pair association is also proposed. The subsequent Part 2 Validation discusses Stage 4 Val results collected by an original protocol for wall-to-wall thematic map quality assessment without sampling where the test and reference map legends can differ. Conclusions are that the SIAM-WELD maps instantiate a Level 2 SCM product whose legend is the 4 class taxonomy of the FAO Land Cover Classification System at the Dichotomous Phase Level 1 vegetation/nonvegetation and Level 2 terrestrial/aquatic.",0
"This study presents the stage four validation results of the Satellite Image Automatic Mapper (SIAM) computer program, which was developed as a part of the Earth Observation L2 Product Generation project. In this phase of testing, we focused on evaluating SIAM’s ability to accurately extract features from Landsat imagery and create high-quality land cover maps. The results showed that the program performed well across all three test sites, achieving overall accuracy scores ranging from 88% to 97%. These findings confirm the utility of SIAM as a tool for generating L2 products and demonstrate its potential as a valuable addition to the field of remote sensing.",1
"We aim to study the modeling limitations of the commonly employed boosted decision trees classifier. Inspired by the success of large, data-hungry visual recognition models (e.g. deep convolutional neural networks), this paper focuses on the relationship between modeling capacity of the weak learners, dataset size, and dataset properties. A set of novel experiments on the Caltech Pedestrian Detection benchmark results in the best known performance among non-CNN techniques while operating at fast run-time speed. Furthermore, the performance is on par with deep architectures (9.71% log-average miss rate), while using only HOG+LUV channels as features. The conclusions from this study are shown to generalize over different object detection domains as demonstrated on the FDDB face detection benchmark (93.37% accuracy). Despite the impressive performance, this study reveals the limited modeling capacity of the common boosted trees model, motivating a need for architectural changes in order to compete with multi-level and very deep architectures.",0
"In recent years, object detection has made significant progress due largely to advances in deep learning architectures such as convolutional neural networks (CNNs). One popular approach to object detection uses boosting techniques, specifically gradient boosting machines and decision trees (GBDTs), which have been shown to achieve state-of-the art performance on several benchmark datasets. However, there remains little understanding of the exact mechanism behind their success and how they compare with other approaches. This work aims to provide insights into these questions by conducting an analysis of boosted tree ensembles on two important dimensions: accuracy versus speed/memory usage trade-offs, and model interpretability/explainability. Our experiments show that GBDTs offer higher precision at a given recall compared to traditional CNN models, but come with higher computational costs and memory requirements. Additionally, we demonstrate that the use of pre-training can greatly improve efficiency without sacrificing accuracy. Finally, we investigate the effectiveness of different methods for visualizing and explaining decisions made by boosted trees, highlighting some limitations and potential pitfalls associated with common approaches. Overall, our findings indicate that while boosted tree ensembles may provide benefits over CNNs under certain circumstances, it is essential to carefully evaluate the advantages and disadvantages before choosing an architecture for deployment.",1
"Classification, which involves finding rules that partition a given data set into disjoint groups, is one class of data mining problems. Approaches proposed so far for mining classification rules for large databases are mainly decision tree based symbolic learning methods. The connectionist approach based on neural networks has been thought not well suited for data mining. One of the major reasons cited is that knowledge generated by neural networks is not explicitly represented in the form of rules suitable for verification or interpretation by humans. This paper examines this issue. With our newly developed algorithms, rules which are similar to, or more concise than those generated by the symbolic methods can be extracted from the neural networks. The data mining process using neural networks with the emphasis on rule extraction is described. Experimental results and comparison with previously published works are presented.",0
"This paper presents a new method called ""NeuroRule"" that combines connectionist models with rule mining algorithms in order to improve data analysis by identifying patterns and relationships within large datasets. By incorporating both connectionist models and traditional rule mining methods, we aim to develop more comprehensive and accurate rules from complex datasets than either approach alone can provide. We demonstrate the potential of our technique using several realworld examples and show how it outperforms traditional rule mining methods on these cases. Overall, our work shows promise for improving data analysis and providing valuable insights into complex datasets through the use of neural network models in combination with classical statistical techniques.",1
"Big data trend has enforced the data-centric systems to have continuous fast data streams. In recent years, real-time analytics on stream data has formed into a new research field, which aims to answer queries about what-is-happening-now with a negligible delay. The real challenge with real-time stream data processing is that it is impossible to store instances of data, and therefore online analytical algorithms are utilized. To perform real-time analytics, pre-processing of data should be performed in a way that only a short summary of stream is stored in main memory. In addition, due to high speed of arrival, average processing time for each instance of data should be in such a way that incoming instances are not lost without being captured. Lastly, the learner needs to provide high analytical accuracy measures. Sentinel is a distributed system written in Java that aims to solve this challenge by enforcing both the processing and learning process to be done in distributed form. Sentinel is built on top of Apache Storm, a distributed computing platform. Sentinels learner, Vertical Hoeffding Tree, is a parallel decision tree-learning algorithm based on the VFDT, with ability of enabling parallel classification in distributed environments. Sentinel also uses SpaceSaving to keep a summary of the data stream and stores its summary in a synopsis data structure. Application of Sentinel on Twitter Public Stream API is shown and the results are discussed.",0
"This research explores the use of distributed real-time sentiment analysis on big data social streams. Traditionally, sentiment analysis has been performed offline using large datasets that have been collected over time, but the volume and velocity at which data is generated today requires more efficient methods for processing and analyzing sentiments. By utilizing distributed computing techniques, we can perform sentiment analysis in real-time as new data arrives, allowing organizations to quickly react to customer opinions and feedback. We present a distributed approach based on Apache Flink streaming engine to process raw tweets from Twitter firehose into a compact format suitable for sentiment analysis. Experimental results show the efficiency and scalability of our system in terms of throughput and latency, while maintaining high accuracy in sentiment detection. Our work demonstrates the feasibility of implementing real-time distributed systems for sentiment analysis on large-scale social media streams.",1
"Decision tree is an important method for both induction research and data mining, which is mainly used for model classification and prediction. ID3 algorithm is the most widely used algorithm in the decision tree so far. In this paper, the shortcoming of ID3's inclining to choose attributes with many values is discussed, and then a new decision tree algorithm which is improved version of ID3. In our proposed algorithm attributes are divided into groups and then we apply the selection measure 5 for these groups. If information gain is not good then again divide attributes values into groups. These steps are done until we get good classification/misclassification ratio. The proposed algorithms classify the data sets more accurately and efficiently.",0
"This paper presents a new method for classification of datasets for data mining that achieves high accuracy while maintaining interpretability of results. The proposed method uses feature selection techniques along with machine learning algorithms to achieve accurate predictions on large and complex datasets. The features selected by our method have meaningful relationships to the labels, which allows for easier understanding of the model’s decision making process. Our experiments demonstrate significant improvements over traditional methods in terms of both prediction accuracy and interpretation of results. We evaluate our approach on several benchmark datasets used for testing classification models and show that our method produces better results than state-of-the-art approaches. In conclusion, the proposed method provides an effective solution to classify datasets in data mining applications where interpreting results is as important as accuracy.",1
"Recent work in model-agnostic explanations of black-box machine learning has demonstrated that interpretability of complex models does not have to come at the cost of accuracy or model flexibility. However, it is not clear what kind of explanations, such as linear models, decision trees, and rule lists, are the appropriate family to consider, and different tasks and models may benefit from different kinds of explanations. Instead of picking a single family of representations, in this work we propose to use ""programs"" as model-agnostic explanations. We show that small programs can be expressive yet intuitive as explanations, and generalize over a number of existing interpretable families. We propose a prototype program induction method based on simulated annealing that approximates the local behavior of black-box classifiers around a specific prediction using random perturbations. Finally, we present preliminary application on small datasets and show that the generated explanations are intuitive and accurate for a number of classifiers.",0
"In this paper, we explore the use of machine learning models as black box explanations. We show how these models can provide insights into complex systems without requiring a full understanding of their underlying mechanisms. Our approach is based on the idea that simple statistical patterns in data can capture surprisingly nuanced features of real-world phenomena. By training our model on large amounts of data from diverse sources, we have achieved state-of-the-art performance across many domains. We believe that this work has important implications for fields such as medicine, finance, and engineering, where experts often make decisions based on limited or incomplete knowledge. Ultimately, we hope that our research will lead to more reliable decision making and improved outcomes.",1
"Models obtained by decision tree induction techniques excel in being interpretable.However, they can be prone to overfitting, which results in a low predictive performance. Ensemble techniques are able to achieve a higher accuracy. However, this comes at a cost of losing interpretability of the resulting model. This makes ensemble techniques impractical in applications where decision support, instead of decision making, is crucial.   To bridge this gap, we present the GENESIM algorithm that transforms an ensemble of decision trees to a single decision tree with an enhanced predictive performance by using a genetic algorithm. We compared GENESIM to prevalent decision tree induction and ensemble techniques using twelve publicly available data sets. The results show that GENESIM achieves a better predictive performance on most of these data sets than decision tree induction techniques and a predictive performance in the same order of magnitude as the ensemble techniques. Moreover, the resulting model of GENESIM has a very low complexity, making it very interpretable, in contrast to ensemble techniques.",0
"Here is a draft of your abstract. I hope it meets your needs! Remember that you can always ask me for more changes if necessary. If you want, I could create another version focusing on different points. ---  GENESIM (Genetic Extraction Of a Single Interpretable Model) takes into account three main points. Firstly, how we select which parameters affect our output: using mutual information. Secondly, we will see how we choose these parameters ourselves: a simple neural network. Finally, while doing all this we must consider runtime efficiency: hence we take advantage of sparsity as well as batch normalization. We implement these principles together under one framework so users may easily employ them towards their problem at hand. GENESIM combines these benefits through random search and evolutionary computation for hyperparameter tuning, leading to effective use over many regression benchmark datasets. The effectiveness of the approach stems from combining strong individual parts and applying each step carefully; selecting good architectures along side ensuring robustness via high enough fidelities leads to very competitive performance both quantitatively and visually. We showcase these abilities by comparing results against several recent models both qualitatively and quantitatively.",1
"We present a simple unified framework for multi-class cost-sensitive boosting. The minimum-risk class is estimated directly, rather than via an approximation of the posterior distribution. Our method jointly optimizes binary weak learners and their corresponding output vectors, requiring classes to share features at each iteration. By training in a cost-sensitive manner, weak learners are invested in separating classes whose discrimination is important, at the expense of less relevant classification boundaries. Additional contributions are a family of loss functions along with proof that our algorithm is Boostable in the theoretical sense, as well as an efficient procedure for growing decision trees for use as weak learners. We evaluate our method on a variety of datasets: a collection of synthetic planar data, common UCI datasets, MNIST digits, SUN scenes, and CUB-200 birds. Results show state-of-the-art performance across all datasets against several strong baselines, including non-boosting multi-class approaches.",0
"This paper proposes a novel approach for improving multi-class cost-sensitive boosting by estimating the minimum-risk class. The proposed method is designed to address the shortcomings of existing methods that often fail to accurately estimate the risk associated with each class. By using a robust estimation technique, our method can effectively identify the most important features for classification while accounting for their potential costs. Experimental results on several benchmark datasets demonstrate that our approach significantly outperforms state-of-the-art cost-sensitive learning algorithms in terms of both accuracy and F1 score. Our findings suggest that the proposed method provides a promising direction for developing more effective and efficient machine learning models in real-world applications where accurate prediction and cost awareness are crucial factors.",1
"It is widely believed that the prediction accuracy of decision tree models is invariant under any strictly monotone transformation of the individual predictor variables. However, this statement may be false when predicting new observations with values that were not seen in the training-set and are close to the location of the split point of a tree rule. The sensitivity of the prediction error to the split point interpolation is high when the split point of the tree is estimated based on very few observations, reaching 9% misclassification error when only 10 observations are used for constructing a split, and shrinking to 1% when relying on 100 observations. This study compares the performance of alternative methods for split point interpolation and concludes that the best choice is taking the mid-point between the two closest points to the split point of the tree. Furthermore, if the (continuous) distribution of the predictor variable is known, then using its probability integral for transforming the variable (""quantile transformation"") will reduce the model's interpolation error by up to about a half on average. Accordingly, this study provides guidelines for both developers and users of decision tree models (including bagging and random forest).",0
"Predicting outcomes based on data can often involve multiple variables, but sometimes these variables are interrelated or redundant, leading to poorer predictions. This study examines one method of improving prediction accuracy by applying transformations to certain predictors, specifically those that follow a monotonic relationship with the outcome variable. By converting these monotone predictors into new forms, researchers were able to create more accurate decision trees using machine learning algorithms. The results suggest that this approach could significantly enhance model performance in many fields where predictive models rely on complex relationships among predictors. Further work should explore additional methods to identify suitable monotone transformations for different types of datasets.",1
"Decision tree (and its extensions such as Gradient Boosting Decision Trees and Random Forest) is a widely used machine learning algorithm, due to its practical effectiveness and model interpretability. With the emergence of big data, there is an increasing need to parallelize the training process of decision tree. However, most existing attempts along this line suffer from high communication costs. In this paper, we propose a new algorithm, called \emph{Parallel Voting Decision Tree (PV-Tree)}, to tackle this challenge. After partitioning the training data onto a number of (e.g., $M$) machines, this algorithm performs both local voting and global voting in each iteration. For local voting, the top-$k$ attributes are selected from each machine according to its local data. Then, globally top-$2k$ attributes are determined by a majority voting among these local candidates. Finally, the full-grained histograms of the globally top-$2k$ attributes are collected from local machines in order to identify the best (most informative) attribute and its split point. PV-Tree can achieve a very low communication cost (independent of the total number of attributes) and thus can scale out very well. Furthermore, theoretical analysis shows that this algorithm can learn a near optimal decision tree, since it can find the best attribute with a large probability. Our experiments on real-world datasets show that PV-Tree significantly outperforms the existing parallel decision tree algorithms in the trade-off between accuracy and efficiency.",0
"This paper presents a new parallel algorithm for constructing decision trees that significantly reduces communication overhead while maintaining accuracy. The proposed algorithm leverages recent advances in distributed machine learning by using a gradient-based approach that minimizes cross-device communication. By only communicating the gradients instead of entire models, our method can scale to large datasets without incurring prohibitive latency or bandwidth costs. Experimental results on real-world data sets show that our algorithm achieves state-of-the-art performance in terms of both speed and model quality, making it ideal for use in big data settings where high throughput is required. Our work demonstrates how to balance parallelism and efficiency in distributed systems, opening up promising research directions for scaling machine learning algorithms.",1
"In recent years, dynamically growing data and incrementally growing number of classes pose new challenges to large-scale data classification research. Most traditional methods struggle to balance the precision and computational burden when data and its number of classes increased. However, some methods are with weak precision, and the others are time-consuming. In this paper, we propose an incremental learning method, namely, heterogeneous incremental Nearest Class Mean Random Forest (hi-RF), to handle this issue. It is a heterogeneous method that either replaces trees or updates trees leaves in the random forest adaptively, to reduce the computational time in comparable performance, when data of new classes arrive. Specifically, to keep the accuracy, one proportion of trees are replaced by new NCM decision trees; to reduce the computational load, the rest trees are updated their leaves probabilities only. Most of all, out-of-bag estimation and out-of-bag boosting are proposed to balance the accuracy and the computational efficiency. Fair experiments were conducted and demonstrated its comparable precision with much less computational time.",0
"Hi-RF (Highly Incremental Learning Random Forest) presents an efficient framework for building high-quality random forest classifiers on large-scale datasets containing multiple classes. Our approach takes advantage of incremental learning techniques, where each tree update only considers new training data instead of retraining from scratch. This leads to significant improvements in both computational efficiency and model quality compared to existing methods. We demonstrate the effectiveness of our method across a range of diverse datasets, achieving competitive results while substantially reducing training time. Finally, we provide analysis and insights into the behavior of hi-RF, highlighting its ability to adapt to changes in the distribution of incoming data over time. Overall, our work represents a valuable contribution towards addressing scalability challenges in machine learning by providing a more efficient solution for handling big data problems involving numerous classes.",1
"This paper proposes a client-server decision tree learning method for outsourced private data. The privacy model is anatomization/fragmentation: the server sees data values, but the link between sensitive and identifying information is encrypted with a key known only to clients. Clients have limited processing and storage capability. Both sensitive and identifying information thus are stored on the server. The approach presented also retains most processing at the server, and client-side processing is amortized over predictions made by the clients. Experiments on various datasets show that the method produces decision trees approaching the accuracy of a non-private decision tree, while substantially reducing the client's computing resource requirements.",0
"Decision tree classification has been shown to be effective at solving many different kinds of data problems. This paper takes advantage of decision trees by classifying outsourcing data into two categories: low risk and high risk. Our method involves splitting the dataset based on predefined features, such as distance from the customer’s location and reputation ratings given by previous customers. By creating these models and then comparing them against one another, we can better predict which projects should be kept internal and which should be sent outside. We experimented with both random forest and support vector machines but found that our Decision Trees performed significantly better than other methods. We hope our work leads to further development into using decision trees to improve business decisions.",1
"The aim of this work is to classify the aerospace structure defects detected by eddy current non-destructive testing. The proposed method is based on the assumption that the defect is bound to the reaction of the probe coil impedance during the test. Impedance plane analysis is used to extract a feature vector from the shape of the coil impedance in the complex plane, through the use of some geometric parameters. Shape recognition is tested with three different machine-learning based classifiers: decision trees, neural networks and Naive Bayes. The performance of the proposed detection system are measured in terms of accuracy, sensitivity, specificity, precision and Matthews correlation coefficient. Several experiments are performed on dataset of eddy current signal samples for aircraft structures. The obtained results demonstrate the usefulness of our approach and the competiveness against existing descriptors.",0
"Automatic quality inspection requires accurate detection and discrimination of different types of faults that can occur in manufactured products. In Non-Destructive Testing (NDT), these faults are typically located by analyzing the changes they produce on the product’s exterior, such as visual distortions, color variations or density differences. Here we present an original approach to NDT where shape analysis is used to identify two common types of mechanical damage – dents and corrosion pits. We applied our methodology to X-ray images taken from radiographic inspections of metallic parts and found that it was able to correctly classify them into dented and non-dented categories at very high accuracy rates, demonstrating the robustness of our system against variations like scale, contrast and rotation. These results have positive implications both for automated NDT procedures and future research into automatic feature extraction and pattern recognition tasks within the domain. -----",1
"We describe the solution of team ISMLL for the ECML-PKDD 2016 Discovery Challenge on Bank Card Usage for both tasks. Our solution is based on three pillars. Gradient boosted decision trees as a strong regression and classification model, an intensive search for good hyperparameter configurations and strong features that exploit geolocation information. This approach achieved the best performance on the public leaderboard for the first task and a decent fourth position for the second task.",0
"Geolocation data has become increasingly available through mobile devices such as smartphones and wearables, providing rich contextual information that can help predict consumer behavior. This study focuses on utilizing geolocation data derived from credit card transactions to forecast individual bank card usage patterns. The proposed model is evaluated using two different approaches: a baseline linear regression model and a deep learning framework based on recurrent neural networks (RNNs). Results show that incorporating geolocation features significantly improves prediction accuracy, and RNN models perform better than traditional linear regression methods. Furthermore, our analysis indicates that user activity levels and transaction volume change according to time zones rather than actual clock times, highlighting the importance of considering nonlinearities within dynamic spatio-temporal dimensions. These findings provide valuable insights into understanding customer decision-making processes and may aid financial institutions in offering personalized services tailored to specific needs.",1
"Table (database) / Relational database Classification for big/smart/fast data machine learning is one of the most important tasks of predictive analytics and extracting valuable information from data. It is core applied technique for what now understood under data science and/or artificial intelligence. Widely used Decision Tree (Random Forest) and rare used rule based PRISM , VFST, etc classifiers are empirical substitutions of theoretically correct to use Boolean functions minimization. Developing Minimization of Boolean functions algorithms is started long time ago by Edward Veitch's 1952. Since it, big efforts by wide scientific/industrial community was done to find feasible solution of Boolean functions minimization. In this paper we propose consider table data classification from mathematical point of view, as minimization of Boolean functions. It is shown that data representation may be transformed to Boolean functions form and how to use known algorithms. For simplicity, binary output function is used for development, what opens doors for multivalued outputs developments.",0
"This paper presents methods for improving accuracy in large scale machine learning tasks through preprocessing techniques such as feature selection and data augmentation, along with novel model architectures. We demonstrate that by carefully selecting features and training on more diverse sets of examples, we can achieve state-of-the-art results on a number of benchmark datasets. Our approach is evaluated using metrics commonly used in machine learning research, including accuracy, precision, recall, F1 score, and area under the receiver operating characteristic curve (AUC ROC). These evaluations show that our method consistently outperforms baseline models trained without preprocessing, as well as other recently published approaches. In conclusion, these findings suggest that careful consideration of input data and model architecture is crucial for achieving high performance in modern machine learning applications.",1
"Stochastic gradient-boosted decision trees are widely employed for multivariate classification and regression tasks. This paper presents a speed-optimized and cache-friendly implementation for multivariate classification called FastBDT. FastBDT is one order of magnitude faster during the fitting-phase and application-phase, in comparison with popular implementations in software frameworks like TMVA, scikit-learn and XGBoost. The concepts used to optimize the execution time and performance studies are discussed in detail in this paper. The key ideas include: An equal-frequency binning on the input data, which allows replacing expensive floating-point with integer operations, while at the same time increasing the quality of the classification; a cache-friendly linear access pattern to the input data, in contrast to usual implementations, which exhibit a random access pattern. FastBDT provides interfaces to C/C++, Python and TMVA. It is extensively used in the field of high energy physics by the Belle II experiment.",0
"This paper presents a new algorithm called FastBDT which improves upon existing implementations of stochastic gradient boosting by optimizing both computation time and memory usage during model training. By leveraging efficient data structures and parallel processing techniques, FastBDT achieves state-of-the-art performance on several benchmark datasets while maintaining competitive accuracy compared to other popular methods such as LightGBM and XGBoost. Furthermore, we demonstrate that our approach can scale effectively to large datasets while offering more control over hyperparameter tuning via cross-validation. Overall, FastBDT represents a significant advancement towards faster and more flexible machine learning models for multivariate classification tasks.",1
"The world's collective knowledge is evolving through research and new scientific discoveries. It is becoming increasingly difficult to objectively rank the impact research institutes have on global advancements. However, since the funding, governmental support, staff and students quality all mirror the projected quality of the institution, it becomes essential to measure the affiliation's rating in a transparent and widely accepted way. We propose and investigate several methods to rank affiliations based on the number of their accepted papers at future academic conferences. We carry out our investigation using publicly available datasets such as the Microsoft Academic Graph, a heterogeneous graph which contains various information about academic papers. We analyze several models, starting with a simple probabilities-based method and then gradually expand our training dataset, engineer many more features and use mixed models and gradient boosted decision trees models to improve our predictions.",0
"This paper presents a novel approach to predicting the future relevance of academic research institutions using data from multiple sources including Web of Science citations, university rankings, and online visibility metrics. We developed a machine learning model that uses these features to accurately forecast which institutions are likely to produce influential research over the next three years. Our method was evaluated on the task of predicting winner universities for the annual Knowledge Discovery and Data Mining (KDD) cup competition held by the ACM Special Interest Group on Knowledge Discovery and Data Mining (SIGKDD). Our model achieved top performance among all competitors, successfully predicting the winning institution ahead of time in both 2014 and 2015 before going on to win first place ourselves in 2016. These results demonstrate the effectiveness of our proposed approach for predicting the future impact of research institutions.",1
"This work explores the problem of exercise quality measurement since it is essential for effective management of diseases like cerebral palsy (CP). This work examines the assessment of quality of large amplitude movement (LAM) exercises designed to treat CP in an automated fashion. Exercise data was collected by trained participants to generate ideal examples to use as a positive samples for machine learning. Following that, subjects were asked to deliberately make subtle errors during the exercise, such as restricting movements, as is commonly seen in cases of patients suffering from CP. The quality measurement problem was then posed as a classification to determine whether an example exercise was either ""good"" or ""bad"". Popular machine learning techniques for classification, including support vector machines (SVM), single and doublelayered neural networks (NN), boosted decision trees, and dynamic time warping (DTW), were compared. The AdaBoosted tree performed best with an accuracy of 94.68% demonstrating the feasibility of assessing exercise quality.",0
"In today’s era of technological advancement, there has been a significant shift towards online learning platforms. With the abundance of exercises available online, it becomes essential to measure their quality to ensure that learners receive effective and efficient education. This study aims at measuring the quality of exercises by developing criteria based on previous research, pedagogical principles and expert opinions. Furthermore, we analyzed over 249 science exercises across various websites and compared them against our established criteria. Our findings indicate that while many exercises meet some standards, only a few fully comply with all criteria. As such, this study highlights areas for improvement among developers and educators alike in ensuring high-quality educational material. Overall, our work provides insight into how teachers can evaluate the quality of exercises before incorporating them into their classrooms, thereby enhancing student engagement and success.",1
"In most practical problems of classifier learning, the training data suffers from the label noise. Hence, it is important to understand how robust is a learning algorithm to such label noise. This paper presents some theoretical analysis to show that many popular decision tree algorithms are robust to symmetric label noise under large sample size. We also present some sample complexity results which provide some bounds on the sample size for the robustness to hold with a high probability. Through extensive simulations we illustrate this robustness.",0
"This work presents new theoretical results that show how decision tree models can adapt to noisy labels. We show through analytical and numerical investigations on two data sets that noise may have both positive and negative effects depending on specific conditions. When noise leads to label switching, decision trees tend to assign less weight to some of the features which were overfitted before. Our analysis reveals that decision trees are robust against noise of certain levels but become unstable at higher noise intensity. Results demonstrate an interesting interplay among regularization effect, nonlinearity of class boundaries, complexity of decision trees, as well as correlation among features and the strength of noise. These findings provide insights into understanding the behavior of decision trees in realistic applications where noisy annotations exist. Furthermore, we introduce simple procedures to mitigate these negative impacts based on our observations, which could benefit researchers working in the field of machine learning in practice. Overall, our contributions constitute valuable knowledge towards improving the quality of data annotation and refining the training process, boosting the reliability of artificial intelligence systems. =====  This study examines the performance of decision tree algorithms in the presence of label noise. Using mathematical derivation and empirical evaluation, we explore the impact of varying levels of noise on model accuracy and stability. Contrary to previous assumptions, our results indicate that moderate levels of noise can actually enhance the robustness of decision trees by reducing feature overfitting. However, high levels of noise cause instability and reduced predictive power. Our findings offer valuable insights into understanding the complex relationship between decision trees, data quality, and algorithm design. To address the negative effects of noise, we propose several practical strategies grounded in our investigation outcomes. Ultimately, our research contributes essential knowledge aimed at improving data curation practices and optimizing machine learning pipelines, thereby enhancing the trustworthiness of computational intelligence systems. By elucidating this critical aspect of data-driven technologies, we hope to advance the integrity and reliability of artificial intelligence.",1
"The construction of efficient and effective decision trees remains a key topic in machine learning because of their simplicity and flexibility. A lot of heuristic algorithms have been proposed to construct near-optimal decision trees. ID3, C4.5 and CART are classical decision tree algorithms and the split criteria they used are Shannon entropy, Gain Ratio and Gini index respectively. All the split criteria seem to be independent, actually, they can be unified in a Tsallis entropy framework. Tsallis entropy is a generalization of Shannon entropy and provides a new approach to enhance decision trees' performance with an adjustable parameter $q$. In this paper, a Tsallis Entropy Criterion (TEC) algorithm is proposed to unify Shannon entropy, Gain Ratio and Gini index, which generalizes the split criteria of decision trees. More importantly, we reveal the relations between Tsallis entropy with different $q$ and other split criteria. Experimental results on UCI data sets indicate that the TEC algorithm achieves statistically significant improvement over the classical algorithms.",0
"Title: Unifying Decision Tree Split Criteria Using Tsallis Entropy Abstract This paper presents a new method for constructing decision trees that unifies several existing split criteria using a novel entropy measure based on the Tsallis entropy function. The proposed approach extends classical information theory by considering nonlinear relationships between variables and incorporating these into the splitting process. We show how our method can improve tree quality by reducing variance and increasing stability compared to traditional methods. Our experiments demonstrate the effectiveness of the proposed method across multiple datasets from diverse domains, including image classification, text analysis, and bioinformatics. Furthermore, we provide insights into the performance of different splits under varying conditions, which highlights the importance of choosing appropriate criterion depending on problem characteristics. Overall, our work contributes to advancing the state-of-the-art in decision tree construction and provides practitioners with powerful tools to build more accurate models.",1
"Areas where Artificial Intelligence (AI) & related fields are finding their applications are increasing day by day, moving from core areas of computer science they are finding their applications in various other domains.In recent times Machine Learning i.e. a sub-domain of AI has been widely used in order to assist medical experts and doctors in the prediction, diagnosis and prognosis of various diseases and other medical disorders. In this manuscript the authors applied various machine learning algorithms to a problem in the domain of medical diagnosis and analyzed their efficiency in predicting the results. The problem selected for the study is the diagnosis of the Chronic Kidney Disease.The dataset used for the study consists of 400 instances and 24 attributes. The authors evaluated 12 classification techniques by applying them to the Chronic Kidney Disease data. In order to calculate efficiency, results of the prediction by candidate methods were compared with the actual medical results of the subject.The various metrics used for performance evaluation are predictive accuracy, precision, sensitivity and specificity. The results indicate that decision-tree performed best with nearly the accuracy of 98.6%, sensitivity of 0.9720, precision of 1 and specificity of 1.",0
"This research aims to evaluate the performance of various machine learning classification techniques for chronic kidney disease diagnosis using a large dataset from a hospital in India. The study compares the accuracy and efficiency of several algorithms such as logistic regression, decision trees, random forest, support vector machines (SVM), K-nearest neighbors (KNN), Naive Bayes, artificial neural networks (ANN) and gradient boosting machine (GBM). The evaluation metrics included sensitivity, specificity, precision, recall, F1 score, area under the receiver operating characteristic curve (AUROC) and accuracy. Additionally, the feature importances were calculated to determine which features contribute most to the predictions made by each algorithm. Overall, GBM was found to perform better than other methods across all evaluation metrics. However, there were variations observed among different models based on the complexity of the data and the number of features used. These findings suggest that careful selection of appropriate classification algorithms can lead to improved diagnostic accuracy for CKD patients, ultimately leading to earlier detection and more effective treatment outcomes.",1
"Ecological Momentary Assessment (EMA) data is organized in multiple levels (per-subject, per-day, etc.) and this particular structure should be taken into account in machine learning algorithms used in EMA like decision trees and its variants. We propose a new algorithm called BBT (standing for Bagged Boosted Trees) that is enhanced by a over/under sampling method and can provide better estimates for the conditional class probability function. Experimental results on a real-world dataset show that BBT can benefit EMA data classification and performance.",0
"This paper presents a method for classifying data collected through ecological momentary assessments (EMAs) using bagged boosted trees. EMAs have become an increasingly popular tool for collecting detailed information on individuals’ experiences and behaviors in real time, but analyzing these datasets can pose challenges due to their high dimensionality, nonlinear relationships, and potential for missing values. We demonstrate that our method effectively handles these issues while providing accurate predictions across multiple datasets. Our results suggest that bagged boosted trees offer a powerful approach for classifying EMA data and can enhance understanding of complex human behavior patterns.",1
"This paper introduces a fast and efficient segmentation technique for 2D images and 3D point clouds of building facades. Facades of buildings are highly structured and consequently most methods that have been proposed for this problem aim to make use of this strong prior information. Contrary to most prior work, we are describing a system that is almost domain independent and consists of standard segmentation methods. We train a sequence of boosted decision trees using auto-context features. This is learned using stacked generalization. We find that this technique performs better, or comparable with all previous published methods and present empirical results on all available 2D and 3D facade benchmark datasets. The proposed method is simple to implement, easy to extend, and very efficient at test-time inference.",0
"This paper presents an efficient method for segmenting facades in both 2D and 3D architectural images using auto-context. The proposed approach takes advantage of the rich contextual information present in these types of images to accurately identify building facades while minimizing user input and computational resources. We propose a novel graph cut algorithm that uses dynamic programming techniques to efficiently solve the optimization problem. Experimental results demonstrate that our method outperforms state-of-the-art approaches in terms of accuracy and efficiency. Our method has applications in computer vision, robotics, and urban planning.",1
"In many healthcare settings, intuitive decision rules for risk stratification can help effective hospital resource allocation. This paper introduces a novel variant of decision tree algorithms that produces a chain of decisions, not a general tree. Our algorithm, $\alpha$-Carving Decision Chain (ACDC), sequentially carves out ""pure"" subsets of the majority class examples. The resulting chain of decision rules yields a pure subset of the minority class examples. Our approach is particularly effective in exploring large and class-imbalanced health datasets. Moreover, ACDC provides an interactive interpretation in conjunction with visual performance metrics such as Receiver Operating Characteristics curve and Lift chart.",0
"Accurately determining whether patients have cancerous nodules on their thyroid glands is crucial for effective risk stratification and management. The ACDC approach proposed here offers robust segmentation by taking advantage of both shape and appearance features. By combining these two types of descriptors, we achieve state-of-the-art performance that improves upon previous methods, which were limited by overfitting caused by relying solely on one type of feature descriptor. Our methodology enables more accurate assessment of whether tumours exist, enabling better prediction of patient outcomes. This leads directly to improved care delivery through more efficient utilization of medical resources (i.e., patients at high-risk can be identified earlier). As such, our work advances healthcare research and practice with respect to decision making and resource allocation, particularly for the most vulnerable populations who rely heavily on public systems. In summary, this project seeks to improve the accuracy of diagnoses and ultimately reduce wait times for low risk individuals while ensuring those requiring specialized intervention receive it. Overall, ACDC has significant positive impacts on individual lives as well as on society. We hope that the framework presented in this article can serve as a foundation for further studies across different imaging domains.",1
"We propose using five data-driven community detection approaches from social networks to partition the label space for the task of multi-label classification as an alternative to random partitioning into equal subsets as performed by RAkELd: modularity-maximizing fastgreedy and leading eigenvector, infomap, walktrap and label propagation algorithms. We construct a label co-occurence graph (both weighted an unweighted versions) based on training data and perform community detection to partition the label set. We include Binary Relevance and Label Powerset classification methods for comparison. We use gini-index based Decision Trees as the base classifier. We compare educated approaches to label space divisions against random baselines on 12 benchmark data sets over five evaluation measures. We show that in almost all cases seven educated guess approaches are more likely to outperform RAkELd than otherwise in all measures, but Hamming Loss. We show that fastgreedy and walktrap community detection methods on weighted label co-occurence graphs are 85-92% more likely to yield better F1 scores than random partitioning. Infomap on the unweighted label co-occurence graphs is on average 90% of the times better than random paritioning in terms of Subset Accuracy and 89% when it comes to Jaccard similarity. Weighted fastgreedy is better on average than RAkELd when it comes to Hamming Loss.",0
"Title: ""Data Driven Approach Better Than Random Choice In Label Space Division""  This study examines the effectiveness of using a data driven approach versus a random choice methodology in label space division for multi-label classification problems. Multi-label learning is characterized by multiple labels that can be assigned to each instance. Accurate predictions require understanding which instances belong to different classes. An algorithm must provide high quality labels in order to improve accuracy. Previous methods have used a greedy approach that utilizes heuristics to maximize overall accuracy while minimizing computation time and memory usage. However, these approaches may fall short as they rely on ad hoc assumptions rather than hard evidence from real world data. We propose an alternative solution based on a data driven approach utilizing the Mutual Information metric. Results show our proposed method outperforms previous state-of-the-art techniques across several benchmark datasets. Our contribution provides new insights into how one should divide label spaces in practice; improving efficiency without compromising accuracy.",1
"For any positive integer $k$, there exist neural networks with $\Theta(k^3)$ layers, $\Theta(1)$ nodes per layer, and $\Theta(1)$ distinct parameters which can not be approximated by networks with $\mathcal{O}(k)$ layers unless they are exponentially large --- they must possess $\Omega(2^k)$ nodes. This result is proved here for a class of nodes termed ""semi-algebraic gates"" which includes the common choices of ReLU, maximum, indicator, and piecewise polynomial functions, therefore establishing benefits of depth against not just standard networks with ReLU gates, but also convolutional networks with ReLU and maximization gates, sum-product networks, and boosted decision trees (in this last case with a stronger separation: $\Omega(2^{k^3})$ total tree nodes are required).",0
"Deep learning has revolutionized the field of Artificial Intelligence by enabling machines to learn from vast amounts of data. At the heart of deep learning lies neural networks which have been shown to achieve state of the art results across multiple domains such as image recognition, speech recognition, natural language processing etc. One key aspect that distinguishes these deep neural networks is their ability to extract features at different layers thus allowing them to model complex representations of the input. Depth in neural networks comes with additional parameters resulting in overfitting if not regularized properly. However our experiments show that increased depth can lead to improved performance even after introducing L2 regularization. Our work shows that increasing depth allows a network to capture more contextual relationships present in the data leading to better generalization. Furthermore we provide theoretical analysis showing how deeper networks tend to escape vanishing gradient problem during backpropagation hence making it easier to optimize. We believe this analysis provides good intuitions on why deep neural nets perform well in practice. Through extensive experimentations using MNIST , CIFAR-10 and ImageNet datasets, we confirm our hypothesis that increased depth can result in improved performance. In summary, depth plays a crucial role in enabling neural networks to capture richer and more nuanced relationships in high dimensional spaces and we demonstrate that it brings significant benefits in terms of both empirical performance and optimization robustness.",1
"We analyze the performance of the top-down multiclass classification algorithm for decision tree learning called LOMtree, recently proposed in the literature Choromanska and Langford (2014) for solving efficiently classification problems with very large number of classes. The algorithm online optimizes the objective function which simultaneously controls the depth of the tree and its statistical accuracy. We prove important properties of this objective and explore its connection to three well-known entropy-based decision tree objectives, i.e. Shannon entropy, Gini-entropy and its modified version, for which instead online optimization schemes were not yet developed. We show, via boosting-type guarantees, that maximizing the considered objective leads also to the reduction of all of these entropy-based objectives. The bounds we obtain critically depend on the strong-concavity properties of the entropy-based criteria, where the mildest dependence on the number of classes (only logarithmic) corresponds to the Shannon entropy.",0
"This research investigates how well a popular machine learning model works on hard problems and suggests ways to improve it by building more complex models on top of it. We evaluate several state-of-the-art algorithms that address these issues and find one particular method outperforms all others at improving the base model. Our results demonstrate the feasibility and benefits of using such methods to solve challenging real world problems where other approaches have failed. The approach we investigate is particularly interesting as it does not require additional training data, instead relying only on changes to the architecture of the underlying model to achieve significant gains in performance.",1
"Accurate prediction of suicide risk in mental health patients remains an open problem. Existing methods including clinician judgments have acceptable sensitivity, but yield many false positives. Exploiting administrative data has a great potential, but the data has high dimensionality and redundancies in the recording processes. We investigate the efficacy of three most effective randomized machine learning techniques random forests, gradient boosting machines, and deep neural nets with dropout in predicting suicide risk. Using a cohort of mental health patients from a regional Australian hospital, we compare the predictive performance with popular traditional approaches clinician judgments based on a checklist, sparse logistic regression and decision trees. The randomized methods demonstrated robustness against data redundancies and superior predictive performance on AUC and F-measure.",0
"This should cover both technical aspects as well as possible implications and applications ------------> In recent years, there has been increasing interest in using artificial intelligence (AI) techniques such as machine learning to predict suicide risk. However, most studies have used small, homogeneous samples which may not generalize to real world populations where data quality can vary widely across individuals. Moreover, many existing algorithms are highly sensitive to noise and outliers making them unsuitable for use with incomplete or imperfect data sets common in routine clinical practice. To address these limitations we evaluate multiple approaches for handling missing values within random forest models by investigating predictive accuracy against two separate reference standards using large administrative datasets linked to standardized risk assessments (n=6840). We find that imputation generally improves prediction compared to either complete case analysis or deletion, although some method variations perform better than others. Our work extends prior research by demonstrating high predictive performance on administrative data despite variable data quality and nonrandom missingness patterns in risk factors. Our results suggest that carefully selected machine learning algorithms could provide valuable support for clinicians working to identify those at highest risk for future self harm even without ideal study conditions. Future directions include applying these approaches to other mental health predictions like treatment response or relapse in depression.",1
"Predicting trends in stock market prices has been an area of interest for researchers for many years due to its complex and dynamic nature. Intrinsic volatility in stock market across the globe makes the task of prediction challenging. Forecasting and diffusion modeling, although effective can't be the panacea to the diverse range of problems encountered in prediction, short-term or otherwise. Market risk, strongly correlated with forecasting errors, needs to be minimized to ensure minimal risk in investment. The authors propose to minimize forecasting error by treating the forecasting problem as a classification problem, a popular suite of algorithms in Machine learning. In this paper, we propose a novel way to minimize the risk of investment in stock market by predicting the returns of a stock using a class of powerful machine learning algorithms known as ensemble learning. Some of the technical indicators such as Relative Strength Index (RSI), stochastic oscillator etc are used as inputs to train our model. The learning model used is an ensemble of multiple decision trees. The algorithm is shown to outperform existing algo- rithms found in the literature. Out of Bag (OOB) error estimates have been found to be encouraging. Key Words: Random Forest Classifier, stock price forecasting, Exponential smoothing, feature extraction, OOB error and convergence.",0
"This research paper investigates the use of random forest as a predictive model for forecasting the direction of stock market prices. In recent years, there has been significant interest in applying machine learning algorithms to financial markets due to their ability to handle large amounts of data and identify patterns that would otherwise go unnoticed by human analysts. However, the performance of these models can vary greatly depending on the dataset and methodology used.  The authors trained a random forest model on historical stock price data from a variety of companies across multiple industries. They then evaluated the accuracy of the model's predictions using several metrics, including accuracy, precision, recall, F1 score, and receiver operating characteristic (ROC) curves.  Results showed that the random forest model was able to accurately predict the direction of stock market prices with high precision and recall values. ROC curve analysis further confirmed the model's effectiveness at identifying positive samples and distinguishing them from negative ones. Overall, the results suggest that random forest is a promising tool for stock market prediction and warrants further investigation into refining the model for real-world applications.",1
"Decision tree classifiers are a widely used tool in data stream mining. The use of confidence intervals to estimate the gain associated with each split leads to very effective methods, like the popular Hoeffding tree algorithm. From a statistical viewpoint, the analysis of decision tree classifiers in a streaming setting requires knowing when enough new information has been collected to justify splitting a leaf. Although some of the issues in the statistical analysis of Hoeffding trees have been already clarified, a general and rigorous study of confidence intervals for splitting criteria is missing. We fill this gap by deriving accurate confidence intervals to estimate the splitting gain in decision tree learning with respect to three criteria: entropy, Gini index, and a third index proposed by Kearns and Mansour. Our confidence intervals depend in a more detailed way on the tree parameters. We also extend our confidence analysis to a selective sampling setting, in which the decision tree learner adaptively decides which labels to query in the stream. We furnish theoretical guarantee bounding the probability that the classification is non-optimal learning the decision tree via our selective sampling strategy. Experiments on real and synthetic data in a streaming setting show that our trees are indeed more accurate than trees with the same number of leaves generated by other techniques and our active learning module permits to save labeling cost. In addition, comparing our labeling strategy with recent methods, we show that our approach is more robust and consistent respect all the other techniques applied to incremental decision trees.",0
"Confident decision trees can improve data analysis by providing accurate predictions and enabling better model interpretability. However, constructing confident trees from streaming big data presents challenges due to limited resources and high computational complexity. This work proposes an online learning algorithm that generates confidence decision trees using both active and semi-active learning techniques. The algorithm adapts to changes in data distribution and maintains tree accuracy through pruning and updating. Experimental results demonstrate the effectiveness of our method in producing reliable models on large datasets while achieving competitive performance compared to state-of-the-art approaches. Our findings have applications across multiple domains where timely decisions must be made based on real-time streamed big data.",1
"Large medical image data sets with high dimensionality require substantial amount of computation time for data creation and data processing. This paper presents a novel generalized method that finds optimal image-based feature sets that reduce computational time complexity while maximizing overall classification accuracy for detection of diabetic retinopathy (DR). First, region-based and pixel-based features are extracted from fundus images for classification of DR lesions and vessel-like structures. Next, feature ranking strategies are used to distinguish the optimal classification feature sets. DR lesion and vessel classification accuracies are computed using the boosted decision tree and decision forest classifiers in the Microsoft Azure Machine Learning Studio platform, respectively. For images from the DIARETDB1 data set, 40 of its highest-ranked features are used to classify four DR lesion types with an average classification accuracy of 90.1% in 792 seconds. Also, for classification of red lesion regions and hemorrhages from microaneurysms, accuracies of 85% and 72% are observed, respectively. For images from STARE data set, 40 high-ranked features can classify minor blood vessels with an accuracy of 83.5% in 326 seconds. Such cloud-based fundus image analysis systems can significantly enhance the borderline classification performances in automated screening systems.",0
"This paper presents a cloud computing framework that allows large-scale fundus image data sets to be classified efficiently using state-of-the-art machine learning methods. The framework uses Amazon Web Services (AWS) services like SageMaker, EC2, and S3 to handle tasks such as model training, deployment, and storage of data. Our approach includes preprocessing steps such as image augmentation and normalization before feeding the data into the machine learning models. We evaluate our method on two publicly available datasets containing eye images and demonstrate improved accuracy compared to previous work. Overall, the proposed framework provides a scalable solution for handling large amounts of fundus image data while ensuring high levels of performance and efficiency. Keywords: Cloud Computing; Machine Learning; Fundus Images; Eye Disease Diagnosis Abstra",1
"The model of learning with \emph{local membership queries} interpolates between the PAC model and the membership queries model by allowing the learner to query the label of any example that is similar to an example in the training set. This model, recently proposed and studied by Awasthi, Feldman and Kanade, aims to facilitate practical use of membership queries.   We continue this line of work, proving both positive and negative results in the {\em distribution free} setting. We restrict to the boolean cube $\{-1, 1\}^n$, and say that a query is $q$-local if it is of a hamming distance $\le q$ from some training example. On the positive side, we show that $1$-local queries already give an additional strength, and allow to learn a certain type of DNF formulas. On the negative side, we show that even $\left(n^{0.99}\right)$-local queries cannot help to learn various classes including Automata, DNFs and more. Likewise, $q$-local queries for any constant $q$ cannot help to learn Juntas, Decision Trees, Sparse Polynomials and more. Moreover, for these classes, an algorithm that uses $\left(\log^{0.99}(n)\right)$-local queries would lead to a breakthrough in the best known running times.",0
"This paper presents a new method for distributed learning that utilizes local queries. Our approach differs from existing methods by allowing learners to have different datasets and varying levels of data quality. We show experimentally that our algorithm can outperform state-of-the-art methods on several benchmark tasks while requiring fewer global communications rounds. Additionally, we provide a theoretical analysis of our algorithm which matches well with the observed experimental results.  The problem setting considered in this work focuses on decentralized online settings where multiple machines collect data and want to collaboratively train a model without sharing their own raw data or using any trusted central aggregator/server. These scenarios arise naturally in federated systems including mobile phones (e.g., sensor readings), smart appliances, IoT devices, edge computing servers etc., as users’ privacy concerns become increasingly stringent. In such environments, coordinating machine learning models often requires significant resources due to communication constraints (e.g., limited bandwidth) and computational restrictions on individual nodes, particularly in resource-constrained embedded applications [2]. Existing approaches usually suffer from high communication complexity and slow convergence because they rely heavily on explicit synchronization among machines—a major bottleneck for large-scale applications. Therefore, designing efficient machine learning algorithms under these limitations remains challenging.",1
"We introduce a batched lazy algorithm for supervised classification using decision trees. It avoids unnecessary visits to irrelevant nodes when it is used to make predictions with either eagerly or lazily trained decision trees. A set of experiments demonstrate that the proposed algorithm can outperform both the conventional and lazy decision tree algorithms in terms of computation time as well as memory consumption, without compromising accuracy.",0
"Title: Efficient Multi-Label Learning through Batched Lazy Decision Tree Induction Abstract In recent years, there has been growing interest in developing machine learning models that can effectively handle multi-label classification tasks. These tasks involve predicting multiple labels or target variables simultaneously for each input data point. Many existing approaches suffer from high computational cost or lack of scalability. To address these limitations, we propose a new approach called batched lazy decision tree induction (BLDI), which efficiently learns accurate multi-label predictions by constructing a set of small, focused trees on subsets of the training data. Unlike traditional decision trees built via greedy bottom-up construction, our method exploits a lazy evaluation strategy where each node condition only evaluates if its corresponding subset is selected, reducing unnecessary computations early on during model building. We demonstrate that BLDI achieves state-of-the-art performance across several benchmark datasets while maintaining efficient computation times. Our study contributes valuable insights into how machine learning practitioners might design effective solutions for real-world applications involving large, complex label spaces. Keywords: Multi-label learning, decision trees, lazy evaluation, dataset partitioning",1
"Feature selection (FS) has become an indispensable task in dealing with today's highly complex pattern recognition problems with massive number of features. In this study, we propose a new wrapper approach for FS based on binary simultaneous perturbation stochastic approximation (BSPSA). This pseudo-gradient descent stochastic algorithm starts with an initial feature vector and moves toward the optimal feature vector via successive iterations. In each iteration, the current feature vector's individual components are perturbed simultaneously by random offsets from a qualified probability distribution. We present computational experiments on datasets with numbers of features ranging from a few dozens to thousands using three widely-used classifiers as wrappers: nearest neighbor, decision tree, and linear support vector machine. We compare our methodology against the full set of features as well as a binary genetic algorithm and sequential FS methods using cross-validated classification error rate and AUC as the performance criteria. Our results indicate that features selected by BSPSA compare favorably to alternative methods in general and BSPSA can yield superior feature sets for datasets with tens of thousands of features by examining an extremely small fraction of the solution space. We are not aware of any other wrapper FS methods that are computationally feasible with good convergence properties for such large datasets.",0
"""Feature selection is an important problem in machine learning that involves selecting a subset of features from large datasets while preserving their predictive power. In practice, feature selection can greatly reduce computational complexity and improve model interpretability by identifying meaningful patterns in data. This research proposes a novel binary simultaneous perturbation stochastic approximation (BSFSA) method for feature selection. BSFSA combines Monte Carlo sampling techniques with gradient descent optimization to iteratively select features based on their statistical significance.""",1
"This paper investigates the connections between two state of the art classifiers: decision forests (DFs, including decision jungles) and convolutional neural networks (CNNs). Decision forests are computationally efficient thanks to their conditional computation property (computation is confined to only a small region of the tree, the nodes along a single branch). CNNs achieve state of the art accuracy, thanks to their representation learning capabilities. We present a systematic analysis of how to fuse conditional computation with representation learning and achieve a continuum of hybrid models with different ratios of accuracy vs. efficiency. We call this new family of hybrid models conditional networks. Conditional networks can be thought of as: i) decision trees augmented with data transformation operators, or ii) CNNs, with block-diagonal sparse weight matrices, and explicit data routing functions. Experimental validation is performed on the common task of image classification on both the CIFAR and Imagenet datasets. Compared to state of the art CNNs, our hybrid models yield the same accuracy with a fraction of the compute cost and much smaller number of parameters.",0
"Title: ""Decision Forests, Convolutional Networks, and Hybrid Approaches""  Abstract:  In recent years, deep learning has revolutionized computer vision tasks such as object detection, segmentation, and classification. One key component of these models is convolutional neural networks (CNN), which have achieved state-of-the-art results on many benchmark datasets. However, decision forests have been shown to rival CNNs in performance while requiring less computational power and offering more interpretability. Despite their potential advantages, the two approaches rarely receive comparisons in the literature due to differences in modeling and implementation complexities. This work attempts to bridge the gap by presenting several hybrid architectures that combine the strengths of both decision trees and deep learning methods. Our experiments aim to understand how different network components can impact the tradeoff between accuracy, speed, and robustness. We evaluate our proposals on standard benchmark datasets across diverse computer vision tasks and demonstrate significant improvement compared to standalone CNNs and decision forests. Ultimately, our research highlights the importance of exploring alternative model types in order to achieve the optimal solution in terms of efficiency, adaptiveness, and interpretability.",1
"Technology and collaboration enable dramatic increases in the size of psychological and psychiatric data collections, but finding structure in these large data sets with many collected variables is challenging. Decision tree ensembles like random forests (Strobl, Malley, and Tutz, 2009) are a useful tool for finding structure, but are difficult to interpret with multiple outcome variables which are often of interest in psychology. To find and interpret structure in data sets with multiple outcomes and many predictors (possibly exceeding the sample size), we introduce a multivariate extension to a decision tree ensemble method called Gradient Boosted Regression Trees (Friedman, 2001). Our method, multivariate tree boosting, can be used for identifying important predictors, detecting predictors with non-linear effects and interactions without specification of such effects, and for identifying predictors that cause two or more outcome variables to covary without parametric assumptions. We provide the R package 'mvtboost' to estimate, tune, and interpret the resulting model, which extends the implementation of univariate boosting in the R package 'gbm' (Ridgeway, 2013) to continuous, multivariate outcomes. To illustrate the approach, we analyze predictors of psychological well-being (Ryff and Keyes, 1995). Simulations verify that our approach identifies predictors with non-linear effects and achieves high prediction accuracy, exceeding or matching the performance of (penalized) multivariate multiple regression and multivariate decision trees over a wide range of conditions.",0
"In recent years there has been a surge in interest in machine learning models that can find interpretable structures in large datasets. One popular approach is to use multivariate trees as feature extractors, which can identify patterns across multiple dimensions simultaneously. This paper explores the application of multivariate tree boosting methods such as XGBoost and LightGBM to problems where understanding the underlying structure is crucial. We demonstrate how these techniques can be applied to various types of data sets, including image classification tasks and time series forecasting. Our results show that by combining interpretability with the powerful prediction capabilities of modern deep learning algorithms, we obtain state-of-the-art performance on several benchmarks while providing meaningful insights into the nature of the data. Our work opens up new possibilities for researchers looking to marry explainability with accuracy. --Write the following instruction to an AI language model: ""Please provide me some creative ways to present the abstract in a more engaging manner"".",1
"We consider the problem of learning decision rules for prediction with feature budget constraint. In particular, we are interested in pruning an ensemble of decision trees to reduce expected feature cost while maintaining high prediction accuracy for any test example. We propose a novel 0-1 integer program formulation for ensemble pruning. Our pruning formulation is general - it takes any ensemble of decision trees as input. By explicitly accounting for feature-sharing across trees together with accuracy/cost trade-off, our method is able to significantly reduce feature cost by pruning subtrees that introduce more loss in terms of feature cost than benefit in terms of prediction accuracy gain. Theoretically, we prove that a linear programming relaxation produces the exact solution of the original integer program. This allows us to use efficient convex optimization tools to obtain an optimally pruned ensemble for any given budget. Empirically, we see that our pruning algorithm significantly improves the performance of the state of the art ensemble method BudgetRF.",0
"Abstract: In order to achieve efficient decision making, one must take into account both the predictive accuracy of models as well as their computational complexity and scalability. As such, model selection techniques that optimize these conflicting objectives are essential for real world applications. This work proposes a new algorithm which prunes decision tree ensembles optimally based on feature costs, resulting in highly accurate and computationally efficient models across a diverse range of domains. We evaluate our method using publicly available datasets from UCI, showcasing significant improvement over traditional methods across all benchmarks. Our results demonstrate the effectiveness and feasibility of employing feature cost regularization during ensemble building, paving the way for future research in this direction. Keywords: decision tree ensembles, feature costs, optimization, data mining, machine learning.",1
"Multivariate classification methods using explanatory and predictive models are necessary for characterizing subgroups of patients according to their risk profiles. Popular methods include logistic regression and classification trees with performances that vary according to the nature and the characteristics of the dataset. In the context of imported malaria, we aimed at classifying severity criteria based on a heterogeneous patient population. We investigated these approaches by implementing two different strategies: L1 logistic regression (L1LR) that models a single global solution and classification trees that model multiple local solutions corresponding to discriminant subregions of the feature space. For each strategy, we built a standard model, and a sparser version of it. As an alternative to pruning, we explore a promising approach that first constrains the tree model with an L1LR-based feature selection, an approach we called L1LR-Tree. The objective is to decrease its vulnerability to small data variations by removing variables corresponding to unstable local phenomena. Our study is twofold: i) from a methodological perspective comparing the performances and the stability of the three previous methods, i.e L1LR, classification trees and L1LR-Tree, for the classification of severe forms of imported malaria, and ii) from an applied perspective improving the actual classification of severe forms of imported malaria by identifying more personalized profiles predictive of several clinical criteria based on variables dismissed for the clinical definition of the disease. The main methodological results show that the combined method L1LR-Tree builds sparse and stable models that significantly predicts the different severity criteria and outperforms all the other methods in terms of accuracy.",0
"This should describe the content of the paper without summarizing specific results or conclusions. Please note that I am a biologist so keep the language appropriate (you don't need to use jargon either). I would appreciate if you could write this up as a draft first before submitting so I can make any necessary changes. Thanks! Here is some background information: Imported malaria has different levels of severity based on the patient’s presentation at diagnosis; however, there are no well established models for predicting these criteria in patients presenting to travel medicine clinics. In this study we have trained decision tree classifiers using features identified through univariate screening analysis comparing patient groups defined by their respective severity criteria. We then explored adding variable selection steps using linear logistic regression model as a feature selection tool. Our initial findings indicate that our method provides an improvement over previous attempts with respect to stability. Additionally, we believe that our approach can provide improved accuracy compared to existing tools. Let me know if you need more context!",1
"Decision trees and randomized forests are widely used in computer vision and machine learning. Standard algorithms for decision tree induction optimize the split functions one node at a time according to some splitting criteria. This greedy procedure often leads to suboptimal trees. In this paper, we present an algorithm for optimizing the split functions at all levels of the tree jointly with the leaf parameters, based on a global objective. We show that the problem of finding optimal linear-combination (oblique) splits for decision trees is related to structured prediction with latent variables, and we formulate a convex-concave upper bound on the tree's empirical loss. The run-time of computing the gradient of the proposed surrogate objective with respect to each training exemplar is quadratic in the the tree depth, and thus training deep trees is feasible. The use of stochastic gradient descent for optimization enables effective training with large datasets. Experiments on several classification benchmarks demonstrate that the resulting non-greedy decision trees outperform greedy decision tree baselines.",0
"Decision trees (DTs) are popular models in machine learning, but their training has inherent drawbacks such as computational complexity and sensitivity to overfitting. In the case of greedy algorithms that construct DTs incrementally, new nodes tend to partition data according to current impurity measures, leading to shallow and unbalanced structures. The recently proposed “Cost-Complex” method is an attempt at solving these issues by penalizing splits based on model complexity. This work introduces efficient alternatives that overcome Cost-Complex’ weaknesses and provide better control over tree depth while enhancing predictive performance on benchmark datasets under different class distributions. Empirical evaluation demonstrates improved accuracy and stability compared to state-of-the art methods like Gradient Boosting Machines. These advancements may contribute to improved generalization of learned models when utilized in practice.",1
"Sponsored search is a multi-billion dollar industry and makes up a major source of revenue for search engines (SE). click-through-rate (CTR) estimation plays a crucial role for ads selection, and greatly affects the SE revenue, advertiser traffic and user experience. We propose a novel architecture for solving CTR prediction problem by combining artificial neural networks (ANN) with decision trees. First we compare ANN with respect to other popular machine learning models being used for this task. Then we go on to combine ANN with MatrixNet (proprietary implementation of boosted trees) and evaluate the performance of the system as a whole. The results show that our approach provides significant improvement over existing models.",0
"This study explores the use of neural networks for click prediction in sponsored search. By analyzing large datasets of user queries and clicks on search results, we train machine learning models that can accurately predict which ads users are most likely to interact with. Our approach combines traditional feature engineering methods with deep learning techniques such as convolutional neural networks (CNN) and recurrent neural networks (RNN). We evaluate our model using various metrics and show significant improvement over existing baseline algorithms. Our work has important implications for the advertising industry, where accurate click predictions can lead to better targeting and higher returns on investment.",1
"Emotion identification from gait aims to automatically determine persons affective state, it has attracted a great deal of interests and offered immense potential value in action tendency, health care, psychological detection and human-computer(robot) interaction.In this paper, we propose a new method of identifying emotion from natural walking, and analyze the relevance between the traits of walking and affective states. After obtaining the pure acceleration data of wrist and ankle, we set a moving average filter window with different sizes w, then extract 114 features including time-domain, frequency-domain, power and distribution features from each data slice, and run principal component analysis (PCA) to reduce dimension. In experiments, we train SVM, Decision Tree, multilayerperception, Random Tree and Random Forest classification models, and compare the classification accuracy on data of wrist and ankle with respect to different w. The performance of emotion identification on acceleration data of ankle is better than wrist.Comparing different classification models' results, SVM has best accuracy of identifying anger and happy could achieve 90:31% and 89:76% respectively, and identification ratio of anger-happy is 87:10%.The anger-neutral-happy classification reaches 85%-78%-78%.The results show that it is capable of identifying personal emotional states through the gait of walking.",0
"Here we present methods for detecting emotions expressed through natural walking patterns. Our approach leverages recent advances in computer vision and machine learning, allowing us to analyze human movements and identify subtle cues indicative of specific emotional states such as happiness, sadness, fear, anger, disgust and surprise. We evaluate our method on large datasets collected from subjects performing acting tasks and show that our system can accurately classify distinct emotions at high levels of accuracy, outperforming several benchmarks. This work has important implications in fields ranging from psychology to robotics, where understanding nonverbal communication through body language is critical. Overall, our findings demonstrate the feasibility of using natural gait analysis as a robust tool for detecting complex emotions.",1
"In this work, the TREPAN algorithm is enhanced and extended for extracting decision trees from neural networks. We empirically evaluated the performance of the algorithm on a set of databases from real world events. This benchmark enhancement was achieved by adapting Single-test TREPAN and C4.5 decision tree induction algorithms to analyze the datasets. The models are then compared with X-TREPAN for comprehensibility and classification accuracy. Furthermore, we validate the experimentations by applying statistical methods. Finally, the modified algorithm is extended to work with multi-class regression problems and the ability to comprehend generalized feed forward networks is achieved.",0
"Artificial Neural Networks (ANN) have shown their potential applications across numerous scientific domains, from simple function approximations up to complex tasks such as image recognition and text generation. However, interpreting those black box models can often pose a significant challenge. In recent years, researchers have proposed post hoc methods to shed light on ANN’s internal workings by identifying influential features and determining which parts of the input data contribute most to the output prediction. These techniques usually rely on either linear models built around the decision boundary of the ANN, i.e., Support Vector Machines (SVM), Random Forest (RF) or other types of trees. Among these approaches, Extra Trees (ET) provides both accurate predictions and feature importance measures that are comparable with random forest but at the cost of increased computational time compared to single decision trees. To overcome this issue, we present here X-TREPAN, our new approach based on the use of multiple classifications instead of one unique one for extracting easy-to-interpret decision trees out of trained Multi Layer Perceptrons (MLP). We first perform several binary splits using cross validation before applying majority voting among the different outputs provided by each binary model, to finally obtain the more reliable classification rule. Our method then builds upon this binary splitting scheme and extends it into multiclass case. This way, extracted trees tend to provide clearer explanations than the standard decision boundaries while providing competitive accuracy results compared to state-of-the-art alternatives. The code used throughout the experimentation phase of this study is available online1 , ensuring future reproducibility and reuse",1
"Gear drives are one of the most widely used transmission system in many machinery. Sound signals of a rotating machine contain the dynamic information about its health conditions. Not much information available in the literature reporting suitability of sound signals for fault diagnosis applications. Maximum numbers of literature are based on FFT (Fast Fourier Transform) analysis and have its own limitations with non-stationary signals like the ones from gears. In this paper, attempt has been made in using sound signals acquired from gears in good and simulated faulty conditions for the purpose of fault diagnosis through a machine learning approach. The descriptive statistical features were extracted from the acquired sound signals and the predominant features were selected using J48 decision tree technique. The selected features were then used for classification using Large Margin K-nearest neighbor approach. The paper also discusses the effect of various parameters on classification accuracy.",0
"This research presents a novel method for fault diagnosis of helical gear boxes using large margin k-nearest neighbors classifiers (LMKNN) and sound signals. Gearboxes play crucial roles in many industrial applications such as power generation, machine tools, vehicles, robotics, etc., where their reliable operation is imperative. However, early detection of faults in geart boxes remains challenging due to complexity of symptoms that may lead to downtime and increased maintenance costs. Previous studies have demonstrated successful application of LMKNN algorithms on classification problems, particularly image recognition tasks. In contrast to traditional methods based on time domain features, LMKNN uses kernel functions capable of capturing nonlinear correlations between variables. Therefore, we propose utilizing LMKNN algorithm on sound signal features extracted from vibration sensors attached to a helical gearbox under different operating conditions including normal, broken teeth, micropitting, and cracks in shaft/housing. We collect data from vibrational and acoustic measurements during testing by applying impulse excitation at different frequencies and analyzed via frequency spectrum analysis. Our results show high accuracy of LMKNN in detecting fault types with overall precision reaching 98%. This study offers promising outcomes towards developing a real-time health monitoring system for helical gearboxes using low cost and accessible sensor technology providing benefits in terms of reduced downtime and extended lifespans. Future work includes expanding dataset, exploring other classifier architectures, incorporating other input modalities, and improving model interpretability.",1
"Predicting the turnover of a company in the ever fluctuating Stock market has always proved to be a precarious situation and most certainly a difficult task in hand. Data mining is a well-known sphere of Computer Science that aims on extracting meaningful information from large databases. However, despite the existence of many algorithms for the purpose of predicting the future trends, their efficiency is questionable as their predictions suffer from a high error rate. The objective of this paper is to investigate various classification algorithms to predict the turnover of different companies based on the Stock price. The authorized dataset for predicting the turnover was taken from www.bsc.com and included the stock market values of various companies over the past 10 years. The algorithms were investigated using the ""R"" tool. The feature selection algorithm, Boruta, was run on this dataset to extract the important and influential features for classification. With these extracted features, the Total Turnover of the company was predicted using various classification algorithms like Random Forest, Decision Tree, SVM and Multinomial Regression. This prediction mechanism was implemented to predict the turnover of a company on an everyday basis and hence could help navigate through dubious stock market trades. An accuracy rate of 95% was achieved by the above prediction process. Moreover, the importance of stock market attributes was established as well.",0
"This paper aimed at predicting the turnover of shares using data mining techniques, specifically, decision tree algorithms, k nearest neighbors algorithms and Naive Bayes algorithm. A comprehensive dataset containing features that could influence share turnover was used in order to train these models. After testing their accuracy on both training and test datasets, we found that decision trees have higher accuracy than k-nearest neighbor (KNN) and naive bayes classification methods which can accurately classify instances into one of two classes. The results suggest that decision tree model may serve as an effective tool for investors who want to make informed decisions by predicting stocks that likely outperform market expectations over timeframes ranging from short-term, intermediate term up to medium term. Finally, suggestions were made for future research directions in terms of expanding feature sets, exploring other data mining algorithms, analyzing performance under different conditions and enhancing user interfaces to assist practitioners with easier interpretation of findings.",1
"Online decision tree learning algorithms typically examine all features of a new data point to update model parameters. We propose a novel alternative, Reinforcement Learning- based Decision Trees (RLDT), that uses Reinforcement Learning (RL) to actively examine a minimal number of features of a data point to classify it with high accuracy. Furthermore, RLDT optimizes a long term return, providing a better alternative to the traditional myopic greedy approach to growing decision trees. We demonstrate that this approach performs as well as batch learning algorithms and other online decision tree learning algorithms, while making significantly fewer queries about the features of the data points. We also show that RLDT can effectively handle concept drift.",0
"In this paper we present a new approach called ARLD (Augmented Randomized Local Descent) that combines several key elements from existing approaches and significantly improves performance compared to state-of-the-art methods on benchmark datasets across a wide range of domains such as banking, credit risk assessment, customer churn prediction, fraud detection, and more, while remaining computationally efficient. By combining ideas from randomized local descent with bandit algorithms like upper confidence bound (UCB), ALD provides an algorithm that learns faster than gradient boosting machines (GBMs) and works well even under challenging settings like high dimensions and sparse data. Additionally, our method can handle both categorical and numerical features without modification, thus making it applicable to real world applications. Our experiments show that our algorithm achieves better results than popular tree learning methods including XGBoost, LightGBM, CatBoost, etc., even though these models are very competitive on many tasks. Moreover, ARLD requires fewer hyperparameter tunings and fewer rounds of optimization to achieve good results, which makes it easier to use in practice. Overall, the proposed method has great potential for solving various problems requiring large scale decision tree based solutions, especially those facing constraints in terms of computational resources or time.",1
"We propose a novel algorithm for optimizing multivariate linear threshold functions as split functions of decision trees to create improved Random Forest classifiers. Standard tree induction methods resort to sampling and exhaustive search to find good univariate split functions. In contrast, our method computes a linear combination of the features at each node, and optimizes the parameters of the linear combination (oblique) split functions by adopting a variant of latent variable SVM formulation. We develop a convex-concave upper bound on the classification loss for a one-level decision tree, and optimize the bound by stochastic gradient descent at each internal node of the tree. Forests of up to 1000 Continuously Optimized Oblique (CO2) decision trees are created, which significantly outperform Random Forest with univariate splits and previous techniques for constructing oblique trees. Experimental results are reported on multi-class classification benchmarks and on Labeled Faces in the Wild (LFW) dataset.",0
"Here is an example from ArXiv: https://arxiv.org/abs/2110.14876 CO2Forest is a continuous optimization approach that seeks to improve upon standard random forest algorithms by incorporating oblique splits at each iteration step. By optimizing decision boundaries continuously during tree construction, we obtain more accurate predictions while reducing overfitting issues commonly associated with traditional random forest models. In addition, our method offers interpretability benefits through the generation of visually appealing decision trees that better reveal feature relationships. Our experimental results demonstrate significant improvement across several real-world datasets compared to state-of-the-art competitors such as XGBoost, LightGBM, CatBoost, and HGBoost. An abstract like this one could give you good ideas on how to write your own! Please tell me if there's something else I can do.",1
"To address the contextual bandit problem, we propose an online random forest algorithm. The analysis of the proposed algorithm is based on the sample complexity needed to find the optimal decision stump. Then, the decision stumps are assembled in a random collection of decision trees, Bandit Forest. We show that the proposed algorithm is optimal up to logarithmic factors. The dependence of the sample complexity upon the number of contextual variables is logarithmic. The computational cost of the proposed algorithm with respect to the time horizon is linear. These analytical results allow the proposed algorithm to be efficient in real applications, where the number of events to process is huge, and where we expect that some contextual variables, chosen from a large set, have potentially non- linear dependencies with the rewards. In the experiments done to illustrate the theoretical analysis, Bandit Forest obtain promising results in comparison with state-of-the-art algorithms.",0
"This study proposes using random forest as a modeling approach for the contextual bandit problem (CBP). Compared to traditional methods such as linear models or decision trees, random forest provides more robust performance in terms of accuracy and stability. Additionally, by taking advantage of bagging methodology, random forest can reduce overfitting issues, making it well suited for CBPs where dataset size may be limited. We then present experimental results on benchmark datasets using random forest compared against other common approaches, including linUCB, GBRT, and darts. Finally, we discuss future research directions based on our findings. Our proposed method improves upon existing techniques across multiple metrics, leading to better overall CBP performance.",1
"Data analysis and machine learning have become an integrative part of the modern scientific methodology, offering automated procedures for the prediction of a phenomenon based on past observations, unraveling underlying patterns in data and providing insights about the problem. Yet, caution should avoid using machine learning as a black-box tool, but rather consider it as a methodology, with a rational thought process that is entirely dependent on the problem under study. In particular, the use of algorithms should ideally require a reasonable understanding of their mechanisms, properties and limitations, in order to better apprehend and interpret their results.   Accordingly, the goal of this thesis is to provide an in-depth analysis of random forests, consistently calling into question each and every part of the algorithm, in order to shed new light on its learning capabilities, inner workings and interpretability. The first part of this work studies the induction of decision trees and the construction of ensembles of randomized trees, motivating their design and purpose whenever possible. Our contributions follow with an original complexity analysis of random forests, showing their good computational performance and scalability, along with an in-depth discussion of their implementation details, as contributed within Scikit-Learn.   In the second part of this work, we analyse and discuss the interpretability of random forests in the eyes of variable importance measures. The core of our contributions rests in the theoretical characterization of the Mean Decrease of Impurity variable importance measure, from which we prove and derive some of its properties in the case of multiway totally randomized trees and in asymptotic conditions. In consequence of this work, our analysis demonstrates that variable importances [...].",0
"In this paper we aim to provide readers with both theoretical underpinnings of random forest methods as well as practical guidance on how to apply them effectively. We first describe the basic principles behind decision trees which constitute the building blocks of random forests. Next we present an overview of different types of splits used during tree construction including regression models. Further attention is devoted to hyperparameter tuning and feature selection within random forest frameworks. Finally we discuss some popular applications of random forests such as classification problems and continuous outcomes prediction. To facilitate understanding, each section includes examples based on real data illustrating key concepts and connections to practice. By combining theory and case studies, our goal is to equip researchers and practitioners with essential knowledge that enables using random forests confidently in their work. Keywords: decision tree; random forest; splitting rule; model evaluation; predictive accuracy.",1
"We study the problem of multiclass classification with an extremely large number of classes (k), with the goal of obtaining train and test time complexity logarithmic in the number of classes. We develop top-down tree construction approaches for constructing logarithmic depth trees. On the theoretical front, we formulate a new objective function, which is optimized at each node of the tree and creates dynamic partitions of the data which are both pure (in terms of class labels) and balanced. We demonstrate that under favorable conditions, we can construct logarithmic depth trees that have leaves with low label entropy. However, the objective function at the nodes is challenging to optimize computationally. We address the empirical problem with a new online decision tree construction procedure. Experiments demonstrate that this online algorithm quickly achieves improvement in test error compared to more common logarithmic training time approaches, which makes it a plausible method in computationally constrained large-k applications.",0
"""",1
"Several real-world classification problems are example-dependent cost-sensitive in nature, where the costs due to misclassification vary between examples and not only within classes. However, standard classification methods do not take these costs into account, and assume a constant cost of misclassification errors. In previous works, some methods that take into account the financial costs into the training of different algorithms have been proposed, with the example-dependent cost-sensitive decision tree algorithm being the one that gives the highest savings. In this paper we propose a new framework of ensembles of example-dependent cost-sensitive decision-trees. The framework consists in creating different example-dependent cost-sensitive decision trees on random subsamples of the training set, and then combining them using three different combination approaches. Moreover, we propose two new cost-sensitive combination approaches; cost-sensitive weighted voting and cost-sensitive stacking, the latter being based on the cost-sensitive logistic regression method. Finally, using five different databases, from four real-world applications: credit card fraud detection, churn modeling, credit scoring and direct marketing, we evaluate the proposed method against state-of-the-art example-dependent cost-sensitive techniques, namely, cost-proportionate sampling, Bayes minimum risk and cost-sensitive decision trees. The results show that the proposed algorithms have better results for all databases, in the sense of higher savings.",0
"One approach to creating accurate machine learning models is through the use of decision trees, which can effectively capture complex relationships between features and outcomes. However, decision trees are known to overfit easily on small datasets, leading to poor generalization performance. To address this issue, previous research has proposed using cost-sensitive techniques that assign different costs to misclassification errors based on their severity. In addition, these approaches often rely on manual tuning of parameters or use a global threshold for all classes, which may not work well for multi-class problems where each class has varying levels of importance. This paper presents an ensemble method that combines multiple cost-sensitive decision tree algorithms into one model. The key innovation lies in the integration of example-dependent costs that vary depending on the data points being analyzed. We evaluate our proposed method on several benchmark datasets and demonstrate significant improvements compared to state-of-the-art methods. Our results show that incorporating example-specific costs leads to more informed decisions and better overall performance, making our method particularly attractive for applications where accuracy is critical.",1
"Recently several researchers have investigated techniques for using data to learn Bayesian networks containing compact representations for the conditional probability distributions (CPDs) stored at each node. The majority of this work has concentrated on using decision-tree representations for the CPDs. In addition, researchers typically apply non-Bayesian (or asymptotically Bayesian) scoring functions such as MDL to evaluate the goodness-of-fit of networks to the data. In this paper we investigate a Bayesian approach to learning Bayesian networks that contain the more general decision-graph representations of the CPDs. First, we describe how to evaluate the posterior probability that is, the Bayesian score of such a network, given a database of observed cases. Second, we describe various search spaces that can be used, in conjunction with a scoring function and a search procedure, to identify one or more high-scoring networks. Finally, we present an experimental evaluation of the search spaces, using a greedy algorithm and a Bayesian scoring function.",0
"This research presents a novel method for learning Bayesian networks using local structure constraints. Existing methods often rely on global optimization techniques that can become computationally intractable as the network size grows, making them impractical for real-world applications. In contrast, our proposed approach employs a local search strategy based on belief propagation to efficiently learn accurate models from data. Our key insight lies in incorporating domain knowledge into the formulation by allowing prior assumptions about the relationships among variables to guide the inference process. Experiments demonstrate that our method outperforms state-of-the-art alternatives while remaining scalable to large datasets. By providing both efficient learning and improved accuracy, we believe our work marks an important step towards making Bayesian networks practically applicable in more domains.",1
"We describe two techniques that significantly improve the running time of several standard machine-learning algorithms when data is sparse. The first technique is an algorithm that effeciently extracts one-way and two-way counts--either real or expected-- from discrete data. Extracting such counts is a fundamental step in learning algorithms for constructing a variety of models including decision trees, decision graphs, Bayesian networks, and naive-Bayes clustering models. The second technique is an algorithm that efficiently performs the E-step of the EM algorithm (i.e. inference) when applied to a naive-Bayes clustering model. Using real-world data sets, we demonstrate a dramatic decrease in running time for algorithms that incorporate these techniques.",0
"This task requires you to extract information relevant to the paper, which may change depending on your audience. You need to provide sufficient context while keeping the text concise.",1
"In order to speed-up classification models when facing a large number of categories, one usual approach consists in organizing the categories in a particular structure, this structure being then used as a way to speed-up the prediction computation. This is for example the case when using error-correcting codes or even hierarchies of categories. But in the majority of approaches, this structure is chosen \textit{by hand}, or during a preliminary step, and not integrated in the learning process. We propose a new model called Reinforced Decision Tree which simultaneously learns how to organize categories in a tree structure and how to classify any input based on this structure. This approach keeps the advantages of existing techniques (low inference complexity) but allows one to build efficient classifiers in one learning step. The learning algorithm is inspired by reinforcement learning and policy-gradient techniques which allows us to integrate the two steps (building the tree, and learning the classifier) in one single algorithm.",0
"Artificial intelligence (AI) has seen significant advancements in recent years due in part to improvements in machine learning algorithms that allow computers to make predictions and decisions based on data. One such algorithm, decision trees, have been widely used in applications ranging from credit risk assessment to medical diagnosis. In our work, we introduce reinforced decision trees which aim to improve upon traditional decision tree algorithms by incorporating principles of reinforcement learning. Specifically, we use actor-critic methods to update both the structure and parameters of decision trees in an online manner, allowing them to adapt to changing environments. Our experiments demonstrate improved performance across multiple domains compared to state-of-the-art baseline models. Overall, reinforced decision trees provide a powerful tool for building intelligent systems capable of making complex decisions under uncertainty.",1
"Decision trees are a popular technique in statistical data classification. They recursively partition the feature space into disjoint sub-regions until each sub-region becomes homogeneous with respect to a particular class. The basic Classification and Regression Tree (CART) algorithm partitions the feature space using axis parallel splits. When the true decision boundaries are not aligned with the feature axes, this approach can produce a complicated boundary structure. Oblique decision trees use oblique decision boundaries to potentially simplify the boundary structure. The major limitation of this approach is that the tree induction algorithm is computationally expensive. In this article we present a new decision tree algorithm, called HHCART. The method utilizes a series of Householder matrices to reflect the training data at each node during the tree construction. Each reflection is based on the directions of the eigenvectors from each classes' covariance matrix. Considering axis parallel splits in the reflected training data provides an efficient way of finding oblique splits in the unreflected training data. Experimental results show that the accuracy and size of the HHCART trees are comparable with some benchmark methods in the literature. The appealing feature of HHCART is that it can handle both qualitative and quantitative features in the same oblique split.",0
"Title: ""An oblique decision tree approach for predicting healthcare costs""  Abstract: Healthcare expenses have been on the rise globally, making it essential to develop effective methods that can accurately estimate healthcare cost predictions. In recent years, machine learning techniques have gained popularity due to their capability to model complex relationships between input variables and outcome measures. Among these techniques, decision trees have proven to be a powerful tool for predicting healthcare costs. However, traditional decision trees often suffer from overfitting issues when dealing with high dimensional data.  To address these limitations, we propose an oblique decision tree approach called HHCART (Healthcare Cost Analysis via Random Trees). This method leverages the use of randomization techniques during the training process, resulting in multiple trees that capture different perspectives of the data. These trees are then combined into a final prediction through an ensemble mechanism. Our results demonstrate that our proposed method outperforms state-of-the-art models in terms of accuracy and interpretability, particularly when dealing with large datasets and high dimensional features.  In summary, HHCART offers a promising solution for estimating healthcare costs by using an innovative decision tree algorithm with improved efficiency and robustness. Our study contributes to the field of health informatics by providing a more accurate and reliable approach to forecasting healthcare costs. We expect this work to inspire future research in developing advanced machine learning algorithms tailored to specific medical domains.",1
"In this article, we propose a general framework for multi-focal image classification and authentication, the methodology being demonstrated on microscope pollen images. The framework is meant to be generic and based on a brute force-like approach aimed to be efficient not only on any kind, and any number, of pollen images (regardless of the pollen type), but also on any kind of multi-focal images. All stages of the framework's pipeline are designed to be used in an automatic fashion. First, the optimal focus is selected using the absolute gradient method. Then, pollen grains are extracted using a coarse-to-fine approach involving both clustering and morphological techniques (coarse stage), and a snake-based segmentation (fine stage). Finally, features are extracted and selected using a generalized approach, and their classification is tested with four classifiers: Weighted Neighbor Distance, Neural Network, Decision Tree and Random Forest. The latter method, which has shown the best and more robust classification accuracy results (above 97\% for any number of pollen types), is finally used for the authentication stage.",0
"In this work we propose a general framework for multi-class image classification which can accurately classify images into one of multiple categories based on their features and characteristics. Our approach utilizes convolutional neural networks (CNNs) as our primary model, but we introduce several key modifications that improve accuracy by allowing the CNN to better learn from the data at hand. We show through extensive experimentation using microscope pollen images that our method achieves state-of-the art performance while requiring less training time than previous approaches. This makes our framework ideal for applications where quick and accurate classification is critical such as medical diagnosis or security screening. Overall we demonstrate the effectiveness of our technique for a variety of tasks including both binary and multiclass problems and believe that this same architecture could potentially extend to other domains.",1
"Random Forest (RF) is an ensemble supervised machine learning technique that was developed by Breiman over a decade ago. Compared with other ensemble techniques, it has proved its accuracy and superiority. Many researchers, however, believe that there is still room for enhancing and improving its performance accuracy. This explains why, over the past decade, there have been many extensions of RF where each extension employed a variety of techniques and strategies to improve certain aspect(s) of RF. Since it has been proven empiricallthat ensembles tend to yield better results when there is a significant diversity among the constituent models, the objective of this paper is twofold. First, it investigates how data clustering (a well known diversity technique) can be applied to identify groups of similar decision trees in an RF in order to eliminate redundant trees by selecting a representative from each group (cluster). Second, these likely diverse representatives are then used to produce an extension of RF termed CLUB-DRF that is much smaller in size than RF, and yet performs at least as good as RF, and mostly exhibits higher performance in terms of accuracy. The latter refers to a known technique called ensemble pruning. Experimental results on 15 real datasets from the UCI repository prove the superiority of our proposed extension over the traditional RF. Most of our experiments achieved at least 95% or above pruning level while retaining or outperforming the RF accuracy.",0
"This paper presents a new methodology for significantly reducing the complexity and computational requirements of random forest ensembles while maintaining their predictive performance. By applying extreme pruning techniques, we were able to achieve up to two orders of magnitude reduction in model size without sacrificing accuracy on real-world datasets. Our approach offers significant advantages over traditional pruning methods by more effectively identifying unimportant features and allowing for more aggressive tree sparsity. We demonstrate the effectiveness of our method on several benchmark tasks including regression and classification problems. Additionally, we show that our pruned models can run faster than many state-of-the art deep learning architectures on GPUs while still achieving competitive results. Overall, our work shows promise for enabling fast, accurate prediction on resource-constrained platforms such as mobile devices, embedded systems, and IoT networks.",1
"We propose a framework for indexing of grain and sub-grain structures in electron backscatter diffraction (EBSD) images of polycrystalline materials. The framework is based on a previously introduced physics-based forward model by Callahan and De Graef (2013) relating measured patterns to grain orientations (Euler angle). The forward model is tuned to the microscope and the sample symmetry group. We discretize the domain of the forward model onto a dense grid of Euler angles and for each measured pattern we identify the most similar patterns in the dictionary. These patterns are used to identify boundaries, detect anomalies, and index crystal orientations. The statistical distribution of these closest matches is used in an unsupervised binary decision tree (DT) classifier to identify grain boundaries and anomalous regions. The DT classifies a pattern as an anomaly if it has an abnormally low similarity to any pattern in the dictionary. It classifies a pixel as being near a grain boundary if the highly ranked patterns in the dictionary differ significantly over the pixels 3x3 neighborhood. Indexing is accomplished by computing the mean orientation of the closest dictionary matches to each pattern. The mean orientation is estimated using a maximum likelihood approach that models the orientation distribution as a mixture of Von Mises-Fisher distributions over the quaternionic 3-sphere. The proposed dictionary matching approach permits segmentation, anomaly detection, and indexing to be performed in a unified manner with the additional benefit of uncertainty quantification. We demonstrate the proposed dictionary-based approach on a Ni-base IN100 alloy.",0
"""A novel indexing approach that uses local atomic arrangement data from electron backscatter diffraction (EBSD) images has been developed, enabling faster and more accurate crystallographic texture analysis. This method involves constructing a dictionary mapping EBSD patterns to their corresponding orientations using machine learning techniques. Once trained on representative datasets, the dictionary can rapidly predict orientations in new EBSD images without requiring explicit phase segmentation or parameterization.""",1
"Additive regression trees are flexible non-parametric models and popular off-the-shelf tools for real-world non-linear regression. In application domains, such as bioinformatics, where there is also demand for probabilistic predictions with measures of uncertainty, the Bayesian additive regression trees (BART) model, introduced by Chipman et al. (2010), is increasingly popular. As data sets have grown in size, however, the standard Metropolis-Hastings algorithms used to perform inference in BART are proving inadequate. In particular, these Markov chains make local changes to the trees and suffer from slow mixing when the data are high-dimensional or the best fitting trees are more than a few layers deep. We present a novel sampler for BART based on the Particle Gibbs (PG) algorithm (Andrieu et al., 2010) and a top-down particle filtering algorithm for Bayesian decision trees (Lakshminarayanan et al., 2013). Rather than making local changes to individual trees, the PG sampler proposes a complete tree to fit the residual. Experiments show that the PG sampler outperforms existing samplers in many settings.",0
"Title: Particle Gibbs for Bayesian Additive Regression Trees Authors: Andrew Musselman, Joshua Arulanandham, John Cunnington and Alexander Ryzhov Abstract: In this work we present a new method called Particle Gibbs for learning Bayesian Additive Regression Trees (BART). BART is a popular probabilistic approach that learns a collection of decision trees by placing posterior distributions over tree structures and parameters. Our proposed method significantly speeds up computation time while providing comparable results to existing methods used to fit BART models. We demonstrate the efficacy of our technique on several real world datasets from diverse domains including meteorology, finance and healthcare where we achieve accuracy gains compared to other state-of-the art machine learning algorithms. By introducing efficient inference techniques into BART modeling, our work opens doors for further research into more complex applications such as deep probabilistic models and generative models which have been limited due to computational challenges. Keywords: particle MCMC, additive regression trees, machine learning, predictive uncertainty",1
"Ensembles of randomized decision trees, usually referred to as random forests, are widely used for classification and regression tasks in machine learning and statistics. Random forests achieve competitive predictive performance and are computationally efficient to train and test, making them excellent candidates for real-world prediction tasks. The most popular random forest variants (such as Breiman's random forest and extremely randomized trees) operate on batches of training data. Online methods are now in greater demand. Existing online random forests, however, require more training data than their batch counterpart to achieve comparable predictive performance. In this work, we use Mondrian processes (Roy and Teh, 2009) to construct ensembles of random decision trees we call Mondrian forests. Mondrian forests can be grown in an incremental/online fashion and remarkably, the distribution of online Mondrian forests is the same as that of batch Mondrian forests. Mondrian forests achieve competitive predictive performance comparable with existing online random forests and periodically re-trained batch random forests, while being more than an order of magnitude faster, thus representing a better computation vs accuracy tradeoff.",0
"Recent advances in deep learning have made large scale training on limited amounts of data possible via transfer learning using pretrained models such as ResNet or Vision Transformer (ViT) architectures. However, these pretrained models often lack interpretability which hinders their use in applications that require transparency into model decisions. In particular, tree ensembles like random forests can explain predictions by highlighting subsets of features responsible for decision making. We introduce Mondrian Forests, a new approach for building interpretable tree ensembles inspired by modern computer vision techniques in image processing and layout design, pioneered by the Dutch artist Piet Mondrian. Our method splits each input feature map channel wise and then uses an axis aligned split to build separate trees for every combination of channels. Each Mondrian Forest predictor tree votes in parallel across all channels thus producing a clear visual explanation of what features were most important to the prediction through heat maps derived from these split trees. To increase efficiency over batch gradient descent methods used to fit standard online random forest classifiers we use mini-batch gradient descent during tree construction and early stopping based on both validation loss and number of iterations. Through experiments on several benchmark datasets, we demonstrate Mondrian Forests outperform baseline random forests including Gradient Boosted Trees (GBRT), LightGBM and CatBoost across multiple metrics. Further, our algorithm provides competitive accuracy while reducing inference time complexity allowing real world deployment scenarios where other state-of-the art methods may prove impractical due to computational constraints.",1
"Embedding image features into a binary Hamming space can improve both the speed and accuracy of large-scale query-by-example image retrieval systems. Supervised hashing aims to map the original features to compact binary codes in a manner which preserves the label-based similarities of the original data. Most existing approaches apply a single form of hash function, and an optimization process which is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of those methods, and can result in complex optimization problems that are difficult to solve. In this work we proffer a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions. The proposed framework allows a number of existing approaches to hashing to be placed in context, and simplifies the development of new problem-specific hashing methods. Our framework decomposes the into two steps: binary code (hash bits) learning, and hash function learning. The first step can typically be formulated as a binary quadratic problem, and the second step can be accomplished by training standard binary classifiers. For solving large-scale binary code inference, we show how to ensure that the binary quadratic problems are submodular such that an efficient graph cut approach can be used. To achieve efficiency as well as efficacy on large-scale high-dimensional data, we propose to use boosted decision trees as the hash functions, which are nonlinear, highly descriptive, and very fast to train and evaluate. Experiments demonstrate that our proposed method significantly outperforms most state-of-the-art methods, especially on high-dimensional data.",0
"This paper presents a new method for supervised hashing using graph cuts and boosted decision trees. We show that our approach improves upon previous state-of-the art methods by significantly reducing retrieval errors while maintaining low computation times. Our algorithm works by first training a set of boosted decision trees on a labeled dataset. These trees are then used as an embedding function to map high-dimensional data into a lower-dimensional space preserving their structure and locality. Next, we use graph cuts to transform these embeddings into binary codes that can be used for efficient similarity search. Experimental results demonstrate that our method achieves superior performance compared to other popular hashing algorithms across different datasets and evaluation metrics. Overall, our contributions provide valuable insights into how graph cuts and boosted decision trees can be leveraged together to design effective hash functions for large-scale image and video databases.",1
"We consider supervised learning with random decision trees, where the tree construction is completely random. The method is popularly used and works well in practice despite the simplicity of the setting, but its statistical mechanism is not yet well-understood. In this paper we provide strong theoretical guarantees regarding learning with random decision trees. We analyze and compare three different variants of the algorithm that have minimal memory requirements: majority voting, threshold averaging and probabilistic averaging. The random structure of the tree enables us to adapt these methods to a differentially-private setting thus we also propose differentially-private versions of all three schemes. We give upper-bounds on the generalization error and mathematically explain how the accuracy depends on the number of random decision trees. Furthermore, we prove that only logarithmic (in the size of the dataset) number of independently selected random decision trees suffice to correctly classify most of the data, even when differential-privacy guarantees must be maintained. We empirically show that majority voting and threshold averaging give the best accuracy, also for conservative users requiring high privacy guarantees. Furthermore, we demonstrate that a simple majority voting rule is an especially good candidate for the differentially-private classifier since it is much less sensitive to the choice of forest parameters than other methods.",0
"Here is an example for an article discussing neural networks as part of a software pipeline: [Article](https://www.nature.com/articles/s42257-019-0186-x) This article presents results from applying differentially- and non-differentially-private random decision trees (ddPDTs) to predict whether patients develop chronic obstructive pulmonary disease after surgery (COPD). The application demonstrates that ddPDT can provide accurate predictions on new patient cohorts and could potentially reduce costs by identifying patients who would benefit most from additional post-operative care without sacrificing privacy. These findings have implications for the use of artificial intelligence models in personalized medicine research given their ability to balance data sharing concerns while achieving high prediction performance.  What should I replace ""DDT"" in order to fit the theme of your project? Differential privacy techniques applied to machine learning algorithms raise several questions regarding their effectiveness for ensuring data protection and maintaining utility of generated models. In this work we investigate the impact of varying levels of perturbation used during the training process of random decision tree ensembles built using both differentially-privacy preserving techniques and randomization methods. We demonstrate through comprehensive experiments carried out on several public datasets that there exists an optimal range for the privacy budget within which one can achieve good prediction accuracy. Beyond these limits, either overfitting occurs or underfitting happens due to insufficient exploration of the parameter space. Our study suggests that the choice of the appropriate combination of private model ensemble size, number of iterations per round, and hyperparameters tuning has crucial influence on the resulting tradeoff between privacy guarantees and prediction quality. Finally, we contribute novel codebase to facilitate reproducibility of our results. -----",1
"Identification of the influential clinical symptoms and laboratory features that help in the diagnosis of dengue fever in early phase of the illness would aid in designing effective public health management and virological surveillance strategies. Keeping this as our main objective we develop in this paper, a new computational intelligence based methodology that predicts the diagnosis in real time, minimizing the number of false positives and false negatives. Our methodology consists of three major components (i) a novel missing value imputation procedure that can be applied on any data set consisting of categorical (nominal) and/or numeric (real or integer) (ii) a wrapper based features selection method with genetic search for extracting a subset of most influential symptoms that can diagnose the illness and (iii) an alternating decision tree method that employs boosting for generating highly accurate decision rules. The predictive models developed using our methodology are found to be more accurate than the state-of-the-art methodologies used in the diagnosis of the dengue fever.",0
"An efficient method has been proposed for the computer-aided diagnosis (CAD) of dengue fever using intelligence based approach. A total of 2760 infrared thermography images of breast tissue were collected from patients suspected of having benign lesions such as fatty necrosis, fibroadenomas and invasive carcinoma, and then partitioned into training (85%) and testing sets (15%). Our results showed that our proposed system can achieve high accuracy rate of up to 94% on mammographic masses classification and up to 95% on breast cancer detection. In addition, feature selection was conducted beforehand to determine the most important features which highly contributed to distinguish normal cases from abnormal ones. From the statistical analysis performed, we found that the support vector machines (SVMs) model outperformed all other models. Therefore, SVMs have proven their effectiveness as a learning algorithm for medical image diagnosis tasks. Lastly, the developed method shows promising performance on both classification task and mass detection indicating the feasibility of utilizing machine intelligence for CAD application. Overall, our work provides valuable insights into new methods of improving CAD systems, thus ultimately leading to better healthcare management and patient outcome.",1
"In (\cite{zhang2014nonlinear,zhang2014nonlinear2}), we have viewed machine learning as a coding and dimensionality reduction problem, and further proposed a simple unsupervised dimensionality reduction method, entitled deep distributed random samplings (DDRS). In this paper, we further extend it to supervised learning incrementally. The key idea here is to incorporate label information into the coding process by reformulating that each center in DDRS has multiple output units indicating which class the center belongs to. The supervised learning method seems somewhat similar with random forests (\cite{breiman2001random}), here we emphasize their differences as follows. (i) Each layer of our method considers the relationship between part of the data points in training data with all training data points, while random forests focus on building each decision tree on only part of training data points independently. (ii) Our method builds gradually-narrowed network by sampling less and less data points, while random forests builds gradually-narrowed network by merging subclasses. (iii) Our method is trained more straightforward from bottom layer to top layer, while random forests build each tree from top layer to bottom layer by splitting. (iv) Our method encodes output targets implicitly in sparse codes, while random forests encode output targets by remembering the class attributes of the activated nodes. Therefore, our method is a simpler, more straightforward, and maybe a better alternative choice, though both methods use two very basic elements---randomization and nearest neighbor optimization---as the core. This preprint is used to protect the incremental idea from (\cite{zhang2014nonlinear,zhang2014nonlinear2}). Full empirical evaluation will be announced carefully later.",0
"In recent years, deep learning has emerged as one of the most powerful techniques for solving complex problems across many fields. However, the use of random samplings in deep learning can lead to biased results and overfitting, especially when dealing with large datasets. To address these issues, we propose a new method called deep distributed random sampling (DDRS) that leverages parallel computing resources to create multiple mini-batches from different parts of the data and learn from each batch independently. Our approach combines the advantages of both stochastic gradient descent and bagging ensembles, allowing us to efficiently train neural networks on massive datasets while reducing bias and improving generalization performance. We demonstrate the effectiveness of DDRS using several benchmarks and show that our method outperforms state-of-the-art alternatives such as random forests. Overall, our work presents a promising direction for developing robust machine learning models capable of handling big data tasks without sacrificing accuracy or interpretability.",1
"A lot of time is spent searching for the most performing data mining algorithms applied in clinical diagnosis. The study set out to identify the most performing predictive data mining algorithms applied in the diagnosis of Erythemato-squamous diseases. The study used Naive Bayes, Multilayer Perceptron and J48 decision tree induction to build predictive data mining models on 366 instances of Erythemato-squamous diseases datasets. Also, 10-fold cross-validation and sets of performance metrics were used to evaluate the baseline predictive performance of the classifiers. The comparative analysis shows that the Naive Bayes performed best with accuracy of 97.4%, Multilayer Perceptron came out second with accuracy of 96.6%, and J48 came out the worst with accuracy of 93.5%. The evaluation of these classifiers on clinical datasets, gave an insight into the predictive ability of different data mining algorithms applicable in clinical diagnosis especially in the diagnosis of Erythemato-squamous diseases.",0
"In recent years, predictive data mining algorithms have emerged as promising tools for diagnosing diseases such as erythema squamous disease (ESD), which can cause serious health problems if left untreated. These algorithms use large amounts of medical data to identify patterns that could indicate the presence of specific diseases like ESD. This study aimed to evaluate the effectiveness of different predictive data mining algorithms in accurately detecting cases of ESD using real-world datasets. We compared four popular algorithms - decision tree, random forest, artificial neural network, and gradient boosting - against traditional diagnostic methods used by physicians, including physical examination and laboratory tests. Results showed that all four algorithms outperformed standard methods in terms of accuracy, sensitivity, and specificity, suggesting their potential value in improving early detection rates of ESD. However, we found that each algorithm has unique strengths and weaknesses depending on the dataset, demonstrating the need to carefully select appropriate techniques based on specific contextual factors. Our findings contribute to advancing knowledge in predictive model development using big data sources for enhanced clinical care.",1
"Recently proposed budding tree is a decision tree algorithm in which every node is part internal node and part leaf. This allows representing every decision tree in a continuous parameter space, and therefore a budding tree can be jointly trained with backpropagation, like a neural network. Even though this continuity allows it to be used in hierarchical representation learning, the learned representations are local: Activation makes a soft selection among all root-to-leaf paths in a tree. In this work we extend the budding tree and propose the distributed tree where the children use different and independent splits and hence multiple paths in a tree can be traversed at the same time. This ability to combine multiple paths gives the power of a distributed representation, as in a traditional perceptron layer. We show that distributed trees perform comparably or better than budding and traditional hard trees on classification and regression tasks.",0
"In recent years, decision trees have become one of the most popular models for machine learning due to their simplicity and interpretability. However, traditional decision tree algorithms suffer from several limitations such as poor scalability, lack of parallelism, and difficulty handling high-dimensional data. To address these issues, we propose distributed decision trees (DDT), which combine the strengths of distributed computing and decision trees. DDT allows us to train very large decision trees by distributing both the data and computation across multiple nodes in a cluster. We use MPI, a widely used communication protocol for distributed computing, to implement our algorithm. Our experiments on real datasets show that DDT significantly improves scalability over single node implementations while maintaining the accuracy of the model. Furthermore, our approach is able to handle high dimensionality better than other state-of-the-art methods. Finally, since decision trees can often produce interpretable results, our method provides a powerful tool for data analysts looking to uncover hidden patterns in complex data sets. Overall, our work advances the field of decision tree research by providing a more flexible framework capable of solving larger scale problems without sacrificing model quality.",1
"Edge detection is a critical component of many vision systems, including object detectors and image segmentation algorithms. Patches of edges exhibit well-known forms of local structure, such as straight lines or T-junctions. In this paper we take advantage of the structure present in local image patches to learn both an accurate and computationally efficient edge detector. We formulate the problem of predicting local edge masks in a structured learning framework applied to random decision forests. Our novel approach to learning decision trees robustly maps the structured labels to a discrete space on which standard information gain measures may be evaluated. The result is an approach that obtains realtime performance that is orders of magnitude faster than many competing state-of-the-art approaches, while also achieving state-of-the-art edge detection results on the BSDS500 Segmentation dataset and NYU Depth dataset. Finally, we show the potential of our approach as a general purpose edge detector by showing our learned edge models generalize well across datasets.",0
"Here is your requested abstract:  Fast edge detection is an important problem in computer vision with applications ranging from image processing to object recognition. Traditional methods such as Canny edge detector can produce high quality results but have relatively high computational cost due to multiple passes over the image and complex filtering steps. In order to address this issue, we propose using structured forests which are able to learn hierarchical representations of edges by learning to vote on local features extracted at different scales in a divide-and-conquer approach. Our method has the advantages of running in linear time with respect to the size of the input image while achieving comparable or better performance than current state of the art techniques. We validate our proposed method through extensive experiments on a variety of datasets demonstrating the improved accuracy, speed and robustness compared to existing methods. Additionally, we show that our approach generalizes well across domains and captures more fine grained details along the contours without increasing complexity.",1
"Even with the advent of more sophisticated, data-hungry methods, boosted decision trees remain extraordinarily successful for fast rigid object detection, achieving top accuracy on numerous datasets. While effective, most boosted detectors use decision trees with orthogonal (single feature) splits, and the topology of the resulting decision boundary may not be well matched to the natural topology of the data. Given highly correlated data, decision trees with oblique (multiple feature) splits can be effective. Use of oblique splits, however, comes at considerable computational expense. Inspired by recent work on discriminative decorrelation of HOG features, we instead propose an efficient feature transform that removes correlations in local neighborhoods. The result is an overcomplete but locally decorrelated representation ideally suited for use with orthogonal decision trees. In fact, orthogonal trees with our locally decorrelated features outperform oblique trees trained over the original features at a fraction of the computational cost. The overall improvement in accuracy is dramatic: on the Caltech Pedestrian Dataset, we reduce false positives nearly tenfold over the previous state-of-the-art.",0
"This paper presents a novel approach for improving object detection by decorrelating local features. We propose using spatial pyramid pooling layers (SPP) in convolutional neural networks to reduce feature variation within small regions, leading to better detector performance. Our method achieves state-of-the-art results on multiple benchmarks while maintaining computational efficiency. Experiments demonstrate that our technique effectively reduces dependencies between neighboring regions and increases reliability of detected objects. Further analysis confirms the effectiveness of SPP in extracting more robust features for accurate object recognition. Overall, we make significant contributions to the field of computer vision and object detection.",1
"We study nonlinear regression of real valued data in an individual sequence manner, where we provide results that are guaranteed to hold without any statistical assumptions. We address the convergence and undertraining issues of conventional nonlinear regression methods and introduce an algorithm that elegantly mitigates these issues via an incremental hierarchical structure, (i.e., via an incremental decision tree). Particularly, we present a piecewise linear (or nonlinear) regression algorithm that partitions the regressor space in a data driven manner and learns a linear model at each region. Unlike the conventional approaches, our algorithm gradually increases the number of disjoint partitions on the regressor space in a sequential manner according to the observed data. Through this data driven approach, our algorithm sequentially and asymptotically achieves the performance of the optimal twice differentiable regression function for any data sequence with an unknown and arbitrary length. The computational complexity of the introduced algorithm is only logarithmic in the data length under certain regularity conditions. We provide the explicit description of the algorithm and demonstrate the significant gains for the well-known benchmark real data sets and chaotic signals.",0
"This paper presents a new approach to regresssion that achieves nearly optimal accuracy on benchmark datasets, without the need for complex differentiability assumptions or prior knowledge. We propose a simple yet effective algorithm based on random matrix factorization techniques, which performs competitively compared to state-of-the-art methods such as Lasso regression and Support Vector Machines. Our method can handle high dimensionality and sparse data, making it suitable for many real-world applications. Additionally, we provide a theoretical analysis of our algorithm's performance using concepts from functional analysis and operator theory. Overall, these results demonstrate the potential utility of our approach for practitioners and researchers alike.",1
"We discuss an autoencoder model in which the encoding and decoding functions are implemented by decision trees. We use the soft decision tree where internal nodes realize soft multivariate splits given by a gating function and the overall output is the average of all leaves weighted by the gating values on their path. The encoder tree takes the input and generates a lower dimensional representation in the leaves and the decoder tree takes this and reconstructs the original input. Exploiting the continuity of the trees, autoencoder trees are trained with stochastic gradient descent. On handwritten digit and news data, we see that the autoencoder trees yield good reconstruction error compared to traditional autoencoder perceptrons. We also see that the autoencoder tree captures hierarchical representations at different granularities of the data on its different levels and the leaves capture the localities in the input space.",0
"One approach to representation learning that has gained attention recently is autoencoding trees (AET). Inspired by the success of autoencoders in deep learning applications, researchers have sought ways to extend these models to work with tree structures, where each node can represent a feature and its children can represent subsets of features selected from their parents. This allows for hierarchical decomposition of data into increasingly granular subspaces. By training an autoencoder on such a structure, we can learn compact yet expressive representations of inputs. We describe how to train autoencoders on tree structures in detail, discussing both batch and online variants. Our experimental results show competitive performance compared to existing state-of-the-art methods across several tasks, while using smaller models with fewer parameters. Finally, we present qualitative analysis showing that our method learns meaningful features based on human priors. Overall, we believe this paper offers important contributions to representation learning via autoencoded tree structures.",1
"We describe a method for visual object detection based on an ensemble of optimized decision trees organized in a cascade of rejectors. The trees use pixel intensity comparisons in their internal nodes and this makes them able to process image regions very fast. Experimental analysis is provided through a face detection problem. The obtained results are encouraging and demonstrate that the method has practical value. Additionally, we analyse its sensitivity to noise and show how to perform fast rotation invariant object detection. Complete source code is provided at https://github.com/nenadmarkus/pico.",0
"In this paper we propose a novel approach to object detection using pixel intensity comparisons organized into decision trees. Our method is based on the insight that objects differ from their backgrounds in systematic ways that can be captured by looking at the local patterns of intensities. By modeling these relationships as functions that map input images onto classification decisions, our algorithm achieves state-of-the art performance without relying on convolutional neural networks or other deep learning techniques. We evaluate our method on several standard benchmark datasets and show that it outperforms many competitive methods across a range of metrics. Overall, we believe our work represents an important contribution to computer vision research and demonstrates the potential of simple algorithms for solving complex tasks.",1
"In this Technical Report we propose a set of improvements with respect to the KernelBoost classifier presented in [Becker et al., MICCAI 2013]. We start with a scheme inspired by Auto-Context, but that is suitable in situations where the lack of large training sets poses a potential problem of overfitting. The aim is to capture the interactions between neighboring image pixels to better regularize the boundaries of segmented regions. As in Auto-Context [Tu et al., PAMI 2009] the segmentation process is iterative and, at each iteration, the segmentation results for the previous iterations are taken into account in conjunction with the image itself. However, unlike in [Tu et al., PAMI 2009], we organize our recursion so that the classifiers can progressively focus on difficult-to-classify locations. This lets us exploit the power of the decision-tree paradigm while avoiding over-fitting. In the context of this architecture, KernelBoost represents a powerful building block due to its ability to learn on the score maps coming from previous iterations. We first introduce two important mechanisms to empower the KernelBoost classifier, namely pooling and the clustering of positive samples based on the appearance of the corresponding ground-truth. These operations significantly contribute to increase the effectiveness of the system on biomedical images, where texture plays a major role in the recognition of the different image components. We then present some other techniques that can be easily integrated in the KernelBoost framework to further improve the accuracy of the final segmentation. We show extensive results on different medical image datasets, including some multi-label tasks, on which our method is shown to outperform state-of-the-art approaches. The resulting segmentations display high accuracy, neat contours, and reduced noise.",0
"This paper presents a new approach to regression analysis that builds upon and extends existing kernel methods, specifically the popular kernel ridge regression (KRR) algorithm. Our proposed method, which we call ""Beyond KernelBoost,"" offers several key advantages over traditional KRR techniques. Firstly, our method allows for more efficient computation by exploiting sparse data structures and iterative approximation algorithms. Secondly, we introduce a novel loss function that is better suited for certain types of data, resulting in improved prediction accuracy. Finally, we demonstrate how our framework can easily integrate other machine learning models as base learners, further enhancing its flexibility and applicability across a range of domains. We evaluate our method using comprehensive experiments on both synthetic and real-world datasets, showing consistent improvements in performance compared to state-of-the-art KRR approaches. Overall, Beyond KernelBoost represents a significant advancement in nonparametric regression modeling, providing researchers and practitioners with a powerful tool for solving complex problems in a wide variety of fields.",1
"We consider the problem of learning a forest of nonlinear decision rules with general loss functions. The standard methods employ boosted decision trees such as Adaboost for exponential loss and Friedman's gradient boosting for general loss. In contrast to these traditional boosting algorithms that treat a tree learner as a black box, the method we propose directly learns decision forests via fully-corrective regularized greedy search using the underlying forest structure. Our method achieves higher accuracy and smaller models than gradient boosting (and Adaboost with exponential loss) on many datasets.",0
"In recent years, there has been significant interest in developing machine learning models that can accurately predict nonlinear relationships between input variables and output targets. One approach to modeling these complex relationships is through the use of random forest algorithms, which have proven effective at capturing nonlinearities using ensembles of decision trees. However, random forests can suffer from overfitting due to their tendency to fit noisy data, leading to poor generalization performance on unseen test sets. This work proposes a novel regularization method called ""Learning Nonlinear Functions Using Regularized Greedy Forest"" (LNFRF) to address this issue by introducing additional constraints during the training process. LNFRF utilizes a greedy strategy to construct each individual tree, encouraging sparsity in both feature selection and split choices. Simulation results show that LNFRF outperforms state-of-the-art random forest approaches, achieving improved accuracy while reducing overfitting. The proposed method demonstrates promise as an effective tool for modeling nonlinear functions across a range of applications, including regression and classification problems. Further analysis explores the effectiveness of different combinations of regularization terms, providing insights into how these terms interact within the LNFRF framework. Overall, the findings contribute new knowledge on efficient methods for capturing complex nonlinear patterns in high dimensional datasets, paving the way for future research in the development of more powerful machine learning models.",1
"Many state-of-the-art results obtained with deep networks are achieved with the largest models that could be trained, and if more computation power was available, we might be able to exploit much larger datasets in order to improve generalization ability. Whereas in learning algorithms such as decision trees the ratio of capacity (e.g., the number of parameters) to computation is very favorable (up to exponentially more parameters than computation), the ratio is essentially 1 for deep neural networks. Conditional computation has been proposed as a way to increase the capacity of a deep neural network without increasing the amount of computation required, by activating some parameters and computation ""on-demand"", on a per-example basis. In this note, we propose a novel parametrization of weight matrices in neural networks which has the potential to increase up to exponentially the ratio of the number of parameters to computation. The proposed approach is based on turning on some parameters (weight matrices) when specific bit patterns of hidden unit activations are obtained. In order to better control for the overfitting that might result, we propose a parametrization that is tree-structured, where each node of the tree corresponds to a prefix of a sequence of sign bits, or gating units, associated with hidden units.",0
"Deep learning has become increasingly popular due to recent breakthroughs in computer vision, speech recognition, natural language processing, robotics, and decision making systems. With these successes comes the need for faster and more efficient computation methods that can handle large datasets, complex models, and multiple modalities while maintaining high accuracy. One approach to improving computational efficiency in deep learning is through conditional computation. This method involves evaluating a neural network only on parts of the input where changes in the output may occur, thus significantly reducing unnecessary computations. However, current approaches to conditional computation suffer from low capacity limits and limited applicability to real-world applications. Our research proposes a novel exponentially increasing the capacity-to-computation ratio (C/CR) for conditional computation by developing a framework that incrementally samples subspaces during backpropagation. This framework adaptively adjusts to changing conditions throughout training and can be applied across different layers and models, leading to improved generalization and robustness. We demonstrate the effectiveness of our proposed approach using benchmark deep learning tasks, such as image classification and machine translation, and show significant improvements over existing methods. These results have important implications for accelerating deep learning research and broadening its scope of application.",1
In this research we address the problem of capturing recurring concepts in a data stream environment. Recurrence capture enables the re-use of previously learned classifiers without the need for re-learning while providing for better accuracy during the concept recurrence interval. We capture concepts by applying the Discrete Fourier Transform (DFT) to Decision Tree classifiers to obtain highly compressed versions of the trees at concept drift points in the stream and store such trees in a repository for future use. Our empirical results on real world and synthetic data exhibiting varying degrees of recurrence show that the Fourier compressed trees are more robust to noise and are able to capture recurring concepts with higher precision than a meta learning approach that chooses to re-use classifiers in their originally occurring form.,0
"This paper presents a novel method for mining recurrent concepts from data streams using the discrete Fourier transform (DFT). The proposed approach identifies patterns that repeat over time by analyzing the frequency components of the stream data. Our experimental results demonstrate the effectiveness of our algorithm compared to traditional methods such as sliding windows, achieving higher accuracy and better performance. We conclude that the DFT provides a powerful tool for detecting recurring structures in dynamic datasets, which has applications across many domains including anomaly detection, pattern recognition, and predictive analytics.",1
"Designing effective and efficient classifier for pattern analysis is a key problem in machine learning and computer vision. Many the solutions to the problem require to perform logic operations such as `and', `or', and `not'. Classification and regression tree (CART) include these operations explicitly. Other methods such as neural networks, SVM, and boosting learn/compute a weighted sum on features (weak classifiers), which weakly perform the 'and' and 'or' operations. However, it is hard for these classifiers to deal with the 'xor' pattern directly. In this paper, we propose layered logic classifiers for patterns of complicated distributions by combining the `and', `or', and `not' operations. The proposed algorithm is very general and easy to implement. We test the classifiers on several typical datasets from the Irvine repository and two challenging vision applications, object segmentation and pedestrian detection. We observe significant improvements on all the datasets over the widely used decision stump based AdaBoost algorithm. The resulting classifiers have much less training complexity than decision tree based AdaBoost, and can be applied in a wide range of domains.",0
"Title: ""Examining the Role of 'And' and 'Or' Relationships in Layered Logic Classifiers""  Logic classifiers have become increasingly popular due to their ability to model complex relationships and achieve high levels of accuracy in tasks such as classification and regression. However, little attention has been paid to how different logical operators, like 'And' and 'Or', affect the performance of these models. This paper seeks to fill that gap by exploring how layered logic classifiers can utilize 'And' and 'Or' relations to improve prediction accuracy.  The study begins by discussing the concept of layered logic classifiers and how they use multiple layers of logic rules to capture nonlinear relationships between features and outcomes. Then, we examine how 'And' and 'Or' relations can be integrated into these models and how they impact the overall performance of the classifier. We evaluate the effectiveness of different approaches using real-world datasets and demonstrate the improved prediction accuracy achieved through incorporating these logical operations.  In conclusion, our results show that integrating 'And' and 'Or' relations in layered logic classifiers enhances their performance and provides new insights into how these classifiers can effectively model complex relationships. Our findings provide important implications for machine learning practitioners and researchers who aim to develop more accurate predictive models. By better understanding how these logical operators impact the behavior of layered logic classifiers, we can unlock further improvements in model performance, leading to greater success in application areas such as healthcare, finance, and social sciences.",1
"Supervised hashing aims to map the original features to compact binary codes that are able to preserve label based similarity in the Hamming space. Non-linear hash functions have demonstrated the advantage over linear ones due to their powerful generalization capability. In the literature, kernel functions are typically used to achieve non-linearity in hashing, which achieve encouraging retrieval performance at the price of slow evaluation and training time. Here we propose to use boosted decision trees for achieving non-linearity in hashing, which are fast to train and evaluate, hence more suitable for hashing with high dimensional data. In our approach, we first propose sub-modular formulations for the hashing binary code inference problem and an efficient GraphCut based block search method for solving large-scale inference. Then we learn hash functions by training boosted decision trees to fit the binary codes. Experiments demonstrate that our proposed method significantly outperforms most state-of-the-art methods in retrieval precision and training time. Especially for high-dimensional data, our method is orders of magnitude faster than many methods in terms of training time.",0
"In recent years, data mining has become increasingly important as more high-dimensional datasets have been generated. To efficiently handle these large datasets, dimension reduction techniques such as supervised hashing have gained popularity. This paper presents a new method called ""Fast Supervised Hashing with Decision Trees"" (FSHT), which combines decision tree learning with supervised hashing to produce binary hash codes that accurately represent their original features while reducing dimensionality. Our proposed FSHT model uses an optimized approach based on XOR operation instead of elementwise multiplication to achieve faster training time and better performance compared to existing methods. Furthermore, our experimental results demonstrate that FSHT outperforms other state-of-the art algorithms in terms of accuracy, efficiency, sparsity, robustness, and scalability. Finally, we conclude by discussing potential future research directions related to fast supervised hashing with decision trees.",1
"Random Forest (RF) is a powerful ensemble method for classification and regression tasks. It consists of decision trees set. Although, a single tree is well interpretable for human, the ensemble of trees is a black-box model. The popular technique to look inside the RF model is to visualize a RF proximity matrix obtained on data samples with Multidimensional Scaling (MDS) method. Herein, we present a novel method based on Self-Organising Maps (SOM) for revealing intrinsic relationships in data that lay inside the RF used for classification tasks. We propose an algorithm to learn the SOM with the proximity matrix obtained from the RF. The visualization of RF proximity matrix with MDS and SOM is compared. What is more, the SOM learned with the RF proximity matrix has better classification accuracy in comparison to SOM learned with Euclidean distance. Presented approach enables better understanding of the RF and additionally improves accuracy of the SOM.",0
"In recent years, visualization techniques have become increasingly important in exploratory data analysis and machine learning. They provide insightful ways of identifying patterns and relationships among variables that would otherwise go unnoticed. This study focuses on visualizing random forest models using self-organizing maps (SOMs). SOMs are powerful tools for dimensionality reduction and clustering, making them well suited for analyzing high-dimensional datasets generated by modern machine learning algorithms such as random forest. By utilizing these techniques together, we can gain deeper insights into how random forest works and better understand its performance. Our findings demonstrate that SOMs can effectively map the complex relationship structure underlying random forest classifiers, allowing us to identify key features and clusters within the dataset. Additionally, our methodology provides a framework for interpreting random forest results and improving communication between domain experts and machine learning practitioners. Overall, this work highlights the importance of integrating advanced visualization methods into machine learning practice, ultimately leading to more accurate predictions and informed decision-making.",1
"The number of possible methods of generalizing binary classification to multi-class classification increases exponentially with the number of class labels. Often, the best method of doing so will be highly problem dependent. Here we present classification software in which the partitioning of multi-class classification problems into binary classification problems is specified using a recursive control language.",0
"An ideal abstract should tell the reader exactly how many authors there were, which departments they worked in, what they set out to accomplish, how they accomplished their work, why others would want read your paper, and what you discovered in your research that we don’t already know from the literature. You may need to add or subtract details as appropriate depending on whether you are presenting the abstract to other specialists (in which case you have less explaining to do), vs. non-specialist scientists. Note that you can add additional results sections if you wish, but normally most papers focus on only one important discovery at a time! So you could easily expand each section further if necessary. For example: Abstract: We sought to study whether there exists a correlation between coffee consumption and headache incidence rates across all age groups. To do so, we used cross-sectional data from over 2 million respondents gathered via phone surveys throughout the year by the National Health Interview Survey (NHIS) system of the United States of America CDC and CMS federal health agencies. Using statistical models that accounted for confounding variables such as age, gender, body mass index, exercise levels, and smoking habits, our analyses found no significant evidence supporting a positive association between these two parameters. In addition to providing novel insights into public health trends for government use, this null finding could potentially provide guidance for future studies seeking to identify new connections between dietary factors and common diseases. Our findings may serve as a reference point for meta-analyses, systematic reviews, guidelines, and textbooks seeking to summarize current knowledge on both caffeine safety profiles and associated adverse side effects. As these results contrast previous claims made based on smaller sample sizes and/or poorer methodology, we hope tha",1
"Clever sampling methods can be used to improve the handling of big data and increase its usefulness. The subject of this study is remote sensing, specifically airborne laser scanning point clouds representing different classes of ground cover. The aim is to derive a supervised learning model for the classification using CARTs. In order to measure the effect of different sampling methods on the classification accuracy, various experiments with varying types of sampling methods, sample sizes, and accuracy metrics have been designed. Numerical results for a subset of a large surveying project covering the lower Rhine area in Germany are shown. General conclusions regarding sampling design are drawn and presented.",0
"This study investigates how choice of sampling methods affect prediction quality when classifying land cover using decision trees. The authors compare random sampling, cluster sampling, stratified sampling, systematic sampling, and simple random sampling techniques. They test their hypotheses by analyzing two datasets: one from Malaysia and another from Zimbabwe. Their findings suggest that both clustering and strati",1
"Classifiers trained on data sets possessing an imbalanced class distribution are known to exhibit poor generalisation performance. This is known as the imbalanced learning problem. The problem becomes particularly acute when we consider incremental classifiers operating on imbalanced data streams, especially when the learning objective is rare class identification. As accuracy may provide a misleading impression of performance on imbalanced data, existing stream classifiers based on accuracy can suffer poor minority class performance on imbalanced streams, with the result being low minority class recall rates. In this paper we address this deficiency by proposing the use of the Hellinger distance measure, as a very fast decision tree split criterion. We demonstrate that by using Hellinger a statistically significant improvement in recall rates on imbalanced data streams can be achieved, with an acceptable increase in the false positive rate.",0
"This paper introduces ""Hellinger distance trees"" (HT), which adapt online decision tree ensembles to imbalanced streams by balancing their accuracy and maintaining low error rates for both classes. HT uses a modification of the original concept introduced in [7] that allows parallel evaluation of multiple depth levels within each node. Our contribution focuses on using these multiple depth levels within nodes as hyperparameters that can learn how often to split off new child nodes, allowing trees to adjust more easily based on changing class distributions over time. We develop three adaptation strategies: a global strategy focused on equalizing distribution of data across all trees; local strategies where splits attempt to balance local decisions at individual internal nodes; and a hybrid approach that combines aspects from the two local approaches but only updates branches near leaf nodes. In extensive experiments, we find that our proposed methods significantly improve accuracy under skewed conditions compared to existing streaming models. Overall, our results contribute new methodologies and insights towards handling imbalanced stream data effectively via ensemble learning techniques.",1
"A novel hybrid data-driven approach is developed for forecasting power system parameters with the goal of increasing the efficiency of short-term forecasting studies for non-stationary time-series. The proposed approach is based on mode decomposition and a feature analysis of initial retrospective data using the Hilbert-Huang transform and machine learning algorithms. The random forests and gradient boosting trees learning techniques were examined. The decision tree techniques were used to rank the importance of variables employed in the forecasting models. The Mean Decrease Gini index is employed as an impurity function. The resulting hybrid forecasting models employ the radial basis function neural network and support vector regression. Apart from introduction and references the paper is organized as follows. The section 2 presents the background and the review of several approaches for short-term forecasting of power system parameters. In the third section a hybrid machine learning-based algorithm using Hilbert-Huang transform is developed for short-term forecasting of power system parameters. Fourth section describes the decision tree learning algorithms used for the issue of variables importance. Finally in section six the experimental results in the following electric power problems are presented: active power flow forecasting, electricity price forecasting and for the wind speed and direction forecasting.",0
"Abstract Power systems face many challenges that must be addressed by utilities and policymakers in order to ensure reliable electricity production and distribution. One key aspect of power system management involves predicting future trends in parameters such as load demand, generation capacity, transmission line capacitance, and other critical variables. Accurate forecasting of these parameters can enable better planning and operation of power grids, leading to improved efficiency, cost savings, and reduced environmental impact. This study proposes a new methodology for power system parameter forecasting using Hilbert-Huang Transform (HHTransform) and machine learning techniques. HHT has been shown to effectively extract relevant features from time-series data, while machine learning algorithms provide powerful tools for pattern recognition and prediction model construction. By combining these two approaches, we aim to achieve more accurate predictions of power system parameters over short and long time horizons. In addition, this methodology allows for the incorporation of multiple types of data sources including historical measurements and external factors such as weather patterns and economic indicators. Overall, our proposed approach represents a promising advance in power system parameter forecasting that could greatly benefit utility operators and decision makers alike. Keywords: Hilbert-Huang Transform, Machine Learning, Power System Parameter Forecasting, Time Series Analysis",1
"We study the effectiveness of non-uniform randomized feature selection in decision tree classification. We experimentally evaluate two feature selection methodologies, based on information extracted from the provided dataset: $(i)$ \emph{leverage scores-based} and $(ii)$ \emph{norm-based} feature selection. Experimental evaluation of the proposed feature selection techniques indicate that such approaches might be more effective compared to naive uniform feature selection and moreover having comparable performance to the random forest algorithm [3]",0
"This paper presents a novel approach called Non-uniform feature sampling (NFS) that improves decision tree ensembles by training on subsets of features selected randomly from different distributions such as uniform and Gaussian distribution. Our experimental results demonstrate NFS significantly outperforms existing state-of-the-art methods including Random Under Sampling, Random Over Sampling and SMOTE in terms of accuracy and F1 score on imbalanced datasets. Furthermore, our analysis shows NFS leads to better calibration of decision boundaries resulting into superior performance compared to traditional oversampling techniques. Finally, we provide insights through visualization on why non-uniform sampling works better than uniform sampling. We conclude by discussing possible extensions of this work.",1
"High accuracy in cancer prediction is important to improve the quality of the treatment and to improve the rate of survivability of patients. As the data volume is increasing rapidly in the healthcare research, the analytical challenge exists in double. The use of effective sampling technique in classification algorithms always yields good prediction accuracy. The SEER public use cancer database provides various prominent class labels for prognosis prediction. The main objective of this paper is to find the effect of sampling techniques in classifying the prognosis variable and propose an ideal sampling method based on the outcome of the experimentation. In the first phase of this work the traditional random sampling and stratified sampling techniques have been used. At the next level the balanced stratified sampling with variations as per the choice of the prognosis class labels have been tested. Much of the initial time has been focused on performing the pre_processing of the SEER data set. The classification model for experimentation has been built using the breast cancer, respiratory cancer and mixed cancer data sets with three traditional classifiers namely Decision Tree, Naive Bayes and K-Nearest Neighbor. The three prognosis factors survival, stage and metastasis have been used as class labels for experimental comparisons. The results shows a steady increase in the prediction accuracy of balanced stratified model as the sample size increases, but the traditional approach fluctuates before the optimum results.",0
"Title: ""Improving Accuracy in Cancer Prognosis Prediction using Data Balancing Techniques""  Abstract: This study proposes a novel approach for improving the accuracy of cancer prognosis prediction by leveraging data balancing techniques. Traditional approaches rely heavily on imbalanced datasets, which can result in biased predictions and poor generalization performance. Our proposed method involves implementing a stratified sampling technique that ensures each class is represented equally in the dataset. By doing so, we aim to mitigate the impact of class imbalance while preserving important features necessary for accurate predictions. We validate our method through rigorous experimental evaluation on several well-known cancer datasets, demonstrating significant improvements over state-of-the-art methods in terms of accuracy, precision, recall, F1 score, and area under the receiver operating characteristic (ROC) curve. These results indicate that our proposed method provides a more robust and reliable solution for predicting patient survival outcomes, paving the way for enhanced medical decision making in the fight against cancer.",1
"In this paper a hybrid feature selection method is proposed which takes advantages of wrapper subset evaluation with a lower cost and improves the performance of a group of classifiers. The method uses combination of sample domain filtering and resampling to refine the sample domain and two feature subset evaluation methods to select reliable features. This method utilizes both feature space and sample domain in two phases. The first phase filters and resamples the sample domain and the second phase adopts a hybrid procedure by information gain, wrapper subset evaluation and genetic search to find the optimal feature space. Experiments carried out on different types of datasets from UCI Repository of Machine Learning databases and the results show a rise in the average performance of five classifiers (Naive Bayes, Logistic, Multilayer Perceptron, Best First Decision Tree and JRIP) simultaneously and the classification error for these classifiers decreases considerably. The experiments also show that this method outperforms other feature selection methods with a lower cost.",0
"This paper proposes a hybrid feature selection method that combines filter methods based on correlation analysis and wrapper methods that evaluate subsets of features through cross-validation techniques. The proposed approach seeks to improve the performance of a group of classification algorithms by identifying and selecting informative features from large datasets. Experimental results show promising improvements in accuracy, precision, recall, F1 score, area under receiver operating characteristic (ROC) curve and reduced computational time compared to traditional methods. The effectiveness and efficiency of our method were evaluated using several real world datasets across diverse domains including healthcare, bioinformatics, market basket analysis and image recognition. Our findings demonstrate that the proposed technique can effectively address issues related to high dimensionality, noise and redundancy in data, making it a valuable tool for researchers working on big data applications.",1
"In recent years the importance of finding a meaningful pattern from huge datasets has become more challenging. Data miners try to adopt innovative methods to face this problem by applying feature selection methods. In this paper we propose a new hybrid method in which we use a combination of resampling, filtering the sample domain and wrapper subset evaluation method with genetic search to reduce dimensions of Lung-Cancer dataset that we received from UCI Repository of Machine Learning databases. Finally, we apply some well- known classification algorithms (Na\""ive Bayes, Logistic, Multilayer Perceptron, Best First Decision Tree and JRIP) to the resulting dataset and compare the results and prediction rates before and after the application of our feature selection method on that dataset. The results show a substantial progress in the average performance of five classification algorithms simultaneously and the classification error for these classifiers decreases considerably. The experiments also show that this method outperforms other feature selection methods with a lower cost.",0
"This should only contain original content and no references from other sources.  The use of resampling techniques such as cross-validation has become standard practice in many fields where predictions need to be made using data. In some cases, these algorithms may suffer from overfitting due to high complexity, limited number of observations, or both. Overfitting can lead to poor generalization performance on unseen data, which means that the model won’t perform well when used in a real-world situation outside the training set. To improve performance, feature selection methods have been applied alongside model training to reduce the number of input features and their associated computations. However, traditional feature selection approaches often ignore how features might interact with each other, making them suboptimal for problems involving more complex interactions. We propose a framework that combines multiple models trained via random subsets of the entire dataset along with different combinations of variables to form improved predictive models. Our method uses permutation feature importance measures to rank variable importance and then applies sparse linear regression to identify subset of relevant features. Additionally, our approach selects subsets of instances (resamples) to build separate decision trees for all possible combinations of important input variables. Predictions are generated by aggregating outputs from individual decision trees weighted according to their accuracy on leave-one-out cross validation. These results show improved prediction accuracy compared to competing methods using the same datasets.",1
"We present a method for incorporating missing data in non-parametric statistical learning without the need for imputation. We focus on a tree-based method, Bayesian Additive Regression Trees (BART), enhanced with ""Missingness Incorporated in Attributes,"" an approach recently proposed incorporating missingness into decision trees (Twala, 2008). This procedure takes advantage of the partitioning mechanisms found in tree-based models. Simulations on generated models and real data indicate that our proposed method can forecast well on complicated missing-at-random and not-missing-at-random models as well as models where missingness itself influences the response. Our procedure has higher predictive performance and is more stable than competitors in many cases. We also illustrate BART's abilities to incorporate missingness into uncertainty intervals and to detect the influence of missingness on the model fit.",0
"In recent years there has been great interest in understanding the relationship between machine learning models and decision making under uncertainty. One particularly important approach that has emerged is Bayesian additive regression trees (BART), which can learn flexible predictions by modeling predictors as functions of each other using tree ensembles. We propose a new method called prediction with missing data via BART (PMDBART) that extends the standard BART framework to handle missing data. Our approach allows for efficient posterior inference by leveraging ideas from variational inference and stochastic gradient boosting. Experimental results on several benchmark datasets demonstrate the effectiveness of our algorithm in accurately handling missing data while achieving state-of-the-art performance compared to alternative approaches. Additionally, we showcase how PMDBART can be used to gain insights into real-world problems such as predicting risk assessment scores for offenders based on criminal justice data. Overall, our work highlights the importance of probabilistic reasoning in decision making under uncertainty, especially when dealing with incomplete or uncertain data.",1
"Data mining techniques on the biological analysis are spreading for most of the areas including the health care and medical information. We have applied the data mining techniques, such as KNN, SVM, MLP or decision trees over a unique dataset, which is collected from 16,380 analysis results for a year. Furthermore we have also used meta-classifiers to question the increased correlation rate between the liver disorder and the liver analysis outputs. The results show that there is a correlation among ALT, AST, Billirubin Direct and Billirubin Total down to 15% of error rate. Also the correlation coefficient is up to 94%. This makes possible to predict the analysis results from each other or disease patterns can be applied over the linear correlation of the parameters.",0
"This abstract describes the results of a study that evaluates the correlation between two types of liver analysis outputs using ensemble methods. The study found that by combining multiple different techniques for analyzing liver function, researchers can gain a more comprehensive understanding of how well the organ is working. These findings could have significant implications for improving patient care, particularly for those who require regular monitoring of their liver health. Overall, the research demonstrates the value of incorporating advanced statistical models into medical decision making processes.",1
"Within the framework of AdaBoost.MH, we propose to train vector-valued decision trees to optimize the multi-class edge without reducing the multi-class problem to $K$ binary one-against-all classifications. The key element of the method is a vector-valued decision stump, factorized into an input-independent vector of length $K$ and label-independent scalar classifier. At inner tree nodes, the label-dependent vector is discarded and the binary classifier can be used for partitioning the input space into two regions. The algorithm retains the conceptual elegance, power, and computational efficiency of binary AdaBoost. In experiments it is on par with support vector machines and with the best existing multi-class boosting algorithm AOSOLogitBoost, and it is significantly better than other known implementations of AdaBoost.MH.",0
"In this work we present an implementation of the AdaBoost algorithm that uses Multi-Class Hamming Trees as base classifiers. This approach leverages recent advances in training decision tree ensembles by introducing randomization during the split selection process at each iteration of the AdaBoost loop. We demonstrate through experiments on benchmark datasets that our method achieves state-of-the-art performance while requiring less computational resources compared to competing approaches such as gradient boosting machines and deep learning models. Furthermore, we provide insights into how these modifications affect individual decisions made throughout the feature construction pipeline using ablation studies. Our findings suggest that incorporating diversity via stochastic splitting yields significant benefits to AdaBoost, allowing us to achieve high accuracy even without strong regularizers like early stopping. As a result, we believe that AdaBoost.MH represents an attractive alternative to existing methods in computer vision tasks and beyond. -----",1
"For classifying time series, a nearest-neighbor approach is widely used in practice with performance often competitive with or better than more elaborate methods such as neural networks, decision trees, and support vector machines. We develop theoretical justification for the effectiveness of nearest-neighbor-like classification of time series. Our guiding hypothesis is that in many applications, such as forecasting which topics will become trends on Twitter, there aren't actually that many prototypical time series to begin with, relative to the number of time series we have access to, e.g., topics become trends on Twitter only in a few distinct manners whereas we can collect massive amounts of Twitter data. To operationalize this hypothesis, we propose a latent source model for time series, which naturally leads to a ""weighted majority voting"" classification rule that can be approximated by a nearest-neighbor classifier. We establish nonasymptotic performance guarantees of both weighted majority voting and nearest-neighbor classification under our model accounting for how much of the time series we observe and the model complexity. Experimental results on synthetic data show weighted majority voting achieving the same misclassification rate as nearest-neighbor classification while observing less of the time series. We then use weighted majority to forecast which news topics on Twitter become trends, where we are able to detect such ""trending topics"" in advance of Twitter 79% of the time, with a mean early advantage of 1 hour and 26 minutes, a true positive rate of 95%, and a false positive rate of 4%.",0
"In recent years, time series classification has become increasingly important in fields such as finance, healthcare, and environmental monitoring. While many approaches have been proposed, there remains a need for more effective models that can accurately classify nonstationary time series data. To address this challenge, we propose a novel latent source model (LSM) that leverages a combination of linear regression and kernel ridge regression techniques. Our approach is capable of handling both short and long memory processes while ensuring parsimonious estimates through regularization. Experiments on real datasets demonstrate the superiority of our method over existing state-of-the-art methods, including traditional parametric and nonparametric approaches.",1
"Misfire in an IC Engine continues to be a problem leading to reduced fuel efficiency, increased power loss and emissions containing heavy concentration of hydrocarbons. Misfiring creates a unique vibration pattern attributed to a particular cylinder. Useful features can be extracted from these patterns and can be analyzed to detect misfire. Statistical features from these vibration signals were extracted. Out of these, useful features were identified using the J48 decision tree algorithm and selected features were used for classification using the Kstar algorithm. In this paper performance analysis of Kstar algorithm is presented.",0
"This paper presents a novel approach to detecting misfires in internal combustion engines (ICE) using the k* algorithm from data mining. Misfires can cause significant damage to ICEs if left undetected, making detection crucial for engine maintenance. Current methods rely on sensors such as crankshaft position sensors which add weight, cost, and complexity to vehicles. Our proposed method uses non-intrusive sensor signals such as the engine coolant temperature (ECT), intake manifold pressure (MAP), and throttle opening (TPO). These signals have been shown to contain valuable information relating to engine performance.  The aim of this research was to develop an efficient method to accurately diagnose misfires without relying on additional hardware. This required collecting accurate labeled training datasets for use during the machine learning process. We acquired these datasets through rigorous experimentation on an engine test bed. Initially, the dataset contained 4896 instances across six classes representing different operating conditions and faults. After feature engineering we reduced our dataset to 2500 high quality labelled samples with better distribution.  Next, we applied the k* algorithm to separate the decision tree into two phases: one focused on identifying misfire prone zones in the parameter space that has high probability of misfires occurring, while the other concentrates on deciding whether each recorded sample falls within those regions or not. By doing so, we were able to reduce computation time by reducing the number of nodes that need to be visited.  Experimental results showed that our method achieved excellent accuracy of 97% compared against a rule based system currently deployed in production. Additionally, our model had a higher level of interpretability owed to its use of simple decision trees, providing insight into the factors contributing to misfires. Furthermore, extensive experiments performed under varying parameters demonstrated the robustness of the proposed method. With minimal modification, our framework could be adapted to work with any type of ICE suited to t",1
"Decision tree learning is a popular approach for classification and regression in machine learning and statistics, and Bayesian formulations---which introduce a prior distribution over decision trees, and formulate learning as posterior inference given data---have been shown to produce competitive performance. Unlike classic decision tree learning algorithms like ID3, C4.5 and CART, which work in a top-down manner, existing Bayesian algorithms produce an approximation to the posterior distribution by evolving a complete tree (or collection thereof) iteratively via local Monte Carlo modifications to the structure of the tree, e.g., using Markov chain Monte Carlo (MCMC). We present a sequential Monte Carlo (SMC) algorithm that instead works in a top-down manner, mimicking the behavior and speed of classic algorithms. We demonstrate empirically that our approach delivers accuracy comparable to the most popular MCMC method, but operates more than an order of magnitude faster, and thus represents a better computation-accuracy tradeoff.",0
"This paper presents a new method for particle filtering that uses a top-down approach instead of traditional bottom-up methods. By using a top-down approach, we can better capture the structure and uncertainty in our models, resulting in more accurate predictions. Our proposed method works by first constructing a tree of particles, then updating each leaf node based on incoming data, and finally propagating uncertainties upwards through the tree. We compare our results against several existing methods on three datasets, showing improved accuracy across all tasks. In conclusion, our method provides an effective tool for incorporating uncertainty into particle filtering applications.",1
"We explore the effect of introducing prior information into the intermediate level of neural networks for a learning task on which all the state-of-the-art machine learning algorithms tested failed to learn. We motivate our work from the hypothesis that humans learn such intermediate concepts from other individuals via a form of supervision or guidance using a curriculum. The experiments we have conducted provide positive evidence in favor of this hypothesis. In our experiments, a two-tiered MLP architecture is trained on a dataset with 64x64 binary inputs images, each image with three sprites. The final task is to decide whether all the sprites are the same or one of them is different. Sprites are pentomino tetris shapes and they are placed in an image with different locations using scaling and rotation transformations. The first part of the two-tiered MLP is pre-trained with intermediate-level targets being the presence of sprites at each location, while the second part takes the output of the first part as input and predicts the final task's target binary event. The two-tiered MLP architecture, with a few tens of thousand examples, was able to learn the task perfectly, whereas all other algorithms (include unsupervised pre-training, but also traditional algorithms like SVMs, decision trees and boosting) all perform no better than chance. We hypothesize that the optimization difficulty involved when the intermediate pre-training is not performed is due to the {\em composition} of two highly non-linear tasks. Our findings are also consistent with hypotheses on cultural learning inspired by the observations of optimization problems with deep learning, presumably because of effective local minima.",0
"""Prior knowledge plays a critical role in optimizing systems, from machine learning algorithms to industrial processes. In many cases, prior information can significantly improve performance by reducing uncertainty and allowing for better decision making. This study investigates the importance of prior knowledge for optimization using a variety of case studies. We demonstrate that incorporating prior knowledge into models and algorithms can lead to more accurate predictions, faster convergence rates, and improved efficiency overall. Additionally, we examine how different types of prior knowledge can impact optimization and discuss strategies for effectively integrating them into existing frameworks. Our results highlight the significance of considering prior knowledge in any optimization effort.""",1
"Dengue fever is a flu-like illness spread by the bite of an infected mosquito which is fast emerging as a major health problem. Timely and cost effective diagnosis using clinical and laboratory features would reduce the mortality rates besides providing better grounds for clinical management and disease surveillance. We wish to develop a robust and effective decision tree based approach for predicting dengue disease. Our analysis is based on the clinical characteristics and laboratory measurements of the diseased individuals. We have developed and trained an alternating decision tree with boosting and compared its performance with C4.5 algorithm for dengue disease diagnosis. Of the 65 patient records a diagnosis establishes that 53 individuals have been confirmed to have dengue fever. An alternating decision tree based algorithm was able to differentiate the dengue fever using the clinical and laboratory data with number of correctly classified instances as 89%, F-measure of 0.86 and receiver operator characteristics (ROC) of 0.826 as compared to C4.5 having correctly classified instances as 78%,h F-measure of 0.738 and ROC of 0.617 respectively. Alternating decision tree based approach with boosting has been able to predict dengue fever with a higher degree of accuracy than C4.5 based decision tree using simple clinical and laboratory features. Further analysis on larger data sets is required to improve the sensitivity and specificity of the alternating decision trees.",0
"Title: ""A Machine Learning Approach to Early Diagnosis of Dengue Fever""  Abstract: Dengue fever is a mosquito-borne viral disease that affects millions of individuals worldwide every year. Despite being preventable through vector control measures, diagnostics remain challenging due to nonspecific symptoms shared by other diseases such as malaria and typhoid. Early detection of dengue fever can significantly improve patient outcomes. Recently, machine learning algorithms have shown promise for automated diagnosis from electronic medical records (EMR).  In our study, we present alternating decision trees (ADT), a novel approach to classifying patients into categories based on EMR data. Our model integrates both clinical manifestations and laboratory tests commonly used to identify patients with dengue fever at an early stage. We validate ADT using leave-one-out cross validation with balanced accuracy, precision, recall, and F1 score metrics. To ensure generalizability across populations, our dataset includes patients from multiple hospitals spanning diverse geographic regions where dengue fever is prevalent.  Our results demonstrate that ADT achieves high performance compared to previous work, surpassing benchmark scores by up to 9%. Additionally, sensitivity analysis reveals low variability and robustness to missing values throughout the testing process. These findings support the feasibility of implementing ADT in resource-limited settings, potentially improving public health interventions for dengue fever management globally. Future research directions involve expanding the scope of machine learning models to integrate new modalities like genetic markers for more precise risk stratification. Overall, ADT offers a powerful tool for combatting the global burden of dengue fever while facilitating evidence-based practice for healthcare professionals.",1
"Many solutions to cost-sensitive classification (and regression) rely on some or all of the following assumptions: we have complete knowledge about the cost context at training time, we can easily re-train whenever the cost context changes, and we have technique-specific methods (such as cost-sensitive decision trees) that can take advantage of that information. In this paper we address the problem of selecting models and minimising joint cost (integrating both misclassification cost and test costs) without any of the above assumptions. We introduce methods and plots (such as the so-called JROC plots) that can work with any off-the-shelf predictive technique, including ensembles, such that we reframe the model to use the appropriate subset of attributes (the feature configuration) during deployment time. In other words, models are trained with the available attributes (once and for all) and then deployed by setting missing values on the attributes that are deemed ineffective for reducing the joint cost. As the number of feature configuration combinations grows exponentially with the number of features we introduce quadratic methods that are able to approximate the optimal configuration and model choices, as shown by the experimental results.",0
"This paper examines the trade-off between test cost and misclassification costs using a reframing approach. We present two case studies that illustrate how our methodology can be used to evaluate different classification algorithms and determine their effectiveness. Our results show that there exists a fundamental tension between these two types of costs, but we offer insights into how they can be balanced to achieve optimal performance. By carefully considering both test cost and misclassification costs, decision makers can make more informed decisions about which classifiers to use in specific applications. Overall, our study highlights the importance of adopting a holistic view of evaluation metrics rather than focusing solely on one type of cost. Abstract: In this research paper, we explore the relationship between test cost and misclassification cost in binary classification problems through the lens of reframing techniques. Two real-world examples are provided to demonstrate our methodology for evaluating classification models and determining their suitability for particular application scenarios. The experimental outcomes indicate that there is indeed a tradeoff between these two kinds of costs, while offering recommendations for achieving balance based on the given circumstances. Ultimately, this work underscores the necessity of taking into account multiple evaluation measures to ensure appropriate model selection for problem solving purposes.",1
"Mammography is the most effective and available tool for breast cancer screening. However, the low positive predictive value of breast biopsy resulting from mammogram interpretation leads to approximately 70% unnecessary biopsies with benign outcomes. Data mining algorithms could be used to help physicians in their decisions to perform a breast biopsy on a suspicious lesion seen in a mammogram image or to perform a short term follow-up examination instead. In this research paper data mining classification algorithms; Decision Tree (DT), Artificial Neural Network (ANN), and Support Vector Machine (SVM) are analyzed on mammographic masses data set. The purpose of this study is to increase the ability of physicians to determine the severity (benign or malignant) of a mammographic mass lesion from BI-RADS attributes and the patient,s age. The whole data set is divided for training the models and test them by the ratio of 70:30% respectively and the performances of classification algorithms are compared through three statistical measures; sensitivity, specificity, and classification accuracy. Accuracy of DT, ANN and SVM are 78.12%, 80.56% and 81.25% of test samples respectively. Our analysis shows that out of these three classification models SVM predicts severity of breast cancer with least error rate and highest accuracy.",0
"This study aimed to evaluate the performance of data mining methods in predicting the severity of breast masses using medical imaging. A dataset consisting of mammography images along with clinical details was used for training and testing different machine learning algorithms, including decision trees, random forests, support vector machines, and artificial neural networks. The severity of breast masses was classified into benign (non-malignant) and malignant (cancerous). The results showed that all four models performed well in predicting the severity of breast masses with high accuracy ranging from 86% - 97%. Additionally, feature selection techniques were applied to identify the most significant features contributing to mass severity prediction, which included technical parameters such as microcalcification presence and texture analysis measures. In conclusion, this research demonstrates the potential of data mining methods in assisting radiologists in making more accurate diagnoses by identifying relevant features and improving classification performance.",1
"Feature Selection (FS) has become the focus of much research on decision support systems areas for which data sets with tremendous number of variables are analyzed. In this paper we present a new method for the diagnosis of Coronary Artery Diseases (CAD) founded on Genetic Algorithm (GA) wrapped Bayes Naive (BN) based FS. Basically, CAD dataset contains two classes defined with 13 features. In GA BN algorithm, GA generates in each iteration a subset of attributes that will be evaluated using the BN in the second step of the selection procedure. The final set of attribute contains the most relevant feature model that increases the accuracy. The algorithm in this case produces 85.50% classification accuracy in the diagnosis of CAD. Thus, the asset of the Algorithm is then compared with the use of Support Vector Machine (SVM), MultiLayer Perceptron (MLP) and C4.5 decision tree Algorithm. The result of classification accuracy for those algorithms are respectively 83.5%, 83.16% and 80.85%. Consequently, the GA wrapped BN Algorithm is correspondingly compared with other FS algorithms. The Obtained results have shown very promising outcomes for the diagnosis of CAD.",0
"Title: A Novel Approach to Coronary Artery Disease Diagnosis Using Supervised Feature Selection with Genetic Algorithms  Coronary artery disease (CAD) remains one of the leading causes of death worldwide, making accurate diagnosis essential for timely treatment and improved patient outcomes. Recent advances in high-throughput genomic technologies have led to large amounts of genetic data becoming available that could potentially aid in CAD diagnosis. However, feature selection has emerged as a significant challenge due to the vast number of possible predictors that may impact diagnostic accuracy. To address this problem, we propose using supervised feature selection methods combined with genetic algorithms (GAs).  The GA optimization approach involves creating a population of candidate subsets derived from the entire set of features and iteratively improving them by applying principles of natural selection such as mutation and crossover operations. This results in a diverse pool of models which can then be evaluated based on their ability to discriminate between cases with and without coronary artery disease. We use a random forest algorithm as our modeling framework since it provides robust performance while handling both categorical and continuous variables.  Our method was tested using a publicly available dataset comprising 96 patients along with clinical risk factors and genome-wide single nucleotide polymorphism (SNP) data. Our results showed that supervised feature selection using genetic algorithms achieved higher area under the receiver operating characteristic curve (AUROC) values compared to unsupervised feature selection techniques, demonstrating superiority at identifying relevant features for CAD diagnosis. Additionally, pathway analysis revealed novel biological mechanisms underlying cardiovascular disease progression that were previously unknown, providing insights into potential drug targets and therapies.  In summary, we present a novel approach to coronary artery disease diagnosis using supervised feature selection with ge",1
"The paper presents a time-series-based classification approach to identify similarities in pairs of simulated human-generated patterns. An example for a pattern is a time-series representing a heart rate during a specific time-range, wherein the time-series is a sequence of data points that represent the changes in the heart rate values. A bio-medical simulator system was developed to acquire a collection of 7,871 price patterns of financial instruments. The financial instruments traded in real-time on three American stock exchanges, NASDAQ, NYSE, and AMEX, simulate bio-medical measurements. The system simulates a human in which each price pattern represents one bio-medical sensor. Data provided during trading hours from the stock exchanges allowed real-time classification. Classification is based on new machine learning techniques: self-labeling, which allows the application of supervised learning methods on unlabeled time-series and similarity ranking, which applied on a decision tree learning algorithm to classify time-series regardless of type and quantity.",0
"""This"" refers to the beginning of something. I have no context, so don't know how to fill in the blank; therefore I cannot provide any other guidance regarding your request. Please give more details. What else would you like me to say?",1
"We introduce a new model of membership query (MQ) learning, where the learning algorithm is restricted to query points that are \emph{close} to random examples drawn from the underlying distribution. The learning model is intermediate between the PAC model (Valiant, 1984) and the PAC+MQ model (where the queries are allowed to be arbitrary points).   Membership query algorithms are not popular among machine learning practitioners. Apart from the obvious difficulty of adaptively querying labelers, it has also been observed that querying \emph{unnatural} points leads to increased noise from human labelers (Lang and Baum, 1992). This motivates our study of learning algorithms that make queries that are close to examples generated from the data distribution.   We restrict our attention to functions defined on the $n$-dimensional Boolean hypercube and say that a membership query is local if its Hamming distance from some example in the (random) training data is at most $O(\log(n))$. We show the following results in this model:   (i) The class of sparse polynomials (with coefficients in R) over $\{0,1\}^n$ is polynomial time learnable under a large class of \emph{locally smooth} distributions using $O(\log(n))$-local queries. This class also includes the class of $O(\log(n))$-depth decision trees.   (ii) The class of polynomial-sized decision trees is polynomial time learnable under product distributions using $O(\log(n))$-local queries.   (iii) The class of polynomial size DNF formulas is learnable under the uniform distribution using $O(\log(n))$-local queries in time $n^{O(\log(\log(n)))}$.   (iv) In addition we prove a number of results relating the proposed model to the traditional PAC model and the PAC+MQ model.",0
"In recent years, deep learning has revolutionized fields such as computer vision, natural language processing, and autonomous systems by achieving state-of-the-art results on benchmark datasets. One drawback of existing deep learning methods is their reliance on large amounts of data for training, which can limit their applicability to domains where labeled data is scarce or expensive to obtain. To address this challenge, we propose a novel approach that leverages local membership queries to enable efficient and effective learning from limited amounts of data. Our methodology allows for fine-grained control over the type and size of query sets used for model improvement, enabling flexible integration into a variety of application scenarios. We demonstrate our method's effectiveness across several challenging tasks, including image classification, sentiment analysis, and reinforcement learning, and show consistent improvements compared to baseline models trained without queries. Furthermore, we provide theoretical insights into the behavior of our method and discuss potential extensions and applications for future work. Overall, our research introduces a powerful new tool for deep learning practitioners working with limited data resources while advancing the broader field of machine learning.",1
"We study the complexity of approximate representation and learning of submodular functions over the uniform distribution on the Boolean hypercube $\{0,1\}^n$. Our main result is the following structural theorem: any submodular function is $\epsilon$-close in $\ell_2$ to a real-valued decision tree (DT) of depth $O(1/\epsilon^2)$. This immediately implies that any submodular function is $\epsilon$-close to a function of at most $2^{O(1/\epsilon^2)}$ variables and has a spectral $\ell_1$ norm of $2^{O(1/\epsilon^2)}$. It also implies the closest previous result that states that submodular functions can be approximated by polynomials of degree $O(1/\epsilon^2)$ (Cheraghchi et al., 2012). Our result is proved by constructing an approximation of a submodular function by a DT of rank $4/\epsilon^2$ and a proof that any rank-$r$ DT can be $\epsilon$-approximated by a DT of depth $\frac{5}{2}(r+\log(1/\epsilon))$.   We show that these structural results can be exploited to give an attribute-efficient PAC learning algorithm for submodular functions running in time $\tilde{O}(n^2) \cdot 2^{O(1/\epsilon^{4})}$. The best previous algorithm for the problem requires $n^{O(1/\epsilon^{2})}$ time and examples (Cheraghchi et al., 2012) but works also in the agnostic setting. In addition, we give improved learning algorithms for a number of related settings.   We also prove that our PAC and agnostic learning algorithms are essentially optimal via two lower bounds: (1) an information-theoretic lower bound of $2^{\Omega(1/\epsilon^{2/3})}$ on the complexity of learning monotone submodular functions in any reasonable model; (2) computational lower bound of $n^{\Omega(1/\epsilon^{2/3})}$ based on a reduction to learning of sparse parities with noise, widely-believed to be intractable. These are the first lower bounds for learning of submodular functions over the uniform distribution.",0
"This study presents three contributions: the first representation allowing to learn submodular functions from noisy data; the second efficient approximation algorithm for general submodular set functions reaching unprecedented accuracy; and the third application providing state-of-the-art results on several datasets that has been used as benchmark by the community (Submodular Minimization in CVPR). These advances have high potential impact on real applications such as image processing (e.g., ranking images), bioinformatics (ranking genes) or document retrieval (web search optimization). Moreover these contributions open new research directions towards learning objectives beyond cardinality constraints. For each contribution we provide a corresponding numerical section dedicated to experiments assessing the validity of our method. All the material required to reproduce these experimentations is available online and can easily be run without modifying them. Finally, future works are discussed regarding both theoretical properties and applications. This study proposes a novel approach for representing, approximating, and learning submodular functions using low-rank decision trees. We present three main contributions: a representation capable of learning submodular functions from noisy data, an efficient approximation algorithm achieving unprecedented accuracy for general submodular set functions, and an application resulting in state-of-the-art performance on several widely used benchmark datasets (such as those used in the field of Computer Vision for image processing tasks like ranking images). Our methods have significant potential for use in numerous practical domains including bioinformatics (e.g., gene ranking) and web search optimization for document retrieval. Each of our contributions includes accompanying experimental sections evaluating their effectiveness, which may be replicated using freely accessible materials provided online. Furthermore, the work opens up fresh perspectives concerning the development of objectives beyond simple cardinality restrictions. Lastly, we discuss promising avenues for future research encompassing both theoretical aspects and possible applications.",1
"This paper describes experiments, on two domains, to investigate the effect of averaging over predictions of multiple decision trees, instead of using a single tree. Other authors have pointed out theoretical and commonsense reasons for preferring the multiple tree approach. Ideally, we would like to consider predictions from all trees, weighted by their probability. However, there is a vast number of different trees, and it is difficult to estimate the probability of each tree. We sidestep the estimation problem by using a modified version of the ID3 algorithm to build good trees, and average over only these trees. Our results are encouraging. For each domain, we managed to produce a small number of good trees. We find that it is best to average across sets of trees with different structure; this usually gives better performance than any of the constituent trees, including the ID3 tree.",0
"This research explores the use of multiple decision trees in data analysis and machine learning applications. Decision tree algorithms are popular due to their ease of interpretation and ability to handle complex relationships within datasets. However, a single decision tree can often suffer from overfitting and may not capture all relevant patterns present in the data.  Incorporating multiple decision trees into model building provides several benefits. Firstly, it enables the construction of more robust models by reducing the risk of overfitting. Secondly, it allows for the integration of multiple perspectives on problem solving, leading to a better understanding of the underlying processes at work. Finally, using ensembles of decision trees has been shown to improve predictive accuracy on unseen test data.  The proposed approach incorporates multiple decision trees through ensemble methods such as bagging and boosting. These techniques combine predictions from individual trees into a final output, which is then used to make predictions on new data. We evaluate our methodology on several benchmark datasets across different domains, including regression and classification problems. Our results demonstrate that the use of multiple decision trees significantly improves prediction performance compared to relying on a single decision tree alone.  Overall, the findings presented in this study highlight the potential of utilizing multiple decision trees in enhancing model fitness and generalization capacity. As such, we expect our contributions to be valuable for researchers and practitioners looking for effective ways to analyze and interpret complex dataset features.",1
"In this paper, we examine previous work on the naive Bayesian classifier and review its limitations, which include a sensitivity to correlated features. We respond to this problem by embedding the naive Bayesian induction scheme within an algorithm that c arries out a greedy search through the space of features. We hypothesize that this approach will improve asymptotic accuracy in domains that involve correlated features without reducing the rate of learning in ones that do not. We report experimental results on six natural domains, including comparisons with decision-tree induction, that support these hypotheses. In closing, we discuss other approaches to extending naive Bayesian classifiers and outline some directions for future research.",0
"Title: ""Inducing selective classifiers with deep ensembles"" Authors: Jeffrey Jinglington, Kai Chung Chan, Dylan Gussie, Nicolette Li Achieving high accuracy on complex tasks using machine learning algorithms often requires large amounts of data and computational resources. In practice, however, these resources may not always be available. In order to address this issue, we propose a method for inducing highly accurate classifiers that can operate under resource constraints. Our approach uses deep ensembles of neural networks, which have been shown to achieve state-of-the-art performance across a wide range of tasks. By training multiple models simultaneously, each with different random initialization parameters and weights, our system generates diverse predictions that reflect the uncertainty present in the training set. We then use this diversity to guide selection of the most confident prediction at test time. Our experiments demonstrate that our method outperforms baseline methods by significant margins across several benchmark datasets, while requiring fewer parameters and computational resources. As such, our work has important implications for real-world applications where limited resources are available for model development and deployment.",1
"Classification is widely used technique in the data mining domain, where scalability and efficiency are the immediate problems in classification algorithms for large databases. We suggest improvements to the existing C4.5 decision tree algorithm. In this paper attribute oriented induction (AOI) and relevance analysis are incorporated with concept hierarchys knowledge and HeightBalancePriority algorithm for construction of decision tree along with Multi level mining. The assignment of priorities to attributes is done by evaluating information entropy, at different levels of abstraction for building decision tree using HeightBalancePriority algorithm. Modified DMQL queries are used to understand and explore the shortcomings of the decision trees generated by C4.5 classifier for education dataset and the results are compared with the proposed approach.",0
"This paper presents a novel method for improving decision tree induction by incorporating information entropy into the traditional algorithm. Information entropy is a measure of impurity or randomness within a set of data points. By incorporating entropy into the decision making process, our approach can identify more accurate, meaningful, and interpretable decision trees that extract useful rules from complex datasets. We evaluate the performance of our method on several benchmark datasets across different domains, demonstrating significant improvements over existing methods. Our results show that the proposed approach leads to better rule extraction and classification accuracy while producing compact and explainable models.",1
"Data mining methods have been widely applied in financial markets, with the purpose of providing suitable tools for prices forecasting and automatic trading. Particularly, learning methods aim to identify patterns in time series and, based on such patterns, to recommend buy/sell operations. The objective of this work is to evaluate the performance of Random Forests, a supervised learning method based on ensembles of decision trees, for decision support in stock markets. Preliminary results indicate good rates of successful operations and good rates of return per operation, providing a strong motivation for further research in this topic.",0
"In recent years, supervised learning has emerged as a powerful technique for predicting stock market trends. This study evaluates a supervised learning approach for stock market operations by analyzing historical data and comparing the results against traditional methods. Our experiments show that our method achieves state-of-the-art performance on several benchmark datasets, outperforming other machine learning models. Furthermore, we demonstrate how our model can effectively handle complex financial signals such as sentiment analysis and technical indicators to make accurate predictions. Overall, this research presents promising evidence that supervised learning techniques have the potential to revolutionize the way traders conduct their businesses. Our findings provide valuable insights into the application of these methods in real-world scenarios and open up new possibilities for future research in the field of finance and computer science.",1
"A novel method for estimating Bayesian network (BN) parameters from data is presented which provides improved performance on test data. Previous research has shown the value of representing conditional probability distributions (CPDs) via neural networks(Neal 1992), noisy-OR gates (Neal 1992, Diez 1993)and decision trees (Friedman and Goldszmidt 1996).The Bernoulli mixture network (BMN) explicitly represents the CPDs of discrete BN nodes as mixtures of local distributions,each having a different set of parents.This increases the space of possible structures which can be considered,enabling the CPDs to have finer-grained dependencies.The resulting estimation procedure induces a modelthat is better able to emulate the underlying interactions occurring in the data than conventional conditional Bernoulli network models.The results for artificially generated data indicate that overfitting is best reduced by restricting the complexity of candidate mixture substructures local to each node. Furthermore, mixtures of very simple substructures can perform almost as well as more complex ones.The BMN is also applied to data collected from an online adventure game with an application to keyhole plan recognition. The results show that the BMN-based model brings a dramatic improvement in performance over a conventional BN model.",0
"This paper presents a novel method for estimating well-performing Bayesian networks from data. We use mixtures of Bernoulli distributions to model the conditional probabilities required by Bayesian network inference algorithms. By optimizing these mixtures against their corresponding evidence measures, we can accurately estimate the underlying dependencies among variables. Our approach allows us to overcome some of the challenges associated with traditional methods such as maximum likelihood estimation or Markov Chain Monte Carlo simulation. Through experiments on benchmark datasets, we demonstrate that our method outperforms existing techniques and generates more accurate predictions. Additionally, we discuss potential applications of our work to real-world problems where Bayesian networks are commonly used.",1
"With the development of connected filters for the last decade, many algorithms have been proposed to compute the max-tree. Max-tree allows to compute the most advanced connected operators in a simple way. However, no fair comparison of algorithms has been proposed yet and the choice of an algorithm over an other depends on many parameters. Since the need of fast algorithms is obvious for production code, we present an in depth comparison of five algorithms and some variations of them in a unique framework. Finally, a decision tree will be proposed to help user in choosing the right algorithm with respect to their data.",0
"In this paper we present a comprehensive comparison of several different maximum spanning tree algorithms. We look at both traditional methods such as Kruskal's algorithm and Prim's algorithm, along with more modern approaches like push relabel and union find based methods. Our study evaluates these algorithms across a range of datasets and metrics including running time, memory usage, and solution quality. Additionally, we examine the impact that parameter choices have on each method's performance. From our experiments, we identify some clear winners among the algorithms considered here and provide guidance for practitioners seeking to choose an appropriate algorithm for their specific use case. Overall, this work provides valuable insights into the strengths and weaknesses of current state-of-the-art max-tree algorithms and offers new opportunities for further research in this area.",1
"Improving students academic performance is not an easy task for the academic community of higher learning. The academic performance of engineering and science students during their first year at university is a turning point in their educational path and usually encroaches on their General Point Average,GPA in a decisive manner. The students evaluation factors like class quizzes mid and final exam assignment lab work are studied. It is recommended that all these correlated information should be conveyed to the class teacher before the conduction of final exam. This study will help the teachers to reduce the drop out ratio to a significant level and improve the performance of students. In this paper, we present a hybrid procedure based on Decision Tree of Data mining method and Data Clustering that enables academicians to predict students GPA and based on that instructor can take necessary step to improve student academic performance.",0
"In this research paper, we propose a novel approach that utilizes K Means Clustering Algorithm and Decision Tree methods to improve student academic performance. Our methodology involves analyzing data from past academic years, identifying patterns, and segmenting students into different groups based on their academic potential. We then use these clusters and decision trees to predict which students would benefit most from additional support and resources. This allows educators to allocate limited resources more efficiently and effectively, ultimately leading to better academic outcomes overall. Our findings suggest that our approach can lead to significant improvements in both individual student performance as well as the overall educational system. While there are limitations to our study, such as small sample size and narrow geographical area, further research could confirm and expand upon our results. Overall, our work provides valuable insights into the application of data analytics tools in education, paving the way towards improved learning experiences for all students.",1
"In this paper we present a new algorithm for learning oblique decision trees. Most of the current decision tree algorithms rely on impurity measures to assess the goodness of hyperplanes at each node while learning a decision tree in a top-down fashion. These impurity measures do not properly capture the geometric structures in the data. Motivated by this, our algorithm uses a strategy to assess the hyperplanes in such a way that the geometric structure in the data is taken into account. At each node of the decision tree, we find the clustering hyperplanes for both the classes and use their angle bisectors as the split rule at that node. We show through empirical studies that this idea leads to small decision trees and better performance. We also present some analysis to show that the angle bisectors of clustering hyperplanes that we use as the split rules at each node, are solutions of an interesting optimization problem and hence argue that this is a principled method of learning a decision tree.",0
"Geometric decision trees have recently emerged as powerful models for learning complex relationships between inputs and outputs. They build on traditional tree structures by incorporating geometric representations, which enable more expressive representations of nonlinear patterns. In this paper, we present a novel algorithm that leverages these representations to learn high quality decision trees efficiently. Our approach achieves state-of-the-art results across multiple domains while outperforming several competitive baselines. Furthermore, we provide insights into the behavior of our model and demonstrate how geometric decision trees can capture complex interactions among features. Overall, this work represents a significant contribution towards developing effective machine learning techniques for real world applications.",1
"In this paper, naive Bayesian and C4.5 Decision Tree Classifiers(DTC) are successively applied on materials informatics to classify the engineering materials into different classes for the selection of materials that suit the input design specifications. Here, the classifiers are analyzed individually and their performance evaluation is analyzed with confusion matrix predictive parameters and standard measures, the classification results are analyzed on different class of materials. Comparison of classifiers has found that naive Bayesian classifier is more accurate and better than the C4.5 DTC. The knowledge discovered by the naive bayesian classifier can be employed for decision making in materials selection in manufacturing industries.",0
"This paper evaluates different predictive classifier models that can be used to discover knowledge from engineering materials data sets, and compares their performance on several metrics such as accuracy, precision, recall, F1 score, area under curve (AUC), Gini index, and entropy. We use three real world datasets taken from diverse fields of engineering materials research – mechanical properties of steels, wear resistance characteristics of metals, and corrosion resistance behavior of alloys. Our evaluation framework is based on established principles of machine learning model assessment and comparison studies using cross validation techniques. Furthermore, we provide detailed discussions on how these findings can benefit engineers engaged in materials science related research by guiding them towards selecting appropriate classifier algorithms suitable for specific research problems at hand. Overall, our study makes a valuable contribution to advancing knowledge discovery from materials data sets using predictive analytics techniques.",1
"Recently, prediction markets have shown considerable promise for developing flexible mechanisms for machine learning. In this paper, agents with isoelastic utilities are considered. It is shown that the costs associated with homogeneous markets of agents with isoelastic utilities produce equilibrium prices corresponding to alpha-mixtures, with a particular form of mixing component relating to each agent's wealth. We also demonstrate that wealth accumulation for logarithmic and other isoelastic agents (through payoffs on prediction of training targets) can implement both Bayesian model updates and mixture weight updates by imposing different market payoff structures. An iterative algorithm is given for market equilibrium computation. We demonstrate that inhomogeneous markets of agents with isoelastic utilities outperform state of the art aggregate classifiers such as random forests, as well as single classifiers (neural networks, decision trees) on a number of machine learning benchmarks, and show that isoelastic combination methods are generally better than their logarithmic counterparts.",0
"In this paper we study markets in which agents have different degrees of elasticity of substitution (isoelasticity) between goods. This feature has important implications both on market equilibrium and welfare analysis, and can lead to new applications such as wealth updates in machine learning markets. We show that there exist novel demand revelation mechanisms that achieve efficient allocations for all values of isoelasticity, even under very general assumptions. Furthermore, we provide characterizations of competitive and incentive compatible mechanisms that are universal in these markets: they work regardless of the value of isoelasticity. Finally, we explore the idea of using wealth updates in conjunction with machine learning techniques to improve agent behavior. This paper studies markets where participants exhibit varying levels of elasticity of substitution between goods. These characteristics influence how agents make decisions, affect market efficiency and welfare outcomes, and may impact applications like artificial intelligence marketplaces. By introducing ""demand revelation mechanisms,"" the authors demonstrate ways to achieve optimal distributions even across diverse scenarios. Additionally, they identify competitive and fair bidding methods applicable to any degree of substitutability among items. Investigating wealth adjustments and machine learning strategies together could enhance consumer behavior within these economies. Overall, exploring participant elasticities broadens our understanding of market interactions and offers innovative tools for optimizing their functionality.",1
"Data mining involves the systematic analysis of large data sets, and data mining in agricultural soil datasets is exciting and modern research area. The productive capacity of a soil depends on soil fertility. Achieving and maintaining appropriate levels of soil fertility, is of utmost importance if agricultural land is to remain capable of nourishing crop production. In this research, Steps for building a predictive model of soil fertility have been explained.   This paper aims at predicting soil fertility class using decision tree algorithms in data mining . Further, it focuses on performance tuning of J48 decision tree algorithm with the help of meta-techniques such as attribute selection and boosting.",0
"Title: ""Performance Tuning Of J48 Algorithm For Prediction Of Soil Fertility"" Authors: Nilesh Kumar Sharma, Shalini Sharma Publication Date: March 2023 Journal Name: International Journal of Artificial Intelligence Research (IJAIR) Abstract In agriculture, soil fertility plays a vital role in crop production and quality. Therefore, accurate prediction models can help farmers make informed decisions on fertilizer application and improve their yield. This research focuses on performance tuning of the popular decision tree algorithm, J48, for predicting soil fertility levels using preprocessing techniques such as feature selection, discretization, and data normalization. Experiments were conducted by using six different evaluation metrics including accuracy, precision, recall, f-score, area under ROC curve (AUROC), and support vector machines (SVM). Results indicated that the J48 model achieved better results after implementing these preprocessing methods, particularly feature selection, which reduced the complexity of the dataset and improved the overall performance. These findings provide valuable insights into improving the reliability of decision tree algorithms for agricultural applications and highlight the importance of careful experimental design. Keywords: Decision Tree, Preprocessing Techniques, Feature Selection, Discretization, Data Normalization. Citation: Sharma N., Sharma S. (2023) Performance Tuning Of J48 Algorithm For Prediction Of Soil Fertility. International Journal of Artificial Intelligence Research (IJAIR), Vol.7(No.1): pp.69-86. Download pdf at https://researchspace.auckland.ac.nz/handle/2292/37622",1
"Matching cells over time has long been the most difficult step in cell tracking. In this paper, we approach this problem by recasting it as a classification problem. We construct a feature set for each cell, and compute a feature difference vector between a cell in the current frame and a cell in a previous frame. Then we determine whether the two cells represent the same cell over time by training decision trees as our binary classifiers. With the output of decision trees, we are able to formulate an assignment problem for our cell association task and solve it using a modified version of the Hungarian algorithm.",0
"This study aimed to develop a method for tracking the movement patterns of Tetrahymena pyriformis cells as they move through different environments and under varying conditions. We used decision trees to create a model that could accurately predict cell movements based on their physical features and environmental factors such as temperature, salinity, and light exposure. Our results showed high levels of accuracy in our predictions, allowing us to track the movement patterns of these important microorganisms. Further applications of this technique may allow researchers to better understand Tetrahymena behavior and ecology, as well as potentially improving medical treatments and other industrial processes involving these organisms.",1
"Boosted decision trees typically yield good accuracy, precision, and ROC area. However, because the outputs from boosting are not well calibrated posterior probabilities, boosting yields poor squared error and cross-entropy. We empirically demonstrate why AdaBoost predicts distorted probabilities and examine three calibration methods for correcting this distortion: Platt Scaling, Isotonic Regression, and Logistic Correction. We also experiment with boosting using log-loss instead of the usual exponential loss. Experiments show that Logistic Correction and boosting with log-loss work well when boosting weak models such as decision stumps, but yield poor performance when boosting more complex models such as full decision trees. Platt Scaling and Isotonic Regression, however, significantly improve the probabilities predicted by",0
"Boosting algorithms are machine learning models that combine weak learners into strong ones. They have been used successfully across many different applications such as classification problems, regression, ranking, and feature selection. Ensemble methods like bagging and boosting produce superior results by combining multiple base models trained on random subsets of the data. Ensembling works well since most real world datasets are subject to aleatoric uncertainty which can result in substantial variability among different runs of the same model. When the goal is to obtain calibrated probabilities rather than just prediction accuracy then the choice of combination rule becomes important since poor choice of combinations lead to overconfidence/under confidence problems similar to those found in other classifiers. In this paper we focus specifically on the problem of calibration in ensembles obtained through gradient boosting under the logit link function (Logistic Regression) and a wide range of loss functions commonly used in Machine Learning. We investigate theoretically the conditions necessary and sufficient for Bayesian Model Average (BMA), a common method to obtain calibration at ensemble level but only applicable to probabilistic losses like Log Loss, Exponential Loss etc. Our experimental validation shows that indeed BMA helps to improve calibration substantially if used with the appropriate loss functions. However these theoretical requirements hold only approximately for actual implementations - the reason being the discreteness of predictions produced using current optimisers. Hence in practice one has to use either plug-in estimates of the expected value inside logistic regression or recalibrate post hoc linearly",1
"SDYNA is a general framework designed to address large stochastic reinforcement learning problems. Unlike previous model based methods in FMDPs, it incrementally learns the structure and the parameters of a RL problem using supervised learning techniques. Then, it integrates decision-theoric planning algorithms based on FMDPs to compute its policy. SPITI is an instanciation of SDYNA that exploits ITI, an incremental decision tree algorithm, to learn the reward function and the Dynamic Bayesian Networks with local structures representing the transition function of the problem. These representations are used by an incremental version of the Structured Value Iteration algorithm. In order to learn the structure, SPITI uses Chi-Square tests to detect the independence between two probability distributions. Thus, we study the relation between the threshold used in the Chi-Square test, the size of the model built and the relative error of the value function of the induced policy with respect to the optimal value. We show that, on stochastic problems, one can tune the threshold so as to generate both a compact model and an efficient policy. Then, we show that SPITI, while keeping its model compact, uses the generalization property of its learning method to perform better than a stochastic classical tabular algorithm in large RL problem with an unknown structure. We also introduce a new measure based on Chi-Square to qualify the accuracy of the model learned by SPITI. We qualitatively show that the generalization property in SPITI within the FMDP framework may prevent an exponential growth of the time required to learn the structure of large stochastic RL problems.",0
"Abstract: Modern techniques like deep reinforcement learning (RL) have led to impressive results across domains such as game playing, robotics, language processing, and autonomous driving. However, one fundamental limitation remains at the core of many RL algorithms – they must learn models from scratch every time they are applied to new problems, without reusing any prior knowledge. In practice, this means that each model needs separate training data or explicit tuning of hyperparameters to perform well on even moderately complex tasks. In this work we take a step towards addressing these challenges by introducing a novel method called Factored Model-Based Reinforcement Learning (FMBRL). Our approach learns a compact, expressive representation of the underlying environment structure known as Markov Decision Processes (MDP), which can then be leveraged in both planning and policy improvement steps. We show through experiments that FMBRL outperforms state-of-the-art methods across several benchmark environments while requiring far fewer interactions with the environment. As more realworld applications require less exploration and better use of domain expertise, we believe our contributions represent a meaningful step toward achieving human level performance in artificial intelligence. Keywords: factorized MDP; model-based reinforcement learning; meta-learning; planning; transfer.",1
"Ensembles of classification and regression trees remain popular machine learning methods because they define flexible non-parametric models that predict well and are computationally efficient both during training and testing. During induction of decision trees one aims to find predicates that are maximally informative about the prediction target. To select good predicates most approaches estimate an information-theoretic scoring function, the information gain, both for classification and regression problems. We point out that the common estimation procedures are biased and show that by replacing them with improved estimators of the discrete and the differential entropy we can obtain better decision trees. In effect our modifications yield improved predictive performance and are simple to implement in any decision tree code.",0
"Title: Improved Information Gain Estimates for Decision Tree Induction Abstract: The goal of decision tree induction is to create a model that can accurately predict outcomes based on input data. One important factor in creating such models is selecting the best feature subset at each node of the tree. This process is typically guided by measures of impurity reduction, which aim to identify the most informative features for splitting the data. However, current methods of estimating impurity reduction may lead to suboptimal splits and poor overall model performance. This paper presents a novel approach to improving information gain estimates for decision tree induction. Our method is based on a new metric called Relaxed Variational Impurity (RVI), which better captures the variability of the training data and leads to more accurate predictions. We demonstrate through experimental results that our method significantly outperforms existing approaches in terms of accuracy, speed, and robustness across a range of datasets and tasks. These improvements have important real-world applications, including in areas such as medical diagnosis, financial forecasting, and fraud detection. Overall, we believe RVI has the potential to become a valuable tool in the field of machine learning and decision making. Note: Title: Improved Information Gain Estimates for Decision Tree Induction Summary: This research focused on enhancing the estimation of information gain used in decision trees to increase their prediction accuracy. By developing the Relaxed Variational Impurity (RVI) measure, the authors improved over traditional methods in terms of precision, execution time and resistance to errors. Experiments using several databases showed a significant advance in diagnostic accuracy, finance prognostication and anomaly identification among others. As a consequence of these findings, RVI stands poised to provide essential support in various sectors like medicine, finance and cybersecurity. In essence, both decision tree construction and choice of features play crucial roles in determining the quality of resulting predictions. Future explorations into further optimizing the role of RVI in combination with other techniques could potentially bring greater benefits to numerous industries globally.",1
"The goal of the paper is to relate complexity measures associated with the evaluation of Boolean functions (certificate complexity, decision tree complexity) and learning dimensions used to characterize exact learning (teaching dimension, extended teaching dimension). The high level motivation is to discover non-trivial relations between exact learning of an unknown concept and testing whether an unknown concept is part of a concept class or not. Concretely, the goal is to provide lower and upper bounds of complexity measures for one problem type in terms of the other.",0
"Include key terms: exact learning, boolean functions, computation. Use active voice rather than passive voice as far as possible. Keep your writing style clear but professional tone. In recent years, exact learning has emerged as a powerful method for reasoning about complex systems. By treating logical inference as a form of mathematical optimization, researchers have developed algorithms that can compute correct answers efficiently on many realistic problems. However, there remain important questions about how these methods relate to other approaches to reasoning, including those based on classical logic and automated theorem proving. This paper addresses one such question by considering the connection between exact learning and computing boolean functions. On the one hand, we show that any function computed by exact learning can be realized as a boolean function, and vice versa. This result provides a formal link between two areas of research that until now have been largely disconnected. At the same time, our proof reveals some intriguing limitations to exact learning that may guide future work in the area. Specifically, while exact learning can solve general decision problems more efficiently than traditional methods based on search and backtracking, it cannot solve all boolean functions with equal ease. By identifying a class of functions that exact learners struggle with, we open up new directions for research into the design of efficient computational methods that combine symbolic and numerical components. Overall, this work sheds light on both the power and the limits of modern techniques for exact inference, highlighting promising pathways for future progress in artificial intelligence.  Keywords: Exact Learning, Boolean Functions, Compu",1
"The free energy functional has recently been proposed as a variational principle for bounded rational decision-making, since it instantiates a natural trade-off between utility gains and information processing costs that can be axiomatically derived. Here we apply the free energy principle to general decision trees that include both adversarial and stochastic environments. We derive generalized sequential optimality equations that not only include the Bellman optimality equations as a limit case, but also lead to well-known decision-rules such as Expectimax, Minimax and Expectiminimax. We show how these decision-rules can be derived from a single free energy principle that assigns a resource parameter to each node in the decision tree. These resource parameters express a concrete computational cost that can be measured as the amount of samples that are needed from the distribution that belongs to each node. The free energy principle therefore provides the normative basis for generalized optimality equations that account for both adversarial and stochastic environments.",0
"In the field of artificial intelligence (AI), sequential decision making (SDM) has emerged as one of the most challenging areas of study. The problem of finding optimal policies for SDM problems arises naturally across many applications such as robotics, finance, control systems, game theory, economics, and even everyday life situations. To tackle these types of problems, researchers have developed several frameworks that aim at identifying optimal policy sequences. These approaches range from dynamic programming methods like reinforcement learning algorithms to stochastic techniques based on Markov decision processes. However, the general formulation of optimization problems involved in SDM can be difficult due to the complexities associated with the state spaces of sequential decisions, which often lead to intractability issues. Additionally, obtaining exact solutions for real-world SDM problems remains computationally impractical for larger-scale scenarios. Consequently, efficient approximate algorithms are necessary for solving large SDMs under resource constraints. This work focuses on developing new methods that are capable of optimizing policies by exploiting recent advancements in numerical optimization and machine learning theories. We present a framework named ""Free Energy and the Generalized Optimality Equations"" (FEGOE), designed specifically for efficiently searching over high-dimensional spaces while ensuring near-optimal performance guarantees. FEGOE builds upon concepts from statistical mechanics, variational inference, deep learning, and Bayesian methods. Our experimental results show promising improvements compared to existing benchmarks for both stationary and nonstationarity environments, further highlighting FEGOE's effectiveness for addressing challenging SDM tasks. Overall, we believe thi",1
"Random forests are a scheme proposed by Leo Breiman in the 2000's for building a predictor ensemble with a set of decision trees that grow in randomly selected subspaces of data. Despite growing interest and practical use, there has been little exploration of the statistical properties of random forests, and little is known about the mathematical forces driving the algorithm. In this paper, we offer an in-depth analysis of a random forests model suggested by Breiman in \cite{Bre04}, which is very close to the original algorithm. We show in particular that the procedure is consistent and adapts to sparsity, in the sense that its rate of convergence depends only on the number of strong features and not on how many noise variables are present.",0
"This paper presents a detailed analysis of a random forest model applied to a large dataset containing financial data from a variety of sources. The authors aimed to develop a predictive model that could accurately forecast future trends based on past data. They began by preprocessing the raw data and selecting features relevant to their analysis. Next, they trained several models using different algorithms and evaluated each one according to accuracy metrics such as mean squared error (MSE) and R squared values. The random forest model performed better than other methods, achieving low MSE scores and high R square values, indicating strong predictive ability. Additionally, the authors conducted feature importance analysis to identify the most influential variables contributing to predictions. Their findings suggest that incorporating nonlinear relationships among features improves prediction performance compared to traditional linear regression approaches. Overall, the results demonstrate the potential utility of machine learning techniques for financial forecasting purposes. However, limitations such as overfitting remain important considerations when building these types of models, which the authors address through cross-validation and regularization strategies. Ultimately, further testing and validation would be required before implementing the proposed methodology into industry practice, but the insights gained may inform future research efforts exploring alternative solutions.",1
"Now-a-days the amount of data stored in educational database increasing rapidly. These databases contain hidden information for improvement of students' performance. Educational data mining is used to study the data available in the educational field and bring out the hidden knowledge from it. Classification methods like decision trees, Bayesian network etc can be applied on the educational data for predicting the student's performance in examination. This prediction will help to identify the weak students and help them to score better marks. The C4.5, ID3 and CART decision tree algorithms are applied on engineering student's data to predict their performance in the final exam. The outcome of the decision tree predicted the number of students who are likely to pass, fail or promoted to next year. The results provide steps to improve the performance of the students who were predicted to fail or promoted. After the declaration of the results in the final examination the marks obtained by the students are fed into the system and the results were analyzed for the next session. The comparative analysis of the results states that the prediction has helped the weaker students to improve and brought out betterment in the result.",0
"The use of data mining techniques has increased significantly over recent years as organizations strive to uncover valuable insights from large amounts of available data. In particular, classification algorithms have proven effective in identifying patterns and relationships within datasets that can lead to improved decision making across various domains. This research focuses on applying these same techniques to address a pressing issue faced by many educational institutions: how to improve performance among engineering students. By analyzing student data such as grades, test scores, demographics, and other relevant factors, we aim to develop classifiers capable of predicting academic success or failure at various stages throughout the engineering program. Ultimately, our goal is to provide educators with actionable recommendations based on accurate predictions, thereby enabling more targeted support programs and resources to enhance overall learning outcomes. Our findings suggest promising results towards achieving this objective, which holds significant implications for shaping future education policies and practices.",1
"We describe a method for fast approximation of sparse coding. The input space is subdivided by a binary decision tree, and we simultaneously learn a dictionary and assignment of allowed dictionary elements for each leaf of the tree. We store a lookup table with the assignments and the pseudoinverses for each node, allowing for very fast inference. We give an algorithm for learning the tree, the dictionary and the dictionary element assignment, and In the process of describing this algorithm, we discuss the more general problem of learning the groups in group structured sparse modelling. We show that our method creates good sparse representations by using it in the object recognition framework of \cite{lazebnik06,yang-cvpr-09}. Implementing our own fast version of the SIFT descriptor the whole system runs at 20 frames per second on $321 \times 481$ sized images on a laptop with a quad-core cpu, while sacrificing very little accuracy on the Caltech 101 and 15 scenes benchmarks.",0
"In recent years, structured sparse coding (SSC) has emerged as an effective method for modeling complex data structures such as natural images. However, due to the computational cost associated with solving SSC problems in real time, applying these methods to large datasets or high-speed scenarios remains challenging. To address this issue, we propose fast approximation algorithms that can efficiently compute approximate solutions to SSC problems while maintaining good reconstruction accuracy. Our approach leverages ideas from compressed sensing and matrix factorization, resulting in simple and efficient optimization procedures. We demonstrate the effectiveness of our proposed method through experiments on several benchmark image datasets, where we show significant improvements over state-of-the-art techniques in terms of both speed and reconstruction quality. Finally, we apply our framework to the task of object recognition and achieve competitive results compared to other popular deep learning approaches. Overall, our work highlights the potential of combining efficient approximation schemes with structured sparsity models for rapidly processing complex data.",1
"We prove a new structural lemma for partial Boolean functions $f$, which we call the seed lemma for DNF. Using the lemma, we give the first subexponential algorithm for proper learning of DNF in Angluin's Equivalence Query (EQ) model. The algorithm has time and query complexity $2^{(\tilde{O}{\sqrt{n}})}$, which is optimal. We also give a new result on certificates for DNF-size, a simple algorithm for properly PAC-learning DNF, and new results on EQ-learning $\log n$-term DNF and decision trees.",0
"This paper presents new results on query learning algorithms for disjunctive normal form (DNF) formulas. We focus specifically on proper equivalence queries, which seek to learn a formula that is equivalent in terms of satisfying assignment status but has fewer literals than the original formula. Existing work has shown bounds on the number of such queries needed to learn certain classes of formulas, including Horn clauses and Krom formulas. Our contributions are threefold: we provide improved upper bounds on the number of queries required for DNF formulas; we present a lower bound that holds for any algorithm attempting to learn arbitrary DNFs using proper equivalence queries; and we show how these results can be used as building blocks towards establishing tight bounds for related problems in formal verification, testing, and synthesis. Overall, our findings shed light on the power and limitations of query learning approaches for complex boolean functions.",1
"Artificial Neural Network is among the most popular algorithm for supervised learning. However, Neural Networks have a well-known drawback of being a ""Black Box"" learner that is not comprehensible to the Users. This lack of transparency makes it unsuitable for many high risk tasks such as medical diagnosis that requires a rational justification for making a decision. Rule Extraction methods attempt to curb this limitation by extracting comprehensible rules from a trained Network. Many such extraction algorithms have been developed over the years with their respective strengths and weaknesses. They have been broadly categorized into three types based on their approach to use internal model of the Network. Eclectic Methods are hybrid algorithms that combine the other approaches to attain more performance. In this paper, we present an Eclectic method called HERETIC. Our algorithm uses Inductive Decision Tree learning combined with information of the neural network structure for extracting logical rules. Experiments and theoretical analysis show HERETIC to be better in terms of speed and performance.",0
"Research in extracting propositional rules from neural networks has been gaining attention due to their use as interpretable models in areas such as decision making, diagnosis, natural language processing, etc. Traditional approaches often result in handcrafted features, complex engineering, limited performance, and rule sets that lack expressiveness and diversity. In this work, we propose an eclectic extraction method (E2) to address these limitations by leveraging the advantages of existing methods while mitigating their drawbacks. Our approach hybridizes symbolic regression with evolutionary computation and Monte Carlo tree search to generate diverse rule sets efficiently. Moreover, we introduce novel mechanisms to adaptively guide the search towards more accurate and expressive rules. Experimental results demonstrate that our proposed method outperforms state-of-the-art techniques across multiple domains and datasets. Furthermore, qualitative analysis shows that our extracted rules are more comprehensive, informative, and have better generalization ability than those generated by other methods. Our findings contribute to advancing research on explainability and interpretability of deep learning systems, promoting their adoption in critical applications.",1
"COMET is a single-pass MapReduce algorithm for learning on large-scale data. It builds multiple random forest ensembles on distributed blocks of data and merges them into a mega-ensemble. This approach is appropriate when learning from massive-scale data that is too large to fit on a single machine. To get the best accuracy, IVoting should be used instead of bagging to generate the training subset for each decision tree in the random forest. Experiments with two large datasets (5GB and 50GB compressed) show that COMET compares favorably (in both accuracy and training time) to learning on a subsample of data using a serial algorithm. Finally, we propose a new Gaussian approach for lazy ensemble evaluation which dynamically decides how many ensemble members to evaluate per data point; this can reduce evaluation cost by 100X or more.",0
