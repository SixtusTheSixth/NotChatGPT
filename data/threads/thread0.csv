"Imitation learning algorithms learn viable policies by imitating an expert's behavior when reward signals are not available. Generative Adversarial Imitation Learning (GAIL) is a state-of-the-art algorithm for learning policies when the expert's behavior is available as a fixed set of trajectories. We evaluate in terms of the expert's cost function and observe that the distribution of trajectory-costs is often more heavy-tailed for GAIL-agents than the expert at a number of benchmark continuous-control tasks. Thus, high-cost trajectories, corresponding to tail-end events of catastrophic failure, are more likely to be encountered by the GAIL-agents than the expert. This makes the reliability of GAIL-agents questionable when it comes to deployment in risk-sensitive applications like robotic surgery and autonomous driving. In this work, we aim to minimize the occurrence of tail-end events by minimizing tail risk within the GAIL framework. We quantify tail risk by the Conditional-Value-at-Risk (CVaR) of trajectories and develop the Risk-Averse Imitation Learning (RAIL) algorithm. We observe that the policies learned with RAIL show lower tail-end risk than those of vanilla GAIL. Thus the proposed RAIL algorithm appears as a potent alternative to GAIL for improved reliability in risk-sensitive applications.",0
"Algorithms that imitate an expert's behavior are useful when reward signals are unavailable. One such algorithm, Generative Adversarial Imitation Learning (GAIL), is currently considered state-of-the-art for learning policies using a fixed set of trajectories from an expert. However, when evaluated using the expert's cost function, GAIL agents often encounter more high-cost trajectories than the expert at various continuous-control tasks. This makes GAIL agents unreliable for risk-sensitive applications like robotic surgery and autonomous driving. To address this problem, we propose a new algorithm called Risk-Averse Imitation Learning (RAIL) that minimizes tail risk within the GAIL framework by quantifying it using the Conditional-Value-at-Risk (CVaR) of trajectories. Our results show that RAIL policies have lower tail-end risk than vanilla GAIL policies. Therefore, RAIL is a promising alternative to GAIL for improved reliability in risk-sensitive applications.",1
"This paper studies directed exploration for reinforcement learning agents by tracking uncertainty about the value of each available action. We identify two sources of uncertainty that are relevant for exploration. The first originates from limited data (parametric uncertainty), while the second originates from the distribution of the returns (return uncertainty). We identify methods to learn these distributions with deep neural networks, where we estimate parametric uncertainty with Bayesian drop-out, while return uncertainty is propagated through the Bellman equation as a Gaussian distribution. Then, we identify that both can be jointly estimated in one network, which we call the Double Uncertain Value Network. The policy is directly derived from the learned distributions based on Thompson sampling. Experimental results show that both types of uncertainty may vastly improve learning in domains with a strong exploration challenge.",0
"The objective of this research is to investigate directed exploration for reinforcement learning agents. The exploration is based on tracking uncertainty about the value of available actions. The study identifies two relevant sources of uncertainty, one arising from limited data, known as parametric uncertainty, and the other arising from the distribution of the returns, known as return uncertainty. The paper proposes deep neural networks to learn these distributions and Bayesian drop-out to estimate parametric uncertainty, while return uncertainty is propagated through the Bellman equation as a Gaussian distribution. The study finds that both types of uncertainty can be jointly estimated in one network, called the Double Uncertain Value Network. Based on the learned distributions, the policy is directly derived using Thompson sampling. The experimental results show that incorporating both types of uncertainty can significantly enhance learning in domains that require extensive exploration.",1
"We investigate the integration of a planning mechanism into sequence-to-sequence models using attention. We develop a model which can plan ahead in the future when it computes its alignments between input and output sequences, constructing a matrix of proposed future alignments and a commitment vector that governs whether to follow or recompute the plan. This mechanism is inspired by the recently proposed strategic attentive reader and writer (STRAW) model for Reinforcement Learning. Our proposed model is end-to-end trainable using primarily differentiable operations. We show that it outperforms a strong baseline on character-level translation tasks from WMT'15, the algorithmic task of finding Eulerian circuits of graphs, and question generation from the text. Our analysis demonstrates that the model computes qualitatively intuitive alignments, converges faster than the baselines, and achieves superior performance with fewer parameters.",0
"Our study focuses on incorporating planning mechanisms into attention-based sequence-to-sequence models. To achieve this, we introduce a model that can anticipate future alignments between input and output sequences by generating a matrix of potential future alignments and a commitment vector that determines whether to proceed with the plan or recalculate. This approach is inspired by the strategic attentive reader and writer (STRAW) model for Reinforcement Learning. Our model is capable of end-to-end training through primarily differentiable operations. We demonstrate its superiority over a strong baseline in character-level translation tasks from WMT'15, the algorithmic task of finding Eulerian circuits of graphs, and text-based question generation. Our evaluation indicates that the model produces intuitive alignments, converges faster than the baselines, and achieves better performance with fewer parameters.",1
"We present a suite of reinforcement learning environments illustrating various safety properties of intelligent agents. These problems include safe interruptibility, avoiding side effects, absent supervisor, reward gaming, safe exploration, as well as robustness to self-modification, distributional shift, and adversaries. To measure compliance with the intended safe behavior, we equip each environment with a performance function that is hidden from the agent. This allows us to categorize AI safety problems into robustness and specification problems, depending on whether the performance function corresponds to the observed reward function. We evaluate A2C and Rainbow, two recent deep reinforcement learning agents, on our environments and show that they are not able to solve them satisfactorily.",0
"A range of reinforcement learning scenarios is presented, demonstrating various safety properties of intelligent agents. These scenarios encompass safe interruptibility, avoidance of side effects, absence of supervisor, reward manipulation, and safe exploration, as well as resilience to self-modification, distributional shift, and adversarial attacks. The compliance of agents with safe behavior is measured by equipping each scenario with a hidden performance function, allowing the classification of AI safety problems into robustness and specification problems, depending on whether the performance function aligns with the observed reward function. The performance of two deep reinforcement learning agents, A2C and Rainbow, is evaluated on these scenarios, revealing their inability to satisfactorily solve them.",1
"Neural networks dominate the modern machine learning landscape, but their training and success still suffer from sensitivity to empirical choices of hyperparameters such as model architecture, loss function, and optimisation algorithm. In this work we present \emph{Population Based Training (PBT)}, a simple asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance. Importantly, PBT discovers a schedule of hyperparameter settings rather than following the generally sub-optimal strategy of trying to find a single fixed set to use for the whole course of training. With just a small modification to a typical distributed hyperparameter training framework, our method allows robust and reliable training of models. We demonstrate the effectiveness of PBT on deep reinforcement learning problems, showing faster wall-clock convergence and higher final performance of agents by optimising over a suite of hyperparameters. In addition, we show the same method can be applied to supervised learning for machine translation, where PBT is used to maximise the BLEU score directly, and also to training of Generative Adversarial Networks to maximise the Inception score of generated images. In all cases PBT results in the automatic discovery of hyperparameter schedules and model selection which results in stable training and better final performance.",0
"Although neural networks are widely used in machine learning, they are still sensitive to empirical choices of hyperparameters such as model architecture, loss function, and optimisation algorithm, which can affect their training and success. To address this issue, we propose Population Based Training (PBT), an asynchronous optimisation algorithm that effectively utilises a fixed computational budget to optimise a population of models and their hyperparameters to maximise performance. Unlike the conventional approach of finding a single fixed set of hyperparameters for the entire training process, PBT discovers a schedule of hyperparameter settings, resulting in stable training and better final performance. By modifying a typical distributed hyperparameter training framework, PBT allows robust and reliable training of models. We demonstrate the effectiveness of PBT on deep reinforcement learning problems, supervised learning for machine translation, and training of Generative Adversarial Networks. PBT results in the automatic discovery of hyperparameter schedules and model selection, leading to faster convergence and higher final performance of agents.",1
"One of the main challenges in reinforcement learning (RL) is generalisation. In typical deep RL methods this is achieved by approximating the optimal value function with a low-dimensional representation using a deep network. While this approach works well in many domains, in domains where the optimal value function cannot easily be reduced to a low-dimensional representation, learning can be very slow and unstable. This paper contributes towards tackling such challenging domains, by proposing a new method, called Hybrid Reward Architecture (HRA). HRA takes as input a decomposed reward function and learns a separate value function for each component reward function. Because each component typically only depends on a subset of all features, the corresponding value function can be approximated more easily by a low-dimensional representation, enabling more effective learning. We demonstrate HRA on a toy-problem and the Atari game Ms. Pac-Man, where HRA achieves above-human performance.",0
"Generalisation is a significant obstacle in reinforcement learning, which is often addressed in conventional deep RL approaches by utilizing a deep network to approximate the optimal value function with a low-dimensional representation. However, this method may not be effective in domains where the optimal value function cannot be easily reduced to a low-dimensional representation, leading to slow and unstable learning. To overcome these challenges, this paper proposes a new approach called Hybrid Reward Architecture (HRA), which takes a decomposed reward function as input and learns a separate value function for each component of the reward function. As each component relies on a subset of all features, the corresponding value function can be more easily approximated using a low-dimensional representation, facilitating more efficient learning. We demonstrate the effectiveness of HRA on Ms. Pac-Man, an Atari game, and a toy problem, achieving performance that surpasses human capability.",1
"Generating natural language descriptions of images is an important capability for a robot or other visual-intelligence driven AI agent that may need to communicate with human users about what it is seeing. Such image captioning methods are typically trained by maximising the likelihood of ground-truth annotated caption given the image. While simple and easy to implement, this approach does not directly maximise the language quality metrics we care about such as CIDEr. In this paper we investigate training image captioning methods based on actor-critic reinforcement learning in order to directly optimise non-differentiable quality metrics of interest. By formulating a per-token advantage and value computation strategy in this novel reinforcement learning based captioning model, we show that it is possible to achieve the state of the art performance on the widely used MSCOCO benchmark.",0
"Creating natural language descriptions of images is a crucial capability for a machine that relies on visual intelligence to communicate with humans about what it perceives. Current image captioning techniques involve training models to produce accurate captions based on ground-truth annotations. However, this method does not prioritize language quality metrics like CIDEr. To address this issue, our paper explores training image captioning models using actor-critic reinforcement learning to optimize non-differentiable quality metrics directly. We introduce a per-token advantage and value computation strategy in this innovative reinforcement learning model, which achieves state-of-the-art performance on the widely used MSCOCO benchmark.",1
"We build a deep reinforcement learning (RL) agent that can predict the likelihood of an individual testing positive for malaria by asking questions about their household. The RL agent learns to determine which survey question to ask next and when to stop to make a prediction about their likelihood of malaria based on their responses hitherto. The agent incurs a small penalty for each question asked, and a large reward/penalty for making the correct/wrong prediction; it thus has to learn to balance the length of the survey with the accuracy of its final predictions. Our RL agent is a Deep Q-network that learns a policy directly from the responses to the questions, with an action defined for each possible survey question and for each possible prediction class. We focus on Kenya, where malaria is a massive health burden, and train the RL agent on a dataset of 6481 households from the Kenya Malaria Indicator Survey 2015. To investigate the importance of having survey questions be adaptive to responses, we compare our RL agent to a supervised learning (SL) baseline that fixes its set of survey questions a priori. We evaluate on prediction accuracy and on the number of survey questions asked on a holdout set and find that the RL agent is able to predict with 80% accuracy, using only 2.5 questions on average. In addition, the RL agent learns to survey adaptively to responses and is able to match the SL baseline in prediction accuracy while significantly reducing survey length.",0
"Our team developed a deep reinforcement learning (RL) agent that can predict an individual's likelihood of testing positive for malaria by asking questions about their household. The RL agent determines which survey question to ask next and when to stop based on the responses received thus far, while incurring a small penalty for each question asked and a large reward or penalty for making the correct or wrong prediction. Our Deep Q-network RL agent learns a policy directly from the responses, with an action defined for each possible survey question and prediction class, and we trained it on a dataset of 6481 households from the 2015 Kenya Malaria Indicator Survey. To evaluate the importance of adaptive survey questions, we compared our RL agent to a supervised learning (SL) baseline that uses a fixed set of questions. Our RL agent achieved 80% accuracy with an average of only 2.5 questions, while also learning to adapt the survey to responses and significantly reducing its length compared to the SL baseline. Our study focuses on Kenya, where malaria is a significant health problem.",1
"Reinforcement learning has shown promise in learning policies that can solve complex problems. However, manually specifying a good reward function can be difficult, especially for intricate tasks. Inverse reinforcement learning offers a useful paradigm to learn the underlying reward function directly from expert demonstrations. Yet in reality, the corpus of demonstrations may contain trajectories arising from a diverse set of underlying reward functions rather than a single one. Thus, in inverse reinforcement learning, it is useful to consider such a decomposition. The options framework in reinforcement learning is specifically designed to decompose policies in a similar light. We therefore extend the options framework and propose a method to simultaneously recover reward options in addition to policy options. We leverage adversarial methods to learn joint reward-policy options using only observed expert states. We show that this approach works well in both simple and complex continuous control tasks and shows significant performance increases in one-shot transfer learning.",0
"Learning policies through reinforcement learning has demonstrated potential in solving intricate problems. Nonetheless, formulating an effective reward function manually can pose a challenge, particularly for complex tasks. A solution to this is inverse reinforcement learning, which enables the direct learning of the underlying reward function from expert demonstrations. However, the set of demonstrations may consist of diverse trajectories arising from various reward functions instead of just one. Hence, it is advantageous to consider such a decomposition in inverse reinforcement learning. The options framework in reinforcement learning is specially created to decompose policies similarly. Consequently, we propose an extension of the options framework that simultaneously recovers reward options and policy options. We employ adversarial methods to learn joint reward-policy options solely through observed expert states. Our approach proves effective in both simple and complex continuous control tasks and shows significant performance improvement in one-shot transfer learning.",1
"We present a new algorithm that significantly improves the efficiency of exploration for deep Q-learning agents in dialogue systems. Our agents explore via Thompson sampling, drawing Monte Carlo samples from a Bayes-by-Backprop neural network. Our algorithm learns much faster than common exploration strategies such as $\epsilon$-greedy, Boltzmann, bootstrapping, and intrinsic-reward-based ones. Additionally, we show that spiking the replay buffer with experiences from just a few successful episodes can make Q-learning feasible when it might otherwise fail.",0
"Introducing a novel algorithm that enhances the effectiveness of exploration for deep Q-learning agents in dialogue systems. Our agents utilize Thompson sampling by extracting Monte Carlo samples from a Bayes-by-Backprop neural network. Our approach surpasses typical exploration techniques such as $\epsilon$-greedy, Boltzmann, bootstrapping, and intrinsic-reward-based methods in terms of efficiency. Furthermore, we demonstrate that supplementing the replay buffer with experiences from a small number of prosperous episodes can enable Q-learning to be successful in scenarios where it would otherwise fail.",1
"The problem of sparse rewards is one of the hardest challenges in contemporary reinforcement learning. Hierarchical reinforcement learning (HRL) tackles this problem by using a set of temporally-extended actions, or options, each of which has its own subgoal. These subgoals are normally handcrafted for specific tasks. Here, though, we introduce a generic class of subgoals with broad applicability in the visual domain. Underlying our approach (in common with work using ""auxiliary tasks"") is the hypothesis that the ability to control aspects of the environment is an inherently useful skill to have. We incorporate such subgoals in an end-to-end hierarchical reinforcement learning system and test two variants of our algorithm on a number of games from the Atari suite. We highlight the advantage of our approach in one of the hardest games -- Montezuma's revenge -- for which the ability to handle sparse rewards is key. Our agent learns several times faster than the current state-of-the-art HRL agent in this game, reaching a similar level of performance. UPDATE 22/11/17: We found that a standard A3C agent with a simple shaped reward, i.e. extrinsic reward + feature control intrinsic reward, has comparable performance to our agent in Montezuma Revenge. In light of the new experiments performed, the advantage of our HRL approach can be attributed more to its ability to learn useful features from intrinsic rewards rather than its ability to explore and reuse abstracted skills with hierarchical components. This has led us to a new conclusion about the result.",0
"One of the toughest problems in modern reinforcement learning is the issue of receiving few rewards. Hierarchical reinforcement learning (HRL) addresses this issue by utilizing a collection of temporally-extended actions, or options, each with its own particular subgoal. Typically, these subgoals are custom-made for specific tasks. However, we introduce a general category of subgoals that can be applied broadly in the visual domain. Our approach is based on the hypothesis, shared with ""auxiliary tasks"" research, that the ability to control elements of the environment is an inherently valuable skill. We incorporate these subgoals into an end-to-end HRL system and test two variations of our algorithm on multiple games from the Atari suite. In Montezuma's revenge, one of the most challenging games, where handling sparse rewards is crucial, our technique proves advantageous. Our agent learns significantly faster than the existing state-of-the-art HRL agent in this game, achieving a comparable level of performance. However, in light of new experiments, we have discovered that a standard A3C agent with a simple shaped reward has similar performance to our agent in Montezuma's Revenge. As a result, we have concluded that the advantage of our HRL method is more due to its ability to learn valuable features from intrinsic rewards rather than its capacity to explore and reuse abstracted skills with hierarchical components.",1
"We address the problem of inverse reinforcement learning in Markov decision processes where the agent is risk-sensitive. In particular, we model risk-sensitivity in a reinforcement learning framework by making use of models of human decision-making having their origins in behavioral psychology, behavioral economics, and neuroscience. We propose a gradient-based inverse reinforcement learning algorithm that minimizes a loss function defined on the observed behavior. We demonstrate the performance of the proposed technique on two examples, the first of which is the canonical Grid World example and the second of which is a Markov decision process modeling passengers' decisions regarding ride-sharing. In the latter, we use pricing and travel time data from a ride-sharing company to construct the transition probabilities and rewards of the Markov decision process.",0
"Our focus is on solving the issue of inverse reinforcement learning in Markov decision processes while considering the agent's risk sensitivity. To achieve this, we incorporate human decision-making models from behavioral psychology, behavioral economics, and neuroscience into a reinforcement learning framework. Our proposed solution is a gradient-based inverse reinforcement learning algorithm that minimizes a loss function based on observed behavior. We test the effectiveness of our approach through two examples: the Grid World example and a Markov decision process that models passengers' ride-sharing decisions using pricing and travel time data from a ride-sharing company to establish transition probabilities and rewards.",1
"In this work, we propose several online methods to build a \emph{learning curriculum} from a given set of target-task-specific training tasks in order to speed up reinforcement learning (RL). These methods can decrease the total training time needed by an RL agent compared to training on the target task from scratch. Unlike traditional transfer learning, we consider creating a sequence from several training tasks in order to provide the most benefit in terms of reducing the total time to train.   Our methods utilize the learning trajectory of the agent on the curriculum tasks seen so far to decide which tasks to train on next. An attractive feature of our methods is that they are weakly coupled to the choice of the RL algorithm as well as the transfer learning method. Further, when there is domain information available, our methods can incorporate such knowledge to further speed up the learning. We experimentally show that these methods can be used to obtain suitable learning curricula that speed up the overall training time on two different domains.",0
"The objective of this study is to propose several online techniques for constructing a ""learning curriculum"" from a set of training tasks that are specific to the target task. The aim is to accelerate reinforcement learning (RL) by reducing the total amount of training time required by the RL agent in comparison to training it from scratch. Unlike traditional transfer learning approaches, the proposed methods involve creating a sequence of training tasks that provide the greatest benefit in terms of reducing the total training time. The methods rely on the learning trajectory of the agent on the curriculum tasks seen thus far to determine which tasks to train on next. A significant advantage of these methods is that they are not strongly dependent on the choice of RL algorithm or transfer learning method. Additionally, when domain information is available, the methods can incorporate this knowledge to expedite the learning process even further. The experimental results demonstrate that these methods can be employed to generate effective learning curricula that decrease the overall training time in two distinct domains.",1
"A major bottleneck for developing general reinforcement learning agents is determining rewards that will yield desirable behaviors under various circumstances. We introduce a general mechanism for automatically specifying meaningful behaviors from raw pixels. In particular, we train a generative adversarial network to produce short sub-goals represented through motion templates. We demonstrate that this approach generates visually meaningful behaviors in unknown environments with novel agents and describe how these motions can be used to train reinforcement learning agents.",0
"One of the main challenges in creating universal reinforcement learning agents is identifying rewards that produce desired actions in different situations. To address this, we propose a technique that automatically defines significant actions based on raw pixel data. Using a generative adversarial network, we teach the system to create concise sub-goals using motion templates. Our results show that this method generates clear and significant actions in unfamiliar environments with new agents, and we explain how these motions can be leveraged to train reinforcement learning agents.",1
"Techniques for automatically designing deep neural network architectures such as reinforcement learning based approaches have recently shown promising results. However, their success is based on vast computational resources (e.g. hundreds of GPUs), making them difficult to be widely used. A noticeable limitation is that they still design and train each network from scratch during the exploration of the architecture space, which is highly inefficient. In this paper, we propose a new framework toward efficient architecture search by exploring the architecture space based on the current network and reusing its weights. We employ a reinforcement learning agent as the meta-controller, whose action is to grow the network depth or layer width with function-preserving transformations. As such, the previously validated networks can be reused for further exploration, thus saves a large amount of computational cost. We apply our method to explore the architecture space of the plain convolutional neural networks (no skip-connections, branching etc.) on image benchmark datasets (CIFAR-10, SVHN) with restricted computational resources (5 GPUs). Our method can design highly competitive networks that outperform existing networks using the same design scheme. On CIFAR-10, our model without skip-connections achieves 4.23\% test error rate, exceeding a vast majority of modern architectures and approaching DenseNet. Furthermore, by applying our method to explore the DenseNet architecture space, we are able to achieve more accurate networks with fewer parameters.",0
"Recent advancements in techniques for automatically designing deep neural network architectures, such as reinforcement learning-based approaches, have shown great promise. However, the requirement of vast computational resources, such as hundreds of GPUs, has made them less accessible for widespread use. One of the main issues is that these approaches still design and train each network from scratch during the exploration of the architecture space, which is highly inefficient. In this paper, we present a new framework that aims to address this challenge by exploring the architecture space based on the current network and reusing its weights. Our approach leverages a reinforcement learning agent as the meta-controller, which grows the network depth or layer width with function-preserving transformations. This enables previously validated networks to be reused for further exploration, resulting in significant computational cost savings. We apply our method to explore the architecture space of plain convolutional neural networks on image benchmark datasets with restricted computational resources. Our approach achieves highly competitive networks that outperform existing networks using the same design scheme. For instance, our model without skip-connections achieves a test error rate of 4.23% on CIFAR-10, which exceeds a vast majority of modern architectures and approaches DenseNet. Furthermore, our method can be applied to explore the DenseNet architecture space, leading to more accurate networks with fewer parameters.",1
"Despite significant progress in a variety of vision-and-language problems, developing a method capable of asking intelligent, goal-oriented questions about images is proven to be an inscrutable challenge. Towards this end, we propose a Deep Reinforcement Learning framework based on three new intermediate rewards, namely goal-achieved, progressive and informativeness that encourage the generation of succinct questions, which in turn uncover valuable information towards the overall goal. By directly optimizing for questions that work quickly towards fulfilling the overall goal, we avoid the tendency of existing methods to generate long series of insane queries that add little value. We evaluate our model on the GuessWhat?! dataset and show that the resulting questions can help a standard Guesser identify a specific object in an image at a much higher success rate.",0
"Despite making significant advancements in various vision-and-language problems, creating a technique that can ask intelligent, purpose-driven questions about images has proven to be a difficult challenge. To address this, we propose a Deep Reinforcement Learning framework that utilizes three new intermediate rewards - goal-achieved, progressive, and informativeness - to encourage the generation of concise questions that reveal valuable information towards the overall goal. By focusing on optimizing questions that efficiently work towards fulfilling the ultimate goal, we avoid the tendency of current methods to generate lengthy, unhelpful queries. Our model is evaluated using the GuessWhat?! dataset and demonstrates that the resulting questions significantly enhance a standard Guesser's ability to identify a specific object in an image.",1
"The Visual Dialogue task requires an agent to engage in a conversation about an image with a human. It represents an extension of the Visual Question Answering task in that the agent needs to answer a question about an image, but it needs to do so in light of the previous dialogue that has taken place. The key challenge in Visual Dialogue is thus maintaining a consistent, and natural dialogue while continuing to answer questions correctly. We present a novel approach that combines Reinforcement Learning and Generative Adversarial Networks (GANs) to generate more human-like responses to questions. The GAN helps overcome the relative paucity of training data, and the tendency of the typical MLE-based approach to generate overly terse answers. Critically, the GAN is tightly integrated into the attention mechanism that generates human-interpretable reasons for each answer. This means that the discriminative model of the GAN has the task of assessing whether a candidate answer is generated by a human or not, given the provided reason. This is significant because it drives the generative model to produce high quality answers that are well supported by the associated reasoning. The method also generates the state-of-the-art results on the primary benchmark.",0
"The task of Visual Dialogue involves conversing with a human about an image, requiring the agent to answer questions while considering previous dialogue. The main challenge is to maintain a natural dialogue while providing accurate answers. Our unique approach utilizes a combination of Reinforcement Learning and Generative Adversarial Networks (GANs) to generate responses that closely resemble human responses. The GAN helps overcome the limitations of limited training data and generates more detailed answers than traditional approaches. Additionally, the GAN is integrated with the attention mechanism that produces human-interpretable reasoning for each answer. This characteristic enables the discriminative model of the GAN to judge whether a candidate answer is human-generated based on the provided reasoning. This results in high-quality answers that are well-supported by the associated reasoning, and our method has achieved state-of-the-art results on the primary benchmark.",1
"The Deep Q-Network proposed by Mnih et al. [2015] has become a benchmark and building point for much deep reinforcement learning research. However, replicating results for complex systems is often challenging since original scientific publications are not always able to describe in detail every important parameter setting and software engineering solution. In this paper, we present results from our work reproducing the results of the DQN paper. We highlight key areas in the implementation that were not covered in great detail in the original paper to make it easier for researchers to replicate these results, including termination conditions and gradient descent algorithms. Finally, we discuss methods for improving the computational performance and provide our own implementation that is designed to work with a range of domains, and not just the original Arcade Learning Environment [Bellemare et al., 2013].",0
"The Deep Q-Network put forth by Mnih et al. [2015] has become a foundation for much of the research in deep reinforcement learning, serving as a benchmark. However, reproducing the results for intricate systems can be challenging, as original scientific publications often lack thorough descriptions of crucial parameter settings and software engineering solutions. This paper presents our work in replicating the results of the DQN paper, emphasizing significant areas in the implementation that were not adequately covered in the original publication. We aim to facilitate the replication of these results for other researchers, specifically regarding termination conditions and gradient descent algorithms. Additionally, we explore ways to enhance computational performance and provide our own implementation that can function in various domains, not only the original Arcade Learning Environment [Bellemare et al., 2013].",1
"Deep reinforcement learning algorithms can learn complex behavioral skills, but real-world application of these methods requires a large amount of experience to be collected by the agent. In practical settings, such as robotics, this involves repeatedly attempting a task, resetting the environment between each attempt. However, not all tasks are easily or automatically reversible. In practice, this learning process requires extensive human intervention. In this work, we propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and reset policy, with the reset policy resetting the environment for a subsequent attempt. By learning a value function for the reset policy, we can automatically determine when the forward policy is about to enter a non-reversible state, providing for uncertainty-aware safety aborts. Our experiments illustrate that proper use of the reset policy can greatly reduce the number of manual resets required to learn a task, can reduce the number of unsafe actions that lead to non-reversible states, and can automatically induce a curriculum.",0
"Although deep reinforcement learning algorithms have the ability to grasp intricate behavioral skills, their implementation in real-world scenarios necessitates the accumulation of a substantial amount of experience by the agent. To attain this experience, one must repeatedly attempt a task while resetting the environment between each attempt, which can be challenging when the task is not easily reversible. As a result, human intervention is often required during the learning process. This study proposes an automated approach to safe and efficient reinforcement learning by learning a forward and reset policy concurrently. The reset policy resets the environment for another attempt, while the value function for the reset policy detects when the forward policy is about to enter an irreversible state, allowing for uncertainty-aware safety aborts. Proper use of the reset policy can significantly decrease the number of manual resets required to learn a task, minimize the occurrence of unsafe actions, and trigger a curriculum automatically.",1
"Recently it has been shown that policy-gradient methods for reinforcement learning can be utilized to train deep end-to-end systems directly on non-differentiable metrics for the task at hand. In this paper we consider the problem of optimizing image captioning systems using reinforcement learning, and show that by carefully optimizing our systems using the test metrics of the MSCOCO task, significant gains in performance can be realized. Our systems are built using a new optimization approach that we call self-critical sequence training (SCST). SCST is a form of the popular REINFORCE algorithm that, rather than estimating a ""baseline"" to normalize the rewards and reduce variance, utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences. Using this approach, estimating the reward signal (as actor-critic methods must do) and estimating normalization (as REINFORCE algorithms typically do) is avoided, while at the same time harmonizing the model with respect to its test-time inference procedure. Empirically we find that directly optimizing the CIDEr metric with SCST and greedy decoding at test-time is highly effective. Our results on the MSCOCO evaluation sever establish a new state-of-the-art on the task, improving the best result in terms of CIDEr from 104.9 to 114.7.",0
"Recently, it has been demonstrated that deep end-to-end systems can be trained using policy-gradient methods for reinforcement learning on non-differentiable metrics for the given task. This research aims to optimize image captioning systems using reinforcement learning and highlights that by meticulously optimizing the systems using the MSCOCO task's test metrics, there can be a significant improvement in performance. The researchers use a new optimization approach called self-critical sequence training (SCST), which is a variation of the REINFORCE algorithm. SCST normalizes the rewards experienced by using the output of its own test-time inference algorithm instead of estimating a baseline to reduce variance. This approach avoids estimating the reward signal and normalization while harmonizing the model concerning its test-time inference procedure. The study finds that optimizing the CIDEr metric with SCST and greedy decoding at test-time is highly effective. The results on the MSCOCO evaluation server establish a new state-of-the-art on the task, outperforming the previous best result in terms of CIDEr from 104.9 to 114.7.",1
"We consider a reinforcement learning (RL) setting in which the agent interacts with a sequence of episodic MDPs. At the start of each episode the agent has access to some side-information or context that determines the dynamics of the MDP for that episode. Our setting is motivated by applications in healthcare where baseline measurements of a patient at the start of a treatment episode form the context that may provide information about how the patient might respond to treatment decisions. We propose algorithms for learning in such Contextual Markov Decision Processes (CMDPs) under an assumption that the unobserved MDP parameters vary smoothly with the observed context. We also give lower and upper PAC bounds under the smoothness assumption. Because our lower bound has an exponential dependence on the dimension, we consider a tractable linear setting where the context is used to create linear combinations of a finite set of MDPs. For the linear setting, we give a PAC learning algorithm based on KWIK learning techniques.",0
"In our study, we examine a reinforcement learning (RL) scenario where an agent interacts with a series of episodic MDPs. Prior to each episode, the agent receives contextual information that determines the MDP's dynamics for that specific episode. Our research is inspired by healthcare applications, where contextual patient data at the beginning of a treatment episode may offer insights into treatment outcomes. We propose CMDP learning algorithms that operate under the assumption that the unknown MDP parameters change smoothly with the observed context, and we provide both lower and upper PAC bounds under this assumption. Due to the exponential dependence on dimension in our lower bound, we focus on a manageable linear setting where the context generates linear combinations of a finite MDP set. For this linear situation, we present a KWIK-based PAC learning algorithm.",1
"We present the Variational Adaptive Newton (VAN) method which is a black-box optimization method especially suitable for explorative-learning tasks such as active learning and reinforcement learning. Similar to Bayesian methods, VAN estimates a distribution that can be used for exploration, but requires computations that are similar to continuous optimization methods. Our theoretical contribution reveals that VAN is a second-order method that unifies existing methods in distinct fields of continuous optimization, variational inference, and evolution strategies. Our experimental results show that VAN performs well on a wide-variety of learning tasks. This work presents a general-purpose explorative-learning method that has the potential to improve learning in areas such as active learning and reinforcement learning.",0
"Introducing the Variational Adaptive Newton (VAN) method, a black-box optimization technique that is particularly well-suited for exploratory learning tasks like active learning and reinforcement learning. VAN estimates a distribution for exploration, similar to Bayesian methods, but uses computations similar to continuous optimization methods. Our theoretical contribution establishes that VAN is a second-order method that unifies existing techniques in continuous optimization, variational inference, and evolution strategies. In our experiments, VAN performed effectively across a diverse range of learning tasks. This study presents a versatile exploratory-learning approach with the potential to enhance learning in fields such as active learning and reinforcement learning.",1
"This paper formalises the problem of online algorithm selection in the context of Reinforcement Learning. The setup is as follows: given an episodic task and a finite number of off-policy RL algorithms, a meta-algorithm has to decide which RL algorithm is in control during the next episode so as to maximize the expected return. The article presents a novel meta-algorithm, called Epochal Stochastic Bandit Algorithm Selection (ESBAS). Its principle is to freeze the policy updates at each epoch, and to leave a rebooted stochastic bandit in charge of the algorithm selection. Under some assumptions, a thorough theoretical analysis demonstrates its near-optimality considering the structural sampling budget limitations. ESBAS is first empirically evaluated on a dialogue task where it is shown to outperform each individual algorithm in most configurations. ESBAS is then adapted to a true online setting where algorithms update their policies after each transition, which we call SSBAS. SSBAS is evaluated on a fruit collection task where it is shown to adapt the stepsize parameter more efficiently than the classical hyperbolic decay, and on an Atari game, where it improves the performance by a wide margin.",0
"The problem of selecting the best online algorithm in Reinforcement Learning is formalized in this paper. The goal is to determine which RL algorithm should be used for the next episode in an episodic task with a finite number of off-policy RL algorithms, in order to maximize the expected return. A new meta-algorithm, ESBAS, is introduced, which involves freezing policy updates at each epoch and utilizing a rebooted stochastic bandit to make algorithm selections. Theoretical analysis shows that ESBAS is nearly optimal with respect to structural sampling budget limitations. ESBAS is evaluated on a dialogue task and is demonstrated to outperform individual algorithms in most configurations. Additionally, SSBAS is adapted for use in a true online setting where policies are updated after each transition, and it is shown to be more efficient than the classical hyperbolic decay in adapting the stepsize parameter. Finally, SSBAS is evaluated on an Atari game and significantly improves performance.",1
"We consider tackling a single-agent RL problem by distributing it to $n$ learners. These learners, called advisors, endeavour to solve the problem from a different focus. Their advice, taking the form of action values, is then communicated to an aggregator, which is in control of the system. We show that the local planning method for the advisors is critical and that none of the ones found in the literature is flawless: the egocentric planning overestimates values of states where the other advisors disagree, and the agnostic planning is inefficient around danger zones. We introduce a novel approach called empathic and discuss its theoretical aspects. We empirically examine and validate our theoretical findings on a fruit collection task.",0
"Our approach to tackling a single-agent RL problem is to distribute it to $n$ learners, also known as advisors, who approach the problem with different perspectives. These advisors provide advice in the form of action values, which are then communicated to an aggregator that controls the system. We demonstrate that the advisors' local planning method is crucial and that none of the existing methods in the literature are flawless. Specifically, egocentric planning overestimates state values where other advisors disagree, while agnostic planning is inefficient around danger zones. Therefore, we propose a novel approach called empathic and explore its theoretical aspects. We validate our theoretical findings by conducting an empirical examination on a fruit collection task.",1
"Humans process visual scenes selectively and sequentially using attention. Central to models of human visual attention is the saliency map. We propose a hierarchical visual architecture that operates on a saliency map and uses a novel attention mechanism to sequentially focus on salient regions and take additional glimpses within those regions. The architecture is motivated by human visual attention, and is used for multi-label image classification on a novel multiset task, demonstrating that it achieves high precision and recall while localizing objects with its attention. Unlike conventional multi-label image classification models, the model supports multiset prediction due to a reinforcement-learning based training process that allows for arbitrary label permutation and multiple instances per label.",0
"The way that humans process visual scenes is through selective and sequential attention. The saliency map is a crucial component of models for human visual attention. Our proposal is a visual architecture that operates hierarchically on a saliency map and uses a unique attention mechanism to focus sequentially on salient areas, taking additional glimpses within those areas. Our architecture is inspired by human visual attention and is used for multi-label image classification on a new multiset task. Our model achieves high precision and recall while localizing objects with attention. Unlike traditional multi-label image classification models, our model supports multiset prediction due to a reinforcement-learning-based training process that allows for arbitrary label permutation and multiple instances per label.",1
"Reinforcement learning is a powerful paradigm for learning optimal policies from experimental data. However, to find optimal policies, most reinforcement learning algorithms explore all possible actions, which may be harmful for real-world systems. As a consequence, learning algorithms are rarely applied on safety-critical systems in the real world. In this paper, we present a learning algorithm that explicitly considers safety, defined in terms of stability guarantees. Specifically, we extend control-theoretic results on Lyapunov stability verification and show how to use statistical models of the dynamics to obtain high-performance control policies with provable stability certificates. Moreover, under additional regularity assumptions in terms of a Gaussian process prior, we prove that one can effectively and safely collect data in order to learn about the dynamics and thus both improve control performance and expand the safe region of the state space. In our experiments, we show how the resulting algorithm can safely optimize a neural network policy on a simulated inverted pendulum, without the pendulum ever falling down.",0
"The use of reinforcement learning for finding optimal policies is a powerful approach, but exploring all potential actions can be detrimental to real-world systems. As a result, reinforcement learning algorithms are seldom employed in safety-critical applications. This study introduces a safety-conscious learning algorithm that utilizes stability guarantees to explicitly consider safety. The algorithm builds on control-theoretic findings concerning Lyapunov stability verification and statistical models of the dynamics. The resulting control policies have high performance and provable stability certificates. With the help of a Gaussian process prior, data can be safely collected to learn about the dynamics, improving control performance and expanding the safe area of the state space. The algorithm's effectiveness is demonstrated by optimizing a neural network policy on a simulated inverted pendulum, which remains stable throughout the experiments.",1
"Generative Adversarial Networks (GANs) are a powerful framework for deep generative modeling. Posed as a two-player minimax problem, GANs are typically trained end-to-end on real-valued data and can be used to train a generator of high-dimensional and realistic images. However, a major limitation of GANs is that training relies on passing gradients from the discriminator through the generator via back-propagation. This makes it fundamentally difficult to train GANs with discrete data, as generation in this case typically involves a non-differentiable function. These difficulties extend to the reinforcement learning setting when the action space is composed of discrete decisions. We address these issues by reframing the GAN framework so that the generator is no longer trained using gradients through the discriminator, but is instead trained using a learned critic in the actor-critic framework with a Temporal Difference (TD) objective. This is a natural fit for sequence modeling and we use it to achieve improvements on language modeling tasks over the standard Teacher-Forcing methods.",0
"GANs are a robust approach to deep generative modeling that can be used to train a high-dimensional and realistic image generator. However, GANs face a significant challenge when training with discrete data since it involves a non-differentiable function that makes gradient-based training difficult. This problem also arises in reinforcement learning when the action space is made up of discrete decisions. To overcome these issues, we propose a new approach that reframes the GAN framework. In this new approach, the generator is trained using a learned critic in the actor-critic framework with a Temporal Difference (TD) objective. This approach is particularly suited for sequence modeling tasks, and we demonstrate its effectiveness by achieving better results on language modeling tasks compared to traditional Teacher-Forcing methods.",1
"Despite widespread interests in reinforcement-learning for task-oriented dialogue systems, several obstacles can frustrate research and development progress. First, reinforcement learners typically require interaction with the environment, so conventional dialogue corpora cannot be used directly. Second, each task presents specific challenges, requiring separate corpus of task-specific annotated data. Third, collecting and annotating human-machine or human-human conversations for task-oriented dialogues requires extensive domain knowledge. Because building an appropriate dataset can be both financially costly and time-consuming, one popular approach is to build a user simulator based upon a corpus of example dialogues. Then, one can train reinforcement learning agents in an online fashion as they interact with the simulator. Dialogue agents trained on these simulators can serve as an effective starting point. Once agents master the simulator, they may be deployed in a real environment to interact with humans, and continue to be trained online. To ease empirical algorithmic comparisons in dialogues, this paper introduces a new, publicly available simulation framework, where our simulator, designed for the movie-booking domain, leverages both rules and collected data. The simulator supports two tasks: movie ticket booking and movie seeking. Finally, we demonstrate several agents and detail the procedure to add and test your own agent in the proposed framework.",0
"The development of reinforcement-learning for task-oriented dialogue systems is impeded by various challenges. Firstly, reinforcement learners necessitate interaction with the environment, making conventional dialogue corpora unsuitable. Secondly, each task requires a specific set of annotated data, which can be time-consuming and expensive to collect. Thirdly, amassing and annotating human-machine or human-human conversations for task-oriented dialogues necessitates extensive domain knowledge. To mitigate these challenges, researchers have opted to construct user simulators based on a corpus of sample dialogues. These simulators can be used to train reinforcement learning agents that can subsequently be deployed in real environments to interact with humans and continue online training. To facilitate empirical algorithmic comparisons in dialogues, this paper introduces a new simulation framework that leverages rules and collected data to support two tasks: movie ticket booking and movie seeking. Furthermore, several agents are demonstrated, and the procedure for adding and testing one's agent in the proposed framework is detailed.",1
"This paper explores the non-convex composition optimization in the form including inner and outer finite-sum functions with a large number of component functions. This problem arises in some important applications such as nonlinear embedding and reinforcement learning. Although existing approaches such as stochastic gradient descent (SGD) and stochastic variance reduced gradient (SVRG) descent can be applied to solve this problem, their query complexity tends to be high, especially when the number of inner component functions is large. In this paper, we apply the variance-reduced technique to derive two variance reduced algorithms that significantly improve the query complexity if the number of inner component functions is large. To the best of our knowledge, this is the first work that establishes the query complexity analysis for non-convex stochastic composition. Experiments validate the proposed algorithms and theoretical analysis.",0
"In this paper, the focus is on investigating non-convex composition optimization, which involves inner and outer finite-sum functions with several component functions. This type of problem is commonly encountered in critical applications like reinforcement learning and nonlinear embedding. Although approaches like stochastic gradient descent (SGD) and stochastic variance reduced gradient (SVRG) descent can be used to solve the problem, their query complexity is typically high, especially when the number of inner component functions is large. To address this issue, this paper proposes two variance-reduced algorithms that significantly lower the query complexity for non-convex stochastic composition. This study is the first to establish a query complexity analysis for this type of problem. The proposed algorithms and theoretical analysis are validated by experiments.",1
"Bayesian neural networks (BNNs) with latent variables are probabilistic models which can automatically identify complex stochastic patterns in the data. We describe and study in these models a decomposition of predictive uncertainty into its epistemic and aleatoric components. First, we show how such a decomposition arises naturally in a Bayesian active learning scenario by following an information theoretic approach. Second, we use a similar decomposition to develop a novel risk sensitive objective for safe reinforcement learning (RL). This objective minimizes the effect of model bias in environments whose stochastic dynamics are described by BNNs with latent variables. Our experiments illustrate the usefulness of the resulting decomposition in active learning and safe RL settings.",0
"Probabilistic models known as Bayesian neural networks (BNNs) that have latent variables can recognize intricate stochastic patterns in data on their own. In this study, we explore a breakdown of predictive uncertainty into its epistemic and aleatoric components in these models. Initially, we demonstrate how this dissection occurs organically in a Bayesian active learning scenario, utilizing an information-theoretic approach. We then employ a similar decomposition to create a fresh risk-sensitive objective for safe reinforcement learning (RL). This objective minimizes the impact of model bias in environments in which BNNs with latent variables define the stochastic dynamics. Our experiments demonstrate the value of the resulting breakdown in active learning and safe RL settings.",1
"We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform's utility for autonomous driving research. The supplementary video can be viewed at https://youtu.be/Hp8Dz-Zek2E",0
"CARLA is a simulator designed for autonomous driving research, which offers the necessary tools to develop, train, and validate self-driving systems in urban environments. Along with the open-source code and protocols, CARLA provides open digital assets such as buildings, vehicles, and urban layouts that can be freely used for this purpose. The simulator is highly customizable, allowing for flexible specification of sensor suites and environmental conditions. To evaluate the performance of various autonomous driving approaches, we used CARLA to conduct controlled scenarios of increasing difficulty. We analyzed the results using CARLA's metrics, which demonstrate the platform's versatility for autonomous driving research. A supplementary video can be found at https://youtu.be/Hp8Dz-Zek2E.",1
"In this paper, we propose surrogate agent-environment interface (SAEI) in reinforcement learning. We also state that learning based on probability surrogate agent-environment interface provides optimal policy of task agent-environment interface. We introduce surrogate probability action and develop the probability surrogate action deterministic policy gradient (PSADPG) algorithm based on SAEI. This algorithm enables continuous control of discrete action. The experiments show PSADPG achieves the performance of DQN in certain tasks with the stochastic optimal policy nature in the initial training stage.",0
"The proposed idea in this study is the use of surrogate agent-environment interface (SAEI) in reinforcement learning. The paper suggests that optimal policy for task agent-environment interface can be achieved through learning with probability surrogate agent-environment interface. The study also presents the concept of surrogate probability action and how it can be used to create the probability surrogate action deterministic policy gradient (PSADPG) algorithm in line with SAEI. PSADPG allows for continuous control of discrete action and the experiments indicate that it performs similarly to DQN in certain tasks, with a stochastic optimal policy nature during the initial training stage.",1
"We study reinforcement learning under model misspecification, where we do not have access to the true environment but only to a reasonably close approximation to it. We address this problem by extending the framework of robust MDPs to the model-free Reinforcement Learning setting, where we do not have access to the model parameters, but can only sample states from it. We define robust versions of Q-learning, SARSA, and TD-learning and prove convergence to an approximately optimal robust policy and approximate value function respectively. We scale up the robust algorithms to large MDPs via function approximation and prove convergence under two different settings. We prove convergence of robust approximate policy iteration and robust approximate value iteration for linear architectures (under mild assumptions). We also define a robust loss function, the mean squared robust projected Bellman error and give stochastic gradient descent algorithms that are guaranteed to converge to a local minimum.",0
"The focus of our research is on reinforcement learning when the model is not accurately specified and only an approximation is available. To tackle this issue, we expand the robust MDPs framework to include model-free reinforcement learning, where the model parameters are not available, but states can be sampled. Our approach involves the development of robust forms of Q-learning, SARSA, and TD-learning, which lead to convergence of an approximately optimal robust policy and an approximate value function. We also demonstrate the scalability of the robust algorithms to large MDPs by using function approximation and proving convergence under two different scenarios. Specifically, we prove the convergence of robust approximate policy iteration and robust approximate value iteration for linear architectures with mild assumptions. In addition, we introduce a robust loss function, the mean squared robust projected Bellman error, and provide stochastic gradient descent algorithms that can converge to a local minimum.",1
"Methods for neural network hyperparameter optimization and meta-modeling are computationally expensive due to the need to train a large number of model configurations. In this paper, we show that standard frequentist regression models can predict the final performance of partially trained model configurations using features based on network architectures, hyperparameters, and time-series validation performance data. We empirically show that our performance prediction models are much more effective than prominent Bayesian counterparts, are simpler to implement, and are faster to train. Our models can predict final performance in both visual classification and language modeling domains, are effective for predicting performance of drastically varying model architectures, and can even generalize between model classes. Using these prediction models, we also propose an early stopping method for hyperparameter optimization and meta-modeling, which obtains a speedup of a factor up to 6x in both hyperparameter optimization and meta-modeling. Finally, we empirically show that our early stopping method can be seamlessly incorporated into both reinforcement learning-based architecture selection algorithms and bandit based search methods. Through extensive experimentation, we empirically show our performance prediction models and early stopping algorithm are state-of-the-art in terms of prediction accuracy and speedup achieved while still identifying the optimal model configurations.",0
"The process of optimizing neural network hyperparameters and meta-modeling is computationally intensive as it requires training a large number of model configurations. This study demonstrates that standard frequentist regression models can predict the final performance of partially trained models by utilizing features based on network architectures, hyperparameters, and time-series validation performance data. Our empirical findings indicate that our performance prediction models are more efficient than prominent Bayesian alternatives, are straightforward to implement, and require less time to train. These models can predict final performance for visual classification and language modeling domains, and can generalize between model classes with varying architectures. Additionally, we introduce an early stopping method for hyperparameter optimization and meta-modeling which results in a speedup of up to 6x. We demonstrate that our early stopping method can be easily incorporated into reinforcement learning-based architecture selection algorithms and bandit-based search methods. Our extensive experimentation validates the state-of-the-art prediction accuracy and speedup achieved by our performance prediction models and early stopping algorithm while still identifying optimal model configurations.",1
"Robustness and security of machine learning (ML) systems are intertwined, wherein a non-robust ML system (classifiers, regressors, etc.) can be subject to attacks using a wide variety of exploits. With the advent of scalable deep learning methodologies, a lot of emphasis has been put on the robustness of supervised, unsupervised and reinforcement learning algorithms. Here, we study the robustness of the latent space of a deep variational autoencoder (dVAE), an unsupervised generative framework, to show that it is indeed possible to perturb the latent space, flip the class predictions and keep the classification probability approximately equal before and after an attack. This means that an agent that looks at the outputs of a decoder would remain oblivious to an attack.",0
"The robustness and security of machine learning (ML) systems are interconnected. If an ML system, such as classifiers or regressors, is not robust, it may be vulnerable to various attacks. The advancement of scalable deep learning techniques has led to a focus on the robustness of supervised, unsupervised, and reinforcement learning algorithms. Our study examines the robustness of the latent space in a deep variational autoencoder (dVAE), which is an unsupervised generative framework. We demonstrate that it is possible to manipulate the latent space, alter the class predictions, and maintain similar classification probabilities before and after an attack. Consequently, an attacker can deceive an agent that relies on the decoder's outputs without detection.",1
"It is becoming increasingly clear that many machine learning classifiers are vulnerable to adversarial examples. In attempting to explain the origin of adversarial examples, previous studies have typically focused on the fact that neural networks operate on high dimensional data, they overfit, or they are too linear. Here we argue that the origin of adversarial examples is primarily due to an inherent uncertainty that neural networks have about their predictions. We show that the functional form of this uncertainty is independent of architecture, dataset, and training protocol; and depends only on the statistics of the logit differences of the network, which do not change significantly during training. This leads to adversarial error having a universal scaling, as a power-law, with respect to the size of the adversarial perturbation. We show that this universality holds for a broad range of datasets (MNIST, CIFAR10, ImageNet, and random data), models (including state-of-the-art deep networks, linear models, adversarially trained networks, and networks trained on randomly shuffled labels), and attacks (FGSM, step l.l., PGD). Motivated by these results, we study the effects of reducing prediction entropy on adversarial robustness. Finally, we study the effect of network architectures on adversarial sensitivity. To do this, we use neural architecture search with reinforcement learning to find adversarially robust architectures on CIFAR10. Our resulting architecture is more robust to white \emph{and} black box attacks compared to previous attempts.",0
"The susceptibility of many machine learning classifiers to adversarial examples is becoming increasingly apparent. Prior research has typically attributed the origin of these examples to high-dimensional data, overfitting, or linearity in neural networks. In contrast, we contend that the primary cause is the inherent uncertainty surrounding neural networks' predictions. Our findings demonstrate that this uncertainty's functional form is architecture-agnostic, dataset-agnostic, and training-agnostic, instead relying solely on the network's logit differences' statistics, which remain constant throughout training. Consequently, adversarial error scales universally with respect to the perturbation's size, following a power-law distribution. This universality is observed across a broad range of datasets, models, and attacks, including state-of-the-art deep networks, linear models, and adversarially trained networks. Moreover, our study investigates the effects of reducing prediction entropy on adversarial robustness and examines the impact of network architectures on adversarial sensitivity using neural architecture search with reinforcement learning. Our resulting architecture exhibits superior resistance to both white and black box attacks compared to previous efforts.",1
"We show how an ensemble of $Q^*$-functions can be leveraged for more effective exploration in deep reinforcement learning. We build on well established algorithms from the bandit setting, and adapt them to the $Q$-learning setting. We propose an exploration strategy based on upper-confidence bounds (UCB). Our experiments show significant gains on the Atari benchmark.",0
Our study demonstrates the utilization of a group of $Q^*$-functions to enhance exploration in deep reinforcement learning. We incorporate existing techniques from the bandit domain and modify them to suit the $Q$-learning environment. Our proposed exploration approach relies on upper-confidence bounds (UCB). Empirical results from our experiments on the Atari benchmark indicate notable improvement.,1
"Boltzmann exploration is a classic strategy for sequential decision-making under uncertainty, and is one of the most standard tools in Reinforcement Learning (RL). Despite its widespread use, there is virtually no theoretical understanding about the limitations or the actual benefits of this exploration scheme. Does it drive exploration in a meaningful way? Is it prone to misidentifying the optimal actions or spending too much time exploring the suboptimal ones? What is the right tuning for the learning rate? In this paper, we address several of these questions in the classic setup of stochastic multi-armed bandits. One of our main results is showing that the Boltzmann exploration strategy with any monotone learning-rate sequence will induce suboptimal behavior. As a remedy, we offer a simple non-monotone schedule that guarantees near-optimal performance, albeit only when given prior access to key problem parameters that are typically not available in practical situations (like the time horizon $T$ and the suboptimality gap $\Delta$). More importantly, we propose a novel variant that uses different learning rates for different arms, and achieves a distribution-dependent regret bound of order $\frac{K\log^2 T}{\Delta}$ and a distribution-independent bound of order $\sqrt{KT}\log K$ without requiring such prior knowledge. To demonstrate the flexibility of our technique, we also propose a variant that guarantees the same performance bounds even if the rewards are heavy-tailed.",0
"The Boltzmann exploration strategy is a widely used tool in Reinforcement Learning for making sequential decisions under uncertainty. However, there is little understanding of its limitations and benefits. This paper explores questions such as whether it meaningfully drives exploration, if it tends to misidentify optimal actions, and what the appropriate learning rate is. The study focuses on stochastic multi-armed bandits and reveals that Boltzmann exploration with a monotone learning-rate sequence leads to suboptimal behavior. A non-monotone schedule is proposed as a solution, but it requires prior knowledge of problem parameters not typically available. A new variant is introduced that utilizes different learning rates for different arms and achieves optimal performance without requiring such prior knowledge. The technique is also shown to work for heavy-tailed rewards.",1
"Reinforcement Learning (RL) can model complex behavior policies for goal-directed sequential decision making tasks. A hallmark of RL algorithms is Temporal Difference (TD) learning: value function for the current state is moved towards a bootstrapped target that is estimated using next state's value function. $\lambda$-returns generalize beyond 1-step returns and strike a balance between Monte Carlo and TD learning methods. While lambda-returns have been extensively studied in RL, they haven't been explored a lot in Deep RL. This paper's first contribution is an exhaustive benchmarking of lambda-returns. Although mathematically tractable, the use of exponentially decaying weighting of n-step returns based targets in lambda-returns is a rather ad-hoc design choice. Our second major contribution is that we propose a generalization of lambda-returns called Confidence-based Autodidactic Returns (CAR), wherein the RL agent learns the weighting of the n-step returns in an end-to-end manner. This allows the agent to learn to decide how much it wants to weigh the n-step returns based targets. In contrast, lambda-returns restrict RL agents to use an exponentially decaying weighting scheme. Autodidactic returns can be used for improving any RL algorithm which uses TD learning. We empirically demonstrate that using sophisticated weighted mixtures of multi-step returns (like CAR and lambda-returns) considerably outperforms the use of n-step returns. We perform our experiments on the Asynchronous Advantage Actor Critic (A3C) algorithm in the Atari 2600 domain.",0
"The goal of Reinforcement Learning (RL) is to model complex behavior policies for sequential decision making tasks. One of the key features of RL algorithms is Temporal Difference (TD) learning, where the value function for the current state is adjusted towards a bootstrapped target estimated using the next state's value function. The use of $\lambda$-returns has been extensively studied in RL for striking a balance between Monte Carlo and TD learning methods, but they have not been explored much in Deep RL. This paper presents two major contributions. Firstly, it offers an exhaustive benchmarking of lambda-returns. Secondly, it proposes a generalization of lambda-returns called Confidence-based Autodidactic Returns (CAR), which allows the RL agent to learn the weighting of the n-step returns in an end-to-end manner. This is in contrast to the exponentially decaying weighting scheme used by lambda-returns, which restricts the agent's decision-making capabilities. Autodidactic returns can be used to improve any RL algorithm that utilizes TD learning, and our experiments show that using CAR and lambda-returns significantly outperforms using n-step returns. The experiments were performed on the Atari 2600 domain using the Asynchronous Advantage Actor Critic (A3C) algorithm.",1
"Conventional reinforcement learning methods for Markov decision processes rely on weakly-guided, stochastic searches to drive the learning process. It can therefore be difficult to predict what agent behaviors might emerge. In this paper, we consider an information-theoretic cost function for performing constrained stochastic searches that promote the formation of risk-averse to risk-favoring behaviors. This cost function is the value of information, which provides the optimal trade-off between the expected return of a policy and the policy's complexity; policy complexity is measured by number of bits and controlled by a single hyperparameter on the cost function. As the policy complexity is reduced, the agents will increasingly eschew risky actions. This reduces the potential for high accrued rewards. As the policy complexity increases, the agents will take actions, regardless of the risk, that can raise the long-term rewards. The obtainable reward depends on a single, tunable hyperparameter that regulates the degree of policy complexity.   We evaluate the performance of value-of-information-based policies on a stochastic version of Ms. Pac-Man. A major component of this paper is the demonstration that ranges of policy complexity values yield different game-play styles and explaining why this occurs. We also show that our reinforcement-learning search mechanism is more efficient than the others we utilize. This result implies that the value of information theory is appropriate for framing the exploitation-exploration trade-off in reinforcement learning.",0
"Typical reinforcement learning techniques for Markov decision processes rely on stochastic searches with little guidance, which makes it difficult to predict the resulting agent behaviors. This paper proposes an information-theoretic cost function that promotes the development of risk-averse or risk-favoring behaviors through constrained stochastic searches. The cost function is based on the value of information, which balances the expected return of a policy with its complexity, measured by the number of bits and controlled by a hyperparameter. Lower policy complexity leads to more conservative behavior, reducing the potential for high rewards, while higher complexity encourages risk-taking for greater long-term rewards. The value-of-information-based policies are evaluated on a stochastic version of Ms. Pac-Man, demonstrating that different ranges of policy complexity values yield different gameplay styles. The study also shows that the reinforcement-learning search mechanism used is more efficient than others. These results suggest that the value of information theory is a suitable framework for the exploitation-exploration trade-off in reinforcement learning.",1
"Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. Unlike previous work, we require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). We compare a reinforcement learning approach and one using a differentiable relaxation (straight-through Gumbel-softmax estimator) and observe that the latter is much faster to converge and it results in more effective protocols. Interestingly, we also observe that the protocol we induce by optimizing the communication success exhibits a degree of compositionality and variability (i.e. the same information can be phrased in different ways), both properties characteristic of natural languages. As the ultimate goal is to ensure that communication is accomplished in natural language, we also perform experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.",0
"Developing a general AI typically requires learning to communicate through interaction rather than explicit supervision. In our study, two agents engage in a referential game and develop a communication protocol from scratch, using language as the format for messages exchanged during training and testing. We compare two approaches, one using reinforcement learning and the other using a differentiable relaxation technique (straight-through Gumbel-softmax estimator), and find that the latter is faster and results in more effective protocols. Our induced protocol exhibits compositionality and variability, similar to natural languages. To further improve the naturalness of communication, we inject prior knowledge about natural language into our model and analyze the resulting protocol.",1
"In recent years, neural networks have enjoyed a renaissance as function approximators in reinforcement learning. Two decades after Tesauro's TD-Gammon achieved near top-level human performance in backgammon, the deep reinforcement learning algorithm DQN achieved human-level performance in many Atari 2600 games. The purpose of this study is twofold. First, we propose two activation functions for neural network function approximation in reinforcement learning: the sigmoid-weighted linear unit (SiLU) and its derivative function (dSiLU). The activation of the SiLU is computed by the sigmoid function multiplied by its input. Second, we suggest that the more traditional approach of using on-policy learning with eligibility traces, instead of experience replay, and softmax action selection with simple annealing can be competitive with DQN, without the need for a separate target network. We validate our proposed approach by, first, achieving new state-of-the-art results in both stochastic SZ-Tetris and Tetris with a small 10$\times$10 board, using TD($\lambda$) learning and shallow dSiLU network agents, and, then, by outperforming DQN in the Atari 2600 domain by using a deep Sarsa($\lambda$) agent with SiLU and dSiLU hidden units.",0
"Neural networks have experienced a resurgence in their use as function approximators for reinforcement learning. After two decades since TD-Gammon's success in achieving near top-level human performance in backgammon, the deep reinforcement learning algorithm DQN has achieved human-level performance in many Atari 2600 games. This study has a dual purpose. Firstly, we introduce two activation functions, the sigmoid-weighted linear unit (SiLU) and its derivative function (dSiLU), for neural network function approximation in reinforcement learning. The SiLU activation is determined by the sigmoid function multiplied by the input. Secondly, we propose that on-policy learning with eligibility traces and softmax action selection with simple annealing, a more traditional approach, can compete with DQN without requiring a separate target network. We validate our proposed approach by attaining new state-of-the-art results in both stochastic SZ-Tetris and Tetris with a small 10x10 board using TD($\lambda$) learning and shallow dSiLU network agents. We also outperform DQN in the Atari 2600 domain by using a deep Sarsa($\lambda$) agent with SiLU and dSiLU hidden units.",1
"Approximate dynamic programming algorithms, such as approximate value iteration, have been successfully applied to many complex reinforcement learning tasks, and a better approximate dynamic programming algorithm is expected to further extend the applicability of reinforcement learning to various tasks. In this paper we propose a new, robust dynamic programming algorithm that unifies value iteration, advantage learning, and dynamic policy programming. We call it generalized value iteration (GVI) and its approximated version, approximate GVI (AGVI). We show AGVI's performance guarantee, which includes performance guarantees for existing algorithms, as special cases. We discuss theoretical weaknesses of existing algorithms, and explain the advantages of AGVI. Numerical experiments in a simple environment support theoretical arguments, and suggest that AGVI is a promising alternative to previous algorithms.",0
"Many challenging reinforcement learning tasks have been successfully tackled using approximate dynamic programming algorithms, such as approximate value iteration. However, there is still room for improvement in this field, and a more effective algorithm could expand the range of tasks that reinforcement learning can handle. In this study, we introduce a new, robust dynamic programming algorithm called generalized value iteration (GVI), which combines advantage learning, dynamic policy programming, and value iteration. We also present AGVI, an approximated version of GVI, and demonstrate its performance guarantee, which encompasses the guarantees of existing algorithms. We highlight the theoretical shortcomings of current algorithms and elucidate the benefits of AGVI. In addition, we provide numerical experiments conducted in a simple environment that support our theoretical arguments and indicate that AGVI is a promising alternative to previous algorithms.",1
"We develop a metalearning approach for learning hierarchically structured policies, improving sample efficiency on unseen tasks through the use of shared primitives---policies that are executed for large numbers of timesteps. Specifically, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.",0
"Our approach involves metalearning for acquiring hierarchically structured policies, which enhances efficiency in learning new tasks by utilizing shared primitives. These primitives refer to policies that are executed for extended periods of time. More specifically, a set of primitives is shared across a range of tasks, and task-specific policies switch between them. To measure the effectiveness of such hierarchies, we introduce a concrete metric that leads to an optimization problem for quickly achieving high rewards on new tasks. To solve this problem, we present an algorithm that works seamlessly with any standard reinforcement learning method. The algorithm involves repeatedly sampling new tasks and resetting task-specific policies. Our approach has been successful in discovering meaningful motor primitives for the directional movement of four-legged robots through interactions with mazes. We have also shown that these primitives can be transferred to solve long-timescale, sparse-reward obstacle courses, and enable 3D humanoid robots to walk and crawl with the same policy in a robust manner.",1
"We examine the problem of learning mappings from state to state, suitable for use in a model-based reinforcement-learning setting, that simultaneously generalize to novel states and can capture stochastic transitions. We show that currently popular generative adversarial networks struggle to learn these stochastic transition models but a modification to their loss functions results in a powerful learning algorithm for this class of problems.",0
"In this study, our focus is on the challenge of acquiring knowledge on state-to-state mappings, which are applicable in a model-based reinforcement learning environment, and have the ability to handle random transitions while being adaptable to new states. Our research indicates that widely used generative adversarial networks encounter difficulties in mastering these stochastic transition models. However, we propose a modification to their loss functions, which leads to a potent learning algorithm that effectively tackles this category of issues.",1
"We consider the composition optimization with two expected-value functions in the form of $\frac{1}{n}\sum\nolimits_{i = 1}^n F_i(\frac{1}{m}\sum\nolimits_{j = 1}^m G_j(x))+R(x)$, { which formulates many important problems in statistical learning and machine learning such as solving Bellman equations in reinforcement learning and nonlinear embedding}. Full Gradient or classical stochastic gradient descent based optimization algorithms are unsuitable or computationally expensive to solve this problem due to the inner expectation $\frac{1}{m}\sum\nolimits_{j = 1}^m G_j(x)$. We propose a duality-free based stochastic composition method that combines variance reduction methods to address the stochastic composition problem. We apply SVRG and SAGA based methods to estimate the inner function, and duality-free method to estimate the outer function. We prove the linear convergence rate not only for the convex composition problem, but also for the case that the individual outer functions are non-convex while the objective function is strongly-convex. We also provide the results of experiments that show the effectiveness of our proposed methods.",0
"The optimization of composition with two expected-value functions is crucial in statistical learning and machine learning. It is expressed as $\frac{1}{n}\sum\nolimits_{i = 1}^n F_i(\frac{1}{m}\sum\nolimits_{j = 1}^m G_j(x))+R(x)$ and involves solving problems such as Bellman equations in reinforcement learning and nonlinear embedding. Traditional optimization algorithms like Full Gradient and stochastic gradient descent are either computationally expensive or unsuitable to solve this problem due to the inner expectation $\frac{1}{m}\sum\nolimits_{j = 1}^m G_j(x)$. To address this, we propose a duality-free based stochastic composition method that uses variance reduction methods. We use SVRG and SAGA based methods to estimate the inner function and duality-free method to estimate the outer function. Our method not only proves linear convergence rate for the convex composition problem, but also for the case where the individual outer functions are non-convex while the objective function is strongly-convex. Our experiments show that our proposed methods are effective.",1
"Due to the lack of enough generalization in the state-space, common methods in Reinforcement Learning (RL) suffer from slow learning speed especially in the early learning trials. This paper introduces a model-based method in discrete state-spaces for increasing learning speed in terms of required experience (but not required computational time) by exploiting generalization in the experiences of the subspaces. A subspace is formed by choosing a subset of features in the original state representation (full-space). Generalization and faster learning in a subspace are due to many-to-one mapping of experiences from the full-space to each state in the subspace. Nevertheless, due to inherent perceptual aliasing in the subspaces, the policy suggested by each subspace does not generally converge to the optimal policy. Our approach, called Model Based Learning with Subspaces (MoBLeS), calculates confidence intervals of the estimated Q-values in the full-space and in the subspaces. These confidence intervals are used in the decision making, such that the agent benefits the most from the possible generalization while avoiding from detriment of the perceptual aliasing in the subspaces. Convergence of MoBLeS to the optimal policy is theoretically investigated. Additionally, we show through several experiments that MoBLeS improves the learning speed in the early trials.",0
"The slow learning speed of common Reinforcement Learning (RL) methods is attributed to the insufficient generalization in the state-space, particularly in the early learning trials. To address this issue, this paper proposes a model-based approach in discrete state-spaces that leverages generalization in the experiences of subspaces to increase learning speed in terms of required experience. A subspace is created by selecting a subset of features from the original state representation (full-space). Generalization and faster learning in a subspace are facilitated by many-to-one mapping of experiences from the full-space to each state in the subspace. However, due to inherent perceptual aliasing in the subspaces, the suggested policy may not always converge to the optimal policy. To overcome this limitation, the proposed Model Based Learning with Subspaces (MoBLeS) method calculates confidence intervals of the estimated Q-values in the full-space and subspaces, which are used in decision making to maximize the benefits of generalization while avoiding the drawbacks of perceptual aliasing. The paper also presents a theoretical investigation of MoBLeS's convergence to the optimal policy and demonstrates through several experiments that MoBLeS improves the learning speed in the early trials.",1
"Reinforcement Learning is divided in two main paradigms: model-free and model-based. Each of these two paradigms has strengths and limitations, and has been successfully applied to real world domains that are appropriate to its corresponding strengths. In this paper, we present a new approach aimed at bridging the gap between these two paradigms. We aim to take the best of the two paradigms and combine them in an approach that is at the same time data-efficient and cost-savvy. We do so by learning a probabilistic dynamics model and leveraging it as a prior for the intertwined model-free optimization. As a result, our approach can exploit the generality and structure of the dynamics model, but is also capable of ignoring its inevitable inaccuracies, by directly incorporating the evidence provided by the direct observation of the cost. Preliminary results demonstrate that our approach outperforms purely model-based and model-free approaches, as well as the approach of simply switching from a model-based to a model-free setting.",0
"Reinforcement Learning can be categorized into two main paradigms: model-free and model-based. Both paradigms possess unique strengths and limitations and have proven successful in various real-world domains. This study introduces a novel approach that aims to bridge the gap between these two paradigms by combining their advantages in a data-efficient and cost-effective manner. This is achieved by utilizing a probabilistic dynamics model as a prior for model-free optimization, allowing for the exploitation of the dynamics model's generality and structure while disregarding its inevitable inaccuracies through direct observation of the cost. Preliminary results indicate that this approach outperforms purely model-based and model-free methods, as well as the approach of switching from a model-based to a model-free setting.",1
"Consider the problem of approximating the optimal policy of a Markov decision process (MDP) by sampling state transitions. In contrast to existing reinforcement learning methods that are based on successive approximations to the nonlinear Bellman equation, we propose a Primal-Dual $\pi$ Learning method in light of the linear duality between the value and policy. The $\pi$ learning method is model-free and makes primal-dual updates to the policy and value vectors as new data are revealed. For infinite-horizon undiscounted Markov decision process with finite state space $S$ and finite action space $A$, the $\pi$ learning method finds an $\epsilon$-optimal policy using the following number of sample transitions $$ \tilde{O}( \frac{(\tau\cdot t^*_{mix})^2 |S| |A| }{\epsilon^2} ),$$ where $t^*_{mix}$ is an upper bound of mixing times across all policies and $\tau$ is a parameter characterizing the range of stationary distributions across policies. The $\pi$ learning method also applies to the computational problem of MDP where the transition probabilities and rewards are explicitly given as the input. In the case where each state transition can be sampled in $\tilde{O}(1)$ time, the $\pi$ learning method gives a sublinear-time algorithm for solving the averaged-reward MDP.",0
"Sampling state transitions to approximate the optimal policy of a Markov decision process (MDP) is a common problem. However, existing reinforcement learning methods that rely on nonlinear approximations to the Bellman equation have limitations. To address this, we propose a Primal-Dual $\pi$ Learning method that takes into account the linear duality between value and policy. This model-free approach makes updates to policy and value vectors as new data is collected. For an infinite-horizon, undiscounted MDP with finite state and action spaces, the $\pi$ learning method can find an $\epsilon$-optimal policy with a number of sample transitions proportional to $(\tau\cdot t^*_{mix})^2 |S| |A| /\epsilon^2$, where $t^*_{mix}$ is an upper bound of mixing times and $\tau$ characterizes the range of stationary distributions. The $\pi$ learning method is also applicable to MDPs with explicitly given transition probabilities and rewards. In the case where state transitions can be sampled in constant time, the $\pi$ learning method provides a sublinear-time algorithm for solving the averaged-reward MDP.",1
"This paper proposes a general method for improving the structure and quality of sequences generated by a recurrent neural network (RNN), while maintaining information originally learned from data, as well as sample diversity. An RNN is first pre-trained on data using maximum likelihood estimation (MLE), and the probability distribution over the next token in the sequence learned by this model is treated as a prior policy. Another RNN is then trained using reinforcement learning (RL) to generate higher-quality outputs that account for domain-specific incentives while retaining proximity to the prior policy of the MLE RNN. To formalize this objective, we derive novel off-policy RL methods for RNNs from KL-control. The effectiveness of the approach is demonstrated on two applications; 1) generating novel musical melodies, and 2) computational molecular generation. For both problems, we show that the proposed method improves the desired properties and structure of the generated sequences, while maintaining information learned from data.",0
"In this paper, a new technique is suggested to enhance the structure and quality of sequences created by a recurrent neural network (RNN) without losing the original data information and sample diversity. The proposed approach involves pre-training an RNN using maximum likelihood estimation (MLE) to learn the probability distribution for the next token in the sequence, which is then treated as a prior policy. Another RNN is trained using reinforcement learning (RL) to generate better outputs based on domain-specific incentives while staying close to the MLE RNN's prior policy. To achieve this objective, novel off-policy RL methods for RNNs are derived from KL-control. The effectiveness of this method is demonstrated on two applications - generating new musical melodies and computational molecular generation. The results show that the suggested approach enhances the desired properties and structure of the generated sequences while retaining information learned from data.",1
"Policy evaluation or value function or Q-function approximation is a key procedure in reinforcement learning (RL). It is a necessary component of policy iteration and can be used for variance reduction in policy gradient methods. Therefore its quality has a significant impact on most RL algorithms. Motivated by manifold regularized learning, we propose a novel kernelized policy evaluation method that takes advantage of the intrinsic geometry of the state space learned from data, in order to achieve better sample efficiency and higher accuracy in Q-function approximation. Applying the proposed method in the Least-Squares Policy Iteration (LSPI) framework, we observe superior performance compared to widely used parametric basis functions on two standard benchmarks in terms of policy quality.",0
"In reinforcement learning, policy evaluation, value function approximation, or Q-function approximation is a crucial process. It is essential for policy iteration and can also help reduce variance in policy gradient methods, making it a key factor in most RL algorithms. Our approach, inspired by manifold regularized learning, introduces a new kernelized policy evaluation technique that utilizes the intrinsic geometry of the state space learned from data. This enables us to achieve better sample efficiency and Q-function approximation accuracy. When implemented in the Least-Squares Policy Iteration (LSPI) framework, we observe better performance compared to commonly used parametric basis functions on two standard benchmarks in terms of policy quality.",1
"The vision for precision medicine is to use individual patient characteristics to inform a personalized treatment plan that leads to the best healthcare possible for each patient. Mobile technologies have an important role to play in this vision as they offer a means to monitor a patient's health status in real-time and subsequently to deliver interventions if, when, and in the dose that they are needed. Dynamic treatment regimes formalize individualized treatment plans as sequences of decision rules, one per stage of clinical intervention, that map current patient information to a recommended treatment. However, existing methods for estimating optimal dynamic treatment regimes are designed for a small number of fixed decision points occurring on a coarse time-scale. We propose a new reinforcement learning method for estimating an optimal treatment regime that is applicable to data collected using mobile technologies in an outpatient setting. The proposed method accommodates an indefinite time horizon and minute-by-minute decision making that are common in mobile health applications. We show the proposed estimators are consistent and asymptotically normal under mild conditions. The proposed methods are applied to estimate an optimal dynamic treatment regime for controlling blood glucose levels in patients with type 1 diabetes.",0
"Precision medicine aims to tailor healthcare to individual patients based on their unique characteristics. Mobile technologies can aid in achieving this goal by providing real-time monitoring and intervention capabilities. Dynamic treatment regimes involve personalized treatment plans that rely on decision rules to map patient information to recommended treatments at each stage of intervention. However, current methods for estimating optimal dynamic treatment regimes are limited to a few fixed decision points on a coarse time scale. To address this issue, we present a new reinforcement learning approach for estimating optimal treatment regimes that can be applied to outpatient data collected using mobile technologies. Our method accommodates an indefinite time horizon and minute-by-minute decision making, which are common in mobile health applications. We demonstrate that our approach is consistent and asymptotically normal under mild conditions and apply it to estimate an optimal dynamic treatment regime for controlling blood glucose levels in patients with type 1 diabetes.",1
"Policy-gradient approaches to reinforcement learning have two common and undesirable overhead procedures, namely warm-start training and sample variance reduction. In this paper, we describe a reinforcement learning method based on a softmax value function that requires neither of these procedures. Our method combines the advantages of policy-gradient methods with the efficiency and simplicity of maximum-likelihood approaches. We apply this new cold-start reinforcement learning method in training sequence generation models for structured output prediction problems. Empirical evidence validates this method on automatic summarization and image captioning tasks.",0
"Reinforcement learning through policy-gradient techniques often involves two burdensome processes - warm-start training and sample variance reduction. However, we present a new method that employs a softmax value function, eliminating the need for these procedures. Our approach merges the benefits of policy-gradient methods with the straightforwardness and effectiveness of maximum-likelihood techniques. We utilize this cold-start reinforcement learning technique to teach sequence generation models for structured output prediction issues. Through practical experimentation, we prove the efficacy of this method in tasks such as image captioning and automatic summarization.",1
"In this paper, a sparse Markov decision process (MDP) with novel causal sparse Tsallis entropy regularization is proposed.The proposed policy regularization induces a sparse and multi-modal optimal policy distribution of a sparse MDP. The full mathematical analysis of the proposed sparse MDP is provided.We first analyze the optimality condition of a sparse MDP. Then, we propose a sparse value iteration method which solves a sparse MDP and then prove the convergence and optimality of sparse value iteration using the Banach fixed point theorem. The proposed sparse MDP is compared to soft MDPs which utilize causal entropy regularization. We show that the performance error of a sparse MDP has a constant bound, while the error of a soft MDP increases logarithmically with respect to the number of actions, where this performance error is caused by the introduced regularization term. In experiments, we apply sparse MDPs to reinforcement learning problems. The proposed method outperforms existing methods in terms of the convergence speed and performance.",0
"This paper presents a new approach to sparse Markov decision processes (MDPs) by employing a unique form of causal sparse Tsallis entropy regularization. The regularization encourages a sparse and multi-modal optimal policy distribution in a sparse MDP. A comprehensive mathematical analysis of the proposed sparse MDP is provided, including an analysis of optimality conditions and a proposed sparse value iteration method. The convergence and optimality of sparse value iteration are proven using the Banach fixed point theorem. A comparison is made between the proposed sparse MDP and soft MDPs that use causal entropy regularization, showing that the performance error of the sparse MDP remains constant while the error of a soft MDP increases logarithmically with the number of actions. In reinforcement learning experiments, the proposed approach outperforms existing methods in terms of convergence speed and overall performance.",1
"The Epicurean Philosophy is commonly thought as simplistic and hedonistic. Here I discuss how this is a misconception and explore its link to Reinforcement Learning. Based on the letters of Epicurus, I construct an objective function for hedonism which turns out to be equivalent of the Reinforcement Learning objective function when omitting the discount factor. I then discuss how Plato and Aristotle 's views that can be also loosely linked to Reinforcement Learning, as well as their weaknesses in relationship to it. Finally, I emphasise the close affinity of the Epicurean views and the Bellman equation.",0
"It is often believed that the Epicurean Philosophy is uncomplicated and self-indulgent, but I aim to prove that this is a misconception and investigate its relationship to Reinforcement Learning. Through an analysis of Epicurus' letters, I establish an objective function for hedonism that is comparable to the Reinforcement Learning objective function without the discount factor. Furthermore, I explore the potential connections between Plato and Aristotle's beliefs and Reinforcement Learning, while highlighting their shortcomings. Lastly, I underscore the strong resemblance between the Epicurean philosophy and the Bellman equation.",1
"Autonomous learning has been a promising direction in control and robotics for more than a decade since data-driven learning allows to reduce the amount of engineering knowledge, which is otherwise required. However, autonomous reinforcement learning (RL) approaches typically require many interactions with the system to learn controllers, which is a practical limitation in real systems, such as robots, where many interactions can be impractical and time consuming. To address this problem, current learning approaches typically require task-specific knowledge in form of expert demonstrations, realistic simulators, pre-shaped policies, or specific knowledge about the underlying dynamics. In this article, we follow a different approach and speed up learning by extracting more information from data. In particular, we learn a probabilistic, non-parametric Gaussian process transition model of the system. By explicitly incorporating model uncertainty into long-term planning and controller learning our approach reduces the effects of model errors, a key problem in model-based learning. Compared to state-of-the art RL our model-based policy search method achieves an unprecedented speed of learning. We demonstrate its applicability to autonomous learning in real robot and control tasks.",0
"For over a decade, autonomous learning has been a promising field in control and robotics due to its ability to reduce the amount of engineering knowledge required through data-driven learning. However, autonomous reinforcement learning (RL) approaches can be impractical and time-consuming, as they require many interactions with the system to learn controllers. To tackle this issue, current learning approaches rely on task-specific knowledge, such as expert demonstrations or pre-shaped policies. In contrast, our article proposes a novel approach that extracts more information from data by learning a probabilistic, non-parametric Gaussian process transition model of the system. This method reduces the impact of model errors and achieves an unprecedented speed of learning compared to state-of-the-art RL. We demonstrate the applicability of our model-based policy search method to real robot and control tasks.",1
"Domain shift refers to the well known problem that a model trained in one source domain performs poorly when applied to a target domain with different statistics. {Domain Generalization} (DG) techniques attempt to alleviate this issue by producing models which by design generalize well to novel testing domains. We propose a novel {meta-learning} method for domain generalization. Rather than designing a specific model that is robust to domain shift as in most previous DG work, we propose a model agnostic training procedure for DG. Our algorithm simulates train/test domain shift during training by synthesizing virtual testing domains within each mini-batch. The meta-optimization objective requires that steps to improve training domain performance should also improve testing domain performance. This meta-learning procedure trains models with good generalization ability to novel domains. We evaluate our method and achieve state of the art results on a recent cross-domain image classification benchmark, as well demonstrating its potential on two classic reinforcement learning tasks.",0
"The problem of domain shift is widely known, wherein models trained on one domain perform poorly on another domain with different statistics. To address this issue, Domain Generalization (DG) techniques are employed to produce models that perform well on novel testing domains. Our proposed method for DG involves a novel approach to meta-learning, where we do not design a specific model that is robust to domain shift as in previous DG work. Instead, we employ a model-agnostic training procedure that simulates train/test domain shift during training by creating virtual testing domains. Our meta-optimization objective is to improve training domain performance and testing domain performance simultaneously, which results in models with good generalization ability to novel domains. We demonstrate the effectiveness of our method by achieving state of the art results on a cross-domain image classification benchmark and by showcasing its potential on two classic reinforcement learning tasks.",1
"Feature representations from pre-trained deep neural networks have been known to exhibit excellent generalization and utility across a variety of related tasks. Fine-tuning is by far the simplest and most widely used approach that seeks to exploit and adapt these feature representations to novel tasks with limited data. Despite the effectiveness of fine-tuning, itis often sub-optimal and requires very careful optimization to prevent severe over-fitting to small datasets. The problem of sub-optimality and over-fitting, is due in part to the large number of parameters used in a typical deep convolutional neural network. To address these problems, we propose a simple yet effective regularization method for fine-tuning pre-trained deep networks for the task of k-shot learning. To prevent overfitting, our key strategy is to cluster the model parameters while ensuring intra-cluster similarity and inter-cluster diversity of the parameters, effectively regularizing the dimensionality of the parameter search space. In particular, we identify groups of neurons within each layer of a deep network that shares similar activation patterns. When the network is to be fine-tuned for a classification task using only k examples, we propagate a single gradient to all of the neuron parameters that belong to the same group. The grouping of neurons is non-trivial as neuron activations depend on the distribution of the input data. To efficiently search for optimal groupings conditioned on the input data, we propose a reinforcement learning search strategy using recurrent networks to learn the optimal group assignments for each network layer. Experimental results show that our method can be easily applied to several popular convolutional neural networks and improve upon other state-of-the-art fine-tuning based k-shot learning strategies by more than10%",0
"Pre-trained deep neural networks are known for their exceptional ability to generalize and be useful in a range of related tasks. Although fine-tuning is the most commonly used method to adapt these networks to new tasks with limited data, it can still be sub-optimal and lead to over-fitting on small datasets due to the large number of parameters involved. To address this issue, we propose a regularization method for fine-tuning pre-trained networks for k-shot learning. Our method involves clustering the model parameters to ensure intra-cluster similarity and inter-cluster diversity, which effectively regularizes the dimensionality of the parameter search space. We identify groups of neurons within each layer of the network that share similar activation patterns and propagate a single gradient to all neuron parameters in the same group when fine-tuning for classification using only k examples. To efficiently search for optimal groupings, we use a reinforcement learning search strategy with recurrent networks to learn the optimal group assignments for each network layer. Our experimental results show that our method can be easily applied to popular convolutional neural networks and outperforms other state-of-the-art fine-tuning based k-shot learning strategies by more than 10%.",1
"Augmenting an agent's control with useful higher-level behaviors called options can greatly reduce the sample complexity of reinforcement learning, but manually designing options is infeasible in high-dimensional and abstract state spaces. While recent work has proposed several techniques for automated option discovery, they do not scale to multi-level hierarchies and to expressive representations such as deep networks. We present Discovery of Deep Options (DDO), a policy-gradient algorithm that discovers parametrized options from a set of demonstration trajectories, and can be used recursively to discover additional levels of the hierarchy. The scalability of our approach to multi-level hierarchies stems from the decoupling of low-level option discovery from high-level meta-control policy learning, facilitated by under-parametrization of the high level. We demonstrate that using the discovered options to augment the action space of Deep Q-Network agents can accelerate learning by guiding exploration in tasks where random actions are unlikely to reach valuable states. We show that DDO is effective in adding options that accelerate learning in 4 out of 5 Atari RAM environments chosen in our experiments. We also show that DDO can discover structure in robot-assisted surgical videos and kinematics that match expert annotation with 72% accuracy.",0
"Options, which are higher-level behaviors that enhance an agent's control, can significantly reduce the sample complexity of reinforcement learning. However, manually designing options is impractical in complex and abstract state spaces. Although there are various techniques for automated option discovery, they are not suitable for multi-level hierarchies and expressive representations like deep networks. To address this issue, we propose Discovery of Deep Options (DDO), a policy-gradient algorithm that discovers parametrized options from a set of demonstration trajectories, and can recursively explore additional levels of the hierarchy. Our approach's scalability to multi-level hierarchies is possible by separating low-level option discovery from high-level meta-control policy learning, aided by under-parametrization of the high level. Using the discovered options to expand the action space of Deep Q-Network agents can expedite learning by directing exploration towards valuable states in tasks where random actions are inadequate. In our experiments, we demonstrate the effectiveness of DDO in speeding up learning in four out of five Atari RAM environments. Furthermore, we demonstrate that DDO can identify structure in robot-assisted surgical videos and kinematics that match expert annotation with 72% accuracy.",1
"Deep reinforcement learning has shown promising results in learning control policies for complex sequential decision-making tasks. However, these neural network-based policies are known to be vulnerable to adversarial examples. This vulnerability poses a potentially serious threat to safety-critical systems such as autonomous vehicles. In this paper, we propose a defense mechanism to defend reinforcement learning agents from adversarial attacks by leveraging an action-conditioned frame prediction module. Our core idea is that the adversarial examples targeting at a neural network-based policy are not effective for the frame prediction model. By comparing the action distribution produced by a policy from processing the current observed frame to the action distribution produced by the same policy from processing the predicted frame from the action-conditioned frame prediction module, we can detect the presence of adversarial examples. Beyond detecting the presence of adversarial examples, our method allows the agent to continue performing the task using the predicted frame when the agent is under attack. We evaluate the performance of our algorithm using five games in Atari 2600. Our results demonstrate that the proposed defense mechanism achieves favorable performance against baseline algorithms in detecting adversarial examples and in earning rewards when the agents are under attack.",0
"Learning control policies for complex sequential decision-making tasks has shown promising results through deep reinforcement learning. However, these policies, based on neural networks, are known to be susceptible to adversarial examples, which poses a potential danger to safety-critical systems such as autonomous vehicles. To address this issue, we propose a defense mechanism that leverages an action-conditioned frame prediction module to protect reinforcement learning agents from adversarial attacks. Our approach involves comparing the action distribution produced by a policy from processing the current observed frame to the action distribution produced by the same policy from processing the predicted frame from the action-conditioned frame prediction module. This allows us to detect the presence of adversarial examples and enable the agent to continue performing the task using the predicted frame when under attack. We evaluate our algorithm's performance using five games in Atari 2600 and demonstrate favorable results in detecting adversarial examples and earning rewards when agents are under attack compared to baseline algorithms.",1
"Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep $Q$-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.",0
"The field of AI is on the verge of a revolution with deep reinforcement learning, which is taking us one step closer to developing independent systems with a better comprehension of the visual world. Previously insurmountable problems, like learning to play video games from pixels, are now being tackled through the use of deep learning, which enables reinforcement learning to scale. Robotics is another area where deep reinforcement learning is being applied, allowing robots to learn control policies directly from real-world camera inputs. This survey begins with an overview of reinforcement learning and then explores the main value-based and policy-based methods. It goes on to discuss key algorithms in deep reinforcement learning, such as the deep $Q$-network, trust region policy optimisation and asynchronous advantage actor-critic. Additionally, the survey emphasizes the visual understanding through deep neural networks and the benefits they bring. Finally, we conclude by highlighting current research areas in the field.",1
"Few-shot learning is challenging for learning algorithms that learn each task in isolation and from scratch. In contrast, meta-learning learns from many related tasks a meta-learner that can learn a new task more accurately and faster with fewer examples, where the choice of meta-learners is crucial. In this paper, we develop Meta-SGD, an SGD-like, easily trainable meta-learner that can initialize and adapt any differentiable learner in just one step, on both supervised learning and reinforcement learning. Compared to the popular meta-learner LSTM, Meta-SGD is conceptually simpler, easier to implement, and can be learned more efficiently. Compared to the latest meta-learner MAML, Meta-SGD has a much higher capacity by learning to learn not just the learner initialization, but also the learner update direction and learning rate, all in a single meta-learning process. Meta-SGD shows highly competitive performance for few-shot learning on regression, classification, and reinforcement learning.",0
"Learning algorithms that tackle few-shot learning face challenges when learning each task independently and from scratch. On the other hand, meta-learning can develop a meta-learner from numerous associated tasks. This meta-learner can then accurately and rapidly learn a new task with fewer examples. It is crucial to select the appropriate meta-learner. This paper introduces Meta-SGD, an easily trainable meta-learner that functions similarly to SGD. It can initialize and adapt any differentiable learner in a single step on both supervised and reinforcement learning. Meta-SGD is simpler and easier to implement than the popular meta-learner LSTM. Additionally, it can be learned more efficiently. Compared to the latest meta-learner MAML, Meta-SGD has a higher capacity. It can learn not only the learner initialization but also the learner update direction and learning rate in a single meta-learning process. Meta-SGD demonstrates highly competitive performance for few-shot learning on regression, classification, and reinforcement learning.",1
"A novel reinforcement learning benchmark, called Industrial Benchmark, is introduced. The Industrial Benchmark aims at being be realistic in the sense, that it includes a variety of aspects that we found to be vital in industrial applications. It is not designed to be an approximation of any real system, but to pose the same hardness and complexity.",0
"Introducing the Industrial Benchmark, a novel reinforcement learning benchmark that strives for realism by incorporating various essential aspects found in industrial applications. Its purpose is not to imitate any actual system but to offer the same level of difficulty and complexity.",1
"We designed a grid world task to study human planning and re-planning behavior in an unknown stochastic environment. In our grid world, participants were asked to travel from a random starting point to a random goal position while maximizing their reward. Because they were not familiar with the environment, they needed to learn its characteristics from experience to plan optimally. Later in the task, we randomly blocked the optimal path to investigate whether and how people adjust their original plans to find a detour. To this end, we developed and compared 12 different models. These models were different on how they learned and represented the environment and how they planned to catch the goal. The majority of our participants were able to plan optimally. We also showed that people were capable of revising their plans when an unexpected event occurred. The result from the model comparison showed that the model-based reinforcement learning approach provided the best account for the data and outperformed heuristics in explaining the behavioral data in the re-planning trials.",0
"To examine human planning and re-planning behavior in an unfamiliar stochastic environment, we created a grid world task. Participants were required to navigate from a random starting point to a random goal position while optimizing their reward. Since the environment was unknown, they had to learn its characteristics through experience to plan effectively. Later, we obstructed the optimal path randomly to investigate how individuals adapted their plans to find an alternate route. We constructed and compared 12 models that differed in their methods of learning, environment representation, and goal-catching planning. Most participants could plan optimally, and they could adjust their plans when faced with unforeseen circumstances. The model-based reinforcement learning approach proved to be the most effective in explaining the behavioral data during the re-planning trials when compared to heuristics.",1
The OpenAI Gym provides researchers and enthusiasts with simple to use environments for reinforcement learning. Even the simplest environment have a level of complexity that can obfuscate the inner workings of RL approaches and make debugging difficult. This whitepaper describes a Python framework that makes it very easy to create simple Markov-Decision-Process environments programmatically by specifying state transitions and rewards of deterministic and non-deterministic MDPs in a domain-specific language in Python. It then presents results and visualizations created with this MDP framework.,0
"The OpenAI Gym is a user-friendly tool for those interested in reinforcement learning, offering environments of varying complexity. However, even basic environments can pose challenges for understanding the intricacies of RL methods, making troubleshooting a daunting task. This document outlines a Python-based framework that simplifies the process of creating MDP environments by defining state transitions and rewards for deterministic and non-deterministic MDPs using a domain-specific language. The paper also showcases the outcomes and visuals generated using this MDP framework.",1
"Recurrent neural networks (RNNs) are a vital modeling technique that rely on internal states learned indirectly by optimization of a supervised, unsupervised, or reinforcement training loss. RNNs are used to model dynamic processes that are characterized by underlying latent states whose form is often unknown, precluding its analytic representation inside an RNN. In the Predictive-State Representation (PSR) literature, latent state processes are modeled by an internal state representation that directly models the distribution of future observations, and most recent work in this area has relied on explicitly representing and targeting sufficient statistics of this probability distribution. We seek to combine the advantages of RNNs and PSRs by augmenting existing state-of-the-art recurrent neural networks with Predictive-State Decoders (PSDs), which add supervision to the network's internal state representation to target predicting future observations. Predictive-State Decoders are simple to implement and easily incorporated into existing training pipelines via additional loss regularization. We demonstrate the effectiveness of PSDs with experimental results in three different domains: probabilistic filtering, Imitation Learning, and Reinforcement Learning. In each, our method improves statistical performance of state-of-the-art recurrent baselines and does so with fewer iterations and less data.",0
"Recurrent neural networks (RNNs) are a crucial modeling technique that depend on internal states acquired indirectly through the optimization of supervised, unsupervised, or reinforcement training loss. These networks are used to model dynamic processes where the underlying latent states are often unknown, making it impossible to represent them analytically within an RNN. In the Predictive-State Representation (PSR) literature, latent state processes are modeled using an internal state representation that models the future observation distribution directly. Recent work in this area has focused on representing and targeting sufficient statistics of this probability distribution. We propose augmenting existing state-of-the-art recurrent neural networks with Predictive-State Decoders (PSDs) to combine the advantages of RNNs and PSRs. PSDs add supervision to the network's internal state representation and target future observation prediction. PSDs can be easily integrated into existing training pipelines via additional loss regularization. We demonstrate the effectiveness of PSDs through experimental results in three different domains: probabilistic filtering, Imitation Learning, and Reinforcement Learning. Our method improves statistical performance compared to state-of-the-art recurrent baselines and achieves this with fewer iterations and less data.",1
"We consider the problem of active feature acquisition, where we sequentially select the subset of features in order to achieve the maximum prediction performance in the most cost-effective way. In this work, we formulate this active feature acquisition problem as a reinforcement learning problem, and provide a novel framework for jointly learning both the RL agent and the classifier (environment). We also introduce a more systematic way of encoding subsets of features that can properly handle innate challenge with missing entries in active feature acquisition problems, that uses the orderless LSTM-based set encoding mechanism that readily fits in the joint learning framework. We evaluate our model on a carefully designed synthetic dataset for the active feature acquisition as well as several real datasets such as electric health record (EHR) datasets, on which it outperforms all baselines in terms of prediction performance as well feature acquisition cost.",0
"The main focus of this study is the process of active feature acquisition, which involves the selection of a subset of features in a sequential manner to achieve the highest prediction performance while minimizing costs. In this research, we approach the problem of active feature acquisition by presenting a new reinforcement learning framework that enables us to learn both the classifier (environment) and the RL agent simultaneously. We also introduce a systematic method for encoding feature subsets that effectively addresses the issue of missing entries, using the orderless LSTM-based set encoding mechanism, which we integrate into the joint learning framework. To evaluate the performance of our model, we utilize a synthetic dataset and various real datasets, including electric health record datasets, and our results show that our proposed model outperforms all baseline approaches in terms of both prediction performance and feature acquisition costs.",1
"We present Shapechanger, a library for transfer reinforcement learning specifically designed for robotic tasks. We consider three types of knowledge transfer---from simulation to simulation, from simulation to real, and from real to real---and a wide range of tasks with continuous states and actions. Shapechanger is under active development and open-sourced at: https://github.com/seba-1511/shapechanger/.",0
"Shapechanger is a library that has been created for transfer reinforcement learning, keeping in mind the requirements of robotic tasks. Our focus is on three types of knowledge transfer, namely from simulation to simulation, from simulation to real, and from real to real. We have taken into consideration a variety of tasks that involve continuous states and actions. Shapechanger is being actively developed and is available as open-source software at: https://github.com/seba-1511/shapechanger/.",1
"Deep Reinforcement Learning has been able to achieve amazing successes in a variety of domains from video games to continuous control by trying to maximize the cumulative reward. However, most of these successes rely on algorithms that require a large amount of data to train in order to obtain results on par with human-level performance. This is not feasible if we are to deploy these systems on real world tasks and hence there has been an increased thrust in exploring data efficient algorithms. To this end, we propose the Shared Learning framework aimed at making $Q$-ensemble algorithms data-efficient. For achieving this, we look into some principles of transfer learning which aim to study the benefits of information exchange across tasks in reinforcement learning and adapt transfer to learning our value function estimates in a novel manner. In this paper, we consider the special case of transfer between the value function estimates in the $Q$-ensemble architecture of BootstrappedDQN. We further empirically demonstrate how our proposed framework can help in speeding up the learning process in $Q$-ensembles with minimum computational overhead on a suite of Atari 2600 Games.",0
"By maximizing the cumulative reward, Deep Reinforcement Learning has achieved remarkable successes in various domains, including video games and continuous control. However, these triumphs mostly rely on algorithms that necessitate a large quantity of data to train, which is unfeasible for implementing these systems in real-world tasks. Consequently, there has been a significant push to explore data-efficient algorithms. To address this challenge, we propose the Shared Learning framework, which aims to enhance the data efficiency of $Q$-ensemble algorithms. This is achieved by investigating transfer learning principles, which seek to study the advantages of information exchange across tasks in reinforcement learning, and adapting transfer to learn our value function estimates in a novel way. Our focus is on the transfer between the value function estimates in the $Q$-ensemble architecture of BootstrappedDQN. We also demonstrate empirically how our proposed framework can accelerate the learning process in $Q$-ensembles, with minimal computational overhead, across a range of Atari 2600 Games.",1
"We consider the problem of learning an unknown Markov Decision Process (MDP) that is weakly communicating in the infinite horizon setting. We propose a Thompson Sampling-based reinforcement learning algorithm with dynamic episodes (TSDE). At the beginning of each episode, the algorithm generates a sample from the posterior distribution over the unknown model parameters. It then follows the optimal stationary policy for the sampled model for the rest of the episode. The duration of each episode is dynamically determined by two stopping criteria. The first stopping criterion controls the growth rate of episode length. The second stopping criterion happens when the number of visits to any state-action pair is doubled. We establish $\tilde O(HS\sqrt{AT})$ bounds on expected regret under a Bayesian setting, where $S$ and $A$ are the sizes of the state and action spaces, $T$ is time, and $H$ is the bound of the span. This regret bound matches the best available bound for weakly communicating MDPs. Numerical results show it to perform better than existing algorithms for infinite horizon MDPs.",0
"In this article, we investigate how to learn an unknown Markov Decision Process (MDP) that is weakly communicating in an infinite horizon setting. We propose a reinforcement learning algorithm called TSDE, which is based on Thompson Sampling and uses dynamic episodes. At the beginning of each episode, the algorithm generates a sample from the posterior distribution over the unknown model parameters. It then follows the optimal stationary policy for the sampled model for the remainder of the episode. The duration of each episode is determined dynamically by two stopping criteria, the first of which controls the growth rate of episode length, and the second of which occurs when the number of visits to any state-action pair is doubled. We establish expected regret bounds of $\tilde O(HS\sqrt{AT})$ under a Bayesian setting, where $S$ and $A$ are the sizes of the state and action spaces, $T$ is time, and $H$ is the bound of the span. This regret bound matches the best available bound for weakly communicating MDPs, and our numerical results show that it outperforms existing algorithms for infinite horizon MDPs.",1
"Visual object tracking is a fundamental and time-critical vision task. Recent years have seen many shallow tracking methods based on real-time pixel-based correlation filters, as well as deep methods that have top performance but need a high-end GPU. In this paper, we learn to improve the speed of deep trackers without losing accuracy. Our fundamental insight is to take an adaptive approach, where easy frames are processed with cheap features (such as pixel values), while challenging frames are processed with invariant but expensive deep features. We formulate the adaptive tracking problem as a decision-making process, and learn an agent to decide whether to locate objects with high confidence on an early layer, or continue processing subsequent layers of a network. This significantly reduces the feed-forward cost for easy frames with distinct or slow-moving objects. We train the agent offline in a reinforcement learning fashion, and further demonstrate that learning all deep layers (so as to provide good features for adaptive tracking) can lead to near real-time average tracking speed of 23 fps on a single CPU while achieving state-of-the-art performance. Perhaps most tellingly, our approach provides a 100X speedup for almost 50% of the time, indicating the power of an adaptive approach.",0
"The task of visual object tracking is critical and essential for vision. Over the years, both shallow and deep tracking methods have been developed. Shallow methods use real-time pixel-based correlation filters, while deep methods require high-end GPUs and have top performance. This paper proposes an adaptive approach to improve the speed of deep trackers without compromising accuracy. Easy frames are processed with cheap features, while challenging frames use expensive deep features. The adaptive tracking problem is treated as a decision-making process, where an agent decides whether to locate objects with high confidence on an early layer or continue processing subsequent layers of a network. This approach significantly reduces the feed-forward cost for easy frames with distinct or slow-moving objects. The agent is trained offline in a reinforcement learning fashion, and when all deep layers are learned, near real-time average tracking speed of 23 fps on a single CPU is achieved while achieving state-of-the-art performance. The approach provides a 100X speedup for almost 50% of the time, demonstrating the effectiveness of an adaptive approach.",1
"We consider $d$-dimensional linear stochastic approximation algorithms (LSAs) with a constant step-size and the so called Polyak-Ruppert (PR) averaging of iterates. LSAs are widely applied in machine learning and reinforcement learning (RL), where the aim is to compute an appropriate $\theta_{*} \in \mathbb{R}^d$ (that is an optimum or a fixed point) using noisy data and $O(d)$ updates per iteration. In this paper, we are motivated by the problem (in RL) of policy evaluation from experience replay using the \emph{temporal difference} (TD) class of learning algorithms that are also LSAs. For LSAs with a constant step-size, and PR averaging, we provide bounds for the mean squared error (MSE) after $t$ iterations. We assume that data is \iid with finite variance (underlying distribution being $P$) and that the expected dynamics is Hurwitz. For a given LSA with PR averaging, and data distribution $P$ satisfying the said assumptions, we show that there exists a range of constant step-sizes such that its MSE decays as $O(\frac{1}{t})$.   We examine the conditions under which a constant step-size can be chosen uniformly for a class of data distributions $\mathcal{P}$, and show that not all data distributions `admit' such a uniform constant step-size. We also suggest a heuristic step-size tuning algorithm to choose a constant step-size of a given LSA for a given data distribution $P$. We compare our results with related work and also discuss the implication of our results in the context of TD algorithms that are LSAs.",0
"This paper focuses on $d$-dimensional linear stochastic approximation algorithms (LSAs) with a constant step-size and Polyak-Ruppert (PR) averaging of iterates. These algorithms are widely used in machine learning and reinforcement learning (RL) to compute an appropriate $\theta_{*} \in \mathbb{R}^d$ using noisy data and $O(d)$ updates per iteration. Specifically, we investigate the problem of policy evaluation from experience replay using the temporal difference (TD) class of learning algorithms, which are also LSAs. Our goal is to provide bounds for the mean squared error (MSE) after $t$ iterations for LSAs with a constant step-size and PR averaging, assuming that the data is independently and identically distributed (iid) with finite variance and the expected dynamics are Hurwitz, and we show that there exists a range of constant step-sizes such that the MSE decays as $O(\frac{1}{t})$. We also examine the conditions under which a constant step-size can be chosen uniformly for a class of data distributions $\mathcal{P}$ and suggest a heuristic step-size tuning algorithm. Our results are compared with related work and discussed in the context of TD algorithms that are LSAs.",1
"We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.",0
"We investigate the potential of Evolution Strategies (ES), which belong to a category of black box optimization algorithms, as a substitute for Q-learning and Policy Gradients, which are popular MDP-based RL techniques. Our experiments on MuJoCo and Atari demonstrate that ES is a feasible problem-solving approach that can scale efficiently with the number of CPUs at hand. Our ES implementation employs a unique communication method using shared random numbers, enabling us to exchange only scalar values, making it possible to parallelize up to one thousand workers. This allows us to solve 3D humanoid walking in just 10 minutes and achieve competitive results on most Atari games with just one hour of training. Furthermore, we highlight several benefits of ES as a black box optimization technique, such as its ability to handle action frequency and delayed rewards, tolerate extended horizons, and avoid requiring temporal discounting or value function approximation.",1
"Much combinatorial optimisation problems constitute a non-polynomial (NP) hard optimisation problem, i.e., they can not be solved in polynomial time. One such problem is finding the shortest route between two nodes on a graph. Meta-heuristic algorithms such as $A^{*}$ along with mixed-integer programming (MIP) methods are often employed for these problems. Our work demonstrates that it is possible to approximate solutions generated by a meta-heuristic algorithm using a deep recurrent neural network. We compare different methodologies based on reinforcement learning (RL) and recurrent neural networks (RNN) to gauge their respective quality of approximation. We show the viability of recurrent neural network solutions on a graph that has over 300 nodes and argue that a sequence-to-sequence network rather than other recurrent networks has improved approximation quality. Additionally, we argue that homotopy continuation -- that increases chances of hitting an extremum -- further improves the estimate generated by a vanilla RNN.",0
"Many combinatorial optimization problems are classified as non-polynomial (NP) hard, meaning they cannot be solved in polynomial time. One example of such a problem is determining the shortest route between two nodes on a graph. To tackle these issues, meta-heuristic algorithms like $A^{*}$ and mixed-integer programming (MIP) methods are often utilized. Our research exhibits that approximations produced by a deep recurrent neural network can emulate the solutions generated by a meta-heuristic algorithm. We assess various techniques based on reinforcement learning (RL) and recurrent neural networks (RNN) to evaluate their respective approximation quality. Our study shows that recurrent neural network solutions are feasible on graphs with over 300 nodes, and a sequence-to-sequence network improves approximation quality compared to other recurrent networks. We further argue that homotopy continuation, which enhances the likelihood of hitting an extremum, enhances the estimate produced by a vanilla RNN.",1
"We propose a dynamic computational time model to accelerate the average processing time for recurrent visual attention (RAM). Rather than attention with a fixed number of steps for each input image, the model learns to decide when to stop on the fly. To achieve this, we add an additional continue/stop action per time step to RAM and use reinforcement learning to learn both the optimal attention policy and stopping policy. The modification is simple but could dramatically save the average computational time while keeping the same recognition performance as RAM. Experimental results on CUB-200-2011 and Stanford Cars dataset demonstrate the dynamic computational model can work effectively for fine-grained image recognition.The source code of this paper can be obtained from https://github.com/baidu-research/DT-RAM",0
"Our proposal is a dynamic computational time model that aims to expedite the processing time for recurrent visual attention (RAM). Instead of having a fixed number of steps for each input image, our model can determine when to stop on the go. We have modified RAM by incorporating an extra continue/stop action for each time step and have used reinforcement learning to learn both the optimal attention policy and stopping policy. This modification is uncomplicated yet can significantly reduce the average computational time while maintaining the same recognition performance as RAM. Our experiments on CUB-200-2011 and Stanford Cars dataset reveal that the dynamic computational model is effective for fine-grained image recognition. To access the source code of this paper, please visit https://github.com/baidu-research/DT-RAM.",1
"We present LADDER, the first deep reinforcement learning agent that can successfully learn control policies for large-scale real-world problems directly from raw inputs composed of high-level semantic information. The agent is based on an asynchronous stochastic variant of DQN (Deep Q Network) named DASQN. The inputs of the agent are plain-text descriptions of states of a game of incomplete information, i.e. real-time large scale online auctions, and the rewards are auction profits of very large scale. We apply the agent to an essential portion of JD's online RTB (real-time bidding) advertising business and find that it easily beats the former state-of-the-art bidding policy that had been carefully engineered and calibrated by human experts: during JD.com's June 18th anniversary sale, the agent increased the company's ads revenue from the portion by more than 50%, while the advertisers' ROI (return on investment) also improved significantly.",0
"Our presentation introduces LADDER, an innovative deep reinforcement learning agent that can learn control policies for vast real-world problems directly from high-level semantic information. The agent, which employs an asynchronous stochastic variation of DQN (Deep Q Network) called DASQN, utilizes plain-text descriptions of states in incomplete information games, such as large-scale real-time auctions, as inputs, while the auction profits serve as rewards. We implemented the agent in JD's online RTB (real-time bidding) advertising business, and it outperformed the previous state-of-the-art bidding policy that was carefully crafted and calibrated by human experts. During JD.com's June 18th anniversary sale, the agent increased the company's ads revenue from the portion by over 50%, while also improving advertisers' ROI (return on investment) significantly.",1
"Generating molecules with desired chemical properties is important for drug discovery. The use of generative neural networks is promising for this task. However, from visual inspection, it often appears that generated samples lack diversity. In this paper, we quantify this internal chemical diversity, and we raise the following challenge: can a nontrivial AI model reproduce natural chemical diversity for desired molecules? To illustrate this question, we consider two generative models: a Reinforcement Learning model and the recently introduced ORGAN. Both fail at this challenge. We hope this challenge will stimulate research in this direction.",0
"The discovery of drugs requires the production of molecules that possess specific chemical characteristics. One promising approach for achieving this is through the use of generative neural networks. However, these networks often produce samples that are visually similar and lack diversity. This paper aims to measure the internal chemical diversity of generated samples and presents a challenge: can an advanced AI model replicate natural chemical diversity in desired molecules? To demonstrate this challenge, we examine two generative models - a Reinforcement Learning model and the newly introduced ORGAN - both of which fail to meet this challenge. This study hopes to inspire further research in this area.",1
"We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a ""surrogate"" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.",0
"Our team suggests a fresh category of policy gradient approaches for reinforcement learning that involve two steps: gathering data through interaction with the environment and enhancing a ""surrogate"" objective function by using stochastic gradient ascent. While standard policy gradient approaches conduct one gradient update for each data sample, our innovative objective function allows for multiple epochs of minibatch updates. We have named these methods proximal policy optimization (PPO), and they offer advantages similar to trust region policy optimization (TRPO) but are easier to implement, more versatile, and more efficient in terms of sample complexity (based on empirical data). Our experiments included benchmark tasks such as simulated robotic locomotion and Atari game playing, and our results show that PPO outperforms other online policy gradient methods by maintaining a favorable trade-off between sample complexity, simplicity, and wall-time.",1
"As a new way of training generative models, Generative Adversarial Nets (GAN) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real-valued data. However, it has limitations when the goal is for generating sequences of discrete tokens. A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model. Also, the discriminative model can only assess a complete sequence, while for a partially generated sequence, it is non-trivial to balance its current score and the future one once the entire sequence has been generated. In this paper, we propose a sequence generation framework, called SeqGAN, to solve the problems. Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update. The RL reward signal comes from the GAN discriminator judged on a complete sequence, and is passed back to the intermediate state-action steps using Monte Carlo search. Extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over strong baselines.",0
"The Generative Adversarial Nets (GAN) approach, which employs a discriminative model to guide the generative model, has been successful in generating real-valued data. However, it faces limitations in generating sequences of discrete tokens due to difficulties in passing the gradient update from the discriminative model to the generative model, and the inability of the discriminative model to assess a partially generated sequence. To overcome these challenges, we propose SeqGAN, a sequence generation framework that models the data generator as a stochastic policy in reinforcement learning (RL). By directly performing gradient policy update, SeqGAN bypasses the generator differentiation problem. The RL reward signal comes from the GAN discriminator, which assesses a complete sequence and is passed back to intermediate state-action steps using Monte Carlo search. Our extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over strong baselines.",1
"In the wake of the vast population of smart device users worldwide, mobile health (mHealth) technologies are hopeful to generate positive and wide influence on people's health. They are able to provide flexible, affordable and portable health guides to device users. Current online decision-making methods for mHealth assume that the users are completely heterogeneous. They share no information among users and learn a separate policy for each user. However, data for each user is very limited in size to support the separate online learning, leading to unstable policies that contain lots of variances. Besides, we find the truth that a user may be similar with some, but not all, users, and connected users tend to have similar behaviors. In this paper, we propose a network cohesion constrained (actor-critic) Reinforcement Learning (RL) method for mHealth. The goal is to explore how to share information among similar users to better convert the limited user information into sharper learned policies. To the best of our knowledge, this is the first online actor-critic RL for mHealth and first network cohesion constrained (actor-critic) RL method in all applications. The network cohesion is important to derive effective policies. We come up with a novel method to learn the network by using the warm start trajectory, which directly reflects the users' property. The optimization of our model is difficult and very different from the general supervised learning due to the indirect observation of values. As a contribution, we propose two algorithms for the proposed online RLs. Apart from mHealth, the proposed methods can be easily applied or adapted to other health-related tasks. Extensive experiment results on the HeartSteps dataset demonstrates that in a variety of parameter settings, the proposed two methods obtain obvious improvements over the state-of-the-art methods.",0
"The widespread use of smart devices worldwide has created an opportunity for mobile health (mHealth) technologies to positively impact people's health by providing flexible, affordable, and portable health guidance. However, current online decision-making methods assume that users are completely different and do not share information. This leads to unstable policies with significant variations due to limited user data. Additionally, users may have similarities, and connected users tend to have similar behaviors. Therefore, we propose a network cohesion constrained (actor-critic) Reinforcement Learning (RL) method for mHealth that allows for the sharing of information among similar users to improve policy learning. We present two online RL algorithms for mHealth that can also be applied to other health-related tasks. Our experiments on the HeartSteps dataset demonstrate significant improvements compared to existing methods in various parameter settings.",1
"This works handles the inverse reinforcement learning problem in high-dimensional state spaces, which relies on an efficient solution of model-based high-dimensional reinforcement learning problems. To solve the computationally expensive reinforcement learning problems, we propose a function approximation method to ensure that the Bellman Optimality Equation always holds, and then estimate a function based on the observed human actions for inverse reinforcement learning problems. The time complexity of the proposed method is linearly proportional to the cardinality of the action set, thus it can handle high-dimensional even continuous state spaces efficiently. We test the proposed method in a simulated environment to show its accuracy, and three clinical tasks to show how it can be used to evaluate a doctor's proficiency.",0
"The objective of this study is to address the issue of inverse reinforcement learning in high-dimensional state spaces, which necessitates an effective solution to model-based reinforcement learning problems. In order to overcome the computational burden of reinforcement learning problems, we suggest a function approximation technique that guarantees the validity of the Bellman Optimality Equation. Subsequently, we estimate a function based on human actions to solve inverse reinforcement learning problems. The proposed method has a time complexity that is linearly proportional to the action set's cardinality, enabling it to handle even continuous state spaces efficiently. We verify the accuracy of our technique in a simulated environment and demonstrate its utility in evaluating a physician's competence in three clinical tasks.",1
"We introduce a general framework for visual forecasting, which directly imitates visual sequences without additional supervision. As a result, our model can be applied at several semantic levels and does not require any domain knowledge or handcrafted features. We achieve this by formulating visual forecasting as an inverse reinforcement learning (IRL) problem, and directly imitate the dynamics in natural sequences from their raw pixel values. The key challenge is the high-dimensional and continuous state-action space that prohibits the application of previous IRL algorithms. We address this computational bottleneck by extending recent progress in model-free imitation with trainable deep feature representations, which (1) bypasses the exhaustive state-action pair visits in dynamic programming by using a dual formulation and (2) avoids explicit state sampling at gradient computation using a deep feature reparametrization. This allows us to apply IRL at scale and directly imitate the dynamics in high-dimensional continuous visual sequences from the raw pixel values. We evaluate our approach at three different level-of-abstraction, from low level pixels to higher level semantics: future frame generation, action anticipation, visual story forecasting. At all levels, our approach outperforms existing methods.",0
"Our approach presents a general framework for visual forecasting that imitates visual sequences without additional supervision. It can be applied at various semantic levels without requiring any domain knowledge or handcrafted features. We achieve this by framing visual forecasting as an inverse reinforcement learning problem, where we directly imitate the dynamics in natural sequences from their raw pixel values. However, the challenge lies in the high-dimensional and continuous state-action space, which makes it difficult to apply previous IRL algorithms. To overcome this, we employ recent advancements in model-free imitation with trainable deep feature representations. This extension bypasses the exhaustive state-action pair visits in dynamic programming by using a dual formulation and avoids explicit state sampling at gradient computation using a deep feature reparametrization. With this, we can apply IRL at scale and directly imitate the dynamics in high-dimensional continuous visual sequences from raw pixel values. We evaluate our approach at three different levels of abstraction: future frame generation, action anticipation, and visual story forecasting. Our approach outperforms existing methods at all levels.",1
"In this work, we propose to apply trust region optimization to deep reinforcement learning using a recently proposed Kronecker-factored approximation to the curvature. We extend the framework of natural policy gradient and propose to optimize both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region; hence we call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge, this is the first scalable trust region natural gradient method for actor-critic methods. It is also a method that learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs. We tested our approach across discrete domains in Atari games as well as continuous domains in the MuJoCo environment. With the proposed methods, we are able to achieve higher rewards and a 2- to 3-fold improvement in sample efficiency on average, compared to previous state-of-the-art on-policy actor-critic methods. Code is available at https://github.com/openai/baselines",0
"The objective of this study is to introduce a method of utilizing trust region optimization in deep reinforcement learning through the use of Kronecker-factored approximation to the curvature. We have expanded upon the natural policy gradient framework and put forward a proposal to optimize both the actor and critic components using Kronecker-factored approximate curvature (K-FAC) with trust region. Our method, Actor Critic using Kronecker-Factored Trust Region (ACKTR), is the first scalable trust region natural gradient method for actor-critic methods that learns non-trivial tasks in both continuous and discrete control policies directly from raw pixel inputs. We conducted tests on the MuJoCo environment and discrete domains in Atari games, and compared to previous state-of-the-art on-policy actor-critic methods, our method achieved higher rewards and a 2- to 3-fold improvement in sample efficiency on average. The code for our approach is available at https://github.com/openai/baselines.",1
"When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that state of art deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.",0
"Humans are capable of deducing various physical attributes of new objects by interacting with them in a purposeful manner. This process is akin to a scientist conducting experiments to uncover hidden information. Although recent advancements in artificial intelligence have enabled machines to surpass human capabilities in domains like Go, Atari, language processing, and control problems, it remains uncertain whether these machines can match the scientific intuition of even a young child. To address this, we present a set of basic tasks that require agents to estimate properties like mass and cohesion of objects in an interactive simulated environment where they can manipulate objects and observe the outcomes. We discovered that advanced deep reinforcement learning techniques can enable agents to perform requisite experiments to uncover such hidden properties. By systematically varying the difficulty of the problem and the cost incurred by the agent for conducting experiments, we found that agents adopt different strategies that balance information gathering costs against the cost of committing errors in different scenarios.",1
"This paper introduces SC2LE (StarCraft II Learning Environment), a reinforcement learning environment based on the StarCraft II game. This domain poses a new grand challenge for reinforcement learning, representing a more difficult class of problems than considered in most prior work. It is a multi-agent problem with multiple players interacting; there is imperfect information due to a partially observed map; it has a large action space involving the selection and control of hundreds of units; it has a large state space that must be observed solely from raw input feature planes; and it has delayed credit assignment requiring long-term strategies over thousands of steps. We describe the observation, action, and reward specification for the StarCraft II domain and provide an open source Python-based interface for communicating with the game engine. In addition to the main game maps, we provide a suite of mini-games focusing on different elements of StarCraft II gameplay. For the main game maps, we also provide an accompanying dataset of game replay data from human expert players. We give initial baseline results for neural networks trained from this data to predict game outcomes and player actions. Finally, we present initial baseline results for canonical deep reinforcement learning agents applied to the StarCraft II domain. On the mini-games, these agents learn to achieve a level of play that is comparable to a novice player. However, when trained on the main game, these agents are unable to make significant progress. Thus, SC2LE offers a new and challenging environment for exploring deep reinforcement learning algorithms and architectures.",0
"The SC2LE (StarCraft II Learning Environment) is a reinforcement learning platform that is based on the game StarCraft II. This platform presents a challenging problem for reinforcement learning, as it involves multiple agents interacting with each other, a partially observed map, a large action space, and a large state space. Additionally, long-term strategies are required for delayed credit assignment over thousands of steps. The observation, action, and reward specifications for this domain are explained, and a Python-based interface for communicating with the game engine is provided. Furthermore, a set of mini-games is presented to focus on different aspects of StarCraft II gameplay, along with a dataset of game replay data from expert players. Baseline results for neural networks trained with this data to predict game outcomes and player actions are given, as well as initial baseline results for deep reinforcement learning agents applied to the StarCraft II domain. These agents are able to achieve a novice player level of play in the mini-games, but are unable to make significant progress when trained on the main game. Therefore, SC2LE offers a challenging and unique environment to explore deep reinforcement learning algorithms and architectures.",1
"A crucial capability of real-world intelligent agents is their ability to plan a sequence of actions to achieve their goals in the visual world. In this work, we address the problem of visual semantic planning: the task of predicting a sequence of actions from visual observations that transform a dynamic environment from an initial state to a goal state. Doing so entails knowledge about objects and their affordances, as well as actions and their preconditions and effects. We propose learning these through interacting with a visual and dynamic environment. Our proposed solution involves bootstrapping reinforcement learning with imitation learning. To ensure cross task generalization, we develop a deep predictive model based on successor representations. Our experimental results show near optimal results across a wide range of tasks in the challenging THOR environment.",0
"The ability to plan a sequence of actions to achieve goals is a crucial skill for intelligent agents in the real world. This study focuses on the problem of visual semantic planning, which involves predicting a series of actions based on visual observations that transform an environment from its initial state to its goal state. This requires knowledge of objects and their capabilities, as well as actions and their requirements and effects. We propose to learn these skills by interacting with a dynamic visual environment, using a combination of reinforcement and imitation learning. To ensure that our solution can be applied to various tasks, we use a deep predictive model based on successor representations. Our experimental results demonstrate near-optimal performance across a broad range of tasks in the challenging THOR environment.",1
"We show that the equations of reinforcement learning and light transport simulation are related integral equations. Based on this correspondence, a scheme to learn importance while sampling path space is derived. The new approach is demonstrated in a consistent light transport simulation algorithm that uses reinforcement learning to progressively learn where light comes from. As using this information for importance sampling includes information about visibility, too, the number of light transport paths with zero contribution is dramatically reduced, resulting in much less noisy images within a fixed time budget.",0
"In our study, we establish a connection between the integral equations of reinforcement learning and light transport simulation. Using this association, we develop a method to acquire knowledge about the importance of sampling path space. We apply this innovative approach in a coherent light transport simulation algorithm, where reinforcement learning is utilized to gradually comprehend the source of light. By incorporating visibility information, the technique significantly reduces the number of light transport paths with no contribution, leading to clearer images in a shorter time frame.",1
"Due to the popularity of smartphones and wearable devices nowadays, mobile health (mHealth) technologies are promising to bring positive and wide impacts on people's health. State-of-the-art decision-making methods for mHealth rely on some ideal assumptions. Those methods either assume that the users are completely homogenous or completely heterogeneous. However, in reality, a user might be similar with some, but not all, users. In this paper, we propose a novel group-driven reinforcement learning method for the mHealth. We aim to understand how to share information among similar users to better convert the limited user information into sharper learned RL policies. Specifically, we employ the K-means clustering method to group users based on their trajectory information similarity and learn a shared RL policy for each group. Extensive experiment results have shown that our method can achieve clear gains over the state-of-the-art RL methods for mHealth.",0
"Mobile health technologies, or mHealth, have the potential to positively impact people's health due to the widespread use of smartphones and wearable devices. However, current decision-making methods for mHealth rely on assumptions that users are either completely alike or completely different. In reality, users may share some similarities but not all. To address this issue, we propose a new approach called group-driven reinforcement learning for mHealth. Our goal is to determine how to share information among similar users to improve the quality of learned reinforcement learning policies. We use the K-means clustering method to group users based on their similarity in trajectory information and develop a shared reinforcement learning policy for each group. Our extensive experimental data demonstrates that our approach outperforms previous reinforcement learning methods for mHealth.",1
"This paper introduces a new method for inverse reinforcement learning in large-scale and high-dimensional state spaces. To avoid solving the computationally expensive reinforcement learning problems in reward learning, we propose a function approximation method to ensure that the Bellman Optimality Equation always holds, and then estimate a function to maximize the likelihood of the observed motion. The time complexity of the proposed method is linearly proportional to the cardinality of the action set, thus it can handle large state spaces efficiently. We test the proposed method in a simulated environment, and show that it is more accurate than existing methods and significantly better in scalability. We also show that the proposed method can extend many existing methods to high-dimensional state spaces. We then apply the method to evaluating the effect of rehabilitative stimulations on patients with spinal cord injuries based on the observed patient motions.",0
"A novel technique for inverse reinforcement learning in state spaces that are both large-scale and high-dimensional is presented in this paper. Instead of solving reinforcement learning problems that are computationally expensive to derive rewards, we suggest utilizing a function approximation method to ensure the Bellman Optimality Equation is maintained. Then, a function is estimated to increase the probability of observed motion. Since the time complexity of the proposed approach is directly proportional to the action set's cardinality, it is highly efficient in handling large state spaces. Our simulated environment tests demonstrate that the proposed method is more precise than existing methods and has superior scalability. We also demonstrate that the proposed method can expand many existing techniques to high-dimensional state spaces. Finally, we utilize the method to evaluate the impact of rehabilitative stimulations on patients with spinal cord injuries based on observed patient motions.",1
"Despite the substantial progress in recent years, the image captioning techniques are still far from being perfect.Sentences produced by existing methods, e.g. those based on RNNs, are often overly rigid and lacking in variability. This issue is related to a learning principle widely used in practice, that is, to maximize the likelihood of training samples. This principle encourages high resemblance to the ""ground-truth"" captions while suppressing other reasonable descriptions. Conventional evaluation metrics, e.g. BLEU and METEOR, also favor such restrictive methods. In this paper, we explore an alternative approach, with the aim to improve the naturalness and diversity -- two essential properties of human expression. Specifically, we propose a new framework based on Conditional Generative Adversarial Networks (CGAN), which jointly learns a generator to produce descriptions conditioned on images and an evaluator to assess how well a description fits the visual content. It is noteworthy that training a sequence generator is nontrivial. We overcome the difficulty by Policy Gradient, a strategy stemming from Reinforcement Learning, which allows the generator to receive early feedback along the way. We tested our method on two large datasets, where it performed competitively against real people in our user study and outperformed other methods on various tasks.",0
"Despite recent advancements, image captioning techniques still have a long way to go before reaching perfection. Many existing methods, which rely on RNNs, tend to produce inflexible and repetitive sentences. This issue stems from the widely used learning principle that prioritizes maximizing the likelihood of training samples, leading to a preference for ""ground-truth"" captions over other valid descriptions. Additionally, conventional evaluation metrics such as BLEU and METEOR reinforce this tendency towards restrictive methods. To address these limitations, our paper proposes an alternative approach that focuses on enhancing naturalness and diversity in human expression. This is achieved through a novel framework based on Conditional Generative Adversarial Networks (CGAN), which simultaneously learns a generator to create descriptions based on images and an evaluator to assess the relevance of the description to the visual content. Given the complexity of training a sequence generator, we implemented Policy Gradient, a technique derived from Reinforcement Learning, to provide early feedback to the generator. Our method was tested on two large datasets and was found to perform competitively against human-generated captions in a user study. Furthermore, our approach outperformed other methods in various tasks.",1
"Policy gradient methods in reinforcement learning have become increasingly prevalent for state-of-the-art performance in continuous control tasks. Novel methods typically benchmark against a few key algorithms such as deep deterministic policy gradients and trust region policy optimization. As such, it is important to present and use consistent baselines experiments. However, this can be difficult due to general variance in the algorithms, hyper-parameter tuning, and environment stochasticity. We investigate and discuss: the significance of hyper-parameters in policy gradients for continuous control, general variance in the algorithms, and reproducibility of reported results. We provide guidelines on reporting novel results as comparisons against baseline methods such that future researchers can make informed decisions when investigating novel methods.",0
"Reinforcement learning has seen an increase in the use of policy gradient methods for achieving state-of-the-art performance in continuous control tasks. When developing new methods, it is important to compare them against established algorithms such as deep deterministic policy gradients and trust region policy optimization. Consistent baseline experiments are necessary to ensure fair comparisons, but this can be challenging due to algorithm variability, hyper-parameter tuning, and environment stochasticity. In this study, we examine the impact of hyper-parameters on policy gradients for continuous control, the variability in algorithms, and the reproducibility of reported results. We offer guidance on reporting novel results by comparing them to established methods, which will assist future researchers in making informed decisions when developing new methods.",1
"Face hallucination is a domain-specific super-resolution problem with the goal to generate high-resolution (HR) faces from low-resolution (LR) input images. In contrast to existing methods that often learn a single patch-to-patch mapping from LR to HR images and are regardless of the contextual interdependency between patches, we propose a novel Attention-aware Face Hallucination (Attention-FH) framework which resorts to deep reinforcement learning for sequentially discovering attended patches and then performing the facial part enhancement by fully exploiting the global interdependency of the image. Specifically, in each time step, the recurrent policy network is proposed to dynamically specify a new attended region by incorporating what happened in the past. The state (i.e., face hallucination result for the whole image) can thus be exploited and updated by the local enhancement network on the selected region. The Attention-FH approach jointly learns the recurrent policy network and local enhancement network through maximizing the long-term reward that reflects the hallucination performance over the whole image. Therefore, our proposed Attention-FH is capable of adaptively personalizing an optimal searching path for each face image according to its own characteristic. Extensive experiments show our approach significantly surpasses the state-of-the-arts on in-the-wild faces with large pose and illumination variations.",0
"The aim of face hallucination is to generate high-resolution (HR) faces from low-resolution (LR) images, and this is a domain-specific super-resolution problem. Existing methods tend to learn a patch-to-patch mapping that does not consider the contextual interdependency between patches. In contrast, our approach, the Attention-aware Face Hallucination (Attention-FH) framework, employs deep reinforcement learning to discover attended patches and enhance facial parts while fully exploiting the global interdependency of the image. Our method uses a recurrent policy network to specify a new attended region dynamically and update the state based on the selected region. The Attention-FH approach jointly learns the recurrent policy network and local enhancement network by maximizing the long-term reward that reflects the hallucination performance over the whole image. This approach personalizes an optimal searching path for each face image, adapting to its unique characteristics. Our experiments demonstrate that the Attention-FH approach significantly outperforms the state-of-the-art methods on in-the-wild faces with large pose and illumination variations.",1
"In this paper we study how to learn stochastic, multimodal transition dynamics in reinforcement learning (RL) tasks. We focus on evaluating transition function estimation, while we defer planning over this model to future work. Stochasticity is a fundamental property of many task environments. However, discriminative function approximators have difficulty estimating multimodal stochasticity. In contrast, deep generative models do capture complex high-dimensional outcome distributions. First we discuss why, amongst such models, conditional variational inference (VI) is theoretically most appealing for model-based RL. Subsequently, we compare different VI models on their ability to learn complex stochasticity on simulated functions, as well as on a typical RL gridworld with multimodal dynamics. Results show VI successfully predicts multimodal outcomes, but also robustly ignores these for deterministic parts of the transition dynamics. In summary, we show a robust method to learn multimodal transitions using function approximation, which is a key preliminary for model-based RL in stochastic domains.",0
"The aim of this research is to explore the acquisition of stochastic, multimodal transition dynamics in reinforcement learning (RL) tasks. The focus of the study is on the evaluation of transition function estimation, while planning is reserved for future work. Many task environments possess stochasticity, but discriminative function approximators often struggle to estimate multimodal stochasticity. In contrast, deep generative models can capture complex high-dimensional outcome distributions. The paper discusses why conditional variational inference (VI) is the most appealing model for model-based RL amongst such models. The study compares different VI models to determine their ability to learn complex stochasticity on simulated functions and a typical RL gridworld with multimodal dynamics. The results indicate that VI successfully predicts multimodal outcomes while robustly ignoring these for deterministic parts of the transition dynamics. The study shows a robust method for learning multimodal transitions using function approximation, which is a key preliminary for model-based RL in stochastic domains.",1
"We tackle the problem of learning robotic sensorimotor control policies that can generalize to visually diverse and unseen environments. Achieving broad generalization typically requires large datasets, which are difficult to obtain for task-specific interactive processes such as reinforcement learning or learning from demonstration. However, much of the visual diversity in the world can be captured through passively collected datasets of images or videos. In our method, which we refer to as GPLAC (Generalized Policy Learning with Attentional Classifier), we use both interaction data and weakly labeled image data to augment the generalization capacity of sensorimotor policies. Our method combines multitask learning on action selection and an auxiliary binary classification objective, together with a convolutional neural network architecture that uses an attentional mechanism to avoid distractors. We show that pairing interaction data from just a single environment with a diverse dataset of weakly labeled data results in greatly improved generalization to unseen environments, and show that this generalization depends on both the auxiliary objective and the attentional architecture that we propose. We demonstrate our results in both simulation and on a real robotic manipulator, and demonstrate substantial improvement over standard convolutional architectures and domain adaptation methods.",0
"Our focus is on developing robotic sensorimotor control policies that can be applied in visually diverse and unfamiliar environments. Obtaining large datasets for interactive processes like reinforcement learning or learning from demonstration is challenging. However, passively collected datasets of images or videos can capture much of the world's visual diversity. Our approach, GPLAC (Generalized Policy Learning with Attentional Classifier), combines interaction data with weakly labeled image data to enhance the generalization capability of sensorimotor policies. We utilize multitask learning, an auxiliary binary classification objective, and a convolutional neural network architecture with an attentional mechanism to avoid distractions. We demonstrate that combining interaction data from a single environment with a varied dataset of weakly labeled data leads to significant improvement in generalization to new environments. Our proposed attentional architecture and auxiliary objective are crucial for this generalization. Our results show considerable progress over standard convolutional architectures and domain adaptation techniques, as evidenced by experiments in both simulation and real robotic manipulation scenarios.",1
"We address the problem of incrementally modeling and forecasting long-term goals of a first-person camera wearer: what the user will do, where they will go, and what goal they seek. In contrast to prior work in trajectory forecasting, our algorithm, DARKO, goes further to reason about semantic states (will I pick up an object?), and future goal states that are far in terms of both space and time. DARKO learns and forecasts from first-person visual observations of the user's daily behaviors via an Online Inverse Reinforcement Learning (IRL) approach. Classical IRL discovers only the rewards in a batch setting, whereas DARKO discovers the states, transitions, rewards, and goals of a user from streaming data. Among other results, we show DARKO forecasts goals better than competing methods in both noisy and ideal settings, and our approach is theoretically and empirically no-regret.",0
"Our focus is on the gradual development and prediction of extensive objectives for an individual who is wearing a first-person camera. This includes forecasting their future actions, destinations, and desired outcomes. Unlike previous research on trajectory prediction, our algorithm, DARKO, takes into account semantic states (such as whether the user will pick up an object) and remote goal states that involve both time and space. By utilizing an Online Inverse Reinforcement Learning (IRL) technique, DARKO is able to learn and predict from the user's daily visual observations. Unlike classical IRL, which only uncovers rewards in a batch environment, DARKO identifies states, transitions, rewards, and goals from streaming data. Our findings indicate that DARKO outperforms other methods in predicting goals, both in noisy and ideal conditions. Additionally, our strategy is theoretically and empirically no-regret.",1
"We initiate the study of fairness in reinforcement learning, where the actions of a learning algorithm may affect its environment and future rewards. Our fairness constraint requires that an algorithm never prefers one action over another if the long-term (discounted) reward of choosing the latter action is higher. Our first result is negative: despite the fact that fairness is consistent with the optimal policy, any learning algorithm satisfying fairness must take time exponential in the number of states to achieve non-trivial approximation to the optimal policy. We then provide a provably fair polynomial time algorithm under an approximate notion of fairness, thus establishing an exponential gap between exact and approximate fairness",0
"The study of fairness in reinforcement learning is our focus, considering the impact of a learning algorithm's actions on its environment and future rewards. Our fairness constraint mandates that an algorithm must not favor one action over another if the latter action results in higher long-term (discounted) rewards. Our research initially yielded negative results: although fairness aligns with the optimal policy, any algorithm that adheres to fairness will require an exponential amount of time in the number of states to attain a satisfactory approximation to the optimal policy. However, we subsequently devised a polynomial time algorithm that is provably fair under an approximate definition of fairness, thereby highlighting the exponential discrepancy between exact and approximate fairness.",1
"Given a textual description of an image, phrase grounding localizes objects in the image referred by query phrases in the description. State-of-the-art methods address the problem by ranking a set of proposals based on the relevance to each query, which are limited by the performance of independent proposal generation systems and ignore useful cues from context in the description. In this paper, we adopt a spatial regression method to break the performance limit, and introduce reinforcement learning techniques to further leverage semantic context information. We propose a novel Query-guided Regression network with Context policy (QRC Net) which jointly learns a Proposal Generation Network (PGN), a Query-guided Regression Network (QRN) and a Context Policy Network (CPN). Experiments show QRC Net provides a significant improvement in accuracy on two popular datasets: Flickr30K Entities and Referit Game, with 14.25% and 17.14% increase over the state-of-the-arts respectively.",0
"The process of phrase grounding involves identifying the objects in an image that match the query phrases in a given textual description. However, current methods that rank proposals based on query relevance are limited by the quality of independent proposal generation systems and disregard relevant contextual clues. To improve the accuracy of this process, we propose a novel approach that combines a spatial regression method with reinforcement learning techniques. Our Query-guided Regression network with Context policy (QRC Net) integrates a Proposal Generation Network (PGN), a Query-guided Regression Network (QRN), and a Context Policy Network (CPN) to optimize performance. Our experiments on the Flickr30K Entities and Referit Game datasets demonstrate a significant improvement in accuracy compared to state-of-the-art methods, with QRC Net achieving a 14.25% and 17.14% increase in accuracy, respectively.",1
"High-dimensional representations, such as radial basis function networks or tile coding, are common choices for policy evaluation in reinforcement learning. Learning with such high-dimensional representations, however, can be expensive, particularly for matrix methods, such as least-squares temporal difference learning or quasi-Newton methods that approximate matrix step-sizes. In this work, we explore the utility of sketching for these two classes of algorithms. We highlight issues with sketching the high-dimensional features directly, which can incur significant bias. As a remedy, we demonstrate how to use sketching more sparingly, with only a left-sided sketch, that can still enable significant computational gains and the use of these matrix-based learning algorithms that are less sensitive to parameters. We empirically investigate these algorithms, in four domains with a variety of representations. Our aim is to provide insights into effective use of sketching in practice.",0
"Reinforcement learning often uses high-dimensional representations like radial basis function networks or tile coding for policy evaluation. However, using these representations can be expensive, particularly for matrix methods such as least-squares temporal difference learning or quasi-Newton methods that estimate matrix step-sizes. This study examines the effectiveness of sketching for these algorithms and identifies issues with sketching high-dimensional features directly, which can lead to bias. Instead, the study proposes using a left-sided sketch more selectively, which can still provide significant computational benefits and allow for the use of less parameter-sensitive matrix-based learning algorithms. The study empirically evaluates these algorithms in four domains with varying representations, with the goal of providing practical insights into the optimal use of sketching.",1
"Wireless systems perform rate adaptation to transmit at highest possible instantaneous rates. Rate adaptation has been increasingly granular over generations of wireless systems. The base-station uses SINR and packet decode feedback called acknowledgement/no acknowledgement (ACK/NACK) to perform rate adaptation. SINR is used for rate anchoring called inner look adaptation and ACK/NACK is used for fine offset adjustments called Outer Loop Link Adaptation (OLLA). We cast the OLLA as a reinforcement learning problem of the class of Multi-Armed Bandits (MAB) where the different offset values are the arms of the bandit. In OLLA, as the offset values increase, the probability of packet error also increase, and every user equipment (UE) has a desired Block Error Rate (BLER) to meet certain Quality of Service (QoS) requirements. For this MAB we propose a binary search based algorithm which achieves a Probably Approximately Correct (PAC) solution making use of bounds from large deviation theory and confidence bounds. In addition to this we also discuss how a Thompson sampling or UCB based method will not help us meet the target objectives. Finally, simulation results are provided on an LTE system simulator and thereby prove the efficacy of our proposed algorithm.",0
"Wireless systems adjust their transmission rate to maximize their instantaneous rate, and this process has become more precise with each new generation of wireless technology. To achieve rate adaptation, the base-station uses two types of feedback: SINR for inner look adaptation and ACK/NACK for outer loop link adaptation (OLLA). OLLA involves making fine offset adjustments to maintain a desired Block Error Rate (BLER) for each user equipment (UE), while meeting Quality of Service (QoS) requirements. We model OLLA as a reinforcement learning problem using Multi-Armed Bandits (MAB), where the different offset values are treated as the arms of the bandit. To solve this problem, we propose a binary search algorithm that achieves a Probably Approximately Correct (PAC) solution based on bounds from large deviation theory and confidence bounds. We also demonstrate that a Thompson sampling or UCB approach is not effective in meeting our target objectives. Finally, we present simulation results from an LTE system simulator that validate the effectiveness of our proposed algorithm.",1
"We develop an approach to training generative models based on unrolling a variational auto-encoder into a Markov chain, and shaping the chain's trajectories using a technique inspired by recent work in Approximate Bayesian computation. We show that the global minimizer of the resulting objective is achieved when the generative model reproduces the target distribution. To allow finer control over the behavior of the models, we add a regularization term inspired by techniques used for regularizing certain types of policy search in reinforcement learning. We present empirical results on the MNIST and TFD datasets which show that our approach offers state-of-the-art performance, both quantitatively and from a qualitative point of view.",0
"Our method for training generative models involves unrolling a variational auto-encoder into a Markov chain and manipulating the chain's trajectories using a technique influenced by recent work in Approximate Bayesian computation. By doing so, we demonstrate that the generative model can accurately reproduce the target distribution, as the global minimizer of our objective is achieved. Additionally, we utilize a regularization term based on reinforcement learning policy search techniques to provide more control over our models. Through experiments on the MNIST and TFD datasets, we show that our approach achieves top-notch performance in both quantitative and qualitative aspects.",1
"The Particle Swarm Optimization Policy (PSO-P) has been recently introduced and proven to produce remarkable results on interacting with academic reinforcement learning benchmarks in an off-policy, batch-based setting. To further investigate the properties and feasibility on real-world applications, this paper investigates PSO-P on the so-called Industrial Benchmark (IB), a novel reinforcement learning (RL) benchmark that aims at being realistic by including a variety of aspects found in industrial applications, like continuous state and action spaces, a high dimensional, partially observable state space, delayed effects, and complex stochasticity. The experimental results of PSO-P on IB are compared to results of closed-form control policies derived from the model-based Recurrent Control Neural Network (RCNN) and the model-free Neural Fitted Q-Iteration (NFQ). Experiments show that PSO-P is not only of interest for academic benchmarks, but also for real-world industrial applications, since it also yielded the best performing policy in our IB setting. Compared to other well established RL techniques, PSO-P produced outstanding results in performance and robustness, requiring only a relatively low amount of effort in finding adequate parameters or making complex design decisions.",0
"Recently, the Particle Swarm Optimization Policy (PSO-P) has demonstrated impressive outcomes when interacting with academic reinforcement learning benchmarks in an off-policy, batch-based environment. This study aims to examine the properties and feasibility of PSO-P in real-world applications by testing it on the Industrial Benchmark (IB), a novel reinforcement learning (RL) benchmark that aims to replicate various aspects found in industrial applications. The IB has characteristics like continuous state and action spaces, a high dimensional, partially observable state space, delayed effects, and complex stochasticity to make it more realistic. PSO-P's performance on the IB is compared to results from model-based Recurrent Control Neural Network (RCNN) and model-free Neural Fitted Q-Iteration (NFQ) closed-form control policies. The results reveal that PSO-P is not only useful for academic benchmarks but also for real-world industrial applications, as it outperformed the other techniques in the IB setting. PSO-P exhibited exceptional performance and robustness compared to other established RL methods, and it only requires a relatively low amount of effort to find suitable parameters or make complex design decisions.",1
"When an agent cannot represent a perfectly accurate model of its environment's dynamics, model-based reinforcement learning (MBRL) can fail catastrophically. Planning involves composing the predictions of the model; when flawed predictions are composed, even minor errors can compound and render the model useless for planning. Hallucinated Replay (Talvitie 2014) trains the model to ""correct"" itself when it produces errors, substantially improving MBRL with flawed models. This paper theoretically analyzes this approach, illuminates settings in which it is likely to be effective or ineffective, and presents a novel error bound, showing that a model's ability to self-correct is more tightly related to MBRL performance than one-step prediction error. These results inspire an MBRL algorithm for deterministic MDPs with performance guarantees that are robust to model class limitations.",0
"MBRL can fail dramatically when an agent is unable to accurately represent the dynamics of its environment. Planning relies on the predictions made by the model, and even minor flaws can compound and render the model useless for planning purposes. Talvitie's 2014 Hallucinated Replay method trains the model to rectify its mistakes, which significantly improves MBRL with flawed models. This paper examines this approach theoretically, identifies scenarios where it is likely to be effective or ineffective, and introduces a new error bound, demonstrating that a model's ability to self-correct is more closely related to MBRL performance than one-step prediction error. These findings have inspired a robust MBRL algorithm for deterministic MDPs that is resilient to model class limitations.",1
"This paper considers a demand response agent that must find a near-optimal sequence of decisions based on sparse observations of its environment. Extracting a relevant set of features from these observations is a challenging task and may require substantial domain knowledge. One way to tackle this problem is to store sequences of past observations and actions in the state vector, making it high dimensional, and apply techniques from deep learning. This paper investigates the capabilities of different deep learning techniques, such as convolutional neural networks and recurrent neural networks, to extract relevant features for finding near-optimal policies for a residential heating system and electric water heater that are hindered by sparse observations. Our simulation results indicate that in this specific scenario, feeding sequences of time-series to an LSTM network, which is a specific type of recurrent neural network, achieved a higher performance than stacking these time-series in the input of a convolutional neural network or deep neural network.",0
"The article examines a demand response agent's task of making near-optimal decisions based on limited observations of the environment. It is difficult to identify significant characteristics from these observations, which may necessitate substantial domain expertise. One approach to resolving this problem is to keep a record of past observations and actions in the state vector, resulting in a high-dimensional vector, and applying deep learning techniques. The article explores how different deep learning methods, such as convolutional neural networks and recurrent neural networks, can be used to extract critical features for discovering near-optimal policies for a residential heating system and electric water heater when sparse observations are a constraint. Our simulation outcomes indicate that, in this particular situation, providing a sequence of time-series data to an LSTM network, a type of recurrent neural network, resulted in better performance than stacking these time-series data in the input of a convolutional neural network or deep neural network.",1
"This paper develops an inverse reinforcement learning algorithm aimed at recovering a reward function from the observed actions of an agent. We introduce a strategy to flexibly handle different types of actions with two approximations of the Bellman Optimality Equation, and a Bellman Gradient Iteration method to compute the gradient of the Q-value with respect to the reward function. These methods allow us to build a differentiable relation between the Q-value and the reward function and learn an approximately optimal reward function with gradient methods. We test the proposed method in two simulated environments by evaluating the accuracy of different approximations and comparing the proposed method with existing solutions. The results show that even with a linear reward function, the proposed method has a comparable accuracy with the state-of-the-art method adopting a non-linear reward function, and the proposed method is more flexible because it is defined on observed actions instead of trajectories.",0
"In this article, we present a novel inverse reinforcement learning algorithm that aims to recover a reward function from an agent's observed actions. To handle various types of actions, we introduce a flexible strategy that utilizes two approximations of the Bellman Optimality Equation and a Bellman Gradient Iteration method to compute the gradient of the Q-value. This approach enables us to establish a differentiable relationship between the Q-value and the reward function, allowing us to learn an approximately optimal reward function using gradient methods. To evaluate the effectiveness of our proposed method, we conducted experiments in two simulated environments and compared our results with existing solutions, testing the accuracy of various approximations. Our findings indicate that, even with a linear reward function, our method's accuracy is comparable to that of the state-of-the-art non-linear reward function method. Moreover, our proposed method is more flexible since it is defined based on observed actions rather than trajectories.",1
"We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.",0
"Our proposed approach enables the learning of expressive energy-based policies for continuous states and actions, which was previously only possible in tabular domains. By applying this method to maximum entropy policies, we have developed a new algorithm named soft Q-learning. This algorithm represents the optimal policy through a Boltzmann distribution and uses the amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of our algorithm include better exploration and compositionality, allowing for skill transfer between tasks. This has been verified through simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be seen as performing approximate inference on the corresponding energy-based model.",1
"In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.",0
"This paper proposes that the value distribution, or the distribution of random returns received by a reinforcement learning agent, is of utmost importance. This differs from the conventional approach, which focuses on modeling the expected return or value. While previous literature has explored the value distribution for specific purposes, such as risk-aware behavior, we present new theoretical results that reveal significant distributional instability in the control setting. To address this, we introduce a new algorithm that applies Bellman's equation to the learning of approximate value distributions. Our evaluation using the Arcade Learning Environment demonstrates state-of-the-art results and emphasizes the significance of the value distribution in approximate reinforcement learning. We combine theoretical and empirical evidence to illustrate how the value distribution affects learning in the approximate setting.",1
"Semantic parsing of large-scale 3D point clouds is an important research topic in computer vision and remote sensing fields. Most existing approaches utilize hand-crafted features for each modality independently and combine them in a heuristic manner. They often fail to consider the consistency and complementary information among features adequately, which makes them difficult to capture high-level semantic structures. The features learned by most of the current deep learning methods can obtain high-quality image classification results. However, these methods are hard to be applied to recognize 3D point clouds due to unorganized distribution and various point density of data. In this paper, we propose a 3DCNN-DQN-RNN method which fuses the 3D convolutional neural network (CNN), Deep Q-Network (DQN) and Residual recurrent neural network (RNN) for an efficient semantic parsing of large-scale 3D point clouds. In our method, an eye window under control of the 3D CNN and DQN can localize and segment the points of the object class efficiently. The 3D CNN and Residual RNN further extract robust and discriminative features of the points in the eye window, and thus greatly enhance the parsing accuracy of large-scale point clouds. Our method provides an automatic process that maps the raw data to the classification results. It also integrates object localization, segmentation and classification into one framework. Experimental results demonstrate that the proposed method outperforms the state-of-the-art point cloud classification methods.",0
"The investigation of semantic parsing of vast 3D point clouds is a crucial study area in computer vision and remote sensing. Most current techniques employ hand-crafted features for each modality separately and combine them in a heuristic way, failing to adequately consider the consistency and complementary information among features, which makes it challenging to capture high-level semantic structures. Although the features acquired by current deep learning methods can obtain excellent image classification results, they are difficult to apply to recognize 3D point clouds due to the unorganized distribution and diverse point density of data. This paper presents a 3DCNN-DQN-RNN method that combines the 3D convolutional neural network (CNN), Deep Q-Network (DQN), and Residual recurrent neural network (RNN) to efficiently perform semantic parsing of vast 3D point clouds. Our method includes an eye window controlled by the 3D CNN and DQN that effectively localizes and segments the points of the object class. The 3D CNN and Residual RNN then extract robust and discriminative features of the points in the eye window, significantly improving the accuracy of parsing large-scale point clouds. Our approach offers an automated process that maps raw data to classification results, integrating object localization, segmentation, and classification into one framework. Experimental results demonstrate that our proposed method outperforms the current state-of-the-art point cloud classification techniques.",1
"Unprecedented high volumes of data are becoming available with the growth of the advanced metering infrastructure. These are expected to benefit planning and operation of the future power system, and to help the customers transition from a passive to an active role. In this paper, we explore for the first time in the smart grid context the benefits of using Deep Reinforcement Learning, a hybrid type of methods that combines Reinforcement Learning with Deep Learning, to perform on-line optimization of schedules for building energy management systems. The learning procedure was explored using two methods, Deep Q-learning and Deep Policy Gradient, both of them being extended to perform multiple actions simultaneously. The proposed approach was validated on the large-scale Pecan Street Inc. database. This highly-dimensional database includes information about photovoltaic power generation, electric vehicles as well as buildings appliances. Moreover, these on-line energy scheduling strategies could be used to provide real-time feedback to consumers to encourage more efficient use of electricity.",0
"The growth of the advanced metering infrastructure has resulted in an unprecedented increase in the availability of data. This data is expected to have a positive impact on the planning and operation of future power systems, and on the transition of customers from a passive role to an active one. In this study, we investigate the benefits of using Deep Reinforcement Learning, a hybrid method that combines Reinforcement Learning with Deep Learning, to optimize schedules for building energy management systems. We explore two methods of learning, Deep Q-learning and Deep Policy Gradient, both of which can perform multiple actions simultaneously. The proposed approach was tested on the large-scale Pecan Street Inc. database, which includes data on photovoltaic power generation, electric vehicles, and building appliances. Furthermore, these scheduling strategies can provide real-time feedback to consumers, encouraging more efficient electricity use.",1
"We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.",0
"Our proposed meta-learning algorithm is not specific to any particular model and can be used for a range of learning problems, including regression, classification, and reinforcement learning, as long as they are trained using gradient descent. The aim of meta-learning is to teach a model to solve new learning tasks using only a small number of training samples. Our approach explicitly trains the model parameters so that it can be fine-tuned easily with a small amount of data from a new task. We have observed that our method leads to better performance than other approaches on two few-shot image classification benchmarks, produces satisfactory outcomes on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.",1
"Autonomous unpowered flight is a challenge for control and guidance systems: all the energy the aircraft might use during flight has to be harvested directly from the atmosphere. We investigate the design of an algorithm that optimizes the closed-loop control of a glider's bank and sideslip angles, while flying in the lower convective layer of the atmosphere in order to increase its mission endurance. Using a Reinforcement Learning approach, we demonstrate the possibility for real-time adaptation of the glider's behaviour to the time-varying and noisy conditions associated with thermal soaring flight. Our approach is online, data-based and model-free, hence avoids the pitfalls of aerological and aircraft modelling and allow us to deal with uncertainties and non-stationarity. Additionally, we put a particular emphasis on keeping low computational requirements in order to make on-board execution feasible. This article presents the stochastic, time-dependent aerological model used for simulation, together with a standard aircraft model. Then we introduce an adaptation of a Q-learning algorithm and demonstrate its ability to control the aircraft and improve its endurance by exploiting updrafts in non-stationary scenarios.",0
"Designing an algorithm for controlling and guiding an unpowered aircraft is a challenging task as the energy required for flight must be obtained from the atmosphere. Our research focuses on optimizing the closed-loop control of a glider's bank and sideslip angles while flying in the lower convective layer of the atmosphere. By using a Reinforcement Learning approach, we establish the possibility of real-time adaptation of the glider's behavior to varying and noisy conditions associated with thermal soaring flight. Our approach is model-free and data-based, avoiding the limitations of aerological and aircraft modeling, enabling us to deal with uncertainties and non-stationarity. We emphasize the importance of low computational requirements to enable on-board execution. This article outlines the stochastic, time-dependent aerological model used for simulation and a standard aircraft model. We introduce an adaptation of a Q-learning algorithm, demonstrating its effectiveness in controlling the aircraft and improving its endurance by exploiting updrafts in non-stationary scenarios.",1
"We formulate tracking as an online decision-making process, where a tracking agent must follow an object despite ambiguous image frames and a limited computational budget. Crucially, the agent must decide where to look in the upcoming frames, when to reinitialize because it believes the target has been lost, and when to update its appearance model for the tracked object. Such decisions are typically made heuristically. Instead, we propose to learn an optimal decision-making policy by formulating tracking as a partially observable decision-making process (POMDP). We learn policies with deep reinforcement learning algorithms that need supervision (a reward signal) only when the track has gone awry. We demonstrate that sparse rewards allow us to quickly train on massive datasets, several orders of magnitude more than past work. Interestingly, by treating the data source of Internet videos as unlimited streams, we both learn and evaluate our trackers in a single, unified computational stream.",0
"Tracking an object despite unclear images and limited computational resources is framed as an online decision-making process. The tracking agent must determine where to focus in upcoming frames, when to restart tracking, and when to update the object's appearance model. While these decisions are typically made through heuristics, we suggest learning an ideal decision-making policy using a partially observable decision-making process (POMDP). Deep reinforcement learning algorithms are used to learn policies, and sparse rewards allow for quick training on large datasets. Additionally, we utilize Internet videos as an infinite data source to both learn and evaluate our trackers in a unified computational stream.",1
"For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.",0
"To enable advanced reinforcement learning (RL) systems to engage with real-world environments, it is necessary to convey intricate goals to these systems. This study investigates goals expressed in terms of human preferences (non-expert) between pairs of trajectory segments. Our findings demonstrate that this method can proficiently tackle complex RL tasks, such as Atari games and simulated robot locomotion, without access to the reward function. Moreover, it offers feedback on less than one percent of our agent's interactions with the environment, thus reducing the cost of human oversight to a level that can be feasibly applied to cutting-edge RL systems. To showcase the versatility of our approach, we illustrate that we can effectively teach intricate and unprecedented behaviors in just one hour of human time. These behaviors and environments are significantly more complex than any previously learned through human feedback.",1
"Many real-world tasks involve multiple agents with partial observability and limited communication. Learning is challenging in these settings due to local viewpoints of agents, which perceive the world as non-stationary due to concurrently-exploring teammates. Approaches that learn specialized policies for individual tasks face problems when applied to the real world: not only do agents have to learn and store distinct policies for each task, but in practice identities of tasks are often non-observable, making these approaches inapplicable. This paper formalizes and addresses the problem of multi-task multi-agent reinforcement learning under partial observability. We introduce a decentralized single-task learning approach that is robust to concurrent interactions of teammates, and present an approach for distilling single-task policies into a unified policy that performs well across multiple related tasks, without explicit provision of task identity.",0
"Learning in real-world scenarios can be difficult, especially when multiple agents are involved with limited communication and partial observability. This presents a challenge as the agents' local viewpoints can cause the world to appear non-stationary due to the actions of their teammates. Specialized policies for individual tasks also face problems when applied in reality, as agents must learn and store multiple policies and task identities are often not observable. To address this issue, this paper introduces a decentralized single-task learning approach that can handle concurrent interactions between teammates. Additionally, a method for combining single-task policies into a unified policy that performs effectively across multiple related tasks is presented without the need for explicit task identification.",1
"Most deep reinforcement learning algorithms are data inefficient in complex and rich environments, limiting their applicability to many scenarios. One direction for improving data efficiency is multitask learning with shared neural network parameters, where efficiency may be improved through transfer across related tasks. In practice, however, this is not usually observed, because gradients from different tasks can interfere negatively, making learning unstable and sometimes even less data efficient. Another issue is the different reward schemes between tasks, which can easily lead to one task dominating the learning of a shared model. We propose a new approach for joint training of multiple tasks, which we refer to as Distral (Distill & transfer learning). Instead of sharing parameters between the different workers, we propose to share a ""distilled"" policy that captures common behaviour across tasks. Each worker is trained to solve its own task while constrained to stay close to the shared policy, while the shared policy is trained by distillation to be the centroid of all task policies. Both aspects of the learning process are derived by optimizing a joint objective function. We show that our approach supports efficient transfer on complex 3D environments, outperforming several related methods. Moreover, the proposed learning process is more robust and more stable---attributes that are critical in deep reinforcement learning.",0
"Deep reinforcement learning algorithms often struggle with data inefficiency in complex and rich environments, making them unsuitable for many scenarios. One potential solution for improving data efficiency is through multitask learning with shared neural network parameters. However, this approach is often unsuccessful due to negative interference between task gradients, which leads to instability and lower data efficiency. Additionally, different reward schemes among tasks can result in one task dominating the shared model's learning. To address these issues, we propose a new approach called Distral (Distill & transfer learning) that involves sharing a ""distilled"" policy capturing common behavior across tasks instead of shared parameters. Each worker is then trained to solve their task while staying close to the shared policy, and the shared policy is trained through distillation to be the centroid of all task policies. This joint optimization approach supports efficient transfer on complex 3D environments and outperforms several related methods while being more robust and stable.",1
"Given a large number of unlabeled face images, face grouping aims at clustering the images into individual identities present in the data. This task remains a challenging problem despite the remarkable capability of deep learning approaches in learning face representation. In particular, grouping results can still be egregious given profile faces and a large number of uninteresting faces and noisy detections. Often, a user needs to correct the erroneous grouping manually. In this study, we formulate a novel face grouping framework that learns clustering strategy from ground-truth simulated behavior. This is achieved through imitation learning (a.k.a apprenticeship learning or learning by watching) via inverse reinforcement learning (IRL). In contrast to existing clustering approaches that group instances by similarity, our framework makes sequential decision to dynamically decide when to merge two face instances/groups driven by short- and long-term rewards. Extensive experiments on three benchmark datasets show that our framework outperforms unsupervised and supervised baselines.",0
"The objective of face grouping is to cluster a large number of unidentified face images into separate identities. Despite the impressive ability of deep learning methods to learn face representation, this task is still challenging. This is especially true for profile faces and when there are numerous unremarkable faces and noisy detections, which can result in erroneous grouping that requires manual correction. Our study presents a new approach to face grouping that utilizes imitation learning (also known as apprenticeship learning or learning by watching) via inverse reinforcement learning (IRL) to learn clustering strategy from simulated ground-truth behavior. Unlike existing clustering methods that group instances based on similarity, our framework makes sequential decisions to determine when to merge two face instances/groups using short- and long-term rewards. Our experiments on three benchmark datasets demonstrate that our approach outperforms unsupervised and supervised baselines.",1
"In this paper, we propose a principled deep reinforcement learning (RL) approach that is able to accelerate the convergence rate of general deep neural networks (DNNs). With our approach, a deep RL agent (synonym for optimizer in this work) is used to automatically learn policies about how to schedule learning rates during the optimization of a DNN. The state features of the agent are learned from the weight statistics of the optimizee during training. The reward function of this agent is designed to learn policies that minimize the optimizee's training time given a certain performance goal. The actions of the agent correspond to changing the learning rate for the optimizee during training. As far as we know, this is the first attempt to use deep RL to learn how to optimize a large-sized DNN. We perform extensive experiments on a standard benchmark dataset and demonstrate the effectiveness of the policies learned by our approach.",0
"This paper introduces a deep reinforcement learning (RL) technique that can enhance the convergence rate of general deep neural networks (DNNs) in a systematic way. The proposed approach deploys a deep RL agent as an optimizer, which learns to schedule the learning rates for the DNN during the optimization process. The agent's state features are learned from the weight statistics of the optimizee, while its reward function aims to minimize the optimizee's training time for a given performance goal. The agent's actions correspond to changes in the learning rate during training. To the best of our knowledge, this is the first attempt to apply deep RL to optimize large-sized DNNs. Extensive experiments on a standard benchmark dataset demonstrate the efficacy of the learned policies.",1
"Visual servoing involves choosing actions that move a robot in response to observations from a camera, in order to reach a goal configuration in the world. Standard visual servoing approaches typically rely on manually designed features and analytical dynamics models, which limits their generalization capability and often requires extensive application-specific feature and model engineering. In this work, we study how learned visual features, learned predictive dynamics models, and reinforcement learning can be combined to learn visual servoing mechanisms. We focus on target following, with the goal of designing algorithms that can learn a visual servo using low amounts of data of the target in question, to enable quick adaptation to new targets. Our approach is based on servoing the camera in the space of learned visual features, rather than image pixels or manually-designed keypoints. We demonstrate that standard deep features, in our case taken from a model trained for object classification, can be used together with a bilinear predictive model to learn an effective visual servo that is robust to visual variation, changes in viewing angle and appearance, and occlusions. A key component of our approach is to use a sample-efficient fitted Q-iteration algorithm to learn which features are best suited for the task at hand. We show that we can learn an effective visual servo on a complex synthetic car following benchmark using just 20 training trajectory samples for reinforcement learning. We demonstrate substantial improvement over a conventional approach based on image pixels or hand-designed keypoints, and we show an improvement in sample-efficiency of more than two orders of magnitude over standard model-free deep reinforcement learning algorithms. Videos are available at http://rll.berkeley.edu/visual_servoing .",0
"Visual servoing is the process of making a robot move based on what it sees through a camera, in order to achieve a certain goal in the real world. Typically, this is done using pre-designed features and analytical models, which can limit its usefulness and require specific engineering for each application. This study looks at how learned visual features, predictive models, and reinforcement learning can be combined to create more adaptable visual servoing mechanisms. The focus is on target following, using minimal data to quickly adapt to new targets. Instead of using image pixels or manual keypoints, the method uses learned visual features to control the camera. A fitted Q-iteration algorithm is used to determine the best features for the task at hand. The results show a significant improvement over conventional methods using image pixels or hand-designed keypoints, and a sample-efficiency boost of more than two orders of magnitude over standard model-free deep reinforcement learning algorithms. Videos are available on the website.",1
"This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.",0
"The paper outlines a stable and sample-efficient actor-critic deep reinforcement learning agent that excels in complex environments, such as the 57-game Atari domain and various continuous control problems. The agent utilizes experience replay and introduces novel techniques such as truncated importance sampling with bias correction, stochastic dueling network architectures, and a trust region policy optimization method.",1
"We consider the problem of provably optimal exploration in reinforcement learning for finite horizon MDPs. We show that an optimistic modification to value iteration achieves a regret bound of $\tilde{O}( \sqrt{HSAT} + H^2S^2A+H\sqrt{T})$ where $H$ is the time horizon, $S$ the number of states, $A$ the number of actions and $T$ the number of time-steps. This result improves over the best previous known bound $\tilde{O}(HS \sqrt{AT})$ achieved by the UCRL2 algorithm of Jaksch et al., 2010. The key significance of our new results is that when $T\geq H^3S^3A$ and $SA\geq H$, it leads to a regret of $\tilde{O}(\sqrt{HSAT})$ that matches the established lower bound of $\Omega(\sqrt{HSAT})$ up to a logarithmic factor. Our analysis contains two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in $S$), and we define Bernstein-based ""exploration bonuses"" that use the empirical variance of the estimated values at the next states (to improve scaling in $H$).",0
"The focus of our study is achieving optimal exploration in reinforcement learning for finite horizon MDPs. Through an optimistic modification to value iteration, we have developed a regret bound of $\tilde{O}( \sqrt{HSAT} + H^2S^2A+H\sqrt{T})$, taking into account the time horizon ($H$), number of states ($S$), number of actions ($A$), and number of time-steps ($T$). This result is an improvement over the previous best-known bound of $\tilde{O}(HS \sqrt{AT})$ achieved by the UCRL2 algorithm. Our new results are particularly significant because they match the established lower bound of $\Omega(\sqrt{HSAT})$ up to a logarithmic factor when $T\geq H^3S^3A$ and $SA\geq H$. Our analysis is based on two key insights: the careful application of concentration inequalities to the optimal value function as a whole, rather than to the transition probabilities (to improve scaling in $S$), and the definition of Bernstein-based ""exploration bonuses"" that use the empirical variance of the estimated values at the next states (to improve scaling in $H$).",1
"We propose a new neural sequence model training method in which the objective function is defined by $\alpha$-divergence. We demonstrate that the objective function generalizes the maximum-likelihood (ML)-based and reinforcement learning (RL)-based objective functions as special cases (i.e., ML corresponds to $\alpha \to 0$ and RL to $\alpha \to1$). We also show that the gradient of the objective function can be considered a mixture of ML- and RL-based objective gradients. The experimental results of a machine translation task show that minimizing the objective function with $\alpha > 0$ outperforms $\alpha \to 0$, which corresponds to ML-based methods.",0
"Our proposed neural sequence model training method employs an objective function defined by $\alpha$-divergence. By doing so, we establish that this objective function encompasses both maximum-likelihood (ML)-based and reinforcement learning (RL)-based objective functions as special cases, with ML corresponding to $\alpha \to 0$ and RL to $\alpha \to 1$. Moreover, we demonstrate that the gradient of this objective function can be seen as a combination of gradients from ML- and RL-based objectives. Our experiments on machine translation show that minimizing the objective function with $\alpha > 0$ outperforms the ML-based method, which corresponds to $\alpha \to 0$.",1
"We propose a novel and flexible approach to meta-learning for learning-to-learn from only a few examples. Our framework is motivated by actor-critic reinforcement learning, but can be applied to both reinforcement and supervised learning. The key idea is to learn a meta-critic: an action-value function neural network that learns to criticise any actor trying to solve any specified task. For supervised learning, this corresponds to the novel idea of a trainable task-parametrised loss generator. This meta-critic approach provides a route to knowledge transfer that can flexibly deal with few-shot and semi-supervised conditions for both reinforcement and supervised learning. Promising results are shown on both reinforcement and supervised learning problems.",0
"Our approach to meta-learning for learning-to-learn from limited examples is innovative and adaptable. While actor-critic reinforcement learning inspired our framework, it can be employed in both supervised and reinforcement learning. The main concept is to develop a meta-critic, which is a neural network action-value function that can evaluate any actor attempting to solve a particular task. In supervised learning, this entails a unique trainable task-parametrised loss generator. This meta-critic strategy allows for flexible knowledge transfer in few-shot and semi-supervised settings for both types of learning. Encouraging outcomes have been observed in both reinforcement and supervised learning scenarios.",1
"The past few years have witnessed a growth in size and computational requirements for training and inference with neural networks. Currently, a common approach to address these requirements is to use a heterogeneous distributed environment with a mixture of hardware devices such as CPUs and GPUs. Importantly, the decision of placing parts of the neural models on devices is often made by human experts based on simple heuristics and intuitions. In this paper, we propose a method which learns to optimize device placement for TensorFlow computational graphs. Key to our method is the use of a sequence-to-sequence model to predict which subsets of operations in a TensorFlow graph should run on which of the available devices. The execution time of the predicted placements is then used as the reward signal to optimize the parameters of the sequence-to-sequence model. Our main result is that on Inception-V3 for ImageNet classification, and on RNN LSTM, for language modeling and neural machine translation, our model finds non-trivial device placements that outperform hand-crafted heuristics and traditional algorithmic methods.",0
"Over the past few years, there has been an increase in both size and computational demands for training and inference with neural networks. To address these requirements, a popular approach involves utilizing a heterogeneous distributed environment with a mix of hardware devices like CPUs and GPUs. However, the decision of which parts of the neural models to place on certain devices is typically made by human experts using basic heuristics and intuition. This paper introduces a method that uses a sequence-to-sequence model to learn how to optimize device placement for TensorFlow computational graphs. The model predicts which subsets of operations in a TensorFlow graph should run on which device, with the execution time of the predicted placements serving as the reward signal to optimize the model's parameters. Results indicate that this method outperforms hand-crafted heuristics and traditional algorithmic methods for both Inception-V3 for ImageNet classification and RNN LSTM for language modeling and neural machine translation.",1
"Existing action detection algorithms usually generate action proposals through an extensive search over the video at multiple temporal scales, which brings about huge computational overhead and deviates from the human perception procedure. We argue that the process of detecting actions should be naturally one of observation and refinement: observe the current window and refine the span of attended window to cover true action regions. In this paper, we propose an active action proposal model that learns to find actions through continuously adjusting the temporal bounds in a self-adaptive way. The whole process can be deemed as an agent, which is firstly placed at a position in the video at random, adopts a sequence of transformations on the current attended region to discover actions according to a learned policy. We utilize reinforcement learning, especially the Deep Q-learning algorithm to learn the agent's decision policy. In addition, we use temporal pooling operation to extract more effective feature representation for the long temporal window, and design a regression network to adjust the position offsets between predicted results and the ground truth. Experiment results on THUMOS 2014 validate the effectiveness of the proposed approach, which can achieve competitive performance with current action detection algorithms via much fewer proposals.",0
"The current approach to action detection algorithms involves a time-consuming search across multiple temporal scales, which does not align with human perception. Our paper proposes a more natural approach to action detection, where the observed window is refined to cover the true action regions. We introduce an active action proposal model that continuously adjusts the temporal bounds in a self-adaptive way. This agent is placed randomly and uses a learned policy to discover actions through a sequence of transformations on the current attended region. We utilize reinforcement learning, specifically the Deep Q-learning algorithm, to learn the agent's decision policy. Furthermore, we use temporal pooling and a regression network to improve feature representation and adjust position offsets. Our proposed approach achieves competitive performance with current action detection algorithms using significantly fewer proposals, as validated in our experiments on THUMOS 2014.",1
"Observational learning is a type of learning that occurs as a function of observing, retaining and possibly replicating or imitating the behaviour of another agent. It is a core mechanism appearing in various instances of social learning and has been found to be employed in several intelligent species, including humans. In this paper, we investigate to what extent the explicit modelling of other agents is necessary to achieve observational learning through machine learning. Especially, we argue that observational learning can emerge from pure Reinforcement Learning (RL), potentially coupled with memory. Through simple scenarios, we demonstrate that an RL agent can leverage the information provided by the observations of an other agent performing a task in a shared environment. The other agent is only observed through the effect of its actions on the environment and never explicitly modeled. Two key aspects are borrowed from observational learning: i) the observer behaviour needs to change as a result of viewing a 'teacher' (another agent) and ii) the observer needs to be motivated somehow to engage in making use of the other agent's behaviour. The later is naturally modeled by RL, by correlating the learning agent's reward with the teacher agent's behaviour.",0
"Observational learning involves the process of observing, retaining and possibly replicating the behavior of another agent, and is a fundamental mechanism in social learning among intelligent species, including humans. This study aims to explore the extent to which explicit modeling of other agents is necessary for observational learning through machine learning. The study argues that pure Reinforcement Learning (RL) coupled with memory can facilitate observational learning. The study demonstrates through simple scenarios that an RL agent can leverage the information gathered from observing another agent's task performance in a shared environment without explicitly modeling the other agent. The study identifies two essential elements of observational learning: the observer's behavior must change after observing a ""teacher,"" and the observer must be motivated to utilize the teacher's behavior. The latter is naturally modeled by RL by linking the learning agent's reward to the teacher's behavior.",1
"Accurate traffic participant prediction is the prerequisite for collision avoidance of autonomous vehicles. In this work, we predict pedestrians by emulating their own motion planning. From online observations, we infer a mixture density function for possible destinations. We use this result as the goal states of a planning stage that performs motion prediction based on common behavior patterns. The entire system is modeled as one monolithic neural network and trained via inverse reinforcement learning. Experimental validation on real world data shows the system's ability to predict both, destinations and trajectories accurately.",0
"For autonomous vehicles to avoid collisions, it is essential to accurately predict the behavior of traffic participants. The focus of this study is on predicting the movement of pedestrians using their motion planning as a reference. By analyzing online observations, we create a mixture density function that represents potential destinations. This information is utilized in a planning stage where common behavior patterns are used to predict motion. The system is treated as a single neural network and trained using inverse reinforcement learning. Real-world data experiments confirm the system's capability to predict both destinations and trajectories with high precision.",1
"Node-perturbation learning is a type of statistical gradient descent algorithm that can be applied to problems where the objective function is not explicitly formulated, including reinforcement learning. It estimates the gradient of an objective function by using the change in the object function in response to the perturbation. The value of the objective function for an unperturbed output is called a baseline. Cho et al. proposed node-perturbation learning with a noisy baseline. In this paper, we report on building the statistical mechanics of Cho's model and on deriving coupled differential equations of order parameters that depict learning dynamics. We also show how to derive the generalization error by solving the differential equations of order parameters. On the basis of the results, we show that Cho's results are also apply in general cases and show some general performances of Cho's model.",0
"Node-perturbation learning is a statistical gradient descent algorithm that can be utilized for problems where the objective function is not explicitly defined, such as reinforcement learning. The algorithm approximates the gradient of the objective function by observing the change in the function when perturbed. The baseline is the value of the objective function for an unperturbed output. Cho et al. introduced node-perturbation learning with a noisy baseline. Our study presents the statistical mechanics of Cho's model and the coupled differential equations of order parameters that describe the learning process. Additionally, we demonstrate how to calculate the generalization error by solving the differential equations of order parameters. Our results indicate that Cho's model is applicable in general cases, and we provide an overview of its performance.",1
"Teaching a computer to read and answer general questions pertaining to a document is a challenging yet unsolved problem. In this paper, we describe a novel neural network architecture called the Reasoning Network (ReasoNet) for machine comprehension tasks. ReasoNets make use of multiple turns to effectively exploit and then reason over the relation among queries, documents, and answers. Different from previous approaches using a fixed number of turns during inference, ReasoNets introduce a termination state to relax this constraint on the reasoning depth. With the use of reinforcement learning, ReasoNets can dynamically determine whether to continue the comprehension process after digesting intermediate results, or to terminate reading when it concludes that existing information is adequate to produce an answer. ReasoNets have achieved exceptional performance in machine comprehension datasets, including unstructured CNN and Daily Mail datasets, the Stanford SQuAD dataset, and a structured Graph Reachability dataset.",0
"Developing a computer program that can read and answer general questions related to a document is a difficult issue that remains unsolved. This study introduces a new neural network architecture, the Reasoning Network (ReasoNet), designed to enhance machine comprehension tasks. Unlike previous approaches that use a fixed number of turns during inference, ReasoNets introduce a termination state that allows for more flexible reasoning depth. By using reinforcement learning, ReasoNets can dynamically decide whether to continue the comprehension process or terminate reading when it determines that the current information is sufficient to provide an answer. As a result, ReasoNets have achieved impressive performance on various machine comprehension datasets, including the unstructured CNN and Daily Mail datasets, the Stanford SQuAD dataset, and a structured Graph Reachability dataset.",1
"We propose the first multistage intervention framework that tackles fake news in social networks by combining reinforcement learning with a point process network activity model. The spread of fake news and mitigation events within the network is modeled by a multivariate Hawkes process with additional exogenous control terms. By choosing a feature representation of states, defining mitigation actions and constructing reward functions to measure the effectiveness of mitigation activities, we map the problem of fake news mitigation into the reinforcement learning framework. We develop a policy iteration method unique to the multivariate networked point process, with the goal of optimizing the actions for maximal total reward under budget constraints. Our method shows promising performance in real-time intervention experiments on a Twitter network to mitigate a surrogate fake news campaign, and outperforms alternatives on synthetic datasets.",0
"Our proposal presents a novel framework for addressing the issue of fake news in social media. Our approach combines reinforcement learning with a point process network activity model to create a multistage intervention plan. The spread of fake news and mitigation events in the network are modeled using a multivariate Hawkes process, along with additional exogenous control terms. By establishing a feature representation of states, defining mitigation actions, and creating reward functions to evaluate the effectiveness of mitigation activities, we can use the reinforcement learning framework to tackle the fake news problem. We have developed a policy iteration method that is specific to the multivariate networked point process, which optimizes actions for maximum total reward while adhering to budget constraints. Our approach has demonstrated promising results in real-time intervention experiments on a Twitter network, outperforming other methods on synthetic datasets.",1
"In this paper we combine one method for hierarchical reinforcement learning - the options framework - with deep Q-networks (DQNs) through the use of different ""option heads"" on the policy network, and a supervisory network for choosing between the different options. We utilise our setup to investigate the effects of architectural constraints in subtasks with positive and negative transfer, across a range of network capacities. We empirically show that our augmented DQN has lower sample complexity when simultaneously learning subtasks with negative transfer, without degrading performance when learning subtasks with positive transfer.",0
"This paper presents a combination of the options framework and deep Q-networks (DQNs) for hierarchical reinforcement learning. We achieve this by implementing various ""option heads"" on the policy network and a supervisory network for option selection. Through our approach, we examine the influence of architectural constraints on subtasks with positive and negative transfer, across various network capacities. Our experimental results demonstrate that our enhanced DQN requires less sample complexity when learning subtasks with negative transfer, while maintaining optimal performance in subtasks with positive transfer.",1
"This paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems. We also present the novel continual learning method of incremental learning, where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment. We show that incremental learning can produce vastly superior results than standard methods by providing a strong baseline method across ten Dex environments. We finally develop a saliency method for qualitative analysis of reinforcement learning, which shows the impact incremental learning has on network attention.",0
"Dex, a specialized reinforcement learning environment toolkit for training and evaluating continual learning methods and general reinforcement learning problems is introduced in this paper. Additionally, we introduce the incremental learning method for continual learning, which involves solving challenging environments using optimal weight initialization learned from solving similar easier environments. Our study demonstrates that incremental learning can produce significantly better results than standard methods, as evidenced by ten Dex environment experiments. Furthermore, we develop a saliency method for qualitative analysis of reinforcement learning, which reveals the impact of incremental learning on network attention.",1
"We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them---specifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). To learn from sketches, we present a model that associates every subtask with a modular subpolicy, and jointly maximizes reward over full task-specific policies by tying parameters across shared subpolicies. Optimization is accomplished via a decoupled actor--critic training objective that facilitates learning common behaviors from multiple dissimilar reward functions. We evaluate the effectiveness of our approach in three environments featuring both discrete and continuous control, and with sparse rewards that can be obtained only after completing a number of high-level subgoals. Experiments show that using our approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks.",0
"A framework is presented for multitask deep reinforcement learning that is guided by policy sketches. These sketches provide information about high-level structural relationships among tasks but do not provide the detailed guidance used in previous work on learning policy abstractions for RL. To learn from sketches, a model is introduced that associates every subtask with a modular subpolicy, and optimizes reward over full task-specific policies by tying parameters across shared subpolicies. The optimization is achieved through a decoupled actor-critic training objective that facilitates learning common behaviors from multiple dissimilar reward functions. The approach is evaluated in three environments with sparse rewards that can only be obtained after completing a number of high-level subgoals. The experiments demonstrate that using this approach to learn policies guided by sketches yields better performance than existing techniques for learning task-specific or shared policies. Furthermore, the approach naturally induces a library of interpretable primitive behaviors that can be recombined to quickly adapt to new tasks.",1
"Representation learning and option discovery are two of the biggest challenges in reinforcement learning (RL). Proto-value functions (PVFs) are a well-known approach for representation learning in MDPs. In this paper we address the option discovery problem by showing how PVFs implicitly define options. We do it by introducing eigenpurposes, intrinsic reward functions derived from the learned representations. The options discovered from eigenpurposes traverse the principal directions of the state space. They are useful for multiple tasks because they are discovered without taking the environment's rewards into consideration. Moreover, different options act at different time scales, making them helpful for exploration. We demonstrate features of eigenpurposes in traditional tabular domains as well as in Atari 2600 games.",0
"Reinforcement learning (RL) faces significant challenges with representation learning and option discovery. To tackle representation learning, Proto-value functions (PVFs) are commonly employed in Markov decision processes (MDPs). In this study, we focus on option discovery and propose that PVFs implicitly define options. To demonstrate this, we introduce eigenpurposes, intrinsic reward functions that arise from the learned representations. These options traverse the principal directions of the state space, making them versatile for various tasks since they are independent of the environment's rewards. Additionally, different options operate at varying time scales, making them useful for exploration. We showcase the effectiveness of eigenpurposes in both traditional tabular domains and Atari 2600 games.",1
"This paper presents the input convex neural network architecture. These are scalar-valued (potentially deep) neural networks with constraints on the network parameters such that the output of the network is a convex function of (some of) the inputs. The networks allow for efficient inference via optimization over some inputs to the network given others, and can be applied to settings including structured prediction, data imputation, reinforcement learning, and others. In this paper we lay the basic groundwork for these models, proposing methods for inference, optimization and learning, and analyze their representational power. We show that many existing neural network architectures can be made input-convex with a minor modification, and develop specialized optimization algorithms tailored to this setting. Finally, we highlight the performance of the methods on multi-label prediction, image completion, and reinforcement learning problems, where we show improvement over the existing state of the art in many cases.",0
"The input convex neural network architecture is introduced in this paper. These neural networks are scalar-valued and may have multiple layers. The network parameters are constrained to ensure that the output of the network is a convex function of some of the inputs. These networks enable efficient inference through optimization over some inputs given others. They have various applications, including structured prediction, data imputation, and reinforcement learning. The paper presents the fundamental concepts of these models, including methods for inference, optimization, and learning, and examines their representational power. The authors demonstrate that many existing neural network architectures can be modified to become input-convex, and they develop specialized optimization algorithms for this purpose. The article also showcases the performance of these methods on multi-label prediction, image completion, and reinforcement learning problems, demonstrating their superiority over existing state-of-the-art methods in many cases.",1
"We discuss the relative merits of optimistic and randomized approaches to exploration in reinforcement learning. Optimistic approaches presented in the literature apply an optimistic boost to the value estimate at each state-action pair and select actions that are greedy with respect to the resulting optimistic value function. Randomized approaches sample from among statistically plausible value functions and select actions that are greedy with respect to the random sample. Prior computational experience suggests that randomized approaches can lead to far more statistically efficient learning. We present two simple analytic examples that elucidate why this is the case. In principle, there should be optimistic approaches that fare well relative to randomized approaches, but that would require intractable computation. Optimistic approaches that have been proposed in the literature sacrifice statistical efficiency for the sake of computational efficiency. Randomized approaches, on the other hand, may enable simultaneous statistical and computational efficiency.",0
"The advantages of optimistic and randomized methods for exploration in reinforcement learning are compared. Optimistic methods involve increasing the value estimate for each state-action pair and choosing actions based on the resulting optimistic value function. Randomized methods, on the other hand, sample from plausible value functions and choose actions based on the resulting random sample. Previous studies suggest that randomized methods are more statistically efficient in learning. Two analytical examples are presented to explain why this is the case. While optimistic methods may be effective, they require impractical computation and sacrifice statistical efficiency for computational efficiency. In contrast, randomized methods can achieve both statistical and computational efficiency simultaneously.",1
"Computational results demonstrate that posterior sampling for reinforcement learning (PSRL) dramatically outperforms algorithms driven by optimism, such as UCRL2. We provide insight into the extent of this performance boost and the phenomenon that drives it. We leverage this insight to establish an $\tilde{O}(H\sqrt{SAT})$ Bayesian expected regret bound for PSRL in finite-horizon episodic Markov decision processes, where $H$ is the horizon, $S$ is the number of states, $A$ is the number of actions and $T$ is the time elapsed. This improves upon the best previous bound of $\tilde{O}(H S \sqrt{AT})$ for any reinforcement learning algorithm.",0
"The results of the computations show that PSRL is much more effective than optimism-based algorithms like UCRL2 in reinforcement learning. We investigate the reasons behind this performance enhancement and use our findings to establish a Bayesian expected regret bound for PSRL in finite-horizon episodic Markov decision processes. This bound is represented by $\tilde{O}(H\sqrt{SAT})$, where $H$ is the horizon, $S$ is the number of states, $A$ is the number of actions and $T$ is the time elapsed. Our bound is an improvement over the previous best result of $\tilde{O}(H S \sqrt{AT})$ for any reinforcement learning algorithm.",1
"Reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning (RL) agents in the real world. In many real-world tasks, designing a reward function takes considerable hand engineering and often requires additional sensors to be installed just to measure whether the task has been executed successfully. Furthermore, many interesting tasks consist of multiple implicit intermediate steps that must be executed in sequence. Even when the final outcome can be measured, it does not necessarily provide feedback on these intermediate steps. To address these issues, we propose leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations. We present a method that is able to identify key intermediate steps of a task from only a handful of demonstration sequences, and automatically identify the most discriminative features for identifying these steps. This method makes use of the features in a pre-trained deep model, but does not require any explicit specification of sub-goals. The resulting reward functions can then be used by an RL agent to learn to perform the task in real-world settings. To evaluate the learned reward, we present qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function. We also show that our method can be used to learn a real-world door opening skill using a real robot, even when the demonstration used for reward learning is provided by a human using their own hand. To our knowledge, these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task. Supplementary material and data are available at https://sermanet.github.io/rewards",0
"The main challenges in deploying reinforcement learning (RL) agents in real-world scenarios are the time required to design reward functions and the need to explore various possibilities. In many tasks, designing a reward function requires significant manual input and additional sensors to measure task execution. Additionally, certain tasks involve multiple implicit intermediate steps that must be executed sequentially, and measuring the final outcome does not provide feedback on these steps. To overcome these issues, we suggest utilizing the intermediate visual representations generated by deep models to quickly deduce perceptual reward functions from only a few demonstrations. Our method identifies key intermediate steps and the most distinctive features for recognizing them without explicit sub-goal specifications. The learned reward function can be applied by an RL agent to learn the task in real-world settings. We demonstrate the effectiveness of our approach on two real-world tasks and compare it with a human-designed reward function. We also showcase our technique by teaching a real robot to open doors using a human's hand as the demonstration. Our study is the first to demonstrate that complex robotic manipulation skills can be learned directly from a video of a human performing the task, without supervised labels. Supplementary materials and data are available at https://sermanet.github.io/rewards.",1
"We learn recurrent neural network optimizers trained on simple synthetic functions by gradient descent. We show that these learned optimizers exhibit a remarkable degree of transfer in that they can be used to efficiently optimize a broad range of derivative-free black-box functions, including Gaussian process bandits, simple control objectives, global optimization benchmarks and hyper-parameter tuning tasks. Up to the training horizon, the learned optimizers learn to trade-off exploration and exploitation, and compare favourably with heavily engineered Bayesian optimization packages for hyper-parameter tuning.",0
"Through the use of gradient descent, we educate recurrent neural network optimizers on basic synthetic functions. These optimizers demonstrate an impressive ability to transfer their skills, as they efficiently optimize a diverse range of derivative-free black-box functions such as Gaussian process bandits, global optimization benchmarks, simple control objectives, and hyper-parameter tuning tasks. During their training period, these optimizers learn how to balance exploration and exploitation, and are comparable to extensively designed Bayesian optimization packages for hyper-parameter tuning.",1
"In this paper we explore methods to exploit symmetries for ensuring sample efficiency in reinforcement learning (RL), this problem deserves ever increasing attention with the recent advances in the use of deep networks for complex RL tasks which require large amount of training data. We introduce a novel method to detect symmetries using reward trails observed during episodic experience and prove its completeness. We also provide a framework to incorporate the discovered symmetries for functional approximation. Finally we show that the use of potential based reward shaping is especially effective for our symmetry exploitation mechanism. Experiments on various classical problems show that our method improves the learning performance significantly by utilizing symmetry information.",0
"The focus of our paper is to investigate how symmetries can be utilized to increase efficiency in reinforcement learning (RL), a problem that is becoming increasingly crucial due to the use of deep networks for complex RL tasks that require substantial amounts of training data. To address this, we present a new method for detecting symmetries by analyzing reward trails from episodic experience, which we prove to be comprehensive. We also propose a framework for incorporating these symmetries into functional approximation and demonstrate the effectiveness of potential-based reward shaping for our approach. Our experiments on classical problems indicate that our method significantly enhances learning performance by leveraging symmetry information.",1
"Understanding the simultaneously very diverse and intricately fine-grained set of possible human actions is a critical open problem in computer vision. Manually labeling training videos is feasible for some action classes but doesn't scale to the full long-tailed distribution of actions. A promising way to address this is to leverage noisy data from web queries to learn new actions, using semi-supervised or ""webly-supervised"" approaches. However, these methods typically do not learn domain-specific knowledge, or rely on iterative hand-tuned data labeling policies. In this work, we instead propose a reinforcement learning-based formulation for selecting the right examples for training a classifier from noisy web search results. Our method uses Q-learning to learn a data labeling policy on a small labeled training dataset, and then uses this to automatically label noisy web data for new visual concepts. Experiments on the challenging Sports-1M action recognition benchmark as well as on additional fine-grained and newly emerging action classes demonstrate that our method is able to learn good labeling policies for noisy data and use this to learn accurate visual concept classifiers.",0
"A major challenge in computer vision is comprehending the diverse and complex array of possible human actions. While manual labeling of training videos may work for certain classes of actions, it is not practical for the complete range of actions. One solution is to employ semi-supervised or ""webly-supervised"" approaches that use noisy data from web queries to learn new actions. However, these methods often lack domain-specific knowledge and rely on iterative hand-tuned data labeling policies. This study proposes a reinforcement learning-based approach that selects appropriate examples for training a classifier from noisy web search results. The method applies Q-learning to learn a data labeling policy based on a small labeled training dataset, which is then used to automatically label noisy web data for new visual concepts. Tests on the Sports-1M action recognition benchmark and other emerging action classes indicate that the method can acquire effective labeling policies for noisy data and use them to develop accurate visual concept classifiers.",1
"Policy evaluation is a crucial step in many reinforcement-learning procedures, which estimates a value function that predicts states' long-term value under a given policy. In this paper, we focus on policy evaluation with linear function approximation over a fixed dataset. We first transform the empirical policy evaluation problem into a (quadratic) convex-concave saddle point problem, and then present a primal-dual batch gradient method, as well as two stochastic variance reduction methods for solving the problem. These algorithms scale linearly in both sample size and feature dimension. Moreover, they achieve linear convergence even when the saddle-point problem has only strong concavity in the dual variables but no strong convexity in the primal variables. Numerical experiments on benchmark problems demonstrate the effectiveness of our methods.",0
"Many reinforcement-learning procedures require policy evaluation, which involves estimating a value function to predict the long-term value of states under a given policy. This paper focuses on linear function approximation for policy evaluation using a fixed dataset. The authors convert the empirical policy evaluation problem into a convex-concave saddle point problem, which they solve using a primal-dual batch gradient method and two stochastic variance reduction methods. These algorithms have linear scalability in sample size and feature dimension, and they achieve linear convergence even when the saddle-point problem has strong concavity in the dual variables but no strong convexity in the primal variables. The authors conduct numerical experiments on benchmark problems to demonstrate the effectiveness of their methods.",1
"Using Reinforcement Learning (RL) in simulation to construct policies useful in real life is challenging. This is often attributed to the sequential decision making aspect: inaccuracies in simulation accumulate over multiple steps, hence the simulated trajectories diverge from what would happen in reality.   In our work we show the need to consider another important aspect: the mismatch in simulating control. We bring attention to the need for modeling control as well as dynamics, since oversimplifying assumptions about applying actions of RL policies could make the policies fail on real-world systems.   We design a simulator for solving a pivoting task (of interest in Robotics) and demonstrate that even a simple simulator designed with RL in mind outperforms high-fidelity simulators when it comes to learning a policy that is to be deployed on a real robotic system. We show that a phenomenon that is hard to model - friction - could be exploited successfully, even when RL is performed using a simulator with a simple dynamics and noise model. Hence, we demonstrate that as long as the main sources of uncertainty are identified, it could be possible to learn policies applicable to real systems even using a simple simulator.   RL-compatible simulators could open the possibilities for applying a wide range of RL algorithms in various fields. This is important, since currently data sparsity in fields like healthcare and education frequently forces researchers and engineers to only consider sample-efficient RL approaches. Successful simulator-aided RL could increase flexibility of experimenting with RL algorithms and help applying RL policies to real-world settings in fields where data is scarce. We believe that lessons learned in Robotics could help other fields design RL-compatible simulators, so we summarize our experience and conclude with suggestions.",0
"Constructing useful policies in real life using Reinforcement Learning (RL) in simulation is a challenging task due to the sequential decision making aspect. Simulation inaccuracies accumulate over multiple steps, leading to simulated trajectories that diverge from reality. However, our work highlights the importance of considering another critical factor, namely the mismatch in simulating control. Oversimplified assumptions about applying actions of RL policies can cause policy failures in real-world systems. To address this, we designed a simple simulator for solving a pivoting task in Robotics, which outperformed high-fidelity simulators in learning a policy for deployment on real robotic systems. We successfully exploited friction, a challenging phenomenon to model, even when using a simulator with a simple dynamics and noise model, demonstrating that identifying the primary sources of uncertainty can enable learning policies applicable to real systems. RL-compatible simulators could open up possibilities for applying a wide range of RL algorithms in various fields, especially in data-scarce domains such as healthcare and education. Our experience in Robotics can inform the design of RL-compatible simulators in other fields, and we conclude with suggestions for future work.",1
"Deep reinforcement learning has emerged as a promising and powerful technique for automatically acquiring control policies that can process raw sensory inputs, such as images, and perform complex behaviors. However, extending deep RL to real-world robotic tasks has proven challenging, particularly in safety-critical domains such as autonomous flight, where a trial-and-error learning process is often impractical. In this paper, we explore the following question: can we train vision-based navigation policies entirely in simulation, and then transfer them into the real world to achieve real-world flight without a single real training image? We propose a learning method that we call CAD$^2$RL, which can be used to perform collision-free indoor flight in the real world while being trained entirely on 3D CAD models. Our method uses single RGB images from a monocular camera, without needing to explicitly reconstruct the 3D geometry of the environment or perform explicit motion planning. Our learned collision avoidance policy is represented by a deep convolutional neural network that directly processes raw monocular images and outputs velocity commands. This policy is trained entirely on simulated images, with a Monte Carlo policy evaluation algorithm that directly optimizes the network's ability to produce collision-free flight. By highly randomizing the rendering settings for our simulated training set, we show that we can train a policy that generalizes to the real world, without requiring the simulator to be particularly realistic or high-fidelity. We evaluate our method by flying a real quadrotor through indoor environments, and further evaluate the design choices in our simulator through a series of ablation studies on depth prediction. For supplementary video see: https://youtu.be/nXBWmzFrj5s",0
"The technique of deep reinforcement learning has shown great potential for acquiring control policies that can process raw sensory inputs and perform complex tasks. However, applying this technique to real-world robotic tasks, especially in safety-critical domains like autonomous flight, can be difficult due to the impracticality of a trial-and-error learning process. This paper investigates the possibility of training vision-based navigation policies in simulation and transferring them to the real world for flight without any real training images. To achieve this, the authors propose a learning method called CAD$^2$RL, which can successfully perform collision-free indoor flight in the real world while being trained entirely on 3D CAD models. Their approach uses single RGB images from a monocular camera and a deep convolutional neural network to process the raw images and output velocity commands for collision avoidance. The policy is trained entirely on simulated images, and the authors show that it can generalize to the real world without requiring a highly realistic simulator. The authors evaluate their method by flying a real quadrotor through indoor environments and conducting ablation studies on depth prediction. A supplementary video is available for further reference.",1
"In several realistic situations, an interactive learning agent can practice and refine its strategy before going on to be evaluated. For instance, consider a student preparing for a series of tests. She would typically take a few practice tests to know which areas she needs to improve upon. Based of the scores she obtains in these practice tests, she would formulate a strategy for maximizing her scores in the actual tests. We treat this scenario in the context of an agent exploring a fixed-horizon episodic Markov Decision Process (MDP), where the agent can practice on the MDP for some number of episodes (not necessarily known in advance) before starting to incur regret for its actions.   During practice, the agent's goal must be to maximize the probability of following an optimal policy. This is akin to the problem of Pure Exploration (PE). We extend the PE problem of Multi Armed Bandits (MAB) to MDPs and propose a Bayesian algorithm called Posterior Sampling for Pure Exploration (PSPE), which is similar to its bandit counterpart. We show that the Bayesian simple regret converges at an optimal exponential rate when using PSPE.   When the agent starts being evaluated, its goal would be to minimize the cumulative regret incurred. This is akin to the problem of Reinforcement Learning (RL). The agent uses the Posterior Sampling for Reinforcement Learning algorithm (PSRL) initialized with the posteriors of the practice phase. We hypothesize that this PSPE + PSRL combination is an optimal strategy for minimizing regret in RL problems with an initial practice phase. We show empirical results which prove that having a lower simple regret at the end of the practice phase results in having lower cumulative regret during evaluation.",0
"An interactive learning agent can benefit from practicing and refining its strategy before evaluation in many realistic scenarios. For instance, a student may take practice tests to identify areas of improvement and formulate a strategy for maximizing scores in actual tests. In the context of a fixed-horizon episodic Markov Decision Process (MDP), an agent can practice for a certain number of episodes to maximize the probability of following an optimal policy. This is similar to the Pure Exploration (PE) problem of Multi Armed Bandits (MAB), which we extend to MDPs and propose a Bayesian algorithm called Posterior Sampling for Pure Exploration (PSPE). The Bayesian simple regret converges optimally when using PSPE. When evaluation begins, the agent aims to minimize cumulative regret, similar to Reinforcement Learning (RL). We suggest using the Posterior Sampling for Reinforcement Learning algorithm (PSRL) with posteriors from the practice phase. We hypothesize that combining PSPE and PSRL is an optimal strategy for minimizing regret in RL problems with an initial practice phase, as shown by empirical results demonstrating lower cumulative regret with lower simple regret at the end of practice.",1
"Off-policy model-free deep reinforcement learning methods using previously collected data can improve sample efficiency over on-policy policy gradient techniques. On the other hand, on-policy algorithms are often more stable and easier to use. This paper examines, both theoretically and empirically, approaches to merging on- and off-policy updates for deep reinforcement learning. Theoretical results show that off-policy updates with a value function estimator can be interpolated with on-policy policy gradient updates whilst still satisfying performance bounds. Our analysis uses control variate methods to produce a family of policy gradient algorithms, with several recently proposed algorithms being special cases of this family. We then provide an empirical comparison of these techniques with the remaining algorithmic details fixed, and show how different mixing of off-policy gradient estimates with on-policy samples contribute to improvements in empirical performance. The final algorithm provides a generalization and unification of existing deep policy gradient techniques, has theoretical guarantees on the bias introduced by off-policy updates, and improves on the state-of-the-art model-free deep RL methods on a number of OpenAI Gym continuous control benchmarks.",0
"This paper explores the combination of on- and off-policy updates for deep reinforcement learning, both theoretically and practically. While off-policy methods using previous data can enhance sample efficiency, on-policy algorithms are often more stable and user-friendly. The authors demonstrate that it is possible to merge off-policy updates with a value function estimator and on-policy policy gradient updates, satisfying performance bounds. They use control variate techniques to create a family of policy gradient algorithms, including recently proposed ones, and compare their efficacy empirically. The results show that different combinations of off-policy gradient estimates with on-policy samples can enhance performance. The final algorithm presents a generalization and unification of existing deep policy gradient methods, with theoretical guarantees on the bias introduced by off-policy updates, and outperforms the state-of-the-art model-free deep RL methods on several OpenAI Gym continuous control benchmarks.",1
"Stochastic gradient descent (SGD), which updates the model parameters by adding a local gradient times a learning rate at each step, is widely used in model training of machine learning algorithms such as neural networks. It is observed that the models trained by SGD are sensitive to learning rates and good learning rates are problem specific. We propose an algorithm to automatically learn learning rates using neural network based actor-critic methods from deep reinforcement learning (RL).In particular, we train a policy network called actor to decide the learning rate at each step during training, and a value network called critic to give feedback about quality of the decision (e.g., the goodness of the learning rate outputted by the actor) that the actor made. The introduction of auxiliary actor and critic networks helps the main network achieve better performance. Experiments on different datasets and network architectures show that our approach leads to better convergence of SGD than human-designed competitors.",0
"Stochastic gradient descent (SGD) is a popular model training method for machine learning algorithms, including neural networks. However, the effectiveness of SGD is dependent on learning rates, which are specific to each problem. To address this issue, we propose an algorithm that uses neural network-based actor-critic methods from deep reinforcement learning (RL) to automatically learn learning rates. Our approach involves training an actor policy network to determine the learning rate during training and a critic value network to provide feedback on the quality of the actor's decision. The auxiliary actor and critic networks improve the performance of the main network. Our experiments on various datasets and network architectures demonstrate that our approach leads to better SGD convergence than human-designed competitors.",1
"Partially observable environments present an important open challenge in the domain of sequential control learning with delayed rewards. Despite numerous attempts during the two last decades, the majority of reinforcement learning algorithms and associated approximate models, applied to this context, still assume Markovian state transitions. In this paper, we explore the use of a recently proposed attention-based model, the Gated End-to-End Memory Network, for sequential control. We call the resulting model the Gated End-to-End Memory Policy Network. More precisely, we use a model-free value-based algorithm to learn policies for partially observed domains using this memory-enhanced neural network. This model is end-to-end learnable and it features unbounded memory. Indeed, because of its attention mechanism and associated non-parametric memory, the proposed model allows us to define an attention mechanism over the observation stream unlike recurrent models. We show encouraging results that illustrate the capability of our attention-based model in the context of the continuous-state non-stationary control problem of stock trading. We also present an OpenAI Gym environment for simulated stock exchange and explain its relevance as a benchmark for the field of non-Markovian decision process learning.",0
"The challenge of learning sequential control in environments with delayed rewards becomes more complex when the environment is only partially observable. Despite many attempts over the last two decades, most reinforcement learning algorithms and approximate models still assume Markovian state transitions. This paper explores a new approach to this challenge, using an attention-based model called the Gated End-to-End Memory Network. This model is end-to-end learnable, has unbounded memory, and allows for an attention mechanism over the observation stream. We demonstrate the effectiveness of this model in the context of stock trading, using a model-free value-based algorithm to learn policies for partially observed domains. We also present an OpenAI Gym environment for simulated stock exchange, which serves as a benchmark for non-Markovian decision process learning.",1
"Deep neural network (DNN) based approaches hold significant potential for reinforcement learning (RL) and have already shown remarkable gains over state-of-art methods in a number of applications. The effectiveness of DNN methods can be attributed to leveraging the abundance of supervised data to learn value functions, Q-functions, and policy function approximations without the need for feature engineering. Nevertheless, the deployment of DNN-based predictors with very deep architectures can pose an issue due to computational and other resource constraints at test-time in a number of applications. We propose a novel approach for reducing the average latency by learning a computationally efficient gating function that is capable of recognizing states in a sequential decision process for which policy prescriptions of a shallow network suffices and deeper layers of the DNN have little marginal utility. The overall system is adaptive in that it dynamically switches control actions based on state-estimates in order to reduce average latency without sacrificing terminal performance. We experiment with a number of alternative loss-functions to train gating functions and shallow policies and show that in a number of applications a speed-up of up to almost 5X can be obtained with little loss in performance.",0
"DNN-based techniques have shown great potential for RL, surpassing state-of-the-art methods in various applications. This is due to their ability to learn value functions, Q-functions, and policy function approximations without feature engineering, thanks to the abundance of supervised data. However, the use of DNN-based predictors with deep architectures can be challenging due to computational and resource constraints during testing. To address this, we propose a new approach that reduces average latency by learning a computationally efficient gating function that recognizes states where shallow networks suffice and deeper layers have little marginal utility. This system is adaptive, switching control actions based on state-estimates to reduce latency without sacrificing terminal performance. We experimented with different loss-functions and found that this approach can achieve almost 5X speed-up with minimal loss in performance in various applications.",1
"For many applications of reinforcement learning it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. For example, systems that physically interact with or around humans should satisfy safety constraints. Recent advances in policy search algorithms (Mnih et al., 2016, Schulman et al., 2015, Lillicrap et al., 2016, Levine et al., 2016) have enabled new capabilities in high-dimensional control, but do not consider the constrained setting.   We propose Constrained Policy Optimization (CPO), the first general-purpose policy search algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration. Our method allows us to train neural network policies for high-dimensional control while making guarantees about policy behavior all throughout training. Our guarantees are based on a new theoretical result, which is of independent interest: we prove a bound relating the expected returns of two policies to an average divergence between them. We demonstrate the effectiveness of our approach on simulated robot locomotion tasks where the agent must satisfy constraints motivated by safety.",0
"Instead of relying solely on a reward function, it can be more practical for certain reinforcement learning applications to specify both reward functions and constraints. For instance, systems that interact with humans require safety constraints. While recent advancements in policy search algorithms have enabled high-dimensional control, they do not address the constrained setting. In response, we propose Constrained Policy Optimization (CPO), a general-purpose policy search algorithm that guarantees near-constraint satisfaction at every iteration. Our approach allows us to train neural network policies for high-dimensional control while ensuring policy behavior throughout training. We establish a new theoretical result that proves a bound between two policies' expected returns and their average divergence. Through simulated robot locomotion tasks, we demonstrate the effectiveness of our approach in complying with safety constraints.",1
"Recent theoretical and experimental results suggest the possibility of using current and near-future quantum hardware in challenging sampling tasks. In this paper, we introduce free energy-based reinforcement learning (FERL) as an application of quantum hardware. We propose a method for processing a quantum annealer's measured qubit spin configurations in approximating the free energy of a quantum Boltzmann machine (QBM). We then apply this method to perform reinforcement learning on the grid-world problem using the D-Wave 2000Q quantum annealer. The experimental results show that our technique is a promising method for harnessing the power of quantum sampling in reinforcement learning tasks.",0
It has been suggested by recent theoretical and experimental findings that utilizing current and soon-to-be-available quantum hardware may be feasible for difficult sampling endeavors. This article presents the concept of free energy-based reinforcement learning (FERL) as a possible use of quantum hardware. Our proposed approach involves utilizing a quantum annealer's recorded qubit spin configurations to approximate the free energy of a quantum Boltzmann machine (QBM). We then utilize this approach to execute reinforcement learning on the grid-world problem using the D-Wave 2000Q quantum annealer. The obtained experimental results indicate that our technique demonstrates the potential for effectively utilizing quantum sampling in reinforcement learning tasks.,1
"In this paper, we present a novel method of no-reference image quality assessment (NR-IQA), which is to predict the perceptual quality score of a given image without using any reference image. The proposed method harnesses three functions (i) the visual attention mechanism, which affects many aspects of visual perception including image quality assessment, however, is overlooked in the NR-IQA literature. The method assumes that the fixation areas on an image contain key information to the process of IQA. (ii) the robust averaging strategy, which is a means \--- supported by psychology studies \--- to integrating multiple/step-wise evidence to make a final perceptual judgment. (iii) the multi-task learning, which is believed to be an effectual means to shape representation learning and could result in a more generalized model.   To exploit the synergy of the three, we consider the NR-IQA as a dynamic perception process, in which the model samples a sequence of ""informative"" areas and aggregates the information to learn a representation for the tasks of jointly predicting the image quality score and the distortion type.   The model learning is implemented by a reinforcement strategy, in which the rewards of both tasks guide the learning of the optimal sampling policy to acquire the ""task-informative"" image regions so that the predictions can be made accurately and efficiently (in terms of the sampling steps). The reinforcement learning is realized by a deep network with the policy gradient method and trained through back-propagation.   In experiments, the model is tested on the TID2008 dataset and it outperforms several state-of-the-art methods. Furthermore, the model is very efficient in the sense that a small number of fixations are used in NR-IQA.",0
"The aim of this paper is to introduce a new approach to no-reference image quality assessment (NR-IQA) that predicts the quality score of an image without using any reference image. Our approach incorporates three functions that have been overlooked in previous NR-IQA literature. Firstly, we utilize the visual attention mechanism which is known to play a crucial role in image quality assessment. Secondly, we employ the robust averaging strategy which integrates multiple evidence to make a final perceptual judgment. Lastly, we use multi-task learning to shape representation learning and create a more generalized model. Our proposed approach treats NR-IQA as a dynamic perception process that samples informative areas and aggregates information for the tasks of predicting image quality score and distortion type. We implement a reinforcement learning strategy to guide the learning of the optimal sampling policy using a deep network with the policy gradient method and back-propagation. Our experiments on the TID2008 dataset show that our model outperforms several state-of-the-art methods while using a small number of fixations in NR-IQA.",1
"Recent advances in combining deep neural network architectures with reinforcement learning techniques have shown promising potential results in solving complex control problems with high dimensional state and action spaces. Inspired by these successes, in this paper, we build two kinds of reinforcement learning algorithms: deep policy-gradient and value-function based agents which can predict the best possible traffic signal for a traffic intersection. At each time step, these adaptive traffic light control agents receive a snapshot of the current state of a graphical traffic simulator and produce control signals. The policy-gradient based agent maps its observation directly to the control signal, however the value-function based agent first estimates values for all legal control signals. The agent then selects the optimal control action with the highest value. Our methods show promising results in a traffic network simulated in the SUMO traffic simulator, without suffering from instability issues during the training process.",0
"Combining deep neural network architectures with reinforcement learning techniques has recently shown potential in solving complex control problems with high dimensional state and action spaces. Inspired by these successes, our paper introduces two reinforcement learning algorithms: deep policy-gradient and value-function based agents that can determine the best traffic signal for a traffic intersection. These adaptive traffic light control agents receive a snapshot of the current state of a graphical traffic simulator at each time step and produce control signals. While the policy-gradient based agent maps its observation directly to the control signal, the value-function based agent first estimates values for all legal control signals and then selects the optimal control action with the highest value. Our methods have shown promising results in a traffic network simulated in the SUMO traffic simulator, without experiencing instability issues during the training process.",1
"Deep reinforcement learning algorithms have been shown to learn complex tasks using highly general policy classes. However, sparse reward problems remain a significant challenge. Exploration methods based on novelty detection have been particularly successful in such settings but typically require generative or predictive models of the observations, which can be difficult to train when the observations are very high-dimensional and complex, as in the case of raw images. We propose a novelty detection algorithm for exploration that is based entirely on discriminatively trained exemplar models, where classifiers are trained to discriminate each visited state against all others. Intuitively, novel states are easier to distinguish against other states seen during training. We show that this kind of discriminative modeling corresponds to implicit density estimation, and that it can be combined with count-based exploration to produce competitive results on a range of popular benchmark tasks, including state-of-the-art results on challenging egocentric observations in the vizDoom benchmark.",0
"Although deep reinforcement learning algorithms have demonstrated their ability to learn complex tasks using highly general policy classes, sparse reward problems still pose a significant challenge. While exploration methods based on novelty detection have been successful in addressing this issue, they typically rely on generative or predictive models of high-dimensional and complex observations, such as raw images, which can be difficult to train. To address this challenge, we propose a novelty detection algorithm for exploration that is solely based on discriminatively trained exemplar models. This approach involves training classifiers to distinguish each visited state from all others, with the idea that novel states are easier to differentiate from other states seen during training. We demonstrate that this type of discriminative modeling corresponds to implicit density estimation, and that it can be effectively combined with count-based exploration to achieve competitive results on various popular benchmark tasks, including state-of-the-art performance on challenging egocentric observations in the vizDoom benchmark.",1
"In recent years, model-free methods that use deep learning have achieved great success in many different reinforcement learning environments. Most successful approaches focus on solving a single task, while multi-task reinforcement learning remains an open problem. In this paper, we present a model based approach to deep reinforcement learning which we use to solve different tasks simultaneously. We show that our approach not only does not degrade but actually benefits from learning multiple tasks. For our model, we also present a new kind of recurrent neural network inspired by residual networks that decouples memory from computation allowing to model complex environments that do not require lots of memory.",0
"Deep learning-based model-free methods have gained remarkable success in various reinforcement learning environments in recent years. However, these approaches mainly concentrate on resolving a single task, and multi-task reinforcement learning is still an unresolved issue. This study introduces a model-based approach to deep reinforcement learning, which concurrently solves multiple tasks. Our findings indicate that learning multiple tasks does not deteriorate but enhances the performance of our approach. Moreover, we propose a novel type of recurrent neural network inspired by residual networks that separates memory from computation, facilitating the modeling of complicated environments with minimal memory usage.",1
"Sepsis is a leading cause of mortality in intensive care units (ICUs) and costs hospitals billions annually. Treating a septic patient is highly challenging, because individual patients respond very differently to medical interventions and there is no universally agreed-upon treatment for sepsis. Understanding more about a patient's physiological state at a given time could hold the key to effective treatment policies. In this work, we propose a new approach to deduce optimal treatment policies for septic patients by using continuous state-space models and deep reinforcement learning. Learning treatment policies over continuous spaces is important, because we retain more of the patient's physiological information. Our model is able to learn clinically interpretable treatment policies, similar in important aspects to the treatment policies of physicians. Evaluating our algorithm on past ICU patient data, we find that our model could reduce patient mortality in the hospital by up to 3.6% over observed clinical policies, from a baseline mortality of 13.7%. The learned treatment policies could be used to aid intensive care clinicians in medical decision making and improve the likelihood of patient survival.",0
"Intensive care units (ICUs) face a significant challenge in treating septic patients, which is a leading cause of mortality and incurs billions in hospital costs annually. The absence of a universally accepted treatment for sepsis and the varying response of patients towards medical interventions make the task highly challenging. However, understanding a patient's physiological state at a given time could lead to effective treatment policies. This study proposes a novel approach that leverages continuous state-space models and deep reinforcement learning to deduce optimal treatment policies for septic patients. The method aims to preserve more of the patient's physiological information by learning treatment policies over continuous spaces. The proposed model generates clinically interpretable treatment policies that are similar to those of physicians. By evaluating the algorithm on past ICU patient data, the study shows that the proposed model could potentially reduce patient mortality by up to 3.6% compared to observed clinical policies, from a baseline mortality rate of 13.7%. The proposed approach could assist intensive care clinicians in making better medical decisions and increase the likelihood of patient survival.",1
"Dropout is used as a practical tool to obtain uncertainty estimates in large vision models and reinforcement learning (RL) tasks. But to obtain well-calibrated uncertainty estimates, a grid-search over the dropout probabilities is necessary - a prohibitive operation with large models, and an impossible one with RL. We propose a new dropout variant which gives improved performance and better calibrated uncertainties. Relying on recent developments in Bayesian deep learning, we use a continuous relaxation of dropout's discrete masks. Together with a principled optimisation objective, this allows for automatic tuning of the dropout probability in large models, and as a result faster experimentation cycles. In RL this allows the agent to adapt its uncertainty dynamically as more data is observed. We analyse the proposed variant extensively on a range of tasks, and give insights into common practice in the field where larger dropout probabilities are often used in deeper model layers.",0
"In order to obtain reliable uncertainty estimates in large vision models and reinforcement learning tasks, dropout is commonly utilized. However, achieving well-calibrated uncertainty estimates requires a grid-search of dropout probabilities, which is impractical for large models and impossible for RL. To address this issue, we introduce a new variant of dropout that yields better performance and more accurate uncertainty estimates. Leveraging recent advancements in Bayesian deep learning, our approach uses a continuous relaxation of dropout's discrete masks and a principled optimization objective to automatically adjust dropout probabilities in large models, leading to faster experimentation cycles. Additionally, our method enables RL agents to adapt their uncertainty estimates in real-time as more data is acquired. Through extensive analysis on various tasks, we provide insights into the common practice of using larger dropout probabilities in deeper model layers.",1
"We propose a general framework for entropy-regularized average-reward reinforcement learning in Markov decision processes (MDPs). Our approach is based on extending the linear-programming formulation of policy optimization in MDPs to accommodate convex regularization functions. Our key result is showing that using the conditional entropy of the joint state-action distributions as regularization yields a dual optimization problem closely resembling the Bellman optimality equations. This result enables us to formalize a number of state-of-the-art entropy-regularized reinforcement learning algorithms as approximate variants of Mirror Descent or Dual Averaging, and thus to argue about the convergence properties of these methods. In particular, we show that the exact version of the TRPO algorithm of Schulman et al. (2015) actually converges to the optimal policy, while the entropy-regularized policy gradient methods of Mnih et al. (2016) may fail to converge to a fixed point. Finally, we illustrate empirically the effects of using various regularization techniques on learning performance in a simple reinforcement learning setup.",0
"Our study presents a comprehensive structure for entropy-regularized average-reward reinforcement learning in Markov decision processes (MDPs). We expand the linear-programming approach to policy optimization in MDPs, allowing for convex regularization functions. Our research demonstrates that using the conditional entropy of joint state-action distributions as regularization results in a dual optimization problem similar to the Bellman optimality equations. As a result, we can formulate various state-of-the-art entropy-regularized reinforcement learning algorithms as approximate versions of Mirror Descent or Dual Averaging and analyze their convergence properties. Specifically, we prove that the TRPO algorithm of Schulman et al. (2015) converges to the optimal policy, while the entropy-regularized policy gradient methods of Mnih et al. (2016) may not converge to a fixed point. Finally, we provide empirical evidence of the impact of different regularization techniques on learning performance in a basic reinforcement learning scenario.",1
"Online reinforcement learning (RL) is increasingly popular for the personalized mobile health (mHealth) intervention. It is able to personalize the type and dose of interventions according to user's ongoing statuses and changing needs. However, at the beginning of online learning, there are usually too few samples to support the RL updating, which leads to poor performances. A delay in good performance of the online learning algorithms can be especially detrimental in the mHealth, where users tend to quickly disengage with the mHealth app. To address this problem, we propose a new online RL methodology that focuses on an effective warm start. The main idea is to make full use of the data accumulated and the decision rule achieved in a former study. As a result, we can greatly enrich the data size at the beginning of online learning in our method. Such case accelerates the online learning process for new users to achieve good performances not only at the beginning of online learning but also through the whole online learning process. Besides, we use the decision rules achieved in a previous study to initialize the parameter in our online RL model for new users. It provides a good initialization for the proposed online RL algorithm. Experiment results show that promising improvements have been achieved by our method compared with the state-of-the-art method.",0
"The use of online reinforcement learning (RL) for personalized mobile health (mHealth) interventions is becoming increasingly popular. This approach allows for the customization of interventions based on the user's ongoing statuses and changing needs. However, in the initial stages of online learning, there may not be enough samples to support RL updating, resulting in poor performance. This delay in good performance can be particularly harmful in mHealth, where users may quickly disengage from the app. To address this issue, we propose a new online RL methodology that focuses on an effective warm start. Our approach utilizes the data and decision rules from a previous study, enabling us to significantly increase the data size at the beginning of online learning. This accelerates the online learning process, leading to good performance not only at the start but throughout the entire process. Additionally, we use the decision rules achieved in the previous study to initialize the parameter in our online RL model for new users. This provides a good starting point for the proposed online RL algorithm. Our method has shown promising improvements compared to the state-of-the-art method, according to experiment results.",1
"Deep Reinforcement Learning (DRL) methods have performed well in an increasing numbering of high-dimensional visual decision making domains. Among all such visual decision making problems, those with discrete action spaces often tend to have underlying compositional structure in the said action space. Such action spaces often contain actions such as go left, go up as well as go diagonally up and left (which is a composition of the former two actions). The representations of control policies in such domains have traditionally been modeled without exploiting this inherent compositional structure in the action spaces. We propose a new learning paradigm, Factored Action space Representations (FAR) wherein we decompose a control policy learned using a Deep Reinforcement Learning Algorithm into independent components, analogous to decomposing a vector in terms of some orthogonal basis vectors. This architectural modification of the control policy representation allows the agent to learn about multiple actions simultaneously, while executing only one of them. We demonstrate that FAR yields considerable improvements on top of two DRL algorithms in Atari 2600: FARA3C outperforms A3C (Asynchronous Advantage Actor Critic) in 9 out of 14 tasks and FARAQL outperforms AQL (Asynchronous n-step Q-Learning) in 9 out of 13 tasks.",0
"Increasingly, Deep Reinforcement Learning (DRL) methods have shown success in high-dimensional visual decision making scenarios. Visual decision making problems with discrete action spaces often have a compositional structure within the action space, including actions like ""go left"" and ""go up"" as well as more complex actions that combine the former two, such as ""go diagonally up and left."" However, traditional representations of control policies in these domains do not take advantage of this inherent structure. To address this, we propose a new learning approach called Factored Action space Representations (FAR), which breaks down a control policy learned using a DRL algorithm into independent components, similar to decomposing a vector with orthogonal basis vectors. This modification allows the agent to learn multiple actions simultaneously while executing only one of them. We demonstrate that FAR significantly improves upon two DRL algorithms in Atari 2600, with FARA3C outperforming A3C in 9 out of 14 tasks and FARAQL outperforming AQL in 9 out of 13 tasks.",1
"Adversarial examples have been shown to exist for a variety of deep learning architectures. Deep reinforcement learning has shown promising results on training agent policies directly on raw inputs such as image pixels. In this paper we present a novel study into adversarial attacks on deep reinforcement learning polices. We compare the effectiveness of the attacks using adversarial examples vs. random noise. We present a novel method for reducing the number of times adversarial examples need to be injected for a successful attack, based on the value function. We further explore how re-training on random noise and FGSM perturbations affects the resilience against adversarial examples.",0
"The existence of adversarial examples has been demonstrated across various deep learning architectures. Deep reinforcement learning has shown potential in training agent policies using raw inputs like image pixels. This study introduces a new investigation into adversarial attacks on deep reinforcement learning policies. We evaluate the efficacy of attacks using adversarial examples and random noise, and suggest a new approach for reducing the frequency of injecting adversarial examples to achieve a successful attack based on the value function. Additionally, we examine the impact of re-training on random noise and FGSM perturbations and its effect on resilience against adversarial examples.",1
"We propose a novel framework for efficient parallelization of deep reinforcement learning algorithms, enabling these algorithms to learn from multiple actors on a single machine. The framework is algorithm agnostic and can be applied to on-policy, off-policy, value based and policy gradient based algorithms. Given its inherent parallelism, the framework can be efficiently implemented on a GPU, allowing the usage of powerful models while significantly reducing training time. We demonstrate the effectiveness of our framework by implementing an advantage actor-critic algorithm on a GPU, using on-policy experiences and employing synchronous updates. Our algorithm achieves state-of-the-art performance on the Atari domain after only a few hours of training. Our framework thus opens the door for much faster experimentation on demanding problem domains. Our implementation is open-source and is made public at https://github.com/alfredvc/paac",0
"Our innovative approach proposes a framework that optimizes the parallelization of deep reinforcement learning algorithms, which facilitates learning from multiple actors on a single machine. The framework is flexible and can be employed with on-policy, off-policy, value-based, and policy gradient-based algorithms. The framework's inherent parallelism makes it possible to efficiently use GPU, which enables the use of powerful models while significantly reducing the training time. To demonstrate the effectiveness of our framework, we implemented an advantage actor-critic algorithm on a GPU, utilizing on-policy experiences and synchronous updates. Our approach achieved state-of-the-art performance on the Atari domain after a few hours of training. This framework makes it possible to conduct experiments at a much faster pace in challenging problem domains. Our implementation is open-source and can be accessed at https://github.com/alfredvc/paac.",1
"This article provides the first survey of computational models of emotion in reinforcement learning (RL) agents. The survey focuses on agent/robot emotions, and mostly ignores human user emotions. Emotions are recognized as functional in decision-making by influencing motivation and action selection. Therefore, computational emotion models are usually grounded in the agent's decision making architecture, of which RL is an important subclass. Studying emotions in RL-based agents is useful for three research fields. For machine learning (ML) researchers, emotion models may improve learning efficiency. For the interactive ML and human-robot interaction (HRI) community, emotions can communicate state and enhance user investment. Lastly, it allows affective modelling (AM) researchers to investigate their emotion theories in a successful AI agent class. This survey provides background on emotion theory and RL. It systematically addresses 1) from what underlying dimensions (e.g., homeostasis, appraisal) emotions can be derived and how these can be modelled in RL-agents, 2) what types of emotions have been derived from these dimensions, and 3) how these emotions may either influence the learning efficiency of the agent or be useful as social signals. We also systematically compare evaluation criteria, and draw connections to important RL sub-domains like (intrinsic) motivation and model-based RL. In short, this survey provides both a practical overview for engineers wanting to implement emotions in their RL agents, and identifies challenges and directions for future emotion-RL research.",0
"The initial scope of this article is to present the first examination of computational models of emotion in reinforcement learning (RL) agents, focusing on the emotions of agents/robots instead of human user emotions. Emotions are recognized as functional in decision-making by influencing motivation and action selection, and thus, computational emotion models are typically embedded in the agent's decision-making architecture, with RL being a crucial subclass. Studying emotions in RL-based agents benefits three research fields: machine learning (ML), interactive ML and human-robot interaction (HRI), and affective modelling (AM) researchers. This survey offers insight into emotion theory and RL, methodically addressing how emotions can be derived from underlying dimensions and modelled in RL-agents, what types of emotions have been derived, and how they can affect agent learning efficiency or function as social signals. Furthermore, it compares evaluation criteria and connects to significant RL sub-domains such as (intrinsic) motivation and model-based RL. Overall, this survey is a practical guide for engineers implementing emotions in their RL agents, while also identifying future challenges and research directions.",1
"Portfolio management is the decision-making process of allocating an amount of fund into different financial investment products. Cryptocurrencies are electronic and decentralized alternatives to government-issued money, with Bitcoin as the best-known example of a cryptocurrency. This paper presents a model-less convolutional neural network with historic prices of a set of financial assets as its input, outputting portfolio weights of the set. The network is trained with 0.7 years' price data from a cryptocurrency exchange. The training is done in a reinforcement manner, maximizing the accumulative return, which is regarded as the reward function of the network. Backtest trading experiments with trading period of 30 minutes is conducted in the same market, achieving 10-fold returns in 1.8 months' periods. Some recently published portfolio selection strategies are also used to perform the same back-tests, whose results are compared with the neural network. The network is not limited to cryptocurrency, but can be applied to any other financial markets.",0
"The process of portfolio management involves deciding how to distribute a sum of money among various financial investments. Cryptocurrencies, such as Bitcoin, are digital and decentralized forms of currency that serve as an alternative to government-issued money. This study introduces a model-less convolutional neural network that utilizes historical prices of a group of financial assets as input and produces portfolio weights as output. The network is trained with 0.7 years' worth of price data from a cryptocurrency exchange and optimized through reinforcement learning to maximize cumulative return. Backtesting experiments with a trading period of 30 minutes were conducted in the same market, resulting in a 10-fold return over 1.8 months. Other recently published portfolio selection strategies were also tested and compared to the neural network. This network is not restricted to cryptocurrency and can be applied to any financial market.",1
"We present a new deep meta reinforcement learner, which we call Deep Episodic Value Iteration (DEVI). DEVI uses a deep neural network to learn a similarity metric for a non-parametric model-based reinforcement learning algorithm. Our model is trained end-to-end via back-propagation. Despite being trained using the model-free Q-learning objective, we show that DEVI's model-based internal structure provides `one-shot' transfer to changes in reward and transition structure, even for tasks with very high-dimensional state spaces.",0
"Introducing Deep Episodic Value Iteration (DEVI), a novel deep meta reinforcement learning system that utilizes a deep neural network to acquire a similarity metric for a non-parametric model-based reinforcement learning algorithm. Our approach trains the model end-to-end using back-propagation, despite leveraging the model-free Q-learning objective. We demonstrate that DEVI's model-based internal architecture enables seamless adaptation to alterations in reward and transition structure, even for tasks with high-dimensional state spaces, with the added benefit of requiring only one-shot transfer.",1
"There has been a recent explosion in the capabilities of game-playing artificial intelligence. Many classes of RL tasks, from Atari games to motor control to board games, are now solvable by fairly generic algorithms, based on deep learning, that learn to play from experience with minimal knowledge of the specific domain of interest. In this work, we will investigate the performance of these methods on Super Smash Bros. Melee (SSBM), a popular console fighting game. The SSBM environment has complex dynamics and partial observability, making it challenging for human and machine alike. The multi-player aspect poses an additional challenge, as the vast majority of recent advances in RL have focused on single-agent environments. Nonetheless, we will show that it is possible to train agents that are competitive against and even surpass human professionals, a new result for the multi-player video game setting.",0
"Game-playing artificial intelligence has recently experienced a significant advancement in its capabilities. With the help of deep learning, generic algorithms can now solve various RL tasks, including Atari games, motor control, and board games with minimal domain knowledge. This study aims to explore the effectiveness of these methods in Super Smash Bros. Melee (SSBM), a complex console fighting game with partial observability and intricate dynamics. The multiplayer aspect of the game poses an additional challenge as most RL advancements have been in single-agent environments. Nevertheless, this research demonstrates that it is feasible to train agents that can compete against and even outperform human professionals, marking a breakthrough in the multiplayer video game environment.",1
"Many machine learning systems are built to solve the hardest examples of a particular task, which often makes them large and expensive to run---especially with respect to the easier examples, which might require much less computation. For an agent with a limited computational budget, this ""one-size-fits-all"" approach may result in the agent wasting valuable computation on easy examples, while not spending enough on hard examples. Rather than learning a single, fixed policy for solving all instances of a task, we introduce a metacontroller which learns to optimize a sequence of ""imagined"" internal simulations over predictive models of the world in order to construct a more informed, and more economical, solution. The metacontroller component is a model-free reinforcement learning agent, which decides both how many iterations of the optimization procedure to run, as well as which model to consult on each iteration. The models (which we call ""experts"") can be state transition models, action-value functions, or any other mechanism that provides information useful for solving the task, and can be learned on-policy or off-policy in parallel with the metacontroller. When the metacontroller, controller, and experts were trained with ""interaction networks"" (Battaglia et al., 2016) as expert models, our approach was able to solve a challenging decision-making problem under complex non-linear dynamics. The metacontroller learned to adapt the amount of computation it performed to the difficulty of the task, and learned how to choose which experts to consult by factoring in both their reliability and individual computational resource costs. This allowed the metacontroller to achieve a lower overall cost (task loss plus computational cost) than more traditional fixed policy approaches. These results demonstrate that our approach is a powerful framework for using...",0
"To solve a particular task, many machine learning systems are designed to tackle the most difficult examples, which often results in them being large and costly to operate. This approach can be inefficient as it may lead to the agent wasting computational resources on simple examples while not dedicating enough resources to challenging ones. Rather than adopting a ""one-size-fits-all"" strategy, our approach introduces a metacontroller that utilizes a series of imagined internal simulations to optimize predictive models of the world, thus creating a more economical and informed solution. The metacontroller is a model-free reinforcement learning agent that determines both the number of optimization iterations required and the model to consult at each iteration. The experts, which can be state transition models, action-value functions, or any other useful mechanism for the task, are learned on-policy or off-policy in parallel with the metacontroller. Our approach, utilizing interaction networks as expert models, was able to solve a complex decision-making problem with non-linear dynamics. The metacontroller learned to adjust the computation performed to the task's difficulty, and to select experts by considering their reliability and individual computational costs. Our method proved to be more cost-effective than traditional fixed policy approaches, demonstrating its strength as a framework for machine learning.",1
"We propose a new approach to inverse reinforcement learning (IRL) based on the deep Gaussian process (deep GP) model, which is capable of learning complicated reward structures with few demonstrations. Our model stacks multiple latent GP layers to learn abstract representations of the state feature space, which is linked to the demonstrations through the Maximum Entropy learning framework. Incorporating the IRL engine into the nonlinear latent structure renders existing deep GP inference approaches intractable. To tackle this, we develop a non-standard variational approximation framework which extends previous inference schemes. This allows for approximate Bayesian treatment of the feature space and guards against overfitting. Carrying out representation and inverse reinforcement learning simultaneously within our model outperforms state-of-the-art approaches, as we demonstrate with experiments on standard benchmarks (""object world"",""highway driving"") and a new benchmark (""binary world"").",0
"Our novel approach to inverse reinforcement learning (IRL) utilizing the deep Gaussian process (deep GP) model is capable of learning complex reward structures from just a few demonstrations. By stacking multiple latent GP layers, our model can learn abstract representations of the state feature space, which is linked to the demonstrations through the Maximum Entropy learning framework. However, incorporating the IRL engine into the nonlinear latent structure makes existing deep GP inference approaches impractical. To address this, we have developed a non-standard variational approximation framework that extends previous inference schemes. This allows for approximate Bayesian treatment of the feature space and prevents overfitting. Our model outperforms state-of-the-art approaches by simultaneously conducting representation and inverse reinforcement learning within our model, as demonstrated through experiments on standard benchmarks (""object world"", ""highway driving"") and a new benchmark (""binary world"").",1
"We analyze how the knowledge to autonomously handle one type of intersection, represented as a Deep Q-Network, translates to other types of intersections (tasks). We view intersection handling as a deep reinforcement learning problem, which approximates the state action Q function as a deep neural network. Using a traffic simulator, we show that directly copying a network trained for one type of intersection to another type of intersection decreases the success rate. We also show that when a network that is pre-trained on Task A and then is fine-tuned on a Task B, the resulting network not only performs better on the Task B than an network exclusively trained on Task A, but also retained knowledge on the Task A. Finally, we examine a lifelong learning setting, where we train a single network on five different types of intersections sequentially and show that the resulting network exhibited catastrophic forgetting of knowledge on previous tasks. This result suggests a need for a long-term memory component to preserve knowledge.",0
"Our research investigates the transferability of knowledge gained from autonomously handling one type of intersection, represented by a Deep Q-Network, to other intersection tasks. We approach intersection handling as a deep reinforcement learning problem, where a deep neural network approximates the state action Q function. We conduct experiments using a traffic simulator and find that directly copying a network trained for one intersection type to another decreases success rates. However, when a pre-trained network on Task A is fine-tuned for Task B, the resulting network performs better on Task B and retains knowledge on Task A. Furthermore, we explore lifelong learning and find that training a single network on five different intersection types sequentially results in catastrophic forgetting of previous tasks, indicating a need for a long-term memory component to preserve knowledge.",1
"We consider the task of learning control policies for a robotic mechanism striking a puck in an air hockey game. The control signal is a direct command to the robot's motors. We employ a model free deep reinforcement learning framework to learn the motoric skills of striking the puck accurately in order to score. We propose certain improvements to the standard learning scheme which make the deep Q-learning algorithm feasible when it might otherwise fail. Our improvements include integrating prior knowledge into the learning scheme, and accounting for the changing distribution of samples in the experience replay buffer. Finally we present our simulation results for aimed striking which demonstrate the successful learning of this task, and the improvement in algorithm stability due to the proposed modifications.",0
"Our focus is on mastering the control policies for a robotic mechanism that plays air hockey by striking a puck. The control signal given to the robot's motors is a direct command. To achieve accuracy in striking the puck and scoring, we utilize a model-free deep reinforcement learning framework. We suggest enhancements to the standard learning approach to enable the deep Q-learning algorithm to function in situations where it might not otherwise work. Our improvements include integrating prior knowledge into the learning process and taking into account the changing distribution of experience replay buffer samples. Finally, we present the results of our simulation for aimed striking, which demonstrates our successful learning of the task and the algorithm's improved stability due to the proposed modifications.",1
"In recent years there is a growing interest in using deep representations for reinforcement learning. In this paper, we present a methodology and tools to analyze Deep Q-networks (DQNs) in a non-blind matter. Moreover, we propose a new model, the Semi Aggregated Markov Decision Process (SAMDP), and an algorithm that learns it automatically. The SAMDP model allows us to identify spatio-temporal abstractions directly from features and may be used as a sub-goal detector in future work. Using our tools we reveal that the features learned by DQNs aggregate the state space in a hierarchical fashion, explaining its success. Moreover, we are able to understand and describe the policies learned by DQNs for three different Atari2600 games and suggest ways to interpret, debug and optimize deep neural networks in reinforcement learning.",0
"Deep representations for reinforcement learning have gained increasing interest in recent years. This paper introduces a methodology and tools for analyzing Deep Q-networks (DQNs) in a non-blind manner. Additionally, a new model, the Semi Aggregated Markov Decision Process (SAMDP), is proposed along with an algorithm that can automatically learn it. The SAMDP model can identify spatio-temporal abstractions from features and be utilized as a sub-goal detector in future work. By using these tools, it was discovered that the features learned by DQNs aggregate the state space hierarchically, which explains their success. Furthermore, this approach allows for a comprehensive understanding and description of the policies learned by DQNs in three different Atari2600 games, as well as providing suggestions for interpreting, debugging, and optimizing deep neural networks in reinforcement learning.",1
"While a large body of empirical results show that temporally-extended actions and options may significantly affect the learning performance of an agent, the theoretical understanding of how and when options can be beneficial in online reinforcement learning is relatively limited. In this paper, we derive an upper and lower bound on the regret of a variant of UCRL using options. While we first analyze the algorithm in the general case of semi-Markov decision processes (SMDPs), we show how these results can be translated to the specific case of MDPs with options and we illustrate simple scenarios in which the regret of learning with options can be \textit{provably} much smaller than the regret suffered when learning with primitive actions.",0
"Although there is a significant amount of empirical evidence demonstrating the impact of temporally-extended actions and options on an agent's learning performance, the theoretical understanding of the advantages of options in online reinforcement learning is limited. This paper establishes bounds on the regret of a variant of UCRL that employs options, both upper and lower. Initially, the algorithm is analyzed in the context of semi-Markov decision processes (SMDPs), but we demonstrate how these findings can be applied to MDPs with options. Additionally, we present straightforward scenarios illustrating how the regret of learning with options can be verifiably much smaller than that of learning with primitive actions.",1
"Image captioning is a challenging problem owing to the complexity in understanding the image content and diverse ways of describing it in natural language. Recent advances in deep neural networks have substantially improved the performance of this task. Most state-of-the-art approaches follow an encoder-decoder framework, which generates captions using a sequential recurrent prediction model. However, in this paper, we introduce a novel decision-making framework for image captioning. We utilize a ""policy network"" and a ""value network"" to collaboratively generate captions. The policy network serves as a local guidance by providing the confidence of predicting the next word according to the current state. Additionally, the value network serves as a global and lookahead guidance by evaluating all possible extensions of the current state. In essence, it adjusts the goal of predicting the correct words towards the goal of generating captions similar to the ground truth captions. We train both networks using an actor-critic reinforcement learning model, with a novel reward defined by visual-semantic embedding. Extensive experiments and analyses on the Microsoft COCO dataset show that the proposed framework outperforms state-of-the-art approaches across different evaluation metrics.",0
"Due to the intricate nature of comprehending image content and the various ways of describing it in natural language, image captioning poses a challenging problem. However, recent advancements in deep neural networks have significantly enhanced the performance of this task. Although most advanced approaches adopt an encoder-decoder framework, this paper introduces a unique decision-making framework for image captioning. This framework utilizes a ""policy network"" and a ""value network"" to collaboratively create captions. The policy network acts as a local guide by determining the confidence of predicting the next word based on the current state. The global and lookahead guidance is provided by the value network, which evaluates all possible extensions of the current state and adjusts the goal of predicting the correct words towards the goal of generating captions resembling the ground truth captions. We employ an actor-critic reinforcement learning model to train both networks, with a novel reward defined by visual-semantic embedding. The proposed framework surpasses state-of-the-art approaches across various evaluation metrics, as demonstrated by extensive experiments and analyses conducted on the Microsoft COCO dataset.",1
"State-of-the-art computer vision algorithms often achieve efficiency by making discrete choices about which hypotheses to explore next. This allows allocation of computational resources to promising candidates, however, such decisions are non-differentiable. As a result, these algorithms are hard to train in an end-to-end fashion. In this work we propose to learn an efficient algorithm for the task of 6D object pose estimation. Our system optimizes the parameters of an existing state-of-the art pose estimation system using reinforcement learning, where the pose estimation system now becomes the stochastic policy, parametrized by a CNN. Additionally, we present an efficient training algorithm that dramatically reduces computation time. We show empirically that our learned pose estimation procedure makes better use of limited resources and improves upon the state-of-the-art on a challenging dataset. Our approach enables differentiable end-to-end training of complex algorithmic pipelines and learns to make optimal use of a given computational budget.",0
"Advanced computer vision algorithms often achieve efficiency by selectively choosing which hypotheses to explore next, allowing for computational resources to be allocated to more promising candidates. However, these choices are non-differentiable, making it difficult to train these algorithms in an end-to-end manner. This study proposes a method for efficiently learning a 6D object pose estimation algorithm. The proposed system optimizes the parameters of an existing pose estimation system using reinforcement learning, where the stochastic policy is now parametrized by a CNN. Additionally, a new training algorithm is presented that significantly reduces computation time. The results demonstrate that the learned pose estimation procedure makes better use of limited resources and outperforms existing methods on a challenging dataset. This approach enables differentiable end-to-end training of complex algorithmic pipelines and learns to optimize the use of a given computational budget.",1
"Deep learning and reinforcement learning methods have recently been used to solve a variety of problems in continuous control domains. An obvious application of these techniques is dexterous manipulation tasks in robotics which are difficult to solve using traditional control theory or hand-engineered approaches. One example of such a task is to grasp an object and precisely stack it on another. Solving this difficult and practically relevant problem in the real world is an important long-term goal for the field of robotics. Here we take a step towards this goal by examining the problem in simulation and providing models and techniques aimed at solving it. We introduce two extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), a model-free Q-learning based method, which make it significantly more data-efficient and scalable. Our results show that by making extensive use of off-policy data and replay, it is possible to find control policies that robustly grasp objects and stack them. Further, our results hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots.",0
"Recently, deep learning and reinforcement learning methods have been utilized to tackle a range of challenges in continuous control domains. These techniques are particularly useful for dexterous manipulation tasks in robotics, which are difficult to solve using traditional control theory or manual approaches. One such task involves grasping an object and precisely stacking it on another. This is a challenging and essential problem to solve for the field of robotics. Therefore, in this study, we attempt to address this issue by examining it in simulation and presenting models and methods for solving it. To achieve this goal, we propose two extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), which is a model-free Q-learning-based technique. Our extensions enhance the scalability and data-efficiency of the algorithm. Our findings indicate that by utilizing off-policy data and replay, we can discover control strategies that successfully grasp and stack objects. Furthermore, our results imply that it may soon be possible to train stacking policies by gathering interactions on real robots.",1
"In this paper we introduce a fully end-to-end approach for visual tracking in videos that learns to predict the bounding box locations of a target object at every frame. An important insight is that the tracking problem can be considered as a sequential decision-making process and historical semantics encode highly relevant information for future decisions. Based on this intuition, we formulate our model as a recurrent convolutional neural network agent that interacts with a video overtime, and our model can be trained with reinforcement learning (RL) algorithms to learn good tracking policies that pay attention to continuous, inter-frame correlation and maximize tracking performance in the long run. The proposed tracking algorithm achieves state-of-the-art performance in an existing tracking benchmark and operates at frame-rates faster than real-time. To the best of our knowledge, our tracker is the first neural-network tracker that combines convolutional and recurrent networks with RL algorithms.",0
"This paper presents a novel approach to visual tracking in videos, which involves predicting the bounding box location of a target object in every frame. The authors highlight that tracking can be seen as a sequential decision-making process, where historical data can inform future decisions. To address this, they propose a recurrent convolutional neural network that interacts with videos over time, which can be trained with reinforcement learning algorithms. The model is designed to pay attention to inter-frame correlation and maximize tracking performance in the long term. The results show that this approach outperforms existing tracking benchmarks and operates at faster-than-real-time frame rates. Importantly, this is the first neural-network tracker that combines convolutional and recurrent networks with RL algorithms.",1
"Reinforcement learning is considered to be a strong AI paradigm which can be used to teach machines through interaction with the environment and learning from their mistakes. Despite its perceived utility, it has not yet been successfully applied in automotive applications. Motivated by the successful demonstrations of learning of Atari games and Go by Google DeepMind, we propose a framework for autonomous driving using deep reinforcement learning. This is of particular relevance as it is difficult to pose autonomous driving as a supervised learning problem due to strong interactions with the environment including other vehicles, pedestrians and roadworks. As it is a relatively new area of research for autonomous driving, we provide a short overview of deep reinforcement learning and then describe our proposed framework. It incorporates Recurrent Neural Networks for information integration, enabling the car to handle partially observable scenarios. It also integrates the recent work on attention models to focus on relevant information, thereby reducing the computational complexity for deployment on embedded hardware. The framework was tested in an open source 3D car racing simulator called TORCS. Our simulation results demonstrate learning of autonomous maneuvering in a scenario of complex road curvatures and simple interaction of other vehicles.",0
"Reinforcement learning is a powerful AI approach that involves teaching machines by exposing them to environmental stimuli and learning from their mistakes. Despite its usefulness, it has not been effectively utilized in the automotive industry. Encouraged by Google DeepMind's success in teaching machines to learn Atari games and Go, we propose a framework for autonomous driving using deep reinforcement learning. This is particularly relevant as autonomous driving cannot be treated as a supervised learning problem due to the complex interactions with the environment, including pedestrians, roadworks, and other vehicles. As this is a relatively new field of research, we provide a brief overview of deep reinforcement learning and then present our proposed framework. It utilizes Recurrent Neural Networks to integrate information and enable the car to handle partially observable situations. Moreover, we have integrated recent work on attention models to reduce computational complexity when deploying the framework on embedded hardware. We tested the framework in an open source 3D car racing simulator called TORCS, and the results demonstrate that the car can learn to maneuver autonomously in complex road conditions with simple interactions with other vehicles.",1
"Policy gradient methods have been successfully applied to many complex reinforcement learning problems. However, policy gradient methods suffer from high variance, slow convergence, and inefficient exploration. In this work, we introduce a maximum entropy policy optimization framework which explicitly encourages parameter exploration, and show that this framework can be reduced to a Bayesian inference problem. We then propose a novel Stein variational policy gradient method (SVPG) which combines existing policy gradient methods and a repulsive functional to generate a set of diverse but well-behaved policies. SVPG is robust to initialization and can easily be implemented in a parallel manner. On continuous control problems, we find that implementing SVPG on top of REINFORCE and advantage actor-critic algorithms improves both average return and data efficiency.",0
"Numerous complex reinforcement learning problems have successfully employed policy gradient methods. Nevertheless, policy gradient methods are hindered by high variance, sluggish convergence, and ineffective exploration. This study introduces a maximum entropy policy optimization framework that explicitly promotes parameter exploration. Furthermore, it depicts that this framework can be simplified to a Bayesian inference problem. Subsequently, a pioneering Stein variational policy gradient (SVPG) method is introduced that combines existing policy gradient methods with a repulsive function to create a diverse yet well-behaved set of policies. SVPG is resilient to initialization and can be implemented in a parallel fashion. On continuous control problems, it was discovered that applying SVPG on top of REINFORCE and advantage actor-critic algorithms enhanced both average return and data efficiency.",1
"Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as 'PGQL', for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.",0
"The policy gradient method is an effective way to enhance a policy in a reinforcement learning scenario. However, regular online versions of this technique only work on-policy and do not allow for the utilization of off-policy data. In this article, we present a novel approach that combines policy gradient with off-policy Q-learning, by utilizing experience from a replay buffer. This is motivated by the link between the fixed points of the regularized policy gradient algorithm and the Q-values, which enables us to estimate Q-values from the policy's action preferences, and then update them using Q-learning. We call this approach 'PGQL', representing a combination of policy gradient and Q-learning. We also establish that action-value fitting techniques and actor-critic algorithms are equivalent, demonstrating that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. Finally, we provide numerical examples that show improved data efficiency and stability of PGQL. Notably, we tested PGQL on the complete range of Atari games, and it performed better than both asynchronous advantage actor-critic (A3C) and Q-learning.",1
"It is well known that options can make planning more efficient, among their many benefits. Thus far, algorithms for autonomously discovering a set of useful options were heuristic. Naturally, a principled way of finding a set of useful options may be more promising and insightful. In this paper we suggest a mathematical characterization of good sets of options using tools from information theory. This characterization enables us to find conditions for a set of options to be optimal and an algorithm that outputs a useful set of options and illustrate the proposed algorithm in simulation.",0
"Among the numerous benefits of options, their ability to enhance planning efficiency is widely recognized. However, current heuristic algorithms for discovering useful options lack a principled approach. A more promising and insightful method for identifying such options may be achieved through a mathematically characterized set of options utilizing information theory tools. In this paper, we present a method for determining the optimality of a set of options and an algorithm for producing useful options. We demonstrate the effectiveness of the proposed algorithm through simulation.",1
"Model-free reinforcement learning algorithms, such as Q-learning, perform poorly in the early stages of learning in noisy environments, because much effort is spent unlearning biased estimates of the state-action value function. The bias results from selecting, among several noisy estimates, the apparent optimum, which may actually be suboptimal. We propose G-learning, a new off-policy learning algorithm that regularizes the value estimates by penalizing deterministic policies in the beginning of the learning process. We show that this method reduces the bias of the value-function estimation, leading to faster convergence to the optimal value and the optimal policy. Moreover, G-learning enables the natural incorporation of prior domain knowledge, when available. The stochastic nature of G-learning also makes it avoid some exploration costs, a property usually attributed only to on-policy algorithms. We illustrate these ideas in several examples, where G-learning results in significant improvements of the convergence rate and the cost of the learning process.",0
"In noisy environments, Q-learning and other model-free reinforcement learning algorithms do not perform well during early learning stages due to biased estimates of the state-action value function. This bias is caused by selecting the apparent optimum from several noisy estimates, which may actually be suboptimal. To address this issue, we introduce a new off-policy learning algorithm called G-learning, which regulates value estimates by penalizing deterministic policies at the beginning of the learning process. G-learning reduces bias in value-function estimation, leading to faster convergence to optimal values and policies. Additionally, G-learning allows for the inclusion of prior domain knowledge and avoids exploration costs. We demonstrate the effectiveness of G-learning in various examples, showcasing significant improvements in convergence rates and the cost of the learning process.",1
"In this paper, we propose a framework for solving a single-agent task by using multiple agents, each focusing on different aspects of the task. This approach has two main advantages: 1) it allows for training specialized agents on different parts of the task, and 2) it provides a new way to transfer knowledge, by transferring trained agents. Our framework generalizes the traditional hierarchical decomposition, in which, at any moment in time, a single agent has control until it has solved its particular subtask. We illustrate our framework with empirical experiments on two domains.",0
"Our paper presents a framework that employs multiple agents to solve a single-agent task, with each agent handling specific task aspects. This approach offers two key benefits: firstly, it enables training of specialized agents for different task components, and secondly, it facilitates knowledge transfer by transferring trained agents. Our framework expands on the conventional hierarchical decomposition method, where a single agent has control until it solves its subtask. We substantiate our framework through empirical experiments conducted on two domains.",1
"We consider the exploration-exploitation tradeoff in linear quadratic (LQ) control problems, where the state dynamics is linear and the cost function is quadratic in states and controls. We analyze the regret of Thompson sampling (TS) (a.k.a. posterior-sampling for reinforcement learning) in the frequentist setting, i.e., when the parameters characterizing the LQ dynamics are fixed. Despite the empirical and theoretical success in a wide range of problems from multi-armed bandit to linear bandit, we show that when studying the frequentist regret TS in control problems, we need to trade-off the frequency of sampling optimistic parameters and the frequency of switches in the control policy. This results in an overall regret of $O(T^{2/3})$, which is significantly worse than the regret $O(\sqrt{T})$ achieved by the optimism-in-face-of-uncertainty algorithm in LQ control problems.",0
"The exploration-exploitation tradeoff is examined in linear quadratic (LQ) control problems, where the state dynamics are linear and the cost function is quadratic in both states and controls. The regret of Thompson sampling (TS) in the frequentist setting is analyzed, which means that the LQ dynamics parameters are fixed. Although TS has been successful in various problems, we show that when investigating frequentist regret TS in control problems, the frequency of sampling optimistic parameters must be balanced with the frequency of control policy switches. This results in an overall regret of $O(T^{2/3})$, which is notably worse than the regret of $O(\sqrt{T})$ obtained by the optimism-in-face-of-uncertainty algorithm in LQ control problems.",1
"Inverse reinforcement learning (IRL) has become a useful tool for learning behavioral models from demonstration data. However, IRL remains mostly unexplored for multi-agent systems. In this paper, we show how the principle of IRL can be extended to homogeneous large-scale problems, inspired by the collective swarming behavior of natural systems. In particular, we make the following contributions to the field: 1) We introduce the swarMDP framework, a sub-class of decentralized partially observable Markov decision processes endowed with a swarm characterization. 2) Exploiting the inherent homogeneity of this framework, we reduce the resulting multi-agent IRL problem to a single-agent one by proving that the agent-specific value functions in this model coincide. 3) To solve the corresponding control problem, we propose a novel heterogeneous learning scheme that is particularly tailored to the swarm setting. Results on two example systems demonstrate that our framework is able to produce meaningful local reward models from which we can replicate the observed global system dynamics.",0
"The use of Inverse reinforcement learning (IRL) has proven to be an effective tool for acquiring behavioral models through demonstration data. However, IRL has not been extensively explored for multi-agent systems. This paper focuses on expanding IRL to homogeneous large-scale problems by drawing inspiration from the collective swarming behavior of natural systems. The paper presents several contributions to the field, including the introduction of the swarMDP framework, a decentralized Markov decision process with a swarm characterization. The paper also proposes a heterogeneous learning scheme that is tailored to the swarm setting, which can solve the corresponding control problem. The results from two example systems demonstrate that the framework can produce local reward models that accurately replicate the global system dynamics. Furthermore, the paper shows that the multi-agent IRL problem can be reduced to a single-agent one by proving that the agent-specific value functions coincide due to the inherent homogeneity of the framework.",1
"At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using $Q$-learning with an $\epsilon$-greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks.",0
"Currently, the creation of convolutional neural network (CNN) architectures involves significant human effort and expertise. Architects typically engage in careful experimentation or adapt existing networks to come up with new designs. To address this challenge, we introduce MetaQNN, an algorithm that employs reinforcement learning to automatically generate high-performing CNN architectures for specific learning tasks. Our learning agent uses $Q$-learning, an $\epsilon$-greedy exploration strategy, and experience replay to sequentially select CNN layers. The agent explores numerous possible architectures and discovers designs that improve performance on the learning task. Our agent-designed networks, which feature only standard convolution, pooling, and fully-connected layers, outperform existing networks with similar layer types and closely rival advanced methods that use more complex layer types. Additionally, our meta-modeling approach beats out existing techniques for network design on image classification tasks.",1
"We propose an end-to-end approach to the natural language object retrieval task, which localizes an object within an image according to a natural language description, i.e., referring expression. Previous works divide this problem into two independent stages: first, compute region proposals from the image without the exploration of the language description; second, score the object proposals with regard to the referring expression and choose the top-ranked proposals. The object proposals are generated independently from the referring expression, which makes the proposal generation redundant and even irrelevant to the referred object. In this work, we train an agent with deep reinforcement learning, which learns to move and reshape a bounding box to localize the object according to the referring expression. We incorporate both the spatial and temporal context information into the training procedure. By simultaneously exploiting local visual information, the spatial and temporal context and the referring language a priori, the agent selects an appropriate action to take at each time. A special action is defined to indicate when the agent finds the referred object, and terminate the procedure. We evaluate our model on various datasets, and our algorithm significantly outperforms the compared algorithms. Notably, the accuracy improvement of our method over the recent method GroundeR and SCRC on the ReferItGame dataset are 7.67% and 18.25%, respectively.",0
"Our proposed approach for the natural language object retrieval task is end-to-end, meaning it localizes an object within an image based on a natural language description. In contrast to previous methods, which divide the problem into separate stages of computing region proposals and scoring object proposals, our approach trains an agent using deep reinforcement learning. This agent learns to move and reshape a bounding box to accurately localize the object according to the referring expression. Our approach incorporates spatial and temporal context information and the referring language to select appropriate actions. Once the referred object is found, the procedure terminates. We evaluated our approach on various datasets and found that it outperforms other algorithms, with a notable 7.67% and 18.25% accuracy improvement over GroundeR and SCRC, respectively, on the ReferItGame dataset.",1
"We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative 'image guessing' game between two agents -- Qbot and Abot -- who communicate in natural language dialog so that Qbot can select an unseen image from a lineup of images. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end -- from pixels to multi-agent multi-round dialog to game reward.   We demonstrate two experimental results.   First, as a 'sanity check' demonstration of pure RL (from scratch), we show results on a synthetic world, where the agents communicate in ungrounded vocabulary, i.e., symbols with no pre-specified meanings (X, Y, Z). We find that two bots invent their own communication protocol and start using certain symbols to ask/answer about certain visual attributes (shape/color/style). Thus, we demonstrate the emergence of grounded language and communication among 'visual' dialog agents with no human supervision.   Second, we conduct large-scale real-image experiments on the VisDial dataset, where we pretrain with supervised dialog data and show that the RL 'fine-tuned' agents significantly outperform SL agents. Interestingly, the RL Qbot learns to ask questions that Abot is good at, ultimately resulting in more informative dialog and a better team.",0
"We have developed a novel training approach for visual question answering and dialog agents with a goal-driven focus. Our method involves a cooperative 'image guessing' game between two agents, Qbot and Abot, who engage in natural language dialog to enable Qbot to select an unseen image from a set of options. The agents' policies are learned using deep reinforcement learning, encompassing pixels, multi-agent dialog, and game reward. Through two experimental demonstrations, we showcase the effectiveness of our approach. Firstly, in a synthetic world, the agents develop their own communication protocol and use symbols to inquire about specific visual attributes, showcasing the emergence of grounded language and communication without human supervision. Secondly, we conduct real-image experiments on the VisDial dataset, where our RL 'fine-tuned' agents outperform supervised learning agents. Notably, the RL Qbot learns to ask questions that Abot is skilled at, leading to more informative dialog and a stronger team.",1
"Fine-grained recognition is challenging due to its subtle local inter-class differences versus large intra-class variations such as poses. A key to address this problem is to localize discriminative parts to extract pose-invariant features. However, ground-truth part annotations can be expensive to acquire. Moreover, it is hard to define parts for many fine-grained classes. This work introduces Fully Convolutional Attention Networks (FCANs), a reinforcement learning framework to optimally glimpse local discriminative regions adaptive to different fine-grained domains. Compared to previous methods, our approach enjoys three advantages: 1) the weakly-supervised reinforcement learning procedure requires no expensive part annotations; 2) the fully-convolutional architecture speeds up both training and testing; 3) the greedy reward strategy accelerates the convergence of the learning. We demonstrate the effectiveness of our method with extensive experiments on four challenging fine-grained benchmark datasets, including CUB-200-2011, Stanford Dogs, Stanford Cars and Food-101.",0
"Recognizing fine-grained details is a difficult task because of the subtle differences between classes in small local areas, as well as the large variations within classes caused by different poses. To tackle this issue, identifying the discriminative parts and extracting features that are invariant to pose is crucial. However, it can be costly to obtain ground-truth part annotations, and it is challenging to define parts for many fine-grained categories. This study proposes Fully Convolutional Attention Networks (FCANs), a framework that utilizes reinforcement learning to identify local discriminative regions that are adaptable to various fine-grained domains. Compared to previous methods, our approach has three advantages: 1) the weakly-supervised reinforcement learning process does not require expensive part annotations; 2) the fully-convolutional architecture speeds up both training and testing; and 3) the greedy reward strategy accelerates learning convergence. We demonstrate the effectiveness of our method through extensive experiments on four challenging fine-grained benchmark datasets, including CUB-200-2011, Stanford Dogs, Stanford Cars, and Food-101.",1
"Dexterous multi-fingered hands can accomplish fine manipulation behaviors that are infeasible with simple robotic grippers. However, sophisticated multi-fingered hands are often expensive and fragile. Low-cost soft hands offer an appealing alternative to more conventional devices, but present considerable challenges in sensing and actuation, making them difficult to apply to more complex manipulation tasks. In this paper, we describe an approach to learning from demonstration that can be used to train soft robotic hands to perform dexterous manipulation tasks. Our method uses object-centric demonstrations, where a human demonstrates the desired motion of manipulated objects with their own hands, and the robot autonomously learns to imitate these demonstrations using reinforcement learning. We propose a novel algorithm that allows us to blend and select a subset of the most feasible demonstrations to learn to imitate on the hardware, which we use with an extension of the guided policy search framework to use multiple demonstrations to learn generalizable neural network policies. We demonstrate our approach on the RBO Hand 2, with learned motor skills for turning a valve, manipulating an abacus, and grasping.",0
"Simple robotic grippers are limited in their ability to perform fine manipulation behaviors that can be easily accomplished by dexterous multi-fingered hands. However, these hands are typically expensive and fragile. To address this issue, low-cost soft hands have been proposed as an alternative, but their implementation in complex manipulation tasks is challenging due to difficulties in sensing and actuation. In this paper, we present a learning from demonstration approach to train soft robotic hands in dexterous manipulation tasks. Our method involves object-centric demonstrations, where a human illustrates the desired motion of manipulated objects using their own hands. The robot then autonomously learns to imitate these demonstrations using reinforcement learning. We have developed a novel algorithm that enables us to blend and select a subset of feasible demonstrations to learn on the hardware. This algorithm, in conjunction with an extension of the guided policy search framework, allows us to use multiple demonstrations to learn generalizable neural network policies. We have demonstrated the effectiveness of our approach on the RBO Hand 2, which has learned motor skills for grasping, manipulating an abacus, and turning a valve.",1
"Mobile edge computing (a.k.a. fog computing) has recently emerged to enable in-situ processing of delay-sensitive applications at the edge of mobile networks. Providing grid power supply in support of mobile edge computing, however, is costly and even infeasible (in certain rugged or under-developed areas), thus mandating on-site renewable energy as a major or even sole power supply in increasingly many scenarios. Nonetheless, the high intermittency and unpredictability of renewable energy make it very challenging to deliver a high quality of service to users in energy harvesting mobile edge computing systems. In this paper, we address the challenge of incorporating renewables into mobile edge computing and propose an efficient reinforcement learning-based resource management algorithm, which learns on-the-fly the optimal policy of dynamic workload offloading (to the centralized cloud) and edge server provisioning to minimize the long-term system cost (including both service delay and operational cost). Our online learning algorithm uses a decomposition of the (offline) value iteration and (online) reinforcement learning, thus achieving a significant improvement of learning rate and run-time performance when compared to standard reinforcement learning algorithms such as Q-learning. We prove the convergence of the proposed algorithm and analytically show that the learned policy has a simple monotone structure amenable to practical implementation. Our simulation results validate the efficacy of our algorithm, which significantly improves the edge computing performance compared to fixed or myopic optimization schemes and conventional reinforcement learning algorithms.",0
"The emergence of mobile edge computing (also known as fog computing) has allowed for on-site processing of time-sensitive applications at the edge of mobile networks. However, providing reliable power supply for mobile edge computing can be expensive and impractical in some areas, necessitating the use of renewable energy as a primary or sole power source. Despite this, the unpredictable and intermittent nature of renewable energy makes it difficult to maintain a high quality of service for users in energy harvesting mobile edge computing systems. To address this challenge, we propose a reinforcement learning-based resource management algorithm that dynamically offloads workloads to the centralized cloud and provisions edge servers to minimize long-term system costs. Our algorithm combines offline value iteration and online reinforcement learning to improve learning rate and runtime performance, outperforming standard reinforcement learning algorithms like Q-learning. We prove the algorithm's convergence and demonstrate its practical implementation through analytical analysis. Simulation results confirm the effectiveness of our algorithm, which outperforms fixed or myopic optimization schemes and conventional reinforcement learning algorithms in improving edge computing performance.",1
"The policy gradients of the expected return objective can react slowly to rare rewards. Yet, in some cases agents may wish to emphasize the low or high returns regardless of their probability. Borrowing from the economics and control literature, we review the risk-sensitive value function that arises from an exponential utility and illustrate its effects on an example. This risk-sensitive value function is not always applicable to reinforcement learning problems, so we introduce the particle value function defined by a particle filter over the distributions of an agent's experience, which bounds the risk-sensitive one. We illustrate the benefit of the policy gradients of this objective in Cliffworld.",0
"Agents may sometimes want to prioritize low or high returns regardless of their rarity, but the expected return objective's policy gradients can be slow to respond to infrequent rewards. To address this, we explore the risk-sensitive value function that arises from an exponential utility, drawing from economics and control literature. However, this approach is not always suitable for reinforcement learning problems. That's why we introduce the particle value function, which uses a particle filter to track an agent's experience distributions and provides a bound for the risk-sensitive function. We demonstrate the advantages of using this objective's policy gradients in Cliffworld.",1
"This paper presents a novel form of policy gradient for model-free reinforcement learning (RL) with improved exploration properties. Current policy-based methods use entropy regularization to encourage undirected exploration of the reward landscape, which is ineffective in high dimensional spaces with sparse rewards. We propose a more directed exploration strategy that promotes exploration of under-appreciated reward regions. An action sequence is considered under-appreciated if its log-probability under the current policy under-estimates its resulting reward. The proposed exploration strategy is easy to implement, requiring small modifications to an implementation of the REINFORCE algorithm. We evaluate the approach on a set of algorithmic tasks that have long challenged RL methods. Our approach reduces hyper-parameter sensitivity and demonstrates significant improvements over baseline methods. Our algorithm successfully solves a benchmark multi-digit addition task and generalizes to long sequences. This is, to our knowledge, the first time that a pure RL method has solved addition using only reward feedback.",0
"In this paper, a new type of policy gradient for model-free reinforcement learning (RL) is introduced, which has superior exploration characteristics. The current policy-based techniques employ entropy regularization to promote undirected exploration of the reward landscape, but this is inadequate in high-dimensional spaces with sparse rewards. The proposed exploration strategy is more focused and encourages exploration of under-appreciated reward regions. An action sequence is defined as under-appreciated if its log-probability under the current policy underestimates its ultimate reward. The proposed exploration method is straightforward to implement and requires minor modifications to the REINFORCE algorithm. The effectiveness of the approach is evaluated on a variety of algorithmic tasks that have long challenged RL methods. Our method reduces hyper-parameter sensitivity and outperforms baseline methods significantly. We successfully apply our algorithm to a benchmark multi-digit addition task and show that it can generalize to long sequences. This is the first instance, to the best of our knowledge, where a pure RL approach has solved addition using only reward feedback.",1
"A key challenge in scaling up robot learning to many skills and environments is removing the need for human supervision, so that robots can collect their own data and improve their own performance without being limited by the cost of requesting human feedback. Model-based reinforcement learning holds the promise of enabling an agent to learn to predict the effects of its actions, which could provide flexible predictive models for a wide range of tasks and environments, without detailed human supervision. We develop a method for combining deep action-conditioned video prediction models with model-predictive control that uses entirely unlabeled training data. Our approach does not require a calibrated camera, an instrumented training set-up, nor precise sensing and actuation. Our results show that our method enables a real robot to perform nonprehensile manipulation -- pushing objects -- and can handle novel objects not seen during training.",0
"A major obstacle in expanding robot learning to multiple skills and environments is eliminating the need for human oversight. This would allow robots to gather their own data and enhance their performance without being restricted by the expense of obtaining human input. The potential of model-based reinforcement learning lies in providing a means for an agent to learn the repercussions of its actions, allowing for flexible predictive models for various tasks and settings without extensive human supervision. Our team has created a technique that integrates deep action-conditioned video prediction models with model-predictive control, utilizing entirely unlabeled training data. Our strategy does not necessitate a calibrated camera, an instrumented training environment, or precise sensing and actuation. Our findings demonstrate that our method permits a physical robot to execute nonprehensile manipulation, such as pushing objects, and can handle unfamiliar objects not encountered during training.",1
"In reinforcement learning, the state of the real world is often represented by feature vectors. However, not all of the features may be pertinent for solving the current task. We propose Feature Selection Explore and Exploit (FS-EE), an algorithm that automatically selects the necessary features while learning a Factored Markov Decision Process, and prove that under mild assumptions, its sample complexity scales with the in-degree of the dynamics of just the necessary features, rather than the in-degree of all features. This can result in a much better sample complexity when the in-degree of the necessary features is smaller than the in-degree of all features.",0
"The use of feature vectors is common in reinforcement learning to represent the state of the real world. However, not all features may be relevant for solving the current task. Our proposed algorithm, Feature Selection Explore and Exploit (FS-EE), automatically selects necessary features while learning a Factored Markov Decision Process. We prove that under mild assumptions, the sample complexity of FS-EE scales with the in-degree of only the necessary features rather than all features. This yields improved sample complexity when the in-degree of necessary features is smaller than that of all features.",1
"Deep reinforcement learning (RL) can acquire complex behaviors from low-level inputs, such as images. However, real-world applications of such methods require generalizing to the vast variability of the real world. Deep networks are known to achieve remarkable generalization when provided with massive amounts of labeled data, but can we provide this breadth of experience to an RL agent, such as a robot? The robot might continuously learn as it explores the world around it, even while deployed. However, this learning requires access to a reward function, which is often hard to measure in real-world domains, where the reward could depend on, for example, unknown positions of objects or the emotional state of the user. Conversely, it is often quite practical to provide the agent with reward functions in a limited set of situations, such as when a human supervisor is present or in a controlled setting. Can we make use of this limited supervision, and still benefit from the breadth of experience an agent might collect on its own? In this paper, we formalize this problem as semisupervised reinforcement learning, where the reward function can only be evaluated in a set of ""labeled"" MDPs, and the agent must generalize its behavior to the wide range of states it might encounter in a set of ""unlabeled"" MDPs, by using experience from both settings. Our proposed method infers the task objective in the unlabeled MDPs through an algorithm that resembles inverse RL, using the agent's own prior experience in the labeled MDPs as a kind of demonstration of optimal behavior. We evaluate our method on challenging tasks that require control directly from images, and show that our approach can improve the generalization of a learned deep neural network policy by using experience for which no reward function is available. We also show that our method outperforms direct supervised learning of the reward.",0
"Low-level inputs, such as images, can be used to teach complex behaviors through deep reinforcement learning (RL). However, this method's practicality for real-world applications requires generalizing to the vast variability of the real world. While deep networks can achieve remarkable generalization when given significant labeled data, providing this breadth of experience to an RL agent, such as a robot, is challenging. The robot can learn continuously as it explores the world, but this requires access to a reward function that is often difficult to measure in real-world domains. Alternatively, the agent can be given reward functions in a limited set of situations, such as when a human supervisor is present. This limited supervision can still benefit the agent's experience. We formalize this problem as semisupervised reinforcement learning, where the reward function can only be evaluated in a set of labeled MDPs, and the agent must generalize its behavior to a wide range of states in unlabeled MDPs by using experience from both settings. Our proposed method infers the task objective in the unlabeled MDPs through an algorithm similar to inverse RL, using the agent's own prior experience in the labeled MDPs as a demonstration of optimal behavior. We evaluate our approach on image-based tasks and show that it improves the generalization of a learned deep neural network policy by using experience for which no reward function is available. Our method also outperforms direct supervised learning of the reward.",1
"Reinforcement learning optimizes policies for expected cumulative reward. Need the supervision be so narrow? Reward is delayed and sparse for many tasks, making it a difficult and impoverished signal for end-to-end optimization. To augment reward, we consider a range of self-supervised tasks that incorporate states, actions, and successors to provide auxiliary losses. These losses offer ubiquitous and instantaneous supervision for representation learning even in the absence of reward. While current results show that learning from reward alone is feasible, pure reinforcement learning methods are constrained by computational and data efficiency issues that can be remedied by auxiliary losses. Self-supervised pre-training and joint optimization improve the data efficiency and policy returns of end-to-end reinforcement learning.",0
"The process of reinforcement learning aims to optimize policies based on the expected cumulative reward. However, this approach is often limited by the narrow scope of supervision provided by delayed and sparse rewards, which can make it challenging to achieve end-to-end optimization. To address this issue, self-supervised tasks that incorporate states, actions, and successors can be used to augment reward and provide additional auxiliary losses. These losses offer a more ubiquitous and instantaneous form of supervision for representation learning, even when reward is not readily available. While learning from reward alone is possible, auxiliary losses can help overcome computational and data efficiency issues associated with pure reinforcement learning methods. By incorporating self-supervised pre-training and joint optimization, end-to-end reinforcement learning can become more data-efficient and produce better policy returns.",1
"Despite progress in visual perception tasks such as image classification and detection, computers still struggle to understand the interdependency of objects in the scene as a whole, e.g., relations between objects or their attributes. Existing methods often ignore global context cues capturing the interactions among different object instances, and can only recognize a handful of types by exhaustively training individual detectors for all possible relationships. To capture such global interdependency, we propose a deep Variation-structured Reinforcement Learning (VRL) framework to sequentially discover object relationships and attributes in the whole image. First, a directed semantic action graph is built using language priors to provide a rich and compact representation of semantic correlations between object categories, predicates, and attributes. Next, we use a variation-structured traversal over the action graph to construct a small, adaptive action set for each step based on the current state and historical actions. In particular, an ambiguity-aware object mining scheme is used to resolve semantic ambiguity among object categories that the object detector fails to distinguish. We then make sequential predictions using a deep RL framework, incorporating global context cues and semantic embeddings of previously extracted phrases in the state vector. Our experiments on the Visual Relationship Detection (VRD) dataset and the large-scale Visual Genome dataset validate the superiority of VRL, which can achieve significantly better detection results on datasets involving thousands of relationship and attribute types. We also demonstrate that VRL is able to predict unseen types embedded in our action graph by learning correlations on shared graph nodes.",0
"Computers have made progress in tasks related to visual perception, such as image detection and classification. However, they still find it difficult to comprehend the relationship between objects in a scene, including their attributes and interactions. Current methods do not consider global context cues that could capture the interactions among different objects. Consequently, they can only recognize a limited number of types by training individual detectors exhaustively for all possible relationships. To address this gap, we propose a deep Variation-structured Reinforcement Learning (VRL) framework that can identify object relationships and attributes in the whole image. Our approach involves building a directed semantic action graph that represents semantic correlations between object categories, predicates, and attributes. A variation-structured traversal over the action graph is used to construct an adaptive action set for each step. We use an ambiguity-aware object mining scheme to resolve semantic ambiguity among object categories that the object detector fails to distinguish. We then make sequential predictions based on global context cues and semantic embeddings of previously extracted phrases in the state vector. Our experiments on the Visual Relationship Detection (VRD) dataset and the large-scale Visual Genome dataset demonstrate the superiority of VRL, which achieves significantly better detection results on datasets involving thousands of relationship and attribute types. Additionally, we show that VRL can predict unseen types embedded in our action graph by learning correlations on shared graph nodes.",1
"Existing object proposal algorithms usually search for possible object regions over multiple locations and scales separately, which ignore the interdependency among different objects and deviate from the human perception procedure. To incorporate global interdependency between objects into object localization, we propose an effective Tree-structured Reinforcement Learning (Tree-RL) approach to sequentially search for objects by fully exploiting both the current observation and historical search paths. The Tree-RL approach learns multiple searching policies through maximizing the long-term reward that reflects localization accuracies over all the objects. Starting with taking the entire image as a proposal, the Tree-RL approach allows the agent to sequentially discover multiple objects via a tree-structured traversing scheme. Allowing multiple near-optimal policies, Tree-RL offers more diversity in search paths and is able to find multiple objects with a single feed-forward pass. Therefore, Tree-RL can better cover different objects with various scales which is quite appealing in the context of object proposal. Experiments on PASCAL VOC 2007 and 2012 validate the effectiveness of the Tree-RL, which can achieve comparable recalls with current object proposal algorithms via much fewer candidate windows.",0
"Most existing algorithms for object proposal search for object regions in individual locations and scales, without taking into account the interdependence of different objects. This approach differs from human perception. To address this, we propose a Tree-structured Reinforcement Learning (Tree-RL) approach that utilizes both current observations and historical search paths to discover objects sequentially. The approach learns multiple searching policies to maximize the long-term reward and improve localization accuracy. By allowing for multiple near-optimal policies, Tree-RL offers more diversity in search paths and can find multiple objects with a single feed-forward pass. This approach can better cover objects of different scales, making it appealing for object proposal. Our experiments on PASCAL VOC 2007 and 2012 demonstrate that Tree-RL is effective and can achieve comparable recalls with current object proposal algorithms with fewer candidate windows.",1
"Deep neural networks coupled with fast simulation and improved computation have led to recent successes in the field of reinforcement learning (RL). However, most current RL-based approaches fail to generalize since: (a) the gap between simulation and real world is so large that policy-learning approaches fail to transfer; (b) even if policy learning is done in real world, the data scarcity leads to failed generalization from training to test scenarios (e.g., due to different friction or object masses). Inspired from H-infinity control methods, we note that both modeling errors and differences in training and test scenarios can be viewed as extra forces/disturbances in the system. This paper proposes the idea of robust adversarial reinforcement learning (RARL), where we train an agent to operate in the presence of a destabilizing adversary that applies disturbance forces to the system. The jointly trained adversary is reinforced -- that is, it learns an optimal destabilization policy. We formulate the policy learning as a zero-sum, minimax objective function. Extensive experiments in multiple environments (InvertedPendulum, HalfCheetah, Swimmer, Hopper and Walker2d) conclusively demonstrate that our method (a) improves training stability; (b) is robust to differences in training/test conditions; and c) outperform the baseline even in the absence of the adversary.",0
"Recent successes in reinforcement learning (RL) have been achieved through the use of deep neural networks, fast simulation, and improved computation. However, current RL-based approaches face a common issue of failing to generalize due to the significant gap between simulation and real-world environments, leading to policy-learning approaches being unable to transfer. Additionally, even if policy learning is conducted in the real world, data scarcity makes it difficult to generalize from training to test scenarios, which can have different factors such as friction or object masses. Inspired by H-infinity control methods, we propose the idea of robust adversarial reinforcement learning (RARL) to train an agent to operate in the presence of a destabilizing adversary that applies disturbance forces to the system. The jointly trained adversary is reinforced, meaning it learns an optimal destabilization policy, and we formulate the policy learning as a zero-sum, minimax objective function. Our extensive experiments in multiple environments conclusively demonstrate that RARL improves training stability, is robust to differences in training/test conditions, and outperforms the baseline even without the presence of the adversary.",1
"We present an algorithm for model-based reinforcement learning that combines Bayesian neural networks (BNNs) with random roll-outs and stochastic optimization for policy learning. The BNNs are trained by minimizing $\alpha$-divergences, allowing us to capture complicated statistical patterns in the transition dynamics, e.g. multi-modality and heteroskedasticity, which are usually missed by other common modeling approaches. We illustrate the performance of our method by solving a challenging benchmark where model-based approaches usually fail and by obtaining promising results in a real-world scenario for controlling a gas turbine.",0
"Our proposed algorithm for reinforcement learning using models incorporates Bayesian neural networks (BNNs), random roll-outs, and stochastic optimization for policy learning. By minimizing $\alpha$-divergences during BNN training, we can capture complex statistical patterns in transition dynamics that are typically overlooked by other modeling methods, such as multi-modality and heteroskedasticity. We demonstrate the effectiveness of our approach by successfully solving a difficult benchmark problem that traditional model-based methods struggle with and achieving favorable outcomes in an actual gas turbine control scenario.",1
"Deep reinforcement learning methods attain super-human performance in a wide range of environments. Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable performance. We propose Neural Episodic Control: a deep reinforcement learning agent that is able to rapidly assimilate new experiences and act upon them. Our agent uses a semi-tabular representation of the value function: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value function. We show across a wide range of environments that our agent learns significantly faster than other state-of-the-art, general purpose deep reinforcement learning agents.",0
"Despite achieving exceptional performance in various settings, deep reinforcement learning techniques are typically inefficient, necessitating significantly more data than humans to achieve satisfactory results. In this study, we introduce Neural Episodic Control, a deep reinforcement learning agent that can promptly absorb new experiences and act on them. Our agent adopts a semi-tabular value function representation, comprising a buffer of prior experiences featuring gradually evolving state representations and promptly updated estimations of the value function. We demonstrate that our agent outperforms other contemporary, general-purpose deep reinforcement learning agents and learns considerably faster in a wide range of environments.",1
"Exploration in complex domains is a key challenge in reinforcement learning, especially for tasks with very sparse rewards. Recent successes in deep reinforcement learning have been achieved mostly using simple heuristic exploration strategies such as $\epsilon$-greedy action selection or Gaussian control noise, but there are many tasks where these methods are insufficient to make any learning progress. Here, we consider more complex heuristics: efficient and scalable exploration strategies that maximize a notion of an agent's surprise about its experiences via intrinsic motivation. We propose to learn a model of the MDP transition probabilities concurrently with the policy, and to form intrinsic rewards that approximate the KL-divergence of the true transition probabilities from the learned model. One of our approximations results in using surprisal as intrinsic motivation, while the other gives the $k$-step learning progress. We show that our incentives enable agents to succeed in a wide range of environments with high-dimensional state spaces and very sparse rewards, including continuous control tasks and games in the Atari RAM domain, outperforming several other heuristic exploration techniques.",0
"Exploring complex domains is a significant challenge in reinforcement learning, particularly for tasks that offer little reward. Although simple heuristic exploration methods like $\epsilon$-greedy action selection or Gaussian control noise have led to recent successes in deep reinforcement learning, they are not always sufficient to facilitate learning. To tackle this problem, we propose more intricate heuristics that employ efficient and scalable exploration strategies, maximizing an agent's surprise about its experiences through intrinsic motivation. Our approach involves concurrent learning of a model of the MDP transition probabilities and the policy, generating intrinsic rewards that approximate the KL-divergence of the true transition probabilities from the learned model. Our approximations enable us to use surprisal as intrinsic motivation, or the $k$-step learning progress. We demonstrate that our incentives lead to success in various environments with high-dimensional state spaces and sparse rewards, including continuous control tasks and games in the Atari RAM domain, surpassing other heuristic exploration techniques.",1
"We present a new public dataset with a focus on simulating robotic vision tasks in everyday indoor environments using real imagery. The dataset includes 20,000+ RGB-D images and 50,000+ 2D bounding boxes of object instances densely captured in 9 unique scenes. We train a fast object category detector for instance detection on our data. Using the dataset we show that, although increasingly accurate and fast, the state of the art for object detection is still severely impacted by object scale, occlusion, and viewing direction all of which matter for robotics applications. We next validate the dataset for simulating active vision, and use the dataset to develop and evaluate a deep-network-based system for next best move prediction for object classification using reinforcement learning. Our dataset is available for download at cs.unc.edu/~ammirato/active_vision_dataset_website/.",0
"A novel collection of public data is presented for the purpose of emulating robotic vision tasks in everyday indoor settings through genuine imagery. The dataset encompasses over 20,000 RGB-D images and 50,000 2D bounding boxes of object instances that were extensively captured in 9 distinct scenarios. Subsequently, we trained an expedient object category detector for instance detection based on our data. By utilizing the dataset, we demonstrate that despite being increasingly precise and swift, object detection's cutting-edge technology is still significantly impacted by object scale, occlusion, and viewing direction, all of which are crucial for robotics applications. Furthermore, we validate the dataset for simulating active vision and employ it to create and assess a deep-network-based system for predicting the next best move for object classification through reinforcement learning. You may access our dataset by downloading it from cs.unc.edu/~ammirato/active_vision_dataset_website/.",1
"Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are represented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning/adaptation.",0
"Reinforcement learning for real-world tasks, particularly when using complex function approximators like deep neural networks, presents significant challenges in terms of sample complexity and safety. To address these challenges, model-based methods that use simulated data to supplement real data have been developed. However, the discrepancies between the simulated and target domains can hinder the effectiveness of simulated training. The EPOpt algorithm overcomes this by using an ensemble of simulated source domains and adversarial training to learn policies that are both robust and generalizable to various target domains, including unmodeled effects. Additionally, the ensemble's probability distribution over source domains can be improved by adapting it using data from the target domain and approximate Bayesian methods. By using a model ensemble and source domain adaptation, EPOpt achieves both robustness and learning/adaptation benefits.",1
"We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a \textit{critic} network that is trained to predict the value of an output token, given the policy of an \textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling.",0
"Our paper proposes a novel method for training neural networks to generate sequences using actor-critic techniques from reinforcement learning. The current log-likelihood training methods are inadequate due to the discrepancy between their training and testing modes. This is because models must generate tokens based on their prior guesses rather than the actual tokens. We have addressed this issue by introducing a critic network that predicts the value of the output token given the policy of an actor network. This approach brings the training process closer to the testing phase, allowing us to optimize directly for task-specific scores such as BLEU. It is important to note that we have utilized these techniques in the supervised learning setting, conditioning the critic network on the actual output. Our method has demonstrated improved performance on both a synthetic task and German-English machine translation. Our findings pave the way for applying these techniques to natural language tasks such as machine translation, caption generation, and dialogue modelling.",1
"Researchers have demonstrated state-of-the-art performance in sequential decision making problems (e.g., robotics control, sequential prediction) with deep neural network models. One often has access to near-optimal oracles that achieve good performance on the task during training. We demonstrate that AggreVaTeD --- a policy gradient extension of the Imitation Learning (IL) approach of (Ross & Bagnell, 2014) --- can leverage such an oracle to achieve faster and better solutions with less training data than a less-informed Reinforcement Learning (RL) technique. Using both feedforward and recurrent neural network predictors, we present stochastic gradient procedures on a sequential prediction task, dependency-parsing from raw image data, as well as on various high dimensional robotics control problems. We also provide a comprehensive theoretical study of IL that demonstrates we can expect up to exponentially lower sample complexity for learning with AggreVaTeD than with RL algorithms, which backs our empirical findings. Our results and theory indicate that the proposed approach can achieve superior performance with respect to the oracle when the demonstrator is sub-optimal.",0
"State-of-the-art performance has been demonstrated by researchers using deep neural network models for sequential decision making problems, such as robotics control and sequential prediction. Typically, an oracle that performs well during training is available. The AggreVaTeD approach, an extension of the Imitation Learning (IL) method by (Ross & Bagnell, 2014), can take advantage of such oracles to achieve faster and better solutions with less training data than Reinforcement Learning (RL) techniques that are less informed. Stochastic gradient procedures are presented, using both feedforward and recurrent neural network predictors, on various high-dimensional robotics control problems and a sequential prediction task of dependency-parsing from raw image data. A theoretical study demonstrates that AggreVaTeD can achieve exponentially lower sample complexity for learning compared to RL algorithms. Our empirical findings and theoretical study indicate that this approach can achieve superior performance with respect to the oracle, even when the demonstrator is sub-optimal.",1
"We introduce a hybrid CPU/GPU version of the Asynchronous Advantage Actor-Critic (A3C) algorithm, currently the state-of-the-art method in reinforcement learning for various gaming tasks. We analyze its computational traits and concentrate on aspects critical to leveraging the GPU's computational power. We introduce a system of queues and a dynamic scheduling strategy, potentially helpful for other asynchronous algorithms as well. Our hybrid CPU/GPU version of A3C, based on TensorFlow, achieves a significant speed up compared to a CPU implementation; we make it publicly available to other researchers at https://github.com/NVlabs/GA3C .",0
"In this study, we present an alternative version of the Asynchronous Advantage Actor-Critic (A3C) algorithm that combines the computing power of both CPU and GPU. Being the most advanced method in reinforcement learning for gaming tasks, we focus on analyzing its computational features and emphasize the essential aspects of exploiting GPU's capabilities. To enhance other asynchronous algorithms, we introduce a queue system and a dynamic scheduling strategy. Our TensorFlow-based hybrid CPU/GPU A3C algorithm demonstrates a noticeable acceleration compared to the CPU variant, and we share it with the research community at https://github.com/NVlabs/GA3C .",1
"Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process. In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks. Taking neural network training with stochastic gradient descent (SGD) as an example, comprehensive experiments with respect to various neural network modeling (e.g., multi-layer perceptron networks, convolutional neural networks and recurrent neural networks) and several applications (e.g., image classification and text understanding) demonstrate that NDF powered SGD can achieve comparable accuracy with standard SGD process by using less data and fewer iterations.",0
"The study of machine learning involves manipulating data to improve performance. A dynamic data selection process that can choose different data during various training stages can lead to a more effective and efficient model. In this paper, we suggest a deep reinforcement learning framework, known as Neural Data Filter (NDF), that can automatically and adaptively select data during training. NDF utilizes a deep neural network to filter important data from a sequence of training data, maximizing the future reward (e.g., convergence speed). Unlike previous data selection methods that rely on heuristic strategies, NDF is versatile and can be applied to various machine learning tasks. Experiments conducted on different neural network models (e.g., multi-layer perceptron networks, convolutional neural networks, and recurrent neural networks) and applications (e.g., image classification and text understanding) demonstrate that NDF powered by stochastic gradient descent (SGD) can achieve comparable accuracy with standard SGD using less data and iterations.",1
"Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.",0
"Although model-free deep reinforcement learning (RL) methods have achieved success in various simulated environments, their high sample complexity presents a significant challenge for real-world applications. While batch policy gradient methods offer stable learning, they come with high variance, necessitating large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased and often require expensive hyperparameter optimization to stabilize. This study seeks to create approaches that combine the stability of policy gradients with the efficiency of off-policy RL. The authors propose Q-Prop, a policy gradient method that utilizes a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both stable and sample-efficient, effectively merging the advantages of on-policy and off-policy methods. The study analyzes the link between Q-Prop and existing model-free algorithms and derives two Q-Prop variants with conservative and aggressive adaptation using control variate theory. Conservative Q-Prop outperforms trust region policy optimization (TRPO) with generalized advantage estimation (GAE) in terms of sample efficiency and is more stable than deep deterministic policy gradient (DDPG), which is currently the state-of-the-art on-policy and off-policy method, on OpenAI Gym's MuJoCo continuous control environments.",1
"A critical component to enabling intelligent reasoning in partially observable environments is memory. Despite this importance, Deep Reinforcement Learning (DRL) agents have so far used relatively simple memory architectures, with the main methods to overcome partial observability being either a temporal convolution over the past k frames or an LSTM layer. More recent work (Oh et al., 2016) has went beyond these architectures by using memory networks which can allow more sophisticated addressing schemes over the past k frames. But even these architectures are unsatisfactory due to the reason that they are limited to only remembering information from the last k frames. In this paper, we develop a memory system with an adaptable write operator that is customized to the sorts of 3D environments that DRL agents typically interact with. This architecture, called the Neural Map, uses a spatially structured 2D memory image to learn to store arbitrary information about the environment over long time lags. We demonstrate empirically that the Neural Map surpasses previous DRL memories on a set of challenging 2D and 3D maze environments and show that it is capable of generalizing to environments that were not seen during training.",0
"Memory is crucial for intelligent reasoning in partially observable environments. However, Deep Reinforcement Learning (DRL) agents have relied on simplistic memory architectures thus far, using either temporal convolutions or LSTM layers to deal with partial observability. While recent work has explored memory networks with more advanced addressing schemes, these still have limitations in their ability to remember information beyond the last k frames. In this paper, we introduce the Neural Map, a memory system with an adaptable write operator specifically designed for the 3D environments that DRL agents typically encounter. This architecture uses a structured 2D memory image to store arbitrary information about the environment over long periods of time. Our empirical results demonstrate that the Neural Map outperforms previous DRL memories on challenging maze environments, even in cases where it has not been trained on the specific environment.",1
"Machine learning models are often used at test-time subject to constraints and trade-offs not present at training-time. For example, a computer vision model operating on an embedded device may need to perform real-time inference, or a translation model operating on a cell phone may wish to bound its average compute time in order to be power-efficient. In this work we describe a mixture-of-experts model and show how to change its test-time resource-usage on a per-input basis using reinforcement learning. We test our method on a small MNIST-based example.",0
"When utilizing machine learning models during testing, there are frequently limitations and compromises that were not evident during the training phase. For instance, a computer vision model working on an embedded device must execute inference in real-time, while a translation model used on a cell phone may want to limit its average processing time to conserve power. This study introduces a mixed-expert model and illustrates how to modify its resource usage during testing on a per-input basis using reinforcement learning. The effectiveness of our approach is tested on a simple example based on MNIST.",1
"The efficiency of reinforcement learning algorithms depends critically on a few meta-parameters that modulates the learning updates and the trade-off between exploration and exploitation. The adaptation of the meta-parameters is an open question in reinforcement learning, which arguably has become more of an issue recently with the success of deep reinforcement learning in high-dimensional state spaces. The long learning times in domains such as Atari 2600 video games makes it not feasible to perform comprehensive searches of appropriate meta-parameter values. We propose the Online Meta-learning by Parallel Algorithm Competition (OMPAC) method. In the OMPAC method, several instances of a reinforcement learning algorithm are run in parallel with small differences in the initial values of the meta-parameters. After a fixed number of episodes, the instances are selected based on their performance in the task at hand. Before continuing the learning, Gaussian noise is added to the meta-parameters with a predefined probability. We validate the OMPAC method by improving the state-of-the-art results in stochastic SZ-Tetris and in standard Tetris with a smaller, 10$\times$10, board, by 31% and 84%, respectively, and by improving the results for deep Sarsa($\lambda$) agents in three Atari 2600 games by 62% or more. The experiments also show the ability of the OMPAC method to adapt the meta-parameters according to the learning progress in different tasks.",0
"The effectiveness of reinforcement learning algorithms relies heavily on a few meta-parameters that control the learning updates and the balance between exploration and exploitation. Adjusting these meta-parameters is a challenge in reinforcement learning, especially with the success of deep reinforcement learning in complex state spaces like Atari 2600 video games, where the learning times are too long for exhaustive searches of appropriate meta-parameter values. To address this issue, we propose the Online Meta-learning by Parallel Algorithm Competition (OMPAC) method. This method involves running multiple instances of a reinforcement learning algorithm simultaneously with slight variations in the initial meta-parameter values. After a set number of episodes, the most successful instances are chosen, and Gaussian noise is added to their meta-parameters before continuing the learning process. We tested the OMPAC method on various tasks, including stochastic SZ-Tetris, standard Tetris on a smaller board, and three Atari 2600 games, and achieved significant improvements in performance. Our experiments also demonstrated the OMPAC method's ability to adapt the meta-parameters based on the learning progress in different tasks.",1
"Many modern commercial sites employ recommender systems to propose relevant content to users. While most systems are focused on maximizing the immediate gain (clicks, purchases or ratings), a better notion of success would be the lifetime value (LTV) of the user-system interaction. The LTV approach considers the future implications of the item recommendation, and seeks to maximize the cumulative gain over time. The Reinforcement Learning (RL) framework is the standard formulation for optimizing cumulative successes over time. However, RL is rarely used in practice due to its associated representation, optimization and validation techniques which can be complex. In this paper we propose a new architecture for combining RL with recommendation systems which obviates the need for hand-tuned features, thus automating the state-space representation construction process. We analyze the practical difficulties in this formulation and test our solutions on batch off-line real-world recommendation data.",0
"Nowadays, a lot of commercial websites use recommender systems to suggest relevant content to their users. However, the majority of these systems aim to maximize immediate results such as clicks, purchases, or ratings, rather than considering the lifetime value (LTV) of the user-system interaction. The LTV approach takes into account the long-term effects of item recommendations and strives to optimize cumulative gains over time. Reinforcement Learning (RL) is the standard framework for optimizing cumulative successes, but it is seldom used in practice due to its complicated representation, optimization, and validation techniques. In this article, we present a novel architecture that combines RL with recommendation systems while eliminating the need for hand-tuned features, thus automating the state-space representation construction process. We examine the practical challenges of this approach and evaluate our solutions using batch off-line real-world recommendation data.",1
"We consider the problem of learning hierarchical policies for Reinforcement Learning able to discover options, an option corresponding to a sub-policy over a set of primitive actions. Different models have been proposed during the last decade that usually rely on a predefined set of options. We specifically address the problem of automatically discovering options in decision processes. We describe a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective. The BONN model is evaluated on different classical RL problems, demonstrating both quantitative and qualitative interesting results.",0
"Our focus is on the issue of developing hierarchical policies for Reinforcement Learning with the ability to identify options, which are essentially sub-policies that operate over a collection of fundamental actions. Over the past decade, various techniques have been introduced that frequently rely on a predetermined set of options. Our specific aim is to overcome the challenge of automatically identifying options in decision-making processes. To this end, we present a novel learning framework, termed Budgeted Option Neural Network (BONN), which leverages a budgeted learning objective to identify options. The effectiveness of the BONN model is evaluated through its application to various classical RL problems, revealing notable quantitative and qualitative outcomes.",1
"Recent advances in one-shot learning have produced models that can learn from a handful of labeled examples, for passive classification and regression tasks. This paper combines reinforcement learning with one-shot learning, allowing the model to decide, during classification, which examples are worth labeling. We introduce a classification task in which a stream of images are presented and, on each time step, a decision must be made to either predict a label or pay to receive the correct label. We present a recurrent neural network based action-value function, and demonstrate its ability to learn how and when to request labels. Through the choice of reward function, the model can achieve a higher prediction accuracy than a similar model on a purely supervised task, or trade prediction accuracy for fewer label requests.",0
"Advancements in one-shot learning have resulted in models capable of learning from only a few labeled examples for passive classification and regression tasks. In this paper, we integrate reinforcement learning with one-shot learning to enable models to determine which examples are worthwhile to label during classification. Our classification task involves a sequence of images that require a decision to either predict a label or pay for the correct label at each time step. We introduce a recurrent neural network-based action-value function and demonstrate its ability to learn when and how to request labels. Depending on the chosen reward function, the model can exceed the prediction accuracy of a similar model solely reliant on supervised learning or sacrifice prediction accuracy for fewer label requests.",1
"Besides independent learning, human learning process is highly improved by summarizing what has been learned, communicating it with peers, and subsequently fusing knowledge from different sources to assist the current learning goal. This collaborative learning procedure ensures that the knowledge is shared, continuously refined, and concluded from different perspectives to construct a more profound understanding. The idea of knowledge transfer has led to many advances in machine learning and data mining, but significant challenges remain, especially when it comes to reinforcement learning, heterogeneous model structures, and different learning tasks. Motivated by human collaborative learning, in this paper we propose a collaborative deep reinforcement learning (CDRL) framework that performs adaptive knowledge transfer among heterogeneous learning agents. Specifically, the proposed CDRL conducts a novel deep knowledge distillation method to address the heterogeneity among different learning tasks with a deep alignment network. Furthermore, we present an efficient collaborative Asynchronous Advantage Actor-Critic (cA3C) algorithm to incorporate deep knowledge distillation into the online training of agents, and demonstrate the effectiveness of the CDRL framework using extensive empirical evaluation on OpenAI gym.",0
"The process of human learning is not only enhanced by independent study, but also through summarizing and sharing information with others. Collaborative learning involves combining knowledge from various sources to achieve a deeper understanding of the subject matter. While machine learning and data mining have benefited from this approach, there are still obstacles to overcome in areas such as reinforcement learning and diverse learning tasks. In this paper, we propose a collaborative deep reinforcement learning (CDRL) framework that allows for adaptive knowledge transfer among diverse learning agents. The CDRL employs a deep knowledge distillation method to address heterogeneity, and a collaborative Asynchronous Advantage Actor-Critic (cA3C) algorithm to incorporate deep knowledge distillation into online training. Our empirical evaluation on OpenAI gym demonstrates the effectiveness of the CDRL framework.",1
"Bottom-Up (BU) saliency models do not perform well in complex interactive environments where humans are actively engaged in tasks (e.g., sandwich making and playing the video games). In this paper, we leverage Reinforcement Learning (RL) to highlight task-relevant locations of input frames. We propose a soft attention mechanism combined with the Deep Q-Network (DQN) model to teach an RL agent how to play a game and where to look by focusing on the most pertinent parts of its visual input. Our evaluations on several Atari 2600 games show that the soft attention based model could predict fixation locations significantly better than bottom-up models such as Itti-Kochs saliency and Graph-Based Visual Saliency (GBVS) models.",0
The effectiveness of Bottom-Up (BU) saliency models is limited in complex interactive environments where people are actively engaged in tasks like making a sandwich or playing video games. This study utilizes Reinforcement Learning (RL) to identify task-relevant areas of input frames. A soft attention mechanism is combined with the Deep Q-Network (DQN) model to train an RL agent to play games and focus on the most relevant parts of its visual input. Our assessment of various Atari 2600 games shows that the soft attention model outperforms BU models like Itti-Kochs saliency and Graph-Based Visual Saliency (GBVS) in predicting fixation locations.,1
"We examine the problem of joint top-down active search of multiple objects under interaction, e.g., person riding a bicycle, cups held by the table, etc.. Such objects under interaction often can provide contextual cues to each other to facilitate more efficient search. By treating each detector as an agent, we present the first collaborative multi-agent deep reinforcement learning algorithm to learn the optimal policy for joint active object localization, which effectively exploits such beneficial contextual information. We learn inter-agent communication through cross connections with gates between the Q-networks, which is facilitated by a novel multi-agent deep Q-learning algorithm with joint exploitation sampling. We verify our proposed method on multiple object detection benchmarks. Not only does our model help to improve the performance of state-of-the-art active localization models, it also reveals interesting co-detection patterns that are intuitively interpretable.",0
"Our focus is on jointly searching for multiple objects that are interacting with each other, such as a person riding a bicycle or cups placed on a table. The interaction between these objects can provide useful contextual cues for more efficient searching. Our approach involves treating each detector as an agent and using a collaborative multi-agent deep reinforcement learning algorithm to learn the optimal policy for joint active object localization. We have developed a novel multi-agent deep Q-learning algorithm with joint exploitation sampling, which facilitates inter-agent communication through cross connections with gates between the Q-networks. We have tested our method on multiple object detection benchmarks and found that it not only improves the performance of state-of-the-art active localization models, but also reveals interesting co-detection patterns that are easy to interpret.",1
"Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.",0
"Neural networks have proven to be effective and versatile models for complex learning tasks such as image, speech, and natural language processing. However, designing these networks can still be challenging. This study proposes using a recurrent network to automatically generate model descriptions for neural networks, which are then trained using reinforcement learning to improve accuracy on a validation set. The results show that this method can create novel network architectures that match or surpass those created by humans in terms of accuracy and speed. Additionally, the proposed recurrent cell outperforms the commonly used LSTM cell and achieves state-of-the-art results on the Penn Treebank dataset. The cell can also be applied to character language modeling with impressive results.",1
"We study reinforcement learning of chatbots with recurrent neural network architectures when the rewards are noisy and expensive to obtain. For instance, a chatbot used in automated customer service support can be scored by quality assurance agents, but this process can be expensive, time consuming and noisy. Previous reinforcement learning work for natural language processing uses on-policy updates and/or is designed for on-line learning settings. We demonstrate empirically that such strategies are not appropriate for this setting and develop an off-policy batch policy gradient method (BPG). We demonstrate the efficacy of our method via a series of synthetic experiments and an Amazon Mechanical Turk experiment on a restaurant recommendations dataset.",0
"Our research focuses on the use of recurrent neural network architectures in the reinforcement learning of chatbots, particularly when obtaining rewards is both noisy and costly. For example, in automated customer service support, quality assurance agents may need to score the chatbot, but this can be a time-consuming and expensive process. Previous studies in natural language processing have employed on-policy updates and on-line learning settings, but we have found that these strategies are not suitable for this context. Instead, we have developed an off-policy batch policy gradient method (BPG) and tested its effectiveness through a series of simulated experiments and an Amazon Mechanical Turk experiment on a dataset related to restaurant recommendations.",1
"Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial.",0
"Adversaries can exploit machine learning classifiers by creating malicious inputs that result in misclassification. These adversarial examples have been thoroughly studied in the field of computer vision. However, our research shows that adversarial attacks can also be used to undermine neural network policies in reinforcement learning. We demonstrate the effectiveness of existing adversarial example crafting techniques in reducing the performance of trained policies during testing. Our threat model assumes that adversaries can make minor changes to the raw input of the policy. We evaluate the extent of this vulnerability across different tasks and training algorithms, using a subset of adversarial-example attacks in white-box and black-box settings. Our findings show that regardless of the specific task or training algorithm, even small adversarial perturbations that go unnoticed by humans can significantly reduce performance. To view our videos, please visit http://rll.berkeley.edu/adversarial.",1
"Mastering a video game requires skill, tactics and strategy. While these attributes may be acquired naturally by human players, teaching them to a computer program is a far more challenging task. In recent years, extensive research was carried out in the field of reinforcement learning and numerous algorithms were introduced, aiming to learn how to perform human tasks such as playing video games. As a result, the Arcade Learning Environment (ALE) (Bellemare et al., 2013) has become a commonly used benchmark environment allowing algorithms to train on various Atari 2600 games. In many games the state-of-the-art algorithms outperform humans. In this paper we introduce a new learning environment, the Retro Learning Environment --- RLE, that can run games on the Super Nintendo Entertainment System (SNES), Sega Genesis and several other gaming consoles. The environment is expandable, allowing for more video games and consoles to be easily added to the environment, while maintaining the same interface as ALE. Moreover, RLE is compatible with Python and Torch. SNES games pose a significant challenge to current algorithms due to their higher level of complexity and versatility.",0
"Acquiring the expertise to excel at a video game requires a combination of skill, tactics, and strategy. However, teaching a computer program to possess these attributes is a much more complex endeavor. Over the years, researchers have conducted extensive studies in the realm of reinforcement learning and have introduced various algorithms with the aim of enabling machines to perform human tasks, including playing video games. As a result, the Arcade Learning Environment (ALE) has emerged as a widely used evaluation platform for algorithms to train on a range of Atari 2600 games. In numerous games, state-of-the-art algorithms have surpassed human capabilities. This article presents a new learning environment, the Retro Learning Environment (RLE), which can operate games on the Super Nintendo Entertainment System (SNES), Sega Genesis, and other gaming consoles. The expandable environment facilitates the easy addition of more video games and consoles while maintaining the same interface as ALE. Furthermore, RLE is compatible with Python and Torch. SNES games present a significant challenge to current algorithms due to their greater complexity and adaptability.",1
"We propose augmenting deep neural networks with an attention mechanism for the visual object detection task. As perceiving a scene, humans have the capability of multiple fixation points, each attended to scene content at different locations and scales. However, such a mechanism is missing in the current state-of-the-art visual object detection methods. Inspired by the human vision system, we propose a novel deep network architecture that imitates this attention mechanism. As detecting objects in an image, the network adaptively places a sequence of glimpses of different shapes at different locations in the image. Evidences of the presence of an object and its location are extracted from these glimpses, which are then fused for estimating the object class and bounding box coordinates. Due to lacks of ground truth annotations of the visual attention mechanism, we train our network using a reinforcement learning algorithm with policy gradients. Experiment results on standard object detection benchmarks show that the proposed network consistently outperforms the baseline networks that does not model the attention mechanism.",0
"Our proposal involves enhancing deep neural networks with an attention mechanism to improve visual object detection. Human perception of a scene involves focusing on multiple points of interest, each with varying scales and content. However, current state-of-the-art visual object detection methods lack this mechanism. To mimic the human vision system, we introduce a novel deep network architecture that incorporates this attention mechanism. The network adaptively places glimpses of varying shapes at different locations in the image to detect objects. We extract evidence of object presence and location from these glimpses, and fuse them to estimate object class and bounding box coordinates. Since there are no ground truth annotations for visual attention mechanisms, we train our network using a reinforcement learning algorithm with policy gradients. Our experiments on standard object detection benchmarks demonstrate that our proposed network consistently outperforms baseline networks that do not incorporate the attention mechanism.",1
"Reinforcement learning can enable complex, adaptive behavior to be learned automatically for autonomous robotic platforms. However, practical deployment of reinforcement learning methods must contend with the fact that the training process itself can be unsafe for the robot. In this paper, we consider the specific case of a mobile robot learning to navigate an a priori unknown environment while avoiding collisions. In order to learn collision avoidance, the robot must experience collisions at training time. However, high-speed collisions, even at training time, could damage the robot. A successful learning method must therefore proceed cautiously, experiencing only low-speed collisions until it gains confidence. To this end, we present an uncertainty-aware model-based learning algorithm that estimates the probability of collision together with a statistical estimate of uncertainty. By formulating an uncertainty-dependent cost function, we show that the algorithm naturally chooses to proceed cautiously in unfamiliar environments, and increases the velocity of the robot in settings where it has high confidence. Our predictive model is based on bootstrapped neural networks using dropout, allowing it to process raw sensory inputs from high-bandwidth sensors such as cameras. Our experimental evaluation demonstrates that our method effectively minimizes dangerous collisions at training time in an obstacle avoidance task for a simulated and real-world quadrotor, and a real-world RC car. Videos of the experiments can be found at https://sites.google.com/site/probcoll.",0
"Autonomous robotic platforms can learn complex, adaptive behavior automatically through reinforcement learning. However, the practical implementation of this method must consider the safety of the robot during the training process. This paper focuses on a mobile robot that needs to learn how to navigate an unknown environment while avoiding collisions. Collisions are necessary for the robot to learn how to avoid them, but high-speed collisions can cause damage to the robot. We propose an uncertainty-aware model-based learning algorithm that estimates the probability of collision and the level of uncertainty. By using an uncertainty-dependent cost function, the robot can proceed cautiously in new environments and increase its velocity in familiar ones. Our model uses bootstrapped neural networks with dropout to process sensory inputs from high-bandwidth sensors like cameras. We demonstrate that our algorithm minimizes dangerous collisions during training for a simulated and real-world quadrotor and a real-world RC car. Videos of our experiments are available at https://sites.google.com/site/probcoll.",1
"Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.",0
"Reinforcement learning (RL) faces a significant challenge in achieving scalable and effective exploration. Although some methods provide optimality guarantees in the context of discrete state and action spaces, they are not suitable for high-dimensional deep RL scenarios. Consequently, most contemporary RL algorithms rely on simple heuristics like epsilon-greedy exploration or adding Gaussian noise to the controls. In this paper, we introduce a novel exploration strategy called Variational Information Maximizing Exploration (VIME), which maximizes information gain about the agent's environment dynamics belief. Our practical implementation uses variational inference in Bayesian neural networks, which can efficiently handle continuous state and action spaces. VIME modifies the MDP reward function and can be applied with various RL algorithms. We demonstrate that VIME outperforms heuristic exploration methods significantly across a range of continuous control tasks and algorithms, including tasks with very sparse rewards.",1
"Autonomous control systems onboard planetary rovers and spacecraft benefit from having cognitive capabilities like learning so that they can adapt to unexpected situations in-situ. Q-learning is a form of reinforcement learning and it has been efficient in solving certain class of learning problems. However, embedded systems onboard planetary rovers and spacecraft rarely implement learning algorithms due to the constraints faced in the field, like processing power, chip size, convergence rate and costs due to the need for radiation hardening. These challenges present a compelling need for a portable, low-power, area efficient hardware accelerator to make learning algorithms practical onboard space hardware. This paper presents a FPGA implementation of Q-learning with Artificial Neural Networks (ANN). This method matches the massive parallelism inherent in neural network software with the fine-grain parallelism of an FPGA hardware thereby dramatically reducing processing time. Mars Science Laboratory currently uses Xilinx-Space-grade Virtex FPGA devices for image processing, pyrotechnic operation control and obstacle avoidance. We simulate and program our architecture on a Xilinx Virtex 7 FPGA. The architectural implementation for a single neuron Q-learning and a more complex Multilayer Perception (MLP) Q-learning accelerator has been demonstrated. The results show up to a 43-fold speed up by Virtex 7 FPGAs compared to a conventional Intel i5 2.3 GHz CPU. Finally, we simulate the proposed architecture using the Symphony simulator and compiler from Xilinx, and evaluate the performance and power consumption.",0
"Planetary rovers and spacecraft equipped with autonomous control systems would greatly benefit from cognitive capabilities such as learning, allowing them to adapt to unexpected scenarios in real-time. Although Q-learning has proven effective in solving certain learning problems, the implementation of learning algorithms in embedded systems onboard rovers and spacecraft is often limited by challenges such as processing power, chip size, convergence rate, and radiation hardening costs. To address these limitations, a portable, low-power, and area-efficient hardware accelerator is needed. This paper presents a FPGA implementation of Q-learning with Artificial Neural Networks (ANN) that leverages the parallelism of neural network software and FPGA hardware to significantly reduce processing time. The Xilinx-Space-grade Virtex FPGA devices are currently used by Mars Science Laboratory for various tasks, and our architecture has been successfully simulated and programmed on a Xilinx Virtex 7 FPGA. The architecture includes a single neuron Q-learning and a more complex Multilayer Perception (MLP) Q-learning accelerator. The results demonstrate up to a 43-fold increase in speed compared to a conventional Intel i5 2.3 GHz CPU. The proposed architecture is simulated using the Symphony simulator and compiler from Xilinx, and its performance and power consumption are evaluated.",1
"The Human visual perception of the world is of a large fixed image that is highly detailed and sharp. However, receptor density in the retina is not uniform: a small central region called the fovea is very dense and exhibits high resolution, whereas a peripheral region around it has much lower spatial resolution. Thus, contrary to our perception, we are only able to observe a very small region around the line of sight with high resolution. The perception of a complete and stable view is aided by an attention mechanism that directs the eyes to the numerous points of interest within the scene. The eyes move between these targets in quick, unconscious movements, known as ""saccades"". Once a target is centered at the fovea, the eyes fixate for a fraction of a second while the visual system extracts the necessary information. An artificial visual system was built based on a fully recurrent neural network set within a reinforcement learning protocol, and learned to attend to regions of interest while solving a classification task. The model is consistent with several experimentally observed phenomena, and suggests novel predictions.",0
"The way humans perceive the world visually is through a detailed and sharp fixed image. However, the retina's receptor density is not uniform, with the fovea being the only region with high resolution and density, while the peripheral region has lower spatial resolution. This means that only a small area around the line of sight is observed with high resolution, contrary to our perception of a complete and stable view. To aid this perception, an attention mechanism directs the eyes to various points of interest in the scene, with quick, unconscious movements called ""saccades"". Once a target is centered on the fovea, the eyes fixate for a brief period while the visual system extracts necessary information. A fully recurrent neural network was created as an artificial visual system, which learned to attend to regions of interest while solving a classification task using reinforcement learning. The model is consistent with various observed phenomena and suggests new predictions.",1
"X-rays are commonly performed imaging tests that use small amounts of radiation to produce pictures of the organs, tissues, and bones of the body. X-rays of the chest are used to detect abnormalities or diseases of the airways, blood vessels, bones, heart, and lungs. In this work we present a stochastic attention-based model that is capable of learning what regions within a chest X-ray scan should be visually explored in order to conclude that the scan contains a specific radiological abnormality. The proposed model is a recurrent neural network (RNN) that learns to sequentially sample the entire X-ray and focus only on informative areas that are likely to contain the relevant information. We report on experiments carried out with more than $100,000$ X-rays containing enlarged hearts or medical devices. The model has been trained using reinforcement learning methods to learn task-specific policies.",0
"Using small amounts of radiation, X-rays are a commonly used imaging test to produce images of organs, tissues, and bones. X-rays of the chest are utilized to identify diseases or abnormalities in the lungs, heart, airways, blood vessels, and bones. Our study introduces a stochastic attention-based model, which is a recurrent neural network (RNN), that can learn which areas within a chest X-ray scan should be examined visually to identify a specific radiological abnormality. The proposed model is trained to sample the entire X-ray and concentrate on informative regions that are likely to contain the relevant information. We conducted experiments with over 100,000 X-rays, including those with enlarged hearts or medical devices. The model is trained using reinforcement learning methods to develop task-specific policies.",1
"In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.",0
"Deep reinforcement learning (RL) systems have achieved exceptional performance in various challenging tasks in recent years. However, the requirement for extensive training data is a significant limitation of such applications. The main objective now is to develop deep RL methods that can quickly adapt to new tasks. Our work introduces a new approach to this challenge called deep meta-reinforcement learning. Previous studies have shown that recurrent networks can support meta-learning in a fully supervised context, which we extend to the RL setting. The system we propose is trained using one RL algorithm, but its recurrent dynamics implement a second, distinct RL process. This learned RL algorithm can differ from the original one in arbitrary ways and exploit structure in the training domain. We demonstrate the key aspects of deep meta-RL in seven proof-of-concept experiments. We also discuss the potential implications for neuroscience and the possibilities for scaling up the approach.",1
"Both generative adversarial networks (GAN) in unsupervised learning and actor-critic methods in reinforcement learning (RL) have gained a reputation for being difficult to optimize. Practitioners in both fields have amassed a large number of strategies to mitigate these instabilities and improve training. Here we show that GANs can be viewed as actor-critic methods in an environment where the actor cannot affect the reward. We review the strategies for stabilizing training for each class of models, both those that generalize between the two and those that are particular to that model. We also review a number of extensions to GANs and RL algorithms with even more complicated information flow. We hope that by highlighting this formal connection we will encourage both GAN and RL communities to develop general, scalable, and stable algorithms for multilevel optimization with deep networks, and to draw inspiration across communities.",0
"The optimization of generative adversarial networks (GANs) in unsupervised learning and actor-critic methods in reinforcement learning (RL) has been a challenging task for practitioners. However, various strategies have been developed to improve the training process and overcome these instabilities. Through this study, we demonstrate that GANs can be considered as actor-critic methods within an environment where there is no influence on the reward. We examine the stabilization techniques for both models, including those that are common to both and those that are specific to each. Additionally, we explore the extensions of GANs and RL algorithms that involve more complex information flow. By emphasizing this formal connection, we aim to stimulate the development of general, scalable, and stable algorithms for multilevel optimization with deep networks within both the GAN and RL communities. We also encourage cross-community inspiration and collaboration.",1
"Deep learning classifiers are known to be inherently vulnerable to manipulation by intentionally perturbed inputs, named adversarial examples. In this work, we establish that reinforcement learning techniques based on Deep Q-Networks (DQNs) are also vulnerable to adversarial input perturbations, and verify the transferability of adversarial examples across different DQN models. Furthermore, we present a novel class of attacks based on this vulnerability that enable policy manipulation and induction in the learning process of DQNs. We propose an attack mechanism that exploits the transferability of adversarial examples to implement policy induction attacks on DQNs, and demonstrate its efficacy and impact through experimental study of a game-learning scenario.",0
"It is well-known that deep learning classifiers can be easily manipulated by intentional input perturbations called adversarial examples. This study shows that reinforcement learning techniques based on Deep Q-Networks (DQNs) are also susceptible to such attacks, and that adversarial examples can be transferred across different DQN models. Additionally, the study presents a new type of attack that takes advantage of this vulnerability to manipulate policies and induce changes in the learning process of DQNs. To achieve this, the authors propose an attack mechanism that uses the transferability of adversarial examples to implement policy induction attacks on DQNs, and demonstrate its effectiveness through an experiment involving game-learning.",1
"The combinatorial explosion that plagues planning and reinforcement learning (RL) algorithms can be moderated using state abstraction. Prohibitively large task representations can be condensed such that essential information is preserved, and consequently, solutions are tractably computable. However, exact abstractions, which treat only fully-identical situations as equivalent, fail to present opportunities for abstraction in environments where no two situations are exactly alike. In this work, we investigate approximate state abstractions, which treat nearly-identical situations as equivalent. We present theoretical guarantees of the quality of behaviors derived from four types of approximate abstractions. Additionally, we empirically demonstrate that approximate abstractions lead to reduction in task complexity and bounded loss of optimality of behavior in a variety of environments.",0
"State abstraction can moderate the combinatorial explosion in planning and reinforcement learning algorithms. This involves condensing large task representations while preserving essential information to make solutions more computable. However, exact abstractions only work in situations where all situations are identical. In this study, we explore approximate state abstractions, which treat almost-identical situations as equivalent. We provide theoretical guarantees on the quality of behaviors from four types of approximate abstractions and demonstrate through empirical evidence that approximate abstractions can reduce task complexity and minimize the loss of optimal behavior in various environments.",1
"Providing Reinforcement Learning agents with expert advice can dramatically improve various aspects of learning. Prior work has developed teaching protocols that enable agents to learn efficiently in complex environments; many of these methods tailor the teacher's guidance to agents with a particular representation or underlying learning scheme, offering effective but specialized teaching procedures. In this work, we explore protocol programs, an agent-agnostic schema for Human-in-the-Loop Reinforcement Learning. Our goal is to incorporate the beneficial properties of a human teacher into Reinforcement Learning without making strong assumptions about the inner workings of the agent. We show how to represent existing approaches such as action pruning, reward shaping, and training in simulation as special cases of our schema and conduct preliminary experiments on simple domains.",0
"The inclusion of expert advice for Reinforcement Learning agents has a significant impact on their learning abilities. Previous research has developed teaching methods that aid agents in learning efficiently in complex environments. However, these methods are tailored to suit agents with particular representations or learning schemes, making them effective but specialized. In this study, we explore a schema called protocol programs, which is not agent-specific, for Human-in-the-Loop Reinforcement Learning. Our objective is to integrate the beneficial aspects of a human teacher into Reinforcement Learning without making assumptions concerning the agent's internal workings. We demonstrate how to represent existing approaches, such as action pruning, reward shaping, and training in simulation, as specific instances of our schema and conduct initial experiments on simple domains.",1
"This work presents a fast and scalable algorithm for incremental learning of Gaussian mixture models. By performing rank-one updates on its precision matrices and determinants, its asymptotic time complexity is of \BigO{NKD^2} for $N$ data points, $K$ Gaussian components and $D$ dimensions. The resulting algorithm can be applied to high dimensional tasks, and this is confirmed by applying it to the classification datasets MNIST and CIFAR-10. Additionally, in order to show the algorithm's applicability to function approximation and control tasks, it is applied to three reinforcement learning tasks and its data-efficiency is evaluated.",0
"An algorithm for incremental learning of Gaussian mixture models is presented in this work, which is fast and scalable. The algorithm's precision matrices and determinants are updated using rank-one updates, resulting in an asymptotic time complexity of \BigO{NKD^2} for $N$ data points, $K$ Gaussian components and $D$ dimensions. The algorithm can be applied to high dimensional tasks, as evidenced by its successful application to the classification datasets MNIST and CIFAR-10. To demonstrate its usefulness in function approximation and control tasks, the algorithm is also applied to three reinforcement learning tasks and its data-efficiency is evaluated.",1
"A generic and scalable Reinforcement Learning scheme for Artificial Neural Networks is presented, providing a general purpose learning machine. By reference to a node threshold three features are described 1) A mechanism for Primary Reinforcement, capable of solving linearly inseparable problems 2) The learning scheme is extended to include a mechanism for Conditioned Reinforcement, capable of forming long term strategy 3) The learning scheme is modified to use a threshold-based deep learning algorithm, providing a robust and biologically inspired alternative to backpropagation. The model may be used for supervised as well as unsupervised training regimes.",0
"Presented is a scalable and generic Reinforcement Learning plan for Artificial Neural Networks that serves as a multipurpose learning machine. The scheme discusses three features based on a node threshold: 1) A Primary Reinforcement mechanism that can solve linearly inseparable problems, 2) An extended learning scheme with a Conditioned Reinforcement mechanism capable of creating long-term strategies, and 3) A modified learning scheme using a threshold-based deep learning algorithm that offers a robust and biologically inspired alternative to backpropagation. This model can be utilized for both supervised and unsupervised training methods.",1
"The majority of online display ads are served through real-time bidding (RTB) --- each ad display impression is auctioned off in real-time when it is just being generated from a user visit. To place an ad automatically and optimally, it is critical for advertisers to devise a learning algorithm to cleverly bid an ad impression in real-time. Most previous works consider the bid decision as a static optimization problem of either treating the value of each impression independently or setting a bid price to each segment of ad volume. However, the bidding for a given ad campaign would repeatedly happen during its life span before the budget runs out. As such, each bid is strategically correlated by the constrained budget and the overall effectiveness of the campaign (e.g., the rewards from generated clicks), which is only observed after the campaign has completed. Thus, it is of great interest to devise an optimal bidding strategy sequentially so that the campaign budget can be dynamically allocated across all the available impressions on the basis of both the immediate and future rewards. In this paper, we formulate the bid decision process as a reinforcement learning problem, where the state space is represented by the auction information and the campaign's real-time parameters, while an action is the bid price to set. By modeling the state transition via auction competition, we build a Markov Decision Process framework for learning the optimal bidding policy to optimize the advertising performance in the dynamic real-time bidding environment. Furthermore, the scalability problem from the large real-world auction volume and campaign budget is well handled by state value approximation using neural networks.",0
"Real-time bidding (RTB) is the primary method for serving online display ads. Each ad impression is auctioned off in real-time as it is generated from a user visit. Advertisers must use a learning algorithm to bid on ad impressions in real-time to place ads automatically and optimally. Previous research has treated bid decisions as a static optimization problem, but bidding for a campaign occurs repeatedly during its lifespan, and each bid is strategically correlated with the constrained budget and overall effectiveness of the campaign. Therefore, it is essential to develop an optimal bidding strategy sequentially that dynamically allocates the campaign budget across all available impressions based on immediate and future rewards. We formulate the bid decision process as a reinforcement learning problem, where the state space represents auction information and campaign parameters, and the action is the bid price. Using Markov Decision Process and neural networks, we develop an optimal bidding policy to optimize advertising performance in the dynamic real-time bidding environment and handle the scalability problem of large auction volumes and campaign budgets.",1
"Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting \textit{Deep Recurrent Q-Network} (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.",0
"Complex tasks have been successfully tackled by Deep Reinforcement Learning using proficient controllers. However, these controllers possess limited memory and rely on complete game screen perception at each decision point. This article examines the impact of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The outcome, known as the Deep Recurrent Q-Network (DRQN), can only view a single frame at each timestep, but it can integrate information over time and replicate DQN's performance on standard Atari games and partially observed equivalents that have flickering game screens. DRQN's performance scales as a function of observability when trained with partial observations and evaluated with incrementally more complete observations. Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's. Therefore, recurrency is a viable alternative to stacking a history of frames in DQN's input layer, and while there is no systematic advantage of using a recurrent net to learn how to play the game, it can better adapt at evaluation time if the quality of observations changes.",1
"Deep Reinforcement Learning has enabled the learning of policies for complex tasks in partially observable environments, without explicitly learning the underlying model of the tasks. While such model-free methods achieve considerable performance, they often ignore the structure of task. We present a natural representation of to Reinforcement Learning (RL) problems using Recurrent Convolutional Neural Networks (RCNNs), to better exploit this inherent structure. We define 3 such RCNNs, whose forward passes execute an efficient Value Iteration, propagate beliefs of state in partially observable environments, and choose optimal actions respectively. Backpropagating gradients through these RCNNs allows the system to explicitly learn the Transition Model and Reward Function associated with the underlying MDP, serving as an elegant alternative to classical model-based RL. We evaluate the proposed algorithms in simulation, considering a robot planning problem. We demonstrate the capability of our framework to reduce the cost of replanning, learn accurate MDP models, and finally re-plan with learnt models to achieve near-optimal policies.",0
"Deep Reinforcement Learning has made it possible to learn policies for complicated tasks in partially observable environments without explicitly learning the task's underlying model. However, while model-free methods perform well, they often neglect the task's structure. To address this, we introduce a natural representation of Reinforcement Learning problems using Recurrent Convolutional Neural Networks (RCNNs), which better leverage this inherent structure. We propose three RCNNs that use an efficient Value Iteration during their forward passes, propagate beliefs of the state in partially observable environments, and select optimal actions, respectively. By backpropagating gradients through these RCNNs, the system can learn the Transition Model and Reward Function associated with the underlying MDP explicitly, providing an elegant alternative to classical model-based RL. We assess the proposed algorithms in a robot planning scenario in simulation, demonstrating our framework's ability to reduce the cost of replanning, learn precise MDP models, and finally re-plan with learned models to achieve policies that are almost optimal.",1
"Many machine learning tasks, such as learning with invariance and policy evaluation in reinforcement learning, can be characterized as problems of learning from conditional distributions. In such problems, each sample $x$ itself is associated with a conditional distribution $p(z|x)$ represented by samples $\{z_i\}_{i=1}^M$, and the goal is to learn a function $f$ that links these conditional distributions to target values $y$. These learning problems become very challenging when we only have limited samples or in the extreme case only one sample from each conditional distribution. Commonly used approaches either assume that $z$ is independent of $x$, or require an overwhelmingly large samples from each conditional distribution.   To address these challenges, we propose a novel approach which employs a new min-max reformulation of the learning from conditional distribution problem. With such new reformulation, we only need to deal with the joint distribution $p(z,x)$. We also design an efficient learning algorithm, Embedding-SGD, and establish theoretical sample complexity for such problems. Finally, our numerical experiments on both synthetic and real-world datasets show that the proposed approach can significantly improve over the existing algorithms.",0
"Learning from conditional distributions is a common problem in machine learning, including learning with invariance and policy evaluation in reinforcement learning. Each sample $x$ in these problems is associated with a conditional distribution $p(z|x)$ represented by samples $\{z_i\}_{i=1}^M$, and the goal is to learn a function $f$ that links these conditional distributions to target values $y$. However, limited samples or only one sample from each conditional distribution can make these learning problems very challenging. Existing approaches either assume independence between $z$ and $x, or require an overwhelming number of samples from each conditional distribution. To address these challenges, we propose a new min-max reformulation approach that only requires dealing with the joint distribution $p(z,x)$. We introduce a new learning algorithm, Embedding-SGD, and provide theoretical sample complexity for such problems. Numerical experiments on synthetic and real-world datasets show that our approach outperforms existing algorithms.",1
"Temporal Difference learning or TD($\lambda$) is a fundamental algorithm in the field of reinforcement learning. However, setting TD's $\lambda$ parameter, which controls the timescale of TD updates, is generally left up to the practitioner. We formalize the $\lambda$ selection problem as a bias-variance trade-off where the solution is the value of $\lambda$ that leads to the smallest Mean Squared Value Error (MSVE). To solve this trade-off we suggest applying Leave-One-Trajectory-Out Cross-Validation (LOTO-CV) to search the space of $\lambda$ values. Unfortunately, this approach is too computationally expensive for most practical applications. For Least Squares TD (LSTD) we show that LOTO-CV can be implemented efficiently to automatically tune $\lambda$ and apply function optimization methods to efficiently search the space of $\lambda$ values. The resulting algorithm, ALLSTD, is parameter free and our experiments demonstrate that ALLSTD is significantly computationally faster than the na\""{i}ve LOTO-CV implementation while achieving similar performance.",0
"TD($\lambda$) is an essential algorithm in reinforcement learning, yet the practitioner is typically responsible for determining the value of TD's $\lambda$ parameter, which governs the timescale of TD updates. We propose a method for addressing the $\lambda$ selection problem by balancing bias and variance, with the optimal $\lambda$ value being one that produces the smallest Mean Squared Value Error (MSVE). To accomplish this, we suggest using Leave-One-Trajectory-Out Cross-Validation (LOTO-CV) to explore the range of $\lambda$ values. However, LOTO-CV is computationally expensive for most practical applications. To address this, we present a parameter-free algorithm called ALLSTD, which applies LOTO-CV to Least Squares TD (LSTD) to automatically tune $\lambda$. We also use function optimization methods to search the space of $\lambda$ values efficiently. Our experiments demonstrate that ALLSTD is significantly faster than the naive LOTO-CV implementation while achieving comparable performance.",1
"Reinforcement learning is considered to be a strong AI paradigm which can be used to teach machines through interaction with the environment and learning from their mistakes, but it has not yet been successfully used for automotive applications. There has recently been a revival of interest in the topic, however, driven by the ability of deep learning algorithms to learn good representations of the environment. Motivated by Google DeepMind's successful demonstrations of learning for games from Breakout to Go, we will propose different methods for autonomous driving using deep reinforcement learning. This is of particular interest as it is difficult to pose autonomous driving as a supervised learning problem as it has a strong interaction with the environment including other vehicles, pedestrians and roadworks. As this is a relatively new area of research for autonomous driving, we will formulate two main categories of algorithms: 1) Discrete actions category, and 2) Continuous actions category. For the discrete actions category, we will deal with Deep Q-Network Algorithm (DQN) while for the continuous actions category, we will deal with Deep Deterministic Actor Critic Algorithm (DDAC). In addition to that, We will also discover the performance of these two categories on an open source car simulator for Racing called (TORCS) which stands for The Open Racing car Simulator. Our simulation results demonstrate learning of autonomous maneuvering in a scenario of complex road curvatures and simple interaction with other vehicles. Finally, we explain the effect of some restricted conditions, put on the car during the learning phase, on the convergence time for finishing its learning phase.",0
"Reinforcement learning is an artificial intelligence paradigm that involves the use of environmental interaction and learning from mistakes to teach machines. Despite being a strong AI approach, it has not been successfully applied to automotive applications. However, with the recent advancements in deep learning algorithms and their ability to learn representations of the environment, interest in the topic has been rekindled. Inspired by Google DeepMind's success in teaching games from Breakout to Go, this study proposes different autonomous driving methods using deep reinforcement learning. Autonomous driving is difficult to pose as a supervised learning problem due to its strong interaction with the environment, including other vehicles, pedestrians, and roadworks. As a new area of research, this study formulates two main categories of algorithms: discrete and continuous actions. The former uses the Deep Q-Network Algorithm (DQN), while the latter uses the Deep Deterministic Actor Critic Algorithm (DDAC). The study also evaluates the performance of these categories on an open-source car simulator called The Open Racing Car Simulator (TORCS). Simulation results demonstrate that the proposed methods can effectively teach autonomous maneuvering in complex road curvatures and simple interactions with other vehicles. Additionally, the study explores the impact of restricted conditions imposed on the car during the learning phase on the convergence time for finishing its training.",1
"In machine learning, error back-propagation in multi-layer neural networks (deep learning) has been impressively successful in supervised and reinforcement learning tasks. As a model for learning in the brain, however, deep learning has long been regarded as implausible, since it relies in its basic form on a non-local plasticity rule. To overcome this problem, energy-based models with local contrastive Hebbian learning were proposed and tested on a classification task with networks of rate neurons. We extended this work by implementing and testing such a model with networks of leaky integrate-and-fire neurons. Preliminary results indicate that it is possible to learn a non-linear regression task with hidden layers, spiking neurons and a local synaptic plasticity rule.",0
"The use of error back-propagation in multi-layer neural networks, also known as deep learning, has been highly successful in supervised and reinforcement learning tasks within machine learning. However, when considering it as a model for learning in the brain, deep learning has been viewed as unlikely due to its reliance on a non-local plasticity rule. In order to address this issue, energy-based models were proposed and tested on a classification task using networks of rate neurons with local contrastive Hebbian learning. We extended this research by implementing and testing a similar model with networks of leaky integrate-and-fire neurons. Our preliminary findings suggest that it is feasible to learn a non-linear regression task with hidden layers, spiking neurons, and a local synaptic plasticity rule.",1
"We develop a general problem setting for training and testing the ability of agents to gather information efficiently. Specifically, we present a collection of tasks in which success requires searching through a partially-observed environment, for fragments of information which can be pieced together to accomplish various goals. We combine deep architectures with techniques from reinforcement learning to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We empirically demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.",0
"Our aim is to create a universal scenario for testing the proficiency of agents in efficiently collecting information. To achieve this, we have compiled a set of tasks that involve exploring a partially visible environment to gather relevant fragments of information that can be utilized to achieve different objectives. We have employed a combination of deep architectures and reinforcement learning techniques to train agents that can successfully tackle these tasks. We have influenced the behavior of these agents through a combination of extrinsic and intrinsic rewards. Our experiments have shown that these agents effectively learn to search proactively and astutely for new information to reduce their uncertainty and leverage the information they have already procured.",1
"We study the online estimation of the optimal policy of a Markov decision process (MDP). We propose a class of Stochastic Primal-Dual (SPD) methods which exploit the inherent minimax duality of Bellman equations. The SPD methods update a few coordinates of the value and policy estimates as a new state transition is observed. These methods use small storage and has low computational complexity per iteration. The SPD methods find an absolute-$\epsilon$-optimal policy, with high probability, using $\mathcal{O}\left(\frac{|\mathcal{S}|^4 |\mathcal{A}|^2\sigma^2 }{(1-\gamma)^6\epsilon^2} \right)$ iterations/samples for the infinite-horizon discounted-reward MDP and $\mathcal{O}\left(\frac{|\mathcal{S}|^4 |\mathcal{A}|^2H^6\sigma^2 }{\epsilon^2} \right)$ for the finite-horizon MDP.",0
"Our focus is on the online estimation of the optimal policy of a Markov decision process (MDP). To achieve this, we have developed a class of Stochastic Primal-Dual (SPD) methods that take advantage of the minimax duality inherent in Bellman equations. By updating only a few coordinates of the value and policy estimates when a new state transition is observed, these methods require minimal storage and have low computational complexity per iteration. Using these methods, we can obtain an absolute-$\epsilon$-optimal policy with high probability. For the infinite-horizon discounted-reward MDP, this can be achieved in $\mathcal{O}\left(\frac{|\mathcal{S}|^4 |\mathcal{A}|^2\sigma^2 }{(1-\gamma)^6\epsilon^2} \right)$ iterations/samples, while for the finite-horizon MDP, it takes $\mathcal{O}\left(\frac{|\mathcal{S}|^4 |\mathcal{A}|^2H^6\sigma^2 }{\epsilon^2} \right)$ iterations/samples.",1
"Superoptimization requires the estimation of the best program for a given computational task. In order to deal with large programs, superoptimization techniques perform a stochastic search. This involves proposing a modification of the current program, which is accepted or rejected based on the improvement achieved. The state of the art method uses uniform proposal distributions, which fails to exploit the problem structure to the fullest. To alleviate this deficiency, we learn a proposal distribution over possible modifications using Reinforcement Learning. We provide convincing results on the superoptimization of ""Hacker's Delight"" programs.",0
"The process of superoptimization involves determining the optimal program for a specific computational task. To handle complex programs, stochastic search methods are employed by superoptimization techniques. This entails suggesting a modification to the current program, which is evaluated and either accepted or rejected based on its efficacy. However, the existing approach uses uniform proposal distributions that do not fully utilize the problem framework. To address this limitation, we utilize Reinforcement Learning to learn a proposal distribution that covers potential modifications. Our research produces compelling outcomes in the superoptimization of programs in ""Hacker's Delight.""",1
"Exploration has been a crucial part of reinforcement learning, yet several important questions concerning exploration efficiency are still not answered satisfactorily by existing analytical frameworks. These questions include exploration parameter setting, situation analysis, and hardness of MDPs, all of which are unavoidable for practitioners. To bridge the gap between the theory and practice, we propose a new analytical framework called the success probability of exploration. We show that those important questions of exploration above can all be answered under our framework, and the answers provided by our framework meet the needs of practitioners better than the existing ones. More importantly, we introduce a concrete and practical approach to evaluating the success probabilities in certain MDPs without the need of actually running the learning algorithm. We then provide empirical results to verify our approach, and demonstrate how the success probability of exploration can be used to analyse and predict the behaviours and possible outcomes of exploration, which are the keys to the answer of the important questions of exploration.",0
"Despite the importance of exploration in reinforcement learning, existing analytical frameworks do not satisfactorily answer crucial questions related to exploration efficiency, such as parameter setting, situation analysis, and MDP complexity. To bridge this gap, we propose a new analytical framework called the success probability of exploration, which provides more practical answers to these questions. Our framework allows for the evaluation of success probabilities in certain MDPs without actually implementing the learning algorithm, and we provide empirical results to validate its effectiveness. By analyzing and predicting the behaviors and outcomes of exploration, our framework provides key insights into the important questions of exploration.",1
"This paper studies systematic exploration for reinforcement learning with rich observations and function approximation. We introduce a new model called contextual decision processes, that unifies and generalizes most prior settings. Our first contribution is a complexity measure, the Bellman rank, that we show enables tractable learning of near-optimal behavior in these processes and is naturally small for many well-studied reinforcement learning settings. Our second contribution is a new reinforcement learning algorithm that engages in systematic exploration to learn contextual decision processes with low Bellman rank. Our algorithm provably learns near-optimal behavior with a number of samples that is polynomial in all relevant parameters but independent of the number of unique observations. The approach uses Bellman error minimization with optimistic exploration and provides new insights into efficient exploration for reinforcement learning with function approximation.",0
"The objective of this paper is to investigate the exploration process in reinforcement learning, particularly in scenarios where there is an abundance of observations and function approximation. To achieve this, we propose a new model called contextual decision processes, which combines and extends multiple pre-existing settings. We also introduce a measure of complexity, the Bellman rank, which facilitates the learning of nearly optimal behavior in these processes and is generally small for commonly researched reinforcement learning situations. Additionally, we present a new algorithm for reinforcement learning that systematically explores contextual decision processes with low Bellman rank, resulting in behavior that is almost optimal. The algorithm requires a polynomial number of samples of all relevant parameters, independent of the quantity of unique observations. Our approach employs Bellman error minimization with optimistic exploration, which provides a fresh perspective on effective exploration for reinforcement learning with function approximation.",1
"Handwriting is a skill learned by humans from a very early age. The ability to develop one's own unique handwriting as well as mimic another person's handwriting is a task learned by the brain with practice. This paper deals with this very problem where an intelligent system tries to learn the handwriting of an entity using Generative Adversarial Networks (GANs). We propose a modified architecture of DCGAN (Radford, Metz, and Chintala 2015) to achieve this. We also discuss about applying reinforcement learning techniques to achieve faster learning. Our algorithm hopes to give new insights in this area and its uses include identification of forged documents, signature verification, computer generated art, digitization of documents among others. Our early implementation of the algorithm illustrates a good performance with MNIST datasets.",0
"Humans learn the skill of handwriting early on in life, and with practice, the brain can develop a unique style and even imitate others. This paper focuses on an intelligent system that uses Generative Adversarial Networks (GANs) to learn an entity's handwriting. We propose a modified version of DCGAN (Radford, Metz, and Chintala 2015) and explore the possibility of using reinforcement learning techniques for faster learning. Our algorithm has potential uses in identifying forged documents, signature verification, computer-generated art, and digitization of documents. Our initial implementation of the algorithm has shown promising results with MNIST datasets.",1
"Training robots to perceive, act and communicate using multiple modalities still represents a challenging problem, particularly if robots are expected to learn efficiently from small sets of example interactions. We describe a learning approach as a step in this direction, where we teach a humanoid robot how to play the game of noughts and crosses. Given that multiple multimodal skills can be trained to play this game, we focus our attention to training the robot to perceive the game, and to interact in this game. Our multimodal deep reinforcement learning agent perceives multimodal features and exhibits verbal and non-verbal actions while playing. Experimental results using simulations show that the robot can learn to win or draw up to 98% of the games. A pilot test of the proposed multimodal system for the targeted game---integrating speech, vision and gestures---reports that reasonable and fluent interactions can be achieved using the proposed approach.",0
"Teaching robots to perceive, act, and communicate through various modalities is still a challenging task, especially when they are expected to efficiently learn from limited examples. In this study, we introduce a learning approach that involves a humanoid robot learning how to play noughts and crosses. Although there are several multi-modal skills that can be taught to play this game, we have focused on training the robot to perceive and interact with the game. Our method utilizes a multi-modal deep reinforcement learning agent that can perceive various features and display verbal and non-verbal actions while playing. Simulations have shown that the robot can learn to win or draw up to 98% of the games. A pilot test of the proposed multi-modal system, which integrates speech, vision, and gestures, reports that the approach can achieve reasonable and fluent interactions during gameplay.",1
"Quantum control is valuable for various quantum technologies such as high-fidelity gates for universal quantum computing, adaptive quantum-enhanced metrology, and ultra-cold atom manipulation. Although supervised machine learning and reinforcement learning are widely used for optimizing control parameters in classical systems, quantum control for parameter optimization is mainly pursued via gradient-based greedy algorithms. Although the quantum fitness landscape is often compatible with greedy algorithms, sometimes greedy algorithms yield poor results, especially for large-dimensional quantum systems. We employ differential evolution algorithms to circumvent the stagnation problem of non-convex optimization. We improve quantum control fidelity for noisy system by averaging over the objective function. To reduce computational cost, we introduce heuristics for early termination of runs and for adaptive selection of search subspaces. Our implementation is massively parallel and vectorized to reduce run time even further. We demonstrate our methods with two examples, namely quantum phase estimation and quantum gate design, for which we achieve superior fidelity and scalability than obtained using greedy algorithms.",0
"Quantum control has numerous applications in quantum technologies, such as creating high-fidelity gates for universal quantum computing, manipulating ultra-cold atoms, and improving quantum-enhanced metrology through adaptation. While classical systems often use supervised machine learning and reinforcement learning for optimizing control parameters, quantum control relies mainly on gradient-based greedy algorithms. However, these algorithms can yield poor results, particularly for large-dimensional quantum systems. To address this, we use differential evolution algorithms, which overcome the stagnation problem of non-convex optimization. Furthermore, we enhance quantum control fidelity by averaging over the objective function and introduce heuristics for early termination of runs and adaptive selection of search subspaces to reduce computational cost. Our implementation is parallelized and vectorized, resulting in even faster run times. We showcase the effectiveness of our methods in two examples, quantum phase estimation and quantum gate design, where we achieve superior fidelity and scalability compared to greedy algorithms.",1
"We present a method for performing hierarchical object detection in images guided by a deep reinforcement learning agent. The key idea is to focus on those parts of the image that contain richer information and zoom on them. We train an intelligent agent that, given an image window, is capable of deciding where to focus the attention among five different predefined region candidates (smaller windows). This procedure is iterated providing a hierarchical image analysis.We compare two different candidate proposal strategies to guide the object search: with and without overlap. Moreover, our work compares two different strategies to extract features from a convolutional neural network for each region proposal: a first one that computes new feature maps for each region proposal, and a second one that computes the feature maps for the whole image to later generate crops for each region proposal. Experiments indicate better results for the overlapping candidate proposal strategy and a loss of performance for the cropped image features due to the loss of spatial resolution. We argue that, while this loss seems unavoidable when working with large amounts of object candidates, the much more reduced amount of region proposals generated by our reinforcement learning agent allows considering to extract features for each location without sharing convolutional computation among regions.",0
"Our method employs a deep reinforcement learning agent to guide hierarchical object detection in images. The agent identifies informative parts of the image and zooms in on them, using one of five predefined region candidates. This process is repeated for a hierarchical image analysis. We compare two candidate proposal strategies, with and without overlap, and two feature extraction strategies, one computing new feature maps for each proposal and the other computing feature maps for the entire image and generating crops for each proposal. Our experiments show better results for the overlapping candidate proposal strategy, and a decrease in performance for the cropped image features due to loss of spatial resolution. We suggest that, despite this loss, our reinforcement learning agent generates fewer region proposals, allowing us to extract features for each location without sharing convolutional computation among regions.",1
"Generative adversarial networks (GANs) are a recently proposed class of generative models in which a generator is trained to optimize a cost function that is being simultaneously learned by a discriminator. While the idea of learning cost functions is relatively new to the field of generative modeling, learning costs has long been studied in control and reinforcement learning (RL) domains, typically for imitation learning from demonstrations. In these fields, learning cost function underlying observed behavior is known as inverse reinforcement learning (IRL) or inverse optimal control. While at first the connection between cost learning in RL and cost learning in generative modeling may appear to be a superficial one, we show in this paper that certain IRL methods are in fact mathematically equivalent to GANs. In particular, we demonstrate an equivalence between a sample-based algorithm for maximum entropy IRL and a GAN in which the generator's density can be evaluated and is provided as an additional input to the discriminator. Interestingly, maximum entropy IRL is a special case of an energy-based model. We discuss the interpretation of GANs as an algorithm for training energy-based models, and relate this interpretation to other recent work that seeks to connect GANs and EBMs. By formally highlighting the connection between GANs, IRL, and EBMs, we hope that researchers in all three communities can better identify and apply transferable ideas from one domain to another, particularly for developing more stable and scalable algorithms: a major challenge in all three domains.",0
"Recently introduced, generative adversarial networks (GANs) are a type of generative models that involve a generator being trained to optimize a cost function, while a discriminator is simultaneously learning. Although the concept of learning cost functions is new to generative modeling, it has been studied for many years in control and reinforcement learning (RL) domains, mainly for imitation learning from demonstrations. In these fields, inverse reinforcement learning (IRL) or inverse optimal control is used to learn the cost function underlying observed behavior. While the relationship between cost learning in RL and generative modeling may seem superficial, this paper demonstrates that specific IRL methods are mathematically equivalent to GANs. This study highlights the connection between GANs, IRL, and energy-based models (EBMs), and it discusses the interpretation of GANs as an algorithm for training EBMs. By formally demonstrating the connection between these three communities, researchers can better identify and apply transferable ideas from one domain to another, especially for developing more stable and scalable algorithms, which is a significant challenge in all three domains.",1
"This work presents a multiscale framework to solve an inverse reinforcement learning (IRL) problem for continuous-time/state stochastic systems. We take advantage of a diffusion wavelet representation of the associated Markov chain to abstract the state space. This not only allows for effectively handling the large (and geometrically complex) decision space but also provides more interpretable representations of the demonstrated state trajectories and also of the resulting policy of IRL. In the proposed framework, the problem is divided into the global and local IRL, where the global approximation of the optimal value functions are obtained using coarse features and the local details are quantified using fine local features. An illustrative numerical example on robot path control in a complex environment is presented to verify the proposed method.",0
"This study introduces a multiscale approach for addressing the inverse reinforcement learning (IRL) challenge in stochastic systems with continuous-time/state. The technique leverages a diffusion wavelet representation of the associated Markov chain to simplify the state space, enabling effective management of the vast and complex decision space while also providing more easily interpretable representations of the state trajectories and the resulting IRL policy. The proposed framework divides the problem into two parts: global and local IRL. The optimal value functions are approximated using coarse features for the global component and fine local features for the local component. An example is provided to demonstrate the effectiveness of the proposed method in controlling robot paths in a complex environment.",1
"In this paper we introduce a new unsupervised reinforcement learning method for discovering the set of intrinsic options available to an agent. This set is learned by maximizing the number of different states an agent can reliably reach, as measured by the mutual information between the set of options and option termination states. To this end, we instantiate two policy gradient based algorithms, one that creates an explicit embedding space of options and one that represents options implicitly. The algorithms also provide an explicit measure of empowerment in a given state that can be used by an empowerment maximizing agent. The algorithm scales well with function approximation and we demonstrate the applicability of the algorithm on a range of tasks.",0
"The purpose of this article is to present a novel approach to unsupervised reinforcement learning that enables an agent to discover its intrinsic options. This is achieved by maximizing the number of reliable states that the agent can access, as assessed by the mutual information between the option set and option termination states. We utilize two policy gradient based algorithms, one that builds an explicit embedding space of options and one that represents options implicitly. Additionally, the algorithms provide an explicit measurement of empowerment in a given state which can be utilized by an empowerment maximizing agent. The algorithm is scalable with function approximation and we demonstrate its effectiveness on a variety of tasks.",1
"We present an attention-based model that reasons on human body shape and motion dynamics to identify individuals in the absence of RGB information, hence in the dark. Our approach leverages unique 4D spatio-temporal signatures to address the identification problem across days. Formulated as a reinforcement learning task, our model is based on a combination of convolutional and recurrent neural networks with the goal of identifying small, discriminative regions indicative of human identity. We demonstrate that our model produces state-of-the-art results on several published datasets given only depth images. We further study the robustness of our model towards viewpoint, appearance, and volumetric changes. Finally, we share insights gleaned from interpretable 2D, 3D, and 4D visualizations of our model's spatio-temporal attention.",0
"Our proposed model utilizes attention-based techniques to recognize individuals based on their body shape and movements, even in low-light conditions where RGB information is unavailable. By utilizing unique 4D spatio-temporal signatures, our approach can accurately identify individuals over multiple days. Using a combination of convolutional and recurrent neural networks, we treat identification as a reinforcement learning task and aim to identify small, distinctive regions that indicate human identity. Our model achieves state-of-the-art results on multiple datasets using only depth images, and we analyze its robustness to changes in viewpoint, appearance, and volume. Additionally, we provide insights into our model's spatio-temporal attention using interpretable 2D, 3D, and 4D visualizations.",1
"We present a Reinforcement Learning (RL) solution to the view planning problem (VPP), which generates a sequence of view points that are capable of sensing all accessible area of a given object represented as a 3D model. In doing so, the goal is to minimize the number of view points, making the VPP a class of set covering optimization problem (SCOP). The SCOP is NP-hard, and the inapproximability results tell us that the greedy algorithm provides the best approximation that runs in polynomial time. In order to find a solution that is better than the greedy algorithm, (i) we introduce a novel score function by exploiting the geometry of the 3D model, (ii) we model an intuitive human approach to VPP using this score function, and (iii) we cast VPP as a Markovian Decision Process (MDP), and solve the MDP in RL framework using well-known RL algorithms. In particular, we use SARSA, Watkins-Q and TD with function approximation to solve the MDP. We compare the results of our method with the baseline greedy algorithm in an extensive set of test objects, and show that we can out-perform the baseline in almost all cases.",0
"A Reinforcement Learning (RL) solution for the view planning problem (VPP) is presented, which involves generating a sequence of view points that can sense all accessible areas of a 3D object model. The aim is to minimize the number of view points, making VPP a type of set covering optimization problem (SCOP). Since SCOP is NP-hard, the greedy algorithm provides the best approximation that runs in polynomial time. However, to find a solution better than the greedy algorithm, a novel score function is introduced by exploiting the geometry of the 3D model. An intuitive human approach to VPP is then modeled using this score function, and VPP is cast as a Markovian Decision Process (MDP), which is solved using well-known RL algorithms like SARSA, Watkins-Q, and TD with function approximation. The results of this method are compared with the baseline greedy algorithm using an extensive set of test objects, and it is shown that the proposed method out-performs the baseline in almost all cases.",1
"We introduce exploration potential, a quantity that measures how much a reinforcement learning agent has explored its environment class. In contrast to information gain, exploration potential takes the problem's reward structure into account. This leads to an exploration criterion that is both necessary and sufficient for asymptotic optimality (learning to act optimally across the entire environment class). Our experiments in multi-armed bandits use exploration potential to illustrate how different algorithms make the tradeoff between exploration and exploitation.",0
"The concept of exploration potential is presented as a means of determining the extent to which a reinforcement learning agent has investigated its environment class. Unlike information gain, exploration potential considers the reward structure of the problem, resulting in an exploration criterion that meets both necessary and sufficient conditions for asymptotic optimality. In our multi-armed bandit experiments, we employ exploration potential to demonstrate how various algorithms balance exploration and exploitation.",1
"Balancing between computational efficiency and sample efficiency is an important goal in reinforcement learning. Temporal difference (TD) learning algorithms stochastically update the value function, with a linear time complexity in the number of features, whereas least-squares temporal difference (LSTD) algorithms are sample efficient but can be quadratic in the number of features. In this work, we develop an efficient incremental low-rank LSTD({\lambda}) algorithm that progresses towards the goal of better balancing computation and sample efficiency. The algorithm reduces the computation and storage complexity to the number of features times the chosen rank parameter while summarizing past samples efficiently to nearly obtain the sample complexity of LSTD. We derive a simulation bound on the solution given by truncated low-rank approximation, illustrating a bias- variance trade-off dependent on the choice of rank. We demonstrate that the algorithm effectively balances computational complexity and sample efficiency for policy evaluation in a benchmark task and a high-dimensional energy allocation domain.",0
"The objective of reinforcement learning is to strike a balance between computational efficiency and sample efficiency. Temporal difference (TD) learning algorithms update the value function in a stochastic manner with a time complexity that is linear in the number of features. In contrast, least-squares temporal difference (LSTD) algorithms are sample efficient but can be quadratic in the number of features. The aim of this study is to develop a low-rank LSTD({\lambda}) algorithm that is efficient and incremental, and which balances computation and sample efficiency. The algorithm reduces the computation and storage complexity to the number of features times the selected rank parameter while summarizing past samples effectively to achieve almost the same sample complexity as LSTD. Our research establishes a simulation bound on the solution provided by truncated low-rank approximation, demonstrating a bias-variance trade-off that depends on the rank selection. We demonstrate that the algorithm effectively balances computational complexity and sample efficiency in a benchmark task and a high-dimensional energy allocation domain for policy evaluation.",1
"Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880\% expert human performance, and a challenging suite of first-person, three-dimensional \emph{Labyrinth} tasks leading to a mean speedup in learning of 10$\times$ and averaging 87\% expert human performance on Labyrinth.",0
"Directly maximising cumulative reward has enabled deep reinforcement learning agents to achieve state-of-the-art results. However, training signals in environments are diverse. In this study, we present an agent that utilises reinforcement learning to simultaneously maximise multiple pseudo-reward functions. These tasks share a common representation that develops like unsupervised learning without external rewards. We also introduce a novel mechanism that focuses this representation on extrinsic rewards, facilitating rapid adaptation to the most relevant aspects of the task. Our agent surpasses the previous state-of-the-art results in Atari, with an average of 880\% expert human performance. It also performs well in a challenging suite of first-person, three-dimensional Labyrinth tasks, resulting in a mean speedup in learning of 10$\times$ and averaging 87\% expert human performance.",1
"In classical reinforcement learning, when exploring an environment, agents accept arbitrary short term loss for long term gain. This is infeasible for safety critical applications, such as robotics, where even a single unsafe action may cause system failure. In this paper, we address the problem of safely exploring finite Markov decision processes (MDP). We define safety in terms of an, a priori unknown, safety constraint that depends on states and actions. We aim to explore the MDP under this constraint, assuming that the unknown function satisfies regularity conditions expressed via a Gaussian process prior. We develop a novel algorithm for this task and prove that it is able to completely explore the safely reachable part of the MDP without violating the safety constraint. To achieve this, it cautiously explores safe states and actions in order to gain statistical confidence about the safety of unvisited state-action pairs from noisy observations collected while navigating the environment. Moreover, the algorithm explicitly considers reachability when exploring the MDP, ensuring that it does not get stuck in any state with no safe way out. We demonstrate our method on digital terrain models for the task of exploring an unknown map with a rover.",0
"Classical reinforcement learning involves accepting short term loss in order to gain long term benefits when exploring an environment. However, this approach is not feasible in safety critical applications like robotics where even a single unsafe action can lead to system failure. The aim of this study is to address the problem of exploring finite Markov decision processes (MDP) safely. Safety is defined in terms of an unknown safety constraint that depends on states and actions, and the study assumes that the unknown function satisfies regularity conditions expressed via a Gaussian process prior. To achieve this, a novel algorithm is developed that cautiously explores safe states and actions to gain statistical confidence about the safety of unvisited state-action pairs while navigating the environment. Additionally, the algorithm considers reachability to ensure that it does not get stuck in any state with no safe way out. The study demonstrates the effectiveness of the algorithm by exploring an unknown map with a rover using digital terrain models.",1
"We consider Bayesian optimization of an expensive-to-evaluate black-box objective function, where we also have access to cheaper approximations of the objective. In general, such approximations arise in applications such as reinforcement learning, engineering, and the natural sciences, and are subject to an inherent, unknown bias. This model discrepancy is caused by an inadequate internal model that deviates from reality and can vary over the domain, making the utilization of these approximations a non-trivial task.   We present a novel algorithm that provides a rigorous mathematical treatment of the uncertainties arising from model discrepancies and noisy observations. Its optimization decisions rely on a value of information analysis that extends the Knowledge Gradient factor to the setting of multiple information sources that vary in cost: each sampling decision maximizes the predicted benefit per unit cost.   We conduct an experimental evaluation that demonstrates that the method consistently outperforms other state-of-the-art techniques: it finds designs of considerably higher objective value and additionally inflicts less cost in the exploration process.",0
"Our focus is on Bayesian optimization of a costly-to-evaluate objective function that is concealed from us, and we have access to less expensive approximations of the objective. These approximations are commonly found in various fields, including engineering, natural science, and reinforcement learning, but they are subject to unknown biases that stem from model discrepancies. This discrepancy is caused by a deficient internal model that deviates from reality and varies over the domain, making the use of these approximations challenging. We introduce a new algorithm that mathematically handles the uncertainties of model discrepancies and noisy observations, using a value of information analysis that extends the Knowledge Gradient factor to multiple information sources with varying costs. Our approach optimizes each sampling decision by maximizing the anticipated benefit per unit cost. We conduct an experimental evaluation that demonstrates our method outperforms other state-of-the-art techniques by producing higher objective values and reducing the exploration process's cost.",1
"In this work, we take a fresh look at some old and new algorithms for off-policy, return-based reinforcement learning. Expressing these in a common form, we derive a novel algorithm, Retrace($\lambda$), with three desired properties: (1) it has low variance; (2) it safely uses samples collected from any behaviour policy, whatever its degree of ""off-policyness""; and (3) it is efficient as it makes the best use of samples collected from near on-policy behaviour policies. We analyze the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms. We believe this is the first return-based off-policy control algorithm converging a.s. to $Q^*$ without the GLIE assumption (Greedy in the Limit with Infinite Exploration). As a corollary, we prove the convergence of Watkins' Q($\lambda$), which was an open problem since 1989. We illustrate the benefits of Retrace($\lambda$) on a standard suite of Atari 2600 games.",0
"This study presents a fresh perspective on old and new algorithms for off-policy, return-based reinforcement learning. By standardizing these algorithms, we introduce a novel approach called Retrace($\lambda$) that exhibits three key characteristics: (1) it has low variance; (2) it can safely use samples from any behavior policy, regardless of its degree of ""off-policyness""; and (3) it is efficient by optimizing sample collection from near on-policy behavior policies. Our analysis shows that the related operator is contractive for both off-policy policy evaluation and control settings, and we develop sample-based algorithms. We believe that Retrace($\lambda$) is the first return-based off-policy control algorithm that converges a.s. to $Q^*$ without requiring the GLIE assumption (Greedy in the Limit with Infinite Exploration). Furthermore, we prove the convergence of Watkins' Q($\lambda$), which has been an open problem since 1989. Finally, we demonstrate the benefits of Retrace($\lambda$) using a standard suite of Atari 2600 games.",1
"One of the classical problems in machine learning and data mining is feature selection. A feature selection algorithm is expected to be quick, and at the same time it should show high performance. MeLiF algorithm effectively solves this problem using ensembles of ranking filters. This article describes two different ways to improve MeLiF algorithm performance with parallelization. Experiments show that proposed schemes significantly improves algorithm performance and increase feature selection quality.",0
Feature selection is a well-known challenge in data mining and machine learning. It is necessary for a feature selection algorithm to be both efficient and accurate. The MeLiF algorithm tackles this issue by utilizing ensembles of ranking filters. This article presents two distinct methods for enhancing the performance of the MeLiF algorithm through parallelization. The experiments conducted demonstrate that these approaches considerably improve the algorithm's performance and enhance the quality of feature selection.,1
"We propose a novel training algorithm for reinforcement learning which combines the strength of deep Q-learning with a constrained optimization approach to tighten optimality and encourage faster reward propagation. Our novel technique makes deep reinforcement learning more practical by drastically reducing the training time. We evaluate the performance of our approach on the 49 games of the challenging Arcade Learning Environment, and report significant improvements in both training time and accuracy.",0
"Our suggested approach for reinforcement learning involves integrating deep Q-learning with a constrained optimization methodology to enhance optimality and promote faster reward propagation. This innovative methodology has the potential to considerably decrease the training duration, thereby rendering deep reinforcement learning more feasible. Our experiment involved testing this approach on the challenging Arcade Learning Environment's 49 games, and we observed substantial improvements in training time and accuracy.",1
"Ensuring transportation systems are efficient is a priority for modern society. Technological advances have made it possible for transportation systems to collect large volumes of varied data on an unprecedented scale. We propose a traffic signal control system which takes advantage of this new, high quality data, with minimal abstraction compared to other proposed systems. We apply modern deep reinforcement learning methods to build a truly adaptive traffic signal control agent in the traffic microsimulator SUMO. We propose a new state space, the discrete traffic state encoding, which is information dense. The discrete traffic state encoding is used as input to a deep convolutional neural network, trained using Q-learning with experience replay. Our agent was compared against a one hidden layer neural network traffic signal control agent and reduces average cumulative delay by 82%, average queue length by 66% and average travel time by 20%.",0
"The optimization of transportation systems is a crucial objective for contemporary society. Recent technological advancements have enabled transportation systems to gather substantial amounts of diverse data on an unprecedented scale. Our proposition entails a traffic signal control system that utilizes this novel, high-quality data with minimal abstraction in comparison to other proposed systems. We utilize advanced deep reinforcement learning techniques to develop a genuinely adaptive traffic signal control agent within the traffic microsimulator SUMO. We introduce a new state space, the discrete traffic state encoding, which is highly informative. This encoding is utilized as an input to a deep convolutional neural network that is trained with Q-learning and experience replay. Our agent demonstrated superior performance compared to a neural network traffic signal control agent with just one hidden layer, reducing average cumulative delay by 82%, average queue length by 66%, and average travel time by 20%.",1
"The use of deep reinforcement learning allows for high-dimensional state descriptors, but little is known about how the choice of action representation impacts the learning difficulty and the resulting performance. We compare the impact of four different action parameterizations (torques, muscle-activations, target joint angles, and target joint-angle velocities) in terms of learning time, policy robustness, motion quality, and policy query rates. Our results are evaluated on a gait-cycle imitation task for multiple planar articulated figures and multiple gaits. We demonstrate that the local feedback provided by higher-level action parameterizations can significantly impact the learning, robustness, and quality of the resulting policies.",0
"The utilization of deep reinforcement learning permits the utilization of high-dimensional state descriptors; however, there is inadequate understanding of how the selection of action representation influences the learning complexity and the consequent performance achieved. We investigate the impact of four distinct action parameterizations (torques, muscle-activations, target joint angles, and target joint-angle velocities) concerning the time required for learning, policy robustness, motion quality, and policy query rates. We assess our findings on a gait-cycle imitation task for numerous planar articulated figures and gaits. Our research demonstrates that the local feedback provided by higher-level action parameterizations significantly affects the learning, robustness, and quality of the resulting policies.",1
"In reinforcement learning, the standard criterion to evaluate policies in a state is the expectation of (discounted) sum of rewards. However, this criterion may not always be suitable, we consider an alternative criterion based on the notion of quantiles. In the case of episodic reinforcement learning problems, we propose an algorithm based on stochastic approximation with two timescales. We evaluate our proposition on a simple model of the TV show, Who wants to be a millionaire.",0
"The typical way to assess policies in a given state in reinforcement learning is by measuring the expected (discounted) sum of rewards. However, this approach may not always be appropriate. As an alternative, we suggest using a criterion rooted in the concept of quantiles. For episodic reinforcement learning challenges, we introduce an algorithm utilizing stochastic approximation with two distinct timescales. The effectiveness of our proposal is tested on a basic Who Wants to Be a Millionaire model.",1
"Learning effective configurations in computer systems without hand-crafting models for every parameter is a long-standing problem. This paper investigates the use of deep reinforcement learning for runtime parameters of cloud databases under latency constraints. Cloud services serve up to thousands of concurrent requests per second and can adjust critical parameters by leveraging performance metrics. In this work, we use continuous deep reinforcement learning to learn optimal cache expirations for HTTP caching in content delivery networks. To this end, we introduce a technique for asynchronous experience management called delayed experience injection, which facilitates delayed reward and next-state computation in concurrent environments where measurements are not immediately available. Evaluation results show that our approach based on normalized advantage functions and asynchronous CPU-only training outperforms a statistical estimator.",0
"The issue of acquiring effective configurations in computer systems without manually crafting models for each parameter has been a persistent challenge. This study examines the application of deep reinforcement learning for real-time parameters of cloud databases with limitations on latency. Cloud services can handle thousands of simultaneous requests per second and can alter crucial parameters using performance metrics. Our study employs continuous deep reinforcement learning to determine optimal cache expirations for HTTP caching in content delivery networks. To achieve this, we introduce an asynchronous experience management approach known as delayed experience injection, which facilitates delayed reward and next-state computation in concurrent environments where measurements are not immediately available. Our assessment shows that our method, which is based on normalized advantage functions and asynchronous CPU-only training, outperforms a statistical estimator.",1
"We propose and study a new model for reinforcement learning with rich observations, generalizing contextual bandits to sequential decision making. These models require an agent to take actions based on observations (features) with the goal of achieving long-term performance competitive with a large set of policies. To avoid barriers to sample-efficient learning associated with large observation spaces and general POMDPs, we focus on problems that can be summarized by a small number of hidden states and have long-term rewards that are predictable by a reactive function class. In this setting, we design and analyze a new reinforcement learning algorithm, Least Squares Value Elimination by Exploration. We prove that the algorithm learns near optimal behavior after a number of episodes that is polynomial in all relevant parameters, logarithmic in the number of policies, and independent of the size of the observation space. Our result provides theoretical justification for reinforcement learning with function approximation.",0
"A fresh model for reinforcement learning with extensive observations is proposed and examined, extending contextual bandits to sequential decision making. These models demand an agent to act based on observations with the aim of achieving long-term performance that is competitive with a vast range of policies. To overcome the obstacles associated with learning samples efficiently for large observation spaces and general POMDPs, we concentrate on problems that can be summarized by a small number of hidden states and have long-term rewards that can be predicted by a reactive function class. In this scenario, a new reinforcement learning algorithm, Least Squares Value Elimination by Exploration, has been devised and analyzed. We prove that the algorithm can learn near-optimal behavior after a finite number of episodes, which is polynomial in all relevant parameters, logarithmic in the number of policies, and independent of the size of the observation space. Our findings offer a theoretical foundation for reinforcement learning with function approximation.",1
The paper focuses on the problem of learning saccades enabling visual object search. The developed system combines reinforcement learning with a neural network for learning to predict the possible outcomes of its actions. We validated the solution in three types of environment consisting of (pseudo)-randomly generated matrices of digits. The experimental verification is followed by the discussion regarding elements required by systems mimicking the fovea movement and possible further research directions.,0
"This paper's main objective is to address the issue of acquiring saccades that facilitate visual object search. The approach taken involves a blend of reinforcement learning and neural networks, which enables the system to anticipate the consequences of its actions. To test the effectiveness of this solution, we implemented it in three distinct environments containing matrices of randomly generated digits. After verifying the experimental results, we analyzed the components necessary for mimicking foveal movement in systems and identified future research possibilities.",1
"Conceived in the early 1990s, Experience Replay (ER) has been shown to be a successful mechanism to allow online learning algorithms to reuse past experiences. Traditionally, ER can be applied to all machine learning paradigms (i.e., unsupervised, supervised, and reinforcement learning). Recently, ER has contributed to improving the performance of deep reinforcement learning. Yet, its application to many practical settings is still limited by the memory requirements of ER, necessary to explicitly store previous observations. To remedy this issue, we explore a novel approach, Online Contrastive Divergence with Generative Replay (OCD_GR), which uses the generative capability of Restricted Boltzmann Machines (RBMs) instead of recorded past experiences. The RBM is trained online, and does not require the system to store any of the observed data points. We compare OCD_GR to ER on 9 real-world datasets, considering a worst-case scenario (data points arriving in sorted order) as well as a more realistic one (sequential random-order data points). Our results show that in 64.28% of the cases OCD_GR outperforms ER and in the remaining 35.72% it has an almost equal performance, while having a considerably reduced space complexity (i.e., memory usage) at a comparable time complexity.",0
"Experience Replay (ER) was developed in the early 1990s as a successful mechanism for allowing online learning algorithms to reuse past experiences. ER can be applied to all machine learning paradigms, but its application is limited by the memory requirements needed to store previous observations. To address this issue, we present a novel approach called Online Contrastive Divergence with Generative Replay (OCD_GR). Instead of storing past experiences, OCD_GR uses the generative capability of Restricted Boltzmann Machines (RBMs). The RBM is trained online and does not require the system to store observed data points. We compare OCD_GR with ER on 9 real-world datasets, considering a worst-case scenario and a more realistic sequential random-order data points scenario. Our results show that OCD_GR outperforms ER in 64.28% of cases and has almost equal performance in the remaining 35.72%. Additionally, OCD_GR has a considerably reduced space complexity while maintaining comparable time complexity.",1
"Stochastic linear bandits are a natural and simple generalisation of finite-armed bandits with numerous practical applications. Current approaches focus on generalising existing techniques for finite-armed bandits, notably the optimism principle and Thompson sampling. While prior work has mostly been in the worst-case setting, we analyse the asymptotic instance-dependent regret and show matching upper and lower bounds on what is achievable. Surprisingly, our results show that no algorithm based on optimism or Thompson sampling will ever achieve the optimal rate, and indeed, can be arbitrarily far from optimal, even in very simple cases. This is a disturbing result because these techniques are standard tools that are widely used for sequential optimisation. For example, for generalised linear bandits and reinforcement learning.",0
"Stochastic linear bandits are a straightforward extension of the finite-armed bandits concept and have many practical applications. Existing approaches to stochastic linear bandits focus on adapting techniques from finite-armed bandits, such as the optimism principle and Thompson sampling. Although prior research has mainly examined the worst-case scenario, we investigate the asymptotic instance-dependent regret and provide matching upper and lower bounds on the achievable performance. Surprisingly, our findings demonstrate that algorithms utilizing optimism or Thompson sampling will never attain the optimal rate and may even be significantly suboptimal, even in basic scenarios. This is concerning because these methods are widely implemented in sequential optimization, such as generalized linear bandits and reinforcement learning.",1
"Direct load control of a heterogeneous cluster of residential demand flexibility sources is a high-dimensional control problem with partial observability. This work proposes a novel approach that uses a convolutional neural network to extract hidden state-time features to mitigate the curse of partial observability. More specific, a convolutional neural network is used as a function approximator to estimate the state-action value function or Q-function in the supervised learning step of fitted Q-iteration. The approach is evaluated in a qualitative simulation, comprising a cluster of thermostatically controlled loads that only share their air temperature, whilst their envelope temperature remains hidden. The simulation results show that the presented approach is able to capture the underlying hidden features and successfully reduce the electricity cost the cluster.",0
"The problem of directly controlling the load of a mixed group of residential demand flexibility sources is complex due to its high dimensionality and partial observability. To address this issue, a new method is proposed that utilizes a convolutional neural network to identify hidden state-time features and alleviate the challenges of partial observability. Specifically, the network serves as a function approximator to estimate the state-action value function or Q-function during the supervised learning phase of fitted Q-iteration. To evaluate this approach, a qualitative simulation is conducted using a cluster of thermostatically controlled loads that only share their air temperature, while their envelope temperature remains concealed. The simulation results demonstrate that the proposed approach can effectively capture the underlying hidden features and significantly reduce the electricity cost of the cluster.",1
"Using current reinforcement learning methods, it has recently become possible to learn to play unknown 3D games from raw pixels. In this work, we study the challenges that arise in such complex environments, and summarize current methods to approach these. We choose a task within the Doom game, that has not been approached yet. The goal for the agent is to fight enemies in a 3D world consisting of five rooms. We train the DQN and LSTM-A3C algorithms on this task. Results show that both algorithms learn sensible policies, but fail to achieve high scores given the amount of training. We provide insights into the learned behavior, which can serve as a valuable starting point for further research in the Doom domain.",0
"Recently, reinforcement learning techniques have allowed for the learning of unknown 3D games from raw pixels. This study examines the challenges that arise in these complex environments and outlines current methods for addressing them. Our focus is on an unexplored task within the Doom game where the agent must battle enemies in a 3D world with five rooms. To accomplish this task, we train the DQN and LSTM-A3C algorithms. While both algorithms successfully learn sensible policies, they fall short in achieving high scores with the amount of training provided. We provide insights into the learned behavior, which can serve as a valuable starting point for future research in the Doom domain.",1
"Online model-free reinforcement learning (RL) methods with continuous actions are playing a prominent role when dealing with real-world applications such as Robotics. However, when confronted to non-stationary environments, these methods crucially rely on an exploration-exploitation trade-off which is rarely dynamically and automatically adjusted to changes in the environment. Here we propose an active exploration algorithm for RL in structured (parameterized) continuous action space. This framework deals with a set of discrete actions, each of which is parameterized with continuous variables. Discrete exploration is controlled through a Boltzmann softmax function with an inverse temperature $\beta$ parameter. In parallel, a Gaussian exploration is applied to the continuous action parameters. We apply a meta-learning algorithm based on the comparison between variations of short-term and long-term reward running averages to simultaneously tune $\beta$ and the width of the Gaussian distribution from which continuous action parameters are drawn. When applied to a simple virtual human-robot interaction task, we show that this algorithm outperforms continuous parameterized RL both without active exploration and with active exploration based on uncertainty variations measured by a Kalman-Q-learning algorithm.",0
"In the realm of Robotics, online model-free reinforcement learning (RL) methods with continuous actions have become a vital tool for real-world applications. However, these methods face a significant challenge when dealing with non-stationary environments due to their reliance on an exploration-exploitation trade-off that is not dynamically and automatically adjusted to changes in the environment. To address this issue, we present an active exploration algorithm for RL in structured continuous action space. This approach involves controlling discrete exploration through a Boltzmann softmax function with an inverse temperature $\beta$ parameter, while Gaussian exploration is applied to the continuous action parameters. We use a meta-learning algorithm based on a comparison of short-term and long-term reward running averages to simultaneously adjust $\beta$ and the width of the Gaussian distribution. When tested on a simple virtual human-robot interaction task, our algorithm outperformed continuous parameterized RL, both with and without active exploration based on uncertainty variations measured by a Kalman-Q-learning algorithm.",1
"Autonomous learning of robotic skills can allow general-purpose robots to learn wide behavioral repertoires without requiring extensive manual engineering. However, robotic skill learning methods typically make one of several trade-offs to enable practical real-world learning, such as requiring manually designed policy or value function representations, initialization from human-provided demonstrations, instrumentation of the training environment, or extremely long training times. In this paper, we propose a new reinforcement learning algorithm for learning manipulation skills that can train general-purpose neural network policies with minimal human engineering, while still allowing for fast, efficient learning in stochastic environments. Our approach builds on the guided policy search (GPS) algorithm, which transforms the reinforcement learning problem into supervised learning from a computational teacher (without human demonstrations). In contrast to prior GPS methods, which require a consistent set of initial states to which the system must be reset after each episode, our approach can handle randomized initial states, allowing it to be used in environments where deterministic resets are impossible. We compare our method to existing policy search techniques in simulation, showing that it can train high-dimensional neural network policies with the same sample efficiency as prior GPS methods, and present real-world results on a PR2 robotic manipulator.",0
"The acquisition of robotic skills through autonomous learning can eliminate the need for extensive manual engineering, enabling general-purpose robots to acquire a wide range of behaviors. However, existing robotic skill learning methods require trade-offs, such as the need for human-provided demonstrations, instrumentation of the training environment, or long training times, to achieve practical real-world learning. This paper proposes a novel reinforcement learning algorithm for manipulation skills that can train neural network policies with minimal human engineering while enabling fast and efficient learning in stochastic environments. The algorithm builds on the guided policy search approach, which transforms the reinforcement learning problem into supervised learning from a computational teacher without human demonstrations. Unlike prior GPS methods, which require consistent initial states for system reset after each episode, the proposed method can handle randomized initial states, making it suitable for use in environments where deterministic resets are not possible. The paper demonstrates the effectiveness of the approach through simulation and real-world experiments on a PR2 robotic manipulator, achieving the same sample efficiency as prior GPS methods for training high-dimensional neural network policies.",1
"Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.",0
"Applied machine learning has shown great interest in deep learning tools, but regression and classification tools lack the ability to capture model uncertainty. Bayesian models provide a grounded framework for reasoning about model uncertainty, but their computational cost is often prohibitive. This paper introduces a new theoretical framework that views dropout training in deep neural networks as approximations to Bayesian inference in deep Gaussian processes. This framework offers tools to model uncertainty with dropout neural networks, which extracts previously disregarded information from existing models. This approach addresses the challenge of representing uncertainty in deep learning without sacrificing computational complexity or test accuracy. The study evaluates various network architectures and non-linearities on regression and classification tasks, using MNIST as an example. Results demonstrate a significant improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods. Finally, the study applies dropout's uncertainty to deep reinforcement learning.",1
"In principle, reinforcement learning and policy search methods can enable robots to learn highly complex and general skills that may allow them to function amid the complexity and diversity of the real world. However, training a policy that generalizes well across a wide range of real-world conditions requires far greater quantity and diversity of experience than is practical to collect with a single robot. Fortunately, it is possible for multiple robots to share their experience with one another, and thereby, learn a policy collectively. In this work, we explore distributed and asynchronous policy learning as a means to achieve generalization and improved training times on challenging, real-world manipulation tasks. We propose a distributed and asynchronous version of Guided Policy Search and use it to demonstrate collective policy learning on a vision-based door opening task using four robots. We show that it achieves better generalization, utilization, and training times than the single robot alternative.",0
"Reinforcement learning and policy search methods have the potential to enable robots to learn complex and general skills that are necessary to function in the real world. However, developing a policy that can generalize well across diverse real-world conditions requires a significant amount of experience, which is not practical to obtain using a single robot. Luckily, multiple robots can share their experiences, allowing them to learn a policy collectively. This study explores distributed and asynchronous policy learning to achieve generalization and improve training times for challenging, real-world manipulation tasks. The researchers propose a distributed and asynchronous version of Guided Policy Search and demonstrate collective policy learning on a vision-based door opening task using four robots. The results show that this approach achieves better generalization, utilization, and training times compared to the single robot alternative.",1
"Reinforcement learning (RL) can automate a wide variety of robotic skills, but learning each new skill requires considerable real-world data collection and manual representation engineering to design policy classes or features. Using deep reinforcement learning to train general purpose neural network policies alleviates some of the burden of manual representation engineering by using expressive policy classes, but exacerbates the challenge of data collection, since such methods tend to be less efficient than RL with low-dimensional, hand-designed representations. Transfer learning can mitigate this problem by enabling us to transfer information from one skill to another and even from one robot to another. We show that neural network policies can be decomposed into ""task-specific"" and ""robot-specific"" modules, where the task-specific modules are shared across robots, and the robot-specific modules are shared across all tasks on that robot. This allows for sharing task information, such as perception, between robots and sharing robot information, such as dynamics and kinematics, between tasks. We exploit this decomposition to train mix-and-match modules that can solve new robot-task combinations that were not seen during training. Using a novel neural network architecture, we demonstrate the effectiveness of our transfer method for enabling zero-shot generalization with a variety of robots and tasks in simulation for both visual and non-visual tasks.",0
"Automating various robotic skills through reinforcement learning (RL) requires significant real-world data collection and manual representation engineering to design policy classes or features for learning each new skill. While using deep reinforcement learning to train general purpose neural network policies reduces the burden of manual representation engineering by using expressive policy classes, it intensifies the challenge of data collection, as these methods are less efficient than RL with low-dimensional, hand-designed representations. Nevertheless, transfer learning can alleviate this issue by transferring information from one skill or robot to another. Our study demonstrates that neural network policies can be broken down into ""task-specific"" and ""robot-specific"" modules, with the task-specific modules shared across robots and robot-specific modules shared across all tasks on that robot. This approach enables sharing of task information, such as perception, between robots and sharing of robot information, such as dynamics and kinematics, between tasks. We use this decomposition to train mix-and-match modules that can solve new robot-task combinations not seen during training. Through a novel neural network architecture, we show the effectiveness of our transfer method for enabling zero-shot generalization, which we test for various robots and tasks, both visual and non-visual, in simulation.",1
"In this paper we study a model-based approach to calculating approximately optimal policies in Markovian Decision Processes. In particular, we derive novel bounds on the loss of using a policy derived from a factored linear model, a class of models which generalize numerous previous models out of those that come with strong computational guarantees. For the first time in the literature, we derive performance bounds for model-based techniques where the model inaccuracy is measured in weighted norms. Moreover, our bounds show a decreased sensitivity to the discount factor and, unlike similar bounds derived for other approaches, they are insensitive to measure mismatch. Similarly to previous works, our proofs are also based on contraction arguments, but with the main differences that we use carefully constructed norms building on Banach lattices, and the contraction property is only assumed for operators acting on ""compressed"" spaces, thus weakening previous assumptions, while strengthening previous results.",0
"The aim of this study is to explore a model-based method for computing policies that are approximately optimal in Markovian Decision Processes. Our focus is on factored linear models, which are a class of models that extend several earlier models that offer strong computational guarantees. In this paper, we present new boundaries for evaluating the loss incurred when utilizing a policy derived from a factored linear model. These boundaries are novel because they use weighted norms to measure model inaccuracy, a feature that has not been seen in previous research. Furthermore, our boundaries are less sensitive to the discount factor and are insensitive to measure mismatch, unlike those of other approaches. Our proof builds on contraction arguments, as in previous research, but with a difference: we use carefully constructed norms based on Banach lattices. The contraction property is only assumed for operators acting on ""compressed"" spaces, which departs from previous assumptions and strengthens the findings.",1
"The recent advances in deep neural networks have led to effective vision-based reinforcement learning methods that have been employed to obtain human-level controllers in Atari 2600 games from pixel data. Atari 2600 games, however, do not resemble real-world tasks since they involve non-realistic 2D environments and the third-person perspective. Here, we propose a novel test-bed platform for reinforcement learning research from raw visual information which employs the first-person perspective in a semi-realistic 3D world. The software, called ViZDoom, is based on the classical first-person shooter video game, Doom. It allows developing bots that play the game using the screen buffer. ViZDoom is lightweight, fast, and highly customizable via a convenient mechanism of user scenarios. In the experimental part, we test the environment by trying to learn bots for two scenarios: a basic move-and-shoot task and a more complex maze-navigation problem. Using convolutional deep neural networks with Q-learning and experience replay, for both scenarios, we were able to train competent bots, which exhibit human-like behaviors. The results confirm the utility of ViZDoom as an AI research platform and imply that visual reinforcement learning in 3D realistic first-person perspective environments is feasible.",0
"Recent progress in deep neural networks has enabled the development of successful reinforcement learning techniques based on visual input, which have been used to achieve human-level performance in Atari 2600 games. However, these games are not representative of real-world tasks as they are 2D and viewed from a third-person perspective. To address this, we introduce a new test environment for reinforcement learning research that uses raw visual input from a first-person perspective in a semi-realistic 3D world. Our software, ViZDoom, is based on the classic first-person shooter video game Doom and allows for the creation of bots that play the game using the screen buffer. ViZDoom is efficient, rapid, and fully customizable using user scenarios. In our experiments, we demonstrate the effectiveness of ViZDoom by successfully training bots for two scenarios: a basic move-and-shoot task and a more complex maze-navigation problem. Using convolutional deep neural networks with Q-learning and experience replay, we were able to train competent bots that exhibit human-like behaviors. Our results show that ViZDoom is a useful platform for AI research and that visual reinforcement learning in 3D first-person perspective environments is achievable.",1
"Opponent modeling is necessary in multi-agent settings where secondary agents with competing goals also adapt their strategies, yet it remains challenging because strategies interact with each other and change. Most previous work focuses on developing probabilistic models or parameterized strategies for specific applications. Inspired by the recent success of deep reinforcement learning, we present neural-based models that jointly learn a policy and the behavior of opponents. Instead of explicitly predicting the opponent's action, we encode observation of the opponents into a deep Q-Network (DQN); however, we retain explicit modeling (if desired) using multitasking. By using a Mixture-of-Experts architecture, our model automatically discovers different strategy patterns of opponents without extra supervision. We evaluate our models on a simulated soccer game and a popular trivia game, showing superior performance over DQN and its variants.",0
"In multi-agent settings, opponent modeling is crucial as it involves adapting strategies in the presence of competing secondary agents, which can be challenging due to the interaction and changes in strategies. Previous studies have focused on developing probabilistic models or parameterized strategies for specific applications. However, inspired by the success of deep reinforcement learning, we propose neural-based models that can learn a policy and the behavior of opponents jointly. Our approach involves encoding observations of opponents into a deep Q-Network (DQN) instead of explicitly predicting their actions. We also retain the option of explicit modeling using multitasking. By utilizing a Mixture-of-Experts architecture, our model automatically discovers different strategy patterns of opponents without additional supervision. We evaluated our models on a simulated soccer game and a popular trivia game, demonstrating superior performance over DQN and its variants.",1
"Two less addressed issues of deep reinforcement learning are (1) lack of generalization capability to new target goals, and (2) data inefficiency i.e., the model requires several (and often costly) episodes of trial and error to converge, which makes it impractical to be applied to real-world scenarios. In this paper, we address these two issues and apply our model to the task of target-driven visual navigation. To address the first issue, we propose an actor-critic model whose policy is a function of the goal as well as the current state, which allows to better generalize. To address the second issue, we propose AI2-THOR framework, which provides an environment with high-quality 3D scenes and physics engine. Our framework enables agents to take actions and interact with objects. Hence, we can collect a huge number of training samples efficiently.   We show that our proposed method (1) converges faster than the state-of-the-art deep reinforcement learning methods, (2) generalizes across targets and across scenes, (3) generalizes to a real robot scenario with a small amount of fine-tuning (although the model is trained in simulation), (4) is end-to-end trainable and does not need feature engineering, feature matching between frames or 3D reconstruction of the environment.   The supplementary video can be accessed at the following link: https://youtu.be/SmBxMDiOrvs.",0
"Deep reinforcement learning has two issues that are not adequately addressed: the inability to generalize to new target goals and data inefficiency, which requires many costly trial and error episodes to converge, making it unsuitable for real-world applications. This paper proposes a solution to these issues and applies it to target-driven visual navigation. To address the first issue, we propose an actor-critic model that considers the goal and current state, allowing for better generalization. To address the second issue, we introduce the AI2-THOR framework, which provides a high-quality 3D environment for agents to interact with. Our approach outperforms state-of-the-art deep reinforcement learning methods, generalizes to new targets and scenes, can be fine-tuned for real robot scenarios, is end-to-end trainable, and does not require feature engineering or 3D reconstruction. A supplementary video is available at the link: https://youtu.be/SmBxMDiOrvs.",1
"Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.",0
"A major challenge in attaining human-like intelligence is mastering complex sequences of tasks without experiencing catastrophic forgetting or failing to utilize prior knowledge. The progressive networks strategy is an advancement in this direction, as it is impervious to forgetting and can make use of previous knowledge through lateral connections to previously learned features. We extensively evaluate this approach on various reinforcement learning tasks, including Atari and 3D maze games, and prove its superiority over conventional baselines that rely on pretraining and finetuning. Additionally, we utilize a new sensitivity metric to demonstrate that transfer learning is effective both at the low-level sensory and high-level control layers of the learned policy.",1
"Understanding and using natural processes for intelligent functionalities, referred to as natural intelligence, has recently attracted interest from a variety of fields, including post-silicon computing for artificial intelligence and decision making in the behavioural sciences. In a past study, we successfully used the wave-particle duality of single photons to solve the two-armed bandit problem, which constitutes the foundation of reinforcement learning and decision making. In this study, we propose and confirm a hierarchical architecture for single-photon-based reinforcement learning and decision making that verifies the scalability of the principle. Specifically, the four-armed bandit problem is solved given zero prior knowledge in a two-layer hierarchical architecture, where polarization is autonomously adapted in order to effect adequate decision making using single-photon measurements. In the hierarchical structure, the notion of layer-dependent decisions emerges. The optimal solutions in the coarse layer and in the fine layer, however, conflict with each other in some contradictive problems. We show that while what we call a tournament strategy resolves such contradictions, the probabilistic nature of single photons allows for the direct location of the optimal solution even for contradictive problems, hence manifesting the exploration ability of single photons. This study provides insights into photon intelligence in hierarchical architectures for future artificial intelligence as well as the potential of natural processes for intelligent functionalities.",0
"The concept of natural intelligence, which involves utilizing natural processes for intelligent functions, has recently gained attention across various fields such as post-silicon computing for artificial intelligence and decision-making in the behavioural sciences. In a previous study, we utilized the wave-particle duality of single photons to address the foundational two-armed bandit problem in reinforcement learning and decision-making. In this new study, we propose and validate a hierarchical architecture for single-photon-based decision-making and reinforcement learning, which demonstrates the scalability of this principle. Our approach solves the four-armed bandit problem without any prior knowledge utilizing a two-layer hierarchical architecture, where the polarization is automatically adjusted to facilitate optimal decision-making using single-photon measurements. The hierarchical structure results in layer-dependent decisions, which may lead to conflicts between the optimal solutions in the coarse and fine layers. We address this issue by implementing a tournament strategy, which resolves such conflicts. The probabilistic nature of single photons enables us to identify the optimal solution directly for even the contradictory problems, thereby demonstrating their exploration ability. Our study provides insights into photon intelligence in hierarchical architectures for future artificial intelligence, and highlights the potential of natural processes for intelligent functionalities.",1
"Sample efficiency is a critical property when optimizing policy parameters for the controller of a robot. In this paper, we evaluate two state-of-the-art policy optimization algorithms. One is a recent deep reinforcement learning method based on an actor-critic algorithm, Deep Deterministic Policy Gradient (DDPG), that has been shown to perform well on various control benchmarks. The other one is a direct policy search method, Covariance Matrix Adaptation Evolution Strategy (CMA-ES), a black-box optimization method that is widely used for robot learning. The algorithms are evaluated on a continuous version of the mountain car benchmark problem, so as to compare their sample complexity. From a preliminary analysis, we expect DDPG to be more sample efficient than CMA-ES, which is confirmed by our experimental results.",0
"When optimizing policy parameters for a robot controller, it is crucial to consider sample efficiency. This study assesses two advanced policy optimization techniques: Deep Deterministic Policy Gradient (DDPG) and Covariance Matrix Adaptation Evolution Strategy (CMA-ES). DDPG is a recent deep reinforcement learning algorithm that utilizes an actor-critic approach and has demonstrated its efficacy in various control benchmarks. On the other hand, CMA-ES is a black-box optimization method that directly searches for policies and is commonly used in robot learning. We conduct an evaluation of these algorithms on a continuous version of the mountain car benchmark problem to compare their sample complexity. Our preliminary analysis indicates that DDPG is more sample efficient than CMA-ES, which is supported by our experimental findings.",1
"A recent goal in the Reinforcement Learning (RL) framework is to choose a sequence of actions or a policy to maximize the reward collected or minimize the regret incurred in a finite time horizon. For several RL problems in operation research and optimal control, the optimal policy of the underlying Markov Decision Process (MDP) is characterized by a known structure. The current state of the art algorithms do not utilize this known structure of the optimal policy while minimizing regret. In this work, we develop new RL algorithms that exploit the structure of the optimal policy to minimize regret. Numerical experiments on MDPs with structured optimal policies show that our algorithms have better performance, are easy to implement, have a smaller run-time and require less number of random number generations.",0
"The Reinforcement Learning (RL) framework aims to select a policy or sequence of actions that maximizes the reward or minimizes the regret over a set time period. In many RL problems related to operational research and optimal control, the optimal policy for the underlying Markov Decision Process (MDP) is already known. Unfortunately, current state-of-the-art algorithms do not incorporate this valuable information when minimizing regret. This study presents novel RL algorithms that take advantage of the optimal policy's structure to minimize regret. Results from numerical experiments on MDPs with structured optimal policies demonstrate superior performance, easy implementation, shorter run-time, and fewer random number generations compared to existing algorithms.",1
"Most learning algorithms are not invariant to the scale of the function that is being approximated. We propose to adaptively normalize the targets used in learning. This is useful in value-based reinforcement learning, where the magnitude of appropriate value approximations can change over time when we update the policy of behavior. Our main motivation is prior work on learning to play Atari games, where the rewards were all clipped to a predetermined range. This clipping facilitates learning across many different games with a single learning algorithm, but a clipped reward function can result in qualitatively different behavior. Using the adaptive normalization we can remove this domain-specific heuristic without diminishing overall performance.",0
"Learning algorithms often lack invariance to the function scale being approximated. Our solution is to implement adaptive normalization of the learning targets. This approach is particularly beneficial in value-based reinforcement learning, as value approximations must be adjusted to reflect policy updates. Our inspiration stems from previous research on learning Atari games, where rewards were clipped to a predetermined range. Although this enabled learning across multiple games, the reward function's clipping resulted in varied behavior. By using adaptive normalization, we can eliminate domain-specific heuristics without adversely affecting overall performance.",1
"Control of non-episodic, finite-horizon dynamical systems with uncertain dynamics poses a tough and elementary case of the exploration-exploitation trade-off. Bayesian reinforcement learning, reasoning about the effect of actions and future observations, offers a principled solution, but is intractable. We review, then extend an old approximate approach from control theory---where the problem is known as dual control---in the context of modern regression methods, specifically generalized linear regression. Experiments on simulated systems show that this framework offers a useful approximation to the intractable aspects of Bayesian RL, producing structured exploration strategies that differ from standard RL approaches. We provide simple examples for the use of this framework in (approximate) Gaussian process regression and feedforward neural networks for the control of exploration.",0
"Handling uncertain dynamics in non-episodic, finite-horizon dynamical systems is a challenging task that requires balancing exploration and exploitation. Although Bayesian reinforcement learning provides a sound solution by considering the impact of actions and future observations, it is difficult to implement. In this article, we examine an old control theory approach, referred to as dual control, and modify it using modern regression methods, specifically generalized linear regression. Our simulation-based experiments demonstrate that this framework provides a useful alternative to the complex Bayesian RL approach, resulting in exploration strategies that differ from traditional RL methods. We offer straightforward examples of using this framework with (approximate) Gaussian process regression and feedforward neural networks for exploration control.",1
"One of the key challenges in applying reinforcement learning to complex robotic control tasks is the need to gather large amounts of experience in order to find an effective policy for the task at hand. Model-based reinforcement learning can achieve good sample efficiency, but requires the ability to learn a model of the dynamics that is good enough to learn an effective policy. In this work, we develop a model-based reinforcement learning algorithm that combines prior knowledge from previous tasks with online adaptation of the dynamics model. These two ingredients enable highly sample-efficient learning even in regimes where estimating the true dynamics is very difficult, since the online model adaptation allows the method to locally compensate for unmodeled variation in the dynamics. We encode the prior experience into a neural network dynamics model, adapt it online by progressively refitting a local linear model of the dynamics, and use model predictive control to plan under these dynamics. Our experimental results show that this approach can be used to solve a variety of complex robotic manipulation tasks in just a single attempt, using prior data from other manipulation behaviors.",0
"The main issue with using reinforcement learning for intricate robotic control tasks is the necessity to obtain a significant amount of experience to develop an effective policy. While model-based reinforcement learning can enhance sample efficiency, it requires the ability to learn a dynamic model that is competent enough to develop an effective policy. In this study, we present a model-based reinforcement learning method that merges previous task knowledge with online adaptation of the dynamic model. These two components allow for highly efficient learning, even when estimating the true dynamics is challenging. Our approach integrates prior experience into a neural network dynamic model, adapts it online by progressively refitting a local linear model of the dynamics, and uses model predictive control to plan under these dynamics. Our experimental outcomes demonstrate that this method can solve a range of complex robotic manipulation tasks in a single attempt, using prior data from other manipulation behaviors.",1
"This is a brief technical note to clarify the state of lower bounds on regret for reinforcement learning. In particular, this paper:   - Reproduces a lower bound on regret for reinforcement learning, similar to the result of Theorem 5 in the journal UCRL2 paper (Jaksch et al 2010).   - Clarifies that the proposed proof of Theorem 6 in the REGAL paper (Bartlett and Tewari 2009) does not hold using the standard techniques without further work. We suggest that this result should instead be considered a conjecture as it has no rigorous proof.   - Suggests that the conjectured lower bound given by (Bartlett and Tewari 2009) is incorrect and, in fact, it is possible to improve the scaling of the upper bound to match the weaker lower bounds presented in this paper.   We hope that this note serves to clarify existing results in the field of reinforcement learning and provides interesting motivation for future work.",0
"This technical note aims to provide clarification on the current state of lower bounds for regret in reinforcement learning. Specifically, it replicates a lower bound on regret that resembles the outcome of Theorem 5 in the UCRL2 paper (Jaksch et al 2010). Additionally, it highlights that the proof for Theorem 6 in the REGAL paper (Bartlett and Tewari 2009) is not valid using standard techniques and must be considered a conjecture without rigorous evidence. Furthermore, the note suggests that the conjectured lower bound proposed by (Bartlett and Tewari 2009) is incorrect and proposes a potential improvement to the scaling of the upper bound to match the weaker lower bounds presented in this paper. The overall intention of this note is to provide a clear understanding of the current state of reinforcement learning research and to inspire further exploration in this field.",1
"This is a brief technical note to clarify some of the issues with applying the application of the algorithm posterior sampling for reinforcement learning (PSRL) in environments without fixed episodes. In particular, this paper aims to:   - Review some of results which have been proven for finite horizon MDPs (Osband et al 2013, 2014a, 2014b, 2016) and also for MDPs with finite ergodic structure (Gopalan et al 2014).   - Review similar results for optimistic algorithms in infinite horizon problems (Jaksch et al 2010, Bartlett and Tewari 2009, Abbasi-Yadkori and Szepesvari 2011), with particular attention to the dynamic episode growth.   - Highlight the delicate technical issue which has led to a fault in the proof of the lazy-PSRL algorithm (Abbasi-Yadkori and Szepesvari 2015). We present an explicit counterexample to this style of argument. Therefore, we suggest that the Theorem 2 in (Abbasi-Yadkori and Szepesvari 2015) be instead considered a conjecture, as it has no rigorous proof.   - Present pragmatic approaches to apply PSRL in infinite horizon problems. We conjecture that, under some additional assumptions, it will be possible to obtain bounds $O( \sqrt{T} )$ even without episodic reset.   We hope that this note serves to clarify existing results in the field of reinforcement learning and provides interesting motivation for future work.",0
"This technical note aims to provide clarification on the implementation of the posterior sampling for reinforcement learning (PSRL) algorithm in environments that lack fixed episodes. Our objectives include reviewing proven results for finite horizon Markov decision processes (MDPs) and MDPs with finite ergodic structure, as well as optimistic algorithms in infinite horizon problems with a focus on dynamic episode growth. Additionally, we draw attention to a technical issue in the proof of the lazy-PSRL algorithm and present a counterexample to its argument. Thus, we suggest that Theorem 2 in Abbasi-Yadkori and Szepesvari (2015) should be treated as a conjecture. Finally, we present practical approaches to apply PSRL in infinite horizon problems and conjecture the possibility of obtaining bounds without episodic reset under certain assumptions. We hope this note contributes to the reinforcement learning field and inspires future research.",1
"Neuroscientists have long criticised deep learning algorithms as incompatible with current knowledge of neurobiology. We explore more biologically plausible versions of deep representation learning, focusing here mostly on unsupervised learning but developing a learning mechanism that could account for supervised, unsupervised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-Timing-Dependent Plasticity) arises out of a simple update rule that makes a lot of sense from a machine learning point of view and can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective function (be it supervised, unsupervised, or reward-driven). The second main idea is that this corresponds to a form of the variational EM algorithm, i.e., with approximate rather than exact posteriors, implemented by neural dynamics. Another contribution of this paper is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.",0
"Neurological experts have criticized deep learning algorithms for not aligning with current neurobiological knowledge. In this study, we examine more biologically plausible versions of deep representation learning, with a focus on unsupervised learning while also developing a learning mechanism that could accommodate supervised, unsupervised, and reinforcement learning. Our starting point is that the basic learning rule that governs synaptic weight updates, Spike-Timing-Dependent Plasticity, is a simple update rule that makes sense from a machine learning perspective. The neuronal dynamics must push firing rates towards better values of the objective function, be it supervised, unsupervised, or reward-driven, to interpret it as gradient descent on some objective function. Our second main idea is that this corresponds to a form of the variational EM algorithm, implemented by neural dynamics and with approximate rather than exact posteriors. We also propose an approximation method for estimating the gradients required for updating the hidden states in the variational interpretation. This approximation only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Lastly, we extend the theory of probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders. We validate all these ideas on generative learning tasks.",1
"Online learning has become crucial to many problems in machine learning. As more data is collected sequentially, quickly adapting to changes in the data distribution can offer several competitive advantages such as avoiding loss of prior knowledge and more efficient learning. However, adaptation to changes in the data distribution (also known as covariate shift) needs to be performed without compromising past knowledge already built in into the model to cope with voluminous and dynamic data. In this paper, we propose an online stacked Denoising Autoencoder whose structure is adapted through reinforcement learning. Our algorithm forces the network to exploit and explore favourable architectures employing an estimated utility function that maximises the accuracy of an unseen validation sequence. Different actions, such as Pool, Increment and Merge are available to modify the structure of the network. As we observe through a series of experiments, our approach is more responsive, robust, and principled than its counterparts for non-stationary as well as stationary data distributions. Experimental results indicate that our algorithm performs better at preserving gained prior knowledge and responding to changes in the data distribution.",0
"The significance of online learning in machine learning cannot be overstated. The ability to quickly adapt to changes in data distribution has numerous benefits, including the preservation of previous knowledge and more efficient learning. However, adapting to these changes without compromising past knowledge is challenging, especially when dealing with large and dynamic data sets. To address this challenge, we have developed a novel approach using an online stacked Denoising Autoencoder that employs reinforcement learning to adapt its structure. Our algorithm maximizes accuracy by exploiting and exploring favorable architectures through an estimated utility function. Different actions, such as Pool, Increment, and Merge, are available to modify the network structure. Our experiments demonstrate that our approach is more responsive, robust, and principled than other methods for both stationary and non-stationary data distributions. Furthermore, our algorithm performs better at preserving prior knowledge and responding to changes in the data distribution.",1
"Szubert and Jaskowski successfully used temporal difference (TD) learning together with n-tuple networks for playing the game 2048. However, we observed a phenomenon that the programs based on TD learning still hardly reach large tiles. In this paper, we propose multi-stage TD (MS-TD) learning, a kind of hierarchical reinforcement learning method, to effectively improve the performance for the rates of reaching large tiles, which are good metrics to analyze the strength of 2048 programs. Our experiments showed significant improvements over the one without using MS-TD learning. Namely, using 3-ply expectimax search, the program with MS-TD learning reached 32768-tiles with a rate of 18.31%, while the one with TD learning did not reach any. After further tuned, our 2048 program reached 32768-tiles with a rate of 31.75% in 10,000 games, and one among these games even reached a 65536-tile, which is the first ever reaching a 65536-tile to our knowledge. In addition, MS-TD learning method can be easily applied to other 2048-like games, such as Threes. Based on MS-TD learning, our experiments for Threes also demonstrated similar performance improvement, where the program with MS-TD learning reached 6144-tiles with a rate of 7.83%, while the one with TD learning only reached 0.45%.",0
"Temporal difference (TD) learning with n-tuple networks has been successfully used by Szubert and Jaskowski to play 2048. However, these programs struggle to reach large tiles. In this study, we introduce multi-stage TD (MS-TD) learning, a hierarchical reinforcement learning method that effectively enhances performance in reaching large tiles, which is a key metric for analyzing the strength of 2048 programs. Our experiments showed that the program with MS-TD learning, when combined with 3-ply expectimax search, reached 32768-tiles with an 18.31% rate, whereas the one with TD learning failed to reach any. After tuning, our 2048 program reached 32768-tiles with a rate of 31.75% in 10,000 games, with one game even reaching a 65536-tile, the first instance of this achievement that we are aware of. Furthermore, MS-TD learning can be easily applied to other 2048-like games, such as Threes. Our experiments with Threes demonstrated similar performance improvement, with the program with MS-TD learning reaching 6144-tiles with a rate of 7.83%, while the one with TD learning only reached 0.45%.",1
"Guided policy search algorithms can be used to optimize complex nonlinear policies, such as deep neural networks, without directly computing policy gradients in the high-dimensional parameter space. Instead, these methods use supervised learning to train the policy to mimic a ""teacher"" algorithm, such as a trajectory optimizer or a trajectory-centric reinforcement learning method. Guided policy search methods provide asymptotic local convergence guarantees by construction, but it is not clear how much the policy improves within a small, finite number of iterations. We show that guided policy search algorithms can be interpreted as an approximate variant of mirror descent, where the projection onto the constraint manifold is not exact. We derive a new guided policy search algorithm that is simpler and provides appealing improvement and convergence guarantees in simplified convex and linear settings, and show that in the more general nonlinear setting, the error in the projection step can be bounded. We provide empirical results on several simulated robotic navigation and manipulation tasks that show that our method is stable and achieves similar or better performance when compared to prior guided policy search methods, with a simpler formulation and fewer hyperparameters.",0
"Guided policy search algorithms offer a way to optimize complicated nonlinear policies, such as deep neural networks, without directly computing policy gradients in the high-dimensional parameter space. Instead, these methods utilize supervised learning to train the policy to replicate a ""teacher"" algorithm, such as a trajectory optimizer or a trajectory-centric reinforcement learning method. Although guided policy search methods provide asymptotic local convergence guarantees, it is uncertain how much the policy improves within a small, finite number of iterations. Our study shows that guided policy search algorithms can be seen as an approximate version of mirror descent, where the projection onto the constraint manifold is not precise. We introduce a new guided policy search algorithm that is simpler and provides attractive improvement and convergence guarantees in simplified convex and linear settings. We also demonstrate that in the more general nonlinear setting, the error in the projection step can be limited. Our empirical results on various simulated robotic navigation and manipulation tasks show that our method is reliable and achieves similar or better performance than previous guided policy search methods, with a simpler formulation and fewer hyperparameters.",1
"We propose a reinforcement learning based approach to tackle the cost-sensitive learning problem where each input feature has a specific cost. The acquisition process is handled through a stochastic policy which allows features to be acquired in an adaptive way. The general architecture of our approach relies on representation learning to enable performing prediction on any partially observed sample, whatever the set of its observed features are. The resulting model is an original mix of representation learning and of reinforcement learning ideas. It is learned with policy gradient techniques to minimize a budgeted inference cost. We demonstrate the effectiveness of our proposed method with several experiments on a variety of datasets for the sparse prediction problem where all features have the same cost, but also for some cost-sensitive settings.",0
"Our proposed solution for the cost-sensitive learning problem involves utilizing reinforcement learning. Specifically, we implement a stochastic policy to enable adaptive feature acquisition while taking into account the specific cost of each input feature. Our approach is based on representation learning, allowing for accurate predictions even when only a subset of features is observed. Our model is a unique combination of reinforcement and representation learning concepts, trained using policy gradient techniques to minimize inference costs. We showcase the effectiveness of our method through various experiments on both sparse prediction problems with uniform feature costs and cost-sensitive scenarios.",1
"We consider the problem of reinforcement learning over episodes of a finite-horizon deterministic system and as a solution propose optimistic constraint propagation (OCP), an algorithm designed to synthesize efficient exploration and value function generalization. We establish that when the true value function lies within a given hypothesis class, OCP selects optimal actions over all but at most K episodes, where K is the eluder dimension of the given hypothesis class. We establish further efficiency and asymptotic performance guarantees that apply even if the true value function does not lie in the given hypothesis class, for the special case where the hypothesis class is the span of pre-specified indicator functions over disjoint sets. We also discuss the computational complexity of OCP and present computational results involving two illustrative examples.",0
"The problem we examine involves reinforcement learning in a finite-horizon deterministic system, and we propose a solution called optimistic constraint propagation (OCP). This algorithm is designed to combine efficient exploration and value function generalization. Our research shows that OCP can select optimal actions in all but K episodes, where K is the eluder dimension of the given hypothesis class, assuming the true value function lies within that class. Additionally, we provide efficiency and asymptotic performance guarantees that apply even if the true value function is not in the hypothesis class. We focus on a specific case where the hypothesis class is made up of pre-specified indicator functions over disjoint sets. We also explore the computational complexity of OCP and provide results from two examples to illustrate our findings.",1
"Efficient exploration in complex environments remains a major challenge for reinforcement learning. We propose bootstrapped DQN, a simple algorithm that explores in a computationally and statistically efficient manner through use of randomized value functions. Unlike dithering strategies such as epsilon-greedy exploration, bootstrapped DQN carries out temporally-extended (or deep) exploration; this can lead to exponentially faster learning. We demonstrate these benefits in complex stochastic MDPs and in the large-scale Arcade Learning Environment. Bootstrapped DQN substantially improves learning times and performance across most Atari games.",0
"Reinforcement learning still faces a significant obstacle in effectively exploring intricate environments. Our solution, bootstrapped DQN, overcomes this challenge by utilizing randomized value functions to explore in a manner that is both computationally and statistically efficient. Unlike epsilon-greedy exploration, which employs dithering strategies, bootstrapped DQN conducts deep exploration over extended periods, resulting in exponential increases in learning speed. We have successfully demonstrated the advantages of this approach in complex stochastic MDPs and the vast Arcade Learning Environment, where bootstrapped DQN has notably improved learning times and performance in most Atari games.",1
"Many real-world applications can be described as large-scale games of imperfect information. To deal with these challenging domains, prior work has focused on computing Nash equilibria in a handcrafted abstraction of the domain. In this paper we introduce the first scalable end-to-end approach to learning approximate Nash equilibria without prior domain knowledge. Our method combines fictitious self-play with deep reinforcement learning. When applied to Leduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium, whereas common reinforcement learning methods diverged. In Limit Texas Holdem, a poker game of real-world scale, NFSP learnt a strategy that approached the performance of state-of-the-art, superhuman algorithms based on significant domain expertise.",0
"A lot of real-life scenarios can be characterized as imperfect information games that are massive in scale. To tackle these complex domains, previous research has concentrated on computing Nash equilibria in a pre-designed abstraction of the domain. In this article, we introduce a pioneering, scalable end-to-end approach that enables the learning of approximate Nash equilibria without any prior knowledge of the domain. Our technique merges deep reinforcement learning with fictitious self-play. In the case of Leduc poker, our approach, Neural Fictitious Self-Play (NFSP), reached a Nash equilibrium, whereas traditional reinforcement learning methods failed. In Limit Texas Holdem, an actual-sized poker game, NFSP acquired a strategy that was almost as good as the state-of-the-art, superhuman algorithms that were based on extensive domain expertise.",1
"Deep Reinforcement Learning (DRL) is a trending field of research, showing great promise in many challenging problems such as playing Atari, solving Go and controlling robots. While DRL agents perform well in practice we are still missing the tools to analayze their performance and visualize the temporal abstractions that they learn. In this paper, we present a novel method that automatically discovers an internal Semi Markov Decision Process (SMDP) model in the Deep Q Network's (DQN) learned representation. We suggest a novel visualization method that represents the SMDP model by a directed graph and visualize it above a t-SNE map. We show how can we interpret the agent's policy and give evidence for the hierarchical state aggregation that DQNs are learning automatically. Our algorithm is fully automatic, does not require any domain specific knowledge and is evaluated by a novel likelihood based evaluation criteria.",0
"The field of research known as Deep Reinforcement Learning (DRL) has gained significant attention due to its potential in solving complex problems such as playing Atari, solving Go, and controlling robots. Despite the success of DRL agents in practical applications, there is still a lack of tools to assess their performance and visualize the temporal abstractions that they acquire. This paper introduces a novel approach that automatically detects an internal model of Semi Markov Decision Process (SMDP) in the Deep Q Network's (DQN) learned representation. The proposed method uses a directed graph to represent the SMDP model and visualizes it above a t-SNE map. This visualization technique enables the interpretation of the agent's policy and highlights the hierarchical state aggregation that DQNs can learn on their own. The algorithm is fully automated, does not require any specialized knowledge, and is evaluated using a unique likelihood-based evaluation criterion.",1
"The notion of approachability was introduced by Blackwell [1] in the context of vector-valued repeated games. The famous Blackwell's approachability theorem prescribes a strategy for approachability, i.e., for `steering' the average cost of a given agent towards a given target set, irrespective of the strategies of the other agents. In this paper, motivated by the multi-objective optimization/decision making problems in dynamically changing environments, we address the approachability problem in Stackelberg stochastic games with vector valued cost functions. We make two main contributions. Firstly, we give a simple and computationally tractable strategy for approachability for Stackelberg stochastic games along the lines of Blackwell's. Secondly, we give a reinforcement learning algorithm for learning the approachable strategy when the transition kernel is unknown. We also recover as a by-product Blackwell's necessary and sufficient condition for approachability for convex sets in this set up and thus a complete characterization. We also give sufficient conditions for non-convex sets.",0
"Blackwell [1] introduced the concept of approachability in the context of vector-valued repeated games, and his approachability theorem is well-known for providing a strategy to guide an agent's average cost towards a target set, regardless of the other agents' strategies. In this paper, we focus on the approachability problem in Stackelberg stochastic games with vector-valued cost functions, motivated by multi-objective optimization/decision-making challenges in dynamic environments. We make two significant contributions. Firstly, we propose a straightforward and computationally feasible approach to approachability for Stackelberg stochastic games, following Blackwell's approach. Secondly, we develop a reinforcement learning algorithm to learn the approachable strategy when the transition kernel is unknown. Our work also includes a complete characterization of Blackwell's necessary and sufficient condition for approachability for convex sets and sufficient conditions for non-convex sets.",1
"We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.",0
"Our proposed framework for deep reinforcement learning is both conceptually simple and lightweight. It uses asynchronous gradient descent to optimize deep neural network controllers. We have developed asynchronous versions of four standard reinforcement learning algorithms and have found that parallel actor-learners help to stabilize training, enabling all four methods to effectively train neural network controllers. The most successful method, an asynchronous version of actor-critic, outperforms the current state-of-the-art on the Atari domain while training for only half the time on a single multi-core CPU instead of a GPU. Additionally, we have demonstrated that asynchronous actor-critic is successful in solving a wide range of continuous motor control problems, as well as a novel task of navigating random 3D mazes using visual input.",1
"Deep reinforcement learning has been shown to be a powerful framework for learning policies from complex high-dimensional sensory inputs to actions in complex tasks, such as the Atari domain. In this paper, we explore output representation modeling in the form of temporal abstraction to improve convergence and reliability of deep reinforcement learning approaches. We concentrate on macro-actions, and evaluate these on different Atari 2600 games, where we show that they yield significant improvements in learning speed. Additionally, we show that they can even achieve better scores than DQN. We offer analysis and explanation for both convergence and final results, revealing a problem deep RL approaches have with sparse reward signals.",0
"The effectiveness of deep reinforcement learning has been demonstrated in learning policies for intricate tasks involving high-dimensional sensory inputs, such as those found in Atari games. This study investigates the use of temporal abstraction in output representation modeling to enhance the reliability and convergence of deep reinforcement learning techniques. Specifically, the focus is on macro-actions, and their performance is evaluated on various Atari 2600 games. The results indicate that macro-actions result in significant improvements in learning speed and can even surpass the performance of DQN. The study also provides insights into the challenges that deep RL approaches face with sparse reward signals, which are analyzed and explained.",1
"State of the art deep reinforcement learning algorithms take many millions of interactions to attain human-level performance. Humans, on the other hand, can very quickly exploit highly rewarding nuances of an environment upon first discovery. In the brain, such rapid learning is thought to depend on the hippocampus and its capacity for episodic memory. Here we investigate whether a simple model of hippocampal episodic control can learn to solve difficult sequential decision-making tasks. We demonstrate that it not only attains a highly rewarding strategy significantly faster than state-of-the-art deep reinforcement learning algorithms, but also achieves a higher overall reward on some of the more challenging domains.",0
"Cutting-edge deep reinforcement learning algorithms require millions of interactions to achieve human-level performance. In contrast, humans can swiftly capitalize on lucrative aspects of a new environment. This rapid learning is believed to rely on the hippocampus and its ability to store episodic memories. Our study examines whether a basic model of hippocampal episodic control can learn to tackle complex sequential decision-making tasks. Our findings reveal that this model not only outperforms state-of-the-art deep reinforcement learning algorithms by acquiring a highly rewarding strategy more quickly, but it also attains a greater overall reward in some of the more demanding domains.",1
"Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.",0
"The idea of learning a policy from expert behavior without any interaction or access to reinforcement signals is considered. One method involves using inverse reinforcement learning to recover the expert's cost function and then using reinforcement learning to extract a policy from it. However, this approach is indirect and slow. To address this, a new framework is proposed to directly extract a policy from data as if it were obtained through inverse reinforcement learning and reinforcement learning. This framework draws similarities between imitation learning and generative adversarial networks. A model-free imitation learning algorithm based on this framework is derived, which shows significant performance improvements in imitating complex behaviors in large, high-dimensional environments compared to existing model-free methods.",1
"Learning robust value functions given raw observations and rewards is now possible with model-free and model-based deep reinforcement learning algorithms. There is a third alternative, called Successor Representations (SR), which decomposes the value function into two components -- a reward predictor and a successor map. The successor map represents the expected future state occupancy from any given state and the reward predictor maps states to scalar rewards. The value function of a state can be computed as the inner product between the successor map and the reward weights. In this paper, we present DSR, which generalizes SR within an end-to-end deep reinforcement learning framework. DSR has several appealing properties including: increased sensitivity to distal reward changes due to factorization of reward and world dynamics, and the ability to extract bottleneck states (subgoals) given successor maps trained under a random policy. We show the efficacy of our approach on two diverse environments given raw pixel observations -- simple grid-world domains (MazeBase) and the Doom game engine.",0
"Deep reinforcement learning algorithms, both model-free and model-based, now enable us to learn robust value functions from raw observations and rewards. Alternatively, Successor Representations (SR) is a third option that breaks down the value function into two parts: a reward predictor and a successor map. The successor map predicts the future state occupancy from any given state, while the reward predictor maps states to scalar rewards. The value function of a state can be determined by computing the inner product between the successor map and the reward weights. In this study, we introduce DSR, which extends SR within an end-to-end deep reinforcement learning framework. DSR offers several benefits, such as being more sensitive to distal reward changes due to the separation of reward and world dynamics and the ability to identify bottleneck states (subgoals) using successor maps trained under a random policy. We demonstrate the effectiveness of our method on two distinct environments with raw pixel observations: simple grid-world domains (MazeBase) and the Doom game engine.",1
"Algorithm design is a laborious process and often requires many iterations of ideation and validation. In this paper, we explore automating algorithm design and present a method to learn an optimization algorithm, which we believe to be the first method that can automatically discover a better algorithm. We approach this problem from a reinforcement learning perspective and represent any particular optimization algorithm as a policy. We learn an optimization algorithm using guided policy search and demonstrate that the resulting algorithm outperforms existing hand-engineered algorithms in terms of convergence speed and/or the final objective value.",0
"The process of designing an algorithm is often time-consuming and involves numerous cycles of developing and verifying ideas. This study investigates the possibility of automating algorithm design and introduces a technique for acquiring an optimization algorithm. We believe that our approach is the first to enable automatic discovery of an improved algorithm. Our strategy for tackling this challenge is based on reinforcement learning, where each optimization algorithm is treated as a policy. We utilize guided policy search to learn an optimization algorithm and prove that it surpasses pre-existing, manually crafted algorithms in terms of convergence rate and/or final objective value.",1
"OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.",0
"The software known as OpenAI Gym is a research toolkit for reinforcement learning. Its features consist of a set of benchmark problems that follow a shared interface, as well as a platform where individuals can showcase their findings and measure algorithm performance. This paper delves into the design choices and components that make up OpenAI Gym.",1
"We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.",0
"Weight normalization is a technique we introduce that alters the weight vectors in a neural network by separating their length from their direction. This reparameterization enhances the optimization problem's conditioning and accelerates stochastic gradient descent. Our weight normalization approach is modeled after batch normalization but doesn't create any connections between the examples in a minibatch. This means it can be applied to recurrent models like LSTMs and noise-sensitive applications such as deep reinforcement learning or generative models, which batch normalization isn't as well suited for. Our method is simpler but still provides much of the speed-up of full batch normalization with a lower computational overhead, allowing for more optimization steps in the same timeframe. We demonstrate the benefits of our strategy in supervised image recognition, generative modeling, and deep reinforcement learning applications.",1
"Future advancements in robot autonomy and sophistication of robotics tasks rest on robust, efficient, and task-dependent semantic understanding of the environment. Semantic segmentation is the problem of simultaneous segmentation and categorization of a partition of sensory data. The majority of current approaches tackle this using multi-class segmentation and labeling in a Conditional Random Field (CRF) framework or by generating multiple object hypotheses and combining them sequentially. In practical settings, the subset of semantic labels that are needed depend on the task and particular scene and labelling every single pixel is not always necessary. We pursue these observations in developing a more modular and flexible approach to multi-class parsing of RGBD data based on learning strategies for combining independent binary object-vs-background segmentations in place of the usual monolithic multi-label CRF approach. Parameters for the independent binary segmentation models can be learned very efficiently, and the combination strategy---learned using reinforcement learning---can be set independently and can vary over different tasks and environments. Accuracy is comparable to state-of-art methods on a subset of the NYU-V2 dataset of indoor scenes, while providing additional flexibility and modularity.",0
"The ability of robots to become more autonomous and handle complex tasks depends on their capability to understand the environment in a robust, efficient, and task-specific manner. Semantic segmentation is a challenge that involves simultaneously categorizing and segmenting a sensory data partition. Most existing methods tackle this problem by using multi-class segmentation and labeling in a Conditional Random Field (CRF) framework, or by generating multiple object hypotheses and sequentially combining them. However, in practical situations, the required subset of semantic labels depends on the task and particular scene, and labeling every single pixel is not always necessary. To address these issues, we propose a modular and flexible approach to multi-class parsing of RGBD data, where we combine independent binary object-vs-background segmentations instead of the monolithic multi-label CRF approach. The parameters for the independent binary segmentation models can be learned efficiently, and the combination strategy, which is learned using reinforcement learning, can be set independently and varied for different tasks and environments. Our approach provides additional flexibility and modularity while maintaining accuracy comparable to state-of-the-art methods on a subset of the NYU-V2 dataset of indoor scenes.",1
"We discuss a variant of Thompson sampling for nonparametric reinforcement learning in a countable classes of general stochastic environments. These environments can be non-Markov, non-ergodic, and partially observable. We show that Thompson sampling learns the environment class in the sense that (1) asymptotically its value converges to the optimal value in mean and (2) given a recoverability assumption regret is sublinear.",0
"In this discussion, we explore a modified version of Thompson sampling that applies to nonparametric reinforcement learning in a range of stochastic environments that are countable. These environments have the potential to be partially observable, non-Markov, and non-ergodic. Our findings reveal that Thompson sampling effectively learns the environment class, as demonstrated by two key outcomes: (1) its value asymptotically converges to the optimal value in mean, and (2) the regret is sublinear under the recoverability assumption.",1
"Many interesting real world domains involve reinforcement learning (RL) in partially observable environments. Efficient learning in such domains is important, but existing sample complexity bounds for partially observable RL are at least exponential in the episode length. We give, to our knowledge, the first partially observable RL algorithm with a polynomial bound on the number of episodes on which the algorithm may not achieve near-optimal performance. Our algorithm is suitable for an important class of episodic POMDPs. Our approach builds on recent advances in method of moments for latent variable model estimation.",0
"Reinforcement learning (RL) is applicable to several fascinating real-life domains that are partially observable. The effectiveness of learning in such domains is crucial, but current sample complexity bounds for partially observable RL require exponential time in the episode length. We introduce the first algorithm for partially observable RL that has a polynomial limit on the number of episodes where near-optimal performance may not be achieved. Our algorithm is ideal for an essential group of episodic POMDPs, and we have based our approach on recent developments in the method of moments for latent variable model estimation.",1
"Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma's Revenge'.",0
"Reinforcement learning algorithms face a significant obstacle in learning goal-directed behavior in environments with limited feedback. This difficulty arises from inadequate exploration, which prevents agents from developing robust value functions. To overcome this challenge, intrinsically motivated agents can explore novel behaviors for their own sake, potentially leading to solutions for environment tasks. Our solution to this problem is hierarchical-DQN (h-DQN), which combines hierarchical value functions operating at different time intervals with intrinsically motivated deep reinforcement learning. The top-level value function learns a policy over intrinsic goals, while the lower-level function learns a policy over atomic actions to meet the specified goals. h-DQN enables flexible goal specifications, including functions over entities and relationships, providing an efficient exploration method in complex environments. We showcase the effectiveness of our approach in two situations with minimal, delayed feedback: a complex discrete stochastic decision process and the classic ATARI game ""Montezuma's Revenge.""",1
"This paper presents research in progress investigating the viability and adaptation of reinforcement learning using deep neural network based function approximation for the task of radio control and signal detection in the wireless domain. We demonstrate a successful initial method for radio control which allows naive learning of search without the need for expert features, heuristics, or search strategies. We also introduce Kerlym, an open Keras based reinforcement learning agent collection for OpenAI's Gym.",0
"The current study explores the feasibility and adjustment of reinforcement learning by utilizing deep neural network function approximation to manage radio control and signal detection in the wireless field. Our research illustrates a promising preliminary approach to radio control that enables unsophisticated learning of search without expert features, heuristics, or search strategies. Additionally, we introduce Kerlym, a publicly accessible collection of reinforcement learning agents based on Keras for OpenAI's Gym.",1
"Recently, researchers have made significant progress combining the advances in deep learning for learning feature representations with reinforcement learning. Some notable examples include training agents to play Atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs. However, it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel findings based on the systematic evaluation of a range of implemented reinforcement learning algorithms. Both the benchmark and reference implementations are released at https://github.com/rllab/rllab in order to facilitate experimental reproducibility and to encourage adoption by other researchers.",0
"Deep learning advancements have been combined with reinforcement learning, resulting in significant progress. Notable achievements include training agents to play Atari games and develop advanced manipulation skills using raw sensory inputs. However, the lack of a widely accepted benchmark has made it challenging to measure progress in continuous control. This study introduces a benchmark suite of continuous control tasks, encompassing classic tasks, high dimensionality tasks, partial observation tasks, and hierarchical tasks. The evaluation of various reinforcement learning algorithms revealed new findings. The benchmark and reference implementations are available on https://github.com/rllab/rllab for reproducibility and adoption by other researchers.",1
"Reinforcement learning can acquire complex behaviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice. We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems. Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems. To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering. To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency.",0
"In practice, it can be difficult to define a cost function that accurately encodes the desired task and can be effectively optimized when using reinforcement learning to acquire complex behaviors from high-level specifications. In this study, we investigate how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, specifically in the context of torque control for high-dimensional robotic systems. Our approach tackles two key challenges associated with IOC: the need for informative features and effective regularization to structure the cost, and the difficulty of learning the cost function for high-dimensional continuous systems with unknown dynamics. To address the first challenge, we introduce an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without requiring detailed feature engineering. To address the second challenge, we develop an efficient sample-based approximation for MaxEnt IOC. Through a series of simulated tasks and real-world robotic manipulation problems, we demonstrate that our method outperforms existing approaches in terms of both task complexity and sample efficiency.",1
"In imitation learning, an agent learns how to behave in an environment with an unknown cost function by mimicking expert demonstrations. Existing imitation learning algorithms typically involve solving a sequence of planning or reinforcement learning problems. Such algorithms are therefore not directly applicable to large, high-dimensional environments, and their performance can significantly degrade if the planning problems are not solved to optimality. Under the apprenticeship learning formalism, we develop alternative model-free algorithms for finding a parameterized stochastic policy that performs at least as well as an expert policy on an unknown cost function, based on sample trajectories from the expert. Our approach, based on policy gradients, scales to large continuous environments with guaranteed convergence to local minima.",0
"The process of imitation learning involves an agent mimicking expert demonstrations to learn how to operate in an unfamiliar environment with an unknown cost function. However, current imitation learning algorithms entail resolving a series of planning or reinforcement learning problems, which limits their applicability to large, high-dimensional environments. Moreover, their performance can deteriorate if the planning problems are not solved optimally. To address this, we propose model-free algorithms under the apprenticeship learning framework that aim to discover a parameterized stochastic policy that is at least as effective as the expert policy on an unknown cost function, utilizing sample trajectories from the expert. Our methodology, which relies on policy gradients, can be expanded to encompass large, continuous environments, and it ensures convergence to local minima.",1
"We study the problem of off-policy value evaluation in reinforcement learning (RL), where one aims to estimate the value of a new policy based on data collected by a different policy. This problem is often a critical step when applying RL in real-world problems. Despite its importance, existing general methods either have uncontrolled bias or suffer high variance. In this work, we extend the doubly robust estimator for bandits to sequential decision-making problems, which gets the best of both worlds: it is guaranteed to be unbiased and can have a much lower variance than the popular importance sampling estimators. We demonstrate the estimator's accuracy in several benchmark problems, and illustrate its use as a subroutine in safe policy improvement. We also provide theoretical results on the hardness of the problem, and show that our estimator can match the lower bound in certain scenarios.",0
"The focus of our research is the issue of off-policy value evaluation in reinforcement learning (RL). This pertains to the estimation of a new policy's value based on data obtained from a different policy. This challenge is vital when utilizing RL in real-life scenarios. However, current general techniques suffer from high variance or uncontrolled bias. In this study, we expand the doubly robust estimator for bandits to sequential decision-making problems. This approach offers an unbiased guarantee and a lower variance than the commonly used importance sampling estimators. Through various benchmark problems, we demonstrate the accuracy of our estimator and its application as a subroutine in safe policy improvement. Additionally, we present theoretical findings on the problem's difficulty and demonstrate that our estimator can match the lower limit in specific situations.",1
"Artificial intelligence is commonly defined as the ability to achieve goals in the world. In the reinforcement learning framework, goals are encoded as reward functions that guide agent behaviour, and the sum of observed rewards provide a notion of progress. However, some domains have no such reward signal, or have a reward signal so sparse as to appear absent. Without reward feedback, agent behaviour is typically random, often dithering aimlessly and lacking intentionality. In this paper we present an algorithm capable of learning purposeful behaviour in the absence of rewards. The algorithm proceeds by constructing temporally extended actions (options), through the identification of purposes that are ""just out of reach"" of the agent's current behaviour. These purposes establish intrinsic goals for the agent to learn, ultimately resulting in a suite of behaviours that encourage the agent to visit different parts of the state space. Moreover, the approach is particularly suited for settings where rewards are very sparse, and such behaviours can help in the exploration of the environment until reward is observed.",0
"Artificial intelligence is often defined as the ability to accomplish objectives in the real world. In the framework of reinforcement learning, objectives are expressed as reward functions that direct the actions of an agent, and the sum of rewards obtained reflects progress. Nevertheless, certain domains lack such a signal or have a signal that is so infrequent that it seems non-existent. In the absence of reward feedback, agent behavior is typically haphazard, with no clear intent. In this study, we propose an algorithm that can learn purposeful actions even without rewards. The algorithm operates by creating temporally extended actions (options) by identifying objectives that are nearly unachievable by the agent's current behavior. These objectives establish inherent goals for the agent to acquire, leading to a repertoire of behaviors that encourage the agent to explore different parts of the state space. Furthermore, this approach is particularly effective in situations where rewards are scarce, and such behaviors can facilitate environment exploration until a reward is discovered.",1
"Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory. Specifically, we propose to incorporate Maximum Inner Product Search (MIPS) in the training and inference procedures for our hierarchical memory network. We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task.",0
"Neural networks with explicit memory components, called memory networks, have the ability to both read and write to the memory. The memory is typically addressed using a softmax function, allowing for end-to-end training with backpropagation. However, this approach becomes inefficient for applications that require reading from large memories. Although hard attention mechanisms based on reinforcement learning are known to be difficult to train, they can provide a more efficient solution. In this study, we propose a hierarchical memory network that combines both hard and soft attention mechanisms. The hierarchical structure enables faster reading from the memory while being easier to train than a flat memory with hard attention. We introduce Maximum Inner Product Search (MIPS) in the training and inference procedures of our hierarchical memory network and evaluate its performance on SimpleQuestions, a challenging factoid question answering task. We also explore different state-of-the art approximate MIPS techniques.",1
"A key challenge in fine-grained recognition is how to find and represent discriminative local regions. Recent attention models are capable of learning discriminative region localizers only from category labels with reinforcement learning. However, not utilizing any explicit part information, they are not able to accurately find multiple distinctive regions. In this work, we introduce an attribute-guided attention localization scheme where the local region localizers are learned under the guidance of part attribute descriptions. By designing a novel reward strategy, we are able to learn to locate regions that are spatially and semantically distinctive with reinforcement learning algorithm. The attribute labeling requirement of the scheme is more amenable than the accurate part location annotation required by traditional part-based fine-grained recognition methods. Experimental results on the CUB-200-2011 dataset demonstrate the superiority of the proposed scheme on both fine-grained recognition and attribute recognition.",0
"Fine-grained recognition poses a significant challenge in identifying and representing local regions that can be used for discrimination. While recent attention models have the ability to learn localizers that are discriminative, they can only do so based on category labels and reinforcement learning, without any explicit knowledge of parts. This makes it difficult to accurately locate multiple distinctive regions. To address this issue, we propose an attribute-guided attention localization scheme that uses part attribute descriptions to learn local region localizers. Our reward strategy helps us locate regions that are spatially and semantically distinctive using reinforcement learning. The scheme requires attribute labeling, which is easier than accurate part location annotation required by traditional part-based fine-grained recognition methods. Our experimental results on the CUB-200-2011 dataset demonstrate the superiority of our scheme in both fine-grained recognition and attribute recognition.",1
"Recently, there has been significant progress in understanding reinforcement learning in discounted infinite-horizon Markov decision processes (MDPs) by deriving tight sample complexity bounds. However, in many real-world applications, an interactive learning agent operates for a fixed or bounded period of time, for example tutoring students for exams or handling customer service requests. Such scenarios can often be better treated as episodic fixed-horizon MDPs, for which only looser bounds on the sample complexity exist. A natural notion of sample complexity in this setting is the number of episodes required to guarantee a certain performance with high probability (PAC guarantee). In this paper, we derive an upper PAC bound $\tilde O(\frac{|\mathcal S|^2 |\mathcal A| H^2}{\epsilon^2} \ln\frac 1 \delta)$ and a lower PAC bound $\tilde \Omega(\frac{|\mathcal S| |\mathcal A| H^2}{\epsilon^2} \ln \frac 1 {\delta + c})$ that match up to log-terms and an additional linear dependency on the number of states $|\mathcal S|$. The lower bound is the first of its kind for this setting. Our upper bound leverages Bernstein's inequality to improve on previous bounds for episodic finite-horizon MDPs which have a time-horizon dependency of at least $H^3$.",0
"Significant progress has been made in comprehending reinforcement learning in discounted infinite-horizon Markov decision processes (MDPs) by developing precise sample complexity bounds. However, in real-world scenarios, learning agents often operate for a fixed or limited period, such as tutoring students or managing customer service requests. In such cases, it is more suitable to use episodic fixed-horizon MDPs, for which less precise sample complexity bounds exist. The number of episodes required to ensure a certain performance with high probability (PAC guarantee) is a natural measure of sample complexity in this setting. This study presents an upper PAC bound of $\tilde O(\frac{|\mathcal S|^2 |\mathcal A| H^2}{\epsilon^2} \ln\frac 1 \delta)$ and a lower PAC bound of $\tilde \Omega(\frac{|\mathcal S| |\mathcal A| H^2}{\epsilon^2} \ln \frac 1 {\delta + c})$, which match up to log-terms and an additional linear dependency on the number of states $|\mathcal S|$. The lower bound is the first of its kind for this setting. Our upper bound uses Bernstein's inequality to improve on previous bounds for episodic finite-horizon MDPs, which have a time-horizon dependency of at least $H^3$.",1
"The recently introduced Deep Q-Networks (DQN) algorithm has gained attention as one of the first successful combinations of deep neural networks and reinforcement learning. Its promise was demonstrated in the Arcade Learning Environment (ALE), a challenging framework composed of dozens of Atari 2600 games used to evaluate general competency in AI. It achieved dramatically better results than earlier approaches, showing that its ability to learn good representations is quite robust and general. This paper attempts to understand the principles that underlie DQN's impressive performance and to better contextualize its success. We systematically evaluate the importance of key representational biases encoded by DQN's network by proposing simple linear representations that make use of these concepts. Incorporating these characteristics, we obtain a computationally practical feature set that achieves competitive performance to DQN in the ALE. Besides offering insight into the strengths and weaknesses of DQN, we provide a generic representation for the ALE, significantly reducing the burden of learning a representation for each game. Moreover, we also provide a simple, reproducible benchmark for the sake of comparison to future work in the ALE.",0
"The combination of deep neural networks and reinforcement learning has proven successful in the Deep Q-Networks (DQN) algorithm, which has gained attention for its impressive performance in the challenging Arcade Learning Environment (ALE). This paper aims to understand the principles underlying DQN's success by evaluating the importance of key representational biases encoded in its network. By proposing simple linear representations that incorporate these characteristics, we obtain a feature set that achieves competitive performance to DQN in the ALE. This not only offers insight into DQN's strengths and weaknesses but also provides a generic representation for the ALE, reducing the burden of learning a representation for each game. Additionally, we provide a reproducible benchmark for future work in the ALE.",1
"Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a partially observed guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.",0
"The implementation of policy search techniques can enable robots to acquire control policies for diverse tasks. However, the practical application of policy search often necessitates manually engineered components for low-level control, state estimation, and perception. This study aims to determine whether joint end-to-end training of the perception and control systems yields superior performance compared to the separate training of each component. We introduce a novel approach that uses deep convolutional neural networks (CNNs) with 92,000 parameters to learn policies that directly map raw image observations to torques at the robot's motors. Our method employs a partially observed guided policy search technique that transforms policy search into supervised learning. A simple trajectory-centric reinforcement learning approach provides supervision. We evaluate the proposed method on various real-world manipulation tasks that require close coordination between control and vision, such as screwing a cap onto a bottle, and compare it to other policy search methods using simulations.",1
"Policy advice is a transfer learning method where a student agent is able to learn faster via advice from a teacher. However, both this and other reinforcement learning transfer methods have little theoretical analysis. This paper formally defines a setting where multiple teacher agents can provide advice to a student and introduces an algorithm to leverage both autonomous exploration and teacher's advice. Our regret bounds justify the intuition that good teachers help while bad teachers hurt. Using our formalization, we are also able to quantify, for the first time, when negative transfer can occur within such a reinforcement learning setting.",0
"The method of policy advice involves a student agent receiving guidance from a teacher, which facilitates faster learning. Despite the effectiveness of this and other transfer methods in reinforcement learning, there is a lack of theoretical analysis. This research establishes a framework wherein a student can receive advice from multiple teacher agents and presents an algorithm that utilizes both autonomous exploration and teacher guidance. Our analysis demonstrates that proficient teachers enhance learning while incompetent ones hinder it. Moreover, our framework enables us to quantify instances of negative transfer in this reinforcement learning context, which is a novel contribution.",1
"This work discusses a closed-loop control strategy for complex systems utilizing scarce and streaming data. A discrete embedding space is first built using hash functions applied to the sensor measurements from which a Markov process model is derived, approximating the complex system's dynamics. A control strategy is then learned using reinforcement learning once rewards relevant with respect to the control objective are identified. This method is designed for experimental configurations, requiring no computations nor prior knowledge of the system, and enjoys intrinsic robustness. It is illustrated on two systems: the control of the transitions of a Lorenz 63 dynamical system, and the control of the drag of a cylinder flow. The method is shown to perform well.",0
"The work presented here focuses on a closed-loop control approach for complex systems that rely on limited and continuous data. Initially, a hash function is applied to the sensor measurements to construct a discrete embedding space. From this, a Markov process model is created to approximate the dynamics of the complex system. Once relevant rewards are identified based on the control objective, a control strategy is learned using reinforcement learning. This method is suitable for experimental setups and does not require any prior knowledge or computations, making it intrinsically robust. The effectiveness of this approach is demonstrated through two examples: controlling the transitions in a Lorenz 63 dynamical system and the drag in a cylinder flow. Overall, the results show that this method performs well.",1
"In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.",0
"There have been numerous successful uses of deep representations in reinforcement learning in recent years. However, many of these applications rely on traditional architectures like convolutional networks, LSTMs, or auto-encoders. This paper introduces a new neural network architecture for model-free reinforcement learning, called the dueling network. This network includes two estimators: one for the state value function and another for the state-dependent action advantage function. This factorization offers the advantage of generalized learning across actions without modifying the underlying reinforcement learning algorithm. Our findings demonstrate that this architecture results in improved policy evaluation in situations with multiple similarly valued actions. Additionally, the dueling architecture empowers our RL agent to perform better than the state-of-the-art on the Atari 2600 domain.",1
"In this paper we present a new way of predicting the performance of a reinforcement learning policy given historical data that may have been generated by a different policy. The ability to evaluate a policy from historical data is important for applications where the deployment of a bad policy can be dangerous or costly. We show empirically that our algorithm produces estimates that often have orders of magnitude lower mean squared error than existing methods---it makes more efficient use of the available data. Our new estimator is based on two advances: an extension of the doubly robust estimator (Jiang and Li, 2015), and a new way to mix between model based estimates and importance sampling based estimates.",0
A new method for predicting the performance of a reinforcement learning policy is presented in this paper. This method utilizes historical data that may have been generated by a different policy to evaluate the performance of the current policy. The importance of this method lies in its ability to prevent the deployment of a bad policy which can be dangerous or costly. Empirical evidence shows that the algorithm produces more accurate estimates than existing methods by making more efficient use of available data. This new estimator is based on two advancements: an extension of the doubly robust estimator and a new approach to combining model-based and importance sampling-based estimates.,1
"Off-policy reinforcement learning has many applications including: learning from demonstration, learning multiple goal seeking policies in parallel, and representing predictive knowledge. Recently there has been an proliferation of new policy-evaluation algorithms that fill a longstanding algorithmic void in reinforcement learning: combining robustness to off-policy sampling, function approximation, linear complexity, and temporal difference (TD) updates. This paper contains two main contributions. First, we derive two new hybrid TD policy-evaluation algorithms, which fill a gap in this collection of algorithms. Second, we perform an empirical comparison to elicit which of these new linear TD methods should be preferred in different situations, and make concrete suggestions about practical use.",0
"Off-policy reinforcement learning has a variety of uses, such as learning from demonstrations, developing multiple goal-seeking policies simultaneously, and representing predictive knowledge. In recent times, several policy-evaluation algorithms have emerged to address the long-standing issue of reinforcement learning algorithms. These algorithms combine off-policy sampling, function approximation, linear complexity, and temporal difference (TD) updates. This paper presents two main contributions. Firstly, it introduces two new hybrid TD policy-evaluation algorithms to bridge the gap in the existing set of algorithms. Secondly, it conducts an empirical analysis to determine which of these linear TD methods is best suited for different situations, and provides practical recommendations.",1
"We describe and study a model for an Automated Online Recommendation System (AORS) in which a user's preferences can be time-dependent and can also depend on the history of past recommendations and play-outs. The three key features of the model that makes it more realistic compared to existing models for recommendation systems are (1) user preference is inherently latent, (2) current recommendations can affect future preferences, and (3) it allows for the development of learning algorithms with provable performance guarantees. The problem is cast as an average-cost restless multi-armed bandit for a given user, with an independent partially observable Markov decision process (POMDP) for each item of content. We analyze the POMDP for a single arm, describe its structural properties, and characterize its optimal policy. We then develop a Thompson sampling-based online reinforcement learning algorithm to learn the parameters of the model and optimize utility from the binary responses of the users to continuous recommendations. We then analyze the performance of the learning algorithm and characterize the regret. Illustrative numerical results and directions for extension to the restless hidden Markov multi-armed bandit problem are also presented.",0
"In this study, we examine an Automated Online Recommendation System (AORS) model that takes into account a user's changing preferences and history of past recommendations and play-outs. This model stands out from existing ones in three ways: (1) user preference is latent, (2) current recommendations can influence future preferences, and (3) it allows for the development of learning algorithms that are provably effective. We frame the problem as a restless multi-armed bandit scenario where each item of content is an independent partially observable Markov decision process (POMDP). We analyze the POMDP for a single arm, detail its structural properties, and identify its optimal policy. We then introduce a Thompson sampling-based online reinforcement learning algorithm that adapts the model parameters and maximizes user satisfaction based on their binary responses to continuous recommendations. We evaluate the algorithm's performance and measure the regret. Finally, we present illustrative numerical results and suggest potential extensions to the restless hidden Markov multi-armed bandit problem.",1
"Clearly explaining a rationale for a classification decision to an end-user can be as important as the decision itself. Existing approaches for deep visual recognition are generally opaque and do not output any justification text; contemporary vision-language models can describe image content but fail to take into account class-discriminative image aspects which justify visual predictions. We propose a new model that focuses on the discriminating properties of the visible object, jointly predicts a class label, and explains why the predicted label is appropriate for the image. We propose a novel loss function based on sampling and reinforcement learning that learns to generate sentences that realize a global sentence property, such as class specificity. Our results on a fine-grained bird species classification dataset show that our model is able to generate explanations which are not only consistent with an image but also more discriminative than descriptions produced by existing captioning methods.",0
"As much as the decision itself, it can be crucial to effectively communicate the reasoning behind a classification decision to the end-user. Most current methods for deep visual recognition lack transparency and fail to provide any justification text, while vision-language models overlook key aspects that would justify their predictions. To address this issue, we have developed a new model that not only predicts a class label for an image, but also explains why that label is appropriate by focusing on the distinguishing features of the object in question. To achieve this, we have created a unique loss function that employs sampling and reinforcement learning to generate sentences that exhibit a global sentence property, such as class specificity. Our model has been tested on a fine-grained bird species classification dataset, and our results show that the explanations it generates are both consistent with the image and more discriminating than those produced by existing captioning methods.",1
"Datasets with hundreds to tens of thousands features is the new norm. Feature selection constitutes a central problem in machine learning, where the aim is to derive a representative set of features from which to construct a classification (or prediction) model for a specific task. Our experimental study involves microarray gene expression datasets, these are high-dimensional and noisy datasets that contain genetic data typically used for distinguishing between benign or malicious tissues or classifying different types of cancer. In this paper, we formulate feature selection as a multiagent coordination problem and propose a novel feature selection method using multiagent reinforcement learning. The central idea of the proposed approach is to ""assign"" a reinforcement learning agent to each feature where each agent learns to control a single feature, we refer to this approach as MARL. Applying this to microarray datasets creates an enormous multiagent coordination problem between thousands of learning agents. To address the scalability challenge we apply a form of reward shaping called CLEAN rewards. We compare in total nine feature selection methods, including state-of-the-art methods, and show that the proposed method using CLEAN rewards can significantly scale-up, thus outperforming the rest of learning-based methods. We further show that a hybrid variant of MARL achieves the best overall performance.",0
"Nowadays, it is common to encounter datasets that have hundreds to tens of thousands of features. Machine learning faces a central problem of feature selection, where the goal is to choose a representative set of features to build a classification or prediction model for a specific task. Our research focuses on microarray gene expression datasets, which are high-dimensional and noisy datasets containing genetic information that is typically used to differentiate between benign or malignant tissues or classify various types of cancer. In this paper, we approach feature selection as a multiagent coordination problem and introduce a novel method using multiagent reinforcement learning, known as MARL. This involves assigning a reinforcement learning agent to each feature and using CLEAN rewards to address scalability issues. We compare our method with state-of-the-art techniques and demonstrate that using CLEAN rewards significantly improves scalability and outperforms other learning-based methods. We also show that a hybrid version of MARL achieves the best overall performance.",1
"Stochastic Gradient Descent (SGD) is one of the most widely used techniques for online optimization in machine learning. In this work, we accelerate SGD by adaptively learning how to sample the most useful training examples at each time step. First, we show that SGD can be used to learn the best possible sampling distribution of an importance sampling estimator. Second, we show that the sampling distribution of an SGD algorithm can be estimated online by incrementally minimizing the variance of the gradient. The resulting algorithm - called Adaptive Weighted SGD (AW-SGD) - maintains a set of parameters to optimize, as well as a set of parameters to sample learning examples. We show that AWSGD yields faster convergence in three different applications: (i) image classification with deep features, where the sampling of images depends on their labels, (ii) matrix factorization, where rows and columns are not sampled uniformly, and (iii) reinforcement learning, where the optimized and exploration policies are estimated at the same time, where our approach corresponds to an off-policy gradient algorithm.",0
"One of the most commonly used techniques for online optimization in machine learning is Stochastic Gradient Descent (SGD). In this study, we have accelerated SGD by adaptively learning the most valuable training examples at each time step. Initially, we demonstrate that SGD can be employed to learn the best possible sampling distribution of an importance sampling estimator. Secondly, we demonstrate that the sampling distribution of an SGD algorithm can be assessed online by progressively reducing the gradient's variance. The resulting algorithm, referred to as Adaptive Weighted SGD (AW-SGD), optimizes a set of parameters and samples learning examples using a set of parameters. We demonstrate that AW-SGD achieves faster convergence in three distinct applications, including image classification with deep features, matrix factorization, and reinforcement learning. In the case of reinforcement learning, our approach corresponds to an off-policy gradient algorithm, where the optimized and exploration policies are estimated simultaneously.",1
"In this paper, we present a robotic model-based reinforcement learning method that combines ideas from model identification and model predictive control. We use a feature-based representation of the dynamics that allows the dynamics model to be fitted with a simple least squares procedure, and the features are identified from a high-level specification of the robot's morphology, consisting of the number and connectivity structure of its links. Model predictive control is then used to choose the actions under an optimistic model of the dynamics, which produces an efficient and goal-directed exploration strategy. We present real time experimental results on standard benchmark problems involving the pendulum, cartpole, and double pendulum systems. Experiments indicate that our method is able to learn a range of benchmark tasks substantially faster than the previous best methods. To evaluate our approach on a realistic robotic control task, we also demonstrate real time control of a simulated 7 degree of freedom arm.",0
"This paper proposes a new approach to robotic model-based reinforcement learning that combines model identification and model predictive control. A feature-based representation of the dynamics is used, which allows for simple fitting with a least squares procedure. The features are identified based on a high-level specification of the robot's morphology, specifically the links' connectivity structure and number. Using an optimistic model of the dynamics, model predictive control is employed to choose actions, resulting in an efficient and goal-directed exploration strategy. Experimental results on standard benchmark problems involving the pendulum, cartpole, and double pendulum systems are presented, indicating that this method learns tasks faster than previous methods. Additionally, real-time control of a simulated 7 degree of freedom arm is used to demonstrate the approach's effectiveness on a realistic robotic control task.",1
"This paper presents a general framework for exploiting the representational capacity of neural networks to approximate complex, nonlinear reward functions in the context of solving the inverse reinforcement learning (IRL) problem. We show in this context that the Maximum Entropy paradigm for IRL lends itself naturally to the efficient training of deep architectures. At test time, the approach leads to a computational complexity independent of the number of demonstrations, which makes it especially well-suited for applications in life-long learning scenarios. Our approach achieves performance commensurate to the state-of-the-art on existing benchmarks while exceeding on an alternative benchmark based on highly varying reward structures. Finally, we extend the basic architecture - which is equivalent to a simplified subclass of Fully Convolutional Neural Networks (FCNNs) with width one - to include larger convolutions in order to eliminate dependency on precomputed spatial features and work on raw input representations.",0
"In this paper, we introduce a framework that utilizes neural networks to approximate complex and nonlinear reward functions for solving the inverse reinforcement learning (IRL) problem. We demonstrate that the Maximum Entropy paradigm is well-suited for training deep architectures efficiently in this context. Our approach has a computational complexity that is independent of the number of demonstrations, making it suitable for lifelong learning scenarios. Our method achieves state-of-the-art performance on existing benchmarks and outperforms on an alternative benchmark with highly varying reward structures. Furthermore, we extend the basic architecture to include larger convolutions, eliminating the need for precomputed spatial features and enabling the use of raw input representations.",1
"We present the first differentially private algorithms for reinforcement learning, which apply to the task of evaluating a fixed policy. We establish two approaches for achieving differential privacy, provide a theoretical analysis of the privacy and utility of the two algorithms, and show promising results on simple empirical examples.",0
"Our study introduces novel differential privacy algorithms for reinforcement learning that are specifically designed to evaluate a pre-determined policy. We propose two distinct methods to ensure differential privacy and also conduct a thorough theoretical analysis of the privacy and utility of both approaches. In addition, we demonstrate our algorithms' potential by showcasing their effectiveness in basic empirical scenarios.",1
"Model-free reinforcement learning has been successfully applied to a range of challenging problems, and has recently been extended to handle large neural network policies and value functions. However, the sample complexity of model-free algorithms, particularly when using high-dimensional function approximators, tends to limit their applicability to physical systems. In this paper, we explore algorithms and representations to reduce the sample complexity of deep reinforcement learning for continuous control tasks. We propose two complementary techniques for improving the efficiency of such algorithms. First, we derive a continuous variant of the Q-learning algorithm, which we call normalized adantage functions (NAF), as an alternative to the more commonly used policy gradient and actor-critic methods. NAF representation allows us to apply Q-learning with experience replay to continuous tasks, and substantially improves performance on a set of simulated robotic control tasks. To further improve the efficiency of our approach, we explore the use of learned models for accelerating model-free reinforcement learning. We show that iteratively refitted local linear models are especially effective for this, and demonstrate substantially faster learning on domains where such models are applicable.",0
"Various challenging problems have been successfully tackled with model-free reinforcement learning, and it has been expanded to accommodate large neural network policies and value functions. However, the sample complexity of model-free algorithms is limited in physical systems, particularly when using high-dimensional function approximators. This study investigates algorithms and representations to decrease the sample complexity of deep reinforcement learning for continuous control tasks. Two complementary techniques are proposed to enhance the efficiency of these algorithms. Firstly, a continuous version of the Q-learning algorithm, called normalized advantage functions (NAF), is derived as an alternative to commonly used policy gradient and actor-critic methods. NAF representation enables the use of Q-learning with experience replay for continuous tasks and improves performance on simulated robotic control tasks. Secondly, learned models are explored to accelerate model-free reinforcement learning. Iteratively refitted local linear models are particularly effective, resulting in considerably faster learning on domains where these models are applicable.",1
"Reinforcement learning provides a powerful and flexible framework for automated acquisition of robotic motion skills. However, applying reinforcement learning requires a sufficiently detailed representation of the state, including the configuration of task-relevant objects. We present an approach that automates state-space construction by learning a state representation directly from camera images. Our method uses a deep spatial autoencoder to acquire a set of feature points that describe the environment for the current task, such as the positions of objects, and then learns a motion skill with these feature points using an efficient reinforcement learning method based on local linear models. The resulting controller reacts continuously to the learned feature points, allowing the robot to dynamically manipulate objects in the world with closed-loop control. We demonstrate our method with a PR2 robot on tasks that include pushing a free-standing toy block, picking up a bag of rice using a spatula, and hanging a loop of rope on a hook at various positions. In each task, our method automatically learns to track task-relevant objects and manipulate their configuration with the robot's arm.",0
"Automated acquisition of robotic motion skills can be achieved through reinforcement learning, which offers a versatile and powerful framework. However, a detailed representation of the state, including the configuration of task-relevant objects, is necessary for applying this approach. Our method automates the process of constructing the state-space by directly learning a state representation from camera images. We utilize a deep spatial autoencoder to acquire feature points that describe the environment for the current task, such as the object positions, and then use an efficient reinforcement learning method based on local linear models to learn a motion skill with these feature points. The resulting controller responds continuously to the learned feature points, enabling the robot to manipulate objects in the world with closed-loop control. We showcase the effectiveness of our method with a PR2 robot on tasks, including hanging a loop of rope on a hook, picking up a bag of rice using a spatula, and pushing a free-standing toy block. Our method automatically learns to track task-relevant objects and manipulates their configuration with the robot's arm.",1
"A key problem in reinforcement learning for control with general function approximators (such as deep neural networks and other nonlinear functions) is that, for many algorithms employed in practice, updates to the policy or $Q$-function may fail to improve performance---or worse, actually cause the policy performance to degrade. Prior work has addressed this for policy iteration by deriving tight policy improvement bounds; by optimizing the lower bound on policy improvement, a better policy is guaranteed. However, existing approaches suffer from bounds that are hard to optimize in practice because they include sup norm terms which cannot be efficiently estimated or differentiated. In this work, we derive a better policy improvement bound where the sup norm of the policy divergence has been replaced with an average divergence; this leads to an algorithm, Easy Monotonic Policy Iteration, that generates sequences of policies with guaranteed non-decreasing returns and is easy to implement in a sample-based framework.",0
"Reinforcement learning for control with general function approximators like deep neural networks and other nonlinear functions faces a significant challenge. In many practical algorithms, updates to the policy or $Q$-function may not result in improved performance. In fact, they may even cause a decline in policy performance. Prior research has tackled this issue in policy iteration by deriving precise policy improvement bounds. By optimizing the lower bound on policy improvement, a better policy can be ensured. However, these approaches have limitations since their optimization involves sup norm terms that are difficult to estimate or differentiate. In this study, we present an improved policy improvement bound that replaces the sup norm of the policy divergence with an average divergence. This results in an algorithm called Easy Monotonic Policy Iteration that generates policy sequences with guaranteed non-decreasing returns and is straightforward to implement in a sample-based framework.",1
"Cumulative prospect theory (CPT) is known to model human decisions well, with substantial empirical evidence supporting this claim. CPT works by distorting probabilities and is more general than the classic expected utility and coherent risk measures. We bring this idea to a risk-sensitive reinforcement learning (RL) setting and design algorithms for both estimation and control. The RL setting presents two particular challenges when CPT is applied: estimating the CPT objective requires estimations of the entire distribution of the value function and finding a randomized optimal policy. The estimation scheme that we propose uses the empirical distribution to estimate the CPT-value of a random variable. We then use this scheme in the inner loop of a CPT-value optimization procedure that is based on the well-known simulation optimization idea of simultaneous perturbation stochastic approximation (SPSA). We provide theoretical convergence guarantees for all the proposed algorithms and also illustrate the usefulness of CPT-based criteria in a traffic signal control application.",0
"The effectiveness of Cumulative Prospect Theory (CPT) in modeling human decision-making has been extensively supported by empirical evidence. CPT distorts probabilities and is more versatile than classic expected utility and coherent risk measures. We have adapted this concept to a risk-sensitive Reinforcement Learning (RL) environment, where we have developed algorithms for both estimation and control. However, applying CPT in an RL setting comes with two main challenges: the need to estimate the CPT objective by estimating the entire distribution of the value function and finding a randomized optimal policy. Our proposed estimation scheme utilizes the empirical distribution to establish the CPT-value of a random variable. This scheme is then integrated into an inner loop of a CPT-value optimization process that employs the simultaneous perturbation stochastic approximation (SPSA) simulation optimization principle. We have provided theoretical guarantees for all our algorithms and demonstrated the practical value of CPT-based criteria in optimizing traffic signals.",1
"Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.",0
"The utilization of experience replay by online reinforcement learning agents helps them retain and recycle past experiences. Previous studies employed a method of uniformly sampling experience transitions from a replay memory. However, this technique merely replays transitions at their original frequency, regardless of their level of importance. To improve learning efficiency, we introduce a prioritized experience framework that emphasizes significant transitions for more frequent replay. We apply this framework to Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance in multiple Atari games. Our results indicate that DQN with prioritized experience replay surpasses DQN with uniform replay, achieving a new state-of-the-art performance on 41 out of 49 games.",1
"The ability to act in multiple environments and transfer previous knowledge to new situations can be considered a critical aspect of any intelligent agent. Towards this goal, we define a novel method of multitask and transfer learning that enables an autonomous agent to learn how to behave in multiple tasks simultaneously, and then generalize its knowledge to new domains. This method, termed ""Actor-Mimic"", exploits the use of deep reinforcement learning and model compression techniques to train a single policy network that learns how to act in a set of distinct tasks by using the guidance of several expert teachers. We then show that the representations learnt by the deep policy network are capable of generalizing to new tasks with no prior expert guidance, speeding up learning in novel environments. Although our method can in general be applied to a wide range of problems, we use Atari games as a testing environment to demonstrate these methods.",0
"Being able to perform in different contexts and apply past experiences to new situations is a crucial feature of intelligent agents. To achieve this, we introduce a new approach to multitask and transfer learning, which allows an autonomous agent to learn how to accomplish multiple tasks simultaneously and then utilize this knowledge in new domains. This approach, called ""Actor-Mimic,"" employs deep reinforcement learning and model compression techniques to train a single policy network. This network learns how to complete a set of diverse tasks by utilizing the guidance of several expert teachers. We also demonstrate that the deep policy network's learned representations are capable of generalizing to new tasks without any prior guidance from experts, thus expediting learning in novel environments. Though our method is applicable to a broad range of problems, we use Atari games as a testing ground to showcase our approach.",1
"Model predictive control (MPC) is an effective method for controlling robotic systems, particularly autonomous aerial vehicles such as quadcopters. However, application of MPC can be computationally demanding, and typically requires estimating the state of the system, which can be challenging in complex, unstructured environments. Reinforcement learning can in principle forego the need for explicit state estimation and acquire a policy that directly maps sensor readings to actions, but is difficult to apply to unstable systems that are liable to fail catastrophically during training before an effective policy has been found. We propose to combine MPC with reinforcement learning in the framework of guided policy search, where MPC is used to generate data at training time, under full state observations provided by an instrumented training environment. This data is used to train a deep neural network policy, which is allowed to access only the raw observations from the vehicle's onboard sensors. After training, the neural network policy can successfully control the robot without knowledge of the full state, and at a fraction of the computational cost of MPC. We evaluate our method by learning obstacle avoidance policies for a simulated quadrotor, using simulated onboard sensors and no explicit state estimation at test time.",0
"MPC is an effective approach to control autonomous aerial vehicles like quadcopters. However, it requires estimating the state of the system, which can be challenging in complex environments and is computationally demanding. Reinforcement learning is an alternative that acquires a policy mapping sensor readings to actions without explicit state estimation, but it is difficult to apply to unstable systems that may fail catastrophically during training. To overcome this, we propose combining MPC with reinforcement learning in guided policy search. MPC generates data under full state observation, which is used to train a deep neural network policy. The policy accesses only raw sensor observations and can control the robot without knowledge of full state and at a lower computational cost than MPC. We evaluate our approach by training obstacle avoidance policies for a simulated quadrotor using onboard sensors and no explicit state estimation at test time.",1
"We propose randomized least-squares value iteration (RLSVI) -- a new reinforcement learning algorithm designed to explore and generalize efficiently via linearly parameterized value functions. We explain why versions of least-squares value iteration that use Boltzmann or epsilon-greedy exploration can be highly inefficient, and we present computational results that demonstrate dramatic efficiency gains enjoyed by RLSVI. Further, we establish an upper bound on the expected regret of RLSVI that demonstrates near-optimality in a tabula rasa learning context. More broadly, our results suggest that randomized value functions offer a promising approach to tackling a critical challenge in reinforcement learning: synthesizing efficient exploration and effective generalization.",0
"The introduction of randomized least-squares value iteration (RLSVI) presents a new reinforcement learning algorithm that uses linearly parameterized value functions to explore and generalize effectively. The inefficiencies of utilizing Boltzmann or epsilon-greedy exploration in versions of least-squares value iteration are discussed, and computational results demonstrate the significant efficiency gains achieved by RLSVI. Additionally, an upper bound on the expected regret of RLSVI is established, indicating near-optimality in a tabula rasa learning context. Overall, these findings suggest that randomized value functions may be a promising approach for addressing the challenge of synthesizing efficient exploration and effective generalization in reinforcement learning.",1
"We present a data-efficient reinforcement learning algorithm resistant to observation noise. Our method extends the highly data-efficient PILCO algorithm (Deisenroth & Rasmussen, 2011) into partially observed Markov decision processes (POMDPs) by considering the filtering process during policy evaluation. PILCO conducts policy search, evaluating each policy by first predicting an analytic distribution of possible system trajectories. We additionally predict trajectories w.r.t. a filtering process, achieving significantly higher performance than combining a filter with a policy optimised by the original (unfiltered) framework. Our test setup is the cartpole swing-up task with sensor noise, which involves nonlinear dynamics and requires nonlinear control.",0
"An algorithm that is resistant to observation noise and efficient in data usage is presented. This method is an extension of the PILCO algorithm (Deisenroth & Rasmussen, 2011), which is highly data-efficient, to partially observed Markov decision processes (POMDPs) by incorporating the filtering process during policy evaluation. The PILCO algorithm evaluates policies by predicting an analytic distribution of potential system trajectories. However, in our method, we predict trajectories through a filtering process, resulting in significantly better performance compared to combining a filter with a policy optimized by the original (unfiltered) framework. To test the effectiveness of our algorithm, we use the cartpole swing-up task with sensor noise, which involves nonlinear dynamics and requires nonlinear control.",1
"We propose a general framework for sequential and dynamic acquisition of useful information in order to solve a particular task. While our goal could in principle be tackled by general reinforcement learning, our particular setting is constrained enough to allow more efficient algorithms. In this paper, we work under the Learning to Search framework and show how to formulate the goal of finding a dynamic information acquisition policy in that framework. We apply our formulation on two tasks, sentiment analysis and image recognition, and show that the learned policies exhibit good statistical performance. As an emergent byproduct, the learned policies show a tendency to focus on the most prominent parts of each instance and give harder instances more attention without explicitly being trained to do so.",0
"Our proposal presents a broad approach to obtaining valuable information in a sequential and dynamic manner to solve a specific task. Although the objective can be achieved through general reinforcement learning, our specific situation is restricted enough to accommodate more efficient algorithms. In this article, we utilize the Learning to Search framework to establish a dynamic information acquisition policy. We demonstrate the effectiveness of our approach on two tasks, namely image recognition and sentiment analysis, by showcasing our learned policies' excellent statistical performance. Additionally, our policies exhibit a proclivity for focusing on the most prominent aspects of each instance and allocating more attention to challenging instances without explicit instruction.",1
"Using deep neural nets as function approximator for reinforcement learning tasks have recently been shown to be very powerful for solving problems approaching real-world complexity. Using these results as a benchmark, we discuss the role that the discount factor may play in the quality of the learning process of a deep Q-network (DQN). When the discount factor progressively increases up to its final value, we empirically show that it is possible to significantly reduce the number of learning steps. When used in conjunction with a varying learning rate, we empirically show that it outperforms original DQN on several experiments. We relate this phenomenon with the instabilities of neural networks when they are used in an approximate Dynamic Programming setting. We also describe the possibility to fall within a local optimum during the learning process, thus connecting our discussion with the exploration/exploitation dilemma.",0
"Recent studies have demonstrated the impressive capabilities of deep neural nets as function approximators for reinforcement learning tasks with real-world complexity. In light of these findings, we examine the impact of the discount factor on the effectiveness of the deep Q-network (DQN) learning process. By gradually increasing the discount factor to its maximum value, we observe a significant reduction in the number of required learning steps. When paired with a varying learning rate, this approach surpasses the original DQN in multiple experiments. We attribute this phenomenon to the instability of neural networks in approximate Dynamic Programming scenarios and the risk of encountering local optima during the learning process, thereby linking our discussion to the exploration/exploitation dilemma.",1
"The Neural Turing Machine (NTM) is more expressive than all previously considered models because of its external memory. It can be viewed as a broader effort to use abstract external Interfaces and to learn a parametric model that interacts with them.   The capabilities of a model can be extended by providing it with proper Interfaces that interact with the world. These external Interfaces include memory, a database, a search engine, or a piece of software such as a theorem verifier. Some of these Interfaces are provided by the developers of the model. However, many important existing Interfaces, such as databases and search engines, are discrete.   We examine feasibility of learning models to interact with discrete Interfaces. We investigate the following discrete Interfaces: a memory Tape, an input Tape, and an output Tape. We use a Reinforcement Learning algorithm to train a neural network that interacts with such Interfaces to solve simple algorithmic tasks. Our Interfaces are expressive enough to make our model Turing complete.",0
"The Neural Turing Machine (NTM) surpasses previously considered models due to its external memory, making it more versatile. It aims to develop a broader approach by employing abstract external Interfaces and learning a parametric model that interacts with them. The model's capabilities can be expanded by providing suitable Interfaces that interact with the environment, including memory, databases, search engines, and software like a theorem verifier. While some Interfaces are provided by the model's developers, other essential existing Interfaces like databases and search engines are discrete. Therefore, we explore the possibility of teaching models to interact with discrete Interfaces, such as memory Tape, input Tape, and output Tape. To solve simple algorithmic tasks, we train a neural network using a Reinforcement Learning algorithm that interacts with such Interfaces. Our Interfaces are expressive enough to make the model Turing complete.",1
"Deep learning has become the state-of-art tool in many applications, but the evaluation and training of deep models can be time-consuming and computationally expensive. The conditional computation approach has been proposed to tackle this problem (Bengio et al., 2013; Davis & Arel, 2013). It operates by selectively activating only parts of the network at a time. In this paper, we use reinforcement learning as a tool to optimize conditional computation policies. More specifically, we cast the problem of learning activation-dependent policies for dropping out blocks of units as a reinforcement learning problem. We propose a learning scheme motivated by computation speed, capturing the idea of wanting to have parsimonious activations while maintaining prediction accuracy. We apply a policy gradient algorithm for learning policies that optimize this loss function and propose a regularization mechanism that encourages diversification of the dropout policy. We present encouraging empirical results showing that this approach improves the speed of computation without impacting the quality of the approximation.",0
"Although deep learning has become a leading tool in various applications, the process of evaluating and training deep models can be both time-consuming and computationally expensive. To combat this issue, researchers have suggested implementing a conditional computation approach (Bengio et al., 2013; Davis & Arel, 2013) that activates only certain parts of the network at a time. In this study, we utilize reinforcement learning to optimize conditional computation policies. Our goal is to learn activation-dependent policies for dropping out blocks of units, which we approach as a reinforcement learning problem. We propose a learning scheme that prioritizes computation speed while maintaining prediction accuracy. To achieve this, we employ a policy gradient algorithm and introduce a regularization mechanism to encourage diversification of the dropout policy. Our empirical results indicate that this approach significantly improves computation speed without impacting the quality of the approximation.",1
"Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance. In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient. Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy. We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent.",0
"Deep reinforcement learning has successfully learned policies for complex visual tasks through the use of deep Q-networks (DQN). However, achieving good performance requires relatively large task-specific networks and extensive training. This study introduces a new technique called policy distillation, which extracts the policy of a reinforcement learning agent and trains a new, more efficient network that performs at an expert level. The same method can also consolidate multiple task-specific policies into a single policy. The Atari domain was used to demonstrate the efficacy of this approach, showing that the multi-task distilled agent outperforms both single-task teachers and a jointly-trained DQN agent.",1
"This paper introduces MazeBase: an environment for simple 2D games, designed as a sandbox for machine learning approaches to reasoning and planning. Within it, we create 10 simple games embodying a range of algorithmic tasks (e.g. if-then statements or set negation). A variety of neural models (fully connected, convolutional network, memory network) are deployed via reinforcement learning on these games, with and without a procedurally generated curriculum. Despite the tasks' simplicity, the performance of the models is far from optimal, suggesting directions for future development. We also demonstrate the versatility of MazeBase by using it to emulate small combat scenarios from StarCraft. Models trained on the MazeBase version can be directly applied to StarCraft, where they consistently beat the in-game AI.",0
"MazeBase is a platform for creating uncomplicated 2D games that can be used as a testing ground for machine learning approaches to planning and reasoning. The platform includes 10 games that perform various algorithmic tasks, such as if-then statements or set negation. Reinforcement learning is used to train a range of neural models, including fully connected, convolutional network, and memory network, with or without procedurally generated curricula. Despite the games' simplicity, the models' performance is not optimal, indicating potential areas for future improvement. Additionally, MazeBase is adaptable and can be used to simulate minor combat scenarios from StarCraft, with models trained on the MazeBase version outperforming the in-game AI consistently.",1
"In a variety of problems originating in supervised, unsupervised, and reinforcement learning, the loss function is defined by an expectation over a collection of random variables, which might be part of a probabilistic model or the external world. Estimating the gradient of this loss function, using samples, lies at the core of gradient-based learning algorithms for these problems. We introduce the formalism of stochastic computation graphs---directed acyclic graphs that include both deterministic functions and conditional probability distributions---and describe how to easily and automatically derive an unbiased estimator of the loss function's gradient. The resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm. The generic scheme we propose unifies estimators derived in variety of prior work, along with variance-reduction techniques therein. It could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations, enabling, for example, attention, memory, and control actions.",0
"The loss function in supervised, unsupervised, and reinforcement learning problems is often defined by an expectation over random variables. These variables can be part of a probabilistic model or the external world. The gradient of the loss function is crucial for gradient-based learning algorithms, which rely on samples. To estimate this gradient, we introduce stochastic computation graphs. These graphs are directed acyclic graphs that encompass both deterministic functions and conditional probability distributions. With this formalism, we can easily and automatically derive an unbiased estimator for the loss function's gradient. Our algorithm for computing this gradient estimator is a simple modification of the standard backpropagation algorithm. Our proposed scheme unifies estimators from prior work and includes variance-reduction techniques. It can assist researchers in developing complex models that involve a combination of stochastic and deterministic operations. This could enable attention, memory, and control actions.",1
"We consider a reinforcement learning framework where agents have to navigate from start states to goal states. We prove convergence of a cycle-detection learning algorithm on a class of tasks that we call reducible. Reducible tasks have an acyclic solution. We also syntactically characterize the form of the final policy. This characterization can be used to precisely detect the convergence point in a simulation. Our result demonstrates that even simple algorithms can be successful in learning a large class of nontrivial tasks. In addition, our framework is elementary in the sense that we only use basic concepts to formally prove convergence.",0
"In our reinforcement learning framework, agents are required to move from initial states to target states. Our research proves the cycle-detection learning algorithm's convergence on a set of tasks that we refer to as reducible. These tasks have a solution that is acyclic. Furthermore, we provide a syntactic characterization of the final policy, which can be utilized to identify the point of convergence in a simulation. Our findings indicate that basic algorithms can effectively learn a wide range of complex tasks. Additionally, our framework is straightforward as we rely on fundamental concepts to formally establish convergence.",1
"Motivated by vision-based reinforcement learning (RL) problems, in particular Atari games from the recent benchmark Aracade Learning Environment (ALE), we consider spatio-temporal prediction problems where future (image-)frames are dependent on control variables or actions as well as previous frames. While not composed of natural scenes, frames in Atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability. We propose and evaluate two deep neural network architectures that consist of encoding, action-conditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks. Experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games. To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs.",0
"Our focus is on spatio-temporal prediction problems that arise in vision-based reinforcement learning (RL), specifically in Atari games from the Aracade Learning Environment (ALE) benchmark. Frames in these games are high-dimensional, containing multiple objects that are controlled by actions directly or indirectly. They can also involve deep partial observability, entry and departure of objects, and tens of objects in a single frame. We propose two deep neural network architectures that use convolutional and recurrent neural networks for encoding, action-conditional transformation, and decoding layers. Our experimental results demonstrate that these architectures can generate visually-realistic frames and control approximately 100-step action-conditional futures in some games. Our paper is the first to evaluate long-term predictions on high-dimensional video conditioned by control inputs.",1
"Bounded rationality, that is, decision-making and planning under resource limitations, is widely regarded as an important open problem in artificial intelligence, reinforcement learning, computational neuroscience and economics. This paper offers a consolidated presentation of a theory of bounded rationality based on information-theoretic ideas. We provide a conceptual justification for using the free energy functional as the objective function for characterizing bounded-rational decisions. This functional possesses three crucial properties: it controls the size of the solution space; it has Monte Carlo planners that are exact, yet bypass the need for exhaustive search; and it captures model uncertainty arising from lack of evidence or from interacting with other agents having unknown intentions. We discuss the single-step decision-making case, and show how to extend it to sequential decisions using equivalence transformations. This extension yields a very general class of decision problems that encompass classical decision rules (e.g. EXPECTIMAX and MINIMAX) as limit cases, as well as trust- and risk-sensitive planning.",0
"The issue of bounded rationality, which refers to decision-making and planning with limited resources, is a significant unresolved matter in various fields such as artificial intelligence, reinforcement learning, computational neuroscience, and economics. This article presents an integrated theory of bounded rationality by utilizing information-theoretic concepts. We justify the use of the free energy functional as the objective function for bounded-rational decisions and explain its three crucial properties: controlling the solution space size, exact Monte Carlo planners that avoid exhaustive search, and accounting for model uncertainty due to lack of evidence or interacting with unknown agents. We examine the case of single-step decision-making and illustrate how it can be extended to sequential decisions using equivalence transformations. This extension encompasses a broad range of decision problems, including classical decision rules like EXPECTIMAX and MINIMAX, as well as trust- and risk-sensitive planning.",1
"Partially observed control problems are a challenging aspect of reinforcement learning. We extend two related, model-free algorithms for continuous control -- deterministic policy gradient and stochastic value gradient -- to solve partially observed domains using recurrent neural networks trained with backpropagation through time.   We demonstrate that this approach, coupled with long-short term memory is able to solve a variety of physical control problems exhibiting an assortment of memory requirements. These include the short-term integration of information from noisy sensors and the identification of system parameters, as well as long-term memory problems that require preserving information over many time steps. We also demonstrate success on a combined exploration and memory problem in the form of a simplified version of the well-known Morris water maze task. Finally, we show that our approach can deal with high-dimensional observations by learning directly from pixels.   We find that recurrent deterministic and stochastic policies are able to learn similarly good solutions to these tasks, including the water maze where the agent must learn effective search strategies.",0
"Reinforcement learning faces challenges when dealing with partially observed control problems. To address this issue, we have extended two model-free algorithms, namely deterministic policy gradient and stochastic value gradient, for continuous control. Our solution involves using recurrent neural networks trained with backpropagation through time, with long-short term memory to solve various physical control problems that demand different memory requirements. These problems include short-term information integration from noisy sensors, identification of system parameters, and long-term memory preservation. Additionally, we were successful in solving a combined exploration and memory problem that mimics the Morris water maze task. Our approach can also handle high-dimensional observations by learning directly from pixels. We discovered that recurrent deterministic and stochastic policies can both learn efficient solutions for these tasks, including the water maze where the agent needs to learn effective search strategies.",1
"The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.",0
"Previously, it was unclear whether the popular Q-learning algorithm's tendency to overestimate action values was a common issue, detrimental to performance, and preventable in practice. This paper provides affirmative answers to all of these questions. Specifically, the study finds that the DQN algorithm, which utilizes Q-learning with a deep neural network, experiences significant overestimations in certain Atari 2600 games. To address this issue, the study proposes adapting the Double Q-learning algorithm, originally designed for tabular settings, to function with large-scale function approximation. The resulting algorithm reduces overestimations and improves performance in several games.",1
"A deep learning approach to reinforcement learning led to a general learner able to train on visual input to play a variety of arcade games at the human and superhuman levels. Its creators at the Google DeepMind's team called the approach: Deep Q-Network (DQN). We present an extension of DQN by ""soft"" and ""hard"" attention mechanisms. Tests of the proposed Deep Attention Recurrent Q-Network (DARQN) algorithm on multiple Atari 2600 games show level of performance superior to that of DQN. Moreover, built-in attention mechanisms allow a direct online monitoring of the training process by highlighting the regions of the game screen the agent is focusing on when making decisions.",0
"The Google DeepMind team developed the Deep Q-Network (DQN) approach, using deep learning for reinforcement learning. This approach resulted in a general learner with the ability to play various arcade games at both human and superhuman levels through visual input. The team has now extended DQN with ""soft"" and ""hard"" attention mechanisms, creating the Deep Attention Recurrent Q-Network (DARQN) algorithm. Tests performed on multiple Atari 2600 games have shown that DARQN outperforms DQN in terms of performance. Additionally, the built-in attention mechanisms enable direct monitoring of the training process, as they highlight the regions of the game screen that the agent focuses on when making decisions.",1
"Electric water heaters have the ability to store energy in their water buffer without impacting the comfort of the end user. This feature makes them a prime candidate for residential demand response. However, the stochastic and nonlinear dynamics of electric water heaters, makes it challenging to harness their flexibility. Driven by this challenge, this paper formulates the underlying sequential decision-making problem as a Markov decision process and uses techniques from reinforcement learning. Specifically, we apply an auto-encoder network to find a compact feature representation of the sensor measurements, which helps to mitigate the curse of dimensionality. A wellknown batch reinforcement learning technique, fitted Q-iteration, is used to find a control policy, given this feature representation. In a simulation-based experiment using an electric water heater with 50 temperature sensors, the proposed method was able to achieve good policies much faster than when using the full state information. In a lab experiment, we apply fitted Q-iteration to an electric water heater with eight temperature sensors. Further reducing the state vector did not improve the results of fitted Q-iteration. The results of the lab experiment, spanning 40 days, indicate that compared to a thermostat controller, the presented approach was able to reduce the total cost of energy consumption of the electric water heater by 15%.",0
"Electric water heaters are a suitable option for residential demand response due to their ability to store energy without affecting the user's comfort. However, their stochastic and nonlinear dynamics pose a challenge in utilizing their flexibility effectively. To address this issue, a Markov decision process is formulated in this paper, and reinforcement learning techniques are employed. An auto-encoder network is used to obtain a compact feature representation of the sensor measurements, while fitted Q-iteration is applied to find a control policy based on this representation. Simulation experiments demonstrate that the proposed method achieves good policies faster than using the complete state information, while lab experiments show that further reducing the state vector does not improve the results. Over a 40-day period, the presented approach reduces the total energy consumption cost of the electric water heater by 15% compared to a thermostat controller.",1
"Successful applications of reinforcement learning in real-world problems often require dealing with partially observable states. It is in general very challenging to construct and infer hidden states as they often depend on the agent's entire interaction history and may require substantial domain knowledge. In this work, we investigate a deep-learning approach to learning the representation of states in partially observable tasks, with minimal prior knowledge of the domain. In particular, we propose a new family of hybrid models that combines the strength of both supervised learning (SL) and reinforcement learning (RL), trained in a joint fashion: The SL component can be a recurrent neural networks (RNN) or its long short-term memory (LSTM) version, which is equipped with the desired property of being able to capture long-term dependency on history, thus providing an effective way of learning the representation of hidden states. The RL component is a deep Q-network (DQN) that learns to optimize the control for maximizing long-term rewards. Extensive experiments in a direct mailing campaign problem demonstrate the effectiveness and advantages of the proposed approach, which performs the best among a set of previous state-of-the-art methods.",0
"Dealing with partially observable states is often necessary for successful applications of reinforcement learning in real-world problems. However, constructing and inferring hidden states can be very challenging, as they may depend on the agent's entire interaction history and require substantial domain knowledge. This study explores a deep-learning approach to learning state representation in partially observable tasks with minimal prior domain knowledge. The proposed method combines supervised learning and reinforcement learning in a joint fashion, using a hybrid model that includes a recurrent neural network or its long short-term memory version to capture long-term dependencies and a deep Q-network to optimize control for maximizing long-term rewards. Experimentation on a direct mailing campaign problem shows that the proposed approach outperforms previous state-of-the-art methods.",1
"We present an active detection model for localizing objects in scenes. The model is class-specific and allows an agent to focus attention on candidate regions for identifying the correct location of a target object. This agent learns to deform a bounding box using simple transformation actions, with the goal of determining the most specific location of target objects following top-down reasoning. The proposed localization agent is trained using deep reinforcement learning, and evaluated on the Pascal VOC 2007 dataset. We show that agents guided by the proposed model are able to localize a single instance of an object after analyzing only between 11 and 25 regions in an image, and obtain the best detection results among systems that do not use object proposals for object localization.",0
"A model for detecting and localizing objects in scenes is introduced. The model is specific to certain classes and enables an agent to concentrate on potential areas to pinpoint the exact location of a target object. The agent is taught to adjust a bounding box by executing uncomplicated transformation actions to determine the most precise location of target objects through top-down reasoning. Deep reinforcement learning is used to train the localization agent, which is then tested on the Pascal VOC 2007 dataset. Results indicate that agents guided by the proposed model can locate a single instance of an object after analyzing only 11 to 25 regions of an image, and provide the best detection outcomes among systems that do not use object proposals for object localization.",1
"This paper introduces a machine learning based system for controlling a robotic manipulator with visual perception only. The capability to autonomously learn robot controllers solely from raw-pixel images and without any prior knowledge of configuration is shown for the first time. We build upon the success of recent deep reinforcement learning and develop a system for learning target reaching with a three-joint robot manipulator using external visual observation. A Deep Q Network (DQN) was demonstrated to perform target reaching after training in simulation. Transferring the network to real hardware and real observation in a naive approach failed, but experiments show that the network works when replacing camera images with synthetic images.",0
"In this paper, a novel system based on machine learning is presented for controlling a robotic manipulator using visual perception exclusively. The system demonstrates the ability to learn robot controllers independently, solely from raw-pixel images without prior configuration knowledge. Building upon the success of deep reinforcement learning, the system is designed to learn target reaching with a three-joint robot manipulator using external visual observation. The Deep Q Network (DQN) successfully achieved target reaching after training in simulation. However, attempts to transfer the network to real hardware and observation proved unsuccessful. Subsequent experiments revealed that the network performed effectively when synthetic images were substituted for camera images.",1
"We propose a distributed deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is based on the deep Q-network, a convolutional neural network trained with a variant of Q-learning. Its input is raw pixels and its output is a value function estimating future rewards from taking an action given a system state. To distribute the deep Q-network training, we adapt the DistBelief software framework to the context of efficiently training reinforcement learning agents. As a result, the method is completely asynchronous and scales well with the number of machines. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to achieve reasonable success on a simple game with minimal parameter tuning.",0
"Our proposal involves utilizing a distributed deep learning model that can effectively learn control policies directly from high-dimensional sensory input through reinforcement learning. This model is built on the deep Q-network, which is a convolutional neural network that is trained using a variant of Q-learning. The network takes in raw pixels as its input and outputs a value function that estimates future rewards based on the system state and the action taken. To enable the deep Q-network training to be distributed, we have adapted the DistBelief software framework, which allows for efficient training of reinforcement learning agents. Consequently, the method is completely asynchronous and can scale well with the number of machines employed. We have demonstrated that the deep Q-network agent, with only pixels and the game score as inputs, was able to achieve reasonable success in a simple game with minimal parameter tuning.",1
"The mutual information is a core statistical quantity that has applications in all areas of machine learning, whether this is in training of density models over multiple data modalities, in maximising the efficiency of noisy transmission channels, or when learning behaviour policies for exploration by artificial agents. Most learning algorithms that involve optimisation of the mutual information rely on the Blahut-Arimoto algorithm --- an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications. This paper provides a new approach for scalable optimisation of the mutual information by merging techniques from variational inference and deep learning. We develop our approach by focusing on the problem of intrinsically-motivated learning, where the mutual information forms the definition of a well-known internal drive known as empowerment. Using a variational lower bound on the mutual information, combined with convolutional networks for handling visual input streams, we develop a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions.",0
"The mutual information is a fundamental statistical measure that has a wide range of uses in machine learning, including training density models across multiple data types, enhancing the efficiency of noisy transmission channels, and teaching artificial agents to explore. However, the majority of learning algorithms that optimize mutual information rely on the Blahut-Arimoto algorithm, which is an enumerative algorithm with exponential complexity that is not suitable for contemporary machine learning applications. This study introduces a new approach that combines techniques from variational inference and deep learning to efficiently optimize mutual information. We focus on intrinsically-motivated learning, where mutual information is integral to a well-known internal drive called empowerment. By utilizing a variational lower bound on the mutual information along with convolutional networks to manage visual input streams, we develop a stochastic optimization algorithm that permits scalable information maximization and empowerment-based reasoning directly from pixels to actions.",1
"Empowerment quantifies the influence an agent has on its environment. This is formally achieved by the maximum of the expected KL-divergence between the distribution of the successor state conditioned on a specific action and a distribution where the actions are marginalised out. This is a natural candidate for an intrinsic reward signal in the context of reinforcement learning: the agent will place itself in a situation where its action have maximum stability and maximum influence on the future. The limiting factor so far has been the computational complexity of the method: the only way of calculation has so far been a brute force algorithm, reducing the applicability of the method to environments with a small set discrete states. In this work, we propose to use an efficient approximation for marginalising out the actions in the case of continuous environments. This allows fast evaluation of empowerment, paving the way towards challenging environments such as real world robotics. The method is presented on a pendulum swing up problem.",0
"The concept of empowerment measures an agent's ability to affect its surroundings. To do this, it calculates the highest expected KL-divergence between the distribution of future states based on a specific action, and a distribution where all actions are considered. This could serve as an inherent reward system in reinforcement learning, motivating the agent to seek situations where its actions have the most influence and stability. However, the method's usefulness has been limited by the need for a time-consuming brute force algorithm, which makes it impractical for environments with many discrete states. This study proposes an efficient approximation technique for continuous environments, making it possible to calculate empowerment more quickly and effectively, even in complex real-world robotics scenarios. To demonstrate this, the approach is applied to a pendulum swing up problem.",1
"Partial monitoring is a generic framework for sequential decision-making with incomplete feedback. It encompasses a wide class of problems such as dueling bandits, learning with expect advice, dynamic pricing, dark pools, and label efficient prediction. We study the utility-based dueling bandit problem as an instance of partial monitoring problem and prove that it fits the time-regret partial monitoring hierarchy as an easy - i.e. Theta (sqrt{T})- instance. We survey some partial monitoring algorithms and see how they could be used to solve dueling bandits efficiently. Keywords: Online learning, Dueling Bandits, Partial Monitoring, Partial Feedback, Multiarmed Bandits",0
"The concept of partial monitoring is a versatile approach to decision-making in scenarios where feedback is lacking. This encompasses a broad range of issues, including but not limited to dueling bandits, dynamic pricing, learning with expect advice, label efficient prediction, and dark pools. Our focus is on the dueling bandit problem, which we consider as an example of a partial monitoring problem. We demonstrate that it falls under the time-regret partial monitoring hierarchy, with a time complexity of Theta (sqrt{T}) as an easy instance. By examining various partial monitoring algorithms, we conclude that they can be effectively applied to solve the dueling bandit problem. Our study is centered on keywords such as Partial Monitoring, Online Learning, Multiarmed Bandits, Dueling Bandits, and Partial Feedback.",1
"Transferring knowledge across a sequence of related tasks is an important challenge in reinforcement learning (RL). Despite much encouraging empirical evidence, there has been little theoretical analysis. In this paper, we study a class of lifelong RL problems: the agent solves a sequence of tasks modeled as finite Markov decision processes (MDPs), each of which is from a finite set of MDPs with the same state/action sets and different transition/reward functions. Motivated by the need for cross-task exploration in lifelong learning, we formulate a novel online coupon-collector problem and give an optimal algorithm. This allows us to develop a new lifelong RL algorithm, whose overall sample complexity in a sequence of tasks is much smaller than single-task learning, even if the sequence of tasks is generated by an adversary. Benefits of the algorithm are demonstrated in simulated problems, including a recently introduced human-robot interaction problem.",0
"Reinforcement learning (RL) faces the challenge of transferring knowledge between related tasks, with limited theoretical analysis despite encouraging empirical evidence. In this paper, we examine lifelong RL problems, where the agent solves a sequence of finite Markov decision processes (MDPs) from a finite set of MDPs with the same state/action sets but different transition/reward functions. To enable cross-task exploration, we propose an online coupon-collector problem and an optimal algorithm. This leads to a new lifelong RL algorithm, which has a significantly lower sample complexity than single-task learning, even in the face of an adversary-generated task sequence. Simulated problems, including a human-robot interaction task, demonstrate the benefits of the algorithm.",1
"Gathering the most information by picking the least amount of data is a common task in experimental design or when exploring an unknown environment in reinforcement learning and robotics. A widely used measure for quantifying the information contained in some distribution of interest is its entropy. Greedily minimizing the expected entropy is therefore a standard method for choosing samples in order to gain strong beliefs about the underlying random variables. We show that this approach is prone to temporally getting stuck in local optima corresponding to wrongly biased beliefs. We suggest instead maximizing the expected cross entropy between old and new belief, which aims at challenging refutable beliefs and thereby avoids these local optima. We show that both criteria are closely related and that their difference can be traced back to the asymmetry of the Kullback-Leibler divergence. In illustrative examples as well as simulated and real-world experiments we demonstrate the advantage of cross entropy over simple entropy for practical applications.",0
"In experimental design and reinforcement learning, it is common to gather the most information with the least amount of data when exploring an unknown environment. To quantify the information in a distribution, entropy is widely used. However, minimizing the expected entropy can lead to biased beliefs and being stuck in local optima. Instead, we propose maximizing the expected cross entropy between old and new beliefs, which challenges refutable beliefs to avoid local optima. We demonstrate that the two criteria are closely related but differ due to the asymmetry of the Kullback-Leibler divergence. Through illustrative examples and experiments, we show that maximizing cross entropy is advantageous over simple entropy in practical applications.",1
"Objective: Anemia is a frequent comorbidity in hemodialysis patients that can be successfully treated by administering erythropoiesis-stimulating agents (ESAs). ESAs dosing is currently based on clinical protocols that often do not account for the high inter- and intra-individual variability in the patient's response. As a result, the hemoglobin level of some patients oscillates around the target range, which is associated with multiple risks and side-effects. This work proposes a methodology based on reinforcement learning (RL) to optimize ESA therapy.   Methods: RL is a data-driven approach for solving sequential decision-making problems that are formulated as Markov decision processes (MDPs). Computing optimal drug administration strategies for chronic diseases is a sequential decision-making problem in which the goal is to find the best sequence of drug doses. MDPs are particularly suitable for modeling these problems due to their ability to capture the uncertainty associated with the outcome of the treatment and the stochastic nature of the underlying process. The RL algorithm employed in the proposed methodology is fitted Q iteration, which stands out for its ability to make an efficient use of data.   Results: The experiments reported here are based on a computational model that describes the effect of ESAs on the hemoglobin level. The performance of the proposed method is evaluated and compared with the well-known Q-learning algorithm and with a standard protocol. Simulation results show that the performance of Q-learning is substantially lower than FQI and the protocol.   Conclusion: Although prospective validation is required, promising results demonstrate the potential of RL to become an alternative to current protocols.",0
"The aim of this study is to improve the treatment of anemia in hemodialysis patients using erythropoiesis-stimulating agents (ESAs). Current clinical protocols for ESA dosing do not consider the significant variability in patient response, leading to fluctuating hemoglobin levels and associated risks and side-effects. In this work, a reinforcement learning (RL) approach based on Markov decision processes (MDPs) is proposed to optimize ESA therapy. The RL algorithm used is fitted Q iteration, which efficiently utilizes data. Simulation results comparing to the Q-learning algorithm and a standard protocol demonstrate that the proposed method outperforms both. While further validation is necessary, these findings suggest that RL has potential as an alternative to current protocols.",1
"This paper proposes GProp, a deep reinforcement learning algorithm for continuous policies with compatible function approximation. The algorithm is based on two innovations. Firstly, we present a temporal-difference based method for learning the gradient of the value-function. Secondly, we present the deviator-actor-critic (DAC) model, which comprises three neural networks that estimate the value function, its gradient, and determine the actor's policy respectively. We evaluate GProp on two challenging tasks: a contextual bandit problem constructed from nonparametric regression datasets that is designed to probe the ability of reinforcement learning algorithms to accurately estimate gradients; and the octopus arm, a challenging reinforcement learning benchmark. GProp is competitive with fully supervised methods on the bandit task and achieves the best performance to date on the octopus arm.",0
"In this paper, a new algorithm called GProp is proposed for continuous policies with compatible function approximation. The algorithm incorporates two innovative components. Firstly, a temporal-difference based method is introduced for learning the gradient of the value-function. Secondly, the deviator-actor-critic (DAC) model is presented, which consists of three neural networks that estimate the value function, its gradient, and determine the actor's policy respectively. To evaluate GProp, two challenging tasks are considered: a contextual bandit problem constructed from nonparametric regression datasets and the octopus arm, a reinforcement learning benchmark. GProp's performance is compared to fully supervised methods on the bandit task and achieves the best performance to date on the octopus arm.",1
"We consider how to learn multi-step predictions efficiently. Conventional algorithms wait until observing actual outcomes before performing the computations to update their predictions. If predictions are made at a high rate or span over a large amount of time, substantial computation can be required to store all relevant observations and to update all predictions when the outcome is finally observed. We show that the exact same predictions can be learned in a much more computationally congenial way, with uniform per-step computation that does not depend on the span of the predictions. We apply this idea to various settings of increasing generality, repeatedly adding desired properties and each time deriving an equivalent span-independent algorithm for the conventional algorithm that satisfies these desiderata. Interestingly, along the way several known algorithmic constructs emerge spontaneously from our derivations, including dutch eligibility traces, temporal difference errors, and averaging. This allows us to link these constructs one-to-one to the corresponding desiderata, unambiguously connecting the `how' to the `why'. Each step, we make sure that the derived algorithm subsumes the previous algorithms, thereby retaining their properties. Ultimately we arrive at a single general temporal-difference algorithm that is applicable to the full setting of reinforcement learning.",0
"Our focus is on efficient learning of multi-step predictions. Traditionally, algorithms wait for actual outcomes before updating their predictions, requiring significant computation if the predictions are frequent or span a long time period. However, we demonstrate that the same predictions can be learned with uniform per-step computation that doesn't depend on the prediction span. We extend this idea to various settings, progressively adding desired features and deriving equivalent span-independent algorithms for conventional algorithms that meet these requirements. We observe that certain established algorithmic constructs, such as dutch eligibility traces, temporal difference errors, and averaging, arise naturally during our derivations and are linked to specific desiderata. At each stage, we ensure that the new algorithm encompasses the previous ones while retaining their properties. Ultimately, we arrive at a general temporal-difference algorithm that is suitable for all reinforcement learning scenarios.",1
"Statistical spoken dialogue systems have the attractive property of being able to be optimised from data via interactions with real users. However in the reinforcement learning paradigm the dialogue manager (agent) often requires significant time to explore the state-action space to learn to behave in a desirable manner. This is a critical issue when the system is trained on-line with real users where learning costs are expensive. Reward shaping is one promising technique for addressing these concerns. Here we examine three recurrent neural network (RNN) approaches for providing reward shaping information in addition to the primary (task-orientated) environmental feedback. These RNNs are trained on returns from dialogues generated by a simulated user and attempt to diffuse the overall evaluation of the dialogue back down to the turn level to guide the agent towards good behaviour faster. In both simulated and real user scenarios these RNNs are shown to increase policy learning speed. Importantly, they do not require prior knowledge of the user's goal.",0
"Spoken dialogue systems that use statistical methods have the advantage of being optimized through interactions with real users. However, in the reinforcement learning approach, the dialogue manager may take a considerable amount of time to explore the state-action space to learn how to behave in a desirable manner. This is a significant problem when the system is trained online with real users because learning costs are expensive. To address this issue, reward shaping is a promising technique. In this study, we explore three approaches to recurrent neural networks (RNNs) for providing reward shaping information, in addition to primary task-oriented environmental feedback. These RNNs are trained on returns from dialogues generated by a simulated user and aim to diffuse the overall evaluation of the dialogue down to the turn level. The goal is to guide the agent towards good behavior more quickly. Results show that these RNNs increase policy learning speed in both simulated and real user scenarios. Importantly, there is no need for prior knowledge of the user's goal.",1
"To train a statistical spoken dialogue system (SDS) it is essential that an accurate method for measuring task success is available. To date training has relied on presenting a task to either simulated or paid users and inferring the dialogue's success by observing whether this presented task was achieved or not. Our aim however is to be able to learn from real users acting under their own volition, in which case it is non-trivial to rate the success as any prior knowledge of the task is simply unavailable. User feedback may be utilised but has been found to be inconsistent. Hence, here we present two neural network models that evaluate a sequence of turn-level features to rate the success of a dialogue. Importantly these models make no use of any prior knowledge of the user's task. The models are trained on dialogues generated by a simulated user and the best model is then used to train a policy on-line which is shown to perform at least as well as a baseline system using prior knowledge of the user's task. We note that the models should also be of interest for evaluating SDS and for monitoring a dialogue in rule-based SDS.",0
"Having an accurate way to measure task success is crucial when training a statistical spoken dialogue system (SDS). Thus far, training has relied on presenting a task to either simulated or paid users, and determining the success of the dialogue based on whether or not the task was achieved. However, we aim to learn from real users who act on their own volition, making it difficult to rate success without any prior knowledge of the task. Despite user feedback being available, it has proven to be inconsistent. To address this, we have developed two neural network models that evaluate turn-level features to rate the success of a dialogue without any prior knowledge of the user's task. These models were trained on dialogues generated by a simulated user, and the best model was used to train a policy online. The results showed that our approach performed as well as a baseline system that relied on prior knowledge of the user's task. Our models can also be used for evaluating SDS and monitoring rule-based SDS dialogues.",1
"Online decision tree learning algorithms typically examine all features of a new data point to update model parameters. We propose a novel alternative, Reinforcement Learning- based Decision Trees (RLDT), that uses Reinforcement Learning (RL) to actively examine a minimal number of features of a data point to classify it with high accuracy. Furthermore, RLDT optimizes a long term return, providing a better alternative to the traditional myopic greedy approach to growing decision trees. We demonstrate that this approach performs as well as batch learning algorithms and other online decision tree learning algorithms, while making significantly fewer queries about the features of the data points. We also show that RLDT can effectively handle concept drift.",0
"A new approach to decision tree learning is proposed in which Reinforcement Learning (RL) is utilized to actively examine only a limited number of features of a data point for high accuracy classification. This approach, called Reinforcement Learning-based Decision Trees (RLDT), differs from the traditional online decision tree learning algorithms that examine all features of a new data point to update model parameters. In addition, RLDT optimizes a long-term return, providing a superior alternative to the traditional myopic greedy approach to growing decision trees. The study shows that RLDT performs as well as batch learning algorithms and other online decision tree learning algorithms, while making significantly fewer queries about the features of the data points. The results also indicate that RLDT can effectively handle concept drift.",1
"We present the first massively distributed architecture for deep reinforcement learning. This architecture uses four main components: parallel actors that generate new behaviour; parallel learners that are trained from stored experience; a distributed neural network to represent the value function or behaviour policy; and a distributed store of experience. We used our architecture to implement the Deep Q-Network algorithm (DQN). Our distributed algorithm was applied to 49 games from Atari 2600 games from the Arcade Learning Environment, using identical hyperparameters. Our performance surpassed non-distributed DQN in 41 of the 49 games and also reduced the wall-time required to achieve these results by an order of magnitude on most games.",0
"The initial presentation is about a novel architecture for deep reinforcement learning that is massively distributed. The architecture comprises four main components: parallel actors for producing new behavior, parallel learners for training from saved experience, a distributed neural network for representing the value function or behavior policy, and a distributed experience storage. The authors applied the Deep Q-Network algorithm (DQN) to 49 games from the Atari 2600 games from the Arcade Learning Environment, using the same hyperparameters. The results indicate that the distributed algorithm outperforms the non-distributed DQN in 41 of the 49 games while also reducing the wall-time required to achieve these results by an order of magnitude on most games.",1
"Emphatic algorithms are temporal-difference learning algorithms that change their effective state distribution by selectively emphasizing and de-emphasizing their updates on different time steps. Recent works by Sutton, Mahmood and White (2015), and Yu (2015) show that by varying the emphasis in a particular way, these algorithms become stable and convergent under off-policy training with linear function approximation. This paper serves as a unified summary of the available results from both works. In addition, we demonstrate the empirical benefits from the flexibility of emphatic algorithms, including state-dependent discounting, state-dependent bootstrapping, and the user-specified allocation of function approximation resources.",0
"Temporal-difference learning algorithms, known as emphatic algorithms, can modify their effective state distribution by selectively prioritizing or de-prioritizing updates at different time steps. Recent research by Sutton, Mahmood, White (2015), and Yu (2015) demonstrates that adjusting the emphasis in a specific manner can result in stable and convergent algorithms when training off-policy with linear function approximation. This article presents a comprehensive overview of the findings from both studies and highlights the practical advantages of using emphatic algorithms, such as state-dependent discounting, bootstrapping, and the ability to allocate function approximation resources as desired.",1
"This technical note presents a new approach to carrying out the kind of exploration achieved by Thompson sampling, but without explicitly maintaining or sampling from posterior distributions. The approach is based on a bootstrap technique that uses a combination of observed and artificially generated data. The latter serves to induce a prior distribution which, as we will demonstrate, is critical to effective exploration. We explain how the approach can be applied to multi-armed bandit and reinforcement learning problems and how it relates to Thompson sampling. The approach is particularly well-suited for contexts in which exploration is coupled with deep learning, since in these settings, maintaining or generating samples from a posterior distribution becomes computationally infeasible.",0
"In this technical note, a novel method is introduced for conducting exploration similar to Thompson sampling, but without the need to maintain or sample from posterior distributions explicitly. This method involves a bootstrap technique that utilizes both observed and artificially generated data. The generated data establishes a prior distribution, which we show is crucial for effective exploration. The note discusses the application of this approach in multi-armed bandit and reinforcement learning scenarios and its relationship with Thompson sampling. This method is particularly advantageous in contexts where deep learning is used, as generating or maintaining posterior distribution samples becomes impractical.",1
"Data-efficient learning in continuous state-action spaces using very high-dimensional observations remains a key challenge in developing fully autonomous systems. In this paper, we consider one instance of this challenge, the pixels to torques problem, where an agent must learn a closed-loop control policy from pixel information only. We introduce a data-efficient, model-based reinforcement learning algorithm that learns such a closed-loop policy directly from pixel information. The key ingredient is a deep dynamical model that uses deep auto-encoders to learn a low-dimensional embedding of images jointly with a predictive model in this low-dimensional feature space. Joint learning ensures that not only static but also dynamic properties of the data are accounted for. This is crucial for long-term predictions, which lie at the core of the adaptive model predictive control strategy that we use for closed-loop control. Compared to state-of-the-art reinforcement learning methods for continuous states and actions, our approach learns quickly, scales to high-dimensional state spaces and is an important step toward fully autonomous learning from pixels to torques.",0
"Developing fully autonomous systems is a challenge that requires data-efficient learning in continuous state-action spaces using very high-dimensional observations. In this paper, we focus on the pixels to torques problem, which involves an agent learning a closed-loop control policy from pixel information alone. We introduce a model-based reinforcement learning algorithm that can efficiently learn such a policy directly from pixel information. The algorithm relies on a deep dynamical model that employs deep auto-encoders to learn a low-dimensional embedding of images and a predictive model in this feature space. By jointly learning static and dynamic properties of the data, our approach can make long-term predictions and implement an adaptive model predictive control strategy for closed-loop control. Our approach outperforms state-of-the-art reinforcement learning methods for continuous states and actions in terms of learning speed and scalability, and represents an important step towards fully autonomous learning from pixels to torques.",1
"Bayesian optimization has shown to be a fundamental global optimization algorithm in many applications: ranging from automatic machine learning, robotics, reinforcement learning, experimental design, simulations, etc. The most popular and effective Bayesian optimization relies on a surrogate model in the form of a Gaussian process due to its flexibility to represent a prior over function. However, many algorithms and setups relies on the stationarity assumption of the Gaussian process. In this paper, we present a novel nonstationary strategy for Bayesian optimization that is able to outperform the state of the art in Bayesian optimization both in stationary and nonstationary problems.",0
"Bayesian optimization has been established as a crucial global optimization technique in numerous fields such as automatic machine learning, robotics, reinforcement learning, experimental design, and simulations. The Gaussian process surrogate model is widely utilized in Bayesian optimization due to its ability to represent a prior over the function in a flexible manner. Nevertheless, several algorithms and setups depend on the Gaussian process's stationarity assumption. Our paper introduces a novel approach for Bayesian optimization that is nonstationary and can surpass existing state-of-the-art methods in both stationary and nonstationary problems.",1
"Reinforcement learning algorithms need to deal with the exponential growth of states and actions when exploring optimal control in high-dimensional spaces. This is known as the curse of dimensionality. By projecting the agent's state onto a low-dimensional manifold, we can represent the state space in a smaller and more efficient representation. By using this representation during learning, the agent can converge to a good policy much faster. We test this approach in the Mario Benchmarking Domain. When using dimensionality reduction in Mario, learning converges much faster to a good policy. But, there is a critical convergence-performance trade-off. By projecting onto a low-dimensional manifold, we are ignoring important data. In this paper, we explore this trade-off of convergence and performance. We find that learning in as few as 4 dimensions (instead of 9), we can improve performance past learning in the full dimensional space at a faster convergence rate.",0
"The challenge for reinforcement learning algorithms lies in managing the exponential growth of states and actions when seeking optimal control in high-dimensional spaces, commonly referred to as the curse of dimensionality. However, by projecting an agent's state onto a low-dimensional manifold, the state space can be represented in a more efficient and compact manner, leading to faster policy convergence. To test this approach, we applied it to the Mario Benchmarking Domain and observed that dimensionality reduction resulted in significantly faster policy convergence. However, there is a trade-off between convergence and performance as important data may be overlooked when projecting onto a low-dimensional manifold. In this study, we explore this trade-off and find that even learning in as few as 4 dimensions can surpass full-dimensional learning in terms of performance while converging at a faster rate.",1
"Lifelong reinforcement learning provides a promising framework for developing versatile agents that can accumulate knowledge over a lifetime of experience and rapidly learn new tasks by building upon prior knowledge. However, current lifelong learning methods exhibit non-vanishing regret as the amount of experience increases and include limitations that can lead to suboptimal or unsafe control policies. To address these issues, we develop a lifelong policy gradient learner that operates in an adversarial set- ting to learn multiple tasks online while enforcing safety constraints on the learned policies. We demonstrate, for the first time, sublinear regret for lifelong policy search, and validate our algorithm on several benchmark dynamical systems and an application to quadrotor control.",0
"The concept of lifelong reinforcement learning holds great promise in the creation of adaptable agents that can accumulate knowledge throughout their lifetime and quickly learn new tasks based on prior experience. However, current techniques for lifelong learning have their limitations, such as exhibiting non-vanishing regret as experience accumulates and potentially leading to suboptimal or unsafe control policies. Our solution to these issues comes in the form of a lifelong policy gradient learner that operates in an adversarial environment to learn multiple tasks while also enforcing safety constraints. This approach yields sublinear regret for lifelong policy search, as demonstrated through our validation on various dynamical systems and a quadrotor control application.",1
"We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.",0
"A new algorithm, Bayes by Backprop, has been introduced for learning a probability distribution on neural network weights. The algorithm is efficient and compatible with backpropagation, and it regularizes weights by minimizing a compression cost, which is either the variational free energy or the expected lower bound on the marginal likelihood. This regularization approach is principled and yields performance comparable to dropout on MNIST classification. Additionally, the algorithm demonstrates how learned uncertainty in the weights can improve generalization in non-linear regression problems and how it can be used to drive the exploration-exploitation trade-off in reinforcement learning.",1
"In order to speed-up classification models when facing a large number of categories, one usual approach consists in organizing the categories in a particular structure, this structure being then used as a way to speed-up the prediction computation. This is for example the case when using error-correcting codes or even hierarchies of categories. But in the majority of approaches, this structure is chosen \textit{by hand}, or during a preliminary step, and not integrated in the learning process. We propose a new model called Reinforced Decision Tree which simultaneously learns how to organize categories in a tree structure and how to classify any input based on this structure. This approach keeps the advantages of existing techniques (low inference complexity) but allows one to build efficient classifiers in one learning step. The learning algorithm is inspired by reinforcement learning and policy-gradient techniques which allows us to integrate the two steps (building the tree, and learning the classifier) in one single algorithm.",0
"To accelerate the classification of models when presented with a large number of categories, a common approach is to arrange the categories in a specific structure. This structure is then utilized to hasten the prediction computation. For instance, error-correcting codes or hierarchies of categories are utilized. However, most methods manually select this structure or do so during a preliminary stage, without integrating it into the learning process. We propose the Reinforced Decision Tree model that learns how to organize categories in a tree structure while simultaneously classifying any input based on this structure. This method retains the benefits of existing techniques such as low inference complexity while enabling the construction of efficient classifiers in a single learning step. The reinforcement learning and policy-gradient techniques inspire the learning algorithm, allowing the integration of building the tree and learning the classifier in one algorithm.",1
We present an attention-based model for recognizing multiple objects in images. The proposed model is a deep recurrent neural network trained with reinforcement learning to attend to the most relevant regions of the input image. We show that the model learns to both localize and recognize multiple objects despite being given only class labels during training. We evaluate the model on the challenging task of transcribing house number sequences from Google Street View images and show that it is both more accurate than the state-of-the-art convolutional networks and uses fewer parameters and less computation.,0
"Our study introduces a model based on attention to identify multiple objects in images. The model is a deep recurrent neural network that utilizes reinforcement learning to concentrate on the most significant areas of an input image. We demonstrate that the model can identify and locate multiple objects even when provided only with class labels during the training phase. We assess the model's performance on the complex task of transcribing house number sequences from Google Street View images. Our results indicate that the model surpasses the current state-of-the-art convolutional networks in accuracy, while utilizing fewer parameters and less computation.",1
"This paper describes a novel method to solve average-reward semi-Markov decision processes, by reducing them to a minimal sequence of cumulative reward problems. The usual solution methods for this type of problems update the gain (optimal average reward) immediately after observing the result of taking an action. The alternative introduced, optimal nudging, relies instead on setting the gain to some fixed value, which transitorily makes the problem a cumulative-reward task, solving it by any standard reinforcement learning method, and only then updating the gain in a way that minimizes uncertainty in a minmax sense. The rule for optimal gain update is derived by exploiting the geometric features of the w-l space, a simple mapping of the space of policies. The total number of cumulative reward tasks that need to be solved is shown to be small. Some experiments are presented to explore the features of the algorithm and to compare its performance with other approaches.",0
"This paper presents a fresh approach for tackling average-reward semi-Markov decision processes. The approach involves reducing the problem to a minimal cumulative reward sequence, as opposed to the usual method of updating the optimal average reward immediately after an action is taken. This alternative approach, known as optimal nudging, involves temporarily setting the gain to a fixed value and solving the problem using traditional reinforcement learning methods. The gain is then updated to minimize uncertainty in a minmax sense. The paper derives a rule for optimal gain update by using the geometric properties of the w-l space, a simple mapping of the policy space. The number of cumulative reward tasks required is minimal, and the algorithm's performance is compared to other approaches through experiments.",1
"The paper outlines a framework for autonomous control of a CRM (customer relationship management) system. First, it explores how a modified version of the widely accepted Recency-Frequency-Monetary Value system of metrics can be used to define the state space of clients or donors. Second, it describes a procedure to determine the optimal direct marketing action in discrete and continuous action space for the given individual, based on his position in the state space. The procedure involves the use of model-free Q-learning to train a deep neural network that relates a client's position in the state space to rewards associated with possible marketing actions. The estimated value function over the client state space can be interpreted as customer lifetime value, and thus allows for a quick plug-in estimation of CLV for a given client. Experimental results are presented, based on KDD Cup 1998 mailing dataset of donation solicitations.",0
"The article presents a framework for self-governing control of a CRM system. Initially, it examines how an adjusted version of the commonly accepted Recency-Frequency-Monetary Value metrics system can be utilized to define the condition of customers or contributors. Subsequently, it describes a process to identify the most effective direct marketing approach in both discrete and continuous action spaces for an individual, depending on their position in the condition space. The process involves using model-free Q-learning to educate a deep neural network, which connects a customer's position in the condition space to the rewards associated with potential marketing actions. The approximated value function over the client condition space can be interpreted as customer lifetime value (CLV), which allows for a fast estimation of CLV for a particular customer. The article also includes experimental results based on KDD Cup 1998 mailing dataset for donation solicitations.",1
"We consider reinforcement learning in parameterized Markov Decision Processes (MDPs), where the parameterization may induce correlation across transition probabilities or rewards. Consequently, observing a particular state transition might yield useful information about other, unobserved, parts of the MDP. We present a version of Thompson sampling for parameterized reinforcement learning problems, and derive a frequentist regret bound for priors over general parameter spaces. The result shows that the number of instants where suboptimal actions are chosen scales logarithmically with time, with high probability. It holds for prior distributions that put significant probability near the true model, without any additional, specific closed-form structure such as conjugate or product-form priors. The constant factor in the logarithmic scaling encodes the information complexity of learning the MDP in terms of the Kullback-Leibler geometry of the parameter space.",0
"In this article, we explore reinforcement learning in parameterized Markov Decision Processes (MDPs). The parameterization used may cause correlation across transition probabilities or rewards. This means that observing a specific state transition can provide valuable information about unobserved parts of the MDP. We introduce a version of Thompson sampling for parameterized reinforcement learning problems and establish a frequentist regret bound for priors over general parameter spaces. Our findings indicate that the number of occasions where suboptimal actions are taken increases logarithmically with time with high likelihood. This applies to prior distributions that assign significant probability to the true model, without any added, specific closed-form structure such as conjugate or product-form priors. The constant factor in the logarithmic scaling reflects the information complexity of learning the MDP in terms of the Kullback-Leibler geometry of the parameter space.",1
"Many applications that use empirically estimated functions face a curse of dimensionality, because the integrals over most function classes must be approximated by sampling. This paper introduces a novel regression-algorithm that learns linear factored functions (LFF). This class of functions has structural properties that allow to analytically solve certain integrals and to calculate point-wise products. Applications like belief propagation and reinforcement learning can exploit these properties to break the curse and speed up computation. We derive a regularized greedy optimization scheme, that learns factored basis functions during training. The novel regression algorithm performs competitively to Gaussian processes on benchmark tasks, and the learned LFF functions are with 4-9 factored basis functions on average very compact.",0
"The curse of dimensionality is a common issue faced by applications that rely on empirically estimated functions, as approximating integrals over function classes through sampling can be challenging. To address this problem, this article presents a new regression algorithm that enables linear factored functions (LFF) to be learned. These functions possess structural attributes that allow certain integrals to be analytically solved and point-wise products to be calculated. By leveraging these properties, applications such as reinforcement learning and belief propagation can overcome the curse of dimensionality and enhance computational speed. The algorithm employs a regularized greedy optimization approach to train and learn factored basis functions. In benchmark tasks, the novel regression algorithm performs competitively with Gaussian processes, and the learned LFF functions are highly compact, averaging between 4-9 factored basis functions.",1
"In recent years, there has been growing focus on the study of automated recommender systems. Music recommendation systems serve as a prominent domain for such works, both from an academic and a commercial perspective. A fundamental aspect of music perception is that music is experienced in temporal context and in sequence. In this work we present DJ-MC, a novel reinforcement-learning framework for music recommendation that does not recommend songs individually but rather song sequences, or playlists, based on a model of preferences for both songs and song transitions. The model is learned online and is uniquely adapted for each listener. To reduce exploration time, DJ-MC exploits user feedback to initialize a model, which it subsequently updates by reinforcement. We evaluate our framework with human participants using both real song and playlist data. Our results indicate that DJ-MC's ability to recommend sequences of songs provides a significant improvement over more straightforward approaches, which do not take transitions into account.",0
"The study of automated recommender systems has gained increasing attention in recent years, particularly in the domain of music recommendation. This topic has attracted both academic and commercial interest due to the significance of music perception as an experience that is highly dependent on temporal context and sequence. Our research introduces DJ-MC, a unique reinforcement-learning framework that recommends song sequences or playlists based on a model of preferences for both individual songs and song transitions. Our model is customised for each listener and learns online through the exploitation of user feedback to initialise and update the model by reinforcement. We conducted human participant evaluations using both real song and playlist data, which indicated that DJ-MC's approach to recommending sequences of songs significantly outperforms more straightforward approaches that do not consider song transitions.",1
"Understanding the affective, cognitive and behavioural processes involved in risk taking is essential for treatment and for setting environmental conditions to limit damage. Using Temporal Difference Reinforcement Learning (TDRL) we computationally investigated the effect of optimism in risk perception in a variety of goal-oriented tasks. Optimism in risk perception was studied by varying the calculation of the Temporal Difference error, i.e., delta, in three ways: realistic (stochastically correct), optimistic (assuming action control), and overly optimistic (assuming outcome control). We show that for the gambling task individuals with 'healthy' perception of control, i.e., action optimism, do not develop gambling behaviour while individuals with 'unhealthy' perception of control, i.e., outcome optimism, do. We show that high intensity of sensations and low levels of fear co-occur due to optimistic risk perception. We found that overly optimistic risk perception (outcome optimism) results in risk taking and in persistent gambling behaviour in addition to high intensity of sensations. We discuss how our results replicate risk-taking related phenomena.",0
"To effectively treat and prevent harmful consequences, it is crucial to comprehend the emotional, cognitive, and behavioral aspects of risk-taking. Through the utilization of Temporal Difference Reinforcement Learning (TDRL), we analyzed the impact of optimism on risk perception in various task objectives. The study examined three methods of calculating the Temporal Difference error, which included realistic (statistically accurate), optimistic (assuming control over actions), and excessively optimistic (assuming control over outcomes). Our findings reveal that individuals who possess a 'healthy' sense of control (action optimism) do not engage in gambling behaviors, while those with an 'unhealthy' sense of control (outcome optimism) do. An optimistic risk perception leads to heightened sensations and reduced fear. Conversely, an overly optimistic risk perception (outcome optimism) leads to persistent gambling behaviors, risk-taking, and intense sensations. Our study yields results that align with established risk-related phenomena.",1
"In reinforcement learning, the TD($\lambda$) algorithm is a fundamental policy evaluation method with an efficient online implementation that is suitable for large-scale problems. One practical drawback of TD($\lambda$) is its sensitivity to the choice of the step-size. It is an empirically well-known fact that a large step-size leads to fast convergence, at the cost of higher variance and risk of instability. In this work, we introduce the implicit TD($\lambda$) algorithm which has the same function and computational cost as TD($\lambda$), but is significantly more stable. We provide a theoretical explanation of this stability and an empirical evaluation of implicit TD($\lambda$) on typical benchmark tasks. Our results show that implicit TD($\lambda$) outperforms standard TD($\lambda$) and a state-of-the-art method that automatically tunes the step-size, and thus shows promise for wide applicability.",0
"The TD($\lambda$) algorithm is a widely used policy evaluation technique in reinforcement learning, particularly for large-scale problems due to its efficient online implementation. However, a major issue with TD($\lambda$) is its sensitivity to the chosen step-size, where a larger step-size results in faster convergence but also higher variance and instability. To address this, we propose the implicit TD($\lambda$) algorithm, which has the same function and computational cost as TD($\lambda$) but is significantly more stable. We provide both theoretical and empirical evidence for this increased stability and demonstrate that implicit TD($\lambda$) outperforms both standard TD($\lambda$) and a state-of-the-art method that automatically tunes the step-size. Our findings suggest that implicit TD($\lambda$) has broad potential applications.",1
"Methods of deep machine learning enable to to reuse low-level representations efficiently for generating more abstract high-level representations. Originally, deep learning has been applied passively (e.g., for classification purposes). Recently, it has been extended to estimate the value of actions for autonomous agents within the framework of reinforcement learning (RL). Explicit models of the environment can be learned to augment such a value function. Although ""flat"" connectionist methods have already been used for model-based RL, up to now, only model-free variants of RL have been equipped with methods from deep learning. We propose a variant of deep model-based RL that enables an agent to learn arbitrarily abstract hierarchical representations of its environment. In this paper, we present research on how such hierarchical representations can be grounded in sensorimotor interaction between an agent and its environment.",0
"The use of deep machine learning methods allows for efficient reuse of low-level representations to generate higher-level representations. Initially, deep learning was only used for classification purposes, but it has recently been expanded to estimate the value of actions for autonomous agents in reinforcement learning (RL). By learning explicit models of the environment, the value function can be enhanced. Although flat connectionist methods have been used for model-based RL, only model-free RL has utilized deep learning techniques. Our proposed deep model-based RL variant enables agents to learn abstract hierarchical representations of their environment. This paper presents research on how these representations can be grounded in sensorimotor interaction.",1
"State-of-the-art visual recognition and detection systems increasingly rely on large amounts of training data and complex classifiers. Therefore it becomes increasingly expensive both to manually annotate datasets and to keep running times at levels acceptable for practical applications. In this paper, we propose two solutions to address these issues. First, we introduce a weakly supervised, segmentation-based approach to learn accurate detectors and image classifiers from weak supervisory signals that provide only approximate constraints on target localization. We illustrate our system on the problem of action detection in static images (Pascal VOC Actions 2012), using human visual search patterns as our training signal. Second, inspired from the saccade-and-fixate operating principle of the human visual system, we use reinforcement learning techniques to train efficient search models for detection. Our sequential method is weakly supervised and general (it does not require eye movements), finds optimal search strategies for any given detection confidence function and achieves performance similar to exhaustive sliding window search at a fraction of its computational cost.",0
"Modern visual recognition and detection systems depend more and more on intricate classifiers and large amounts of training data, leading to high costs for manual annotation and practical application run times. This paper proposes two solutions to these issues. Firstly, a segmentation-based, weakly supervised approach to learn image classifiers and detectors accurately using only approximate constraints for target localization. This method is applied to action detection in static images using human visual search patterns as the training signal. Secondly, reinforcement learning techniques inspired by the human visual system's saccade-and-fixate operating principle are used to train efficient search models for detection. This sequential method is weakly supervised, general, and finds optimal search strategies for any given detection confidence function while achieving performance similar to exhaustive sliding window search at a fraction of its computational cost.",1
"Conditional Value at Risk (CVaR) is a prominent risk measure that is being used extensively in various domains. We develop a new formula for the gradient of the CVaR in the form of a conditional expectation. Based on this formula, we propose a novel sampling-based estimator for the CVaR gradient, in the spirit of the likelihood-ratio method. We analyze the bias of the estimator, and prove the convergence of a corresponding stochastic gradient descent algorithm to a local CVaR optimum. Our method allows to consider CVaR optimization in new domains. As an example, we consider a reinforcement learning application, and learn a risk-sensitive controller for the game of Tetris.",0
"The use of Conditional Value at Risk (CVaR) as a risk measure is widespread in different fields. Our research focuses on creating a new formula for the gradient of CVaR using conditional expectation. We propose a unique estimator for the CVaR gradient based on this formula, using the likelihood-ratio method. We examine the estimator's bias and confirm the convergence of a stochastic gradient descent algorithm to a local CVaR optimum. Our approach enables CVaR optimization in previously unexplored areas. To illustrate, we apply our method to a reinforcement learning scenario and develop a risk-sensitive controller for the game of Tetris.",1
"We consider the problem of learning to optimize an unknown Markov decision process (MDP). We show that, if the MDP can be parameterized within some known function class, we can obtain regret bounds that scale with the dimensionality, rather than cardinality, of the system. We characterize this dependence explicitly as $\tilde{O}(\sqrt{d_K d_E T})$ where $T$ is time elapsed, $d_K$ is the Kolmogorov dimension and $d_E$ is the \emph{eluder dimension}. These represent the first unified regret bounds for model-based reinforcement learning and provide state of the art guarantees in several important settings. Moreover, we present a simple and computationally efficient algorithm \emph{posterior sampling for reinforcement learning} (PSRL) that satisfies these bounds.",0
"The problem we are addressing is how to learn to optimize an unknown Markov decision process (MDP). Our research has revealed that if the MDP can be parameterized within a known function class, we can obtain regret bounds that depend on the dimensionality of the system, not its cardinality. This dependency is explicitly characterized as $\tilde{O}(\sqrt{d_K d_E T})$ where $T$ is the elapsed time, $d_K$ is the Kolmogorov dimension, and $d_E$ is the eluder dimension. Our results represent the first unified regret bounds for model-based reinforcement learning and offer state-of-the-art guarantees in several crucial settings. Additionally, we introduce a straightforward and computationally efficient algorithm called Posterior Sampling for Reinforcement Learning (PSRL) that satisfies these bounds.",1
"Any reinforcement learning algorithm that applies to all Markov decision processes (MDPs) will suffer $\Omega(\sqrt{SAT})$ regret on some MDP, where $T$ is the elapsed time and $S$ and $A$ are the cardinalities of the state and action spaces. This implies $T = \Omega(SA)$ time to guarantee a near-optimal policy. In many settings of practical interest, due to the curse of dimensionality, $S$ and $A$ can be so enormous that this learning time is unacceptable. We establish that, if the system is known to be a \emph{factored} MDP, it is possible to achieve regret that scales polynomially in the number of \emph{parameters} encoding the factored MDP, which may be exponentially smaller than $S$ or $A$. We provide two algorithms that satisfy near-optimal regret bounds in this context: posterior sampling reinforcement learning (PSRL) and an upper confidence bound algorithm (UCRL-Factored).",0
"Regret in any reinforcement learning algorithm applicable to all Markov decision processes (MDPs) will be at least $\Omega(\sqrt{SAT})$ on an MDP, where $T$ represents time, and $S$ and $A$ represent state and action space sizes. To ensure a near-optimal policy, a minimum of $T = \Omega(SA)$ time is needed. However, in practical settings, the curse of dimensionality may lead to extremely large $S$ and $A$, making learning time unacceptably long. We demonstrate that if the MDP is factored, regret can be achieved at a polynomial scale based on the number of parameters encoding the factored MDP, which may be exponentially smaller than $S$ or $A$. We present two algorithms, namely, posterior sampling reinforcement learning (PSRL) and an upper confidence bound algorithm (UCRL-Factored), that exhibit near-optimal regret bounds in this scenario.",1
"Reinforcement learning agents have traditionally been evaluated on small toy problems. With advances in computing power and the advent of the Arcade Learning Environment, it is now possible to evaluate algorithms on diverse and difficult problems within a consistent framework. We discuss some challenges posed by the arcade learning environment which do not manifest in simpler environments. We then provide a comparison of model-free, linear learning algorithms on this challenging problem set.",0
"In the past, reinforcement learning agents were mainly tested on minor, insignificant issues. However, due to technological improvements and the creation of the Arcade Learning Environment, it is now feasible to assess algorithms on various and demanding problems in a uniform setting. We explore certain obstacles presented by the arcade learning environment that are absent in less complex environments. We subsequently present a contrast of model-free, linear learning algorithms applied to this formidable range of problems.",1
"Goals express agents' intentions and allow them to organize their behavior based on low-dimensional abstractions of high-dimensional world states. How can agents develop such goals autonomously? This paper proposes a detailed conceptual and computational account to this longstanding problem. We argue to consider goals as high-level abstractions of lower-level intention mechanisms such as rewards and values, and point out that goals need to be considered alongside with a detection of the own actions' effects. We propose Latent Goal Analysis as a computational learning formulation thereof, and show constructively that any reward or value function can by explained by goals and such self-detection as latent mechanisms. We first show that learned goals provide a highly effective dimensionality reduction in a practical reinforcement learning problem. Then, we investigate a developmental scenario in which entirely task-unspecific rewards induced by visual saliency lead to self and goal representations that constitute goal-directed reaching.",0
"This paper addresses the issue of how agents can independently develop goals by expressing their intentions and organizing their behavior based on simplified versions of complex world states. The proposed solution suggests that goals should be viewed as high-level abstractions of lower-level intention mechanisms such as rewards and values, and that they should be considered in conjunction with the detection of the agent's own actions' effects. The paper introduces the computational learning formulation of Latent Goal Analysis and demonstrates that any reward or value function can be explained by goals and self-detection as latent mechanisms. The effectiveness of learned goals in practical reinforcement learning problems is examined, followed by an investigation of a developmental scenario where task-unspecific rewards induced by visual saliency lead to self and goal representations that result in goal-directed reaching.",1
"In Reinforcement Learning (RL), it is common to use optimistic initialization of value functions to encourage exploration. However, such an approach generally depends on the domain, viz., the scale of the rewards must be known, and the feature representation must have a constant norm. We present a simple approach that performs optimistic initialization with less dependence on the domain.",0
"Optimistic initialization of value functions is frequently used in Reinforcement Learning (RL) as a means of promoting exploration. Nevertheless, this method's effectiveness is typically reliant on the specific domain, as the reward scale and feature representation norm must be known. Our proposed solution offers a straightforward alternative that decreases reliance on domain specifics while still employing optimistic initialization.",1
"Neural Networks sequentially build high-level features through their successive layers. We propose here a new neural network model where each layer is associated with a set of candidate mappings. When an input is processed, at each layer, one mapping among these candidates is selected according to a sequential decision process. The resulting model is structured according to a DAG like architecture, so that a path from the root to a leaf node defines a sequence of transformations. Instead of considering global transformations, like in classical multilayer networks, this model allows us for learning a set of local transformations. It is thus able to process data with different characteristics through specific sequences of such local transformations, increasing the expression power of this model w.r.t a classical multilayered network. The learning algorithm is inspired from policy gradient techniques coming from the reinforcement learning domain and is used here instead of the classical back-propagation based gradient descent techniques. Experiments on different datasets show the relevance of this approach.",0
"Our proposed neural network model introduces a unique approach to building high-level features. Each layer within the neural network is paired with a set of candidate mappings. During input processing, a sequential decision process selects one mapping from these candidates at each layer. The resulting DAG-like architecture forms a sequence of transformations from the root to a leaf node. Unlike traditional multilayer networks that consider global transformations, our model learns a set of local transformations. This enables the processing of data with varying characteristics through specific sequences of local transformations, increasing the model's expression power. To train the model, we use a learning algorithm inspired by policy gradient techniques from the reinforcement learning domain, instead of the conventional back-propagation based gradient descent techniques. Our experiments on different datasets demonstrate the effectiveness of this approach.",1
"We consider a reinforcement learning setting introduced in (Maillard et al., NIPS 2011) where the learner does not have explicit access to the states of the underlying Markov decision process (MDP). Instead, she has access to several models that map histories of past interactions to states. Here we improve over known regret bounds in this setting, and more importantly generalize to the case where the models given to the learner do not contain a true model resulting in an MDP representation but only approximations of it. We also give improved error bounds for state aggregation.",0
"In the reinforcement learning scenario presented in (Maillard et al., NIPS 2011), the learner lacks explicit access to the states of the underlying Markov decision process (MDP) but instead has access to multiple models that translate histories of past interactions to states. Our contribution is twofold: firstly, we enhance existing regret bounds in this context, and secondly, we extend the approach to encompass situations where the models provided to the learner are only approximations of the true model, lacking an MDP representation. Additionally, we provide refined error bounds for state aggregation.",1
"We consider the problem of learning by demonstration from agents acting in unknown stochastic Markov environments or games. Our aim is to estimate agent preferences in order to construct improved policies for the same task that the agents are trying to solve. To do so, we extend previous probabilistic approaches for inverse reinforcement learning in known MDPs to the case of unknown dynamics or opponents. We do this by deriving two simplified probabilistic models of the demonstrator's policy and utility. For tractability, we use maximum a posteriori estimation rather than full Bayesian inference. Under a flat prior, this results in a convex optimisation problem. We find that the resulting algorithms are highly competitive against a variety of other methods for inverse reinforcement learning that do have knowledge of the dynamics.",0
"The focus of this study is on learning through observation in stochastic Markov environments or games where the agents are unfamiliar. The goal is to estimate the preferences of these agents to create better policies for the same task they are attempting to complete. To accomplish this, we expand on previous probabilistic techniques used in known Markov Decision Processes (MDPs) to account for unknown dynamics or opponents. We develop two simplified models of the demonstrator's policy and utility and utilize maximum a posteriori estimation for tractability, instead of full Bayesian inference. With a flat prior, this yields a convex optimization problem. Our findings indicate that these algorithms are highly competitive when compared to other inverse reinforcement learning methods that have knowledge of the dynamics.",1
"Kernel-based reinforcement learning (KBRL) stands out among reinforcement learning algorithms for its strong theoretical guarantees. By casting the learning problem as a local kernel approximation, KBRL provides a way of computing a decision policy which is statistically consistent and converges to a unique solution. Unfortunately, the model constructed by KBRL grows with the number of sample transitions, resulting in a computational cost that precludes its application to large-scale or on-line domains. In this paper we introduce an algorithm that turns KBRL into a practical reinforcement learning tool. Kernel-based stochastic factorization (KBSF) builds on a simple idea: when a transition matrix is represented as the product of two stochastic matrices, one can swap the factors of the multiplication to obtain another transition matrix, potentially much smaller, which retains some fundamental properties of its precursor. KBSF exploits such an insight to compress the information contained in KBRL's model into an approximator of fixed size. This makes it possible to build an approximation that takes into account both the difficulty of the problem and the associated computational cost. KBSF's computational complexity is linear in the number of sample transitions, which is the best one can do without discarding data. Moreover, the algorithm's simple mechanics allow for a fully incremental implementation that makes the amount of memory used independent of the number of sample transitions. The result is a kernel-based reinforcement learning algorithm that can be applied to large-scale problems in both off-line and on-line regimes. We derive upper bounds for the distance between the value functions computed by KBRL and KBSF using the same data. We also illustrate the potential of our algorithm in an extensive empirical study in which KBSF is applied to difficult tasks based on real-world data.",0
"Kernel-based reinforcement learning (KBRL) is a reinforcement learning algorithm that is known for its strong theoretical guarantees. It is able to compute a statistically consistent decision policy that converges to a unique solution by using a local kernel approximation to frame the learning problem. However, KBRL's model grows in size with each sample transition, which makes it impractical for use in large-scale or on-line domains due to its high computational cost. To address this issue, we propose an algorithm called kernel-based stochastic factorization (KBSF) that compresses KBRL's model into an approximator of fixed size by leveraging the idea that a transition matrix can be represented as the product of two stochastic matrices. KBSF's computational complexity is linear in the number of sample transitions, and its simple mechanics allow for a fully incremental implementation. This results in a kernel-based reinforcement learning algorithm that can be applied to large-scale problems in both off-line and on-line regimes. We provide upper bounds for the distance between the value functions computed by KBRL and KBSF using the same data, and we demonstrate the potential of our algorithm through an extensive empirical study using real-world data.",1
"Predictive state representations (PSRs) offer an expressive framework for modelling partially observable systems. By compactly representing systems as functions of observable quantities, the PSR learning approach avoids using local-minima prone expectation-maximization and instead employs a globally optimal moment-based algorithm. Moreover, since PSRs do not require a predetermined latent state structure as an input, they offer an attractive framework for model-based reinforcement learning when agents must plan without a priori access to a system model. Unfortunately, the expressiveness of PSRs comes with significant computational cost, and this cost is a major factor inhibiting the use of PSRs in applications. In order to alleviate this shortcoming, we introduce the notion of compressed PSRs (CPSRs). The CPSR learning approach combines recent advancements in dimensionality reduction, incremental matrix decomposition, and compressed sensing. We show how this approach provides a principled avenue for learning accurate approximations of PSRs, drastically reducing the computational costs associated with learning while also providing effective regularization. Going further, we propose a planning framework which exploits these learned models. And we show that this approach facilitates model-learning and planning in large complex partially observable domains, a task that is infeasible without the principled use of compression.",0
"Predictive state representations (PSRs) are a useful tool for modelling partially observable systems. They represent systems as functions of observable quantities, which allows for a globally optimal moment-based algorithm to be used instead of the local-minima prone expectation-maximization. PSRs are also attractive for model-based reinforcement learning because they do not require a predetermined latent state structure as an input. However, the high computational cost of PSRs limits their use in applications. To address this issue, we introduce compressed PSRs (CPSRs), which use dimensionality reduction, incremental matrix decomposition, and compressed sensing to learn accurate approximations of PSRs. The CPSR learning approach not only reduces computational costs but also provides effective regularization. We propose a planning framework that utilizes these learned models and facilitates model-learning and planning in large complex partially observable domains. This approach is necessary for tasks that are infeasible without the principled use of compression.",1
"Tackling large approximate dynamic programming or reinforcement learning problems requires methods that can exploit regularities, or intrinsic structure, of the problem in hand. Most current methods are geared towards exploiting the regularities of either the value function or the policy. We introduce a general classification-based approximate policy iteration (CAPI) framework, which encompasses a large class of algorithms that can exploit regularities of both the value function and the policy space, depending on what is advantageous. This framework has two main components: a generic value function estimator and a classifier that learns a policy based on the estimated value function. We establish theoretical guarantees for the sample complexity of CAPI-style algorithms, which allow the policy evaluation step to be performed by a wide variety of algorithms (including temporal-difference-style methods), and can handle nonparametric representations of policies. Our bounds on the estimation error of the performance loss are tighter than existing results. We also illustrate this approach empirically on several problems, including a large HIV control task.",0
"When dealing with large approximate dynamic programming or reinforcement learning problems, it is important to have methods that can take advantage of the regularities or intrinsic structure of the problem. While most current methods focus on exploiting regularities in either the value function or the policy, we propose a more comprehensive framework called the classification-based approximate policy iteration (CAPI). This framework includes a wide range of algorithms that can exploit regularities in both the value function and the policy space depending on their advantages. CAPI consists of a generic value function estimator and a classifier that learns a policy based on the estimated value function. We provide theoretical guarantees for the sample complexity of CAPI-style algorithms, which can handle nonparametric representations of policies and allow for policy evaluation using various algorithms such as temporal-difference-style methods. Our estimation error bounds for the performance loss are tighter than existing results. We also demonstrate the effectiveness of this approach on various problems, such as a large HIV control task.",1
"In both the fields of computer science and medicine there is very strong interest in developing personalized treatment policies for patients who have variable responses to treatments. In particular, I aim to find an optimal personalized treatment policy which is a non-deterministic function of the patient specific covariate data that maximizes the expected survival time or clinical outcome. I developed an algorithmic framework to solve multistage decision problem with a varying number of stages that are subject to censoring in which the ""rewards"" are expected survival times. In specific, I developed a novel Q-learning algorithm that dynamically adjusts for these parameters. Furthermore, I found finite upper bounds on the generalized error of the treatment paths constructed by this algorithm. I have also shown that when the optimal Q-function is an element of the approximation space, the anticipated survival times for the treatment regime constructed by the algorithm will converge to the optimal treatment path. I demonstrated the performance of the proposed algorithmic framework via simulation studies and through the analysis of chronic depression data and a hypothetical clinical trial. The censored Q-learning algorithm I developed is more effective than the state of the art clinical decision support systems and is able to operate in environments when many covariate parameters may be unobtainable or censored.",0
"There is significant interest in both computer science and medicine to develop personalized treatment plans for patients who have varying responses to treatments. My goal is to create an optimal personalized treatment plan for each patient, using their specific data to maximize their expected survival time or clinical outcome. To achieve this, I developed an algorithmic framework to solve multistage decision problems, accounting for censoring with expected survival times as rewards. My novel Q-learning algorithm can adjust for varying numbers of stages and finite upper bounds on error, and can converge to the optimal treatment path when the Q-function is within the approximation space. I tested my algorithm through simulations and by analyzing chronic depression data and a hypothetical clinical trial. My censored Q-learning algorithm outperforms current clinical decision support systems and can operate in situations where many covariate parameters may not be available.",1
"Demand response (DR) for residential and small commercial buildings is estimated to account for as much as 65% of the total energy savings potential of DR, and previous work shows that a fully automated Energy Management System (EMS) is a necessary prerequisite to DR in these areas. In this paper, we propose a novel EMS formulation for DR problems in these sectors. Specifically, we formulate a fully automated EMS's rescheduling problem as a reinforcement learning (RL) problem, and argue that this RL problem can be approximately solved by decomposing it over device clusters. Compared with existing formulations, our new formulation (1) does not require explicitly modeling the user's dissatisfaction on job rescheduling, (2) enables the EMS to self-initiate jobs, (3) allows the user to initiate more flexible requests and (4) has a computational complexity linear in the number of devices. We also demonstrate the simulation results of applying Q-learning, one of the most popular and classical RL algorithms, to a representative example.",0
"As much as 65% of the potential energy savings for demand response (DR) in residential and small commercial buildings can be attributed to DR. It has been previously established that a fully automated Energy Management System (EMS) is essential for DR in these sectors. In this paper, we introduce a new formulation for EMS that addresses DR problems in these areas. Our formulation entails rescheduling through reinforcement learning (RL), which can be solved approximately by dividing it over device clusters. Our new formulation has several advantages over existing ones, such as not requiring explicit modeling of user dissatisfaction on job rescheduling, enabling self-initiation of jobs by the EMS, and allowing users to request more flexible jobs. Moreover, it has a computational complexity that is linear in the number of devices. We also demonstrate the simulation results of applying Q-learning, one of the most popular RL algorithms, to an example.",1
"Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.",0
"Due to the fact that the computation required for convolutional neural networks increases in proportion to the number of pixels in an image, using them for large images can be computationally demanding. Our innovative recurrent neural network model, on the other hand, can efficiently extract information from images and videos by selectively identifying and processing only high-resolution regions or locations. Though our proposed approach shares some of the translation invariance features of convolutional neural networks, it offers more control over computation, regardless of the input image size. Although non-differentiable, our model can be trained with reinforcement learning techniques to acquire task-specific policies. We tested our model on various image classification tasks and found that it outperformed a convolutional neural network baseline on cluttered images. Additionally, it successfully learned to track a basic object without explicit training signals in a dynamic visual control problem.",1
"Recent work has demonstrated that problems-- particularly imitation learning and structured prediction-- where a learner's predictions influence the input-distribution it is tested on can be naturally addressed by an interactive approach and analyzed using no-regret online learning. These approaches to imitation learning, however, neither require nor benefit from information about the cost of actions. We extend existing results in two directions: first, we develop an interactive imitation learning approach that leverages cost information; second, we extend the technique to address reinforcement learning. The results provide theoretical support to the commonly observed successes of online approximate policy iteration. Our approach suggests a broad new family of algorithms and provides a unifying view of existing techniques for imitation and reinforcement learning.",0
"Recent research has shown that interactive learning and no-regret online learning can effectively tackle problems such as imitation learning and structured prediction, where the learner's predictions affect the input-distribution it is tested on. However, these approaches do not require or benefit from knowledge of action costs. In this study, we expand on these findings by introducing an interactive imitation learning strategy that utilizes cost information and extending the technique to address reinforcement learning. Our results offer theoretical backing for the observed triumphs of online approximate policy iteration and introduce a new class of algorithms while providing a unified perspective on existing methods for reinforcement and imitation learning.",1
"In this paper, we set forth a new vision of reinforcement learning developed by us over the past few years, one that yields mathematically rigorous solutions to longstanding important questions that have remained unresolved: (i) how to design reliable, convergent, and robust reinforcement learning algorithms (ii) how to guarantee that reinforcement learning satisfies pre-specified ""safety"" guarantees, and remains in a stable region of the parameter space (iii) how to design ""off-policy"" temporal difference learning algorithms in a reliable and stable manner, and finally (iv) how to integrate the study of reinforcement learning into the rich theory of stochastic optimization. In this paper, we provide detailed answers to all these questions using the powerful framework of proximal operators.   The key idea that emerges is the use of primal dual spaces connected through the use of a Legendre transform. This allows temporal difference updates to occur in dual spaces, allowing a variety of important technical advantages. The Legendre transform elegantly generalizes past algorithms for solving reinforcement learning problems, such as natural gradient methods, which we show relate closely to the previously unconnected framework of mirror descent methods. Equally importantly, proximal operator theory enables the systematic development of operator splitting methods that show how to safely and reliably decompose complex products of gradients that occur in recent variants of gradient-based temporal difference learning. This key technical innovation makes it possible to finally design ""true"" stochastic gradient methods for reinforcement learning. Finally, Legendre transforms enable a variety of other benefits, including modeling sparsity and domain geometry. Our work builds extensively on recent work on the convergence of saddle-point algorithms, and on the theory of monotone operators.",0
"This paper presents a new perspective on reinforcement learning, which we have developed over the past few years. It offers rigorous solutions to longstanding and important questions that have remained unresolved, including how to design reliable, convergent, and robust reinforcement learning algorithms, how to ensure that reinforcement learning meets pre-specified ""safety"" guarantees and remains in a stable parameter space, how to design stable ""off-policy"" temporal difference learning algorithms, and how to integrate reinforcement learning with stochastic optimization theory. Our paper provides detailed answers to these questions using the powerful framework of proximal operators. The key idea is to use primal dual spaces connected by a Legendre transform, which enables temporal difference updates to occur in dual spaces and provides a variety of technical advantages. Proximal operator theory also enables the systematic development of operator splitting methods for decomposing complex products of gradients that occur in gradient-based temporal difference learning. This technical innovation makes it possible to design ""true"" stochastic gradient methods for reinforcement learning. The Legendre transform also offers additional benefits, including modeling sparsity and domain geometry. Our work builds on recent research on the convergence of saddle-point algorithms and the theory of monotone operators.",1
"In this paper we consider the problem of online stochastic optimization of a locally smooth function under bandit feedback. We introduce the high-confidence tree (HCT) algorithm, a novel any-time $\mathcal{X}$-armed bandit algorithm, and derive regret bounds matching the performance of existing state-of-the-art in terms of dependency on number of steps and smoothness factor. The main advantage of HCT is that it handles the challenging case of correlated rewards, whereas existing methods require that the reward-generating process of each arm is an identically and independent distributed (iid) random process. HCT also improves on the state-of-the-art in terms of its memory requirement as well as requiring a weaker smoothness assumption on the mean-reward function in compare to the previous anytime algorithms. Finally, we discuss how HCT can be applied to the problem of policy search in reinforcement learning and we report preliminary empirical results.",0
"The focus of this article is the online stochastic optimization of a locally smooth function under bandit feedback. We propose a new any-time $\mathcal{X}$-armed bandit algorithm called the high-confidence tree (HCT) algorithm, which achieves regret bounds similar to those of existing state-of-the-art algorithms in terms of their dependence on the number of steps and smoothness factor. The main advantage of HCT is its ability to handle correlated rewards, a challenging case that existing methods struggle with, as they require that the reward-generating process of each arm is an identically and independently distributed (iid) random process. HCT also has a lower memory requirement and requires a weaker smoothness assumption on the mean-reward function compared to previous anytime algorithms. Finally, we explore how HCT can be used for policy search in reinforcement learning and provide preliminary empirical results.",1
"Batch Reinforcement Learning (RL) algorithms attempt to choose a policy from a designer-provided class of policies given a fixed set of training data. Choosing the policy which maximizes an estimate of return often leads to over-fitting when only limited data is available, due to the size of the policy class in relation to the amount of data available. In this work, we focus on learning policy classes that are appropriately sized to the amount of data available. We accomplish this by using the principle of Structural Risk Minimization, from Statistical Learning Theory, which uses Rademacher complexity to identify a policy class that maximizes a bound on the return of the best policy in the chosen policy class, given the available data. Unlike similar batch RL approaches, our bound on return requires only extremely weak assumptions on the true system.",0
"The aim of Batch Reinforcement Learning (RL) algorithms is to select a policy from a set of policies provided by the designer, using a fixed set of training data. However, this can lead to over-fitting when the available data is limited, due to the size of the policy class compared to the data available. To tackle this issue, we focus on discovering policy classes that are appropriately sized for the available data, using the principle of Structural Risk Minimization from Statistical Learning Theory. This approach uses Rademacher complexity to identify a policy class that maximizes the return bound of the best policy in the chosen policy class, given the available data. Unlike other batch RL approaches, our return bound has only weak assumptions about the true system.",1
"This paper proposes an online tree-based Bayesian approach for reinforcement learning. For inference, we employ a generalised context tree model. This defines a distribution on multivariate Gaussian piecewise-linear models, which can be updated in closed form. The tree structure itself is constructed using the cover tree method, which remains efficient in high dimensional spaces. We combine the model with Thompson sampling and approximate dynamic programming to obtain effective exploration policies in unknown environments. The flexibility and computational simplicity of the model render it suitable for many reinforcement learning problems in continuous state spaces. We demonstrate this in an experimental comparison with least squares policy iteration.",0
"An internet-based method for reinforcement learning is put forth in this article, utilizing a tree-based Bayesian approach. The generalized context tree model is utilized for inference, which establishes a distribution on multivariate Gaussian piecewise-linear models that can be easily updated. The cover tree technique is used to build the tree structure, which works well in high dimensional spaces. To achieve efficient exploration policies in unfamiliar environments, the model is combined with Thompson sampling and approximate dynamic programming. The simplicity and adaptability of the model make it ideal for many reinforcement learning issues in continuous state spaces. In comparison to least squares policy iteration, we demonstrate its efficacy through an experimental analysis.",1
"Ubiquitous information access becomes more and more important nowadays and research is aimed at making it adapted to users. Our work consists in applying machine learning techniques in order to bring a solution to some of the problems concerning the acceptance of the system by users. To achieve this, we propose a fundamental shift in terms of how we model the learning of recommender system: inspired by models of human reasoning developed in robotic, we combine reinforcement learning and case-base reasoning to define a recommendation process that uses these two approaches for generating recommendations on different context dimensions (social, temporal, geographic). We describe an implementation of the recommender system based on this framework. We also present preliminary results from experiments with the system and show how our approach increases the recommendation quality.",0
"Nowadays, ubiquitous access to information is becoming increasingly important, and researchers are working towards making it more user-friendly. Our objective is to address some of the challenges related to user acceptance by utilizing machine learning techniques. Our proposed solution involves a significant shift in the way we approach learning in recommender systems. Drawing inspiration from models of human reasoning developed in robotics, we combine reinforcement learning and case-based reasoning to create a recommendation process that considers various context dimensions (such as social, temporal, and geographic). We have implemented this framework and conducted preliminary experiments, which demonstrate how our approach enhances the quality of recommendations.",1
"We compare the performance of Inverse Reinforcement Learning (IRL) with the relative new model of Multi-agent Inverse Reinforcement Learning (MIRL). Before comparing the methods, we extend a published Bayesian IRL approach that is only applicable to the case where the reward is only state dependent to a general one capable of tackling the case where the reward depends on both state and action. Comparison between IRL and MIRL is made in the context of an abstract soccer game, using both a game model in which the reward depends only on state and one in which it depends on both state and action. Results suggest that the IRL approach performs much worse than the MIRL approach. We speculate that the underperformance of IRL is because it fails to capture equilibrium information in the manner possible in MIRL.",0
"We conducted a performance comparison of Inverse Reinforcement Learning (IRL) and Multi-agent Inverse Reinforcement Learning (MIRL), a relatively new model. In order to make the comparison, we first extended a Bayesian IRL method that previously only worked with state-dependent rewards to a more general approach that can handle rewards dependent on both state and action. The comparison was carried out in the context of an abstract soccer game, using a game model with rewards dependent only on state and another model with rewards dependent on both state and action. Our findings indicate that the MIRL approach outperforms IRL by a significant margin. We believe this could be due to the fact that IRL fails to capture equilibrium information, which is possible with the MIRL method.",1
"Recent advances in Bayesian reinforcement learning (BRL) have shown that Bayes-optimality is theoretically achievable by modeling the environment's latent dynamics using Flat-Dirichlet-Multinomial (FDM) prior. In self-interested multi-agent environments, the transition dynamics are mainly controlled by the other agent's stochastic behavior for which FDM's independence and modeling assumptions do not hold. As a result, FDM does not allow the other agent's behavior to be generalized across different states nor specified using prior domain knowledge. To overcome these practical limitations of FDM, we propose a generalization of BRL to integrate the general class of parametric models and model priors, thus allowing practitioners' domain knowledge to be exploited to produce a fine-grained and compact representation of the other agent's behavior. Empirical evaluation shows that our approach outperforms existing multi-agent reinforcement learning algorithms.",0
"Bayesian reinforcement learning (BRL) has made significant progress with recent advancements, demonstrating that it is possible to achieve Bayes-optimality through the use of Flat-Dirichlet-Multinomial (FDM) prior to model the latent dynamics of the environment. However, in self-interested multi-agent environments, the stochastic behavior of the other agents controls the transition dynamics, which contradicts FDM's independence and modeling assumptions. As a result, FDM cannot generalize the other agent's behavior across different states or incorporate prior domain knowledge. To address these practical limitations, we propose a generalization of BRL that integrates a broader range of parametric models and model priors, enabling practitioners to utilize domain knowledge to create a concise and detailed representation of the other agent's behavior. Our empirical evaluation demonstrates that our approach surpasses existing multi-agent reinforcement learning algorithms.",1
"We extend the framework of efficient coding, which has been used to model the development of sensory processing in isolation, to model the development of the perception/action cycle. Our extension combines sparse coding and reinforcement learning so that sensory processing and behavior co-develop to optimize a shared intrinsic motivational signal: the fidelity of the neural encoding of the sensory input under resource constraints. Applying this framework to a model system consisting of an active eye behaving in a time varying environment, we find that this generic principle leads to the simultaneous development of both smooth pursuit behavior and model neurons whose properties are similar to those of primary visual cortical neurons selective for different directions of visual motion. We suggest that this general principle may form the basis for a unified and integrated explanation of many perception/action loops.",0
"Our study expands the efficient coding framework beyond sensory processing in isolation to include the development of the perception/action cycle. By combining sparse coding and reinforcement learning, we demonstrate the co-development of sensory processing and behavior to optimize a shared intrinsic motivational signal. Specifically, we aim to enhance the accuracy of neural encoding of sensory information under resource constraints. Using a model system with an active eye navigating a dynamic environment, we observe the simultaneous development of smooth pursuit behavior and model neurons with properties similar to those of primary visual cortical neurons selective to various directions of visual motion. We propose that this principle could serve as a universal explanation for many perception/action cycles.",1
"Learning policies that generalize across multiple tasks is an important and challenging research topic in reinforcement learning and robotics. Training individual policies for every single potential task is often impractical, especially for continuous task variations, requiring more principled approaches to share and transfer knowledge among similar tasks. We present a novel approach for learning a nonlinear feedback policy that generalizes across multiple tasks. The key idea is to define a parametrized policy as a function of both the state and the task, which allows learning a single policy that generalizes across multiple known and unknown tasks. Applications of our novel approach to reinforcement and imitation learning in real-robot experiments are shown.",0
"The development of policies that can be applied to various tasks is a significant and difficult area of research in the fields of robotics and reinforcement learning. Creating distinct policies for each potential task is often not feasible, particularly when dealing with continuous task variations. As a result, more systematic methods are required to share and transfer knowledge among similar tasks. Our study proposes a new approach to learning a nonlinear feedback policy that can be generalized across multiple tasks. This is accomplished by defining a parametrized policy that is a function of both the state and the task, allowing for the learning of a single policy that can be applied to numerous known and unknown tasks. Real-robot experiments are used to demonstrate the effectiveness of our unique approach to reinforcement and imitation learning.",1
"In this paper, we consider the important problem of safe exploration in reinforcement learning. While reinforcement learning is well-suited to domains with complex transition dynamics and high-dimensional state-action spaces, an additional challenge is posed by the need for safe and efficient exploration. Traditional exploration techniques are not particularly useful for solving dangerous tasks, where the trial and error process may lead to the selection of actions whose execution in some states may result in damage to the learning system (or any other system). Consequently, when an agent begins an interaction with a dangerous and high-dimensional state-action space, an important question arises; namely, that of how to avoid (or at least minimize) damage caused by the exploration of the state-action space. We introduce the PI-SRL algorithm which safely improves suboptimal albeit robust behaviors for continuous state and action control tasks and which efficiently learns from the experience gained from the environment. We evaluate the proposed method in four complex tasks: automatic car parking, pole-balancing, helicopter hovering, and business management.",0
"The focus of this paper is on the crucial issue of ensuring safe exploration in reinforcement learning. Although reinforcement learning is effective for domains with intricate transition dynamics and high-dimensional state-action spaces, it presents a challenge when it comes to exploring such spaces without causing harm. Conventional exploration methods are inadequate for hazardous tasks, where trial and error can lead to the selection of actions that might damage the learning system or other systems. As a result, when an agent encounters a hazardous and complex state-action space, a critical question arises: how can one minimize the damage caused by exploration? The PI-SRL algorithm, which enhances suboptimal yet robust behaviors for continuous state and action control tasks while effectively learning from the environment, is introduced. We assess the proposed approach in four complex tasks: automatic car parking, pole-balancing, helicopter hovering, and business management.",1
"We derive a family of risk-sensitive reinforcement learning methods for agents, who face sequential decision-making tasks in uncertain environments. By applying a utility function to the temporal difference (TD) error, nonlinear transformations are effectively applied not only to the received rewards but also to the true transition probabilities of the underlying Markov decision process. When appropriate utility functions are chosen, the agents' behaviors express key features of human behavior as predicted by prospect theory (Kahneman and Tversky, 1979), for example different risk-preferences for gains and losses as well as the shape of subjective probability curves. We derive a risk-sensitive Q-learning algorithm, which is necessary for modeling human behavior when transition probabilities are unknown, and prove its convergence. As a proof of principle for the applicability of the new framework we apply it to quantify human behavior in a sequential investment task. We find, that the risk-sensitive variant provides a significantly better fit to the behavioral data and that it leads to an interpretation of the subject's responses which is indeed consistent with prospect theory. The analysis of simultaneously measured fMRI signals show a significant correlation of the risk-sensitive TD error with BOLD signal change in the ventral striatum. In addition we find a significant correlation of the risk-sensitive Q-values with neural activity in the striatum, cingulate cortex and insula, which is not present if standard Q-values are used.",0
"We have developed a range of risk-sensitive methods for reinforcement learning that can be used by agents making sequential decisions in uncertain environments. These methods involve applying a utility function to the temporal difference error, which allows for nonlinear transformations to be applied to both rewards and true transition probabilities. By selecting appropriate utility functions, agents can exhibit key features of human behavior predicted by prospect theory, such as varying risk preferences for gains and losses and subjective probability curve shapes. We have also created a risk-sensitive Q-learning algorithm that is necessary for modeling human behavior when transition probabilities are unknown, and we have proven its convergence. To demonstrate the effectiveness of this framework, we applied it to a sequential investment task and found that the risk-sensitive variant provides a significantly better fit to the behavioral data and is consistent with prospect theory. We also found that the risk-sensitive TD error is significantly correlated with BOLD signal change in the ventral striatum, and that risk-sensitive Q-values are significantly correlated with neural activity in the striatum, cingulate cortex, and insula, which is not the case with standard Q-values.",1
"Because reinforcement learning suffers from a lack of scalability, online value (and Q-) function approximation has received increasing interest this last decade. This contribution introduces a novel approximation scheme, namely the Kalman Temporal Differences (KTD) framework, that exhibits the following features: sample-efficiency, non-linear approximation, non-stationarity handling and uncertainty management. A first KTD-based algorithm is provided for deterministic Markov Decision Processes (MDP) which produces biased estimates in the case of stochastic transitions. Than the eXtended KTD framework (XKTD), solving stochastic MDP, is described. Convergence is analyzed for special cases for both deterministic and stochastic transitions. Related algorithms are experimented on classical benchmarks. They compare favorably to the state of the art while exhibiting the announced features.",0
"In recent years, there has been a growing interest in online value (and Q-) function approximation due to the lack of scalability of reinforcement learning. This article presents a new approximation method called the Kalman Temporal Differences (KTD) framework, which offers sample-efficiency, non-linear approximation, non-stationarity handling, and uncertainty management. The KTD algorithm is initially designed for deterministic Markov Decision Processes (MDP), but it produces biased estimates when dealing with stochastic transitions. To address this issue, the eXtended KTD framework (XKTD) is introduced to solve stochastic MDP. The convergence of both deterministic and stochastic transition scenarios is analyzed for special cases. The proposed algorithms are tested on standard benchmarks, demonstrating superior performance compared to the current state of the art while exhibiting the aforementioned features.",1
"Several multiagent reinforcement learning (MARL) algorithms have been proposed to optimize agents decisions. Due to the complexity of the problem, the majority of the previously developed MARL algorithms assumed agents either had some knowledge of the underlying game (such as Nash equilibria) and/or observed other agents actions and the rewards they received.   We introduce a new MARL algorithm called the Weighted Policy Learner (WPL), which allows agents to reach a Nash Equilibrium (NE) in benchmark 2-player-2-action games with minimum knowledge. Using WPL, the only feedback an agent needs is its own local reward (the agent does not observe other agents actions or rewards). Furthermore, WPL does not assume that agents know the underlying game or the corresponding Nash Equilibrium a priori. We experimentally show that our algorithm converges in benchmark two-player-two-action games. We also show that our algorithm converges in the challenging Shapleys game where previous MARL algorithms failed to converge without knowing the underlying game or the NE. Furthermore, we show that WPL outperforms the state-of-the-art algorithms in a more realistic setting of 100 agents interacting and learning concurrently.   An important aspect of understanding the behavior of a MARL algorithm is analyzing the dynamics of the algorithm: how the policies of multiple learning agents evolve over time as agents interact with one another. Such an analysis not only verifies whether agents using a given MARL algorithm will eventually converge, but also reveals the behavior of the MARL algorithm prior to convergence. We analyze our algorithm in two-player-two-action games and show that symbolically proving WPLs convergence is difficult, because of the non-linear nature of WPLs dynamics, unlike previous MARL algorithms that had either linear or piece-wise-linear dynamics. Instead, we numerically solve WPLs dynamics differential equations and compare the solution to the dynamics of previous MARL algorithms.",0
"Numerous MARL algorithms have been proposed to enhance agents' decision-making abilities. However, most of these algorithms have assumed that agents possess some understanding of the game's underlying principles, such as Nash equilibria, or that they can observe other agents' actions and the rewards they receive. Our novel WPL algorithm allows agents to achieve a Nash Equilibrium in two-player-two-action games with minimal knowledge. Only an agent's own local reward is required, and they do not need to know the underlying game or the corresponding Nash Equilibrium beforehand. Our experiments demonstrate that WPL converges in benchmark games and even in the challenging Shapleys game where previous algorithms failed to converge. Furthermore, WPL outperforms state-of-the-art algorithms in a more realistic setting of 100 agents learning concurrently. Analyzing the dynamics of a MARL algorithm is essential to understanding its behavior. We show that WPL's dynamics are non-linear, making symbolic proof of convergence difficult, but we can numerically solve its differential equations and compare them to previous algorithms' dynamics.",1
"Most provably-efficient learning algorithms introduce optimism about poorly-understood states and actions to encourage exploration. We study an alternative approach for efficient exploration, posterior sampling for reinforcement learning (PSRL). This algorithm proceeds in repeated episodes of known duration. At the start of each episode, PSRL updates a prior distribution over Markov decision processes and takes one sample from this posterior. PSRL then follows the policy that is optimal for this sample during the episode. The algorithm is conceptually simple, computationally efficient and allows an agent to encode prior knowledge in a natural way. We establish an $\tilde{O}(\tau S \sqrt{AT})$ bound on the expected regret, where $T$ is time, $\tau$ is the episode length and $S$ and $A$ are the cardinalities of the state and action spaces. This bound is one of the first for an algorithm not based on optimism, and close to the state of the art for any reinforcement learning algorithm. We show through simulation that PSRL significantly outperforms existing algorithms with similar regret bounds.",0
"The majority of learning algorithms that are provably efficient promote exploration by incorporating optimism about poorly understood states and actions. However, we have investigated an alternative technique for efficient exploration called posterior sampling for reinforcement learning (PSRL). This algorithm operates in repeated episodes of known length, where at the beginning of each episode, PSRL adjusts a prior distribution over Markov decision processes and selects one sample from this posterior. During the course of the episode, PSRL follows the policy that is optimal for this sample. The algorithm is straightforward in concept, computationally efficient, and enables an agent to incorporate prior knowledge in a natural manner. We have established an $\tilde{O}(\tau S \sqrt{AT})$ bound on the expected regret, with $T$ representing time, $\tau$ denoting the episode duration, and $S$ and $A$ representing the size of the state and action spaces. This bound is amongst the first for an algorithm not based on optimism and is comparable to the current state of the art for any reinforcement learning algorithm. Through simulation, we have demonstrated that PSRL outperforms existing algorithms with similar regret bounds.",1
"We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.",0
"Introducing the initial deep learning model that can effectively grasp control policies directly from high-dimensional sensory input through reinforcement learning. This model, a convolutional neural network, was trained using a modified form of Q-learning, with raw pixels as input and a value function estimating future rewards as output. We tested our approach on seven Atari 2600 games from the Arcade Learning Environment, without any changes to the architecture or learning algorithm. Our findings indicate that it outperforms all prior methods for six of the games and outdoes a human expert in three of them.",1
"Bayesian model-based reinforcement learning is a formally elegant approach to learning optimal behaviour under model uncertainty, trading off exploration and exploitation in an ideal way. Unfortunately, finding the resulting Bayes-optimal policies is notoriously taxing, since the search space becomes enormous. In this paper we introduce a tractable, sample-based method for approximate Bayes-optimal planning which exploits Monte-Carlo tree search. Our approach outperformed prior Bayesian model-based RL algorithms by a significant margin on several well-known benchmark problems -- because it avoids expensive applications of Bayes rule within the search tree by lazily sampling models from the current beliefs. We illustrate the advantages of our approach by showing it working in an infinite state space domain which is qualitatively out of reach of almost all previous work in Bayesian exploration.",0
"The use of Bayesian model-based reinforcement learning provides a sophisticated means of achieving optimal behavior while accounting for model uncertainty and balancing exploration and exploitation. However, discovering the resulting Bayes-optimal policies can be a daunting task due to the vastness of the search space. To address this issue, we introduce a feasible, sample-based technique for approximating Bayes-optimal planning that employs Monte-Carlo tree search. Our method surpasses previous Bayesian model-based RL algorithms by a significant margin on various well-known benchmark problems, as it avoids costly Bayes rule applications within the search tree by taking advantage of current beliefs and lazy sampling of models. We demonstrate the superiority of our approach by applying it to an infinite state space domain that has been beyond the reach of nearly all prior Bayesian exploration work.",1
"In this paper, we push forward the idea of machine learning systems whose operators can be modified and fine-tuned for each problem. This allows us to propose a learning paradigm where users can write (or adapt) their operators, according to the problem, data representation and the way the information should be navigated. To achieve this goal, data instances, background knowledge, rules, programs and operators are all written in the same functional language, Erlang. Since changing operators affect how the search space needs to be explored, heuristics are learnt as a result of a decision process based on reinforcement learning where each action is defined as a choice of operator and rule. As a result, the architecture can be seen as a 'system for writing machine learning systems' or to explore new operators where the policy reuse (as a kind of transfer learning) is allowed. States and actions are represented in a Q matrix which is actually a table, from which a supervised model is learnt. This makes it possible to have a more flexible mapping between old and new problems, since we work with an abstraction of rules and actions. We include some examples sharing reuse and the application of the system gErl to IQ problems. In order to evaluate gErl, we will test it against some structured problems: a selection of IQ test tasks and some experiments on some structured prediction problems (list patterns).",0
"This paper proposes a learning approach that emphasizes the ability to modify and fine-tune machine learning system operators to address specific problems. Users can adapt their operators, data representation, and information navigation for each problem. To achieve this, data instances, background knowledge, rules, programs, and operators are all written in Erlang, a functional language. Operators impact how the search space is explored, so heuristics are learned using reinforcement learning. The architecture can be viewed as a system for writing and exploring new machine learning systems. States and actions are represented in a Q matrix, which is a table that enables a more flexible mapping between old and new problems. The system gErl is applied to IQ problems as an example. Structured problems, including IQ tests and structured prediction problems, are used to evaluate gErl.",1
"Sophisticated multilayer neural networks have achieved state of the art results on multiple supervised tasks. However, successful applications of such multilayer networks to control have so far been limited largely to the perception portion of the control pipeline. In this paper, we explore the application of deep and recurrent neural networks to a continuous, high-dimensional locomotion task, where the network is used to represent a control policy that maps the state of the system (represented by joint angles) directly to the torques at each joint. By using a recent reinforcement learning algorithm called guided policy search, we can successfully train neural network controllers with thousands of parameters, allowing us to compare a variety of architectures. We discuss the differences between the locomotion control task and previous supervised perception tasks, present experimental results comparing various architectures, and discuss future directions in the application of techniques from deep learning to the problem of optimal control.",0
"Although sophisticated multilayer neural networks have shown to be highly effective in multiple supervised tasks, their successful implementation in control has been limited to the perception aspect of the control pipeline. This paper aims to investigate the potential of utilizing deep and recurrent neural networks in a continuous, high-dimensional locomotion task, where the network serves as a control policy that directly maps the joint angles' state to torques at each joint. Through the application of guided policy search, a recent reinforcement learning algorithm, we can efficiently train neural network controllers with thousands of parameters and compare various architectures. We examine the disparities between locomotion control tasks and earlier supervised perception tasks, present experimental findings that contrast a range of architectures, and explore potential future directions in the use of deep learning techniques for optimal control challenges.",1
"Reinforcement learning has gained wide popularity as a technique for simulation-driven approximate dynamic programming. A less known aspect is that the very reasons that make it effective in dynamic programming can also be leveraged for using it for distributed schemes for certain matrix computations involving non-negative matrices. In this spirit, we propose a reinforcement learning algorithm for PageRank computation that is fashioned after analogous schemes for approximate dynamic programming. The algorithm has the advantage of ease of distributed implementation and more importantly, of being model-free, i.e., not dependent on any specific assumptions about the transition probabilities in the random web-surfer model. We analyze its convergence and finite time behavior and present some supporting numerical experiments.",0
"Reinforcement learning has become popular for simulation-based approximate dynamic programming. However, its potential use for distributed matrix computations involving non-negative matrices is less recognized. Our proposed reinforcement learning algorithm for PageRank computation is based on approximate dynamic programming schemes. One benefit of this algorithm is that it can be easily implemented in a distributed setting. Additionally, it does not depend on specific assumptions about transition probabilities in the random web-surfer model, making it model-free. We have analyzed its convergence and finite time behavior and have conducted numerical experiments to support our findings.",1
"Transferring knowledge across a sequence of reinforcement-learning tasks is challenging, and has a number of important applications. Though there is encouraging empirical evidence that transfer can improve performance in subsequent reinforcement-learning tasks, there has been very little theoretical analysis. In this paper, we introduce a new multi-task algorithm for a sequence of reinforcement-learning tasks when each task is sampled independently from (an unknown) distribution over a finite set of Markov decision processes whose parameters are initially unknown. For this setting, we prove under certain assumptions that the per-task sample complexity of exploration is reduced significantly due to transfer compared to standard single-task algorithms. Our multi-task algorithm also has the desired characteristic that it is guaranteed not to exhibit negative transfer: in the worst case its per-task sample complexity is comparable to the corresponding single-task algorithm.",0
"The transfer of knowledge between reinforcement-learning tasks is a difficult task with important practical applications. While there is evidence that transfer can enhance performance in subsequent tasks, there has been minimal theoretical analysis. This paper presents a new multi-task algorithm for a sequence of reinforcement-learning tasks, where each task is randomly selected from a finite set of Markov decision processes with unknown parameters. Under specific assumptions, we prove that our algorithm significantly reduces the per-task sample complexity of exploration compared to standard single-task algorithms. Additionally, our algorithm ensures that it will not have a negative transfer, and its worst-case per-task sample complexity is comparable to that of a corresponding single-task algorithm.",1
We present a new algorithm for general reinforcement learning where the true environment is known to belong to a finite class of N arbitrary models. The algorithm is shown to be near-optimal for all but O(N log^2 N) time-steps with high probability. Infinite classes are also considered where we show that compactness is a key criterion for determining the existence of uniform sample-complexity bounds. A matching lower bound is given for the finite case.,0
"A novel algorithm for general reinforcement learning is introduced, which assumes that the true environment falls under a finite class of N arbitrary models. The algorithm demonstrates near-optimality for all time-steps, except for O(N log^2 N) with a high likelihood. In addition to finite classes, infinite classes are also examined, and it is established that compactness plays a significant role in determining the presence of uniform sample-complexity bounds. The finite case is supported by a corresponding lower bound.",1
"Learning from prior tasks and transferring that experience to improve future performance is critical for building lifelong learning agents. Although results in supervised and reinforcement learning show that transfer may significantly improve the learning performance, most of the literature on transfer is focused on batch learning tasks. In this paper we study the problem of \textit{sequential transfer in online learning}, notably in the multi-armed bandit framework, where the objective is to minimize the cumulative regret over a sequence of tasks by incrementally transferring knowledge from prior tasks. We introduce a novel bandit algorithm based on a method-of-moments approach for the estimation of the possible tasks and derive regret bounds for it.",0
"The ability to learn from past experiences and apply that knowledge to enhance future performance is crucial in creating agents that can learn for a lifetime. While previous research has demonstrated the potential for transfer learning to improve supervised and reinforcement learning outcomes, most research focuses on batch learning tasks. This study concentrates on the issue of sequential transfer in online learning, particularly in the multi-armed bandit framework. The goal is to minimize regret across a sequence of tasks by gradually transferring knowledge from previous tasks, and a new bandit algorithm based on a method-of-moments approach is proposed. Regret bounds for this algorithm are also derived.",1
"The goal of reinforcement learning (RL) is to let an agent learn an optimal control policy in an unknown environment so that future expected rewards are maximized. The model-free RL approach directly learns the policy based on data samples. Although using many samples tends to improve the accuracy of policy learning, collecting a large number of samples is often expensive in practice. On the other hand, the model-based RL approach first estimates the transition model of the environment and then learns the policy based on the estimated transition model. Thus, if the transition model is accurately learned from a small amount of data, the model-based approach can perform better than the model-free approach. In this paper, we propose a novel model-based RL method by combining a recently proposed model-free policy search method called policy gradients with parameter-based exploration and the state-of-the-art transition model estimator called least-squares conditional density estimation. Through experiments, we demonstrate the practical usefulness of the proposed method.",0
"Reinforcement learning (RL) aims to enable an agent to acquire an optimal control policy in an unfamiliar environment to maximize future expected rewards. The direct method of learning policy using data samples is model-free RL. While using numerous samples can enhance policy learning accuracy, it is often expensive to collect many samples. On the other hand, model-based RL estimates the transition model of the environment before learning the policy based on the estimated transition model. If the transition model is learned accurately from a small amount of data, the model-based approach can outperform the model-free approach. We introduce a novel model-based RL technique that combines policy gradients, a recently suggested model-free policy search method, with parameter-based exploration and the least-squares conditional density estimator, a state-of-the-art transition model estimator. In our experiments, we demonstrate the practical application of the proposed approach.",1
"In some reinforcement learning problems an agent may be provided with a set of input policies, perhaps learned from prior experience or provided by advisors. We present a reinforcement learning with policy advice (RLPA) algorithm which leverages this input set and learns to use the best policy in the set for the reinforcement learning task at hand. We prove that RLPA has a sub-linear regret of \tilde O(\sqrt{T}) relative to the best input policy, and that both this regret and its computational complexity are independent of the size of the state and action space. Our empirical simulations support our theoretical analysis. This suggests RLPA may offer significant advantages in large domains where some prior good policies are provided.",0
"An agent in certain reinforcement learning scenarios may be given a collection of input policies, which could be obtained from previous experience or supplied by advisors. Our proposed algorithm, known as reinforcement learning with policy advice (RLPA), utilizes this input set to learn and apply the most effective policy for the current reinforcement learning task. Our research demonstrates that RLPA has a sub-linear regret of approximately O(\sqrt{T}) compared to the top input policy, and that the regret and computational complexity remain unaffected by the scope of the state and action space. Our empirical testing confirms our theoretical analysis, indicating that RLPA may provide significant advantages in extensive domains where some high-quality policies already exist.",1
"We consider the problem of learning by demonstration from agents acting in unknown stochastic Markov environments or games. Our aim is to estimate agent preferences in order to construct improved policies for the same task that the agents are trying to solve. To do so, we extend previous probabilistic approaches for inverse reinforcement learning in known MDPs to the case of unknown dynamics or opponents. We do this by deriving two simplified probabilistic models of the demonstrator's policy and utility. For tractability, we use maximum a posteriori estimation rather than full Bayesian inference. Under a flat prior, this results in a convex optimisation problem. We find that the resulting algorithms are highly competitive against a variety of other methods for inverse reinforcement learning that do have knowledge of the dynamics.",0
"Our focus is on acquiring knowledge through demonstration by agents in stochastic Markov environments or games that are unfamiliar. Our primary objective is to establish better policies for the same task that these agents are attempting to complete by assessing their preferences. We expand on earlier probabilistic methods for inverse reinforcement learning in known MDPs to account for unknown dynamics or opponents. We accomplish this by developing two simplified probabilistic models of the demonstrator's policy and utility. To simplify the process, we employ maximum a posteriori estimation instead of full Bayesian inference. Under a flat prior, this leads to a convex optimization problem. We discovered that the resulting algorithms are highly competitive compared to other inverse reinforcement learning techniques that have knowledge of the dynamics.",1
"This paper introduces a simple, general framework for likelihood-free Bayesian reinforcement learning, through Approximate Bayesian Computation (ABC). The main advantage is that we only require a prior distribution on a class of simulators (generative models). This is useful in domains where an analytical probabilistic model of the underlying process is too complex to formulate, but where detailed simulation models are available. ABC-RL allows the use of any Bayesian reinforcement learning technique, even in this case. In addition, it can be seen as an extension of rollout algorithms to the case where we do not know what the correct model to draw rollouts from is. We experimentally demonstrate the potential of this approach in a comparison with LSPI. Finally, we introduce a theorem showing that ABC is a sound methodology in principle, even when non-sufficient statistics are used.",0
"A new framework for likelihood-free Bayesian reinforcement learning is presented in this paper, which utilizes Approximate Bayesian Computation (ABC). The framework is simple and applicable to a wide range of scenarios, as it only requires a prior distribution on a specific group of simulators (generative models). This is particularly useful when the underlying process is too complex to formulate an analytical probabilistic model, but detailed simulation models are available. The ABC-RL technique is compatible with any Bayesian reinforcement learning method, making it a versatile option. It can also be seen as an extension of rollout algorithms, but for cases where the correct model for rollouts is unknown. The potential of this approach is demonstrated in a comparison with LSPI through experimentation. The paper concludes by introducing a theorem that proves the soundness of the ABC methodology, even when non-sufficient statistics are used.",1
"We consider large-scale Markov decision processes (MDPs) with parameter uncertainty, under the robust MDP paradigm. Previous studies showed that robust MDPs, based on a minimax approach to handle uncertainty, can be solved using dynamic programming for small to medium sized problems. However, due to the ""curse of dimensionality"", MDPs that model real-life problems are typically prohibitively large for such approaches. In this work we employ a reinforcement learning approach to tackle this planning problem: we develop a robust approximate dynamic programming method based on a projected fixed point equation to approximately solve large scale robust MDPs. We show that the proposed method provably succeeds under certain technical conditions, and demonstrate its effectiveness through simulation of an option pricing problem. To the best of our knowledge, this is the first attempt to scale up the robust MDPs paradigm.",0
"Our focus is on the consideration of Markov decision processes (MDPs) of a large scale, which are subject to parameter uncertainty and analyzed under the robust MDP paradigm. Prior research has indicated that dynamic programming can be utilized to solve robust MDPs for smaller problems via a minimax approach. However, due to the ""curse of dimensionality,"" MDPs that represent real-life problems are typically too large for such an approach. To address this issue, we employ a reinforcement learning approach and introduce a robust approximate dynamic programming method based on a projected fixed point equation to address large-scale robust MDPs. We demonstrate the effectiveness of this method through simulations of an option pricing problem, and it is the first attempt to scale up the robust MDPs paradigm, proven to be successful under certain technical conditions.",1
"This paper presents the first actor-critic algorithm for off-policy reinforcement learning. Our algorithm is online and incremental, and its per-time-step complexity scales linearly with the number of learned weights. Previous work on actor-critic algorithms is limited to the on-policy setting and does not take advantage of the recent advances in off-policy gradient temporal-difference learning. Off-policy techniques, such as Greedy-GQ, enable a target policy to be learned while following and obtaining data from another (behavior) policy. For many problems, however, actor-critic methods are more practical than action value methods (like Greedy-GQ) because they explicitly represent the policy; consequently, the policy can be stochastic and utilize a large action space. In this paper, we illustrate how to practically combine the generality and learning potential of off-policy learning with the flexibility in action selection given by actor-critic methods. We derive an incremental, linear time and space complexity algorithm that includes eligibility traces, prove convergence under assumptions similar to previous off-policy algorithms, and empirically show better or comparable performance to existing algorithms on standard reinforcement-learning benchmark problems.",0
"The article introduces an actor-critic algorithm for off-policy reinforcement learning, which is the first of its kind. It is an online and incremental algorithm whose per-time-step complexity has a linear relationship with the number of learned weights. Previous approaches to actor-critic algorithms only work in the on-policy setting and do not make use of the recent advancements in off-policy gradient temporal-difference learning. Off-policy techniques such as Greedy-GQ make it possible to learn a target policy while gathering data from a behavior policy. However, actor-critic methods are more useful than action value methods like Greedy-GQ in many cases because they can explicitly represent the policy, allowing for a stochastic policy and a larger action space. The article demonstrates how to combine the generality and learning potential of off-policy learning with the flexibility of action selection offered by actor-critic methods. The authors derive an incremental algorithm with a linear time and space complexity that includes eligibility traces, proves convergence under similar assumptions as previous off-policy algorithms, and demonstrates better or comparable performance to existing algorithms on standard reinforcement-learning benchmark problems.",1
"Local Policy Search is a popular reinforcement learning approach for handling large state spaces. Formally, it searches locally in a paramet erized policy space in order to maximize the associated value function averaged over some predefined distribution. It is probably commonly b elieved that the best one can hope in general from such an approach is to get a local optimum of this criterion. In this article, we show th e following surprising result: \emph{any} (approximate) \emph{local optimum} enjoys a \emph{global performance guarantee}. We compare this g uarantee with the one that is satisfied by Direct Policy Iteration, an approximate dynamic programming algorithm that does some form of Poli cy Search: if the approximation error of Local Policy Search may generally be bigger (because local search requires to consider a space of s tochastic policies), we argue that the concentrability coefficient that appears in the performance bound is much nicer. Finally, we discuss several practical and theoretical consequences of our analysis.",0
"Local Policy Search is a widely used method in reinforcement learning to handle large state spaces. It involves searching for a parametrized policy space locally to maximize the value function associated with a predefined distribution. It is commonly believed that this method only leads to local optima. However, this article presents a surprising finding that any approximate local optimum guarantees global performance. We compare this guarantee with Direct Policy Iteration, which also uses Policy Search, but Local Policy Search has a better concentrability coefficient in the performance bound. Finally, we discuss the practical and theoretical implications of this analysis.",1
"We consider the problem of energy-efficient point-to-point transmission of delay-sensitive data (e.g. multimedia data) over a fading channel. Existing research on this topic utilizes either physical-layer centric solutions, namely power-control and adaptive modulation and coding (AMC), or system-level solutions based on dynamic power management (DPM); however, there is currently no rigorous and unified framework for simultaneously utilizing both physical-layer centric and system-level techniques to achieve the minimum possible energy consumption, under delay constraints, in the presence of stochastic and a priori unknown traffic and channel conditions. In this report, we propose such a framework. We formulate the stochastic optimization problem as a Markov decision process (MDP) and solve it online using reinforcement learning. The advantages of the proposed online method are that (i) it does not require a priori knowledge of the traffic arrival and channel statistics to determine the jointly optimal power-control, AMC, and DPM policies; (ii) it exploits partial information about the system so that less information needs to be learned than when using conventional reinforcement learning algorithms; and (iii) it obviates the need for action exploration, which severely limits the adaptation speed and run-time performance of conventional reinforcement learning algorithms. Our results show that the proposed learning algorithms can converge up to two orders of magnitude faster than a state-of-the-art learning algorithm for physical layer power-control and up to three orders of magnitude faster than conventional reinforcement learning algorithms.",0
"The transmission of delay-sensitive data over a fading channel while minimizing energy consumption is a problem we address in this report. Previous research has focused on physical-layer solutions such as power-control and adaptive modulation and coding (AMC), or system-level solutions based on dynamic power management (DPM). However, there is currently no comprehensive framework that combines both physical-layer and system-level techniques to minimize energy consumption while meeting delay constraints in the presence of stochastic and unknown traffic and channel conditions. Our proposed framework addresses this issue by formulating the stochastic optimization problem as a Markov decision process (MDP) and using reinforcement learning to solve it online. Our online method is advantageous because it does not require prior knowledge of traffic arrival and channel statistics, it requires less information to be learned than conventional reinforcement learning algorithms, and it eliminates the need for action exploration. Our results show that our learning algorithms can converge up to two orders of magnitude faster than state-of-the-art learning algorithms for physical layer power-control and up to three orders of magnitude faster than conventional reinforcement learning algorithms.",1
"We propose a reinforcement learning solution to the \emph{soccer dribbling task}, a scenario in which a soccer agent has to go from the beginning to the end of a region keeping possession of the ball, as an adversary attempts to gain possession. While the adversary uses a stationary policy, the dribbler learns the best action to take at each decision point. After defining meaningful variables to represent the state space, and high-level macro-actions to incorporate domain knowledge, we describe our application of the reinforcement learning algorithm \emph{Sarsa} with CMAC for function approximation. Our experiments show that, after the training period, the dribbler is able to accomplish its task against a strong adversary around 58% of the time.",0
"Our proposal involves using reinforcement learning to solve the ""soccer dribbling task"". This involves a soccer player navigating through a region while maintaining possession of the ball, while an opponent tries to take it away. The opponent uses a fixed strategy, while the player learns the best actions to take at each decision point. We created relevant variables to represent the state space and developed macro-actions based on domain knowledge. Our approach utilized the Sarsa reinforcement learning algorithm with CMAC for function approximation. Our experimental results demonstrate that, following training, the player successfully completed the task against a formidable opponent approximately 58% of the time.",1
"Stochastic neurons can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic neurons, i.e., can we ""back-propagate"" through these stochastic neurons? We examine this question, existing approaches, and present two novel families of solutions, applicable in different settings. In particular, it is demonstrated that a simple biologically plausible formula gives rise to an an unbiased (but noisy) estimator of the gradient with respect to a binary stochastic neuron firing probability. Unlike other estimators which view the noise as a small perturbation in order to estimate gradients by finite differences, this estimator is unbiased even without assuming that the stochastic perturbation is small. This estimator is also interesting because it can be applied in very general settings which do not allow gradient back-propagation, including the estimation of the gradient with respect to future rewards, as required in reinforcement learning setups. We also propose an approach to approximating this unbiased but high-variance estimator by learning to predict it using a biased estimator. The second approach we propose assumes that an estimator of the gradient can be back-propagated and it provides an unbiased estimator of the gradient, but can only work with non-linearities unlike the hard threshold, but like the rectifier, that are not flat for all of their range. This is similar to traditional sigmoidal units but has the advantage that for many inputs, a hard decision (e.g., a 0 output) can be produced, which would be convenient for conditional computation and achieving sparse representations and sparse gradients.",0
"Stochastic neurons are beneficial in deep learning models, but they present a challenge in estimating the gradient of a loss function with respect to their input. This is known as ""back-propagation"" through stochastic neurons. We investigate this issue and propose two new solutions that can be applied in different settings. One solution uses a biologically plausible formula to generate an unbiased but noisy estimator of the gradient with respect to binary stochastic neuron firing probability. This estimator is unique because it remains unbiased even when the stochastic perturbation is not small. It can be applied in a variety of settings, including reinforcement learning setups. Another approach approximates the unbiased estimator using a biased estimator. The second approach assumes that the gradient estimator can be back-propagated and provides an unbiased estimator of the gradient, but only works with non-linearities that are not flat for all of their range. This approach is similar to sigmoidal units but has the advantage of producing a hard decision in many cases, which is useful for achieving sparse representations and gradients.",1
"We study the problem of adaptive control of a high dimensional linear quadratic (LQ) system. Previous work established the asymptotic convergence to an optimal controller for various adaptive control schemes. More recently, for the average cost LQ problem, a regret bound of ${O}(\sqrt{T})$ was shown, apart form logarithmic factors. However, this bound scales exponentially with $p$, the dimension of the state space. In this work we consider the case where the matrices describing the dynamic of the LQ system are sparse and their dimensions are large. We present an adaptive control scheme that achieves a regret bound of ${O}(p \sqrt{T})$, apart from logarithmic factors. In particular, our algorithm has an average cost of $(1+\eps)$ times the optimum cost after $T = \polylog(p) O(1/\eps^2)$. This is in comparison to previous work on the dense dynamics where the algorithm requires time that scales exponentially with dimension in order to achieve regret of $\eps$ times the optimal cost.   We believe that our result has prominent applications in the emerging area of computational advertising, in particular targeted online advertising and advertising in social networks.",0
"Our focus is on the adaptive control of high dimensional linear quadratic (LQ) systems. Previous research has demonstrated the asymptotic convergence to an optimal controller for various adaptive control strategies. Recent studies have shown that for the average cost LQ problem, the regret bound is ${O}(\sqrt{T})$, excluding logarithmic factors. However, this bound grows exponentially with $p$, which is the dimension of the state space. In this study, we examine the case where the LQ system's dynamic descriptors are sparse, and their dimensions are extensive. We present an adaptive control approach that achieves a regret bound of ${O}(p \sqrt{T})$, excluding logarithmic factors. Specifically, our algorithm achieves an average cost of $(1+\eps)$ times the optimal cost after $T = \polylog(p) O(1/\eps^2)$. This is in contrast to prior work on dense dynamics, where the algorithm needs time that scales exponentially with dimension to attain regret of $\eps$ times the optimal cost. We believe our results have significant implications for computational advertising, especially for targeted online advertising and advertising in social networks.",1
"We study the use of inverse reinforcement learning (IRL) as a tool for the recognition of agents' behavior on the basis of observation of their sequential decision behavior interacting with the environment. We model the problem faced by the agents as a Markov decision process (MDP) and model the observed behavior of the agents in terms of forward planning for the MDP. We use IRL to learn reward functions and then use these reward functions as the basis for clustering or classification models. Experimental studies with GridWorld, a navigation problem, and the secretary problem, an optimal stopping problem, suggest reward vectors found from IRL can be a good basis for behavior pattern recognition problems. Empirical comparisons of our method with several existing IRL algorithms and with direct methods that use feature statistics observed in state-action space suggest it may be superior for recognition problems.",0
"As a means of recognizing agents' behavior through observation of their interaction with the environment, we examine the use of inverse reinforcement learning (IRL). We represent agents' dilemmas as Markov decision processes (MDPs) and describe their observed behavior in terms of forward planning for the MDP. We apply IRL to learn reward functions and employ them as the foundation for clustering or classification models. Our research, involving GridWorld, a navigation challenge, and the secretary problem, an optimal stopping dilemma, reveals that IRL-derived reward vectors can be an effective basis for identifying behavior patterns. When compared empirically to multiple existing IRL algorithms and direct methods utilizing feature statistics from state-action space, our approach demonstrates superior performance in recognition problems.",1
"We consider an agent interacting with an environment in a single stream of actions, observations, and rewards, with no reset. This process is not assumed to be a Markov Decision Process (MDP). Rather, the agent has several representations (mapping histories of past interactions to a discrete state space) of the environment with unknown dynamics, only some of which result in an MDP. The goal is to minimize the average regret criterion against an agent who knows an MDP representation giving the highest optimal reward, and acts optimally in it. Recent regret bounds for this setting are of order $O(T^{2/3})$ with an additive term constant yet exponential in some characteristics of the optimal MDP. We propose an algorithm whose regret after $T$ time steps is $O(\sqrt{T})$, with all constants reasonably small. This is optimal in $T$ since $O(\sqrt{T})$ is the optimal regret in the setting of learning in a (single discrete) MDP.",0
"In this scenario, an agent interacts with an environment through a continuous stream of actions, observations, and rewards without any reset. The process does not assume a Markov Decision Process (MDP). Instead, the agent has multiple representations of the environment that map past interactions to a discrete state space, some of which may result in an MDP. The objective is to minimize the average regret compared to an agent who knows the MDP representation with the highest optimal reward and acts accordingly. Recent regret bounds for this situation are of the order of $O(T^{2/3})$, with an exponential additive term in some optimal MDP characteristics. To address this, we present an algorithm that achieves a regret of $O(\sqrt{T})$ after $T$ time steps, with reasonably small constants. This is optimal in terms of $T$, as $O(\sqrt{T})$ is the optimal regret in the context of learning in a single discrete MDP.",1
"One of the efficient solutions of improving coverage and increasing capacity in cellular networks is the deployment of femtocells. As the cellular networks are becoming more complex, energy consumption of whole network infrastructure is becoming important in terms of both operational costs and environmental impacts. This paper investigates energy efficiency of two-tier femtocell networks through combining game theory and stochastic learning. With the Stackelberg game formulation, a hierarchical reinforcement learning framework is applied for studying the joint expected utility maximization of macrocells and femtocells subject to the minimum signal-to-interference-plus-noise-ratio requirements. In the learning procedure, the macrocells act as leaders and the femtocells are followers. At each time step, the leaders commit to dynamic strategies based on the best responses of the followers, while the followers compete against each other with no further information but the leaders' transmission parameters. In this paper, we propose two reinforcement learning based intelligent algorithms to schedule each cell's stochastic power levels. Numerical experiments are presented to validate the investigations. The results show that the two learning algorithms substantially improve the energy efficiency of the femtocell networks.",0
"The deployment of femtocells is an efficient solution for enhancing coverage and capacity in cellular networks. As cellular networks become increasingly complex, the energy consumption of the entire network infrastructure is becoming a crucial factor, impacting both operational expenses and the environment. This study explores the energy efficiency of two-tier femtocell networks through a combination of game theory and stochastic learning. The hierarchical reinforcement learning framework applies a Stackelberg game formulation to evaluate the joint expected utility maximization of macrocells and femtocells, while adhering to the minimum signal-to-interference-plus-noise-ratio requirements. In this framework, macrocells act as leaders, and femtocells are followers. The leaders determine dynamic strategies based on the followers' best responses, while the followers compete with each other without any additional information except for the leaders' transmission parameters. This paper proposes two reinforcement learning-based intelligent algorithms to schedule the stochastic power levels of each cell. The numerical experiments confirm the findings, showing that the two learning algorithms significantly improve the energy efficiency of femtocell networks.",1
"This paper introduces a set of algorithms for Monte-Carlo Bayesian reinforcement learning. Firstly, Monte-Carlo estimation of upper bounds on the Bayes-optimal value function is employed to construct an optimistic policy. Secondly, gradient-based algorithms for approximate upper and lower bounds are introduced. Finally, we introduce a new class of gradient algorithms for Bayesian Bellman error minimisation. We theoretically show that the gradient methods are sound. Experimentally, we demonstrate the superiority of the upper bound method in terms of reward obtained. However, we also show that the Bayesian Bellman error method is a close second, despite its significant computational simplicity.",0
"The aim of this paper is to present a range of algorithms for Monte-Carlo Bayesian reinforcement learning. To begin with, an optimistic policy is generated using Monte-Carlo estimation of upper bounds on the Bayes-optimal value function. Next, we introduce gradient-based algorithms that provide approximate upper and lower bounds. Lastly, we present a new category of gradient algorithms that focus on Bayesian Bellman error minimisation. The validity of the gradient methods is established through theoretical analysis. We also conduct experiments to compare the performance of the upper bound and Bayesian Bellman error methods. Results indicate that the upper bound method is superior in terms of reward achieved. Nonetheless, the Bayesian Bellman error method emerges as a close contender, despite its considerably simpler computational requirements.",1
"In adaptive dynamic programming, neurocontrol and reinforcement learning, the objective is for an agent to learn to choose actions so as to minimise a total cost function. In this paper we show that when discretized time is used to model the motion of the agent, it can be very important to do ""clipping"" on the motion of the agent in the final time step of the trajectory. By clipping we mean that the final time step of the trajectory is to be truncated such that the agent stops exactly at the first terminal state reached, and no distance further. We demonstrate that when clipping is omitted, learning performance can fail to reach the optimum; and when clipping is done properly, learning performance can improve significantly.   The clipping problem we describe affects algorithms which use explicit derivatives of the model functions of the environment to calculate a learning gradient. These include Backpropagation Through Time for Control, and methods based on Dual Heuristic Dynamic Programming. However the clipping problem does not significantly affect methods based on Heuristic Dynamic Programming, Temporal Differences or Policy Gradient Learning algorithms. Similarly, the clipping problem does not affect fixed-length finite-horizon problems.",0
"The aim of adaptive dynamic programming, neurocontrol, and reinforcement learning is for an agent to select actions that minimize a total cost function. When employing discrete time to model the agent's movement, it is crucial to perform ""clipping"" on the agent's motion in the final time step of the trajectory. Clipping means that the final time step of the trajectory is truncated so that the agent stops precisely at the first terminal state reached and no farther. This paper demonstrates that failure to clip can prevent learning performance from reaching the optimum, whereas proper clipping can significantly enhance learning performance. The clipping issue primarily impacts algorithms that use explicit derivatives of the model functions of the environment to calculate a learning gradient, such as Backpropagation Through Time for Control and Dual Heuristic Dynamic Programming. However, the clipping problem does not significantly affect methods based on Heuristic Dynamic Programming, Temporal Differences, or Policy Gradient Learning algorithms. Additionally, the clipping problem does not impact fixed-length finite-horizon problems.",1
"The problem of selecting the right state-representation in a reinforcement learning problem is considered. Several models (functions mapping past observations to a finite set) of the observations are given, and it is known that for at least one of these models the resulting state dynamics are indeed Markovian. Without knowing neither which of the models is the correct one, nor what are the probabilistic characteristics of the resulting MDP, it is required to obtain as much reward as the optimal policy for the correct model (or for the best of the correct models, if there are several). We propose an algorithm that achieves that, with a regret of order T^{2/3} where T is the horizon time.",0
"The challenge of choosing the appropriate state-representation in reinforcement learning is being examined. Numerous models are presented, each of which maps past observations to a set of finite values, and it is established that at least one of these models yields Markovian state dynamics. Despite lacking knowledge of which model is accurate or the probabilistic characteristics of the resulting MDP, the aim is to obtain maximum reward comparable to the optimal policy for the correct model (or the most suitable of multiple correct models). Our proposed algorithm accomplishes this with a regret of T^{2/3}, where T is the time horizon.",1
"We derive sublinear regret bounds for undiscounted reinforcement learning in continuous state space. The proposed algorithm combines state aggregation with the use of upper confidence bounds for implementing optimism in the face of uncertainty. Beside the existence of an optimal policy which satisfies the Poisson equation, the only assumptions made are Holder continuity of rewards and transition probabilities.",0
Sublinear regret bounds are obtained for continuous state space undiscounted reinforcement learning. The algorithm suggested uses state aggregation and upper confidence bounds to maintain optimism when dealing with uncertainty. The only requirements are the Poisson equation's optimal policy and the Holder continuity of transition probabilities and rewards.,1
"This paper addresses the problem of learning a task from demonstration. We adopt the framework of inverse reinforcement learning, where tasks are represented in the form of a reward function. Our contribution is a novel active learning algorithm that enables the learning agent to query the expert for more informative demonstrations, thus leading to more sample-efficient learning. For this novel algorithm (Generalized Binary Search for Inverse Reinforcement Learning, or GBS-IRL), we provide a theoretical bound on sample complexity and illustrate its applicability on several different tasks. To our knowledge, GBS-IRL is the first active IRL algorithm with provable sample complexity bounds. We also discuss our method in light of other existing methods in the literature and its general applicability in multi-class classification problems. Finally, motivated by recent work on learning from demonstration in robots, we also discuss how different forms of human feedback can be integrated in a transparent manner in our learning framework.",0
"The issue of learning a task from a demonstration is explored in this paper, utilizing the framework of inverse reinforcement learning. A reward function represents tasks, and our contribution is a new active learning algorithm that enables the learner to request more informative demonstrations, resulting in more efficient learning. We provide a theoretical sample complexity limit for our novel algorithm, Generalized Binary Search for Inverse Reinforcement Learning (GBS-IRL), and demonstrate its usefulness in various tasks. As far as we know, GBS-IRL is the first active IRL algorithm with provable sample complexity limits. We also discuss our approach in comparison to other existing methods in the literature and its applicability in multi-class classification problems. Finally, we examine how different types of human feedback can be seamlessly integrated into our learning framework, motivated by recent research on learning from demonstration in robots.",1
"We present new algorithms for inverse reinforcement learning (IRL, or inverse optimal control) in convex optimization settings. We argue that finite-space IRL can be posed as a convex quadratic program under a Bayesian inference framework with the objective of maximum a posterior estimation. To deal with problems in large or even infinite state space, we propose a Gaussian process model and use preference graphs to represent observations of decision trajectories. Our method is distinguished from other approaches to IRL in that it makes no assumptions about the form of the reward function and yet it retains the promise of computationally manageable implementations for potential real-world applications. In comparison with an establish algorithm on small-scale numerical problems, our method demonstrated better accuracy in apprenticeship learning and a more robust dependence on the number of observations.",0
"Our study introduces novel algorithms for inverse reinforcement learning (IRL), also known as inverse optimal control, in settings of convex optimization. We propose that finite-space IRL can be formulated as a convex quadratic program utilizing a Bayesian inference framework that aims for maximum a posteriori estimation. To tackle issues related to large or infinite state spaces, we suggest a Gaussian process model and preference graphs to represent decision trajectory observations. Our approach does not make any assumptions about the reward function's shape and offers computationally feasible implementations for practical real-world applications. When compared to established algorithms for small-scale numerical problems, our method exhibited superior accuracy in apprenticeship learning and stronger reliance on the number of observations.",1
"The policy gradient approach is a flexible and powerful reinforcement learning method particularly for problems with continuous actions such as robot control. A common challenge in this scenario is how to reduce the variance of policy gradient estimates for reliable policy updates. In this paper, we combine the following three ideas and give a highly effective policy gradient method: (a) the policy gradients with parameter based exploration, which is a recently proposed policy search method with low variance of gradient estimates, (b) an importance sampling technique, which allows us to reuse previously gathered data in a consistent way, and (c) an optimal baseline, which minimizes the variance of gradient estimates with their unbiasedness being maintained. For the proposed method, we give theoretical analysis of the variance of gradient estimates and show its usefulness through extensive experiments.",0
"The policy gradient approach is a reinforcement learning method that is capable of handling problems with continuous actions, such as robot control. However, reducing the variance of policy gradient estimates is a significant challenge in this scenario to ensure reliable policy updates. In this study, we introduce a highly effective policy gradient method by combining three ideas: (a) policy gradients with parameter-based exploration, which is a recently proposed policy search method that offers low variance of gradient estimates, (b) an importance sampling technique that enables us to reuse previously gathered data in a consistent manner, and (c) an optimal baseline that minimizes the variance of gradient estimates while maintaining their unbiasedness. We provide theoretical analysis of the variance of gradient estimates for the proposed method and demonstrate its usefulness through extensive experiments.",1
"There exist a number of reinforcement learning algorithms which learnby climbing the gradient of expected reward. Their long-runconvergence has been proved, even in partially observableenvironments with non-deterministic actions, and without the need fora system model. However, the variance of the gradient estimator hasbeen found to be a significant practical problem. Recent approacheshave discounted future rewards, introducing a bias-variance trade-offinto the gradient estimate. We incorporate a reward baseline into thelearning system, and show that it affects variance without introducingfurther bias. In particular, as we approach the zero-bias,high-variance parameterization, the optimal (or variance minimizing)constant reward baseline is equal to the long-term average expectedreward. Modified policy-gradient algorithms are presented, and anumber of experiments demonstrate their improvement over previous work.",0
"Numerous reinforcement learning algorithms exist that utilize the gradient of expected reward to learn. These algorithms have been proven to achieve long-term convergence, even in partially observable environments with non-deterministic actions, and without requiring a model of the system. However, the gradient estimator's variance has been identified as a significant practical challenge. Recent approaches have introduced a bias-variance trade-off by discounting future rewards. Our approach involves incorporating a reward baseline into the learning system, which reduces variance without introducing additional bias. As we approach the parameterization with high variance and low bias, the optimal constant reward baseline is equal to the long-term average expected reward. We present modified policy-gradient algorithms and demonstrate their improvement over previous work through various experiments.",1
"Reinforcement learning would enjoy better success on real-world problems if domain knowledge could be imparted to the algorithm by the modelers. Most problems have both hidden state and unknown dynamics. Partially observable Markov decision processes (POMDPs) allow for the modeling of both. Unfortunately, they do not provide a natural framework in which to specify knowledge about the domain dynamics. The designer must either admit to knowing nothing about the dynamics or completely specify the dynamics (thereby turning it into a planning problem). We propose a new framework called a partially known Markov decision process (PKMDP) which allows the designer to specify known dynamics while still leaving portions of the environment s dynamics unknown.The model represents NOT ONLY the environment dynamics but also the agents knowledge of the dynamics. We present a reinforcement learning algorithm for this model based on importance sampling. The algorithm incorporates planning based on the known dynamics and learning about the unknown dynamics. Our results clearly demonstrate the ability to add domain knowledge and the resulting benefits for learning.",0
"To increase the effectiveness of reinforcement learning in solving real-world problems, it would be advantageous for modelers to provide domain knowledge to the algorithm. Although partially observable Markov decision processes (POMDPs) can model problems with both hidden state and unknown dynamics, they do not offer a natural way to specify domain knowledge. This means that designers must either admit to having no knowledge about the dynamics or completely specify them, which turns it into a planning problem. To address this issue, we propose a new framework called a partially known Markov decision process (PKMDP) that enables designers to specify known dynamics while leaving some portions of the environment's dynamics unknown. Our model represents not only the environment dynamics but also the agent's knowledge of the dynamics. We have developed a reinforcement learning algorithm based on importance sampling that incorporates planning based on the known dynamics and learning about the unknown dynamics. Our results demonstrate the effectiveness of incorporating domain knowledge and the resulting benefits for learning.",1
"Most reinforcement learning methods operate on propositional representations of the world state. Such representations are often intractably large and generalize poorly. Using a deictic representation is believed to be a viable alternative: they promise generalization while allowing the use of existing reinforcement-learning methods. Yet, there are few experiments on learning with deictic representations reported in the literature. In this paper we explore the effectiveness of two forms of deictic representation and a na\""{i}ve propositional representation in a simple blocks-world domain. We find, empirically, that the deictic representations actually worsen learning performance. We conclude with a discussion of possible causes of these results and strategies for more effective learning in domains with objects.",0
"Most reinforcement learning techniques utilize propositional representations of the state of the world. However, these representations are often too large and do not generalize well. An alternative approach is to use deictic representations, which allow for generalization and the use of existing reinforcement learning methods. Despite their potential, there is limited research on learning with deictic representations. This study examines the effectiveness of two types of deictic representation and a basic propositional representation in a simple blocks-world domain. The results show that, contrary to expectations, the deictic representations actually hinder learning performance. The paper concludes with a discussion of possible reasons for these findings and suggestions for improving learning in object-oriented domains.",1
"To recognize an object in an image, the user must apply a combination of operators, where each operator has a set of parameters. These parameters must be well adjusted in order to reach good results. Usually, this adjustment is made manually by the user. In this paper we propose a new method to automate the process of parameter adjustment for an object recognition task. Our method is based on reinforcement learning, we use two types of agents: User Agent that gives the necessary information and Parameter Agent that adjusts the parameters of each operator. Due to the nature of reinforcement learning the results do not depend only on the system characteristics but also on the user favorite choices.",0
"In order to identify an object in an image, the user must utilize a combination of operators, each of which has its own set of parameters that must be finely tuned to achieve optimal results. Typically, this tuning process is performed manually by the user. However, this paper introduces a new approach to automating parameter adjustment for object recognition using reinforcement learning. The method employs two types of agents: a User Agent that provides necessary information, and a Parameter Agent that adjusts operator parameters. As a result of using reinforcement learning, the outcome is influenced not only by system characteristics, but also by user preferences.",1
"We consider the inverse reinforcement learning problem, that is, the problem of learning from, and then predicting or mimicking a controller based on state/action data. We propose a statistical model for such data, derived from the structure of a Markov decision process. Adopting a Bayesian approach to inference, we show how latent variables of the model can be estimated, and how predictions about actions can be made, in a unified framework. A new Markov chain Monte Carlo (MCMC) sampler is devised for simulation from the posterior distribution. This step includes a parameter expansion step, which is shown to be essential for good convergence properties of the MCMC sampler. As an illustration, the method is applied to learning a human controller.",0
"The problem we are addressing is inverse reinforcement learning, which involves learning from and then imitating a controller using state/action data. We have developed a statistical model for this data that is based on the structure of a Markov decision process. Using a Bayesian approach, we can estimate the model's latent variables and make predictions about actions within a unified framework. To simulate from the posterior distribution, we have created a new Markov chain Monte Carlo (MCMC) sampler that incorporates a parameter expansion step crucial for achieving good convergence properties. We demonstrate the method's effectiveness by applying it to learning a human controller.",1
"The paper considers a class of multi-agent Markov decision processes (MDPs), in which the network agents respond differently (as manifested by the instantaneous one-stage random costs) to a global controlled state and the control actions of a remote controller. The paper investigates a distributed reinforcement learning setup with no prior information on the global state transition and local agent cost statistics. Specifically, with the agents' objective consisting of minimizing a network-averaged infinite horizon discounted cost, the paper proposes a distributed version of $Q$-learning, $\mathcal{QD}$-learning, in which the network agents collaborate by means of local processing and mutual information exchange over a sparse (possibly stochastic) communication network to achieve the network goal. Under the assumption that each agent is only aware of its local online cost data and the inter-agent communication network is \emph{weakly} connected, the proposed distributed scheme is almost surely (a.s.) shown to yield asymptotically the desired value function and the optimal stationary control policy at each network agent. The analytical techniques developed in the paper to address the mixed time-scale stochastic dynamics of the \emph{consensus + innovations} form, which arise as a result of the proposed interactive distributed scheme, are of independent interest.",0
"The article explores a type of multi-agent Markov decision process (MDP) where the agents react differently to a global state and the actions of a remote controller, as evidenced by their random one-stage costs. The goal is to develop a distributed reinforcement learning approach that doesn't rely on prior knowledge of the global state transition or local agent cost statistics. To achieve the objective of minimizing the network-averaged infinite horizon discounted cost, the paper proposes a collaborative approach using $\mathcal{QD}$-learning, in which agents share information over a sparse communication network. The scheme is proven to be effective under the assumption that each agent only has access to its local cost data and the communication network is weakly connected. The analytical methods developed to address the mixed time-scale stochastic dynamics of the consensus + innovations form are of independent interest.",1
"In 1950, Forsythe and Leibler (1950) introduced a statistical technique for finding the inverse of a matrix by characterizing the elements of the matrix inverse as expected values of a sequence of random walks. Barto and Duff (1994) subsequently showed relations between this technique and standard dynamic programming and temporal differencing methods. The advantage of the Monte Carlo matrix inversion (MCMI) approach is that it scales better with respect to state-space size than alternative techniques. In this paper, we introduce an algorithm for performing reinforcement learning policy evaluation using MCMI. We demonstrate that MCMI improves on runtime over a maximum likelihood model-based policy evaluation approach and on both runtime and accuracy over the temporal differencing (TD) policy evaluation approach. We further improve on MCMI policy evaluation by adding an importance sampling technique to our algorithm to reduce the variance of our estimator. Lastly, we illustrate techniques for scaling up MCMI to large state spaces in order to perform policy improvement.",0
"Forsythe and Leibler (1950) devised a statistical method to obtain the inverse of a matrix by characterizing the elements of the matrix inverse as expected values of a sequence of random walks. Barto and Duff (1994) established connections between this technique and standard dynamic programming and temporal differencing methods. The Monte Carlo matrix inversion (MCMI) approach is preferable to other techniques as it is more scalable with respect to state-space size. Therefore, we present an algorithm that utilizes MCMI for reinforcement learning policy evaluation. Our results indicate that MCMI outperforms a maximum likelihood model-based policy evaluation approach in terms of runtime, and both runtime and accuracy of the temporal differencing (TD) policy evaluation approach. We further enhance MCMI policy evaluation with an importance sampling technique to decrease estimator variance. Lastly, we demonstrate techniques for upscaling MCMI to large state spaces for policy improvement.",1
"The task of estimating the gradient of a function in the presence of noise is central to several forms of reinforcement learning, including policy search methods. We present two techniques for reducing gradient estimation errors in the presence of observable input noise applied to the control signal. The first method extends the idea of a reinforcement baseline by fitting a local linear model to the function whose gradient is being estimated; we show how to find the linear model that minimizes the variance of the gradient estimate, and how to estimate the model from data. The second method improves this further by discounting components of the gradient vector that have high variance. These methods are applied to the problem of motor control learning, where actuator noise has a significant influence on behavior. In particular, we apply the techniques to learn locally optimal controllers for a dart-throwing task using a simulated three-link arm; we demonstrate that proposed methods significantly improve the reward function gradient estimate and, consequently, the learning curve, over existing methods.",0
"Reinforcement learning methods, such as policy search, require accurate gradient estimation of functions even when there is observable input noise. To address this, we introduce two techniques. The first method involves fitting a local linear model to the function being estimated, similar to a reinforcement baseline, in order to minimize gradient estimate variance. This model can be estimated from data. The second method improves upon the first by discounting components of the gradient vector that have high variance. We apply these techniques to the problem of motor control learning, specifically in a dart-throwing task using a simulated three-link arm. Our results show that these methods significantly improve the accuracy of the reward function gradient estimate, leading to improved learning curves compared to existing methods.",1
"We propose a new, nonparametric approach to estimating the value function in reinforcement learning. This approach makes use of a recently developed representation of conditional distributions as functions in a reproducing kernel Hilbert space. Such representations bypass the need for estimating transition probabilities, and apply to any domain on which kernels can be defined. Our approach avoids the need to approximate intractable integrals since expectations are represented as RKHS inner products whose computation has linear complexity in the sample size. Thus, we can efficiently perform value function estimation in a wide variety of settings, including finite state spaces, continuous states spaces, and partially observable tasks where only sensor measurements are available. A second advantage of the approach is that we learn the conditional distribution representation from a training sample, and do not require an exhaustive exploration of the state space. We prove convergence of our approach either to the optimal policy, or to the closest projection of the optimal policy in our model class, under reasonable assumptions. In experiments, we demonstrate the performance of our algorithm on a learning task in a continuous state space (the under-actuated pendulum), and on a navigation problem where only images from a sensor are observed. We compare with least-squares policy iteration where a Gaussian process is used for value function estimation. Our algorithm achieves better performance in both tasks.",0
"Our new approach to estimating the value function in reinforcement learning is nonparametric and employs a recently developed representation of conditional distributions as functions in a reproducing kernel Hilbert space. This method eliminates the need to estimate transition probabilities and can be applied to any domain where kernels can be defined. Additionally, our approach avoids the need to approximate intractable integrals by representing expectations as RKHS inner products with linear complexity in the sample size. As a result, value function estimation can be efficiently performed in various settings, including finite and continuous state spaces and partially observable tasks. Furthermore, we learn the conditional distribution representation from a training sample without requiring exhaustive exploration of the state space. We establish convergence of our approach to either the optimal policy or the closest projection of the optimal policy in our model class under reasonable assumptions. In experiments, we demonstrate the superior performance of our algorithm on a continuous state space learning task (the under-actuated pendulum) and a navigation problem where only sensor images are available, compared to least-squares policy iteration that employs a Gaussian process for value function estimation.",1
"Recently, Petrik et al. demonstrated that L1Regularized Approximate Linear Programming (RALP) could produce value functions and policies which compared favorably to established linear value function approximation techniques like LSPI. RALP's success primarily stems from the ability to solve the feature selection and value function approximation steps simultaneously. RALP's performance guarantees become looser if sampled next states are used. For very noisy domains, RALP requires an accurate model rather than samples, which can be unrealistic in some practical scenarios. In this paper, we demonstrate this weakness, and then introduce Locally Smoothed L1-Regularized Approximate Linear Programming (LS-RALP). We demonstrate that LS-RALP mitigates inaccuracies stemming from noise even without an accurate model. We show that, given some smoothness assumptions, as the number of samples increases, error from noise approaches zero, and provide experimental examples of LS-RALP's success on common reinforcement learning benchmark problems.",0
"Petrik et al. have recently exhibited the efficacy of L1Regularized Approximate Linear Programming (RALP) in generating value functions and policies that outperform traditional linear value function approximation techniques, such as LSPI. RALP's triumphs can be attributed to its capability to tackle feature selection and value function approximation simultaneously. However, RALP's performance guarantees are lessened if sampled next states are utilized. For domains with extreme noise, RALP necessitates an accurate model, which may be impractical in certain practical scenarios. In this manuscript, we elucidate this shortcoming and introduce Locally Smoothed L1-Regularized Approximate Linear Programming (LS-RALP). We demonstrate that LS-RALP diminishes inaccuracies arising from noise without requiring an accurate model. We show that, given certain smoothness assumptions, as the number of samples increases, the error from noise approaches zero, and present experimental examples of LS-RALP's success on prevalent reinforcement learning benchmark problems.",1
"This paper explores a new framework for reinforcement learning based on online convex optimization, in particular mirror descent and related algorithms. Mirror descent can be viewed as an enhanced gradient method, particularly suited to minimization of convex functions in highdimensional spaces. Unlike traditional gradient methods, mirror descent undertakes gradient updates of weights in both the dual space and primal space, which are linked together using a Legendre transform. Mirror descent can be viewed as a proximal algorithm where the distance generating function used is a Bregman divergence. A new class of proximal-gradient based temporal-difference (TD) methods are presented based on different Bregman divergences, which are more powerful than regular TD learning. Examples of Bregman divergences that are studied include p-norm functions, and Mahalanobis distance based on the covariance of sample gradients. A new family of sparse mirror-descent reinforcement learning methods are proposed, which are able to find sparse fixed points of an l1-regularized Bellman equation at significantly less computational cost than previous methods based on second-order matrix methods. An experimental study of mirror-descent reinforcement learning is presented using discrete and continuous Markov decision processes.",0
"The focus of this paper is to explore a fresh approach to reinforcement learning that utilizes online convex optimization, specifically mirror descent and related algorithms. Mirror descent is a superior gradient method to minimize convex functions in spaces with numerous dimensions. Unlike conventional gradient methods, mirror descent operates gradient updates of weights in both the primal space and dual space, which are connected using a Legendre transform. Mirror descent can be regarded as a proximal algorithm that employs a Bregman divergence as the distance generating function. This paper presents a new class of temporal-difference (TD) methods based on different Bregman divergences that are more robust than regular TD learning. The article examines various Bregman divergences, including p-norm functions and Mahalanobis distance based on sample gradient covariance. Additionally, it proposes a new set of sparse mirror-descent reinforcement learning methods capable of discovering sparse fixed points of an l1-regularized Bellman equation at a reduced computational cost compared to prior methods based on second-order matrix methods. The article concludes with an experimental study of mirror-descent reinforcement learning using discrete and continuous Markov decision processes.",1
"This paper investigates energy efficiency for two-tier femtocell networks through combining game theory and stochastic learning. With the Stackelberg game formulation, a hierarchical reinforcement learning framework is applied to study the joint average utility maximization of macrocells and femtocells subject to the minimum signal-to-interference-plus-noise-ratio requirements. The macrocells behave as the leaders and the femtocells are followers during the learning procedure. At each time step, the leaders commit to dynamic strategies based on the best responses of the followers, while the followers compete against each other with no further information but the leaders' strategy information. In this paper, we propose two learning algorithms to schedule each cell's stochastic power levels, leading by the macrocells. Numerical experiments are presented to validate the proposed studies and show that the two learning algorithms substantially improve the energy efficiency of the femtocell networks.",0
"The aim of this study is to explore ways to increase energy efficiency in two-tier femtocell networks by using a combination of game theory and stochastic learning. The study uses a hierarchical reinforcement learning framework based on the Stackelberg game formulation to examine how to maximize joint average utility while meeting minimum signal-to-interference-plus-noise-ratio requirements. The macrocells act as leaders and the femtocells as followers during the learning process. The leaders use dynamic strategies based on the followers' best responses, while the followers compete against each other using only the leaders' strategy information. The study proposes two learning algorithms to schedule the stochastic power levels of each cell, with the macrocells leading the way. Numerical experiments confirm the efficacy of the proposed methods, showing significant improvements in femtocell network energy efficiency.",1
Finding an optimal sensing policy for a particular access policy and sensing scheme is a laborious combinatorial problem that requires the system model parameters to be known. In practise the parameters or the model itself may not be completely known making reinforcement learning methods appealing. In this paper a non-parametric reinforcement learning-based method is developed for sensing and accessing multi-band radio spectrum in multi-user cognitive radio networks. A suboptimal sensing policy search algorithm is proposed for a particular multi-user multi-band access policy and the randomized Chair-Varshney rule. The randomized Chair-Varshney rule is used to reduce the probability of false alarms under a constraint on the probability of detection that protects the primary user. The simulation results show that the proposed method achieves a sum profit (e.g. data rate) close to the optimal sensing policy while achieving the desired probability of detection.,0
"The task of determining the best sensing policy for a given access policy and sensing approach can be a time-consuming process involving many variables, necessitating an understanding of the system model parameters. In reality, these parameters or the model itself may not be entirely understood, making reinforcement learning methods an attractive option. This article presents a non-parametric reinforcement learning approach to sensing and accessing multi-band radio spectrum in multi-user cognitive radio networks. The proposed method employs a suboptimal sensing policy search algorithm for a specific multi-user, multi-band access policy, and the randomized Chair-Varshney rule to minimize the probability of false alarms while preserving the primary user's probability of detection. The simulation results indicate that the proposed method achieves a sum profit (e.g. data rate) close to the optimal sensing policy while maintaining the desired probability of detection.",1
"We introduce a class of learning problems where the agent is presented with a series of tasks. Intuitively, if there is relation among those tasks, then the information gained during execution of one task has value for the execution of another task. Consequently, the agent is intrinsically motivated to explore its environment beyond the degree necessary to solve the current task it has at hand. We develop a decision theoretic setting that generalises standard reinforcement learning tasks and captures this intuition. More precisely, we consider a multi-stage stochastic game between a learning agent and an opponent. We posit that the setting is a good model for the problem of life-long learning in uncertain environments, where while resources must be spent learning about currently important tasks, there is also the need to allocate effort towards learning about aspects of the world which are not relevant at the moment. This is due to the fact that unpredictable future events may lead to a change of priorities for the decision maker. Thus, in some sense, the model ""explains"" the necessity of curiosity. Apart from introducing the general formalism, the paper provides algorithms. These are evaluated experimentally in some exemplary domains. In addition, performance bounds are proven for some cases of this problem.",0
"We present a type of learning problem that involves a series of tasks. If there is a connection between these tasks, the knowledge gained while completing one task can be applied to another. This motivates the agent to explore its environment beyond what is required to complete the current task. To capture this intuition, we propose a decision theoretic framework that extends standard reinforcement learning tasks. Specifically, we propose a multi-stage stochastic game between a learning agent and an opponent, which models lifelong learning in uncertain environments. While the agent must allocate resources to learn about important tasks, it must also explore aspects of the world that may not be relevant at the moment, due to unpredictable future events. This explains the importance of curiosity. We provide algorithms and experimental evaluations in different domains, as well as performance bounds for certain cases of the problem.",1
We introduce a method for constructing skills capable of solving tasks drawn from a distribution of parameterized reinforcement learning problems. The method draws example tasks from a distribution of interest and uses the corresponding learned policies to estimate the topology of the lower-dimensional piecewise-smooth manifold on which the skill policies lie. This manifold models how policy parameters change as task parameters vary. The method identifies the number of charts that compose the manifold and then applies non-linear regression in each chart to construct a parameterized skill by predicting policy parameters from task parameters. We evaluate our method on an underactuated simulated robotic arm tasked with learning to accurately throw darts at a parameterized target location.,0
"A method is presented in this study for creating skills that can solve a range of parameterized reinforcement learning problems. To achieve this, the method selects example tasks from a distribution of interest and utilizes the learned policies to assess the topology of the lower-dimensional piecewise-smooth manifold, which represents the way policy parameters change in response to varying task parameters. The number of charts that compose the manifold is identified, and non-linear regression is employed in each chart to construct a parameterized skill by forecasting policy parameters from task parameters. The effectiveness of this approach is demonstrated through an experiment in which an underactuated simulated robotic arm is taught to throw darts accurately at a target location that varies based on parameters.",1
"This paper focuses on reinforcement learning (RL) with limited prior knowledge. In the domain of swarm robotics for instance, the expert can hardly design a reward function or demonstrate the target behavior, forbidding the use of both standard RL and inverse reinforcement learning. Although with a limited expertise, the human expert is still often able to emit preferences and rank the agent demonstrations. Earlier work has presented an iterative preference-based RL framework: expert preferences are exploited to learn an approximate policy return, thus enabling the agent to achieve direct policy search. Iteratively, the agent selects a new candidate policy and demonstrates it; the expert ranks the new demonstration comparatively to the previous best one; the expert's ranking feedback enables the agent to refine the approximate policy return, and the process is iterated. In this paper, preference-based reinforcement learning is combined with active ranking in order to decrease the number of ranking queries to the expert needed to yield a satisfactory policy. Experiments on the mountain car and the cancer treatment testbeds witness that a couple of dozen rankings enable to learn a competent policy.",0
"The aim of this paper is to explore reinforcement learning (RL) in situations where there is limited prior knowledge. For instance, in the field of swarm robotics, it is difficult for experts to design a reward function or demonstrate desired behaviour, which makes standard RL and inverse reinforcement learning impossible. However, even with limited expertise, the expert can still express preferences and rank the agent demonstrations. Previous research has presented an iterative preference-based RL framework, which uses expert preferences to learn an approximate policy return and enable direct policy search. The agent selects a new candidate policy and demonstrates it, and the expert ranks the new demonstration comparatively to the previous best one. This feedback is then used to refine the approximate policy return, and the process is repeated. This paper proposes combining preference-based reinforcement learning with active ranking to reduce the number of ranking queries to the expert required to achieve a satisfactory policy. Experiments on the mountain car and cancer treatment testbeds indicate that a few dozen rankings are sufficient to learn a competent policy.",1
"We present a set of high-probability inequalities that control the concentration of weighted averages of multiple (possibly uncountably many) simultaneously evolving and interdependent martingales. Our results extend the PAC-Bayesian analysis in learning theory from the i.i.d. setting to martingales opening the way for its application to importance weighted sampling, reinforcement learning, and other interactive learning domains, as well as many other domains in probability theory and statistics, where martingales are encountered.   We also present a comparison inequality that bounds the expectation of a convex function of a martingale difference sequence shifted to the [0,1] interval by the expectation of the same function of independent Bernoulli variables. This inequality is applied to derive a tighter analog of Hoeffding-Azuma's inequality.",0
"A collection of high-probability inequalities is introduced to regulate the concentration of weighted averages of multiple martingales that evolve and depend on each other, even in cases where they are countless in number. Our findings expand upon the PAC-Bayesian analysis used in learning theory by extending it from the i.i.d. setting to include martingales, which opens the door to its application in interactive learning domains, such as reinforcement learning and importance weighted sampling. These results are also applicable to other areas of probability theory and statistics where martingales are present. Additionally, a comparison inequality is presented that limits the expectation of a convex function of a martingale difference sequence, which is then shifted to the [0,1] interval by the expectation of the same function of independent Bernoulli variables. This inequality is used to create a tighter version of Hoeffding-Azuma's inequality.",1
"This paper gives specific divergence examples of value-iteration for several major Reinforcement Learning and Adaptive Dynamic Programming algorithms, when using a function approximator for the value function. These divergence examples differ from previous divergence examples in the literature, in that they are applicable for a greedy policy, i.e. in a ""value iteration"" scenario. Perhaps surprisingly, with a greedy policy, it is also possible to get divergence for the algorithms TD(1) and Sarsa(1). In addition to these divergences, we also achieve divergence for the Adaptive Dynamic Programming algorithms HDP, DHP and GDHP.",0
"The paper presents instances of divergence in value-iteration for various significant algorithms in Reinforcement Learning and Adaptive Dynamic Programming, utilizing a function approximator for the value function. These examples are distinct from prior instances of divergence in the literature, since they apply to a greedy policy, specifically in a ""value iteration"" setting. It is noteworthy that TD(1) and Sarsa(1) algorithms can also lead to divergence with a greedy policy. Furthermore, the paper demonstrates divergence in HDP, DHP, and GDHP Adaptive Dynamic Programming algorithms.",1
"We demonstrate an equivalence between reproducing kernel Hilbert space (RKHS) embeddings of conditional distributions and vector-valued regressors. This connection introduces a natural regularized loss function which the RKHS embeddings minimise, providing an intuitive understanding of the embeddings and a justification for their use. Furthermore, the equivalence allows the application of vector-valued regression methods and results to the problem of learning conditional distributions. Using this link we derive a sparse version of the embedding by considering alternative formulations. Further, by applying convergence results for vector-valued regression to the embedding problem we derive minimax convergence rates which are O(\log(n)/n) -- compared to current state of the art rates of O(n^{-1/4}) -- and are valid under milder and more intuitive assumptions. These minimax upper rates coincide with lower rates up to a logarithmic factor, showing that the embedding method achieves nearly optimal rates. We study our sparse embedding algorithm in a reinforcement learning task where the algorithm shows significant improvement in sparsity over an incomplete Cholesky decomposition.",0
"We establish a link between reproducing kernel Hilbert space (RKHS) embeddings of conditional distributions and vector-valued regressors, demonstrating their equivalence. This connection results in a regularized loss function that the RKHS embeddings minimize, offering an intuitive comprehension of the embeddings and a rationale for their implementation. Additionally, the connection enables the use of vector-valued regression techniques and outcomes for learning conditional distributions. By exploring alternative formulations, we derive a sparse version of the embedding. Through the application of convergence results for vector-valued regression to the embedding problem, we establish minimax convergence rates of O(\log(n)/n); these rates are more intuitive and less stringent than the current state of the art rates of O(n^{-1/4}). These minimax upper rates align with lower rates up to a logarithmic factor, demonstrating that the embedding method achieves almost optimal rates. We evaluate our sparse embedding approach in a reinforcement learning task, where it exhibits a substantial improvement in sparsity compared to an incomplete Cholesky decomposition.",1
"In a vision system, every task needs that the operators to apply should be {\guillemotleft} well chosen {\guillemotright} and their parameters should be also {\guillemotleft} well adjusted {\guillemotright}. The diversity of operators and the multitude of their parameters constitute a big challenge for users. As it is very difficult to make the {\guillemotleft} right {\guillemotright} choice, lack of a specific rule, many disadvantages appear and affect the computation time and especially the quality of results. In this paper we present a multi-agent architecture to learn the best operators to apply and their best parameters for a class of images. Our architecture consists of three types of agents: User Agent, Operator Agent and Parameter Agent. The User Agent determines the phases of treatment, a library of operators and the possible values of their parameters. The Operator Agent constructs all possible combinations of operators and the Parameter Agent, the core of the architecture, adjusts the parameters of each combination by treating a large number of images. Through the reinforcement learning mechanism, our architecture does not consider only the system opportunities but also the user preferences.",0
"The selection of appropriate operators and their parameters is crucial for every task in a vision system. However, due to the various operators and their parameters, it becomes a challenging task for users to make the right selection without a specific rule. This often results in disadvantages such as increased computation time and reduced quality of results. To address this issue, we propose a multi-agent architecture that can learn the best operators and their parameters for a specific class of images. The architecture comprises three types of agents: User Agent, Operator Agent, and Parameter Agent. The User Agent defines the treatment phases, operator library, and possible parameter values. The Operator Agent generates all possible combinations of operators, and the Parameter Agent, which is the core of the architecture, adjusts the parameters of each combination by processing a vast number of images. Our architecture employs reinforcement learning to consider not only the system opportunities but also user preferences.",1
"In environments with uncertain dynamics exploration is necessary to learn how to perform well. Existing reinforcement learning algorithms provide strong exploration guarantees, but they tend to rely on an ergodicity assumption. The essence of ergodicity is that any state is eventually reachable from any other state by following a suitable policy. This assumption allows for exploration algorithms that operate by simply favoring states that have rarely been visited before. For most physical systems this assumption is impractical as the systems would break before any reasonable exploration has taken place, i.e., most physical systems don't satisfy the ergodicity assumption. In this paper we address the need for safe exploration methods in Markov decision processes. We first propose a general formulation of safety through ergodicity. We show that imposing safety by restricting attention to the resulting set of guaranteed safe policies is NP-hard. We then present an efficient algorithm for guaranteed safe, but potentially suboptimal, exploration. At the core is an optimization formulation in which the constraints restrict attention to a subset of the guaranteed safe policies and the objective favors exploration policies. Our framework is compatible with the majority of previously proposed exploration methods, which rely on an exploration bonus. Our experiments, which include a Martian terrain exploration problem, show that our method is able to explore better than classical exploration methods.",0
"Exploration is essential in uncertain environments to improve performance. While existing reinforcement learning algorithms offer robust exploration guarantees, they rely on the ergodicity assumption, which implies that any state can be reached from any other state by following a suitable policy. However, this assumption is impractical for most physical systems as they don't satisfy ergodicity and would break before reasonable exploration. This paper proposes a safe exploration method for Markov decision processes by formulating safety through ergodicity. However, restricting attention to the resulting set of guaranteed safe policies is NP-hard, so an efficient algorithm is presented that balances guaranteed safety with exploration. The framework is compatible with most existing exploration methods and is demonstrated to be superior in exploration in experiments, including a Martian terrain exploration problem.",1
"A fundamental problem in control is to learn a model of a system from observations that is useful for controller synthesis. To provide good performance guarantees, existing methods must assume that the real system is in the class of models considered during learning. We present an iterative method with strong guarantees even in the agnostic case where the system is not in the class. In particular, we show that any no-regret online learning algorithm can be used to obtain a near-optimal policy, provided some model achieves low training error and access to a good exploration distribution. Our approach applies to both discrete and continuous domains. We demonstrate its efficacy and scalability on a challenging helicopter domain from the literature.",0
"The central issue in control is acquiring a system model from observations that is useful for developing a controller. To ensure superior performance, current techniques presuppose that the actual system belongs to the class of models studied during the learning process. Our iterative approach provides strong assurances even when the system falls outside of this class. Specifically, we establish that an online learning algorithm with no regrets can generate an almost ideal policy, assuming that a model with low training error and a good exploration distribution is available. This technique is applicable to both continuous and discrete settings. We prove its effectiveness and scalability by employing it on a difficult helicopter problem from the literature.",1
Managing risk in dynamic decision problems is of cardinal importance in many fields such as finance and process control. The most common approach to defining risk is through various variance related criteria such as the Sharpe Ratio or the standard deviation adjusted reward. It is known that optimizing many of the variance related risk criteria is NP-hard. In this paper we devise a framework for local policy gradient style algorithms for reinforcement learning for variance related criteria. Our starting point is a new formula for the variance of the cost-to-go in episodic tasks. Using this formula we develop policy gradient algorithms for criteria that involve both the expected cost and the variance of the cost. We prove the convergence of these algorithms to local minima and demonstrate their applicability in a portfolio planning problem.,0
"In numerous fields like finance and process control, effectively managing risk in constantly evolving decision problems is crucial. The common practice for defining risk involves using various variance related measures like the Sharpe Ratio or the standard deviation adjusted reward. However, optimizing many of these measures can be a challenging task due to their NP-hard nature. In this article, we introduce a framework for local policy gradient-style algorithms in reinforcement learning for variance related criteria. We begin by presenting a fresh formula for the variance of the cost-to-go in episodic tasks, which we then utilize to create policy gradient algorithms for criteria that involve both the expected cost and the cost variance. Our algorithms are proven to converge to local minima, and we showcase their efficacy in a portfolio planning problem.",1
"Motivated by value function estimation in reinforcement learning, we study statistical linear inverse problems, i.e., problems where the coefficients of a linear system to be solved are observed in noise. We consider penalized estimators, where performance is evaluated using a matrix-weighted two-norm of the defect of the estimator measured with respect to the true, unknown coefficients. Two objective functions are considered depending whether the error of the defect measured with respect to the noisy coefficients is squared or unsquared. We propose simple, yet novel and theoretically well-founded data-dependent choices for the regularization parameters for both cases that avoid data-splitting. A distinguishing feature of our analysis is that we derive deterministic error bounds in terms of the error of the coefficients, thus allowing the complete separation of the analysis of the stochastic properties of these errors. We show that our results lead to new insights and bounds for linear value function estimation in reinforcement learning.",0
"Our focus is on statistical linear inverse problems, which involve solving linear systems with observed coefficients affected by noise. Our investigation is prompted by the need for value function estimation in reinforcement learning. Our approach involves using penalized estimators, which are assessed based on the matrix-weighted two-norm of the estimator's defect relative to the true but unknown coefficients. We explore two objective functions, one that squares the error of the defect and another that does not. We offer novel data-dependent choices for regularization parameters that do not require data-splitting and are theoretically well-founded. Our analysis provides deterministic error bounds based on the error of the coefficients, thus enabling the separation of the stochastic properties of these errors. Our findings provide new insights and bounds for linear value function estimation in reinforcement learning.",1
"Bayesian reinforcement learning (BRL) encodes prior knowledge of the world in a model and represents uncertainty in model parameters by maintaining a probability distribution over them. This paper presents Monte Carlo BRL (MC-BRL), a simple and general approach to BRL. MC-BRL samples a priori a finite set of hypotheses for the model parameter values and forms a discrete partially observable Markov decision process (POMDP) whose state space is a cross product of the state space for the reinforcement learning task and the sampled model parameter space. The POMDP does not require conjugate distributions for belief representation, as earlier works do, and can be solved relatively easily with point-based approximation algorithms. MC-BRL naturally handles both fully and partially observable worlds. Theoretical and experimental results show that the discrete POMDP approximates the underlying BRL task well with guaranteed performance.",0
"The concept of Bayesian reinforcement learning (BRL) involves utilizing prior knowledge of the world through a model and taking into account uncertainty in model parameters by maintaining a probability distribution over them. This article introduces a new and straightforward method for BRL known as Monte Carlo BRL (MC-BRL). MC-BRL involves sampling a finite set of hypotheses for the model parameter values upfront and creating a discrete partially observable Markov decision process (POMDP) where the state space is a combination of the state space for the reinforcement learning task and the sampled model parameter space. Unlike previous works, the POMDP used in MC-BRL does not require conjugate distributions for belief representation and can be solved relatively easily with point-based approximation algorithms. MC-BRL is capable of handling both fully and partially observable worlds and has been proven to approximate the underlying BRL task successfully with guaranteed performance through theoretical and experimental results.",1
"We consider the problem of learning the optimal action-value function in the discounted-reward Markov decision processes (MDPs). We prove a new PAC bound on the sample-complexity of model-based value iteration algorithm in the presence of the generative model, which indicates that for an MDP with N state-action pairs and the discount factor \gamma\in[0,1) only O(N\log(N/\delta)/((1-\gamma)^3\epsilon^2)) samples are required to find an \epsilon-optimal estimation of the action-value function with the probability 1-\delta. We also prove a matching lower bound of \Theta (N\log(N/\delta)/((1-\gamma)^3\epsilon^2)) on the sample complexity of estimating the optimal action-value function by every RL algorithm. To the best of our knowledge, this is the first matching result on the sample complexity of estimating the optimal (action-) value function in which the upper bound matches the lower bound of RL in terms of N, \epsilon, \delta and 1/(1-\gamma). Also, both our lower bound and our upper bound significantly improve on the state-of-the-art in terms of 1/(1-\gamma).",0
"In the context of discounted-reward Markov decision processes (MDPs), we examine the task of learning the optimal action-value function. We present a novel PAC bound on the sample-complexity of the model-based value iteration algorithm when confronted with a generative model. Specifically, our results indicate that, for an MDP containing N state-action pairs and a discount factor \gamma\in[0,1), only O(N\log(N/\delta)/((1-\gamma)^3\epsilon^2)) samples are necessary to obtain an \epsilon-optimal estimation of the action-value function with a probability of 1-\delta. Furthermore, we establish a matching lower bound of \Theta (N\log(N/\delta)/((1-\gamma)^3\epsilon^2)) on the sample complexity of estimating the optimal action-value function for all RL algorithms. This is the first matching outcome on the sample complexity of estimating the optimal value function where the upper bound matches the lower bound of RL in terms of N, \epsilon, \delta, and 1/(1-\gamma). Additionally, both our lower and upper bounds significantly outperform the current state-of-the-art regarding 1/(1-\gamma).",1
"Feature selection and regularization are becoming increasingly prominent tools in the efforts of the reinforcement learning (RL) community to expand the reach and applicability of RL. One approach to the problem of feature selection is to impose a sparsity-inducing form of regularization on the learning method. Recent work on $L_1$ regularization has adapted techniques from the supervised learning literature for use with RL. Another approach that has received renewed attention in the supervised learning community is that of using a simple algorithm that greedily adds new features. Such algorithms have many of the good properties of the $L_1$ regularization methods, while also being extremely efficient and, in some cases, allowing theoretical guarantees on recovery of the true form of a sparse target function from sampled data. This paper considers variants of orthogonal matching pursuit (OMP) applied to reinforcement learning. The resulting algorithms are analyzed and compared experimentally with existing $L_1$ regularized approaches. We demonstrate that perhaps the most natural scenario in which one might hope to achieve sparse recovery fails; however, one variant, OMP-BRM, provides promising theoretical guarantees under certain assumptions on the feature dictionary. Another variant, OMP-TD, empirically outperforms prior methods both in approximation accuracy and efficiency on several benchmark problems.",0
"The reinforcement learning (RL) community is increasingly using feature selection and regularization to broaden the scope and usefulness of RL. One way to handle feature selection is to employ a type of regularization that encourages sparsity in the learning method. Recent research on $L_1$ regularization has adapted techniques from supervised learning to use with RL. Another approach that supervised learning has revisited is the use of a simple algorithm that adds new features in a greedy manner. These algorithms possess many of the desirable qualities of $L_1$ regularization while being highly efficient and in some cases provide theoretical guarantees for recovering the true form of a sparse target function from sampled data. This article explores variations of orthogonal matching pursuit (OMP) in RL. The resulting algorithms are analyzed and compared to existing $L_1$ regularized methods. Our findings demonstrate that the most likely scenario for achieving sparse recovery is not feasible. However, one variant, OMP-BRM, provides promising theoretical guarantees under certain assumptions about the feature dictionary. Another variant, OMP-TD, outperforms previous methods in approximation accuracy and efficiency on several benchmark problems.",1
"Previous work in hierarchical reinforcement learning has faced a dilemma: either ignore the values of different possible exit states from a subroutine, thereby risking suboptimal behavior, or represent those values explicitly thereby incurring a possibly large representation cost because exit values refer to nonlocal aspects of the world (i.e., all subsequent rewards). This paper shows that, in many cases, one can avoid both of these problems. The solution is based on recursively decomposing the exit value function in terms of Q-functions at higher levels of the hierarchy. This leads to an intuitively appealing runtime architecture in which a parent subroutine passes to its child a value function on the exit states and the child reasons about how its choices affect the exit value. We also identify structural conditions on the value function and transition distributions that allow much more concise representations of exit state distributions, leading to further state abstraction. In essence, the only variables whose exit values need be considered are those that the parent cares about and the child affects. We demonstrate the utility of our algorithms on a series of increasingly complex environments.",0
"In hierarchical reinforcement learning, prior research has encountered a predicament: either disregard the potential values of various exit states from a subroutine, which may result in suboptimal behavior, or explicitly represent those values, which may incur a significant representation cost because exit values pertain to nonlocal aspects of the world. This study reveals that, in numerous instances, both of these issues can be circumvented. The resolution involves recursively decomposing the exit value function into Q-functions at higher levels of the hierarchy. This approach yields a runtime framework where a parent subroutine transmits a value function to its child on the exit states, and the child determines how its decisions impact the exit value. Additionally, we pinpoint structural conditions on the value function and transition distributions that permit more concise representations of exit state distributions, resulting in further state abstraction. Essentially, the only variables that require consideration for exit values are those that the parent values and the child influences. Our algorithms' usefulness is demonstrated in a series of increasingly intricate environments.",1
"SDYNA is a general framework designed to address large stochastic reinforcement learning problems. Unlike previous model based methods in FMDPs, it incrementally learns the structure and the parameters of a RL problem using supervised learning techniques. Then, it integrates decision-theoric planning algorithms based on FMDPs to compute its policy. SPITI is an instanciation of SDYNA that exploits ITI, an incremental decision tree algorithm, to learn the reward function and the Dynamic Bayesian Networks with local structures representing the transition function of the problem. These representations are used by an incremental version of the Structured Value Iteration algorithm. In order to learn the structure, SPITI uses Chi-Square tests to detect the independence between two probability distributions. Thus, we study the relation between the threshold used in the Chi-Square test, the size of the model built and the relative error of the value function of the induced policy with respect to the optimal value. We show that, on stochastic problems, one can tune the threshold so as to generate both a compact model and an efficient policy. Then, we show that SPITI, while keeping its model compact, uses the generalization property of its learning method to perform better than a stochastic classical tabular algorithm in large RL problem with an unknown structure. We also introduce a new measure based on Chi-Square to qualify the accuracy of the model learned by SPITI. We qualitatively show that the generalization property in SPITI within the FMDP framework may prevent an exponential growth of the time required to learn the structure of large stochastic RL problems.",0
"SDYNA is a framework that addresses large stochastic reinforcement learning problems. Unlike previous model-based methods, it uses supervised learning techniques to incrementally learn the structure and parameters of an RL problem before integrating decision-theoretic planning algorithms based on FMDPs to compute its policy. SPITI, an instantiation of SDYNA, uses ITI, an incremental decision tree algorithm, to learn the reward function and the Dynamic Bayesian Networks with local structures that represent the transition function of the problem. Chi-Square tests are used to detect the independence between two probability distributions, allowing SPITI to learn the structure. We explore the relationship between the threshold used in the Chi-Square test, the size of the model built, and the relative error of the value function of the induced policy with respect to the optimal value. Our results show that by tuning the threshold, SPITI can generate a compact model and an efficient policy on stochastic problems. We also introduce a new measure based on Chi-Square to qualify the accuracy of the model learned by SPITI. Our qualitative analysis shows that the generalization property in SPITI within the FMDP framework may prevent an exponential growth of the time required to learn the structure of large stochastic RL problems, allowing SPITI to perform better than a stochastic classical tabular algorithm.",1
"In this paper we propose a novel gradient algorithm to learn a policy from an expert's observed behavior assuming that the expert behaves optimally with respect to some unknown reward function of a Markovian Decision Problem. The algorithm's aim is to find a reward function such that the resulting optimal policy matches well the expert's observed behavior. The main difficulty is that the mapping from the parameters to policies is both nonsmooth and highly redundant. Resorting to subdifferentials solves the first difficulty, while the second one is over- come by computing natural gradients. We tested the proposed method in two artificial domains and found it to be more reliable and efficient than some previous methods.",0
"This paper presents a new gradient algorithm for learning a policy based on the behavior of an expert, assuming that the expert behaves optimally according to an unknown reward function in a Markovian Decision Problem. The algorithm aims to identify a reward function that produces an optimal policy closely resembling the expert's behavior. The main challenge is that the mapping from parameters to policies is both non-smooth and highly redundant. To tackle this, the paper uses subdifferentials to address the non-smoothness and natural gradients to overcome redundancy. The proposed method is evaluated in two artificial domains, and the results show that it is more efficient and reliable compared to previous approaches.",1
"Oriental ink painting, called Sumi-e, is one of the most appealing painting styles that has attracted artists around the world. Major challenges in computer-based Sumi-e simulation are to abstract complex scene information and draw smooth and natural brush strokes. To automatically find such strokes, we propose to model the brush as a reinforcement learning agent, and learn desired brush-trajectories by maximizing the sum of rewards in the policy search framework. We also provide elaborate design of actions, states, and rewards tailored for a Sumi-e agent. The effectiveness of our proposed approach is demonstrated through simulated Sumi-e experiments.",0
"Sumi-e, an Eastern ink painting style, has become a popular choice among artists worldwide. However, computer-based Sumi-e simulation presents significant challenges such as abstracting intricate scene information and creating smooth, natural brush strokes. Our solution is to model the brush as a reinforcement learning agent and teach it the desired brush trajectories by maximizing the sum of rewards in the policy search framework. We have meticulously designed actions, states, and rewards specific to a Sumi-e agent. Our proposed approach has proven effective in simulated Sumi-e experiments.",1
"There has been a recent focus in reinforcement learning on addressing continuous state and action problems by optimizing parameterized policies. PI2 is a recent example of this approach. It combines a derivation from first principles of stochastic optimal control with tools from statistical estimation theory. In this paper, we consider PI2 as a member of the wider family of methods which share the concept of probability-weighted averaging to iteratively update parameters to optimize a cost function. We compare PI2 to other members of the same family - Cross-Entropy Methods and CMAES - at the conceptual level and in terms of performance. The comparison suggests the derivation of a novel algorithm which we call PI2-CMA for ""Path Integral Policy Improvement with Covariance Matrix Adaptation"". PI2-CMA's main advantage is that it determines the magnitude of the exploration noise automatically.",0
"Recently, reinforcement learning has been focusing on resolving issues with continuous state and action problems by optimizing parameterized policies. One such approach is PI2, which utilizes a combination of stochastic optimal control derivation and statistical estimation theory. This paper considers PI2 as a part of a broader group of methods that use probability-weighted averaging to iteratively update parameters and optimize a cost function. The paper compares PI2 to other members of this group, such as Cross-Entropy Methods and CMAES, in terms of performance and conceptually. From this comparison, a new algorithm called PI2-CMA is derived, which stands for ""Path Integral Policy Improvement with Covariance Matrix Adaptation."" PI2-CMA's primary benefit is that it can automatically determine the level of exploration noise.",1
"Inverse optimal control, also known as inverse reinforcement learning, is the problem of recovering an unknown reward function in a Markov decision process from expert demonstrations of the optimal policy. We introduce a probabilistic inverse optimal control algorithm that scales gracefully with task dimensionality, and is suitable for large, continuous domains where even computing a full policy is impractical. By using a local approximation of the reward function, our method can also drop the assumption that the demonstrations are globally optimal, requiring only local optimality. This allows it to learn from examples that are unsuitable for prior methods.",0
"Inverse reinforcement learning, which is also referred to as inverse optimal control, involves the retrieval of an obscure reward function in a Markov decision process from the optimal policy demonstrated by experts. Our new probabilistic inverse optimal control technique can adapt to task dimensionality and is appropriate for vast, uninterrupted environments where generating an entire policy is unfeasible. Our approach incorporates a nearby estimation of the reward function, which eliminates the necessity for global optimality in the demonstrations. Consequently, our method can learn from examples that were previously deemed unsuitable for other methods.",1
"The term ""nexting"" has been used by psychologists to refer to the propensity of people and many other animals to continually predict what will happen next in an immediate, local, and personal sense. The ability to ""next"" constitutes a basic kind of awareness and knowledge of one's environment. In this paper we present results with a robot that learns to next in real time, predicting thousands of features of the world's state, including all sensory inputs, at timescales from 0.1 to 8 seconds. This was achieved by treating each state feature as a reward-like target and applying temporal-difference methods to learn a corresponding value function with a discount rate corresponding to the timescale. We show that two thousand predictions, each dependent on six thousand state features, can be learned and updated online at better than 10Hz on a laptop computer, using the standard TD(lambda) algorithm with linear function approximation. We show that this approach is efficient enough to be practical, with most of the learning complete within 30 minutes. We also show that a single tile-coded feature representation suffices to accurately predict many different signals at a significant range of timescales. Finally, we show that the accuracy of our learned predictions compares favorably with the optimal off-line solution.",0
"Psychologists use the term ""nexting"" to describe how people and animals predict what will happen next in their immediate environment. This ability to ""next"" is a basic kind of awareness and knowledge. In this study, researchers trained a robot to predict thousands of features of the world's state in real-time, including sensory inputs, over timescales ranging from 0.1 to 8 seconds. They achieved this by treating each state feature as a reward-like target and using temporal-difference methods to learn a corresponding value function. The researchers found that their approach was efficient enough to be practical, with most of the learning complete within 30 minutes. They also found that a single tile-coded feature representation was sufficient to accurately predict many different signals over a significant range of timescales. Finally, they compared the accuracy of their learned predictions with the optimal off-line solution and found that they were comparable.",1
"The free energy functional has recently been proposed as a variational principle for bounded rational decision-making, since it instantiates a natural trade-off between utility gains and information processing costs that can be axiomatically derived. Here we apply the free energy principle to general decision trees that include both adversarial and stochastic environments. We derive generalized sequential optimality equations that not only include the Bellman optimality equations as a limit case, but also lead to well-known decision-rules such as Expectimax, Minimax and Expectiminimax. We show how these decision-rules can be derived from a single free energy principle that assigns a resource parameter to each node in the decision tree. These resource parameters express a concrete computational cost that can be measured as the amount of samples that are needed from the distribution that belongs to each node. The free energy principle therefore provides the normative basis for generalized optimality equations that account for both adversarial and stochastic environments.",0
"Recently, the free energy functional has been proposed as a variational principle for bounded rational decision-making. This principle balances the gains in utility with the costs of processing information, and it can be derived axiomatically. In this study, we utilize the free energy principle in decision trees that encompass both stochastic and adversarial environments. We develop generalized sequential optimality equations that include the Bellman optimality equations, as well as decision-rules such as Expectimax, Minimax, and Expectiminimax. By assigning a resource parameter to each node in the decision tree, the free energy principle allows us to derive these decision-rules from a single principle. These resource parameters indicate the computational cost of each node, measured by the number of required samples from the corresponding distribution. Thus, the free energy principle provides a normative foundation for generalized optimality equations that account for both stochastic and adversarial environments.",1
"This paper presents a new algorithm for online linear regression whose efficiency guarantees satisfy the requirements of the KWIK (Knows What It Knows) framework. The algorithm improves on the complexity bounds of the current state-of-the-art procedure in this setting. We explore several applications of this algorithm for learning compact reinforcement-learning representations. We show that KWIK linear regression can be used to learn the reward function of a factored MDP and the probabilities of action outcomes in Stochastic STRIPS and Object Oriented MDPs, none of which have been proven to be efficiently learnable in the RL setting before. We also combine KWIK linear regression with other KWIK learners to learn larger portions of these models, including experiments on learning factored MDP transition and reward functions together.",0
"In this paper, a novel algorithm for online linear regression is introduced, which guarantees efficiency in compliance with the KWIK (Knows What It Knows) framework. This algorithm surpasses the current state-of-the-art procedure in terms of complexity bounds. The paper examines various applications of this algorithm for learning concise reinforcement learning representations. The KWIK linear regression is found to be effective in learning the reward function of a factored MDP, as well as the probabilities of action outcomes in Stochastic STRIPS and Object Oriented MDPs. Previously, none of these tasks were proven to be efficiently learnable in the RL setting. Additionally, the paper demonstrates the use of KWIK linear regression in combination with other KWIK learners to learn larger portions of these models, including experiments on learning factored MDP transition and reward functions simultaneously.",1
"We introduce and analyze a natural algorithm for multi-venue exploration from censored data, which is motivated by the Dark Pool Problem of modern quantitative finance. We prove that our algorithm converges in polynomial time to a near-optimal allocation policy; prior results for similar problems in stochastic inventory control guaranteed only asymptotic convergence and examined variants in which each venue could be treated independently. Our analysis bears a strong resemblance to that of efficient exploration/ exploitation schemes in the reinforcement learning literature. We describe an extensive experimental evaluation of our algorithm on the Dark Pool Problem using real trading data.",0
"Our study presents a natural algorithm for multi-venue exploration from censored data, inspired by the Dark Pool Problem in modern quantitative finance. Through our research, we demonstrate that our algorithm achieves near-optimal allocation policy in polynomial time. In contrast to previous studies that focused on independent treatment of each venue and guaranteed only asymptotic convergence, our algorithm offers a promising solution. Our analysis shares similarities with efficient exploration/exploitation schemes in the reinforcement learning literature. Additionally, we conduct an extensive experimental evaluation of our algorithm on the Dark Pool Problem with real trading data.",1
"We provide an algorithm that achieves the optimal regret rate in an unknown weakly communicating Markov Decision Process (MDP). The algorithm proceeds in episodes where, in each episode, it picks a policy using regularization based on the span of the optimal bias vector. For an MDP with S states and A actions whose optimal bias vector has span bounded by H, we show a regret bound of ~O(HSpAT). We also relate the span to various diameter-like quantities associated with the MDP, demonstrating how our results improve on previous regret bounds.",0
"Our algorithm attains the optimal rate of regret in a weakly communicating Markov Decision Process (MDP) of unknown nature. The algorithm operates in episodes, selecting a policy for each episode via regularization, based on the optimal bias vector's span. In an MDP with S states and A actions, wherein the optimal bias vector's span is restricted to H, our algorithm guarantees a regret bound of about O(HSpAT). We also establish a correlation between the span and diameter-like values associated with the MDP, indicating that our findings surpass prior regret bounds.",1
"We present a modular approach to reinforcement learning that uses a Bayesian representation of the uncertainty over models. The approach, BOSS (Best of Sampled Set), drives exploration by sampling multiple models from the posterior and selecting actions optimistically. It extends previous work by providing a rule for deciding when to resample and how to combine the models. We show that our algorithm achieves nearoptimal reward with high probability with a sample complexity that is low relative to the speed at which the posterior distribution converges during learning. We demonstrate that BOSS performs quite favorably compared to state-of-the-art reinforcement-learning approaches and illustrate its flexibility by pairing it with a non-parametric model that generalizes across states.",0
"A Bayesian representation of uncertainty over models is utilized in our modular approach to reinforcement learning, which is presented as BOSS (Best of Sampled Set). This approach explores by selecting actions optimistically after sampling multiple models from the posterior. BOSS extends previous work by providing a resampling rule and a model-combining method. Our algorithm achieves near-optimal reward with a high probability and a low sample complexity relative to the learning speed of the posterior distribution. BOSS performs favorably compared to state-of-the-art reinforcement-learning methods and is versatile when used with a non-parametric model that generalizes across states.",1
"In this paper, we propose a distributed reinforcement learning (RL) technique called distributed power control using Q-learning (DPC-Q) to manage the interference caused by the femtocells on macro-users in the downlink. The DPC-Q leverages Q-Learning to identify the sub-optimal pattern of power allocation, which strives to maximize femtocell capacity, while guaranteeing macrocell capacity level in an underlay cognitive setting. We propose two different approaches for the DPC-Q algorithm: namely, independent, and cooperative. In the former, femtocells learn independently from each other while in the latter, femtocells share some information during learning in order to enhance their performance. Simulation results show that the independent approach is capable of mitigating the interference generated by the femtocells on macro-users. Moreover, the results show that cooperation enhances the performance of the femtocells in terms of speed of convergence, fairness and aggregate femtocell capacity.",0
"This paper suggests a method for managing interference caused by femtocells on macro-users in the downlink through a distributed reinforcement learning technique called distributed power control using Q-learning (DPC-Q). The goal of DPC-Q is to maximize femtocell capacity while ensuring macrocell capacity levels in a cognitive setting by identifying sub-optimal power allocation patterns. Two approaches are proposed for DPC-Q: independent and cooperative. In the independent approach, femtocells learn independently, while in the cooperative approach, they share information to improve performance. Simulation results demonstrate that the independent approach effectively reduces interference, while the cooperative approach leads to faster convergence, fairness, and increased aggregate femtocell capacity.",1
"Cyber-physical systems, such as mobile robots, must respond adaptively to dynamic operating conditions. Effective operation of these systems requires that sensing and actuation tasks are performed in a timely manner. Additionally, execution of mission specific tasks such as imaging a room must be balanced against the need to perform more general tasks such as obstacle avoidance. This problem has been addressed by maintaining relative utilization of shared resources among tasks near a user-specified target level. Producing optimal scheduling strategies requires complete prior knowledge of task behavior, which is unlikely to be available in practice. Instead, suitable scheduling strategies must be learned online through interaction with the system. We consider the sample complexity of reinforcement learning in this domain, and demonstrate that while the problem state space is countably infinite, we may leverage the problem's structure to guarantee efficient learning.",0
"In order for cyber-physical systems, like mobile robots, to operate effectively, they must be able to adapt to changing conditions. This means that they need to be able to sense and act quickly. However, they also need to balance mission-specific tasks, like taking pictures of a room, with more general tasks, like avoiding obstacles. To solve this problem, it is important to maintain a relative utilization of shared resources among tasks. Unfortunately, it is difficult to produce optimal scheduling strategies because complete knowledge of task behavior is unlikely to be available. Instead, we must learn suitable strategies through interactions with the system. Although the problem state space is countably infinite, we can use the problem's structure to ensure efficient learning. This article focuses on the sample complexity of reinforcement learning in this domain.",1
"Most conventional Reinforcement Learning (RL) algorithms aim to optimize decision-making rules in terms of the expected returns. However, especially for risk management purposes, other risk-sensitive criteria such as the value-at-risk or the expected shortfall are sometimes preferred in real applications. Here, we describe a parametric method for estimating density of the returns, which allows us to handle various criteria in a unified manner. We first extend the Bellman equation for the conditional expected return to cover a conditional probability density of the returns. Then we derive an extension of the TD-learning algorithm for estimating the return densities in an unknown environment. As test instances, several parametric density estimation algorithms are presented for the Gaussian, Laplace, and skewed Laplace distributions. We show that these algorithms lead to risk-sensitive as well as robust RL paradigms through numerical experiments.",0
"Most Reinforcement Learning (RL) algorithms typically focus on optimizing decision-making rules based on expected returns. However, in real-world applications, alternative risk-sensitive criteria such as value-at-risk or expected shortfall may be preferred for better risk management. In this paper, we propose a parametric method to estimate the density of returns, which enables the handling of various criteria in a unified manner. We achieve this by extending the Bellman equation to cover a conditional probability density of returns, followed by the development of an extension of the TD-learning algorithm for estimating return densities in an unknown environment. We also present several parametric density estimation algorithms for Gaussian, Laplace, and skewed Laplace distributions. Through numerical experiments, we demonstrate that these algorithms lead to risk-sensitive and robust RL paradigms.",1
"The explore{exploit dilemma is one of the central challenges in Reinforcement Learning (RL). Bayesian RL solves the dilemma by providing the agent with information in the form of a prior distribution over environments; however, full Bayesian planning is intractable. Planning with the mean MDP is a common myopic approximation of Bayesian planning. We derive a novel reward bonus that is a function of the posterior distribution over environments, which, when added to the reward in planning with the mean MDP, results in an agent which explores efficiently and effectively. Although our method is similar to existing methods when given an uninformative or unstructured prior, unlike existing methods, our method can exploit structured priors. We prove that our method results in a polynomial sample complexity and empirically demonstrate its advantages in a structured exploration task.",0
"Reinforcement Learning (RL) faces a major challenge known as the explore-exploit dilemma, which Bayesian RL addresses by providing the agent with a prior distribution over environments. However, full Bayesian planning is impractical, and planning with the mean Markov Decision Process (MDP) is a common myopic approximation. To enhance exploration efficiency and effectiveness, we propose a new reward bonus that considers the posterior distribution over environments. Our method accommodates structured priors and outperforms existing methods, especially when confronted with uninformative or unstructured priors. We demonstrate that our technique's sample complexity is polynomial and showcase its efficacy in a structured exploration task.",1
"Contextual bandit learning is a reinforcement learning problem where the learner repeatedly receives a set of features (context), takes an action and receives a reward based on the action and context. We consider this problem under a realizability assumption: there exists a function in a (known) function class, always capable of predicting the expected reward, given the action and context. Under this assumption, we show three things. We present a new algorithm---Regressor Elimination--- with a regret similar to the agnostic setting (i.e. in the absence of realizability assumption). We prove a new lower bound showing no algorithm can achieve superior performance in the worst case even with the realizability assumption. However, we do show that for any set of policies (mapping contexts to actions), there is a distribution over rewards (given context) such that our new algorithm has constant regret unlike the previous approaches.",0
"The problem of contextual bandit learning involves a learner who receives a set of features and must take an action to receive a reward based on the context. This is done under the assumption that there exists a known function capable of predicting the expected reward. We demonstrate three findings under this assumption: the introduction of a new algorithm, Regressor Elimination, with regret similar to the agnostic setting; proof of a new lower bound demonstrating no algorithm can outperform in the worst case scenario even with the realizability assumption; and proof that for any set of policies, there exists a distribution over rewards where our new algorithm has a constant regret, unlike previous methods.",1
"The use of Reinforcement Learning in real-world scenarios is strongly limited by issues of scale. Most RL learning algorithms are unable to deal with problems composed of hundreds or sometimes even dozens of possible actions, and therefore cannot be applied to many real-world problems. We consider the RL problem in the supervised classification framework where the optimal policy is obtained through a multiclass classifier, the set of classes being the set of actions of the problem. We introduce error-correcting output codes (ECOCs) in this setting and propose two new methods for reducing complexity when using rollouts-based approaches. The first method consists in using an ECOC-based classifier as the multiclass classifier, reducing the learning complexity from O(A2) to O(Alog(A)). We then propose a novel method that profits from the ECOC's coding dictionary to split the initial MDP into O(log(A)) seperate two-action MDPs. This second method reduces learning complexity even further, from O(A2) to O(log(A)), thus rendering problems with large action sets tractable. We finish by experimentally demonstrating the advantages of our approach on a set of benchmark problems, both in speed and performance.",0
"The limited use of Reinforcement Learning in real-world situations is mainly due to the problem of scale. Many RL learning algorithms are incapable of handling problems consisting of numerous possible actions, which makes them unsuitable for many real-world issues. To overcome this challenge, we propose a new approach that considers the RL problem within the supervised classification framework. This approach involves obtaining the optimal policy using a multiclass classifier, where the set of classes represents the set of actions in the problem. To reduce complexity, we introduce error-correcting output codes (ECOCs) and propose two methods based on rollouts-based approaches. The first method involves using an ECOC-based classifier as the multiclass classifier, which reduces learning complexity from O(A2) to O(Alog(A)). The second method utilizes the ECOC's coding dictionary to split the initial MDP into separate two-action MDPs, thereby reducing learning complexity even further from O(A2) to O(log(A)), making it possible to tackle problems with large action sets. We demonstrate the effectiveness of our approach through experiments on a set of benchmark problems, which show improved performance and speed.",1
"Bayesian priors offer a compact yet general means of incorporating domain knowledge into many learning tasks. The correctness of the Bayesian analysis and inference, however, largely depends on accuracy and correctness of these priors. PAC-Bayesian methods overcome this problem by providing bounds that hold regardless of the correctness of the prior distribution. This paper introduces the first PAC-Bayesian bound for the batch reinforcement learning problem with function approximation. We show how this bound can be used to perform model-selection in a transfer learning scenario. Our empirical results confirm that PAC-Bayesian policy evaluation is able to leverage prior distributions when they are informative and, unlike standard Bayesian RL approaches, ignore them when they are misleading.",0
"The utilization of Bayesian priors is a concise and universal approach to integrating domain knowledge into various learning tasks. However, the accuracy and correctness of these priors are crucial to the precision of the Bayesian analysis and inference. Nonetheless, PAC-Bayesian techniques offer solutions to this issue by providing bounds that remain valid irrespective of the accuracy of the prior distribution. In this article, we present the initial PAC-Bayesian bound for the batch reinforcement learning problem with function approximation and illustrate its application in model-selection for a transfer learning scenario. Our experimental findings demonstrate that PAC-Bayesian policy evaluation can effectively leverage informative prior distributions, while disregarding misleading ones, in contrast to standard Bayesian RL methods.",1
Reinforcement learning addresses the dilemma between exploration to find profitable actions and exploitation to act according to the best observations already made. Bandit problems are one such class of problems in stateless environments that represent this explore/exploit situation. We propose a learning algorithm for bandit problems based on fractional expectation of rewards acquired. The algorithm is theoretically shown to converge on an eta-optimal arm and achieve O(n) sample complexity. Experimental results show the algorithm incurs substantially lower regrets than parameter-optimized eta-greedy and SoftMax approaches and other low sample complexity state-of-the-art techniques.,0
"The problem of balancing exploration to discover profitable actions and exploitation to act based on prior observations is addressed by reinforcement learning. In stateless environments, bandit problems exemplify this explore/exploit challenge. Our proposed learning algorithm for bandit problems relies on fractional expectation of rewards acquired and is demonstrated to converge on an eta-optimal arm with a sample complexity of O(n). The algorithm outperforms parameter-optimized eta-greedy and SoftMax approaches, as well as other low sample complexity state-of-the-art techniques, with significantly lower regrets according to experimental results.",1
"Inverse reinforcement learning (IRL) addresses the problem of recovering a task description given a demonstration of the optimal policy used to solve such a task. The optimal policy is usually provided by an expert or teacher, making IRL specially suitable for the problem of apprenticeship learning. The task description is encoded in the form of a reward function of a Markov decision process (MDP). Several algorithms have been proposed to find the reward function corresponding to a set of demonstrations. One of the algorithms that has provided best results in different applications is a gradient method to optimize a policy squared error criterion. On a parallel line of research, other authors have presented recently a gradient approximation of the maximum likelihood estimate of the reward signal. In general, both approaches approximate the gradient estimate and the criteria at different stages to make the algorithm tractable and efficient. In this work, we provide a detailed description of the different methods to highlight differences in terms of reward estimation, policy similarity and computational costs. We also provide experimental results to evaluate the differences in performance of the methods.",0
"The problem of recovering a task description from a demonstration of the optimal policy is addressed by Inverse reinforcement learning (IRL). IRL is particularly suitable for apprenticeship learning since the optimal policy is usually provided by an expert or teacher. The task description is encoded as a reward function of a Markov decision process (MDP). Various algorithms have been proposed to find the reward function corresponding to a set of demonstrations. The gradient method to optimize a policy squared error criterion is one such algorithm that has provided the best results in many applications. Recently, other authors have presented a gradient approximation of the maximum likelihood estimate of the reward signal. Both approaches approximate the gradient estimate and criteria at different stages to make the algorithm efficient. This work provides a detailed description of these methods and highlights the differences in terms of reward estimation, policy similarity, and computational costs. Experimental results are also provided to evaluate the performance of the methods.",1
"Traditional Reinforcement Learning (RL) has focused on problems involving many states and few actions, such as simple grid worlds. Most real world problems, however, are of the opposite type, Involving Few relevant states and many actions. For example, to return home from a conference, humans identify only few subgoal states such as lobby, taxi, airport etc. Each valid behavior connecting two such states can be viewed as an action, and there are trillions of them. Assuming the subgoal identification problem is already solved, the quality of any RL method---in real-world settings---depends less on how well it scales with the number of states than on how well it scales with the number of actions. This is where our new method T-Learning excels, by evaluating the relatively few possible transits from one state to another in a policy-independent way, rather than a huge number of state-action pairs, or states in traditional policy-dependent ways. Illustrative experiments demonstrate that performance improvements of T-Learning over Q-learning can be arbitrarily large.",0
"In the past, Reinforcement Learning (RL) has mainly dealt with problems that involve numerous states and a limited number of actions, such as basic grid worlds. Nonetheless, the majority of real-world issues are the opposite, requiring few significant states and many actions. For instance, when people head home from a conference, they only consider a few subgoal states, like the lobby, taxi, and airport. Each possible behavior connecting these states is seen as an action, and there are trillions of them. Assuming the subgoal identification problem is resolved, the effectiveness of any RL method in real-world situations depends less on how well it scales with the number of states, but rather how well it scales with the number of actions. Our new method, T-Learning, outperforms traditional RL methods by evaluating the relatively few potential transits from one state to another in a policy-independent manner, instead of using a vast number of state-action pairs or states in the conventional policy-dependent ways. Demonstrative experiments show that the performance enhancements of T-Learning over Q-learning can be limitless.",1
"Although exploratory behaviors are ubiquitous in the animal kingdom, their computational underpinnings are still largely unknown. Behavioral Psychology has identified learning as a primary drive underlying many exploratory behaviors. Exploration is seen as a means for an animal to gather sensory data useful for reducing its ignorance about the environment. While related problems have been addressed in Data Mining and Reinforcement Learning, the computational modeling of learning-driven exploration by embodied agents is largely unrepresented.   Here, we propose a computational theory for learning-driven exploration based on the concept of missing information that allows an agent to identify informative actions using Bayesian inference. We demonstrate that when embodiment constraints are high, agents must actively coordinate their actions to learn efficiently. Compared to earlier approaches, our exploration policy yields more efficient learning across a range of worlds with diverse structures. The improved learning in turn affords greater success in general tasks including navigation and reward gathering. We conclude by discussing how the proposed theory relates to previous information-theoretic objectives of behavior, such as predictive information and the free energy principle, and how it might contribute to a general theory of exploratory behavior.",0
"The computational basis of exploratory behaviors in the animal kingdom remains largely unknown, despite their widespread occurrence. Behavioral Psychology has identified learning as a primary drive underlying many of these behaviors, which are seen as a means for animals to reduce their ignorance about the environment by gathering useful sensory data. However, the computational modeling of learning-driven exploration by embodied agents has been largely unexplored, with related problems only being addressed in Data Mining and Reinforcement Learning. In this paper, we propose a new computational theory for learning-driven exploration that relies on Bayesian inference and the concept of missing information to identify informative actions. We show that when embodiment constraints are high, agents need to actively coordinate their actions to learn efficiently. Compared to previous approaches, our exploration policy leads to more efficient learning across a range of worlds with diverse structures, resulting in greater success in tasks such as navigation and reward gathering. Finally, we discuss how our proposed theory relates to previous information-theoretic objectives of behavior and how it could contribute to a general theory of exploratory behavior.",1
"Music Sight Reading is a complex process in which when it is occurred in the brain some learning attributes would be emerged. Besides giving a model based on actor-critic method in the Reinforcement Learning, the agent is considered to have a neural network structure. We studied on where the sight reading process is happened and also a serious problem which is how the synaptic weights would be adjusted through the learning process. The model we offer here is a computational model on which an updated weights equation to fix the weights is accompanied too.",0
"Sight reading music is a multifaceted process that triggers the emergence of learning attributes in the brain. Our research delves into the location of this cognitive process and a significant issue regarding the adjustment of synaptic weights during the learning process. In our study, we propose a computational model utilizing an actor-critic method in Reinforcement Learning and a neural network structure of the agent. The model features an updated weights equation to address the problem of fixing the weights.",1
"We generalise the problem of inverse reinforcement learning to multiple tasks, from multiple demonstrations. Each one may represent one expert trying to solve a different task, or as different experts trying to solve the same task. Our main contribution is to formalise the problem as statistical preference elicitation, via a number of structured priors, whose form captures our biases about the relatedness of different tasks or expert policies. In doing so, we introduce a prior on policy optimality, which is more natural to specify. We show that our framework allows us not only to learn to efficiently from multiple experts but to also effectively differentiate between the goals of each. Possible applications include analysing the intrinsic motivations of subjects in behavioural experiments and learning from multiple teachers.",0
"Our focus is on extending the problem of inverse reinforcement learning to encompass multiple tasks, each of which is tackled by one or more experts. Our approach involves formalising the problem through statistical preference elicitation. We achieve this by employing a range of structured priors that reflect our beliefs about the relationship between different tasks or expert policies. By incorporating a prior on policy optimality, we establish a more intuitive means of specification. Our framework not only enables efficient learning from multiple experts, but also facilitates clear discrimination between their respective goals. This approach has various potential applications, including investigating the intrinsic motivations of subjects in behavioural experiments and learning from multiple teachers.",1
"In the Bayesian approach to sequential decision making, exact calculation of the (subjective) utility is intractable. This extends to most special cases of interest, such as reinforcement learning problems. While utility bounds are known to exist for this problem, so far none of them were particularly tight. In this paper, we show how to efficiently calculate a lower bound, which corresponds to the utility of a near-optimal memoryless policy for the decision problem, which is generally different from both the Bayes-optimal policy and the policy which is optimal for the expected MDP under the current belief. We then show how these can be applied to obtain robust exploration policies in a Bayesian reinforcement learning setting.",0
"The calculation of subjective utility in the Bayesian approach to sequential decision making is difficult and impractical. This applies to most relevant cases, including reinforcement learning problems, for which utility bounds have been identified, but none of which have been sufficiently tight. This paper presents a method to efficiently compute a lower bound that represents the utility of a memoryless policy that is nearly optimal for the decision problem. This policy is distinct from both the Bayes-optimal policy and the policy that is optimal for the expected MDP under the current belief. The paper also demonstrates how these findings can be used to develop reliable exploration policies in a Bayesian reinforcement learning context.",1
"The exploration-exploitation trade-off is among the central challenges of reinforcement learning. The optimal Bayesian solution is intractable in general. This paper studies to what extent analytic statements about optimal learning are possible if all beliefs are Gaussian processes. A first order approximation of learning of both loss and dynamics, for nonlinear, time-varying systems in continuous time and space, subject to a relatively weak restriction on the dynamics, is described by an infinite-dimensional partial differential equation. An approximate finite-dimensional projection gives an impression for how this result may be helpful.",0
"Reinforcement learning faces a significant challenge in balancing exploration and exploitation. The ideal Bayesian solution is generally too complex to be practical. This research investigates the feasibility of making analytical observations about optimal learning when all beliefs are Gaussian processes. By imposing a mild restriction on the dynamics and utilizing a first-order approximation of learning for nonlinear, time-varying systems in continuous time and space, an infinite-dimensional partial differential equation can describe the learning of both loss and dynamics. A rough idea of how this finding may be beneficial can be obtained through an approximate finite-dimensional projection.",1
"In this paper we present a general, flexible framework for learning mappings from images to actions by interacting with the environment. The basic idea is to introduce a feature-based image classifier in front of a reinforcement learning algorithm. The classifier partitions the visual space according to the presence or absence of few highly informative local descriptors that are incrementally selected in a sequence of attempts to remove perceptual aliasing. We also address the problem of fighting overfitting in such a greedy algorithm. Finally, we show how high-level visual features can be generated when the power of local descriptors is insufficient for completely disambiguating the aliased states. This is done by building a hierarchy of composite features that consist of recursive spatial combinations of visual features. We demonstrate the efficacy of our algorithms by solving three visual navigation tasks and a visual version of the classical Car on the Hill control problem.",0
"This paper introduces a versatile and broad framework for acquiring knowledge of how to translate images to actions through interactions with the surroundings. The fundamental concept is to implement a feature-based image classifier ahead of a reinforcement learning algorithm. The classifier divides the visual space based on the existence or non-existence of a few highly informative local descriptors that are gradually selected to eliminate perceptual aliasing. Additionally, we tackle the issue of preventing overfitting in such a greedy algorithm. Moreover, we display how high-level visual features can be created if the potential of local descriptors is inadequate for completely disentangling the ambiguous states. We achieve this by constructing a composite feature hierarchy that comprises recursive spatial combinations of visual features. We prove the effectiveness of our methods by resolving three visual navigation tasks and a visual version of the classical Car on the Hill control problem.",1
"This paper introduces a machine learning based collaborative multi-band spectrum sensing policy for cognitive radios. The proposed sensing policy guides secondary users to focus the search of unused radio spectrum to those frequencies that persistently provide them high data rate. The proposed policy is based on machine learning, which makes it adaptive with the temporally and spatially varying radio spectrum. Furthermore, there is no need for dynamic modeling of the primary activity since it is implicitly learned over time. Energy efficiency is achieved by minimizing the number of assigned sensors per each subband under a constraint on miss detection probability. It is important to control the missed detections because they cause collisions with primary transmissions and lead to retransmissions at both the primary and secondary user. Simulations show that the proposed machine learning based sensing policy improves the overall throughput of the secondary network and improves the energy efficiency while controlling the miss detection probability.",0
"In this article, a collaborative multi-band spectrum sensing policy for cognitive radios is presented, which is based on machine learning. The policy directs secondary users to search for unused radio spectrum in frequencies that consistently offer high data rates. The machine learning aspect of the policy enables it to adapt to changes in the radio spectrum over time and space without requiring dynamic modeling of primary activity. To achieve energy efficiency, the policy minimizes the number of sensors assigned per subband while controlling the miss detection probability to avoid collisions with primary transmissions. The simulations conducted indicate that the proposed machine learning-based sensing policy enhances the overall throughput of the secondary network, improves energy efficiency, and controls miss detection probability.",1
"Research in reinforcement learning has produced algorithms for optimal decision making under uncertainty that fall within two main types. The first employs a Bayesian framework, where optimality improves with increased computational time. This is because the resulting planning task takes the form of a dynamic programming problem on a belief tree with an infinite number of states. The second type employs relatively simple algorithm which are shown to suffer small regret within a distribution-free framework. This paper presents a lower bound and a high probability upper bound on the optimal value function for the nodes in the Bayesian belief tree, which are analogous to similar bounds in POMDPs. The bounds are then used to create more efficient strategies for exploring the tree. The resulting algorithms are compared with the distribution-free algorithm UCB1, as well as a simpler baseline algorithm on multi-armed bandit problems.",0
"Algorithms for optimal decision making under uncertainty in reinforcement learning fall into two main categories. The first is based on a Bayesian framework that requires more computational time for improved optimality. This is because the planning task involves dynamic programming on a belief tree with an infinite number of states. The second category uses simpler algorithms that suffer only minor regret within a distribution-free framework. This study introduces lower and upper bounds for the optimal value function at the nodes in the Bayesian belief tree, similar to those in POMDPs. These bounds enhance exploration of the tree and result in more efficient algorithms compared to the distribution-free UCB1 and a basic algorithm in multi-armed bandit problems.",1
"In this paper, we consider Markov Decision Processes (MDPs) with error states. Error states are those states entering which is undesirable or dangerous. We define the risk with respect to a policy as the probability of entering such a state when the policy is pursued. We consider the problem of finding good policies whose risk is smaller than some user-specified threshold, and formalize it as a constrained MDP with two criteria. The first criterion corresponds to the value function originally given. We will show that the risk can be formulated as a second criterion function based on a cumulative return, whose definition is independent of the original value function. We present a model free, heuristic reinforcement learning algorithm that aims at finding good deterministic policies. It is based on weighting the original value function and the risk. The weight parameter is adapted in order to find a feasible solution for the constrained problem that has a good performance with respect to the value function. The algorithm was successfully applied to the control of a feed tank with stochastic inflows that lies upstream of a distillation column. This control task was originally formulated as an optimal control problem with chance constraints, and it was solved under certain assumptions on the model to obtain an optimal solution. The power of our learning algorithm is that it can be used even when some of these restrictive assumptions are relaxed.",0
"The focus of this paper is on Markov Decision Processes (MDPs) that contain error states, which are states that are unsafe or undesirable to enter. The aim is to identify policies that have a lower risk of entering these states, as determined by a user-defined threshold. To achieve this, the authors formalize the problem as a constrained MDP with two criteria: the original value function and the newly defined risk function. They propose a heuristic, model-free reinforcement learning algorithm that optimizes both criteria by adapting the weight parameter. The algorithm is applied to a control task with stochastic inflows and distillation, where it outperforms the optimal control approach under fewer restrictive assumptions.",1
"In this paper, we propose a novel policy iteration method, called dynamic policy programming (DPP), to estimate the optimal policy in the infinite-horizon Markov decision processes. We prove the finite-iteration and asymptotic l\infty-norm performance-loss bounds for DPP in the presence of approximation/estimation error. The bounds are expressed in terms of the l\infty-norm of the average accumulated error as opposed to the l\infty-norm of the error in the case of the standard approximate value iteration (AVI) and the approximate policy iteration (API). This suggests that DPP can achieve a better performance than AVI and API since it averages out the simulation noise caused by Monte-Carlo sampling throughout the learning process. We examine this theoretical results numerically by com- paring the performance of the approximate variants of DPP with existing reinforcement learning (RL) methods on different problem domains. Our results show that, in all cases, DPP-based algorithms outperform other RL methods by a wide margin.",0
"The objective of this paper is to introduce a new method for deriving the most appropriate policy in infinite-horizon Markov decision processes. This method is called dynamic policy programming (DPP). We establish finite-iteration and asymptotic l\infty-norm performance-loss boundaries for DPP, accounting for approximation/estimation errors. Unlike standard approximate value iteration (AVI) and approximate policy iteration (API), these boundaries are based on the l\infty-norm of the average accumulated error. This implies that DPP can outperform AVI and API since it mitigates Monte-Carlo sampling-related simulation noise throughout the learning process. We numerically test this theory by comparing the performance of DPP-based algorithms with existing reinforcement learning (RL) methods across various problem domains. Our findings indicate that DPP-based algorithms consistently outperform other RL methods by a significant margin.",1
"We provide a formal, simple and intuitive theory of rational decision making including sequential decisions that affect the environment. The theory has a geometric flavor, which makes the arguments easy to visualize and understand. Our theory is for complete decision makers, which means that they have a complete set of preferences. Our main result shows that a complete rational decision maker implicitly has a probabilistic model of the environment. We have a countable version of this result that brings light on the issue of countable vs finite additivity by showing how it depends on the geometry of the space which we have preferences over. This is achieved through fruitfully connecting rationality with the Hahn-Banach Theorem. The theory presented here can be viewed as a formalization and extension of the betting odds approach to probability of Ramsey and De Finetti.",0
"Our theory offers a formal, uncomplicated, and intuitive approach to rational decision making, encompassing sequential decisions that impact the surroundings. The theory is imbued with a geometric essence, making it simple to visualize and comprehend. It is designed for complete decision makers with a comprehensive set of preferences. Our primary finding demonstrates that complete rational decision makers inherently possess a probabilistic model of the environment. Our countable version of this result sheds light on the issue of countable versus finite additivity and how it relates to the geometry of the space that we have preferences over, utilizing the Hahn-Banach Theorem. This theory serves as a formalization and expansion of Ramsey and De Finetti's betting odds approach to probability.",1
"We state the problem of inverse reinforcement learning in terms of preference elicitation, resulting in a principled (Bayesian) statistical formulation. This generalises previous work on Bayesian inverse reinforcement learning and allows us to obtain a posterior distribution on the agent's preferences, policy and optionally, the obtained reward sequence, from observations. We examine the relation of the resulting approach to other statistical methods for inverse reinforcement learning via analysis and experimental results. We show that preferences can be determined accurately, even if the observed agent's policy is sub-optimal with respect to its own preferences. In that case, significantly improved policies with respect to the agent's preferences are obtained, compared to both other methods and to the performance of the demonstrated policy.",0
"The problem of inverse reinforcement learning is presented in a manner that involves gathering preferences, leading to a systematic statistical formulation based on Bayesian principles. This extends the previous work done on Bayesian inverse reinforcement learning and enables us to acquire a posterior distribution on the policy, preferences and, if required, the reward sequence based on observations. We analyze and experimentally demonstrate the correlation between this approach and other statistical techniques for inverse reinforcement learning. Our findings indicate that even if the agent's policy is not optimal with regards to its own preferences, preferences can still be determined with accuracy. In such cases, the resulting policies are substantially better than those obtained from other methods and the exhibited policy.",1
"Shaping has proven to be a powerful but precarious means of improving reinforcement learning performance. Ng, Harada, and Russell (1999) proposed the potential-based shaping algorithm for adding shaping rewards in a way that guarantees the learner will learn optimal behavior. In this note, we prove certain similarities between this shaping algorithm and the initialization step required for several reinforcement learning algorithms. More specifically, we prove that a reinforcement learner with initial Q-values based on the shaping algorithm's potential function make the same updates throughout learning as a learner receiving potential-based shaping rewards. We further prove that under a broad category of policies, the behavior of these two learners are indistinguishable. The comparison provides intuition on the theoretical properties of the shaping algorithm as well as a suggestion for a simpler method for capturing the algorithm's benefit. In addition, the equivalence raises previously unaddressed issues concerning the efficiency of learning with potential-based shaping.",0
"The use of shaping to enhance reinforcement learning performance is a potent but risky technique. Ng, Harada, and Russell (1999) proposed the potential-based shaping algorithm, which adds shaping rewards to ensure the learner learns optimal behavior. In this piece, we demonstrate that this shaping algorithm shares similarities with the initialization step of various reinforcement learning algorithms. Specifically, we prove that a reinforcement learner with initial Q-values based on the shaping algorithm's potential function updates itself identically to a learner that receives potential-based shaping rewards. Moreover, we demonstrate that these two learners behave similarly under a wide range of policies. This comparison provides insight into the theoretical properties of the shaping algorithm and suggests a simpler method for capturing its benefits. Additionally, the equivalence raises concerns about the efficiency of learning with potential-based shaping that have not been addressed before.",1
"In this article, a survey of several important equilibrium concepts for decentralized networks is presented. The term decentralized is used here to refer to scenarios where decisions (e.g., choosing a power allocation policy) are taken autonomously by devices interacting with each other (e.g., through mutual interference). The iterative long-term interaction is characterized by stable points of the wireless network called equilibria. The interest in these equilibria stems from the relevance of network stability and the fact that they can be achieved by letting radio devices to repeatedly interact over time. To achieve these equilibria, several learning techniques, namely, the best response dynamics, fictitious play, smoothed fictitious play, reinforcement learning algorithms, and regret matching, are discussed in terms of information requirements and convergence properties. Most of the notions introduced here, for both equilibria and learning schemes, are illustrated by a simple case study, namely, an interference channel with two transmitter-receiver pairs.",0
"This article discusses various equilibrium concepts for decentralized networks, where decisions are made autonomously by devices through mutual interference. Equilibria are stable points of the wireless network resulting from iterative long-term interactions. The importance of these equilibria lies in network stability and their achievability through repeated radio device interactions. To attain these equilibria, the article explores several learning techniques such as best response dynamics, fictitious play, smoothed fictitious play, reinforcement learning algorithms, and regret matching. The article uses a simple case study of an interference channel with two transmitter-receiver pairs to illustrate the concepts of equilibria and learning schemes. Information requirements and convergence properties of the discussed techniques are also analyzed.",1
"The recursive least-squares (RLS) algorithm is one of the most well-known algorithms used in adaptive filtering, system identification and adaptive control. Its popularity is mainly due to its fast convergence speed, which is considered to be optimal in practice. In this paper, RLS methods are used to solve reinforcement learning problems, where two new reinforcement learning algorithms using linear value function approximators are proposed and analyzed. The two algorithms are called RLS-TD(lambda) and Fast-AHC (Fast Adaptive Heuristic Critic), respectively. RLS-TD(lambda) can be viewed as the extension of RLS-TD(0) from lambda=0 to general lambda within interval [0,1], so it is a multi-step temporal-difference (TD) learning algorithm using RLS methods. The convergence with probability one and the limit of convergence of RLS-TD(lambda) are proved for ergodic Markov chains. Compared to the existing LS-TD(lambda) algorithm, RLS-TD(lambda) has advantages in computation and is more suitable for online learning. The effectiveness of RLS-TD(lambda) is analyzed and verified by learning prediction experiments of Markov chains with a wide range of parameter settings. The Fast-AHC algorithm is derived by applying the proposed RLS-TD(lambda) algorithm in the critic network of the adaptive heuristic critic method. Unlike conventional AHC algorithm, Fast-AHC makes use of RLS methods to improve the learning-prediction efficiency in the critic. Learning control experiments of the cart-pole balancing and the acrobot swing-up problems are conducted to compare the data efficiency of Fast-AHC with conventional AHC. From the experimental results, it is shown that the data efficiency of learning control can also be improved by using RLS methods in the learning-prediction process of the critic. The performance of Fast-AHC is also compared with that of the AHC method using LS-TD(lambda). Furthermore, it is demonstrated in the experiments that different initial values of the variance matrix in RLS-TD(lambda) are required to get better performance not only in learning prediction but also in learning control. The experimental results are analyzed based on the existing theoretical work on the transient phase of forgetting factor RLS methods.",0
"The recursive least-squares (RLS) algorithm is a popular technique used in adaptive filtering, system identification, and adaptive control due to its rapid convergence speed, which is optimal in practical applications. This study employs RLS methods to address reinforcement learning problems, proposing two new reinforcement learning algorithms: RLS-TD(lambda) and Fast-AHC (Fast Adaptive Heuristic Critic). RLS-TD(lambda) is a multi-step temporal-difference (TD) learning algorithm that extends RLS-TD(0) from lambda=0 to general lambda within the [0,1] interval. Analyzing RLS-TD(lambda), the study proves its convergence with probability one and limit of convergence for ergodic Markov chains. Compared to the existing LS-TD(lambda) algorithm, RLS-TD(lambda) offers computational advantages and is more suitable for online learning. The study conducts learning prediction experiments of Markov chains with a variety of parameter settings to verify the effectiveness of RLS-TD(lambda). The Fast-AHC algorithm uses RLS-TD(lambda) in the critic network of the adaptive heuristic critic method. Unlike conventional AHC algorithms, Fast-AHC enhances the learning-prediction efficiency in the critic by using RLS methods. The study conducts learning control experiments of the cart-pole balancing and the acrobot swing-up problems to compare the data efficiency of Fast-AHC with conventional AHC. The results demonstrate that RLS methods improve the data efficiency of learning control in the critic. The study also compares the performance of Fast-AHC with that of the AHC method using LS-TD(lambda). Additionally, the study shows that different initial values of the variance matrix in RLS-TD(lambda) improve the performance in learning prediction and learning control. Finally, the experimental results are analyzed based on existing theoretical work on the transient phase of forgetting factor RLS methods.",1
"Imitation can be viewed as a means of enhancing learning in multiagent environments. It augments an agent's ability to learn useful behaviors by making intelligent use of the knowledge implicit in behaviors demonstrated by cooperative teachers or other more experienced agents. We propose and study a formal model of implicit imitation that can accelerate reinforcement learning dramatically in certain cases. Roughly, by observing a mentor, a reinforcement-learning agent can extract information about its own capabilities in, and the relative value of, unvisited parts of the state space. We study two specific instantiations of this model, one in which the learning agent and the mentor have identical abilities, and one designed to deal with agents and mentors with different action sets. We illustrate the benefits of implicit imitation by integrating it with prioritized sweeping, and demonstrating improved performance and convergence through observation of single and multiple mentors. Though we make some stringent assumptions regarding observability and possible interactions, we briefly comment on extensions of the model that relax these restricitions.",0
"In multiagent environments, imitation can be seen as a method to improve learning. By observing cooperative teachers or experienced agents, an agent can enhance its ability to learn effective behaviors using the knowledge gained from the observed behaviors. We have developed a formal model of implicit imitation that can significantly expedite reinforcement learning in certain scenarios. The model enables a reinforcement-learning agent to learn about its own capabilities and the value of unvisited parts of the state space by observing a mentor. We have examined two specific implementations of this model: one with identical abilities between the mentor and learning agent, and one for situations where they have different action sets. By integrating implicit imitation with prioritized sweeping, we have shown improved performance and convergence through observation of one or multiple mentors. While we have imposed some strict assumptions on observability and interactions, we discuss possible extensions of the model that can relax these limitations.",1
"Designing the dialogue policy of a spoken dialogue system involves many nontrivial choices. This paper presents a reinforcement learning approach for automatically optimizing a dialogue policy, which addresses the technical challenges in applying reinforcement learning to a working dialogue system with human users. We report on the design, construction and empirical evaluation of NJFun, an experimental spoken dialogue system that provides users with access to information about fun things to do in New Jersey. Our results show that by optimizing its performance via reinforcement learning, NJFun measurably improves system performance.",0
"The process of creating a dialogue policy for a spoken dialogue system encompasses a multitude of complex decisions. This article introduces a reinforcement learning technique that strives to streamline the optimization of a dialogue policy by addressing the technical difficulties encountered in applying reinforcement learning to a functional dialogue system that interacts with humans. NJFun, a spoken dialogue system that offers users information on enjoyable activities in New Jersey, was designed, constructed, and empirically assessed. Our findings indicate that NJFun enhances its performance by employing reinforcement learning to optimize its functionality.",1
"There are two distinct approaches to solving reinforcement learning problems, namely, searching in value function space and searching in policy space. Temporal difference methods and evolutionary algorithms are well-known examples of these approaches. Kaelbling, Littman and Moore recently provided an informative survey of temporal difference methods. This article focuses on the application of evolutionary algorithms to the reinforcement learning problem, emphasizing alternative policy representations, credit assignment methods, and problem-specific genetic operators. Strengths and weaknesses of the evolutionary approach to reinforcement learning are presented, along with a survey of representative applications.",0
"Reinforcement learning problems can be solved using either the value function space or policy space search approaches. Temporal difference methods and evolutionary algorithms are examples of these approaches. Kaelbling, Littman, and Moore's survey focuses on temporal difference methods, while this article discusses the use of evolutionary algorithms in reinforcement learning. Specific attention is given to alternative policy representations, credit assignment methods, and problem-specific genetic operators. The evolutionary approach's strengths and weaknesses are also presented, and a survey of notable applications is included.",1
"We present two alternative ways to apply PAC-Bayesian analysis to sequences of dependent random variables. The first is based on a new lemma that enables to bound expectations of convex functions of certain dependent random variables by expectations of the same functions of independent Bernoulli random variables. This lemma provides an alternative tool to Hoeffding-Azuma inequality to bound concentration of martingale values. Our second approach is based on integration of Hoeffding-Azuma inequality with PAC-Bayesian analysis. We also introduce a way to apply PAC-Bayesian analysis in situation of limited feedback. We combine the new tools to derive PAC-Bayesian generalization and regret bounds for the multiarmed bandit problem. Although our regret bound is not yet as tight as state-of-the-art regret bounds based on other well-established techniques, our results significantly expand the range of potential applications of PAC-Bayesian analysis and introduce a new analysis tool to reinforcement learning and many other fields, where martingales and limited feedback are encountered.",0
"We offer two different approaches for utilizing PAC-Bayesian analysis on sequences of dependent random variables. The first method utilizes a novel lemma that allows for bounding expectations of convex functions of specific dependent random variables by expectations of the same functions of independent Bernoulli random variables. This alternative tool can be used instead of the Hoeffding-Azuma inequality to limit the concentration of martingale values. Our second approach is a combination of the Hoeffding-Azuma inequality with PAC-Bayesian analysis. Additionally, we introduce a way to apply PAC-Bayesian analysis in scenarios where feedback is limited. We integrate these new tools to derive PAC-Bayesian generalization and regret bounds for the multiarmed bandit problem. While our regret bound is not as precise as those derived from other established techniques, our results broaden the range of applications for PAC-Bayesian analysis and provide a fresh analysis tool for reinforcement learning and other fields that encounter martingales and limited feedback.",1
"In experimenting with off-policy temporal difference (TD) methods in hierarchical reinforcement learning (HRL) systems, we have observed unwanted on-policy learning under reproducible conditions. Here we present modifications to several TD methods that prevent unintentional on-policy learning from occurring. These modifications create a tension between exploration and learning. Traditional TD methods require commitment to finishing subtasks without exploration in order to update Q-values for early actions with high probability. One-step intra-option learning and temporal second difference traces (TSDT) do not suffer from this limitation. We demonstrate that our HRL system is efficient without commitment to completion of subtasks in a cliff-walking domain, contrary to a widespread claim in the literature that it is critical for efficiency of learning. Furthermore, decreasing commitment as exploration progresses is shown to improve both online performance and the resultant policy in the taxicab domain, opening a new avenue for research into when it is more beneficial to continue with the current subtask or to replan.",0
"During our experiments with off-policy temporal difference (TD) methods in hierarchical reinforcement learning (HRL) systems, we have encountered an issue of undesired on-policy learning that occurred repeatedly. We have developed modifications to several TD methods to prevent such unintentional on-policy learning. However, these modifications have created a conflict between exploration and learning. Traditional TD methods require a commitment to completing subtasks without exploration in order to update Q-values for early actions with high probability. In contrast, one-step intra-option learning and temporal second difference traces (TSDT) do not face this challenge. We have demonstrated that our HRL system is efficient without the need for a commitment to complete subtasks in a cliff-walking domain, which contradicts a common belief in the literature that such commitment is essential for efficient learning. Additionally, we have shown that reducing commitment as exploration progresses enhances both online performance and the resultant policy in the taxicab domain. This opens up new avenues for research into determining when it is more advantageous to continue with the current subtask or to replan.",1
"We propose a new approach to value function approximation which combines linear temporal difference reinforcement learning with subspace identification. In practical applications, reinforcement learning (RL) is complicated by the fact that state is either high-dimensional or partially observable. Therefore, RL methods are designed to work with features of state rather than state itself, and the success or failure of learning is often determined by the suitability of the selected features. By comparison, subspace identification (SSID) methods are designed to select a feature set which preserves as much information as possible about state. In this paper we connect the two approaches, looking at the problem of reinforcement learning with a large set of features, each of which may only be marginally useful for value function approximation. We introduce a new algorithm for this situation, called Predictive State Temporal Difference (PSTD) learning. As in SSID for predictive state representations, PSTD finds a linear compression operator that projects a large set of features down to a small set that preserves the maximum amount of predictive information. As in RL, PSTD then uses a Bellman recursion to estimate a value function. We discuss the connection between PSTD and prior approaches in RL and SSID. We prove that PSTD is statistically consistent, perform several experiments that illustrate its properties, and demonstrate its potential on a difficult optimal stopping problem.",0
"A novel approach to value function approximation is suggested in this paper, which combines subspace identification with linear temporal difference reinforcement learning. In practical applications, reinforcement learning encounters challenges because state is either high-dimensional or partially observable. To overcome this obstacle, RL methods rely on features of state instead of state itself, and the success of learning largely depends on the appropriateness of the selected features. In contrast, subspace identification (SSID) techniques are devised to select features that retain as much information about state as possible. This paper proposes a new algorithm called Predictive State Temporal Difference (PSTD) learning, which addresses the problem of reinforcement learning with a large set of marginally useful features for value function approximation. PSTD leverages SSID to discover a linear compression operator that reduces the feature set to a smaller one while preserving maximal predictive information. Then, PSTD employs a Bellman recursion to estimate a value function, as in RL. This paper analyzes the relationship between PSTD and existing approaches in RL and SSID. Statistical consistency of PSTD is proven, and several experiments are conducted to demonstrate its properties and potential on a challenging optimal stopping problem.",1
"In this theoretical paper we are concerned with the problem of learning a value function by a smooth general function approximator, to solve a deterministic episodic control problem in a large continuous state space. It is shown that learning the gradient of the value-function at every point along a trajectory generated by a greedy policy is a sufficient condition for the trajectory to be locally extremal, and often locally optimal, and we argue that this brings greater efficiency to value-function learning. This contrasts to traditional value-function learning in which the value-function must be learnt over the whole of state space.   It is also proven that policy-gradient learning applied to a greedy policy on a value-function produces a weight update equivalent to a value-gradient weight update, which provides a surprising connection between these two alternative paradigms of reinforcement learning, and a convergence proof for control problems with a value function represented by a general smooth function approximator.",0
"This paper focuses on the challenge of learning a value function using a smooth general function approximator to address an episodic control problem in a large continuous state space. The paper proposes that learning the gradient of the value function at each point along a trajectory generated by a greedy policy is sufficient for the trajectory to be locally extremal and, in many cases, locally optimal. This approach is more efficient than traditional value-function learning, which requires learning the value function across the entire state space. Additionally, the paper proves that policy-gradient learning on a value function using a greedy policy results in a weight update that is equivalent to a value-gradient weight update. This connection between two different reinforcement learning paradigms is surprising and provides a convergence proof for control problems involving a value function represented by a general smooth function approximator.",1
"We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.",0
"In this tutorial, we will discuss Bayesian optimization, a method used to locate the maximum of costly cost functions. The process involves setting a prior over the objective function using Bayesian technique, which is then combined with evidence to obtain a posterior function. This allows for a utility-based approach in selecting the next observation to be made on the objective function, considering both exploration (sampling from regions with high uncertainty) and exploitation (sampling from areas likely to improve on the current best observation). Additionally, we will present two Bayesian optimization extensions - active user modelling with preferences and hierarchical reinforcement learning, along with our experiences on the advantages and disadvantages of Bayesian optimization.",1
"We consider model-based reinforcement learning in finite Markov De- cision Processes (MDPs), focussing on so-called optimistic strategies. In MDPs, optimism can be implemented by carrying out extended value it- erations under a constraint of consistency with the estimated model tran- sition probabilities. The UCRL2 algorithm by Auer, Jaksch and Ortner (2009), which follows this strategy, has recently been shown to guarantee near-optimal regret bounds. In this paper, we strongly argue in favor of using the Kullback-Leibler (KL) divergence for this purpose. By studying the linear maximization problem under KL constraints, we provide an ef- ficient algorithm, termed KL-UCRL, for solving KL-optimistic extended value iteration. Using recent deviation bounds on the KL divergence, we prove that KL-UCRL provides the same guarantees as UCRL2 in terms of regret. However, numerical experiments on classical benchmarks show a significantly improved behavior, particularly when the MDP has reduced connectivity. To support this observation, we provide elements of com- parison between the two algorithms based on geometric considerations.",0
"This article discusses the use of optimistic strategies in finite Markov Decision Processes (MDPs) for model-based reinforcement learning. Optimism can be implemented in MDPs by conducting extended value iterations under a constraint of consistency with the estimated model transition probabilities. The UCRL2 algorithm, which follows this strategy, has been shown to provide good regret bounds. However, the authors argue in favor of using the Kullback-Leibler (KL) divergence and present an efficient algorithm called KL-UCRL for solving KL-optimistic extended value iteration. This algorithm provides the same guarantees as UCRL2 in terms of regret, but numerical experiments show that KL-UCRL performs better, especially in MDPs with reduced connectivity. The authors also provide a comparison of the two algorithms based on geometric considerations.",1
"We propose a novel reformulation of the stochastic optimal control problem as an approximate inference problem, demonstrating, that such a interpretation leads to new practical methods for the original problem. In particular we characterise a novel class of iterative solutions to the stochastic optimal control problem based on a natural relaxation of the exact dual formulation. These theoretical insights are applied to the Reinforcement Learning problem where they lead to new model free, off policy methods for discrete and continuous problems.",0
"Our proposed approach involves a fresh perspective on the stochastic optimal control problem, redefining it as an approximate inference problem. By doing so, we have discovered practical methods that were not previously available for the original problem. One of our key findings is the identification of a unique set of iterative solutions to the stochastic optimal control problem. These solutions are based on a relaxed version of the exact dual formulation. We have applied these theoretical discoveries to the field of Reinforcement Learning, where they have led to the development of innovative model-free, off-policy techniques for both discrete and continuous problems.",1
"This paper introduces an approach to Reinforcement Learning Algorithm by comparing their immediate rewards using a variation of Q-Learning algorithm. Unlike the conventional Q-Learning, the proposed algorithm compares current reward with immediate reward of past move and work accordingly. Relative reward based Q-learning is an approach towards interactive learning. Q-Learning is a model free reinforcement learning method that used to learn the agents. It is observed that under normal circumstances algorithm take more episodes to reach optimal Q-value due to its normal reward or sometime negative reward. In this new form of algorithm agents select only those actions which have a higher immediate reward signal in comparison to previous one. The contribution of this article is the presentation of new Q-Learning Algorithm in order to maximize the performance of algorithm and reduce the number of episode required to reach optimal Q-value. Effectiveness of proposed algorithm is simulated in a 20 x20 Grid world deterministic environment and the result for the two forms of Q-Learning Algorithms is given.",0
"This article presents a novel approach to Reinforcement Learning Algorithm by utilizing a modified version of Q-Learning algorithm. Unlike traditional Q-Learning, the proposed algorithm compares current reward with the immediate reward of past moves and adjusts accordingly. This approach, known as Relative reward based Q-learning, is an interactive learning method. Q-Learning is a model-free reinforcement learning method that is commonly used to teach agents. However, it is observed that the algorithm takes more episodes to reach optimal Q-value under normal circumstances, due to the normal or sometimes negative reward. In this new algorithm, agents only select actions with a higher immediate reward signal compared to the previous one. The main contribution of this paper is the introduction of a new Q-Learning Algorithm that maximizes algorithm performance and reduces the number of episodes required to reach optimal Q-value. The effectiveness of the proposed algorithm is simulated in a 20 x 20 Grid world deterministic environment, and the results of the two forms of Q-Learning Algorithms are compared.",1
"This paper introduces a principled approach for the design of a scalable general reinforcement learning agent. This approach is based on a direct approximation of AIXI, a Bayesian optimality notion for general reinforcement learning agents. Previously, it has been unclear whether the theory of AIXI could motivate the design of practical algorithms. We answer this hitherto open question in the affirmative, by providing the first computationally feasible approximation to the AIXI agent. To develop our approximation, we introduce a Monte Carlo Tree Search algorithm along with an agent-specific extension of the Context Tree Weighting algorithm. Empirically, we present a set of encouraging results on a number of stochastic, unknown, and partially observable domains.",0
"In this paper, we present a well-reasoned method for creating a general reinforcement learning agent that can be scaled up. Our technique is based on directly approximating AIXI, which is a Bayesian optimality concept for agents that learn through reinforcement. Until now, it has been uncertain whether practical algorithms could be created using the theory of AIXI. However, we provide a positive response to this previously open question by introducing the first feasible approximation of the AIXI agent. We develop our approximation by introducing a Monte Carlo Tree Search algorithm, along with an agent-specific expansion of the Context Tree Weighting algorithm. We also present a series of promising results on various stochastic, unknown, and partially observable domains.",1
"We consider the problem of reinforcement learning using function approximation, where the approximating basis can change dynamically while interacting with the environment. A motivation for such an approach is maximizing the value function fitness to the problem faced. Three errors are considered: approximation square error, Bellman residual, and projected Bellman residual. Algorithms under the actor-critic framework are presented, and shown to converge. The advantage of such an adaptive basis is demonstrated in simulations.",0
"The focus of our study is on function approximation in reinforcement learning, wherein the approximating basis has the ability to alter dynamically during interactions with the environment. This approach is driven by the objective of enhancing the fitness of the value function to address the problem at hand. Our evaluation takes into account three types of errors, namely the approximation square error, Bellman residual, and projected Bellman residual. We introduce algorithms within the actor-critic framework that demonstrate their convergence. The simulation results showcase the benefits of utilizing an adaptive basis.",1
"There has been a lot of recent work on Bayesian methods for reinforcement learning exhibiting near-optimal online performance. The main obstacle facing such methods is that in most problems of interest, the optimal solution involves planning in an infinitely large tree. However, it is possible to obtain stochastic lower and upper bounds on the value of each tree node. This enables us to use stochastic branch and bound algorithms to search the tree efficiently. This paper proposes two such algorithms and examines their complexity in this setting.",0
"Recent studies have focused on developing Bayesian techniques for reinforcement learning that demonstrate almost perfect online performance. However, the predominant challenge for these methods is that solving most significant issues requires planning in an infinite tree, which is impractical. Nevertheless, stochastic lower and upper bounds can be acquired for each node in the tree, allowing for efficient searching of the tree using stochastic branch and bound algorithms. This study presents two such algorithms and evaluates their complexity in this context.",1
"Actor-Critic based approaches were among the first to address reinforcement learning in a general setting. Recently, these algorithms have gained renewed interest due to their generality, good convergence properties, and possible biological relevance. In this paper, we introduce an online temporal difference based actor-critic algorithm which is proved to converge to a neighborhood of a local maximum of the average reward. Linear function approximation is used by the critic in order estimate the value function, and the temporal difference signal, which is passed from the critic to the actor. The main distinguishing feature of the present convergence proof is that both the actor and the critic operate on a similar time scale, while in most current convergence proofs they are required to have very different time scales in order to converge. Moreover, the same temporal difference signal is used to update the parameters of both the actor and the critic. A limitation of the proposed approach, compared to results available for two time scale convergence, is that convergence is guaranteed only to a neighborhood of an optimal value, rather to an optimal value itself. The single time scale and identical temporal difference signal used by the actor and the critic, may provide a step towards constructing more biologically realistic models of reinforcement learning in the brain.",0
"Initially designed to address reinforcement learning in a general setting, Actor-Critic based approaches have recently experienced a resurgence of interest due to their overall efficiency, possible biological relevance and generality. Our paper introduces an online temporal difference based actor-critic algorithm that has been proven to converge to a neighborhood of a local maximum of the average reward. The critic utilizes linear function approximation to estimate the value function, while the temporal difference signal is passed from the critic to the actor. Our convergence proof is distinct from most current proofs in that both the actor and the critic operate on a similar time scale, and they use the same temporal difference signal to update their parameters. However, the proposed approach has a limitation in that convergence is guaranteed only to a neighborhood of an optimal value, as opposed to an optimal value itself. The use of a single time scale and identical temporal difference signal by the actor and the critic is a step towards constructing more biologically realistic models of reinforcement learning in the brain.",1
"In our previous work, we proposed a systematic cross-layer framework for dynamic multimedia systems, which allows each layer to make autonomous and foresighted decisions that maximize the system's long-term performance, while meeting the application's real-time delay constraints. The proposed solution solved the cross-layer optimization offline, under the assumption that the multimedia system's probabilistic dynamics were known a priori. In practice, however, these dynamics are unknown a priori and therefore must be learned online. In this paper, we address this problem by allowing the multimedia system layers to learn, through repeated interactions with each other, to autonomously optimize the system's long-term performance at run-time. We propose two reinforcement learning algorithms for optimizing the system under different design constraints: the first algorithm solves the cross-layer optimization in a centralized manner, and the second solves it in a decentralized manner. We analyze both algorithms in terms of their required computation, memory, and inter-layer communication overheads. After noting that the proposed reinforcement learning algorithms learn too slowly, we introduce a complementary accelerated learning algorithm that exploits partial knowledge about the system's dynamics in order to dramatically improve the system's performance. In our experiments, we demonstrate that decentralized learning can perform as well as centralized learning, while enabling the layers to act autonomously. Additionally, we show that existing application-independent reinforcement learning algorithms, and existing myopic learning algorithms deployed in multimedia systems, perform significantly worse than our proposed application-aware and foresighted learning methods.",0
"We previously presented a cross-layer framework for dynamic multimedia systems, allowing each layer to make independent decisions that maximize the system's long-term performance while adhering to real-time delay constraints. Our proposed solution solved cross-layer optimization offline, assuming prior knowledge of the system's probabilistic dynamics. However, as this is not always feasible, we address the problem by allowing the layers to autonomously learn and optimize the system's performance at runtime. We introduce two reinforcement learning algorithms for centralized and decentralized optimization, analyzing their computation, memory, and inter-layer communication overheads. However, we find that these algorithms learn too slowly and propose an accelerated learning algorithm that exploits partial knowledge about the system's dynamics. Our experiments show that decentralized learning performs as well as centralized learning while enabling autonomous layer actions. Existing reinforcement learning and myopic learning algorithms perform worse than our application-aware and foresighted methods.",1
"General-purpose, intelligent, learning agents cycle through sequences of observations, actions, and rewards that are complex, uncertain, unknown, and non-Markovian. On the other hand, reinforcement learning is well-developed for small finite state Markov decision processes (MDPs). Up to now, extracting the right state representations out of bare observations, that is, reducing the general agent setup to the MDP framework, is an art that involves significant effort by designers. The primary goal of this work is to automate the reduction process and thereby significantly expand the scope of many existing reinforcement learning algorithms and the agents that employ them. Before we can think of mechanizing this search for suitable MDPs, we need a formal objective criterion. The main contribution of this article is to develop such a criterion. I also integrate the various parts into one learning algorithm. Extensions to more realistic dynamic Bayesian networks are developed in Part II. The role of POMDPs is also considered there.",0
"Intelligent learning agents that are general-purpose go through complex, unknown, uncertain, and non-Markovian sequences of rewards, actions, and observations. However, reinforcement learning is only well-suited for finite state Markov decision processes (MDPs) that are small. Designers have to put in significant effort to extract the correct state representations from bare observations to reduce the general agent setup to the MDP framework. This work aims to automate the reduction process, which will expand the scope of many reinforcement learning algorithms and the agents that use them. A formal objective criterion is necessary for mechanizing the search for appropriate MDPs. This article's primary contribution is to develop such a criterion and integrate various components into a single learning algorithm. Part II of this article deals with extensions to more realistic dynamic Bayesian networks, and the role of POMDPs is also explored.",1
Many reinforcement learning exploration techniques are overly optimistic and try to explore every state. Such exploration is impossible in environments with the unlimited number of states. I propose to use simulated exploration with an optimistic model to discover promising paths for real exploration. This reduces the needs for the real exploration.,0
"Numerous exploration techniques in reinforcement learning have a tendency to be excessively optimistic and strive to investigate every possible state. This is unfeasible in environments with an infinite number of states. To address this issue, I suggest utilizing simulated exploration with an optimistic model to discover potential paths for actual exploration, thus diminishing the necessity for real exploration.",1
"We address the problem of reinforcement learning in which observations may exhibit an arbitrary form of stochastic dependence on past observations and actions, i.e. environments more general than (PO)MDPs. The task for an agent is to attain the best possible asymptotic reward where the true generating environment is unknown but belongs to a known countable family of environments. We find some sufficient conditions on the class of environments under which an agent exists which attains the best asymptotic reward for any environment in the class. We analyze how tight these conditions are and how they relate to different probabilistic assumptions known in reinforcement learning and related fields, such as Markov Decision Processes and mixing conditions.",0
"Our focus is on reinforcement learning, where observations may display stochastic dependence on past observations and actions, creating environments that are more complex than (PO)MDPs. The objective is for an agent to achieve maximum asymptotic reward, despite not knowing the true generating environment, but knowing that it belongs to a known countable family. We explore sufficient conditions for the existence of an agent that can attain the best possible reward in any environment within the given class. Our analysis examines the tightness of these conditions and their relationship with various probabilistic assumptions in reinforcement learning and related fields, including Markov Decision Processes and mixing conditions.",1
"We derive an equation for temporal difference learning from statistical principles. Specifically, we start with the variational principle and then bootstrap to produce an updating rule for discounted state value estimates. The resulting equation is similar to the standard equation for temporal difference learning with eligibility traces, so called TD(lambda), however it lacks the parameter alpha that specifies the learning rate. In the place of this free parameter there is now an equation for the learning rate that is specific to each state transition. We experimentally test this new learning rule against TD(lambda) and find that it offers superior performance in various settings. Finally, we make some preliminary investigations into how to extend our new temporal difference algorithm to reinforcement learning. To do this we combine our update equation with both Watkins' Q(lambda) and Sarsa(lambda) and find that it again offers superior performance without a learning rate parameter.",0
"The equation for temporal difference learning is obtained through statistical principles. The variational principle is used to create an updating rule for discounted state value estimates. This equation is similar to TD(lambda), but lacks the alpha parameter, which is replaced by a state-transition specific learning rate equation. Empirical testing shows that this new learning rule performs better than TD(lambda) in various settings. Additionally, we explore the potential of extending this new temporal difference algorithm to reinforcement learning by combining it with Watkins' Q(lambda) and Sarsa(lambda), and find that it outperforms these methods without requiring a learning rate parameter.",1
"Several researchers have recently investigated the connection between reinforcement learning and classification. We are motivated by proposals of approximate policy iteration schemes without value functions which focus on policy representation using classifiers and address policy learning as a supervised learning problem. This paper proposes variants of an improved policy iteration scheme which addresses the core sampling problem in evaluating a policy through simulation as a multi-armed bandit machine. The resulting algorithm offers comparable performance to the previous algorithm achieved, however, with significantly less computational effort. An order of magnitude improvement is demonstrated experimentally in two standard reinforcement learning domains: inverted pendulum and mountain-car.",0
"In recent times, the relationship between reinforcement learning and classification has been explored by various researchers. Our interest stems from the suggestions of approximate policy iteration methods that do not rely on value functions. These methods concentrate on policy representation with classifiers and tackle policy learning as a supervised learning challenge. This study presents alternative versions of an enhanced policy iteration method that addresses the core sampling issue in evaluating a policy through simulation as a multi-armed bandit machine. The resulting algorithm provides comparable performance to the previous one but with significantly less computational effort. We demonstrate an order of magnitude improvement in two standard domains of reinforcement learning: inverted pendulum and mountain-car.",1
Reinforcement learning means learning a policy--a mapping of observations into actions--based on feedback from the environment. The learning can be viewed as browsing a set of policies while evaluating them by trial through interaction with the environment. We present an application of gradient ascent algorithm for reinforcement learning to a complex domain of packet routing in network communication and compare the performance of this algorithm to other routing methods on a benchmark problem.,0
"In reinforcement learning, the objective is to learn a policy that can map observations to actions based on feedback from the environment. This learning process involves browsing through a set of policies and evaluating them through trial and error, with feedback from the environment. Our study focuses on applying the gradient ascent algorithm to the challenging domain of packet routing in network communication. We also compare the performance of this algorithm with other routing methods on a benchmark problem.",1
"Reinforcement learning is commonly used with function approximation. However, very few positive results are known about the convergence of function approximation based RL control algorithms. In this paper we show that TD(0) and Sarsa(0) with linear function approximation is convergent for a simple class of problems, where the system is linear and the costs are quadratic (the LQ control problem). Furthermore, we show that for systems with Gaussian noise and non-completely observable states (the LQG problem), the mentioned RL algorithms are still convergent, if they are combined with Kalman filtering.",0
"Function approximation is frequently employed with reinforcement learning, but there is limited information regarding the effectiveness of function approximation-based RL control algorithms. This research demonstrates that TD(0) and Sarsa(0) with linear function approximation can converge for uncomplicated problems, such as the LQ control problem, where the costs are quadratic and the system is linear. Additionally, when dealing with systems that have non-completely observable states and Gaussian noise, known as the LQG problem, combining the aforementioned RL algorithms with Kalman filtering can still result in convergence.",1
"We address the problem of reinforcement learning in which observations may exhibit an arbitrary form of stochastic dependence on past observations and actions. The task for an agent is to attain the best possible asymptotic reward where the true generating environment is unknown but belongs to a known countable family of environments. We find some sufficient conditions on the class of environments under which an agent exists which attains the best asymptotic reward for any environment in the class. We analyze how tight these conditions are and how they relate to different probabilistic assumptions known in reinforcement learning and related fields, such as Markov Decision Processes and mixing conditions.",0
"The issue of reinforcement learning is tackled, where past observations and actions may display an unpredictable form of stochastic correlation. The objective is for an agent to achieve the optimal asymptotic reward, given that the actual generating environment is unfamiliar but falls within a recognized countable set. We discover certain adequate criteria for the group of environments, which allow for the existence of an agent that attains the greatest asymptotic reward for any environment in the group. We investigate the accuracy of these criteria and their connection to various probabilistic assumptions in reinforcement learning and associated fields, including Markov Decision Processes and mixing conditions.",1
"In this paper we compare the performance characteristics of our selection based learning algorithm for Web crawlers with the characteristics of the reinforcement learning algorithm. The task of the crawlers is to find new information on the Web. The selection algorithm, called weblog update, modifies the starting URL lists of our crawlers based on the found URLs containing new information. The reinforcement learning algorithm modifies the URL orderings of the crawlers based on the received reinforcements for submitted documents. We performed simulations based on data collected from the Web. The collected portion of the Web is typical and exhibits scale-free small world (SFSW) structure. We have found that on this SFSW, the weblog update algorithm performs better than the reinforcement learning algorithm. It finds the new information faster than the reinforcement learning algorithm and has better new information/all submitted documents ratio. We believe that the advantages of the selection algorithm over reinforcement learning algorithm is due to the small world property of the Web.",0
"This paper presents a comparison of the performance characteristics of two algorithms for Web crawlers: our selection based learning algorithm and the reinforcement learning algorithm. The task of the crawlers is to locate fresh information on the Web. Our selection algorithm, called weblog update, alters the starting URL lists of the crawlers by analyzing the found URLs with new information. In contrast, the reinforcement learning algorithm modifies the URL orderings of the crawlers based on received reinforcements for submitted documents. The collected portion of the Web exhibits a typical scale-free small world (SFSW) structure, and we conducted simulations based on this data. Our findings indicate that on this SFSW, the weblog update algorithm performs better than the reinforcement learning algorithm. It locates new information at a faster pace and has a superior new information to all submitted documents ratio. We attribute these advantages to the small world property of the Web.",1
"There is a growing interest in using Kalman-filter models in brain modelling. In turn, it is of considerable importance to make Kalman-filters amenable for reinforcement learning. In the usual formulation of optimal control it is computed off-line by solving a backward recursion. In this technical note we show that slight modification of the linear-quadratic-Gaussian Kalman-filter model allows the on-line estimation of optimal control and makes the bridge to reinforcement learning. Moreover, the learning rule for value estimation assumes a Hebbian form weighted by the error of the value estimation.",0
"The use of Kalman-filter models in brain modelling is gaining popularity. It is crucial to adapt these models for reinforcement learning. Typically, optimal control is calculated offline through a backward recursion process in the conventional formulation. However, we demonstrate a modification to the linear-quadratic-Gaussian Kalman-filter model that enables the estimation of optimal control in real-time and facilitates the connection to reinforcement learning. Additionally, the learning rule for value estimation adopts a Hebbian form that is influenced by the value estimation error.",1
"Cooperative games are those in which both agents share the same payoff structure. Value-based reinforcement-learning algorithms, such as variants of Q-learning, have been applied to learning cooperative games, but they only apply when the game state is completely observable to both agents. Policy search methods are a reasonable alternative to value-based methods for partially observable environments. In this paper, we provide a gradient-based distributed policy-search method for cooperative games and compare the notion of local optimum to that of Nash equilibrium. We demonstrate the effectiveness of this method experimentally in a small, partially observable simulated soccer domain.",0
"Games that have a shared payoff structure between agents are called cooperative games. While value-based reinforcement-learning algorithms, like Q-learning variations, have been employed for learning these games, they can only be used when the game state is fully observable to both players. Partially observable environments can be handled with policy search techniques as a viable alternative to value-based methods. In this article, we present a distributed policy-search method based on gradients for cooperative games. We compare local optima to Nash equilibrium and conduct experiments in a simulated soccer domain with limited observability to demonstrate the effectiveness of this approach.",1
"Reinforcement learning means finding the optimal course of action in Markovian environments without knowledge of the environment's dynamics. Stochastic optimization algorithms used in the field rely on estimates of the value of a policy. Typically, the value of a policy is estimated from results of simulating that very policy in the environment. This approach requires a large amount of simulation as different points in the policy space are considered. In this paper, we develop value estimators that utilize data gathered when using one policy to estimate the value of using another policy, resulting in much more data-efficient algorithms. We consider the question of accumulating a sufficient experience and give PAC-style bounds.",0
"Reinforcement learning involves determining the best course of action in Markovian environments without prior knowledge of the environment's dynamics. The field employs stochastic optimization algorithms that rely on policy value estimates. Usually, the value of a policy is estimated by simulating it in the environment, which necessitates extensive simulation for various policy space points. However, our paper proposes value estimators that utilize data from one policy to estimate the value of another policy. This results in more data-efficient algorithms, and we explore the question of acquiring adequate experience and provide PAC-style bounds.",1
"This paper surveys the emerging science of how to design a ``COllective INtelligence'' (COIN). A COIN is a large multi-agent system where:   (i) There is little to no centralized communication or control; and   (ii) There is a provided world utility function that rates the possible histories of the full system.   In particular, we are interested in COINs in which each agent runs a reinforcement learning (RL) algorithm. Rather than use a conventional modeling approach (e.g., model the system dynamics, and hand-tune agents to cooperate), we aim to solve the COIN design problem implicitly, via the ``adaptive'' character of the RL algorithms of each of the agents. This approach introduces an entirely new, profound design problem: Assuming the RL algorithms are able to achieve high rewards, what reward functions for the individual agents will, when pursued by those agents, result in high world utility? In other words, what reward functions will best ensure that we do not have phenomena like the tragedy of the commons, Braess's paradox, or the liquidity trap?   Although still very young, research specifically concentrating on the COIN design problem has already resulted in successes in artificial domains, in particular in packet-routing, the leader-follower problem, and in variants of Arthur's El Farol bar problem. It is expected that as it matures and draws upon other disciplines related to COINs, this research will greatly expand the range of tasks addressable by human engineers. Moreover, in addition to drawing on them, such a fully developed scie nce of COIN design may provide much insight into other already established scientific fields, such as economics, game theory, and population biology.",0
"This article explores the emerging field of designing ""Collective Intelligence"" (COIN). COIN refers to a large multi-agent system with minimal centralized communication or control and a world utility function that rates the system's possible histories. The focus is on COINs that utilize reinforcement learning algorithms for each agent, rather than traditional modeling approaches. This approach poses a new design problem: determining reward functions for individual agents that result in high world utility. Research in COIN design has yielded successes in artificial domains, such as packet-routing and the leader-follower problem. As this field matures and incorporates other related disciplines, it is expected to expand the range of tasks that human engineers can address and provide insight into established scientific fields like economics, game theory, and population biology.",1
"Many researchers have explored methods for hierarchical reinforcement learning (RL) with temporal abstractions, in which abstract actions are defined that can perform many primitive actions before terminating. However, little is known about learning with state abstractions, in which aspects of the state space are ignored. In previous work, we developed the MAXQ method for hierarchical RL. In this paper, we define five conditions under which state abstraction can be combined with the MAXQ value function decomposition. We prove that the MAXQ-Q learning algorithm converges under these conditions and show experimentally that state abstraction is important for the successful application of MAXQ-Q learning.",0
"Several scholars have investigated techniques for hierarchical reinforcement learning (RL) that involve temporal abstractions, where abstract actions are created that can execute numerous primitive actions before stopping. Nevertheless, the knowledge about learning with state abstractions, where certain aspects of the state space are disregarded, is limited. Formerly, the MAXQ approach was developed for hierarchical RL. This study outlines five requirements for combining state abstraction with the MAXQ value function decomposition. We demonstrate that the MAXQ-Q learning algorithm converges under these conditions, and we provide empirical evidence that state abstraction is crucial for the effective implementation of MAXQ-Q learning.",1
"This paper presents the MAXQ approach to hierarchical reinforcement learning based on decomposing the target Markov decision process (MDP) into a hierarchy of smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs. The paper defines the MAXQ hierarchy, proves formal results on its representational power, and establishes five conditions for the safe use of state abstractions. The paper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges wih probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the five kinds of state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q through a series of experiments in three domains and shows experimentally that MAXQ-Q (with state abstractions) converges to a recursively optimal policy much faster than flat Q learning. The fact that MAXQ learns a representation of the value function has an important benefit: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration. The paper demonstrates the effectiveness of this non-hierarchical execution experimentally. Finally, the paper concludes with a comparison to related work and a discussion of the design tradeoffs in hierarchical reinforcement learning.",0
"The paper introduces the MAXQ strategy for hierarchical reinforcement learning, which involves breaking down the target Markov decision process (MDP) into smaller MDPs and combining their value functions to create the value function for the target MDP. The MAXQ hierarchy is defined, and the paper establishes five conditions for safely using state abstractions. The paper also presents an online model-free learning algorithm called MAXQ-Q, which converges to a recursively optimal policy even in the presence of state abstractions. The effectiveness of MAXQ-Q is demonstrated through experiments in three domains, and it is shown to converge to a recursively optimal policy faster than flat Q learning. The paper highlights the benefit of MAXQ learning a representation of the value function, which allows for the computation and execution of an improved non-hierarchical policy. The paper concludes with a comparison to related work and a discussion of the design tradeoffs in hierarchical reinforcement learning.",1
"A COllective INtelligence (COIN) is a set of interacting reinforcement learning (RL) algorithms designed in an automated fashion so that their collective behavior optimizes a global utility function. We summarize the theory of COINs, then present experiments using that theory to design COINs to control internet traffic routing. These experiments indicate that COINs outperform all previously investigated RL-based, shortest path routing algorithms.",0
"A group of reinforcement learning algorithms, known as Collective Intelligence (COIN), are automatically designed to optimize a global utility function by interacting with each other. Our study provides an overview of COIN theory and demonstrates its effectiveness in controlling internet traffic routing through various experiments. The results show that COINs surpass all previously tested RL-based, shortest path routing algorithms.",1
"Copulas are a powerful tool for modeling multivariate distributions as they allow to separately estimate the univariate marginal distributions and the joint dependency structure. However, known parametric copulas offer limited flexibility especially in high dimensions, while commonly used non-parametric methods suffer from the curse of dimensionality. A popular remedy is to construct a tree-based hierarchy of conditional bivariate copulas. In this paper, we propose a flexible, yet conceptually simple alternative based on implicit generative neural networks. The key challenge is to ensure marginal uniformity of the estimated copula distribution. We achieve this by learning a multivariate latent distribution with unspecified marginals but the desired dependency structure. By applying the probability integral transform, we can then obtain samples from the high-dimensional copula distribution without relying on parametric assumptions or the need to find a suitable tree structure. Experiments on synthetic and real data from finance, physics, and image generation demonstrate the performance of this approach.",0
"Copulas are a useful instrument for modeling multivariate distributions since they permit the separate estimation of univariate marginal distributions and joint dependency structures. However, parametric copulas are limited in flexibility, particularly in high dimensions, while non-parametric methods are afflicted by the curse of dimensionality. To address this issue, many use a tree-based hierarchy of conditional bivariate copulas. Alternatively, this paper proposes a flexible and straightforward option that relies on implicit generative neural networks. The primary obstacle is to ensure that the estimated copula distribution is marginally uniform. This is accomplished by learning a multivariate latent distribution with the desired dependency structure, but unspecified marginals. By applying the probability integral transform, high-dimensional copula distribution samples can be obtained without relying on parametric assumptions or the need for a suitable tree structure. Experiments on finance, physics, and image generation synthetic and real data demonstrate the effectiveness of this approach.",1
"Solving for the frequency-domain scattered wavefield via physics-informed neural network (PINN) has great potential in seismic modeling and inversion. However, when dealing with high-frequency wavefields, its accuracy and training cost limits its applications. Thus, we propose a novel implementation of PINN using frequency upscaling and neuron splitting, which allows the neural network model to grow in size as we increase the frequency while leveraging the information from the pre-trained model for lower-frequency wavefields, resulting in fast convergence to high-accuracy solutions. Numerical results show that, compared to the commonly used PINN with random initialization, the proposed PINN exhibits notable superiority in terms of convergence and accuracy and can achieve neuron based high-frequency wavefield solutions with a two-hidden-layer model.",0
"The utilization of physics-informed neural network (PINN) to solve for the frequency-domain scattered wavefield holds immense potential in seismic modeling and inversion. Nevertheless, its accuracy and training cost pose limitations when dealing with high-frequency wavefields. Therefore, we suggest a new approach to implementing PINN, which involves frequency upscaling and neuron splitting. This strategy enables the neural network model to expand in size as we increase the frequency, while utilizing information from the pre-trained model for lower-frequency wavefields, leading to rapid convergence towards high-precision solutions. Numerical findings demonstrate that our proposed PINN outperforms the commonly used PINN with random initialization in terms of accuracy and convergence. Additionally, it accomplishes neuron-based high-frequency wavefield solutions with a two-hidden-layer model.",1
"The microstructure is an essential part of materials, storing the genes of materials and having a decisive influence on materials' physical and chemical properties. The material genetic engineering program aims to establish the relationship between material composition/process, organization, and performance to realize the reverse design of materials, thereby accelerating the research and development of new materials. However, tissue analysis methods of materials science, such as metallographic analysis, XRD analysis, and EBSD analysis, cannot directly establish a complete quantitative relationship between tissue structure and performance. Therefore, this paper proposes a novel data-knowledge-driven organization representation and performance prediction method to obtain a quantitative structure-performance relationship. First, a knowledge graph based on EBSD is constructed to describe the material's mesoscopic microstructure. Then a graph representation learning network based on graph attention is constructed, and the EBSD organizational knowledge graph is input into the network to obtain graph-level feature embedding. Finally, the graph-level feature embedding is input to a graph feature mapping network to obtain the material's mechanical properties. The experimental results show that our method is superior to traditional machine learning and machine vision methods.",0
"The microstructure of materials is a crucial aspect that determines their physical and chemical properties by storing their genes. To accelerate the development of new materials, the material genetic engineering program aims to establish a relationship between material composition, organization, and performance. However, current tissue analysis methods such as metallographic analysis, XRD analysis, and EBSD analysis are insufficient in providing a complete quantitative correlation between tissue structure and performance. Therefore, this paper introduces a novel data-knowledge-driven method for organization representation and performance prediction to obtain a structure-performance relationship. The proposed method constructs a knowledge graph based on EBSD and uses a graph attention-based network to obtain graph-level feature embedding. The results show that our method outperforms traditional machine learning and machine vision techniques.",1
"This paper introduces a novel adversarial example generation method against face recognition systems (FRSs). An adversarial example (AX) is an image with deliberately crafted noise to cause incorrect predictions by a target system. The AXs generated from our method remain robust under real-world brightness changes. Our method performs non-linear brightness transformations while leveraging the concept of curriculum learning during the attack generation procedure. We demonstrate that our method outperforms conventional techniques from comprehensive experimental investigations in the digital and physical world. Furthermore, this method enables practical risk assessment of FRSs against brightness agnostic AXs.",0
"The aim of this paper is to present a new technique for creating adversarial examples that can deceive face recognition systems (FRSs). Adversarial examples are images that are intentionally altered to cause a target system to make incorrect predictions. The approach we propose generates adversarial examples that are resistant to changes in brightness in real-world settings. This is achieved by employing non-linear brightness transformations and taking advantage of the concept of curriculum learning throughout the attack generation process. Our experiments demonstrate that our method outperforms traditional techniques in both digital and physical environments. Moreover, this approach allows for practical risk assessment of FRSs against brightness-agnostic adversarial examples.",1
"Sequential decision making in the presence of uncertainty and stochastic dynamics gives rise to distributions over state/action trajectories in reinforcement learning (RL) and optimal control problems. This observation has led to a variety of connections between RL and inference in probabilistic graphical models (PGMs). Here we explore a different dimension to this relationship, examining reinforcement learning using the tools and abstractions of statistical physics. The central object in the statistical physics abstraction is the idea of a partition function $\mathcal{Z}$, and here we construct a partition function from the ensemble of possible trajectories that an agent might take in a Markov decision process. Although value functions and $Q$-functions can be derived from this partition function and interpreted via average energies, the $\mathcal{Z}$-function provides an object with its own Bellman equation that can form the basis of alternative dynamic programming approaches. Moreover, when the MDP dynamics are deterministic, the Bellman equation for $\mathcal{Z}$ is linear, allowing direct solutions that are unavailable for the nonlinear equations associated with traditional value functions. The policies learned via these $\mathcal{Z}$-based Bellman updates are tightly linked to Boltzmann-like policy parameterizations. In addition to sampling actions proportionally to the exponential of the expected cumulative reward as Boltzmann policies would, these policies take entropy into account favoring states from which many outcomes are possible.",0
"Reinforcement learning (RL) and optimal control problems involve making sequential decisions in uncertain and stochastic environments, resulting in the creation of distributions over state/action trajectories. This connection between RL and probabilistic graphical models (PGMs) has been studied extensively, but this paper explores the relationship between RL and statistical physics. The paper introduces the idea of a partition function $\mathcal{Z}$, which is constructed from the possible trajectories an agent might take in a Markov decision process. Value functions and $Q$-functions can be derived from the $\mathcal{Z}$-function, and the Bellman equation associated with it can be used for alternative dynamic programming approaches. When the MDP dynamics are deterministic, the linear Bellman equation for $\mathcal{Z}$ allows for direct solutions, which are not possible with traditional value functions. Policies learned through the $\mathcal{Z}$-based Bellman updates are closely related to Boltzmann-like policy parameterizations, which take into account entropy and favor states with many possible outcomes.",1
"We present Adaptive Multi-layer Contrastive Graph Neural Networks (AMC-GNN), a self-supervised learning framework for Graph Neural Network, which learns feature representations of sample data without data labels. AMC-GNN generates two graph views by data augmentation and compares different layers' output embeddings of Graph Neural Network encoders to obtain feature representations, which could be used for downstream tasks. AMC-GNN could learn the importance weights of embeddings in different layers adaptively through the attention mechanism, and an auxiliary encoder is introduced to train graph contrastive encoders better. The accuracy is improved by maximizing the representation's consistency of positive pairs in the early layers and the final embedding space. Our experiments show that the results can be consistently improved by using the AMC-GNN framework, across four established graph benchmarks: Cora, Citeseer, Pubmed, DBLP citation network datasets, as well as four newly proposed datasets: Co-author-CS, Co-author-Physics, Amazon-Computers, Amazon-Photo.",0
"Our paper introduces the Adaptive Multi-layer Contrastive Graph Neural Networks (AMC-GNN), a framework for self-supervised learning of Graph Neural Networks. This method enables the learning of feature representations from sample data without the need for data labels. The AMC-GNN approach involves generating two graph views through data augmentation and comparing the output embeddings of Graph Neural Network encoders across different layers to obtain feature representations. These representations can then be applied to downstream tasks. The AMC-GNN system can adaptively learn the importance weights of embeddings across different layers using an attention mechanism. Additionally, an auxiliary encoder is introduced to improve the training of graph contrastive encoders. To enhance accuracy, we maximize the consistency of representation for positive pairs in both the early layers and the final embedding space. Our experiments demonstrate that AMC-GNN consistently improves results across four established graph benchmarks (Cora, Citeseer, Pubmed, DBLP citation network datasets) and four newly proposed datasets (Co-author-CS, Co-author-Physics, Amazon-Computers, Amazon-Photo).",1
"Parametric computer-aided design (CAD) tools are the predominant way that engineers specify physical structures, from bicycle pedals to airplanes to printed circuit boards. The key characteristic of parametric CAD is that design intent is encoded not only via geometric primitives, but also by parameterized constraints between the elements. This relational specification can be viewed as the construction of a constraint program, allowing edits to coherently propagate to other parts of the design. Machine learning offers the intriguing possibility of accelerating the design process via generative modeling of these structures, enabling new tools such as autocompletion, constraint inference, and conditional synthesis. In this work, we present such an approach to generative modeling of parametric CAD sketches, which constitute the basic computational building blocks of modern mechanical design. Our model, trained on real-world designs from the SketchGraphs dataset, autoregressively synthesizes sketches as sequences of primitives, with initial coordinates, and constraints that reference back to the sampled primitives. As samples from the model match the constraint graph representation used in standard CAD software, they may be directly imported, solved, and edited according to downstream design tasks. In addition, we condition the model on various contexts, including partial sketches (primers) and images of hand-drawn sketches. Evaluation of the proposed approach demonstrates its ability to synthesize realistic CAD sketches and its potential to aid the mechanical design workflow.",0
"Engineers predominantly use parametric computer-aided design (CAD) tools to specify physical structures, such as bicycle pedals, airplanes, and printed circuit boards. Parametric CAD encodes design intent through geometric primitives and parameterized constraints between elements, creating a constraint program to ensure coherence when making edits. Machine learning offers the potential to accelerate design processes through generative modeling, enabling tools like autocompletion, constraint inference, and conditional synthesis. This work presents an approach to generative modeling of parametric CAD sketches, using real-world designs to train a model that synthesizes sketches as sequences of primitives with coordinates and constraints. The model's samples match the constraint graph representation of standard CAD software, allowing for direct import, solving, and editing for downstream design tasks. The model can also be conditioned on various contexts, including partial sketches and hand-drawn sketch images. Evaluation of the proposed approach shows its ability to synthesize realistic CAD sketches and support mechanical design workflows.",1
"The COVID-19 pandemic has caused many shutdowns in different industries around the world. Sectors such as infrastructure construction and maintenance projects have not been suspended due to their significant effect on people's routine life. In such projects, workers work close together that makes a high risk of infection. The World Health Organization recommends wearing a face mask and practicing physical distancing to mitigate the virus's spread. This paper developed a computer vision system to automatically detect the violation of face mask wearing and physical distancing among construction workers to assure their safety on infrastructure projects during the pandemic. For the face mask detection, the paper collected and annotated 1,000 images, including different types of face mask wearing, and added them to a pre-existing face mask dataset to develop a dataset of 1,853 images. Then trained and tested multiple Tensorflow state-of-the-art object detection models on the face mask dataset and chose the Faster R-CNN Inception ResNet V2 network that yielded the accuracy of 99.8%. For physical distance detection, the paper employed the Faster R-CNN Inception V2 to detect people. A transformation matrix was used to eliminate the camera angle's effect on the object distances on the image. The Euclidian distance used the pixels of the transformed image to compute the actual distance between people. A threshold of six feet was considered to capture physical distance violation. The paper also used transfer learning for training the model. The final model was applied on four videos of road maintenance projects in Houston, TX, that effectively detected the face mask and physical distance. We recommend that construction owners use the proposed system to enhance construction workers' safety in the pandemic situation.",0
"Due to the COVID-19 pandemic, various industries worldwide have had to shut down. However, infrastructure construction and maintenance projects have not been suspended as they play a crucial role in people's daily lives. These projects involve workers working in close proximity, which poses a high risk of infection. The World Health Organization recommends wearing face masks and practicing physical distancing to curb the virus's spread. To ensure the safety of construction workers during the pandemic, this study developed a computer vision system that can automatically detect violations of face mask wearing and physical distancing. The researchers collected and annotated 1,000 images of different types of face mask wearing and added them to a pre-existing face mask dataset, resulting in a dataset of 1,853 images. They then trained and tested several Tensorflow state-of-the-art object detection models on the face mask dataset and chose the Faster R-CNN Inception ResNet V2 network, which achieved an accuracy of 99.8%. For physical distance detection, they used the Faster R-CNN Inception V2 to detect people and employed a transformation matrix to eliminate the camera angle's effect on object distances in the image. The Euclidian distance was used to compute the actual distance between people using the transformed image's pixels, and a threshold of six feet was considered to detect physical distance violations. Transfer learning was used to train the model, which was then applied to four videos of road maintenance projects in Houston, TX, and effectively detected face mask and physical distance violations. Therefore, construction owners are advised to use this system to improve the safety of construction workers during the pandemic.",1
"Deep algorithm unrolling has emerged as a powerful model-based approach to develop deep architectures that combine the interpretability of iterative algorithms with the performance gains of supervised deep learning, especially in cases of sparse optimization. This framework is well-suited to applications in biological imaging, where physics-based models exist to describe the measurement process and the information to be recovered is often highly structured. Here, we review the method of deep unrolling, and show how it improves source localization in several biological imaging settings.",0
"A potent model-based technique known as deep algorithm unrolling has surfaced to create deep architectures that merge the interpretability of iterative algorithms with the performance benefits of supervised deep learning, specifically in situations involving sparse optimization. This framework is particularly fitting for applications in biological imaging, where physics-based models are present to explain the measurement process and the data to be retrieved is frequently organized. This article examines the deep unrolling method, and illustrates how it enhances source localization in various biological imaging scenarios.",1
"Integrating physical inductive biases into machine learning can improve model generalizability. We generalize the successful paradigm of physics-informed learning (PIL) into a more general framework that also includes what we term physics-augmented learning (PAL). PIL and PAL complement each other by handling discriminative and generative properties, respectively. In numerical experiments, we show that PAL performs well on examples where PIL is inapplicable or inefficient.",0
"By incorporating physical inductive biases, the overall performance of machine learning models can be enhanced. Our research expands on the effective approach of physics-informed learning (PIL) by introducing a more comprehensive framework called physics-augmented learning (PAL). These two methods complement each other as PIL focuses on discriminative properties while PAL prioritizes generative properties. Our numerical experiments demonstrate that PAL outperforms PIL in situations where PIL is unsuitable or ineffective.",1
"This paper introduces a framework for combining scientific knowledge of physics-based models with neural networks to advance scientific discovery. This framework, termed physics-guided neural networks (PGNN), leverages the output of physics-based model simulations along with observational features in a hybrid modeling setup to generate predictions using a neural network architecture. Further, this framework uses physics-based loss functions in the learning objective of neural networks to ensure that the model predictions not only show lower errors on the training set but are also scientifically consistent with the known physics on the unlabeled set. We illustrate the effectiveness of PGNN for the problem of lake temperature modeling, where physical relationships between the temperature, density, and depth of water are used to design a physics-based loss function. By using scientific knowledge to guide the construction and learning of neural networks, we are able to show that the proposed framework ensures better generalizability as well as scientific consistency of results. All the code and datasets used in this study have been made available on this link \url{https://github.com/arkadaw9/PGNN}.",0
"The aim of this paper is to propose a framework, called physics-guided neural networks (PGNN), that combines the scientific knowledge of physics-based models with neural networks to enhance scientific discovery. PGNN uses the results of physics-based model simulations and observational features in a hybrid modeling configuration to create predictions using a neural network architecture. Moreover, the framework employs physics-based loss functions in the learning objective of neural networks to ensure that the predictions are both scientifically accurate and have lower errors on the training set. To demonstrate the effectiveness of PGNN, the authors applied it to the problem of lake temperature modeling, utilizing physical relationships between temperature, density, and water depth to design a physics-based loss function. The proposed framework ensures better generalizability and scientific consistency of results by using scientific knowledge to guide the construction and learning of neural networks. The code and datasets used in this study are available at \url{https://github.com/arkadaw9/PGNN}.",1
"Physics-informed neural networks (PINNs) impose known physical laws into the learning of deep neural networks, making sure they respect the physics of the process while decreasing the demand of labeled data. For systems represented by Ordinary Differential Equations (ODEs), the conventional PINN has a continuous time input variable and outputs the solution of the corresponding ODE. In their original form, PINNs do not allow control inputs neither can they simulate for long-range intervals without serious degradation in their predictions. In this context, this work presents a new framework called Physics-Informed Neural Nets for Control (PINC), which proposes a novel PINN-based architecture that is amenable to \emph{control} problems and able to simulate for longer-range time horizons that are not fixed beforehand. The framework has new inputs to account for the initial state of the system and the control action. In PINC, the response over the complete time horizon is split such that each smaller interval constitutes a solution of the ODE conditioned on the fixed values of initial state and control action for that interval. The whole response is formed by feeding back the predictions of the terminal state as the initial state for the next interval. This proposal enables the optimal control of dynamic systems, integrating a priori knowledge from experts and data collected from plants into control applications. We showcase our proposal in the control of two nonlinear dynamic systems: the Van der Pol oscillator and the four-tank system.",0
"Physics-Informed Neural Networks (PINNs) are designed to incorporate known physical laws into deep neural networks, ensuring they adhere to the physics of the process while reducing the need for labeled data. For systems modeled by Ordinary Differential Equations (ODEs), the traditional PINN employs a continuous time input variable and yields the solution to the corresponding ODE. However, the original PINN model lacks the ability to accommodate control inputs or simulate over long-range intervals without compromising accuracy. In this vein, this study introduces a new framework named Physics-Informed Neural Nets for Control (PINC) that proposes a novel PINN-based architecture specifically tailored to control problems and capable of simulating over longer time horizons that are not predetermined. The PINC framework includes new inputs to account for the initial state of the system and the control action. The entire response is divided into smaller intervals, each of which represents a solution of the ODE based on the fixed values of the initial state and control action for that period. The complete response is formed by incorporating predictions of the terminal state as the initial state for the next interval. This approach allows for optimal control of dynamic systems by integrating expert knowledge and plant data into control applications. To demonstrate the effectiveness of the proposed model, we apply it to two nonlinear dynamic systems: the Van der Pol oscillator and the four-tank system.",1
"Lensless cameras are a class of imaging devices that shrink the physical dimensions to the very close vicinity of the image sensor by integrating flat optics and computational algorithms. Here we report a flat lensless camera with spatially-coded Voronoi-Fresnel phase, partly inspired by biological apposition compound eye, to achieve superior image quality. We propose a design principle of maximizing the information in optics to facilitate the computational reconstruction. By introducing a Fourier domain metric, Modulation Transfer Function volume (MTFv), we devise an optimization framework to guide the optimal design of the optical element. The resulting Voronoi-Fresnel phase features an irregular array of quasi-Centroidal Voronoi cells containing a base first-order Fresnel phase function. We demonstrate and verify the imaging performance with a prototype Voronoi-Fresnel lensless camera on a 1.6-megapixel image sensor in various illumination conditions. The proposed design could benefit the development of compact imaging systems working in extreme physical conditions.",0
"A type of imaging devices called lensless cameras integrate flat optics and computational algorithms to reduce their physical dimensions to just the vicinity of the image sensor. Our study showcases a flat lensless camera that employs Voronoi-Fresnel phase coding, inspired by biological apposition compound eyes, to produce high-quality images. We propose a design principle that maximizes the information in optics to facilitate computational reconstruction. To guide the optimal design of the optical element, we introduce a Fourier domain metric called Modulation Transfer Function volume (MTFv) and create an optimization framework. The Voronoi-Fresnel phase has an irregular array of quasi-Centroidal Voronoi cells with a base first-order Fresnel phase function. We demonstrate and verify the imaging performance of our prototype Voronoi-Fresnel lensless camera, which features a 1.6-megapixel image sensor, in various illumination conditions. Our proposed design could be useful for developing compact imaging systems that work in extreme physical conditions.",1
"In this paper, we propose an end-to-end learning framework for event-based motion deblurring in a self-supervised manner, where real-world events are exploited to alleviate the performance degradation caused by data inconsistency. To achieve this end, optical flows are predicted from events, with which the blurry consistency and photometric consistency are exploited to enable self-supervision on the deblurring network with real-world data. Furthermore, a piece-wise linear motion model is proposed to take into account motion non-linearities and thus leads to an accurate model for the physical formation of motion blurs in the real-world scenario. Extensive evaluation on both synthetic and real motion blur datasets demonstrates that the proposed algorithm bridges the gap between simulated and real-world motion blurs and shows remarkable performance for event-based motion deblurring in real-world scenarios.",0
"The aim of this paper is to present a learning framework that can effectively deblur motion in a self-supervised manner, utilizing real-world events to address issues of data inconsistency. By predicting optical flows from events, the deblurring network can be self-supervised using the blurry consistency and photometric consistency of real-world data. Moreover, a piece-wise linear motion model is suggested to account for motion non-linearities, resulting in a more accurate model for the physical formation of motion blurs in the real-world. Our proposed algorithm is evaluated on both synthetic and real motion blur datasets, demonstrating its ability to close the gap between simulated and real-world motion blurs and its remarkable performance for event-based motion deblurring in real-world scenarios.",1
"Deep reinforcement learning (RL) algorithms can learn complex policies to optimize agent operation over time. RL algorithms have shown promising results in solving complicated problems in recent years. However, their application on real-world physical systems remains limited. Despite the advancements in RL algorithms, the industries often prefer traditional control strategies. Traditional methods are simple, computationally efficient and easy to adjust. In this paper, we first propose a new Q-learning algorithm for continuous action space, which can bridge the control and RL algorithms and bring us the best of both worlds. Our method can learn complex policies to achieve long-term goals and at the same time it can be easily adjusted to address short-term requirements without retraining. Next, we present an approximation of our algorithm which can be applied to address short-term requirements of any pre-trained RL algorithm. The case studies demonstrate that both our proposed method as well as its practical approximation can achieve short-term and long-term goals without complex reward functions.",0
"The optimization of agent operation over time can be achieved through complex policies learned via deep reinforcement learning (RL) algorithms. While these algorithms have demonstrated potential in solving intricate problems, their application on physical systems in the real world is still limited. As a result, traditional control strategies are often preferred due to their simplicity, computational efficiency, and ease of adjustment. This paper proposes a new Q-learning algorithm for continuous action space that combines the benefits of control and RL algorithms. Our approach can learn complex policies for achieving long-term goals while also being adaptable to address short-term requirements without requiring retraining. Additionally, we present an approximation of our algorithm that can be applied to any pre-trained RL algorithm to meet short-term requirements. Through case studies, we illustrate that our proposed method and its practical approximation can successfully accomplish both short-term and long-term goals without the need for complex reward functions.",1
"Accurate forecasts of photovoltaic power generation (PVPG) are essential to optimize operations between energy supply and demand. Recently, the propagation of sensors and smart meters has produced an enormous volume of data, which supports the development of data based PVPG forecasting. Although emerging deep learning (DL) models, such as the long short-term memory (LSTM) model, based on historical data, have provided effective solutions for PVPG forecasting with great successes, these models utilize offline learning. As a result, DL models cannot take advantage of the opportunity to learn from newly-arrived data, and are unable to handle concept drift caused by installing extra PV units and unforeseen PV unit failures. Consequently, to improve day-ahead PVPG forecasting accuracy, as well as eliminate the impacts of concept drift, this paper proposes an adaptive LSTM (AD-LSTM) model, which is a DL framework that can not only acquire general knowledge from historical data, but also dynamically learn specific knowledge from newly-arrived data. A two-phase adaptive learning strategy (TP-ALS) is integrated into AD-LSTM, and a sliding window (SDWIN) algorithm is proposed, to detect concept drift in PV systems. Multiple datasets from PV systems are utilized to assess the feasibility and effectiveness of the proposed approaches. The developed AD-LSTM model demonstrates greater forecasting capability than the offline LSTM model, particularly in the presence of concept drift. Additionally, the proposed AD-LSTM model also achieves superior performance in terms of day-ahead PVPG forecasting compared to other traditional machine learning models and statistical models in the literature.",0
"Having accurate forecasts for photovoltaic power generation (PVPG) is crucial for efficient energy management. With the widespread use of sensors and smart meters, there is now an abundance of data that can be leveraged to develop data-driven PVPG forecasting models. However, while deep learning (DL) models like the long short-term memory (LSTM) model have been successful in utilizing historical data for PVPG forecasting, they are limited by offline learning and cannot handle concept drift or adapt to new data. To address these issues, this paper proposes an adaptive LSTM (AD-LSTM) model that integrates a two-phase adaptive learning strategy (TP-ALS) and a sliding window (SDWIN) algorithm to detect concept drift. The AD-LSTM model outperforms traditional machine learning and statistical models in day-ahead PVPG forecasting, particularly in the presence of concept drift.",1
"Transformers are state-of-the-art deep learning models that are composed of stacked attention and point-wise, fully connected layers designed for handling sequential data. Transformers are not only ubiquitous throughout Natural Language Processing (NLP), but, recently, they have inspired a new wave of Computer Vision (CV) applications research. In this work, a Vision Transformer (ViT) is applied to predict the state variables of 2-dimensional Ising model simulations. Our experiments show that ViT outperform state-of-the-art Convolutional Neural Networks (CNN) when using a small number of microstate images from the Ising model corresponding to various boundary conditions and temperatures. This work opens the possibility of applying ViT to other simulations, and raises interesting research directions on how attention maps can learn about the underlying physics governing different phenomena.",0
"Transformers, which consist of attention and fully connected layers for handling sequential data, are advanced deep learning models widely used in Natural Language Processing (NLP) and have recently garnered attention in Computer Vision (CV) applications. Our study applies a Vision Transformer (ViT) to predict the state variables of 2-dimensional Ising model simulations and demonstrates that ViT outperforms state-of-the-art Convolutional Neural Networks (CNN) when using a small number of microstate images from the Ising model with various boundary conditions and temperatures. This research suggests that ViT can be applied to other simulations and presents intriguing research directions regarding how attention maps can learn about the underlying physics underlying different phenomena.",1
"The Poisson equation is critical to get a self-consistent solution in plasma fluid simulations used for Hall effect thrusters and streamers discharges. Solving the 2D Poisson equation with zero Dirichlet boundary conditions using a deep neural network is investigated using multiple-scale architectures, defined in terms of number of branches, depth and receptive field. The latter is found critical to correctly capture large topological structures of the field. The investigation of multiple architectures, losses, and hyperparameters provides an optimum network to solve accurately the steady Poisson problem. Generalization to new resolutions and domain sizes is then proposed using a proper scaling of the network. Finally, found neural network solver, called PlasmaNet, is coupled with an unsteady Euler plasma fluid equations solver. The test case corresponds to electron plasma oscillations which is used to assess the accuracy of the neural network solution in a time-dependent simulation. In this time-evolving problem, a physical loss is necessary to produce a stable simulation. PlasmaNet is then benchmarked on meshes with increasing number of nodes, and compared with an existing solver based on a standard linear system algorithm for the Poisson equation. It outperforms the classical plasma solver, up to speedups 700 times faster on large meshes. PlasmaNet is finally tested on a more complex case of discharge propagation involving chemistry and advection. The guidelines established in previous sections are applied to build the CNN to solve the same Poisson equation but in cylindrical coordinates. Results reveal good CNN predictions with significant speedup. These results pave the way to new computational strategies to predict unsteady problems involving a Poisson equation, including configurations with coupled multiphysics interactions such as in plasma flows.",0
"The Poisson equation is essential for achieving a consistent solution in simulations of plasma fluids used in Hall effect thrusters and streamers discharges. The use of deep neural networks to solve the 2D Poisson equation with zero Dirichlet boundary conditions is explored, with a focus on multiple-scale architectures that vary in terms of the number of branches, depth, and receptive field. The receptive field is found to be particularly crucial for capturing the large topological structures of the field accurately. The investigation of multiple architectures, losses, and hyperparameters leads to the development of an optimal network for accurately solving the steady Poisson problem, which can be scaled for generalization to new resolutions and domain sizes. This network, called PlasmaNet, is then coupled with an unsteady Euler plasma fluid equations solver and tested on a variety of scenarios, including electron plasma oscillations and discharge propagation involving chemistry and advection. PlasmaNet outperforms existing solvers based on standard linear system algorithms for the Poisson equation and offers significant speedup, paving the way for new computational strategies for predicting unsteady problems involving a Poisson equation, including those with coupled multiphysics interactions such as in plasma flows.",1
"Accurate lake temperature estimation is essential for numerous problems tackled in both hydrological and ecological domains. Nowadays physical models are developed to estimate lake dynamics; however, computations needed for accurate estimation of lake surface temperature can get prohibitively expensive. We propose to aggregate simulations of lake temperature at a certain depth together with a range of meteorological features to probabilistically estimate lake surface temperature. Accordingly, we introduce a spatio-temporal neural network that combines Bayesian recurrent neural networks and Bayesian graph convolutional neural networks. This work demonstrates that the proposed graphical model can deliver homogeneously good performance covering the whole lake surface despite having sparse training data available. Quantitative results are compared with a state-of-the-art Bayesian deep learning method. Code for the developed architectural layers, as well as demo scripts, are available on https://renkulab.io/projects/das/bstnn.",0
"Accurately estimating the temperature of a lake is crucial for solving various issues in the hydrological and ecological fields. Although physical models are currently used to evaluate lake dynamics, computing the temperature of the lake's surface can become exceedingly expensive. To address this issue, we suggest combining simulations of the lake's temperature at a specific depth with an assortment of meteorological features to make a probabilistic estimate of the lake's surface temperature. A spatio-temporal neural network that merges Bayesian recurrent neural networks and Bayesian graph convolutional neural networks is introduced. Moreover, this study demonstrates that the proposed graphical model can achieve consistently excellent performance over the whole lake surface area, despite having limited training data. Additionally, quantitative findings are compared to those of a state-of-the-art Bayesian deep learning method. Developed architectural layers and demo scripts can be accessed at https://renkulab.io/projects/das/bstnn.",1
"We propose a novel framework for creating large-scale photorealistic datasets of indoor scenes, with ground truth geometry, material, lighting and semantics. Our goal is to make the dataset creation process widely accessible, transforming scans into photorealistic datasets with high-quality ground truth for appearance, layout, semantic labels, high quality spatially-varying BRDF and complex lighting, including direct, indirect and visibility components. This enables important applications in inverse rendering, scene understanding and robotics. We show that deep networks trained on the proposed dataset achieve competitive performance for shape, material and lighting estimation on real images, enabling photorealistic augmented reality applications, such as object insertion and material editing. We also show our semantic labels may be used for segmentation and multi-task learning. Finally, we demonstrate that our framework may also be integrated with physics engines, to create virtual robotics environments with unique ground truth such as friction coefficients and correspondence to real scenes. The dataset and all the tools to create such datasets will be made publicly available.",0
"Our framework offers a new approach to producing extensive photorealistic datasets of indoor scenes, complete with precise geometry, material, lighting, and semantics. We aim to make this process accessible for all, by converting scans into highly-detailed datasets, featuring accurate ground truth for appearance, layout, semantic labels, high quality spatially-varying BRDF, and sophisticated lighting, encompassing direct, indirect, and visibility components. Such datasets will have a significant impact on inverse rendering, scene comprehension, and robotics. Our deep networks, trained on this dataset, have exhibited impressive results for shape, material, and lighting estimation in real images, opening up photorealistic augmented reality applications like object insertion and material editing. Furthermore, our semantic labels offer segmentation and multi-task learning possibilities. Finally, we have demonstrated that our framework can be integrated with physics engines, creating virtual robotics environments with unprecedented ground truth, like friction coefficients, and correspondence to real scenes. We will make the dataset and all relevant tools publicly available.",1
"Can we improve machine-learning (ML) emulators with synthetic data? If data are scarce or expensive to source and a physical model is available, statistically generated data may be useful for augmenting training sets cheaply. Here we explore the use of copula-based models for generating synthetically augmented datasets in weather and climate by testing the method on a toy physical model of downwelling longwave radiation and corresponding neural network emulator. Results show that for copula-augmented datasets, predictions are improved by up to 62 % for the mean absolute error (from 1.17 to 0.44 W m$^{-2}$).",0
"Is it possible to enhance the performance of machine-learning (ML) emulators using synthetic data? In cases where data is scarce or expensive to obtain and a physical model is accessible, creating statistically generated data can be a cost-effective approach to supplementing training sets. This study examines the use of copula-based models to generate synthetically augmented datasets for weather and climate, utilizing a toy physical model of downwelling longwave radiation and its corresponding neural network emulator. The results indicate that by incorporating copula-augmented datasets, the mean absolute error can be improved by up to 62%, reducing it from 1.17 to 0.44 W m$^{-2}$.",1
"Out-of-home audience measurement aims to count and characterize the people exposed to advertising content in the physical world. While audience measurement solutions based on computer vision are of increasing interest, no commonly accepted benchmark exists to evaluate and compare their performance. In this paper, we propose the first benchmark for digital out-of-home audience measurement that evaluates the vision-based tasks of audience localization and counting, and audience demographics. The benchmark is composed of a novel, dataset captured at multiple locations and a set of performance measures. Using the benchmark, we present an in-depth comparison of eight open-source algorithms on four hardware platforms with GPU and CPU-optimized inferences and of two commercial off-the-shelf solutions for localization, count, age, and gender estimation. This benchmark and related open-source codes are available at http://ava.eecs.qmul.ac.uk.",0
"The objective of out-of-home audience measurement is to tally and define individuals who have been exposed to advertising content in the physical realm. Although there is a growing interest in audience measurement solutions that rely on computer vision, there is no commonly accepted standard to evaluate and compare their effectiveness. This paper introduces the first benchmark for digital out-of-home audience measurement, which assesses the audience localization and counting, as well as audience demographics. The benchmark consists of a new dataset captured at various locations and a series of performance metrics. By utilizing the benchmark, we conduct a thorough comparison of eight open-source algorithms on four hardware platforms with GPU and CPU-optimized inferences, and two commercial off-the-shelf solutions for localization, count, age, and gender estimation. This benchmark and associated open-source codes are accessible at http://ava.eecs.qmul.ac.uk.",1
"Detecting money laundering in gambling is becoming increasingly challenging for the gambling industry as consumers migrate to online channels. Whilst increasingly stringent regulations have been applied over the years to prevent money laundering in gambling, despite this, online gambling is still a channel for criminals to spend proceeds from crime. Complementing online gambling's growth more concerns are raised to its effects compared with gambling in traditional, physical formats, as it might introduce higher levels of problem gambling or fraudulent behaviour due to its nature of immediate interaction with online gambling experience. However, in most cases the main issue when organisations try to tackle those areas is the absence of high quality data. Since fraud detection related issues face the significant problem of the class imbalance, in this paper we propose a novel system based on Generative Adversarial Networks (GANs) for generating synthetic data in order to train a supervised classifier. Our framework Synthetic Data Generation GAN (SDG-GAN), manages to outperformed density based over-sampling methods and improve the classification performance of benchmarks datasets and the real world gambling fraud dataset.",0
"The detection of money laundering in the gambling industry has become increasingly difficult due to the shift of consumers to online channels. Despite the implementation of stricter regulations, criminals still use online gambling to launder money. This growth in online gambling has raised concerns about its impact on problem gambling and fraudulent behavior. However, the main issue in tackling these problems is the lack of high-quality data. To address this problem, we propose a novel system called Synthetic Data Generation GAN (SDG-GAN), which uses Generative Adversarial Networks (GANs) to generate synthetic data for training a supervised classifier. Our framework outperforms density-based oversampling methods and improves the classification performance of benchmark datasets and real-world gambling fraud datasets, despite the significant problem of class imbalance in fraud detection.",1
"Physics perception very often faces the problem that only limited data or partial measurements on the scene are available. In this work, we propose a strategy to learn the full state of sloshing liquids from measurements of the free surface. Our approach is based on recurrent neural networks (RNN) that project the limited information available to a reduced-order manifold so as to not only reconstruct the unknown information, but also to be capable of performing fluid reasoning about future scenarios in real time. To obtain physically consistent predictions, we train deep neural networks on the reduced-order manifold that, through the employ of inductive biases, ensure the fulfillment of the principles of thermodynamics. RNNs learn from history the required hidden information to correlate the limited information with the latent space where the simulation occurs. Finally, a decoder returns data back to the high-dimensional manifold, so as to provide the user with insightful information in the form of augmented reality. This algorithm is connected to a computer vision system to test the performance of the proposed methodology with real information, resulting in a system capable of understanding and predicting future states of the observed fluid in real-time.",0
"The field of physics perception often encounters the issue of having only partial data or measurements available. Our project presents a solution to this problem by utilizing recurrent neural networks (RNN) to learn the complete state of sloshing liquids solely from measurements of the free surface. Our approach involves reducing the available information to a lower-dimensional space to reconstruct the unknown information and make fluid reasoning predictions about future scenarios in real time. Our deep neural networks are trained with inductive biases to ensure that the principles of thermodynamics are met for physically consistent predictions. RNNs learn the hidden information required to correlate limited information with the latent space where the simulation occurs. The algorithm is then connected to a computer vision system to test its performance with real data, resulting in a system that can understand and predict future states of the observed fluid in real-time. Augmented reality is used to provide insightful information to the user, and a decoder returns data to the high-dimensional manifold.",1
"By estimating 3D shape and instances from a single view, we can capture information about an environment quickly, without the need for comprehensive scanning and multi-view fusion. Solving this task for composite scenes (such as object stacks) is challenging: occluded areas are not only ambiguous in shape but also in instance segmentation; multiple decompositions could be valid. We observe that physics constrains decomposition as well as shape in occluded regions and hypothesise that a latent space learned from scenes built under physics simulation can serve as a prior to better predict shape and instances in occluded regions. To this end we propose SIMstack, a depth-conditioned Variational Auto-Encoder (VAE), trained on a dataset of objects stacked under physics simulation. We formulate instance segmentation as a centre voting task which allows for class-agnostic detection and doesn't require setting the maximum number of objects in the scene. At test time, our model can generate 3D shape and instance segmentation from a single depth view, probabilistically sampling proposals for the occluded region from the learned latent space. Our method has practical applications in providing robots some of the ability humans have to make rapid intuitive inferences of partially observed scenes. We demonstrate an application for precise (non-disruptive) object grasping of unknown objects from a single depth view.",0
"The ability to quickly obtain information about an environment from a single view without extensive scanning and multi-view fusion can be achieved through estimating 3D shape and instances. However, this becomes challenging for composite scenes, such as object stacks, due to ambiguous occluded areas in shape and instance segmentation, and multiple valid decompositions. To overcome this challenge, we propose SIMstack, a depth-conditioned Variational Auto-Encoder (VAE), trained on physics simulation scenes to better predict shape and instances in occluded regions. Our model uses centre voting for class-agnostic detection and can generate 3D shape and instance segmentation from a single depth view by probabilistically sampling proposals for the occluded region from the learned latent space. This method has practical applications in providing robots with the ability to make rapid intuitive inferences of partially observed scenes. We demonstrate this with an application for precise object grasping of unknown objects from a single depth view.",1
"Recently, surrogate models based on deep learning have attracted much attention for engineering analysis and optimization. As the construction of data pairs in most engineering problems is time-consuming, data acquisition is becoming the predictive capability bottleneck of most deep surrogate models, which also exists in surrogate for thermal analysis and design. To address this issue, this paper develops a physics-informed convolutional neural network (CNN) for the thermal simulation surrogate. The network can learn a mapping from heat source layout to the steady-state temperature field without labeled data, which equals solving an entire family of partial difference equations (PDEs). To realize the physics-guided training without labeled data, we employ the heat conduction equation and finite difference method to construct the loss function. Since the solution is sensitive to boundary conditions, we properly impose hard constraints by padding in the Dirichlet and Neumann boundary conditions. In addition, the neural network architecture is well-designed to improve the prediction precision of the problem at hand, and pixel-level online hard example mining is introduced to overcome the imbalance of optimization difficulty in the computation domain. The experiments demonstrate that the proposed method can provide comparable predictions with numerical method and data-driven deep learning models. We also conduct various ablation studies to investigate the effectiveness of the network component and training methods proposed in this paper.",0
"Deep learning surrogate models have become increasingly popular for engineering analysis and optimization. However, data acquisition is often time-consuming and can hinder the predictive capabilities of these models. This issue is also present in surrogate models for thermal analysis and design. To address this problem, a physics-informed convolutional neural network (CNN) has been developed for thermal simulation surrogate. The network can learn to map heat source layout to steady-state temperature fields without labeled data. This process is equivalent to solving a family of partial difference equations (PDEs). To achieve physics-guided training without labeled data, the heat conduction equation and finite difference method were used to construct a loss function. Hard constraints were imposed on the Dirichlet and Neumann boundary conditions, and the neural network architecture was designed to improve prediction precision. Pixel-level online hard example mining was introduced to overcome optimization difficulty imbalance in the computation domain. Experiments showed that the proposed method provides comparable predictions to numerical and data-driven deep learning models. Ablation studies were also conducted to investigate the effectiveness of the network components and training methods proposed in the paper.",1
"Modeling complex physical dynamics is a fundamental task in science and engineering. Traditional physics-based models are sample efficient, interpretable but often rely on rigid assumptions. Furthermore, direct numerical approximation is usually computationally intensive, requiring significant computational resources and expertise. While deep learning (DL) provides novel alternatives for efficiently recognizing complex patterns and emulating nonlinear dynamics, its predictions do not necessarily obey the governing laws of physical systems, nor do they generalize well across different systems. Thus, the study of physics-guided DL emerged and has gained great progress. Physics-guided DL aims to take the best from both physics-based modeling and state-of-the-art DL models to better solve scientific problems. In this paper, we provide a structured overview of existing methodologies of integrating prior physical knowledge or physics-based modeling into DL, with a special emphasis on learning dynamical systems. We also discuss the fundamental challenges and emerging opportunities in the area.",0
"The task of modeling intricate physical dynamics plays a crucial role in scientific and engineering fields. Although traditional physics-based models are reliable and easy to understand, they often rely on inflexible assumptions. In addition, the direct numerical approximation method is computationally demanding and requires significant expertise and resources. While deep learning (DL) presents a promising solution for recognizing complex patterns and emulating nonlinear dynamics, its predictions may not adhere to the governing laws of physical systems, nor generalize well across various systems. To address these issues, the study of physics-guided DL has emerged and made significant strides. The objective of physics-guided DL is to combine the strengths of both physics-based modeling and state-of-the-art DL models to better solve scientific problems. This paper offers a comprehensive overview of various methodologies for integrating prior physical knowledge or physics-based modeling into DL, with a particular focus on learning dynamical systems. Additionally, this paper discusses the primary challenges and emerging opportunities in this area.",1
"Networks are a powerful tool to model complex systems, and the definition of many Graph Neural Networks (GNN), Deep Learning algorithms that can handle networks, has opened a new way to approach many real-world problems that would be hardly or even untractable. In this paper, we propose mGNN, a framework meant to generalize GNNs to the case of multi-layer networks, i.e., networks that can model multiple kinds of interactions and relations between nodes. Our approach is general (i.e., not task specific) and has the advantage of extending any type of GNN without any computational overhead. We test the framework into three different tasks (node and network classification, link prediction) to validate it.",0
"Complex systems can be modeled effectively using networks, and the advent of Graph Neural Networks (GNN) has enabled the handling of networks with Deep Learning algorithms. This has created a new approach to solving real-world problems that were previously challenging or impossible. Our paper introduces mGNN, a framework intended to generalize GNNs to multi-layer networks, which can represent various types of interactions and relationships between nodes. Our approach is not task-specific and offers the benefit of extending any GNN type without extra computational burden. We validate our framework by testing it on three tasks: node classification, network classification, and link prediction.",1
"Diffusion tensor imaging (DTI) has been used to study the effects of neurodegenerative diseases on neural pathways, which may lead to more reliable and early diagnosis of these diseases as well as a better understanding of how they affect the brain. We introduce an intelligent visual analytics system for studying patient groups based on their labeled DTI fiber tract data and corresponding statistics. The system's AI-augmented interface guides the user through an organized and holistic analysis space, including the statistical feature space, the physical space, and the space of patients over different groups. We use a custom machine learning pipeline to help narrow down this large analysis space, and then explore it pragmatically through a range of linked visualizations. We conduct several case studies using real data from the research database of Parkinson's Progression Markers Initiative.",0
"The application of diffusion tensor imaging (DTI) in the investigation of the impact of neurodegenerative diseases on neural pathways has the potential to enhance the accuracy and early detection of these diseases, as well as provide deeper insight into their impact on the brain. To this end, we present an intelligent visual analytics system that employs labeled DTI fiber tract data and associated statistics to analyze patient groups. The system incorporates an AI-assisted interface that facilitates an organized and comprehensive analysis of the statistical feature space, physical space, and patient groups. A customized machine learning pipeline is employed to narrow down the analysis space, which is then explored through numerous linked visualizations. We demonstrate the system's efficacy through several case studies utilizing actual data from the Parkinson's Progression Markers Initiative research database.",1
"In recent years, photogrammetry has been widely used in many areas to create photorealistic 3D virtual data representing the physical environment. The innovation of small unmanned aerial vehicles (sUAVs) has provided additional high-resolution imaging capabilities with low cost for mapping a relatively large area of interest. These cutting-edge technologies have caught the US Army and Navy's attention for the purpose of rapid 3D battlefield reconstruction, virtual training, and simulations. Our previous works have demonstrated the importance of information extraction from the derived photogrammetric data to create semantic-rich virtual environments (Chen et al., 2019). For example, an increase of simulation realism and fidelity was achieved by segmenting and replacing photogrammetric trees with game-ready tree models. In this work, we further investigated the semantic information extraction problem and focused on the ground material segmentation and object detection tasks. The main innovation of this work was that we leveraged both the original 2D images and the derived 3D photogrammetric data to overcome the challenges faced when using each individual data source. For ground material segmentation, we utilized an existing convolutional neural network architecture (i.e., 3DMV) which was originally designed for segmenting RGB-D sensed indoor data. We improved its performance for outdoor photogrammetric data by introducing a depth pooling layer in the architecture to take into consideration the distance between the source images and the reconstructed terrain model. To test the performance of our improved 3DMV, a ground truth ground material database was created using data from the One World Terrain (OWT) data repository. Finally, a workflow for importing the segmented ground materials into a virtual simulation scene was introduced, and visual results are reported in this paper.",0
"Photogrammetry has become widely utilized in recent years to create realistic 3D virtual data that represents the physical environment. Small unmanned aerial vehicles (sUAVs) have added high-resolution imaging capabilities at a low cost, making them particularly useful for mapping large areas of interest. The US Army and Navy have taken notice of these innovative technologies, seeing their potential for rapid 3D battlefield reconstruction, virtual training, and simulations. Previous work has shown that extracting information from photogrammetric data can create semantic-rich virtual environments. In this study, the focus was on ground material segmentation and object detection tasks. Both the original 2D images and derived 3D photogrammetric data were utilized to overcome challenges faced when using each data source individually. To improve performance for outdoor photogrammetric data, an existing convolutional neural network architecture was modified by including a depth pooling layer to consider the distance between source images and reconstructed terrain models. The improved architecture was tested using a ground truth ground material database from the One World Terrain (OWT) data repository. A workflow for incorporating segmented ground materials into virtual simulation scenes was also introduced, with visual results reported in the paper.",1
"Time-lapse fluorescent microscopy (TLFM) combined with predictive mathematical modelling is a powerful tool to study the inherently dynamic processes of life on the single-cell level. Such experiments are costly, complex and labour intensive. A complimentary approach and a step towards in silico experimentation, is to synthesise the imagery itself. Here, we propose Multi-StyleGAN as a descriptive approach to simulate time-lapse fluorescence microscopy imagery of living cells, based on a past experiment. This novel generative adversarial network synthesises a multi-domain sequence of consecutive timesteps. We showcase Multi-StyleGAN on imagery of multiple live yeast cells in microstructured environments and train on a dataset recorded in our laboratory. The simulation captures underlying biophysical factors and time dependencies, such as cell morphology, growth, physical interactions, as well as the intensity of a fluorescent reporter protein. An immediate application is to generate additional training and validation data for feature extraction algorithms or to aid and expedite development of advanced experimental techniques such as online monitoring or control of cells.   Code and dataset is available at https://git.rwth-aachen.de/bcs/projects/tp/multi-stylegan.",0
"The study of life processes at the single-cell level is complex and expensive. Time-lapse fluorescent microscopy (TLFM) combined with predictive mathematical modelling is a powerful tool for this purpose. However, to reduce costs and time required, an alternative approach is to synthesise the imagery itself. In this regard, Multi-StyleGAN is a new generative adversarial network that can simulate time-lapse fluorescence microscopy imagery of living cells, based on a previous experiment. Multi-StyleGAN can capture biophysical factors and time dependencies, such as cell morphology, growth, physical interactions, and intensity of fluorescent reporter proteins. The simulation can be used to generate additional training and validation data for feature extraction algorithms or to expedite the development of advanced experimental techniques such as online monitoring or control of cells. The code and dataset for Multi-StyleGAN are available at https://git.rwth-aachen.de/bcs/projects/tp/multi-stylegan.",1
"Automated model discovery of partial differential equations (PDEs) usually considers a single experiment or dataset to infer the underlying governing equations. In practice, experiments have inherent natural variability in parameters, initial and boundary conditions that cannot be simply averaged out. We introduce a randomised adaptive group Lasso sparsity estimator to promote grouped sparsity and implement it in a deep learning based PDE discovery framework. It allows to create a learning bias that implies the a priori assumption that all experiments can be explained by the same underlying PDE terms with potentially different coefficients. Our experimental results show more generalizable PDEs can be found from multiple highly noisy datasets, by this grouped sparsity promotion rather than simply performing independent model discoveries.",0
"The typical approach to discovering automated models for partial differential equations (PDEs) involves using a single experiment or dataset to identify the governing equations. However, this approach fails to account for the natural variability in parameters, initial and boundary conditions that is inherent in experiments and cannot be easily averaged out. To overcome this limitation, we have developed a deep learning-based PDE discovery framework that includes a randomised adaptive group Lasso sparsity estimator, which promotes grouped sparsity. This approach assumes that all experiments can be explained by the same underlying PDE terms, but with different coefficients. Our experimental results demonstrate that this approach leads to the discovery of more generalizable PDEs from multiple highly noisy datasets, rather than relying on independent model discoveries.",1
"Physically-inspired latent force models offer an interpretable alternative to purely data driven tools for inference in dynamical systems. They carry the structure of differential equations and the flexibility of Gaussian processes, yielding interpretable parameters and dynamics-imposed latent functions. However, the existing inference techniques associated with these models rely on the exact computation of posterior kernel terms which are seldom available in analytical form. Most applications relevant to practitioners, such as Hill equations or diffusion equations, are hence intractable. In this paper, we overcome these computational problems by proposing a variational solution to a general class of non-linear and parabolic partial differential equation latent force models. Further, we show that a neural operator approach can scale our model to thousands of instances, enabling fast, distributed computation. We demonstrate the efficacy and flexibility of our framework by achieving competitive performance on several tasks where the kernels are of varying degrees of tractability.",0
"Latent force models that are based on physical principles offer an understandable alternative to data-driven tools for inference in dynamical systems. These models have the characteristics of differential equations and the adaptability of Gaussian processes, giving rise to parameters that are easily interpreted and latent functions that are imposed by the dynamics. However, the present methods for inference with these models depend on precise computation of posterior kernel terms, which are rarely available in analytical form for applications that are relevant to practitioners, such as Hill equations or diffusion equations. Consequently, these applications are difficult to handle. In this study, we solve these computational problems by proposing a variational solution for a broad range of nonlinear and parabolic partial differential equation latent force models. Moreover, we show that a neural operator approach can expand our model to thousands of instances, resulting in swift and distributed computation. We prove the effectiveness and adaptability of our framework by achieving competitive performance on various tasks with kernels that have different levels of tractability.",1
"Hand pose estimation (HPE) can be used for a variety of human-computer interaction applications such as gesture-based control for physical or virtual/augmented reality devices. Recent works have shown that videos or multi-view images carry rich information regarding the hand, allowing for the development of more robust HPE systems. In this paper, we present the Multi-View Video-Based 3D Hand (MuViHand) dataset, consisting of multi-view videos of the hand along with ground-truth 3D pose labels. Our dataset includes more than 402,000 synthetic hand images available in 4,560 videos. The videos have been simultaneously captured from six different angles with complex backgrounds and random levels of dynamic lighting. The data has been captured from 10 distinct animated subjects using 12 cameras in a semi-circle topology where six tracking cameras only focus on the hand and the other six fixed cameras capture the entire body. Next, we implement MuViHandNet, a neural pipeline consisting of image encoders for obtaining visual embeddings of the hand, recurrent learners to learn both temporal and angular sequential information, and graph networks with U-Net architectures to estimate the final 3D pose information. We perform extensive experiments and show the challenging nature of this new dataset as well as the effectiveness of our proposed method. Ablation studies show the added value of each component in MuViHandNet, as well as the benefit of having temporal and sequential information in the dataset.",0
"The estimation of hand poses can be applied in various human-computer interaction scenarios, including controlling physical or virtual/augmented reality devices using gestures. Recent studies have demonstrated that videos and multi-view images yield abundant information about the hand, resulting in more robust hand pose estimation systems. This research paper introduces the Multi-View Video-Based 3D Hand (MuViHand) dataset, which contains multi-view videos of the hand with 3D pose labels. The dataset comprises over 402,000 synthetic hand images distributed across 4,560 videos featuring complex backgrounds and varying dynamic lighting levels, all captured simultaneously from six different angles. The data was collected from ten animated subjects, using twelve cameras in a semi-circle topology. Six of these cameras focus solely on the hand, while the remaining six capture the entire body. To estimate the 3D pose information, we developed MuViHandNet, a neural pipeline that includes image encoders, recurrent learners, and graph networks with U-Net architectures. We conducted extensive experiments that demonstrate the challenging nature of this new dataset and the effectiveness of our proposed method. Our ablation studies also highlight the added value of each component in MuViHandNet, as well as the benefit of having temporal and sequential information in the dataset.",1
"Graph neural networks (GNNs) are attractive for learning properties of atomic structures thanks to their intuitive, physically informed graph encoding of atoms and bonds. However, conventional GNN encodings do not account for angular information, which is critical for describing complex atomic arrangements in disordered materials, interfaces, and molecular distortions. In this work, we extend the recently proposed ALIGNN encoding, which incorporates bond angles, to also include dihedral angles (ALIGNN-d), and we apply the model to capture the structures of aqua copper complexes for spectroscopy prediction. This simple extension is shown to lead to a memory-efficient graph representation capable of capturing the full geometric information of atomic structures. Specifically, the ALIGNN-d encoding is a sparse yet equally expressive representation compared to the dense, maximally-connected graph, in which all bonds are encoded. We also explore model interpretability based on ALIGNN-d by elucidating the relative contributions of individual structural components to the optical response of the copper complexes. Lastly, we briefly discuss future developments to validate the computational efficiency and to extend the interpretability of ALIGNN-d.",0
"The use of graph neural networks (GNNs) for learning properties of atomic structures is appealing due to their intuitive and physically informed graph encoding of atoms and bonds. However, traditional GNN encodings lack angular information, which is essential for describing complex atomic arrangements in disordered materials, interfaces, and molecular distortions. To address this issue, we expand on the ALIGNN encoding, which includes bond angles, by adding dihedral angles (ALIGNN-d). We apply this model to predict spectroscopy in aqua copper complexes, and it is demonstrated that this simple expansion results in a memory-efficient graph representation capable of capturing the full geometric information of atomic structures. The ALIGNN-d encoding is a sparse yet equally effective representation compared to the dense, fully-connected graph. In addition, we investigate model interpretability based on ALIGNN-d by clarifying the individual contributions of structural components to the optical response of copper complexes. Finally, we discuss potential future developments to confirm the computational efficiency and extend the interpretability of ALIGNN-d.",1
"Deep learning techniques have emerged as a promising approach to highly accelerated MRI. However, recent reconstruction challenges have shown several drawbacks in current deep learning approaches, including the loss of fine image details even using models that perform well in terms of global quality metrics. In this study, we propose an end-to-end deep learning framework for image reconstruction and pathology detection, which enables a clinically aware evaluation of deep learning reconstruction quality. The solution is demonstrated for a use case in detecting meniscal tears on knee MRI studies, ultimately finding a loss of fine image details with common reconstruction methods expressed as a reduced ability to detect important pathology like meniscal tears. Despite the common practice of quantitative reconstruction methodology evaluation with metrics such as SSIM, impaired pathology detection as an automated pathology-based reconstruction evaluation approach suggests existing quantitative methods do not capture clinically important reconstruction outcomes.",0
"Deep learning techniques have shown promise for accelerating MRI scans, but recent challenges have revealed limitations that result in the loss of image details, even for models that perform well on global quality metrics. This study proposes an end-to-end deep learning framework for image reconstruction and pathology detection, which allows for a clinically informed evaluation of deep learning reconstruction quality. The framework is applied to the detection of meniscal tears in knee MRI scans, revealing that standard reconstruction methods result in a loss of important details and impaired pathology detection. This suggests that current quantitative evaluation methods, such as SSIM, may not fully capture clinically significant reconstruction outcomes.",1
"Measurement noise is an integral part while collecting data of a physical process. Thus, noise removal is a necessary step to draw conclusions from these data, and it often becomes quite essential to construct dynamical models using these data. We discuss a methodology to learn differential equation(s) using noisy and sparsely sampled measurements. In our methodology, the main innovation can be seen in of integration of deep neural networks with a classical numerical integration method. Precisely, we aim at learning a neural network that implicitly represents the data and an additional neural network that models the vector fields of the dependent variables. We combine these two networks by enforcing the constraint that the data at the next time-steps can be given by following a numerical integration scheme such as the fourth-order Runge-Kutta scheme. The proposed framework to learn a model predicting the vector field is highly effective under noisy measurements. The approach can handle scenarios where dependent variables are not available at the same temporal grid. We demonstrate the effectiveness of the proposed method to learning models using data obtained from various differential equations. The proposed approach provides a promising methodology to learn dynamic models, where the first-principle understanding remains opaque.",0
"When collecting data from a physical process, measurement noise is unavoidable and must be removed in order to draw accurate conclusions. This is especially important when constructing dynamical models using the data. In this paper, we present a methodology for learning differential equations using noisy and sparsely sampled measurements. Our approach combines deep neural networks with a classical numerical integration method, with one network representing the data and another modeling the vector fields of the dependent variables. We enforce the constraint that the data at the next time-steps can be given by following a numerical integration scheme such as the fourth-order Runge-Kutta scheme. This framework is effective for learning models under noisy measurements and can handle scenarios where dependent variables are not available at the same temporal grid. We demonstrate the effectiveness of our approach using data obtained from various differential equations. Our proposed methodology provides a promising solution for learning dynamic models when first-principle understanding is unclear.",1
"We present Hand ArticuLated Occupancy (HALO), a novel representation of articulated hands that bridges the advantages of 3D keypoints and neural implicit surfaces and can be used in end-to-end trainable architectures. Unlike existing statistical parametric hand models (e.g.~MANO), HALO directly leverages 3D joint skeleton as input and produces a neural occupancy volume representing the posed hand surface. The key benefits of HALO are (1) it is driven by 3D key points, which have benefits in terms of accuracy and are easier to learn for neural networks than the latent hand-model parameters; (2) it provides a differentiable volumetric occupancy representation of the posed hand; (3) it can be trained end-to-end, allowing the formulation of losses on the hand surface that benefit the learning of 3D keypoints. We demonstrate the applicability of HALO to the task of conditional generation of hands that grasp 3D objects. The differentiable nature of HALO is shown to improve the quality of the synthesized hands both in terms of physical plausibility and user preference.",0
"Introducing a new method for representing articulated hands called Hand ArticuLated Occupancy (HALO), which combines the benefits of 3D keypoints and neural implicit surfaces. Unlike existing statistical parametric hand models, HALO utilizes 3D joint skeletons as input and generates a neural occupancy volume that represents the hand surface. HALO has several advantages, including higher accuracy, ease of learning for neural networks, and a differentiable volumetric occupancy representation of the posed hand. It can be trained end-to-end, allowing for losses on the hand surface to benefit the learning of 3D keypoints. We demonstrate the usefulness of HALO for generating hands that grasp 3D objects and show that its differentiable nature improves the quality of the synthesized hands in terms of physical plausibility and user preference.",1
"Autonomous systems generate a huge amount of multimodal data that are collected and processed on the Edge, in order to enable AI-based services. The collected datasets are pre-processed in order to extract informative attributes, called features, which are used to feed AI algorithms. Due to the limited computational and communication resources of some CPS, like autonomous vehicles, selecting the subset of relevant features from a dataset is of the utmost importance, in order to improve the result achieved by learning methods and to reduce computation and communication costs. Precisely, feature selection is the candidate approach, which assumes that data contain a certain number of redundant or irrelevant attributes that can be eliminated. The quality of our methods is confirmed by the promising results achieved on two different data sets. In this work, we propose, for the first time, a federated feature selection method suitable for being executed in a distributed manner. Precisely, our results show that a fleet of autonomous vehicles finds a consensus on the optimal set of features that they exploit to reduce data transmission up to 99% with negligible information loss.",0
"The Edge processes a vast amount of multimodal data from autonomous systems to enable AI-based services. Prior to feeding AI algorithms, the collected datasets are pre-processed to extract informative attributes known as features. For some CPS, such as autonomous vehicles, selecting the relevant features from a dataset is crucial to improve learning methods and reduce computation and communication costs. Feature selection is an effective approach that eliminates redundant or irrelevant attributes. Our methods have yielded promising results on two different datasets. In this study, we introduce a distributed federated feature selection method, which allows a fleet of autonomous vehicles to reach a consensus on the optimal set of features. Our results demonstrate that this approach can reduce data transmission up to 99% with minimal loss of information.",1
"Dense vertex-to-vertex correspondence between 3D faces is a fundamental and challenging issue for 3D&2D face analysis. While the sparse landmarks have anatomically ground-truth correspondence, the dense vertex correspondences on most facial regions are unknown. In this view, the current literatures commonly result in reasonable but diverse solutions, which deviate from the optimum to the 3D face dense registration problem. In this paper, we revisit dense registration by a dimension-degraded problem, i.e. proportional segmentation of a line, and employ an iterative dividing and diffusing method to reach the final solution uniquely. This method is then extended to 3D surface by formulating a local registration problem for dividing and a linear least-square problem for diffusing, with constraints on fixed features. On this basis, we further propose a multi-resolution algorithm to accelerate the computational process. The proposed method is linked to a novel local scaling metric, where we illustrate the physical meaning as smooth rearrangement for local cells of 3D facial shapes. Extensive experiments on public datasets demonstrate the effectiveness of the proposed method in various aspects. Generally, the proposed method leads to coherent local registrations and elegant mesh grid routines for fine-grained 3D face dense registrations, which benefits many downstream applications significantly. It can also be applied to dense correspondence for other format of data which are not limited to face. The core code will be publicly available at https://github.com/NaughtyZZ/3D_face_dense_registration.",0
"Establishing a dense vertex-to-vertex correspondence between 3D faces poses a fundamental and challenging issue in 3D&2D face analysis. Although sparse landmarks provide anatomically grounded correspondence, dense vertex correspondences in most facial regions remain unknown. As a result, current literature yields diverse and reasonable solutions that deviate from the optimal solution for 3D face dense registration. In this paper, we propose a method that employs proportional segmentation of a line to revisit dense registration as a dimension-degraded problem, followed by an iterative dividing and diffusing approach to obtain a unique final solution. We extend this method to 3D surfaces by dividing and solving a local registration problem and diffusing using linear least-squares with constraints on fixed features. Furthermore, we propose a multi-resolution algorithm to accelerate the computational process. Our method is linked to a novel local scaling metric, which we demonstrate as smooth rearrangement for local cells of 3D facial shapes. Extensive experiments on public datasets demonstrate the effectiveness of our proposed method in various aspects. It leads to coherent local registrations and elegant mesh grid routines for fine-grained 3D face dense registrations, significantly benefiting many downstream applications. Moreover, our method can be applied to dense correspondence for other data formats beyond faces. The core code is publicly available at https://github.com/NaughtyZZ/3D_face_dense_registration.",1
"The transition to green energy grids depends on detailed wind and solar forecasts to optimize the siting and scheduling of renewable energy generation. Operational forecasts from numerical weather prediction models, however, only have a spatial resolution of 10 to 20-km, which leads to sub-optimal usage and development of renewable energy farms. Weather scientists have been developing super-resolution methods to increase the resolution, but often rely on simple interpolation techniques or computationally expensive differential equation-based models. Recently, machine learning-based models, specifically the physics-informed resolution-enhancing generative adversarial network (PhIREGAN), have outperformed traditional downscaling methods. We provide a thorough and extensible benchmark of leading deep learning-based super-resolution techniques, including the enhanced super-resolution generative adversarial network (ESRGAN) and an enhanced deep super-resolution (EDSR) network, on wind and solar data. We accompany the benchmark with a novel public, processed, and machine learning-ready dataset for benchmarking super-resolution methods on wind and solar data.",0
"The success of transitioning to green energy grids is reliant on precise wind and solar predictions to optimize the placement and scheduling of renewable energy generation. However, operational forecasts from weather prediction models are limited to a spatial resolution of 10-20 km, resulting in suboptimal development and usage of renewable energy farms. Scientists have been working on super-resolution techniques to improve the resolution, but they often rely on expensive differential equation-based models or simple interpolation techniques. Recently, machine learning-based models such as the physics-informed resolution-enhancing generative adversarial network (PhIREGAN) have proven to be more effective than traditional downscaling methods. In this study, we present a comprehensive and adaptable evaluation of leading deep learning-based super-resolution techniques, including the enhanced super-resolution generative adversarial network (ESRGAN) and the enhanced deep super-resolution (EDSR) network, on wind and solar data. Additionally, we provide a novel publicly available dataset that has been processed and is ready for machine learning to benchmark super-resolution methods on wind and solar data.",1
We propose a framework for predictive uncertainty quantification of a neural network that replaces the conventional Bayesian notion of weight probability density function (PDF) with a physics based potential field representation of the model weights in a Gaussian reproducing kernel Hilbert space (RKHS) embedding. This allows us to use perturbation theory from quantum physics to formulate a moment decomposition problem over the model weight-output relationship. The extracted moments reveal successive degrees of regularization of the weight potential field around the local neighborhood of the model output. Such localized moments represent well the PDF tails and provide significantly greater accuracy of the model's predictive uncertainty than the central moments characterized by Bayesian and ensemble methods or their variants. We show that this consequently leads to a better ability to detect false model predictions of test data that has undergone a covariate shift away from the training PDF learned by the model. We evaluate our approach against baseline uncertainty quantification methods on several benchmark datasets that are corrupted using common distortion techniques. Our approach provides fast model predictive uncertainty estimates with much greater precision and calibration.,0
"Our proposed framework for quantifying predictive uncertainty in neural networks utilizes a physics-based potential field representation of model weights in a Gaussian RKHS embedding, in place of the traditional Bayesian weight probability density function. By employing perturbation theory from quantum physics, we can decompose moments and uncover successive levels of regularization in the weight potential field surrounding the local neighborhood of the model output. These localized moments provide a more accurate representation of the PDF tails and enhance the model's predictive uncertainty beyond that of Bayesian and ensemble methods. This improved uncertainty quantification allows for better detection of false model predictions when test data deviates from the training PDF. We evaluated our approach against standard uncertainty quantification methods on several benchmark datasets with common distortion techniques and found that our approach provides faster and more precise model predictive uncertainty estimates with better calibration.",1
"Assessment of mental workload in real-world conditions is key to ensure the performance of workers executing tasks that demand sustained attention. Previous literature has employed electroencephalography (EEG) to this end despite having observed that EEG correlates of mental workload vary across subjects and physical strain, thus making it difficult to devise models capable of simultaneously presenting reliable performance across users. Domain adaptation consists of a set of strategies that aim at allowing for improving machine learning systems performance on unseen data at training time. Such methods, however, might rely on assumptions over the considered data distributions, which typically do not hold for applications of EEG data. Motivated by this observation, in this work we propose a strategy to estimate two types of discrepancies between multiple data distributions, namely marginal and conditional shifts, observed on data collected from different subjects. Besides shedding light on the assumptions that hold for a particular dataset, the estimates of statistical shifts obtained with the proposed approach can be used for investigating other aspects of a machine learning pipeline, such as quantitatively assessing the effectiveness of domain adaptation strategies. In particular, we consider EEG data collected from individuals performing mental tasks while running on a treadmill and pedaling on a stationary bike and explore the effects of different normalization strategies commonly used to mitigate cross-subject variability. We show the effects that different normalization schemes have on statistical shifts and their relationship with the accuracy of mental workload prediction as assessed on unseen participants at training time.",0
"To ensure workers performing tasks that require sustained attention are able to perform well, it is important to assess their mental workload in real-world conditions. Previous research has used electroencephalography (EEG) for this purpose, but the results have shown that the EEG correlates of mental workload vary between individuals and physical exertion levels, making it difficult to create models that can provide reliable performance across all users. Domain adaptation is a set of strategies that aim to improve machine learning systems' performance on unseen data during training, but these methods often rely on assumptions about the data that do not apply to EEG data. In this study, we propose a strategy to estimate the discrepancies between multiple data distributions, which can help researchers understand the assumptions that hold for a particular dataset and assess the effectiveness of domain adaptation strategies. We use EEG data collected from individuals performing mental tasks while running on a treadmill and pedaling on a stationary bike to explore the effects of different normalization strategies on statistical shifts and the accuracy of mental workload prediction.",1
"Neural networks are increasingly used to estimate parameters in quantitative MRI, in particular in magnetic resonance fingerprinting. Their advantages over the gold standard non-linear least square fitting are their superior speed and their immunity to the non-convexity of many fitting problems. We find, however, that in heterogeneous parameter spaces, i.e. in spaces in which the variance of the estimated parameters varies considerably, good performance is hard to achieve and requires arduous tweaking of the loss function, hyper parameters, and the distribution of the training data in parameter space. Here, we address these issues with a theoretically well-founded loss function: the Cram\'er-Rao bound (CRB) provides a theoretical lower bound for the variance of an unbiased estimator and we propose to normalize the squared error with respective CRB. With this normalization, we balance the contributions of hard-to-estimate and not-so-hard-to-estimate parameters and areas in parameter space, and avoid a dominance of the former in the overall training loss. Further, the CRB-based loss function equals one for a maximally-efficient unbiased estimator, which we consider the ideal estimator. Hence, the proposed CRB-based loss function provides an absolute evaluation metric. We compare a network trained with the CRB-based loss with a network trained with the commonly used means squared error loss and demonstrate the advantages of the former in numerical, phantom, and in vivo experiments.",0
"The use of neural networks has risen in the estimation of parameters in quantitative MRI, especially in magnetic resonance fingerprinting. Their speed and ability to handle non-convex fitting problems make them superior to the traditional non-linear least square fitting method. However, when dealing with diverse parameter spaces where the variance of estimated parameters varies significantly, achieving good performance becomes a challenge and requires extensive adjustments to the loss function, hyper parameters, and training data distribution. To address these issues, we propose a theoretically sound loss function based on the Cram\'er-Rao bound (CRB), which provides a theoretical lower limit for the variance of an unbiased estimator. We normalize the squared error with the respective CRB to balance the contributions of challenging-to-estimate and easier-to-estimate parameters and areas in parameter space, thus avoiding the dominance of the former in the overall training loss. Additionally, the CRB-based loss function equals one for a maximally-efficient unbiased estimator, which we consider the ideal estimator. Therefore, the proposed CRB-based loss function provides an absolute evaluation metric. We compare a network trained with the CRB-based loss function to one trained with the commonly used mean squared error loss function and demonstrate the benefits of the former in numerical, phantom, and in vivo experiments.",1
"Recent advances in object segmentation have demonstrated that deep neural networks excel at object segmentation for specific classes in color and depth images. However, their performance is dictated by the number of classes and objects used for training, thereby hindering generalization to never seen objects or zero-shot samples. To exacerbate the problem further, object segmentation using image frames rely on recognition and pattern matching cues. Instead, we utilize the 'active' nature of a robot and their ability to 'interact' with the environment to induce additional geometric constraints for segmenting zero-shot samples.   In this paper, we present the first framework to segment unknown objects in a cluttered scene by repeatedly 'nudging' at the objects and moving them to obtain additional motion cues at every step using only a monochrome monocular camera. We call our framework NudgeSeg. These motion cues are used to refine the segmentation masks. We successfully test our approach to segment novel objects in various cluttered scenes and provide an extensive study with image and motion segmentation methods. We show an impressive average detection rate of over 86% on zero-shot objects.",0
"Recent progress in object segmentation has revealed that deep neural networks are highly skilled at segmenting objects of certain classes in color and depth images. However, their effectiveness is influenced by the number of classes and objects utilized during training, which hinders their ability to generalize to unseen objects or zero-shot samples. Additionally, object segmentation using image frames is reliant on recognition and pattern matching cues, which exacerbates the issue. In contrast, our approach leverages the active nature of robots and their ability to interact with the environment to establish additional geometric constraints for segmenting zero-shot samples. We introduce NudgeSeg, which is the first framework for segmenting unknown objects in a cluttered scene by repeatedly nudging the objects and moving them to obtain additional motion cues using only a monochrome monocular camera. These motion cues are then employed to fine-tune the segmentation masks. We conducted successful tests on our method to segment new objects in various cluttered scenes, and we provide an extensive study comparing image and motion segmentation methods. Our results show an outstanding average detection rate of over 86% on zero-shot objects.",1
"Molecule representation learning (MRL) methods aim to embed molecules into a real vector space. However, existing SMILES-based (Simplified Molecular-Input Line-Entry System) or GNN-based (Graph Neural Networks) MRL methods either take SMILES strings as input that have difficulty in encoding molecule structure information, or over-emphasize the importance of GNN architectures but neglect their generalization ability. Here we propose using chemical reactions to assist learning molecule representation. The key idea of our approach is to preserve the equivalence of molecules with respect to chemical reactions in the embedding space, i.e., forcing the sum of reactant embeddings and the sum of product embeddings to be equal for each chemical equation. This constraint is proven effective to 1) keep the embedding space well-organized and 2) improve the generalization ability of molecule embeddings. Moreover, our model can use any GNN as the molecule encoder and is thus agnostic to GNN architectures. Experimental results demonstrate that our method achieves state-of-the-art performance in a variety of downstream tasks, e.g., 17.4% absolute Hit@1 gain in chemical reaction prediction, 2.3% absolute AUC gain in molecule property prediction, and 18.5% relative RMSE gain in graph-edit-distance prediction, respectively, over the best baseline method. The code is available at https://github.com/hwwang55/MolR.",0
"Molecule representation learning (MRL) methods seek to embed molecules into a real vector space. However, current MRL methods that are based on SMILES or GNN suffer from certain limitations. SMILES-based MRL methods struggle to encode molecule structure information, while GNN-based methods over-emphasize the importance of GNN architectures without considering their generalization ability. To address these issues, we propose a new approach that uses chemical reactions to facilitate learning molecule representation. Our approach preserves the equivalence of molecules with respect to chemical reactions in the embedding space, ensuring that the sum of reactant embeddings and the sum of product embeddings are equal for each chemical equation. This constraint enhances the organization of the embedding space and improves the generalization ability of molecule embeddings. Our model can use any GNN as the molecule encoder and is agnostic to GNN architectures. Our experimental results demonstrate that our method outperforms the best baseline method in various downstream tasks, such as chemical reaction prediction, molecule property prediction, and graph-edit-distance prediction. Our code is available at https://github.com/hwwang55/MolR.",1
"Single image dehazing is a prerequisite which affects the performance of many computer vision tasks and has attracted increasing attention in recent years. However, most existing dehazing methods emphasize more on haze removal but less on the detail recovery of the dehazed images. In this paper, we propose a single image dehazing method with an independent Detail Recovery Network (DRN), which considers capturing the details from the input image over a separate network and then integrates them into a coarse dehazed image. The overall network consists of two independent networks, named DRN and the dehazing network respectively. Specifically, the DRN aims to recover the dehazed image details through local and global branches respectively. The local branch can obtain local detail information through the convolution layer and the global branch can capture more global information by the Smooth Dilated Convolution (SDC). The detail feature map is fused into the coarse dehazed image to obtain the dehazed image with rich image details. Besides, we integrate the DRN, the physical-model-based dehazing network and the reconstruction loss into an end-to-end joint learning framework. Extensive experiments on the public image dehazing datasets (RESIDE-Indoor, RESIDE-Outdoor and the TrainA-TestA) illustrate the effectiveness of the modules in the proposed method and show that our method outperforms the state-of-the-art dehazing methods both quantitatively and qualitatively. The code is released in https://github.com/YanLi-LY/Dehazing-DRN.",0
"In recent years, there has been an increasing interest in single image dehazing, which is a crucial aspect that affects the performance of numerous computer vision tasks. However, most existing dehazing techniques prioritize haze removal over detail recovery in the dehazed images. To address this issue, this paper proposes an independent Detail Recovery Network (DRN) that captures image details from the input image through a separate network and integrates them into a coarse dehazed image. The overall network comprises two independent networks, the DRN and the dehazing network, respectively. The DRN recovers dehazed image details through its local and global branches, which capture local and global detail information using the convolution layer and the Smooth Dilated Convolution (SDC), respectively. The detail feature map is fused into the coarse dehazed image to generate a dehazed image with rich image details. The proposed method incorporates the DRN, the physical-model-based dehazing network, and the reconstruction loss into an end-to-end joint learning framework. Extensive experiments on public image dehazing datasets (RESIDE-Indoor, RESIDE-Outdoor, and TrainA-TestA) demonstrate the effectiveness of the proposed method and its superiority over state-of-the-art dehazing methods in terms of both quantitative and qualitative performance. The code for the proposed method is available at https://github.com/YanLi-LY/Dehazing-DRN.",1
"Most autonomous vehicles (AVs) rely on LiDAR and RGB camera sensors for perception. Using these point cloud and image data, perception models based on deep neural nets (DNNs) have achieved state-of-the-art performance in 3D detection. The vulnerability of DNNs to adversarial attacks has been heavily investigated in the RGB image domain and more recently in the point cloud domain, but rarely in both domains simultaneously. Multi-modal perception systems used in AVs can be divided into two broad types: cascaded models which use each modality independently, and fusion models which learn from different modalities simultaneously. We propose a universal and physically realizable adversarial attack for each type, and study and contrast their respective vulnerabilities to attacks. We place a single adversarial object with specific shape and texture on top of a car with the objective of making this car evade detection. Evaluating on the popular KITTI benchmark, our adversarial object made the host vehicle escape detection by each model type more than 50% of the time. The dense RGB input contributed more to the success of the adversarial attacks on both cascaded and fusion models.",0
"AVs typically rely on LiDAR and RGB camera sensors to perceive their surroundings. Perception models based on deep neural nets have achieved impressive 3D detection performance using these sensors' point cloud and image data. While the vulnerability of DNNs to adversarial attacks has been studied in both the RGB image and point cloud domains, it is uncommon to investigate both simultaneously. Multi-modal perception systems in AVs can be categorized into cascaded models that use each modality independently and fusion models that learn from multiple modalities simultaneously. We propose a universal and physically realizable adversarial attack for each model type and compare their susceptibility to attacks. Our attack involves placing a specific shaped and textured object on top of a car to evade detection. Our experiments on the KITTI benchmark show that the adversarial object successfully helped the host vehicle evade detection by both cascaded and fusion models over 50% of the time, with the dense RGB input contributing more to the attack's success.",1
"Traffic state estimation (TSE) bifurcates into two categories, model-driven and data-driven (e.g., machine learning, ML), while each suffers from either deficient physics or small data. To mitigate these limitations, recent studies introduced a hybrid paradigm, physics-informed deep learning (PIDL), which contains both model-driven and data-driven components. This paper contributes an improved version, called physics-informed deep learning with a fundamental diagram learner (PIDL+FDL), which integrates ML terms into the model-driven component to learn a functional form of a fundamental diagram (FD), i.e., a mapping from traffic density to flow or velocity. The proposed PIDL+FDL has the advantages of performing the TSE learning, model parameter identification, and FD estimation simultaneously. We demonstrate the use of PIDL+FDL to solve popular first-order and second-order traffic flow models and reconstruct the FD relation as well as model parameters that are outside the FD terms. We then evaluate the PIDL+FDL-based TSE using the Next Generation SIMulation (NGSIM) dataset. The experimental results show the superiority of the PIDL+FDL in terms of improved estimation accuracy and data efficiency over advanced baseline TSE methods, and additionally, the capacity to properly learn the unknown underlying FD relation.",0
"There are two types of Traffic state estimation (TSE): model-driven and data-driven, both of which have limitations due to either deficient physics or small data. However, recent studies introduced a hybrid approach called physics-informed deep learning (PIDL) that combines both model-driven and data-driven components. This paper presents an enhanced version called physics-informed deep learning with a fundamental diagram learner (PIDL+FDL) that integrates machine learning (ML) terms into the model-driven component to learn a fundamental diagram (FD). The proposed PIDL+FDL can perform TSE learning, model parameter identification, and FD estimation simultaneously. The method is demonstrated on first-order and second-order traffic flow models to reconstruct the FD relation and model parameters. The PIDL+FDL-based TSE is evaluated using the Next Generation SIMulation (NGSIM) dataset, which shows improved estimation accuracy and data efficiency compared to advanced baseline TSE methods. Additionally, the PIDL+FDL is capable of learning the unknown underlying FD relation.",1
"In this work, we propose an algorithm performing short-term predictions of the flux of vehicles on a stretch of road, using past measurements of the flux. This algorithm is based on a physics-aware recurrent neural network. A discretization of a macroscopic traffic flow model (using the so-called Traffic Reaction Model) is embedded in the architecture of the network and yields flux predictions based on estimated and predicted space-time dependent traffic parameters. These parameters are themselves obtained using a succession of LSTM ans simple recurrent neural networks. Besides, on top of the predictions, the algorithm yields a smoothing of its inputs which is also physically-constrained by the macroscopic traffic flow model. The algorithm is tested on raw flux measurements obtained from loop detectors.",0
"Our work involves proposing an algorithm that can make short-term predictions about vehicle flux on a road segment by analyzing past measurements. To achieve this, we've developed a physics-aware recurrent neural network that utilizes a discretized macroscopic traffic flow model known as the Traffic Reaction Model. By incorporating this model into our network architecture, we can predict flux based on traffic parameters estimated and forecasted over space and time. These parameters are obtained using both LSTM and simple recurrent neural networks. Additionally, our algorithm also smooths out input data while adhering to the constraints of the macroscopic traffic flow model. We've validated our algorithm using raw flux measurements collected via loop detectors.",1
"In many fields of science and engineering, models with different fidelities are available. Physical experiments or detailed simulations that accurately capture the behavior of the system are regarded as high-fidelity models with low model uncertainty, however, they are expensive to run. On the other hand, simplified physical experiments or numerical models are seen as low-fidelity models that are cheaper to evaluate. Although low-fidelity models are often not suitable for direct use in reliability analysis due to their low accuracy, they can offer information about the trend of the high-fidelity model thus providing the opportunity to explore the design space at a low cost. This study presents a new approach called adaptive multi-fidelity Gaussian process for reliability analysis (AMGPRA). Contrary to selecting training points and information sources in two separate stages as done in state-of-the-art mfEGRA method, the proposed approach finds the optimal training point and information source simultaneously using the novel collective learning function (CLF). CLF is able to assess the global impact of a candidate training point from an information source and it accommodates any learning function that satisfies a certain profile. In this context, CLF provides a new direction for quantifying the impact of new training points and can be easily extended with new learning functions to adapt to different reliability problems. The performance of the proposed method is demonstrated by three mathematical examples and one engineering problem concerning the wind reliability of transmission towers. It is shown that the proposed method achieves similar or higher accuracy with reduced computational costs compared to state-of-the-art single and multi-fidelity methods. A key application of AMGPRA is high-fidelity fragility modeling using complex and costly physics-based computational models.",0
"Numerous fields of science and engineering offer models with varying degrees of accuracy. High-fidelity models that precisely capture system behavior through physical experiments or detailed simulations are expensive to run but have low model uncertainty. In contrast, low-fidelity models use simplified physical experiments or numerical models that are cheaper to evaluate but are not suitable for direct use in reliability analysis due to their low accuracy. However, they can provide information about the trend of the high-fidelity model, which allows for exploring the design space at a low cost. This study introduces a new approach called adaptive multi-fidelity Gaussian process for reliability analysis (AMGPRA), which simultaneously finds the optimal training point and information source using the collective learning function (CLF). CLF evaluates the global impact of a candidate training point from an information source and accommodates various learning functions, providing a new direction for quantifying the impact of new training points. The proposed method is demonstrated through mathematical examples and an engineering problem concerning wind reliability of transmission towers, achieving similar or higher accuracy with reduced computational costs compared to state-of-the-art single and multi-fidelity methods. A key application of AMGPRA is high-fidelity fragility modeling using complex and costly physics-based computational models.",1
"Human motion synthesis is an important problem with applications in graphics, gaming and simulation environments for robotics. Existing methods require accurate motion capture data for training, which is costly to obtain. Instead, we propose a framework for training generative models of physically plausible human motion directly from monocular RGB videos, which are much more widely available. At the core of our method is a novel optimization formulation that corrects imperfect image-based pose estimations by enforcing physics constraints and reasons about contacts in a differentiable way. This optimization yields corrected 3D poses and motions, as well as their corresponding contact forces. Results show that our physically-corrected motions significantly outperform prior work on pose estimation. We can then use these to train a generative model to synthesize future motion. We demonstrate both qualitatively and quantitatively significantly improved motion estimation, synthesis quality and physical plausibility achieved by our method on the large scale Human3.6m dataset \cite{h36m_pami} as compared to prior kinematic and physics-based methods. By enabling learning of motion synthesis from video, our method paves the way for large-scale, realistic and diverse motion synthesis.",0
"The synthesis of human motion is a significant issue that has numerous applications in the fields of graphics, gaming, and robotics simulation environments. To train existing methods, precise motion capture data is necessary, which can be expensive to obtain. However, we propose an alternative framework for generating models of realistic human motion directly from widely available monocular RGB videos. Our method employs a unique optimization formulation that uses physics constraints to correct image-based pose estimations and reason about contacts in a differentiable manner. Our optimization yields corrected 3D poses and motions, as well as the corresponding contact forces. Our physically-corrected motions outperform prior work on pose estimation, and we can use them to train a generative model for synthesizing future motion. Our method shows significant improvement in motion estimation, synthesis quality, and physical plausibility compared to prior kinematic and physics-based methods when tested on the Human3.6m dataset. By enabling learning of motion synthesis from video, our method opens up possibilities for large-scale, diverse, and realistic motion synthesis.",1
"A density matrix describes the statistical state of a quantum system. It is a powerful formalism to represent both the quantum and classical uncertainty of quantum systems and to express different statistical operations such as measurement, system combination and expectations as linear algebra operations. This paper explores how density matrices can be used as a building block to build machine learning models exploiting their ability to straightforwardly combine linear algebra and probability. One of the main results of the paper is to show that density matrices coupled with random Fourier features could approximate arbitrary probability distributions over $\mathbb{R}^n$. Based on this finding the paper builds different models for density estimation, classification and regression. These models are differentiable, so it is possible to integrate them with other differentiable components, such as deep learning architectures and to learn their parameters using gradient-based optimization. In addition, the paper presents optimization-less training strategies based on estimation and model averaging. The models are evaluated in benchmark tasks and the results are reported and discussed.",0
"The statistical state of a quantum system can be described by a density matrix, which is a useful formalism for representing both quantum and classical uncertainty. It can also be used to perform statistical operations, such as measurement and system combination, through linear algebra operations. This research explores how density matrices can be utilized to construct machine learning models that combine probability and linear algebra. The study demonstrates that density matrices, when combined with random Fourier features, can approximate any probability distribution over $\mathbb{R}^n$. As a result, the study developed various models for density estimation, classification, and regression, which are differentiable and can be integrated with other differentiable components. The models can also be trained using optimization techniques based on estimation and model averaging. The models were evaluated in benchmark tasks, and the results were reported and discussed.",1
"Reinforcement learning (RL) has shown impressive success in exploring high-dimensional environments to learn complex tasks, but can often exhibit unsafe behaviors and require extensive environment interaction when exploration is unconstrained. A promising strategy for learning in dynamically uncertain environments is requiring that the agent can robustly return to learned safe sets, where task success (and therefore safety) can be guaranteed. While this approach has been successful in low-dimensions, enforcing this constraint in environments with visual observations is exceedingly challenging. We present a novel continuous representation for safe sets by framing it as a binary classification problem in a learned latent space, which flexibly scales to image observations. We then present a new algorithm, Latent Space Safe Sets (LS3), which uses this representation for long-horizon tasks with sparse rewards. We evaluate LS3 on 4 domains, including a challenging sequential pushing task in simulation and a physical cable routing task. We find that LS3 can use prior task successes to restrict exploration and learn more efficiently than prior algorithms while satisfying constraints. See https://tinyurl.com/latent-ss for code and supplementary material.",0
"Reinforcement learning (RL) has been successful in learning complex tasks in high-dimensional environments. However, it can exhibit unsafe behaviors and require extensive environment interaction when exploration is unconstrained. To address this issue in uncertain environments, agents can be required to return to safe sets where task success can be guaranteed. This approach has been successful in low-dimensional environments, but it is difficult to enforce in visual observations. A novel continuous representation for safe sets is presented by framing it as a binary classification problem in a learned latent space that can scale to image observations. A new algorithm, Latent Space Safe Sets (LS3), is introduced, which uses this representation for long-horizon tasks with sparse rewards. LS3 can use prior task successes to restrict exploration and learn more efficiently while satisfying constraints. The performance of LS3 is evaluated on 4 domains, including a challenging sequential pushing task in simulation and a physical cable routing task. The code and supplementary material can be found at https://tinyurl.com/latent-ss.",1
"In this paper, we characterize the noise of stochastic gradients and analyze the noise-induced dynamics during training deep neural networks by gradient-based optimizers. Specifically, we firstly show that the stochastic gradient noise possesses finite variance, and therefore the classical Central Limit Theorem (CLT) applies; this indicates that the gradient noise is asymptotically Gaussian. Such an asymptotic result validates the wide-accepted assumption of Gaussian noise. We clarify that the recently observed phenomenon of heavy tails within gradient noise may not be intrinsic properties, but the consequence of insufficient mini-batch size; the gradient noise, which is a sum of limited i.i.d. random variables, has not reached the asymptotic regime of CLT, thus deviates from Gaussian. We quantitatively measure the goodness of Gaussian approximation of the noise, which supports our conclusion. Secondly, we analyze the noise-induced dynamics of stochastic gradient descent using the Langevin equation, granting for momentum hyperparameter in the optimizer with a physical interpretation. We then proceed to demonstrate the existence of the steady-state distribution of stochastic gradient descent and approximate the distribution at a small learning rate.",0
"This paper examines the noise of stochastic gradients and its effect on deep neural network training through gradient-based optimizers. Our analysis begins by demonstrating that the stochastic gradient noise has finite variance, leading to the application of the Central Limit Theorem and the validation of the widely accepted assumption of Gaussian noise. We also note that heavy tails observed in gradient noise may not be intrinsic properties but instead result from insufficient mini-batch size. Our analysis quantitatively measures the goodness of Gaussian approximation, supporting our conclusion. We then explore the noise-induced dynamics of stochastic gradient descent using the Langevin equation, with momentum hyperparameter allowing for a physical interpretation. Finally, we demonstrate the existence of a steady-state distribution for stochastic gradient descent and approximate the distribution at a small learning rate.",1
"The connection between visual input and tactile sensing is critical for object manipulation tasks such as grasping and pushing. In this work, we introduce the challenging task of estimating a set of tactile physical properties from visual information. We aim to build a model that learns the complex mapping between visual information and tactile physical properties. We construct a first of its kind image-tactile dataset with over 400 multiview image sequences and the corresponding tactile properties. A total of fifteen tactile physical properties across categories including friction, compliance, adhesion, texture, and thermal conductance are measured and then estimated by our models. We develop a cross-modal framework comprised of an adversarial objective and a novel visuo-tactile joint classification loss. Additionally, we develop a neural architecture search framework capable of selecting optimal combinations of viewing angles for estimating a given physical property.",0
"Object manipulation tasks like grasping and pushing rely heavily on the connection between visual input and tactile sensing. Our work introduces a challenging task that involves estimating a set of tactile physical properties from visual information. We aim to create a model that can learn the intricate mapping between visual data and tactile physical attributes. To achieve this, we have generated an image-tactile dataset with over 400 multiview image sequences and their corresponding tactile properties. Our model estimates 15 tactile physical properties, including friction, compliance, adhesion, texture, and thermal conductance. We have developed a cross-modal framework using an adversarial objective and a novel visuo-tactile joint classification loss. Moreover, we have created a neural architecture search framework capable of selecting the optimal viewing angles for estimating a given physical property.",1
"We present an automated method for finding hidden symmetries, defined as symmetries that become manifest only in a new coordinate system that must be discovered. Its core idea is to quantify asymmetry as violation of certain partial differential equations, and to numerically minimize such violation over the space of all invertible transformations, parametrized as invertible neural networks. For example, our method rediscovers the famous Gullstrand-Painleve metric that manifests hidden translational symmetry in the Schwarzschild metric of non-rotating black holes, as well as Hamiltonicity, modularity and other simplifying traits not traditionally viewed as symmetries.",0
"An automated technique has been developed to identify hidden symmetries, which are symmetries that are only apparent in a newly discovered coordinate system. This method measures asymmetry by detecting partial differential equation violations and then minimizes this violation by searching through all invertible transformations. These transformations are represented by invertible neural networks. The technique can uncover hidden translational symmetry in the Schwarzschild metric of non-rotating black holes, as well as other traits that are not typically considered symmetries, such as Hamiltonicity and modularity. As an example, the famous Gullstrand-Painleve metric is rediscovered by this method.",1
"We introduce PowerGym, an open-source reinforcement learning environment for Volt-Var control in power distribution systems. Following OpenAI Gym APIs, PowerGym targets minimizing power loss and voltage violations under physical networked constraints. PowerGym provides four distribution systems (13Bus, 34Bus, 123Bus, and 8500Node) based on IEEE benchmark systems and design variants for various control difficulties. To foster generalization, PowerGym offers a detailed customization guide for users working with their distribution systems. As a demonstration, we examine state-of-the-art reinforcement learning algorithms in PowerGym and validate the environment by studying controller behaviors. The repository is available at \url{https://github.com/siemens/powergym}.",0
"PowerGym, a reinforcement learning environment for Volt-Var control in power distribution systems, is introduced in this paper. PowerGym aims to minimize power loss and voltage violations while adhering to physical networked constraints, using OpenAI Gym APIs. The environment includes four distribution systems (13Bus, 34Bus, 123Bus, and 8500Node), which are based on IEEE benchmark systems and have design variants that cater to various control difficulties. To promote generalization, PowerGym provides users with a detailed customization guide for their distribution systems. The paper demonstrates the use of state-of-the-art reinforcement learning algorithms in PowerGym and validates the environment by studying controller behaviors. The repository for PowerGym is available at \url{https://github.com/siemens/powergym}.",1
"Physics-informed neural networks (PINNs) have become a popular choice for solving high-dimensional partial differential equations (PDEs) due to their excellent approximation power and generalization ability. Recently, Extended PINNs (XPINNs) based on domain decomposition methods have attracted considerable attention due to their effectiveness in modeling multiscale and multiphysics problems and their parallelization. However, theoretical understanding on their convergence and generalization properties remains unexplored. In this study, we take an initial step towards understanding how and when XPINNs outperform PINNs. Specifically, for general multi-layer PINNs and XPINNs, we first provide a prior generalization bound via the complexity of the target functions in the PDE problem, and a posterior generalization bound via the posterior matrix norms of the networks after optimization. Moreover, based on our bounds, we analyze the conditions under which XPINNs improve generalization. Concretely, our theory shows that the key building block of XPINN, namely the domain decomposition, introduces a tradeoff for generalization. On the one hand, XPINNs decompose the complex PDE solution into several simple parts, which decreases the complexity needed to learn each part and boosts generalization. On the other hand, decomposition leads to less training data being available in each subdomain, and hence such model is typically prone to overfitting and may become less generalizable. Empirically, we choose five PDEs to show when XPINNs perform better than, similar to, or worse than PINNs, hence demonstrating and justifying our new theory.",0
"Due to their remarkable approximation power and generalization ability, Physics-informed neural networks (PINNs) have gained popularity for solving high-dimensional partial differential equations (PDEs). Recently, Extended PINNs (XPINNs) have emerged as a promising approach for modeling multiscale and multiphysics problems and for parallelization, using domain decomposition methods. However, despite their effectiveness, further research is needed to understand the convergence and generalization properties of XPINNs. This study aims to explore how and when XPINNs outperform PINNs by analyzing the generalization bounds of both models. We provide a prior generalization bound based on the complexity of the target functions in the PDE problem and a posterior generalization bound using the posterior matrix norms of the networks after optimization. Furthermore, we examine the conditions under which XPINNs improve generalization, showing that domain decomposition presents a tradeoff between boosting generalization by decomposing complex PDE solutions into simpler parts and reducing generalizability by decreasing the amount of training data available in each subdomain. We demonstrate and justify our new theory empirically by comparing the performance of PINNs and XPINNs on five PDEs.",1
"Unobtrusive and smart recognition of human activities using smartphones inertial sensors is an interesting topic in the field of artificial intelligence acquired tremendous popularity among researchers, especially in recent years. A considerable challenge that needs more attention is the real-time detection of physical activities, since for many real-world applications such as health monitoring and elderly care, it is required to recognize users' activities immediately to prevent severe damages to individuals' wellness. In this paper, we propose a human activity recognition (HAR) approach for the online prediction of physical movements, benefiting from the capabilities of incremental learning algorithms. We develop a HAR system containing monitoring software and a mobile application that collects accelerometer and gyroscope data and send them to a remote server via the Internet for classification and recognition operations. Six incremental learning algorithms are employed and evaluated in this work and compared with several batch learning algorithms commonly used for developing offline HAR systems. The Final results indicated that considering all performance evaluation metrics, Incremental K-Nearest Neighbors and Incremental Naive Bayesian outperformed other algorithms, exceeding a recognition accuracy of 95% in real-time.",0
"The use of smartphones' inertial sensors for unobtrusive and intelligent recognition of human activities has become a popular topic in the field of artificial intelligence. However, the real-time detection of physical activities remains a significant challenge, especially for applications such as health monitoring and elderly care, where immediate recognition is crucial to prevent harm. To address this issue, we propose an approach for human activity recognition (HAR) that utilizes incremental learning algorithms to predict physical movements online. Our HAR system comprises a monitoring software and a mobile app that collect accelerometer and gyroscope data and transmit them to a remote server for classification and recognition. We evaluate six incremental learning algorithms and compare them with batch learning algorithms typically used for developing offline HAR systems. Our results show that Incremental K-Nearest Neighbors and Incremental Naive Bayesian outperform other algorithms, achieving a real-time recognition accuracy of over 95%.",1
"A physics-informed neural network (PINN) uses physics-augmented loss functions, e.g., incorporating the residual term from governing differential equations, to ensure its output is consistent with fundamental physics laws. However, it turns out to be difficult to train an accurate PINN model for many problems in practice. In this paper, we address this issue through a novel perspective on the merits of learning in sinusoidal spaces with PINNs. By analyzing asymptotic behavior at model initialization, we first prove that a PINN of increasing size (i.e., width and depth) induces a bias towards flat outputs. Notably, a flat function is a trivial solution to many physics differential equations, hence, deceptively minimizing the residual term of the augmented loss while being far from the true solution. We then show that the sinusoidal mapping of inputs, in an architecture we label as sf-PINN, is able to elevate output variability, thus avoiding being trapped in the deceptive local minimum. In addition, the level of variability can be effectively modulated to match high-frequency patterns in the problem at hand. A key facet of this paper is the comprehensive empirical study that demonstrates the efficacy of learning in sinusoidal spaces with PINNs for a wide range of forward and inverse modelling problems spanning multiple physics domains.",0
"The use of a physics-informed neural network (PINN) involves incorporating physics-based loss functions to ensure adherence to fundamental laws. However, training an accurate PINN model is challenging for many problems. This paper proposes a novel approach that focuses on learning in sinusoidal spaces with PINNs. The study finds that increasing PINN size biases towards flat outputs, which can be a trivial solution to many physics differential equations and far from the true solution. To address this issue, the study introduces an architecture labeled sf-PINN that uses sinusoidal mapping of inputs to increase output variability and avoid being trapped in the deceptive local minimum. The study conducts a comprehensive empirical analysis that demonstrates the effectiveness of sf-PINN in solving various forward and inverse modelling problems across multiple physics domains.",1
"Face recognition has been greatly facilitated by the development of deep neural networks (DNNs) and has been widely applied to many safety-critical applications. However, recent studies have shown that DNNs are very vulnerable to adversarial examples, raising serious concerns on the security of real-world face recognition. In this work, we study sticker-based physical attacks on face recognition for better understanding its adversarial robustness. To this end, we first analyze in-depth the complicated physical-world conditions confronted by attacking face recognition, including the different variations of stickers, faces, and environmental conditions. Then, we propose a novel robust physical attack framework, dubbed PadvFace, to model these challenging variations specifically. Furthermore, considering the difference in attack complexity, we propose an efficient Curriculum Adversarial Attack (CAA) algorithm that gradually adapts adversarial stickers to environmental variations from easy to complex. Finally, we construct a standardized testing protocol to facilitate the fair evaluation of physical attacks on face recognition, and extensive experiments on both dodging and impersonation attacks demonstrate the superior performance of the proposed method.",0
"The advancement of deep neural networks (DNNs) has significantly improved face recognition, which has been extensively used in various safety-critical applications. Nonetheless, recent research has revealed that DNNs are highly susceptible to adversarial examples, which raises concerns about the security of real-world face recognition. In this study, we investigate the use of physical attacks with stickers on face recognition to better comprehend its adversarial robustness. We delve into the complicated physical-world circumstances involved in attacking face recognition, covering various types of stickers, faces, and environmental conditions. We then introduce a novel framework for robust physical attacks called PadvFace, which models these challenging variations. Furthermore, we propose an efficient Curriculum Adversarial Attack (CAA) algorithm that gradually adapts adversarial stickers to environmental variations from simple to complex. Finally, we establish a standardized testing protocol to facilitate a fair evaluation of physical attacks on face recognition. Our extensive experiments on both dodging and impersonation attacks demonstrate the superior performance of the proposed method.",1
"Multi-task learning ideally allows robots to acquire a diverse repertoire of useful skills. However, many multi-task reinforcement learning efforts assume the robot can collect data from all tasks at all times. In reality, the tasks that the robot learns arrive sequentially, depending on the user and the robot's current environment. In this work, we study a practical sequential multi-task RL problem that is motivated by the practical constraints of physical robotic systems, and derive an approach that effectively leverages the data and policies learned for previous tasks to cumulatively grow the robot's skill-set. In a series of simulated robotic manipulation experiments, our approach requires less than half the samples than learning each task from scratch, while avoiding impractical round-robin data collection. On a Franka Emika Panda robot arm, our approach incrementally learns ten challenging tasks, including bottle capping and block insertion.",0
"The goal of multi-task learning is to equip robots with a variety of useful skills. However, most multi-task reinforcement learning methods assume that robots can gather data from all tasks simultaneously, which is not the case in real-world scenarios where tasks are learned sequentially based on environmental and user factors. This study focuses on a practical sequential multi-task reinforcement learning problem, which addresses the physical constraints of robotic systems. Our approach effectively utilizes data and policies learned from previous tasks to expand the robot's skill set. Using simulated robotic manipulation experiments, we demonstrate that our approach requires fewer samples than learning each task from scratch and avoids impractical round-robin data collection. Furthermore, we show that our method can incrementally learn ten challenging tasks, such as bottle capping and block insertion, on a Franka Emika Panda robot arm.",1
"The prediction of future climate scenarios under anthropogenic forcing is critical to understand climate change and to assess the impact of potentially counter-acting technologies. Machine learning and hybrid techniques for this prediction rely on informative metrics that are sensitive to pertinent but often subtle influences. For atmospheric dynamics, a critical part of the climate system, the ""eyeball metric"", i.e. a visual inspection by an expert, is currently still the gold standard. However, it cannot be used as metric in machine learning systems where an algorithmic description is required. Motivated by the success of intermediate neural network activations as basis for learned metrics, e.g. in computer vision, we present a novel, self-supervised representation learning approach specifically designed for atmospheric dynamics. Our approach, called AtmoDist, trains a neural network on a simple, auxiliary task: predicting the temporal distance between elements of a shuffled sequence of atmospheric fields (e.g. the components of the wind field from a reanalysis or simulation). The task forces the network to learn important intrinsic aspects of the data as activations in its layers and from these hence a discriminative metric can be obtained. We demonstrate this by using AtmoDist to define a metric for GAN-based super resolution of vorticity and divergence. Our upscaled data matches closely the true statistics of a high resolution reference and it significantly outperform the state-of-the-art based on mean squared error. Since AtmoDist is unsupervised, only requires a temporal sequence of fields, and uses a simple auxiliary task, it can be used in a wide range of applications that aim to understand and mitigate climate change.",0
"Understanding climate change and evaluating the effectiveness of counteracting technologies necessitates accurate predictions of future climate scenarios under anthropogenic influence. Machine learning and hybrid techniques rely on informative metrics that are sensitive to subtle yet significant influences, particularly in atmospheric dynamics, a critical component of the climate system. Currently, the ""eyeball metric,"" or visual inspection, is the gold standard for atmospheric dynamics, but it cannot be utilized in machine learning systems that require an algorithmic description. To address this challenge, we introduce a novel self-supervised representation learning approach called AtmoDist, which trains a neural network to predict the temporal distance between elements of a shuffled sequence of atmospheric fields. This task forces the network to learn intrinsic aspects of the data, enabling the creation of a discriminative metric. AtmoDist's unsupervised nature, reliance on a temporal sequence of fields, and use of a simple auxiliary task make it applicable to a wide range of applications aimed at understanding and mitigating climate change. We demonstrate the effectiveness of AtmoDist in defining a metric for GAN-based super-resolution of vorticity and divergence, which significantly outperforms the state-of-the-art based on mean squared error while closely matching the true statistics of a high-resolution reference.",1
"The success of deep learning in many real-world tasks has triggered an intense effort to understand the power and limitations of deep learning in the training and generalization of complex tasks, so far with limited progress. In this work, we study the statistical mechanics of learning in Deep Linear Neural Networks (DLNNs) in which the input-output function of an individual unit is linear. Despite the linearity of the units, learning in DLNNs is nonlinear, hence studying its properties reveals some of the features of nonlinear Deep Neural Networks (DNNs). Importantly, we solve exactly the network properties following supervised learning using an equilibrium Gibbs distribution in the weight space. To do this, we introduce the Back-Propagating Kernel Renormalization (BPKR), which allows for the incremental integration of the network weights starting from the network output layer and progressing backward until the first layer's weights are integrated out. This procedure allows us to evaluate important network properties, such as its generalization error, the role of network width and depth, the impact of the size of the training set, and the effects of weight regularization and learning stochasticity. BPKR does not assume specific statistics of the input or the task's output. Furthermore, by performing partial integration of the layers, the BPKR allows us to compute the properties of the neural representations across the different hidden layers. We have proposed an extension of the BPKR to nonlinear DNNs with ReLU. Surprisingly, our numerical simulations reveal that despite the nonlinearity, the predictions of our theory are largely shared by ReLU networks in a wide regime of parameters. Our work is the first exact statistical mechanical study of learning in a family of DNNs, and the first successful theory of learning through successive integration of DoFs in the learned weight space.",0
"The effectiveness of deep learning in various real-world tasks has prompted significant efforts to comprehend its capabilities and limitations in training and generalizing complex tasks, but with limited progress so far. This study explores the statistical mechanics of Deep Linear Neural Networks (DLNNs), where individual unit input-output functions are linear. Despite the units' linearity, DLNNs' learning is nonlinear, revealing features of nonlinear Deep Neural Networks (DNNs). The research solves the network properties exactly through supervised learning using an equilibrium Gibbs distribution in the weight space, introducing Back-Propagating Kernel Renormalization (BPKR) to incrementally integrate network weights from the output layer to the first layer. This approach evaluates network properties such as generalization error, network width and depth, training set size, weight regularization, and learning stochasticity, without assuming specific input or output task statistics. BPKR also calculates neural representations' properties across different hidden layers and extends to nonlinear DNNs with ReLU. Surprisingly, the numerical simulations show that despite nonlinearity, the theory's predictions are largely applicable to ReLU networks in a broad parameter regime. This study is the first exact statistical mechanical research on learning in DNNs and the first successful theory of learning by integrating Degrees of Freedom (DoFs) in the learned weight space.",1
"Molecular representation learning plays an essential role in cheminformatics. Recently, language model-based approaches have been popular as an alternative to traditional expert-designed features to encode molecules. However, these approaches only utilize a single modality for representing molecules. Driven by the fact that a given molecule can be described through different modalities such as Simplified Molecular Line Entry System (SMILES), The International Union of Pure and Applied Chemistry (IUPAC), and The IUPAC International Chemical Identifier (InChI), we propose a multimodal molecular embedding generation approach called MM-Deacon (multimodal molecular domain embedding analysis via contrastive learning). MM-Deacon is trained using SMILES and IUPAC molecule representations as two different modalities. First, SMILES and IUPAC strings are encoded by using two different transformer-based language models independently, then the contrastive loss is utilized to bring these encoded representations from different modalities closer to each other if they belong to the same molecule, and to push embeddings farther from each other if they belong to different molecules. We evaluate the robustness of our molecule embeddings on molecule clustering, cross-modal molecule search, drug similarity assessment and drug-drug interaction tasks.",0
"In cheminformatics, molecular representation learning is crucial. Traditional expert-designed features for encoding molecules have been replaced by language model-based approaches, which are gaining popularity. However, these approaches only use a single modality to represent molecules. It is important to note that a molecule can be described in various ways, such as SMILES, IUPAC, and InChI. To address this, we propose a multimodal molecular embedding generation approach named MM-Deacon, which uses SMILES and IUPAC molecule representations as two different modalities. We encode the SMILES and IUPAC strings using two transformer-based language models independently, and then use a contrastive loss to bring the encoded representations from different modalities closer to each other if they belong to the same molecule, and to push embeddings farther from each other if they belong to different molecules. We evaluate the robustness of our molecule embeddings on various tasks such as molecule clustering, cross-modal molecule search, drug similarity assessment, and drug-drug interaction.",1
"We propose a new binary classification model called Phase Separation Binary Classifier (PSBC). It consists of a discretization of a nonlinear reaction-diffusion equation coupled with an Ordinary Differential Equation, and is inspired by fluids behavior, namely, on how binary fluids phase separate. Thus, parameters and hyperparameters have physical meaning, whose effects are studied in several different scenarios.   PSBC's equations can be seen as a dynamical system whose coefficients are trainable weights, with a similar architecture to that of a Recurrent Neural Network. As such, forward propagation amounts to an initial value problem. Boundary conditions are also present, bearing similarity with figure padding techniques in Computer Vision. Model compression is exploited in several ways, with weight sharing taking place both across and within layers.   The model is tested on pairs of digits of the classical MNIST database. An associated multiclass classifier is also constructed using a combination of Ensemble Learning and one versus one techniques. It is also shown how the PSBC can be combined with other methods - like aggregation and PCA - in order to construct better binary classifiers. The role of boundary conditions and viscosity is thoroughly studied in the case of digits ``0'' and ``1''.",0
"Our proposed model, the Phase Separation Binary Classifier (PSBC), is a novel approach to binary classification. It is based on the behavior of fluids, specifically how binary fluids phase separate, and is created by discretizing a nonlinear reaction-diffusion equation coupled with an Ordinary Differential Equation. The model's parameters and hyperparameters are assigned physical meaning, and their effects are analyzed in various scenarios. The PSBC can be viewed as a trainable weight dynamical system, similar to a Recurrent Neural Network, with boundary conditions resembling figure padding techniques in Computer Vision. Model compression is achieved through weight sharing across and within layers. We test the PSBC on pairs of digits from the MNIST database and construct a multiclass classifier using Ensemble Learning and one versus one techniques. Our study also shows how the PSBC can be combined with other methods like aggregation and PCA to improve binary classifiers. We extensively examine the impact of boundary conditions and viscosity on the classification of digits “0” and “1”.",1
"Future manufacturing requires complex systems that connect simulation platforms and virtualization with physical data from industrial processes. Digital twins incorporate a physical twin, a digital twin, and the connection between the two. Benefits of using digital twins, especially in manufacturing, are abundant as they can increase efficiency across an entire manufacturing life-cycle. The digital twin concept has become increasingly sophisticated and capable over time, enabled by rises in many technologies. In this paper, we detail the cognitive digital twin as the next stage of advancement of a digital twin that will help realize the vision of Industry 4.0. Cognitive digital twins will allow enterprises to creatively, effectively, and efficiently exploit implicit knowledge drawn from the experience of existing manufacturing systems. They also enable more autonomous decisions and control, while improving the performance across the enterprise (at scale). This paper presents graph learning as one potential pathway towards enabling cognitive functionalities in manufacturing digital twins. A novel approach to realize cognitive digital twins in the product design stage of manufacturing that utilizes graph learning is presented.",0
"To meet the demands of future manufacturing, intricate systems that link simulation platforms and virtualization with physical data from industrial processes are necessary. Digital twins comprise a physical and digital twin along with the connection between the two. The advantages of employing digital twins, particularly in manufacturing, are plentiful as they can enhance the efficiency of the entire manufacturing life-cycle. The digital twin notion has grown more sophisticated and capable over time, thanks to the development of numerous technologies. This study focuses on the cognitive digital twin as the next step in the evolution of a digital twin that will aid in achieving Industry 4.0's vision. Cognitive digital twins will enable businesses to creatively, effectively, and efficiently leverage implicit knowledge obtained from existing manufacturing systems' experiences. They also enable more autonomous decision-making and control, while improving performance across the enterprise (at scale). This research introduces graph learning as a possible approach to achieve cognitive functionalities in manufacturing digital twins. A new technique that employs graph learning to realize cognitive digital twins in the product design phase of manufacturing is presented.",1
"Real-time simulation of elastic structures is essential in many applications, from computer-guided surgical interventions to interactive design in mechanical engineering. The Finite Element Method is often used as the numerical method of reference for solving the partial differential equations associated with these problems. Yet, deep learning methods have recently shown that they could represent an alternative strategy to solve physics-based problems 1,2,3. In this paper, we propose a solution to simulate hyper-elastic materials using a data-driven approach, where a neural network is trained to learn the non-linear relationship between boundary conditions and the resulting displacement field. We also introduce a method to guarantee the validity of the solution. In total, we present three contributions: an optimized data set generation algorithm based on modal analysis, a physics-informed loss function, and a Hybrid Newton-Raphson algorithm. The method is applied to two benchmarks: a cantilever beam and a propeller. The results show that our network architecture trained with a limited amount of data can predict the displacement field in less than a millisecond. The predictions on various geometries, topologies, mesh resolutions, and boundary conditions are accurate to a few micrometers for non-linear deformations of several centimeters of amplitude.",0
"The real-time simulation of flexible structures is crucial in various fields, such as computer-assisted surgeries and mechanical engineering design. The Finite Element Method is commonly used to solve the partial differential equations related to these problems. However, recent developments in deep learning have suggested that this method could be an alternative approach to solving physics-based problems. This paper proposes a data-driven solution for simulating hyper-elastic materials using a neural network to learn the non-linear relationship between boundary conditions and displacement field. The proposed method includes an optimized data set generation algorithm, a physics-informed loss function, and a Hybrid Newton-Raphson algorithm to ensure the validity of the solution. The study involves two benchmarks, a cantilever beam, and a propeller, and the results demonstrate that the network architecture can predict the displacement field in less than a millisecond, accurately simulating non-linear deformations of several centimeters of amplitude with a few micrometers' precision on various geometries, topologies, mesh resolutions, and boundary conditions, even with limited data.",1
"It is essential to predict future trajectories of various agents in complex scenes. Whether it is internal personality factors of agents, interactive behavior of the neighborhood, or the influence of surroundings, it will have an impact on their future plannings. It means that even for the same physical type of agents, there are huge differences in their behavior styles. We concentrate on the problem of modeling agents' multi-style characteristics when predicting their trajectories. We propose the Multi-Style Network (MSN) to focus on this problem by dividing agents' behaviors into several hidden behavior categories adaptively, and then train each category's prediction network jointly, thus giving agents multiple styles of predictions simultaneously. Experiments show that MSN outperforms current state-of-the-art methods with 10\% - 20\% performance improvement on two widely used datasets, and presents better multi-style characteristics in predictions.",0
"Predicting the future paths of various agents in complex environments is crucial. Factors such as an agent's internal personality, interaction with their surroundings, and environmental influence can all impact their future plans. This means that even agents of the same physical type can exhibit vastly different behaviors. Our focus is on modeling agents' multi-style characteristics when predicting their trajectories. To address this, we introduce the Multi-Style Network (MSN), which divides agents' behaviors into hidden categories and trains each one's prediction network jointly, resulting in multiple predictions styles for agents. Experiments show that our approach outperforms state-of-the-art methods by 10% to 20% on two widely-used datasets, and exhibits superior multi-style characteristics in predictions.",1
"Symbolic learning represents the most straightforward approach to interpretable modeling, but its applications have been hampered by a single structural design choice: the adoption of propositional logic as the underlying language. Recently, more-than-propositional symbolic learning methods have started to appear, in particular for time-dependent data. These methods exploit the expressive power of modal temporal logics in powerful learning algorithms, such as temporal decision trees, whose classification capabilities are comparable with the best non-symbolic ones, while producing models with explicit knowledge representation.   With the intent of following the same approach in the case of spatial data, in this paper we: i) present a theory of spatial decision tree learning; ii) describe a prototypical implementation of a spatial decision tree learning algorithm based, and strictly extending, the classical C4.5 algorithm; and iii) perform a series of experiments in which we compare the predicting power of spatial decision trees with that of classical propositional decision trees in several versions, for a multi-class image classification problem, on publicly available datasets. Our results are encouraging, showing clear improvements in the performances from the propositional to the spatial models, which in turn show higher levels of interpretability.",0
"The use of propositional logic in symbolic learning has limited its applicability, despite being the most straightforward approach to interpretable modeling. However, recent developments have introduced more-than-propositional symbolic learning methods, particularly for time-dependent data, utilizing modal temporal logics in powerful algorithms like temporal decision trees. These methods produce models with explicit knowledge representation and classification capabilities comparable to the best non-symbolic ones. This paper aims to apply the same approach to spatial data. It presents a theory of spatial decision tree learning, describes a prototypical implementation of a spatial decision tree learning algorithm based on the classical C4.5 algorithm, and performs experiments to compare the predictive power of spatial decision trees with classical propositional decision trees for a multi-class image classification problem using publicly available datasets. The results show clear improvements in performance and interpretability for spatial models compared to propositional models.",1
"We present the Habitat-Matterport 3D (HM3D) dataset. HM3D is a large-scale dataset of 1,000 building-scale 3D reconstructions from a diverse set of real-world locations. Each scene in the dataset consists of a textured 3D mesh reconstruction of interiors such as multi-floor residences, stores, and other private indoor spaces.   HM3D surpasses existing datasets available for academic research in terms of physical scale, completeness of the reconstruction, and visual fidelity. HM3D contains 112.5k m^2 of navigable space, which is 1.4 - 3.7x larger than other building-scale datasets such as MP3D and Gibson. When compared to existing photorealistic 3D datasets such as Replica, MP3D, Gibson, and ScanNet, images rendered from HM3D have 20 - 85% higher visual fidelity w.r.t. counterpart images captured with real cameras, and HM3D meshes have 34 - 91% fewer artifacts due to incomplete surface reconstruction.   The increased scale, fidelity, and diversity of HM3D directly impacts the performance of embodied AI agents trained using it. In fact, we find that HM3D is `pareto optimal' in the following sense -- agents trained to perform PointGoal navigation on HM3D achieve the highest performance regardless of whether they are evaluated on HM3D, Gibson, or MP3D. No similar claim can be made about training on other datasets. HM3D-trained PointNav agents achieve 100% performance on Gibson-test dataset, suggesting that it might be time to retire that episode dataset.",0
"The HM3D dataset is being introduced, which is a collection of 1,000 3D building reconstructions. These scenes are made up of textured 3D mesh recreations of various indoor locations like multi-floor homes, stores, and private spaces. Compared to other datasets available for academic research, HM3D is larger in physical scale, has more complete reconstructions, and has higher visual fidelity. The navigable space in HM3D is 1.4 to 3.7 times larger than other building-scale datasets like MP3D and Gibson. When compared to existing photorealistic 3D datasets, HM3D images have considerably higher visual fidelity and fewer incomplete surface reconstruction artifacts. The larger scale, fidelity, and diversity of HM3D have a direct impact on the performance of embodied AI agents trained on it. In fact, agents trained to perform PointGoal navigation on HM3D achieve the highest performance regardless of whether they are evaluated on HM3D, Gibson, or MP3D. HM3D-trained PointNav agents achieve 100% performance on Gibson-test dataset, which suggests that it may be time to retire that episode dataset.",1
"The laws of physics have been written in the language of dif-ferential equations for centuries. Neural Ordinary Differen-tial Equations (NODEs) are a new machine learning architecture which allows these differential equations to be learned from a dataset. These have been applied to classical dynamics simulations in the form of Lagrangian Neural Net-works (LNNs) and Second Order Neural Differential Equations (SONODEs). However, they either cannot represent the most general equations of motion or lack interpretability. In this paper, we propose Modular Neural ODEs, where each force component is learned with separate modules. We show how physical priors can be easily incorporated into these models. Through a number of experiments, we demonstrate these result in better performance, are more interpretable, and add flexibility due to their modularity.",0
"For centuries, the language of differential equations has been used to express the laws of physics. Recently, the machine learning architecture Neural Ordinary Differential Equations (NODEs) has been developed to learn these equations from data. However, the existing models, such as Lagrangian Neural Networks (LNNs) and Second Order Neural Differential Equations (SONODEs), have limitations in their ability to represent general equations of motion and provide interpretability. This paper introduces Modular Neural ODEs, which utilize separate modules to learn each force component and incorporate physical priors. The experiments demonstrate that these models perform better, are more interpretable, and offer greater flexibility due to their modularity.",1
"Sketching is a stochastic dimension reduction method that preserves geometric structures of data and has applications in high-dimensional regression, low rank approximation and graph sparsification. In this work, we show that sketching can be used to compress simulation data and still accurately estimate time autocorrelation and power spectral density. For a given compression ratio, the accuracy is much higher than using previously known methods. In addition to providing theoretical guarantees, we apply sketching to a molecular dynamics simulation of methanol and find that the estimate of spectral density is 90% accurate using only 10% of the data.",0
"The technique of sketching is a stochastic approach to reduce dimensions while retaining the geometrical properties of data. It is useful in various fields such as high-dimensional regression, low rank approximation and graph sparsification. The present study demonstrates that sketching can be employed to compress simulation data while still accurately estimating time autocorrelation and power spectral density. Comparatively, the accuracy obtained is better than that of previously known methods for a given compression ratio. The study also applies sketching to a molecular dynamics simulation of methanol and shows that with only 10% of the data, the estimate of spectral density is 90% accurate. Along with providing theoretical proofs, the study highlights the practical applications of sketching.",1
"Crowd counting, which is significantly important for estimating the number of people in safety-critical scenes, has been shown to be vulnerable to adversarial examples in the physical world (e.g., adversarial patches). Though harmful, adversarial examples are also valuable for assessing and better understanding model robustness. However, existing adversarial example generation methods in crowd counting scenarios lack strong transferability among different black-box models. Motivated by the fact that transferability is positively correlated to the model-invariant characteristics, this paper proposes the Perceptual Adversarial Patch (PAP) generation framework to learn the shared perceptual features between models by exploiting both the model scale perception and position perception. Specifically, PAP exploits differentiable interpolation and density attention to help learn the invariance between models during training, leading to better transferability. In addition, we surprisingly found that our adversarial patches could also be utilized to benefit the performance of vanilla models for alleviating several challenges including cross datasets and complex backgrounds. Extensive experiments under both digital and physical world scenarios demonstrate the effectiveness of our PAP.",0
"The estimation of crowd numbers is crucial in safety-critical situations, but it has been discovered that this process is susceptible to adversarial examples such as patches. Although these examples are harmful, they are useful for examining model robustness. However, current methods for generating adversarial examples in crowd counting are not easily transferable between different black-box models. This paper proposes the Perceptual Adversarial Patch (PAP) framework, which takes advantage of model scale and position perception to learn shared perceptual traits between models. Through differentiable interpolation and density attention, PAP can learn invariance between models during training, resulting in superior transferability. Surprisingly, we found that our adversarial patches can also improve the performance of vanilla models by overcoming challenges such as cross datasets and complex backgrounds. Our PAP was tested extensively in both digital and real-world scenarios, demonstrating its effectiveness.",1
"Predicting the future trajectories of pedestrians is a challenging problem that has a range of application, from crowd surveillance to autonomous driving. In literature, methods to approach pedestrian trajectory prediction have evolved, transitioning from physics-based models to data-driven models based on recurrent neural networks. In this work, we propose a new approach to pedestrian trajectory prediction, with the introduction of a novel 2D convolutional model. This new model outperforms recurrent models, and it achieves state-of-the-art results on the ETH and TrajNet datasets. We also present an effective system to represent pedestrian positions and powerful data augmentation techniques, such as the addition of Gaussian noise and the use of random rotations, which can be applied to any model. As an additional exploratory analysis, we present experimental results on the inclusion of occupancy methods to model social information, which empirically show that these methods are ineffective in capturing social interaction.",0
"The task of foreseeing the future paths of pedestrians is a complex issue that has various practical uses, ranging from observing crowds to self-driving cars. In previous research, there have been distinct methods employed to tackle pedestrian trajectory prediction, shifting from models founded on physics to data-driven models based on recurrent neural networks. In this study, we suggest a new method for predicting pedestrian trajectories by introducing a unique 2D convolutional model. This fresh model surpasses recurrent models and accomplishes the most advanced outcomes on the ETH and TrajNet datasets. Additionally, we present an efficient system for representing pedestrian positions and potent data augmentation techniques, such as the addition of Gaussian noise and the utilization of random rotations, which are suitable for any model. Furthermore, we conducted an exploratory analysis by showing experimental results on incorporating occupancy methods to model social information, which demonstrate that these methods are inadequate in capturing social interaction.",1
"The precise equivalence between discretized Euclidean field theories and a certain class of probabilistic graphical models, namely the mathematical framework of Markov random fields, opens up the opportunity to investigate machine learning from the perspective of quantum field theory. In this contribution we will demonstrate, through the Hammersley-Clifford theorem, that the $\phi^{4}$ scalar field theory on a square lattice satisfies the local Markov property and can therefore be recast as a Markov random field. We will then derive from the $\phi^{4}$ theory machine learning algorithms and neural networks which can be viewed as generalizations of conventional neural network architectures. Finally, we will conclude by presenting applications based on the minimization of an asymmetric distance between the probability distribution of the $\phi^{4}$ machine learning algorithms and target probability distributions.",0
"The relationship between discretized Euclidean field theories and a specific type of probabilistic graphical models, known as Markov random fields, allows for exploration of quantum field theory in the context of machine learning. This paper will use the Hammersley-Clifford theorem to prove that the $\phi^{4}$ scalar field theory on a square lattice satisfies the local Markov property and can thus be represented as a Markov random field. From this theory, the paper will derive machine learning algorithms and neural networks that are extensions of traditional neural network structures. Finally, the paper will present practical applications that involve minimizing the asymmetric distance between the probability distribution of $\phi^{4}$ machine learning algorithms and target probability distributions.",1
"Recent studies demonstrated the vulnerability of control policies learned through deep reinforcement learning against adversarial attacks, raising concerns about the application of such models to risk-sensitive tasks such as autonomous driving. Threat models for these demonstrations are limited to (1) targeted attacks through real-time manipulation of the agent's observation, and (2) untargeted attacks through manipulation of the physical environment. The former assumes full access to the agent's states/observations at all times, while the latter has no control over attack outcomes. This paper investigates the feasibility of targeted attacks through visually learned patterns placed on physical object in the environment, a threat model that combines the practicality and effectiveness of the existing ones. Through analysis, we demonstrate that a pre-trained policy can be hijacked within a time window, e.g., performing an unintended self-parking, when an adversarial object is present. To enable the attack, we adopt an assumption that the dynamics of both the environment and the agent can be learned by the attacker. Lastly, we empirically show the effectiveness of the proposed attack on different driving scenarios, perform a location robustness test, and study the tradeoff between the attack strength and its effectiveness.",0
"Control policies learned through deep reinforcement learning have been found to be vulnerable to adversarial attacks in recent studies. This has raised concerns about their suitability for risk-sensitive tasks like autonomous driving. The threat models for these attacks have been limited to targeted attacks through real-time manipulation of the agent's observation and untargeted attacks through manipulation of the physical environment. The former assumes full access to the agent's states/observations at all times, while the latter has no control over attack outcomes. This paper explores the feasibility of targeted attacks through visually learned patterns on physical objects in the environment. We demonstrate that a pre-trained policy can be hijacked within a time window when an adversarial object is present. Our attack assumes that the attacker can learn the dynamics of both the environment and the agent. We also show the effectiveness of the proposed attack on different driving scenarios, perform a location robustness test, and analyze the tradeoff between attack strength and effectiveness.",1
"We propose, to the best of our knowledge, the first online algorithm to compute the maximum-likelihood estimate in quantum state tomography. Suppose the quantum state to be estimated corresponds to a $D$-by-$D$ density matrix. The per-iteration computational complexity of the algorithm is $O ( D ^ 3 )$, independent of the data size. The expected optimization error of the algorithm is $O(\sqrt{ ( 1 / T ) D \log D })$, where $T$ denotes the number of iterations. The algorithm can be viewed as a quantum extension of Soft-Bayes, a recent algorithm for online portfolio selection (Orseau et al. Soft-Bayes: Prod for mixtures of experts with log-loss. Int. Conf. Algorithmic Learning Theory. 2017).",0
"We present the first-ever online algorithm to calculate the maximum-likelihood estimate in quantum state tomography. If the quantum state that requires estimation is represented by a density matrix of size $D$-by-$D$, our algorithm's computational complexity per iteration is $O(D^3)$, irrespective of the data's size. The algorithm's expected optimization error is $O(\sqrt{(1/T)D\log D})$, where $T$ stands for the number of iterations. This algorithm can be considered as a quantum variant of Soft-Bayes, a new algorithm for online portfolio selection that was introduced in Orseau et al.'s Soft-Bayes: Prod for mixtures of experts with log-loss presented at the International Conference on Algorithmic Learning Theory in 2017.",1
"Sensors are the key to sensing the environment and imparting benefits to smart cities in many aspects, such as providing real-time air quality information throughout an urban area. However, a prerequisite is to obtain fine-grained knowledge of the environment. There is a limit to how many sensors can be installed in the physical world due to non-negligible expenses. In this paper, we propose to infer real-time information of any given location in a city based on historical and current observations from the available sensors (termed spatiotemporal inference). Our approach decouples the modeling of short-term and long-term patterns, relying on two major components. Firstly, unlike previous studies that separated the spatial and temporal relation learning, we introduce a joint spatiotemporal graph attention network that learns the short-term dependencies across both the spatial and temporal dimensions. Secondly, we propose an adaptive graph recurrent network with a time skip for capturing long-term patterns. The adaptive adjacency matrices are learned inductively first as the inputs of a recurrent network to learn dynamic dependencies. Experimental results on four public read-world datasets show that our method reduces state-of-the-art baseline mean absolute errors by 5%~12%.",0
"The use of sensors is crucial for detecting and providing benefits to smart cities, such as real-time air quality information. However, it is important to have a detailed understanding of the environment before implementing these sensors. Installing numerous sensors can be costly, thus our paper proposes a method to obtain up-to-date information on any given location in a city by analyzing data from existing sensors (known as spatiotemporal inference). Our approach is based on two components: a joint spatiotemporal graph attention network that learns short-term correlations across both spatial and temporal dimensions, and an adaptive graph recurrent network that captures long-term patterns. Our experimental results on four public datasets demonstrate that our method significantly reduces mean absolute errors compared to current approaches, by between 5% and 12%.",1
"The first thermospheric neutral mass density model with robust and reliable uncertainty estimates is developed based on the SET HASDM density database. This database, created by Space Environment Technologies (SET), contains 20 years of outputs from the U.S. Space Force's High Accuracy Satellite Drag Model (HASDM), which represents the state-of-the-art for density and drag modeling. We utilize principal component analysis (PCA) for dimensionality reduction, creating the coefficients upon which nonlinear machine-learned (ML) regression models are trained. These models use three unique loss functions: mean square error (MSE), negative logarithm of predictive density (NLPD), and continuous ranked probability score (CRPS). Three input sets are also tested, showing improved performance when introducing time histories for geomagnetic indices. These models leverage Monte Carlo (MC) dropout to provide uncertainty estimates, and the use of the NLPD loss function results in well-calibrated uncertainty estimates without sacrificing model accuracy (<10% mean absolute error). By comparing the best HASDM-ML model to the HASDM database along satellite orbits, we found that the model provides robust and reliable uncertainties in the density space over all space weather conditions. A storm-time comparison shows that HASDM-ML also supplies meaningful uncertainty measurements during extreme events.",0
"A new model for thermospheric neutral mass density has been developed, which includes reliable uncertainty estimates. The model is based on the SET HASDM density database, which has been created by Space Environment Technologies and contains 20 years of outputs from the U.S. Space Force's High Accuracy Satellite Drag Model. The model uses principal component analysis to reduce dimensionality, and nonlinear machine-learned regression models are trained using three different loss functions. Three input sets are also tested, and the model leverages Monte Carlo dropout to provide uncertainty estimates. The model's accuracy is maintained (<10% mean absolute error) while providing well-calibrated uncertainty estimates through the use of the NLPD loss function. Comparing the best HASDM-ML model to the HASDM database, it has been found that the model provides reliable uncertainties in the density space over all space weather conditions, including during extreme events.",1
"The numerical simulations of physical systems are heavily dependent on mesh-based models. While neural networks have been extensively explored to assist such tasks, they often ignore the interactions or hierarchical relations between input features, and process them as concatenated mixtures. In this work, we generalize the idea of conditional parametrization -- using trainable functions of input parameters to generate the weights of a neural network, and extend them in a flexible way to encode information critical to the numerical simulations. Inspired by discretized numerical methods, choices of the parameters include physical quantities and mesh topology features. The functional relation between the modeled features and the parameters are built into the network architecture. The method is implemented on different networks, which are applied to several frontier scientific machine learning tasks, including the discovery of unmodeled physics, super-resolution of coarse fields, and the simulation of unsteady flows with chemical reactions. The results show that the conditionally parameterized networks provide superior performance compared to their traditional counterparts. A network architecture named CP-GNet is also proposed as the first deep learning model capable of standalone prediction of reacting flows on irregular meshes.",0
"Mesh-based models play a crucial role in numerical simulations of physical systems. Although neural networks have been explored to assist in such tasks, they tend to overlook interactions and hierarchical relationships between input features and treat them as concatenated mixtures. This study presents a novel approach to conditional parametrization, which involves using trainable functions of input parameters to generate the weights of a neural network. Additionally, the approach is extended to encode critical information required for numerical simulations in a flexible manner. The parameters include physical quantities and mesh topology features, and their functional relationship with the modeled features is incorporated into the network architecture. The method is applied to various frontier scientific machine learning tasks, such as discovering unmodeled physics, super-resolution of coarse fields, and simulating unsteady flows with chemical reactions. The results demonstrate that the conditionally parameterized networks outperform their traditional counterparts. The study also proposes a network architecture called CP-GNet that is capable of predicting reacting flows on irregular meshes, making it the first deep learning model to do so independently.",1
"Chemistry research has both high material and computational costs to conduct experiments. Institutions thus consider chemical data to be valuable and there have been few efforts to construct large public datasets for machine learning. Another challenge is that different intuitions are interested in different classes of molecules, creating heterogeneous data that cannot be easily joined by conventional distributed training. In this work, we introduce federated heterogeneous molecular learning to address these challenges. Federated learning allows end-users to build a global model collaboratively while preserving the training data distributed over isolated clients. Due to the lack of related research, we first simulate a federated heterogeneous benchmark called FedChem. FedChem is constructed by jointly performing scaffold splitting and Latent Dirichlet Allocation on existing datasets. Our results on FedChem show that significant learning challenges arise when working with heterogeneous molecules. We then propose a method to alleviate the problem, namely Federated Learning by Instance reweighTing (FLIT). FLIT can align the local training across heterogeneous clients by improving the performance for uncertain samples. Comprehensive experiments conducted on our new benchmark FedChem validate the advantages of this method over other federated learning schemes. FedChem should enable a new type of collaboration for improving AI in chemistry that mitigates concerns about valuable chemical data.",0
"Conducting chemistry research involves substantial material and computational expenses for experimental purposes. As a result, chemical data is highly valued by institutions, and there have been limited attempts to develop extensive public datasets for machine learning. Moreover, the distinct interests of various institutions in specific molecule categories result in a heterogeneous dataset that cannot be easily linked through conventional distributed training. To tackle these obstacles, we propose federated heterogeneous molecular learning, which allows users to collaboratively build a global model while preserving training data. To address the lack of research in this area, we designed a federated heterogeneous benchmark called FedChem, which combines existing datasets through scaffold splitting and Latent Dirichlet Allocation. Our work on FedChem reveals that heterogeneous molecules pose significant learning challenges. To alleviate this issue, we introduce Federated Learning by Instance reweighTing (FLIT), which aligns local training across heterogeneous clients by improving performance for uncertain samples. Through comprehensive experiments on FedChem, we demonstrate that FLIT outperforms other federated learning schemes, enabling a new type of collaboration that enhances AI in chemistry while addressing concerns about valuable chemical data.",1
"This survey gives an overview of Monte Carlo methodologies using surrogate models, for dealing with densities which are intractable, costly, and/or noisy. This type of problem can be found in numerous real-world scenarios, including stochastic optimization and reinforcement learning, where each evaluation of a density function may incur some computationally-expensive or even physical (real-world activity) cost, likely to give different results each time. The surrogate model does not incur this cost, but there are important trade-offs and considerations involved in the choice and design of such methodologies. We classify the different methodologies into three main classes and describe specific instances of algorithms under a unified notation. A modular scheme which encompasses the considered methods is also presented. A range of application scenarios is discussed, with special attention to the likelihood-free setting and reinforcement learning. Several numerical comparisons are also provided.",0
"The purpose of this survey is to provide an overview of how Monte Carlo methodologies can be used with surrogate models to address the challenges of dealing with intractable, costly, and/or noisy densities. Such difficulties are often encountered in stochastic optimization and reinforcement learning, where evaluating a density function can be computationally expensive or incur real-world costs that result in varying outcomes. Surrogate models can avoid these costs, but selecting and designing them requires careful consideration of trade-offs and other factors. We divide the methodologies into three classes and provide examples of algorithms using a unified notation. We also present a modular scheme that encompasses these methods and discuss various scenarios in which they may be applied, focusing on likelihood-free settings and reinforcement learning. Additionally, we include several numerical comparisons to demonstrate the effectiveness of these approaches.",1
"Physical adversarial attacks in object detection have attracted increasing attention. However, most previous works focus on hiding the objects from the detector by generating an individual adversarial patch, which only covers the planar part of the vehicle's surface and fails to attack the detector in physical scenarios for multi-view, long-distance and partially occluded objects. To bridge the gap between digital attacks and physical attacks, we exploit the full 3D vehicle surface to propose a robust Full-coverage Camouflage Attack (FCA) to fool detectors. Specifically, we first try rendering the non-planar camouflage texture over the full vehicle surface. To mimic the real-world environment conditions, we then introduce a transformation function to transfer the rendered camouflaged vehicle into a photo-realistic scenario. Finally, we design an efficient loss function to optimize the camouflage texture. Experiments show that the full-coverage camouflage attack can not only outperform state-of-the-art methods under various test cases but also generalize to different environments, vehicles, and object detectors.",0
"The focus on physical adversarial attacks in object detection has increased, but previous research mainly concentrated on hiding objects from the detector using an individual adversarial patch. This approach only covers a portion of the vehicle's surface and is ineffective for multi-view, long-distance, and partially occluded objects. To overcome this limitation, we propose a Full-coverage Camouflage Attack (FCA) that utilizes the entire 3D surface of the vehicle to deceive detectors. The FCA involves rendering a non-planar camouflage texture over the full surface of the vehicle, followed by a transformation function to create a photo-realistic scenario. We also develop an efficient loss function to optimize the camouflage texture. Our experiments demonstrate that the FCA is superior to state-of-the-art methods in a range of test cases and can apply to various environments, vehicles, and object detectors.",1
"Partial Differential Equations (PDEs) are notoriously difficult to solve. In general, closed-form solutions are not available and numerical approximation schemes are computationally expensive. In this paper, we propose to approach the solution of PDEs based on a novel technique that combines the advantages of two recently emerging machine learning based approaches. First, physics-informed neural networks (PINNs) learn continuous solutions of PDEs and can be trained with little to no ground truth data. However, PINNs do not generalize well to unseen domains. Second, convolutional neural networks provide fast inference and generalize but either require large amounts of training data or a physics-constrained loss based on finite differences that can lead to inaccuracies and discretization artifacts. We leverage the advantages of both of these approaches by using Hermite spline kernels in order to continuously interpolate a grid-based state representation that can be handled by a CNN. This allows for training without any precomputed training data using a physics-informed loss function only and provides fast, continuous solutions that generalize to unseen domains. We demonstrate the potential of our method at the examples of the incompressible Navier-Stokes equation and the damped wave equation. Our models are able to learn several intriguing phenomena such as Karman vortex streets, the Magnus effect, Doppler effect, interference patterns and wave reflections. Our quantitative assessment and an interactive real-time demo show that we are narrowing the gap in accuracy of unsupervised ML based methods to industrial CFD solvers while being orders of magnitude faster.",0
"PDEs are notoriously challenging to solve because closed-form solutions are generally not available and numerical approximation schemes are computationally expensive. In this paper, we propose a novel technique for solving PDEs that combines the advantages of two recently emerging machine learning-based approaches. PINNs can learn continuous solutions of PDEs with little to no ground truth data, but they do not generalize well to unseen domains. On the other hand, CNNs provide fast inference and generalize, but they either require large amounts of training data or a physics-constrained loss based on finite differences that can lead to inaccuracies and discretization artifacts. To overcome these limitations, we use Hermite spline kernels to continuously interpolate a grid-based state representation that can be handled by a CNN. This enables us to train without any precomputed training data using a physics-informed loss function only and provides fast, continuous solutions that generalize to unseen domains. We demonstrate the potential of our method on examples of the incompressible Navier-Stokes equation and the damped wave equation, where our models can learn a range of intriguing phenomena such as Karman vortex streets, the Magnus effect, Doppler effect, interference patterns, and wave reflections. Our quantitative assessment and interactive real-time demo show that our unsupervised ML-based method is narrowing the accuracy gap to industrial CFD solvers while being orders of magnitude faster.",1
"We study the understanding of embodied reference: One agent uses both language and gesture to refer to an object to another agent in a shared physical environment. Of note, this new visual task requires understanding multimodal cues with perspective-taking to identify which object is being referred to. To tackle this problem, we introduce YouRefIt, a new crowd-sourced dataset of embodied reference collected in various physical scenes; the dataset contains 4,195 unique reference clips in 432 indoor scenes. To the best of our knowledge, this is the first embodied reference dataset that allows us to study referring expressions in daily physical scenes to understand referential behavior, human communication, and human-robot interaction. We further devise two benchmarks for image-based and video-based embodied reference understanding. Comprehensive baselines and extensive experiments provide the very first result of machine perception on how the referring expressions and gestures affect the embodied reference understanding. Our results provide essential evidence that gestural cues are as critical as language cues in understanding the embodied reference.",0
"Our focus is on comprehending embodied reference, where one agent employs both language and gesture to indicate an object to another agent in a communal physical environment. To identify the object being referred to, this visual task requires an understanding of multimodal cues and perspective-taking. To address this issue, we have created YouRefIt, a crowd-sourced dataset of embodied reference gathered from various physical settings. This dataset comprises 4,195 distinct reference clips in 432 indoor scenes and is the first of its kind to allow us to examine referring expressions in everyday physical scenes, thereby better understanding referential behavior, human communication, and human-robot interaction. Additionally, we have developed two benchmarks for image-based and video-based embodied reference comprehension. Through extensive experiments and comprehensive baselines, we have obtained the initial machine perception results revealing how referring expressions and gestures impact embodied reference comprehension. Our findings suggest that gestural and language cues are equally important in comprehending embodied reference.",1
"In this paper, we propose a novel query design for the transformer-based detectors. In previous transformer-based detectors, the object queries are a set of learned embeddings. However, each learned embedding does not have an explicit physical meaning and we can not explain where it will focus on. It is difficult to optimize as the prediction slot of each object query does not have a specific mode. In other words, each object query will not focus on a specific region. To solved these problems, in our query design, object queries are based on anchor points, which are widely used in CNN-based detectors. So each object query focus on the objects near the anchor point. Moreover, our query design can predict multiple objects at one position to solve the difficulty: ""one region, multiple objects"". In addition, we design an attention variant, which can reduce the memory cost while achieving similar or better performance than the standard attention in DETR. Thanks to the query design and the attention variant, the proposed detector that we called Anchor DETR, can achieve better performance and run faster than the DETR with 10$\times$ fewer training epochs. For example, it achieves 44.2 AP with 16 FPS on the MSCOCO dataset when using the ResNet50-DC5 feature for training 50 epochs. Extensive experiments on the MSCOCO benchmark prove the effectiveness of the proposed methods. Code is available at https://github.com/megvii-model/AnchorDETR.",0
"This paper presents a new method for designing queries in transformer-based detectors. Previous detectors used learned embeddings for object queries, but these embeddings lacked a clear physical meaning and made optimization difficult since they did not focus on specific regions. To address these issues, our method uses anchor points, which allow each object query to focus on nearby objects and predict multiple objects at one position. We also introduce an attention variant that reduces memory cost while maintaining performance. Our proposed detector, called Anchor DETR, achieves better performance and runs faster than DETR with 10$\times$ fewer training epochs. For instance, it achieves 44.2 AP with 16 FPS on the MSCOCO dataset using the ResNet50-DC5 feature for 50 epochs. Extensive experiments on the MSCOCO benchmark demonstrate the effectiveness of our methods. The code is available at https://github.com/megvii-model/AnchorDETR.",1
"Existing equivariant neural networks for continuous groups require discretization or group representations. All these approaches require detailed knowledge of the group parametrization and cannot learn entirely new symmetries. We propose to work with the Lie algebra (infinitesimal generators) instead of the Lie group.Our model, the Lie algebra convolutional network (L-conv) can learn potential symmetries and does not require discretization of the group. We show that L-conv can serve as a building block to construct any group equivariant architecture. We discuss how CNNs and Graph Convolutional Networks are related to and can be expressed as L-conv with appropriate groups. We also derive the MSE loss for a single L-conv layer and find a deep relation with Lagrangians used in physics, with some of the physics aiding in defining generalization and symmetries in the loss landscape. Conversely, L-conv could be used to propose more general equivariant ans\""atze for scientific machine learning.",0
"Equivariant neural networks for continuous groups that currently exist require either discretization or group representations, both of which necessitate thorough knowledge of the group parametrization and do not allow for the learning of entirely new symmetries. Our proposal is to use the Lie algebra, specifically the infinitesimal generators, instead of the Lie group. Our model, called the Lie algebra convolutional network (L-conv), can learn potential symmetries without group discretization. Furthermore, L-conv can act as a fundamental element in constructing any group equivariant architecture. We explore the relationship between CNNs and Graph Convolutional Networks and L-conv with appropriate groups. We also derive the MSE loss for a single L-conv layer and find a deep relationship with Lagrangians used in physics, which helps define generalization and symmetries in the loss landscape. Conversely, L-conv could be used to propose more general equivariant ans\""atze for scientific machine learning.",1
"The measured spatiotemporal response of various physical processes is utilized to infer the governing partial differential equations (PDEs). We propose SimultaNeous Basis Function Approximation and Parameter Estimation (SNAPE), a technique of parameter estimation of PDEs that is robust against high levels of noise nearly 100 %, by simultaneously fitting basis functions to the measured response and estimating the parameters of both ordinary and partial differential equations. The domain knowledge of the general multidimensional process is used as a constraint in the formulation of the optimization framework. SNAPE not only demonstrates its applicability on various complex dynamic systems that encompass wide scientific domains including Schr\""odinger equation, chaotic duffing oscillator, and Navier-Stokes equation but also estimates an analytical approximation to the process response. The method systematically combines the knowledge of well-established scientific theories and the concepts of data science to infer the properties of the process from the observed data.",0
"To infer the governing partial differential equations (PDEs), the measured spatiotemporal response of different physical processes is utilized. In this study, we introduce SimultaNeous Basis Function Approximation and Parameter Estimation (SNAPE) as a technique for parameter estimation of PDEs. SNAPE is highly robust against high levels of noise, nearly 100%, by fitting basis functions to the measured response and estimating the parameters of both ordinary and partial differential equations simultaneously. The optimization framework formulation incorporates domain knowledge of the general multidimensional process as a constraint. SNAPE not only demonstrates its applicability on various complex dynamic systems encompassing a wide range of scientific domains, such as Schr\""odinger equation, chaotic duffing oscillator, and Navier-Stokes equation, but also estimates an analytical approximation to the process response. The method combines the knowledge of well-established scientific theories and data science concepts systematically to infer the properties of the process from the observed data.",1
"In a multirobot system, a number of cyber-physical attacks (e.g., communication hijack, observation perturbations) can challenge the robustness of agents. This robustness issue worsens in multiagent reinforcement learning because there exists the non-stationarity of the environment caused by simultaneously learning agents whose changing policies affect the transition and reward functions. In this paper, we propose a minimax MARL approach to infer the worst-case policy update of other agents. As the minimax formulation is computationally intractable to solve, we apply the convex relaxation of neural networks to solve the inner minimization problem. Such convex relaxation enables robustness in interacting with peer agents that may have significantly different behaviors and also achieves a certified bound of the original optimization problem. We evaluate our approach on multiple mixed cooperative-competitive tasks and show that our method outperforms the previous state of the art approaches on this topic.",0
"The robustness of agents in a multirobot system can be challenged by various cyber-physical attacks, such as communication hijack and observation perturbations. This issue becomes more severe in multiagent reinforcement learning due to the non-stationarity of the environment, caused by the changing policies of agents that affect the transition and reward functions. To address this problem, we propose a minimax MARL approach that infers the worst-case policy update of other agents. However, the minimax formulation is difficult to solve, so we use the convex relaxation of neural networks to solve the inner minimization problem. This convex relaxation enables robustness in interacting with peer agents with significantly different behaviors and provides a certified bound of the original optimization problem. We evaluate our approach on several mixed cooperative-competitive tasks and show that it outperforms previous state-of-the-art approaches in this area.",1
"Differentiable physics provides a new approach for modeling and understanding the physical systems by pairing the new technology of differentiable programming with classical numerical methods for physical simulation. We survey the rapidly growing literature of differentiable physics techniques and highlight methods for parameter estimation, learning representations, solving differential equations, and developing what we call scientific foundation models using data and inductive priors. We argue that differentiable physics offers a new paradigm for modeling physical phenomena by combining classical analytic solutions with numerical methodology using the bridge of differentiable programming.",0
"The utilization of differentiable programming with classical numerical methods for physical simulation is known as differentiable physics. This novel approach enables modeling and comprehending physical systems. In this review, we discuss the emerging literature on differentiable physics techniques, which includes methods for parameter estimation, representation learning, differential equation solving, and scientific foundation model development via data and inductive priors. By combining classical analytic solutions with numerical methodology using differentiable programming, differentiable physics presents a new paradigm for modeling physical phenomena.",1
"Emotional expressions are the behaviors that communicate our emotional state or attitude to others. They are expressed through verbal and non-verbal communication. Complex human behavior can be understood by studying physical features from multiple modalities; mainly facial, vocal and physical gestures. Recently, spontaneous multi-modal emotion recognition has been extensively studied for human behavior analysis. In this paper, we propose a new deep learning-based approach for audio-visual emotion recognition. Our approach leverages recent advances in deep learning like knowledge distillation and high-performing deep architectures. The deep feature representations of the audio and visual modalities are fused based on a model-level fusion strategy. A recurrent neural network is then used to capture the temporal dynamics. Our proposed approach substantially outperforms state-of-the-art approaches in predicting valence on the RECOLA dataset. Moreover, our proposed visual facial expression feature extraction network outperforms state-of-the-art results on the AffectNet and Google Facial Expression Comparison datasets.",0
"The act of expressing emotions involves both verbal and non-verbal communication which allows us to convey our emotional state or attitude to others. To gain a better understanding of complex human behavior, it is important to study physical features from various modes such as facial, vocal and physical gestures. Recent studies have focused on spontaneous multi-modal emotion recognition for analyzing human behavior. This paper proposes a new audio-visual emotion recognition approach that utilizes deep learning techniques such as knowledge distillation and high-performing deep architectures. Our approach involves fusing deep feature representations from the audio and visual modalities through a model-level fusion strategy. We then use a recurrent neural network to capture the temporal dynamics. Our proposed approach surpasses existing state-of-the-art techniques in predicting valence on the RECOLA dataset. Additionally, our visual facial expression feature extraction network exceeds state-of-the-art results on the AffectNet and Google Facial Expression Comparison datasets.",1
"Deep learning face recognition models are used by state-of-the-art surveillance systems to identify individuals passing through public areas (e.g., airports). Previous studies have demonstrated the use of adversarial machine learning (AML) attacks to successfully evade identification by such systems, both in the digital and physical domains. Attacks in the physical domain, however, require significant manipulation to the human participant's face, which can raise suspicion by human observers (e.g. airport security officers). In this study, we present a novel black-box AML attack which carefully crafts natural makeup, which, when applied on a human participant, prevents the participant from being identified by facial recognition models. We evaluated our proposed attack against the ArcFace face recognition model, with 20 participants in a real-world setup that includes two cameras, different shooting angles, and different lighting conditions. The evaluation results show that in the digital domain, the face recognition system was unable to identify all of the participants, while in the physical domain, the face recognition system was able to identify the participants in only 1.22% of the frames (compared to 47.57% without makeup and 33.73% with random natural makeup), which is below a reasonable threshold of a realistic operational environment.",0
"Advanced surveillance systems utilize deep learning face recognition models to identify individuals in public spaces like airports. Studies have shown that adversarial machine learning (AML) attacks can successfully evade such systems, but physical domain attacks require significant manipulation of the participant's face, which can raise suspicion. This study introduces a new black-box AML attack that involves applying carefully crafted natural makeup to the participant, preventing facial recognition models from identifying them. The proposed attack was evaluated on the ArcFace face recognition model with 20 participants in a real-world setup involving two cameras, different shooting angles, and lighting conditions. The results show that the face recognition system was unable to identify any participants in the digital domain and only 1.22% in the physical domain, compared to 47.57% without makeup and 33.73% with random natural makeup. This falls below a reasonable threshold for a realistic operational environment.",1
"This research is mainly focused on the assessment of machine learning algorithms in the prediction of daylight and visual comfort metrics in the early design stages. A dataset was primarily developed from 2880 simulations derived from Honeybee for Grasshopper. The simulations were done for a shoebox space with a one side window. The alternatives emerged from different physical features, including room dimensions, interior surfaces reflectance, window dimensions and orientations, number of windows, and shading states. 5 metrics were used for daylight evaluations, including UDI, sDA, mDA, ASE, and sVD. Quality Views were analyzed for the same shoebox spaces via a grasshopper-based algorithm, developed from the LEED v4 evaluation framework for Quality Views. The dataset was further analyzed with an Artificial Neural Network algorithm written in Python. The accuracy of the predictions was estimated at 97% on average. The developed model could be used in early design stages analyses without the need for time-consuming simulations in previously used platforms and programs.",0
"The primary focus of this study is evaluating machine learning algorithms that can predict daylight and visual comfort metrics during the early stages of design. A dataset was created from 2880 simulations generated by Honeybee for Grasshopper, using a shoebox space with a single window and various physical features such as room dimensions, surface reflectance, window size and orientation, number of windows, and shading states. Daylight evaluations were based on five metrics, including UDI, sDA, mDA, ASE, and sVD. Quality views were assessed using a grasshopper-based algorithm that followed the LEED v4 evaluation framework. An Artificial Neural Network algorithm written in Python was used to analyze the dataset, resulting in an accuracy rate of 97% on average. This model can be used during early design stages to avoid the need for time-consuming simulations in previously used platforms and programs.",1
"Motivated by the problem of optimization of force-field systems in physics using large-scale computer simulations, we consider exploration of a deterministic complex multivariate response surface. The objective is to find input combinations that generate output close to some desired or ""target"" vector. In spite of reducing the problem to exploration of the input space with respect to a one-dimensional loss function, the search is nontrivial and challenging due to infeasible input combinations, high dimensionalities of the input and output space and multiple ""desirable"" regions in the input space and the difficulty of emulating the objective function well with a surrogate model. We propose an approach that is based on combining machine learning techniques with smart experimental design ideas to locate multiple good regions in the input space.",0
"Our focus is on exploring a complex multivariate response surface with the aim of optimizing force-field systems in physics through large-scale computer simulations. The ultimate goal is to identify input combinations that can produce an output vector that closely matches a desired or ""target"" vector. Despite reducing the problem to exploring the input space using a one-dimensional loss function, the search remains challenging due to the presence of infeasible input combinations, high dimensionalities in both input and output spaces, as well as multiple ""desirable"" regions in the input space. Additionally, accurately emulating the objective function with a surrogate model is a difficult task. To overcome these challenges, we propose a machine learning-based approach that incorporates smart experimental design ideas to locate multiple good regions in the input space.",1
"Effective inclusion of physics-based knowledge into deep neural network models of dynamical systems can greatly improve data efficiency and generalization. Such a-priori knowledge might arise from physical principles (e.g., conservation laws) or from the system's design (e.g., the Jacobian matrix of a robot), even if large portions of the system dynamics remain unknown. We develop a framework to learn dynamics models from trajectory data while incorporating a-priori system knowledge as inductive bias. More specifically, the proposed framework uses physics-based side information to inform the structure of the neural network itself, and to place constraints on the values of the outputs and the internal states of the model. It represents the system's vector field as a composition of known and unknown functions, the latter of which are parametrized by neural networks. The physics-informed constraints are enforced via the augmented Lagrangian method during the model's training. We experimentally demonstrate the benefits of the proposed approach on a variety of dynamical systems -- including a benchmark suite of robotics environments featuring large state spaces, non-linear dynamics, external forces, contact forces, and control inputs. By exploiting a-priori system knowledge during training, the proposed approach learns to predict the system dynamics two orders of magnitude more accurately than a baseline approach that does not include prior knowledge, given the same training dataset.",0
"Deep neural network models of dynamical systems can greatly benefit from the inclusion of physics-based knowledge, leading to improved data efficiency and generalization. This knowledge may be derived from physical principles or the system's design, even if much of the system dynamics are unknown. To achieve this, we have developed a framework that incorporates a-priori system knowledge as an inductive bias when learning dynamics models from trajectory data. Our framework uses physics-based information to inform the structure of the neural network and constrain the values of the outputs and internal states of the model. This involves representing the system's vector field as a composition of known and unknown functions, where the latter is parametrized by neural networks. Physics-informed constraints are enforced via the augmented Lagrangian method during the model's training. We have demonstrated the effectiveness of this approach on a range of dynamical systems, including robotics environments with large state spaces, non-linear dynamics, external forces, contact forces, and control inputs. The proposed approach learns to predict the system dynamics two orders of magnitude more accurately than a baseline approach that does not include prior knowledge, given the same training dataset.",1
"In this work, we address the problem of jointly estimating albedo, normals, depth and 3D spatially-varying lighting from a single image. Most existing methods formulate the task as image-to-image translation, ignoring the 3D properties of the scene. However, indoor scenes contain complex 3D light transport where a 2D representation is insufficient. In this paper, we propose a unified, learning-based inverse rendering framework that formulates 3D spatially-varying lighting. Inspired by classic volume rendering techniques, we propose a novel Volumetric Spherical Gaussian representation for lighting, which parameterizes the exitant radiance of the 3D scene surfaces on a voxel grid. We design a physics based differentiable renderer that utilizes our 3D lighting representation, and formulates the energy-conserving image formation process that enables joint training of all intrinsic properties with the re-rendering constraint. Our model ensures physically correct predictions and avoids the need for ground-truth HDR lighting which is not easily accessible. Experiments show that our method outperforms prior works both quantitatively and qualitatively, and is capable of producing photorealistic results for AR applications such as virtual object insertion even for highly specular objects.",0
"The focus of our study is on the simultaneous estimation of albedo, normals, depth, and 3D spatially-varying lighting from a single image. Most current methods neglect the 3D characteristics of the scene and rely solely on image-to-image translation. However, indoor scenes possess intricate 3D light transport that cannot be appropriately represented in 2D. Our research introduces a learning-based inverse rendering framework that accounts for 3D spatially-varying lighting. We propose a novel Volumetric Spherical Gaussian representation for lighting, inspired by classic volume rendering techniques, that parameterizes the exitant radiance of the 3D scene surfaces on a voxel grid. We have created a physics-based differentiable renderer that utilizes our 3D lighting representation and formulates the energy-conserving image formation process, allowing for the joint training of all intrinsic properties with the re-rendering constraint. Our model ensures physically accurate predictions and eliminates the need for ground-truth HDR lighting that can be challenging to obtain. Our experiments demonstrate that our approach outperforms previous methods both quantitatively and qualitatively and can generate photorealistic results for AR applications, even for highly specular objects.",1
"This paper introduces V-SysId, a novel method that enables simultaneous keypoint discovery, 3D system identification, and extrinsic camera calibration from an unlabeled video taken from a static camera, using only the family of equations of motion of the object of interest as weak supervision. V-SysId takes keypoint trajectory proposals and alternates between maximum likelihood parameter estimation and extrinsic camera calibration, before applying a suitable selection criterion to identify the track of interest. This is then used to train a keypoint tracking model using supervised learning. Results on a range of settings (robotics, physics, physiology) highlight the utility of this approach.",0
"In this paper, a new technique called V-SysId is introduced. This method allows for the simultaneous discovery of keypoints, system identification in 3D, and extrinsic camera calibration from an unlabeled video recorded by a stationary camera. The method relies solely on the equations of motion of the object of interest as weak supervision. V-SysId proposes keypoint trajectory proposals and uses a combination of maximum likelihood parameter estimation and extrinsic camera calibration, followed by a selection criterion that identifies the relevant track. The identified track is then used to train a keypoint tracking model through supervised learning. The approach is found to be useful across various settings such as robotics, physics, and physiology, as evidenced by the results obtained.",1
"We study the performance of Long Short-Term Memory networks for keystroke biometric authentication at large scale in free-text scenarios. For this we explore the performance of Long Short-Term Memory (LSTMs) networks trained with a moderate number of keystrokes per identity and evaluated under different scenarios including: i) three learning approaches depending on the loss function (softmax, contrastive, and triplet loss); ii) different number of training samples and lengths of keystroke sequences; iii) four databases based on two device types (physical vs touchscreen keyboard); and iv) comparison with existing approaches based on both traditional statistical methods and deep learning architectures. Our approach called TypeNet achieves state-of-the-art keystroke biometric authentication performance with an Equal Error Rate of 2.2% and 9.2% for physical and touchscreen keyboards, respectively, significantly outperforming previous approaches. Our experiments demonstrate a moderate increase in error with up to 100,000 subjects, demonstrating the potential of TypeNet to operate at an Internet scale. To the best of our knowledge, the databases used in this work are the largest existing free-text keystroke databases available for research with more than 136 million keystrokes from 168,000 subjects in physical keyboards, and 60,000 subjects with more than 63 million keystrokes acquired on mobile touchscreens.",0
"Our research focuses on evaluating the effectiveness of Long Short-Term Memory (LSTM) networks for keystroke biometric authentication in free-text scenarios on a large scale. To achieve this, we train LSTM networks with a reasonable number of keystrokes per identity and test them under different conditions, including three learning approaches based on loss function, variable training samples and keystroke sequence lengths, databases based on physical and touchscreen keyboards, and comparison with traditional statistical methods and deep learning architectures. Our TypeNet approach achieves superior performance compared to previous methods with an Equal Error Rate of 2.2% and 9.2% for physical and touchscreen keyboards, respectively. We observe a slight increase in error with up to 100,000 subjects, indicating potential for TypeNet to operate on an Internet scale. Our study uses the largest free-text keystroke databases available, including over 136 million keystrokes from 168,000 subjects on physical keyboards and over 63 million keystrokes from 60,000 subjects on mobile touchscreens.",1
"Deep neural networks (DNNs) have become essential for processing the vast amounts of aerial imagery collected using earth-observing satellite platforms. However, DNNs are vulnerable towards adversarial examples, and it is expected that this weakness also plagues DNNs for aerial imagery. In this work, we demonstrate one of the first efforts on physical adversarial attacks on aerial imagery, whereby adversarial patches were optimised, fabricated and installed on or near target objects (cars) to significantly reduce the efficacy of an object detector applied on overhead images. Physical adversarial attacks on aerial images, particularly those captured from satellite platforms, are challenged by atmospheric factors (lighting, weather, seasons) and the distance between the observer and target. To investigate the effects of these challenges, we devised novel experiments and metrics to evaluate the efficacy of physical adversarial attacks against object detectors in aerial scenes. Our results indicate the palpable threat posed by physical adversarial attacks towards DNNs for processing satellite imagery.",0
"The processing of vast amounts of aerial imagery collected by earth-observing satellite platforms necessitates the use of deep neural networks (DNNs). However, DNNs are susceptible to adversarial examples, which is anticipated to extend to aerial imagery. This study presents one of the initial attempts to perform physical adversarial attacks on aerial imagery. Adversarial patches were crafted, manufactured, and placed on or near target objects (cars) to significantly diminish the effectiveness of object detectors in overhead images. Physical adversarial attacks on aerial images are challenging due to atmospheric factors (weather, lighting, seasons) and the distance between the observer and the target. To investigate the impact of these challenges, new experiments and metrics were devised to evaluate the effectiveness of physical adversarial attacks against object detectors in aerial scenes. The results demonstrate a tangible threat posed by physical adversarial attacks to DNNs that process satellite imagery.",1
"Failure trajectories, identifying the probable failure zones, and damage statistics are some of the key quantities of relevance in brittle fracture applications. High-fidelity numerical solvers that reliably estimate these relevant quantities exist but they are computationally demanding requiring a high resolution of the crack. Moreover, independent intensive simulations need to be carried out even for a small change in domain parameters and/or material properties. Therefore, fast and generalizable surrogate models are needed to alleviate the computational burden but the discontinuous nature of fracture mechanics presents a major challenge to developing such models. We propose a physics-informed variational formulation of DeepONet (V-DeepONet) for brittle fracture analysis. V-DeepONet is trained to map the initial configuration of the defect to the relevant fields of interests (e.g., damage and displacement fields). Once the network is trained, the entire global solution can be rapidly obtained for any initial crack configuration and loading steps on that domain. While the original DeepONet is solely data-driven, we take a different path to train the V-DeepONet by imposing the governing equations in variational form and we also use some labelled data. We demonstrate the effectiveness of V-DeepOnet through two benchmarks of brittle fracture, and we verify its accuracy using results from high-fidelity solvers. Encoding the physical laws and also some data to train the network renders the surrogate model capable of accurately performing both interpolation and extrapolation tasks, considering that fracture modeling is very sensitive to fluctuations. The proposed hybrid training of V-DeepONet is superior to state-of-the-art methods and can be applied to a wide array of dynamical systems with complex responses.",0
"In brittle fracture applications, important quantities include failure trajectories, identification of likely failure zones, and damage statistics. Accurate estimation of these quantities is possible with high-fidelity numerical solvers but they require a high resolution of the crack and intensive simulations for even small changes in domain parameters and material properties. To overcome this computational burden, fast and generalizable surrogate models are required, but the discontinuous nature of fracture mechanics presents a challenge. To address this challenge, we propose a physics-informed variational formulation of DeepONet (V-DeepONet) for brittle fracture analysis. V-DeepONet is trained to map initial defect configurations to relevant fields such as damage and displacement fields. Unlike the original DeepONet which is solely data-driven, V-DeepONet is trained using both the governing equations in variational form and some labelled data. We demonstrate the effectiveness of V-DeepONet through two benchmarks of brittle fracture, and we verify its accuracy using results from high-fidelity solvers. The proposed hybrid training of V-DeepONet is superior to state-of-the-art methods and can be applied to a wide array of dynamical systems with complex responses.",1
"3D scene understanding from point clouds plays a vital role for various robotic applications. Unfortunately, current state-of-the-art methods use separate neural networks for different tasks like object detection or room layout estimation. Such a scheme has two limitations: 1) Storing and running several networks for different tasks are expensive for typical robotic platforms. 2) The intrinsic structure of separate outputs are ignored and potentially violated. To this end, we propose the first transformer architecture that predicts 3D objects and layouts simultaneously, using point cloud inputs. Unlike existing methods that either estimate layout keypoints or edges, we directly parameterize room layout as a set of quads. As such, the proposed architecture is termed as P(oint)Q(uad)-Transformer. Along with the novel quad representation, we propose a tailored physical constraint loss function that discourages object-layout interference. The quantitative and qualitative evaluations on the public benchmark ScanNet show that the proposed PQ-Transformer succeeds to jointly parse 3D objects and layouts, running at a quasi-real-time (8.91 FPS) rate without efficiency-oriented optimization. Moreover, the new physical constraint loss can improve strong baselines, and the F1-score of the room layout is significantly promoted from 37.9% to 57.9%.",0
"Various robotic applications rely on 3D scene understanding from point clouds. However, current methods use separate neural networks for different tasks, such as object detection or room layout estimation, which is expensive for typical robotic platforms and ignores the intrinsic structure of separate outputs. To address these limitations, we propose the P(oint)Q(uad)-Transformer, the first transformer architecture that predicts 3D objects and layouts simultaneously using point cloud inputs. Our approach directly parameterizes room layout as a set of quads, unlike existing methods that estimate layout keypoints or edges. We also propose a physical constraint loss function that discourages object-layout interference. Our evaluation on the ScanNet benchmark shows that our approach succeeds in jointly parsing 3D objects and layouts at a quasi-real-time rate without optimization. Additionally, our physical constraint loss improves strong baselines, significantly promoting the F1-score of the room layout from 37.9% to 57.9%.",1
"Discovery of dynamical systems from data forms the foundation for data-driven modeling and recently, structure-preserving geometric perspectives have been shown to provide improved forecasting, stability, and physical realizability guarantees. We present here a unification of the Sparse Identification of Nonlinear Dynamics (SINDy) formalism with neural ordinary differential equations. The resulting framework allows learning of both ""black-box"" dynamics and learning of structure preserving bracket formalisms for both reversible and irreversible dynamics. We present a suite of benchmarks demonstrating effectiveness and structure preservation, including for chaotic systems.",0
"The basis of data-driven modeling is the identification of dynamical systems from data. Recently, it has been revealed that geometric perspectives which preserve structure are more effective in providing guarantees for physical realizability, stability, and forecasting. In this article, we introduce the combination of the Sparse Identification of Nonlinear Dynamics (SINDy) formalism with neural ordinary differential equations. This newly developed framework enables the learning of both ""black-box"" dynamics and the structure-preserving bracket formalisms for both reversible and irreversible dynamics. We provide a range of assessments that demonstrate the effectiveness and preservation of structure, including assessments of chaotic systems.",1
"Flow network models can capture the underlying physics and operational constraints of many networked systems including the power grid and transportation and water networks. However, analyzing reliability of systems using computationally expensive flow-based models faces substantial challenges, especially for rare events. Existing actively trained meta-models, which present a new promising direction in reliability analysis, are not applicable to networks due to the inability of these methods to handle high-dimensional problems as well as discrete or mixed variable inputs. This study presents the first adaptive surrogate-based Network Reliability Analysis using Bayesian Additive Regression Trees (ANR-BART). This approach integrates BART and Monte Carlo simulation (MCS) via an active learning method that identifies the most valuable training samples based on the credible intervals derived by BART over the space of predictor variables as well as the proximity of the points to the estimated limit state. Benchmark power grids including IEEE 30, 57, 118, and 300-bus systems and their power flow models for cascading failure analysis are considered to investigate ANR-BART, MCS, subset simulation, and passively-trained optimal deep neural networks and BART. Results indicate that ANR-BART is robust and yields accurate estimates of network failure probability, while significantly reducing the computational cost of reliability analysis.",0
"Flow network models are useful for understanding the physics and operational limitations of networked systems such as transportation, water, and power grids. However, reliability analysis using computationally intensive flow-based models presents significant challenges, particularly for rare events. Existing meta-models trained through active learning show promise for reliability analysis, but are not suitable for networks due to their inability to handle high-dimensional and mixed variable inputs. To address this issue, this study proposes the first adaptive surrogate-based Network Reliability Analysis using Bayesian Additive Regression Trees (ANR-BART). ANR-BART combines BART and Monte Carlo simulation using an active learning approach that identifies valuable training samples based on credible intervals and proximity to the estimated limit state. The study applies this approach to benchmark power grids and finds that ANR-BART is robust and accurate, resulting in significant reductions in computational cost for reliability analysis.",1
"Physical and cloud storage services are well-served by functioning and reliable high-volume storage systems. Recent observations point to hard disk reliability as one of the most pressing reliability issues in data centers containing massive volumes of storage devices such as HDDs. In this regard, early detection of impending failure at the disk level aids in reducing system downtime and reduces operational loss making proactive health monitoring a priority for AIOps in such settings. In this work, we introduce methods of extracting meaningful attributes associated with operational failure and of pre-processing the highly imbalanced health statistics data for subsequent prediction tasks using data-driven approaches. We use a Bidirectional LSTM with a multi-day look back period to learn the temporal progression of health indicators and baseline them against vanilla LSTM and Random Forest models to come up with several key metrics that establish the usefulness of and superiority of our model under some tightly defined operational constraints. For example, using a 15 day look back period, our approach can predict the occurrence of disk failure with an accuracy of 96.4% considering test data 60 days before failure. This helps to alert operations maintenance well in-advance about potential mitigation needs. In addition, our model reports a mean absolute error of 0.12 for predicting failure up to 60 days in advance, placing it among the state-of-the-art in recent literature.",0
"High-volume storage systems are critical for both physical and cloud storage services, but hard disk reliability has emerged as a significant issue in data centers with massive storage devices such as HDDs. To minimize system downtime and operational loss, proactive health monitoring is essential, requiring early detection of potential disk failure. This study presents methods for extracting meaningful attributes associated with operational failure and pre-processing imbalanced health statistics data for prediction tasks using data-driven approaches. A Bidirectional LSTM with a multi-day look back period is used to analyze the temporal progression of health indicators and compared to vanilla LSTM and Random Forest models. Our model shows superior performance under tightly defined operational constraints, with a 96.4% accuracy in predicting disk failure up to 60 days in advance using a 15 day look back period. Our approach can alert maintenance operations well in advance of potential mitigation needs, with a mean absolute error of 0.12 for predicting failure up to 60 days in advance, making it a state-of-the-art approach in the recent literature.",1
"This digital book contains a practical and comprehensive introduction of everything related to deep learning in the context of physical simulations. As much as possible, all topics come with hands-on code examples in the form of Jupyter notebooks to quickly get started. Beyond standard supervised learning from data, we'll look at physical loss constraints, more tightly coupled learning algorithms with differentiable simulations, as well as reinforcement learning and uncertainty modeling. We live in exciting times: these methods have a huge potential to fundamentally change what computer simulations can achieve.",0
"This e-book offers a thorough and applicable overview of the various aspects of deep learning in relation to physical simulations. The majority of the topics are accompanied by practical code examples in the form of Jupyter notebooks, enabling readers to begin working with them instantly. In addition to the standard supervised learning from data, this guide delves into physical loss constraints, more closely linked learning algorithms that incorporate differentiable simulations, reinforcement learning, and uncertainty modelling. These techniques have the potential to radically transform the capabilities of computer simulations, making these times a thrilling era.",1
"Dynamical models estimate and predict the temporal evolution of physical systems. State Space Models (SSMs) in particular represent the system dynamics with many desirable properties, such as being able to model uncertainty in both the model and measurements, and optimal (in the Bayesian sense) recursive formulations e.g. the Kalman Filter. However, they require significant domain knowledge to derive the parametric form and considerable hand-tuning to correctly set all the parameters. Data driven techniques e.g. Recurrent Neural Networks have emerged as compelling alternatives to SSMs with wide success across a number of challenging tasks, in part due to their ability to extract relevant features from rich inputs. They however lack interpretability and robustness to unseen conditions. In this work, we present DynaNet, a hybrid deep learning and time-varying state-space model which can be trained end-to-end. Our neural Kalman dynamical model allows us to exploit the relative merits of each approach. We demonstrate state-of-the-art estimation and prediction on a number of physically challenging tasks, including visual odometry, sensor fusion for visual-inertial navigation and pendulum control. In addition we show how DynaNet can indicate failures through investigation of properties such as the rate of innovation (Kalman Gain).",0
"The estimation and prediction of the temporal evolution of physical systems can be achieved through dynamical models. State Space Models (SSMs) are particularly useful due to their ability to model uncertainty and optimal recursive formulations, like the Kalman Filter. However, SSMs require domain knowledge and parameter tuning to be effective. Data-driven techniques like Recurrent Neural Networks have emerged as effective alternatives to SSMs due to their ability to extract relevant features from rich inputs. However, they lack interpretability and robustness. In this study, we introduce DynaNet, a hybrid deep learning and time-varying state-space model that can be trained end-to-end. Our neural Kalman dynamical model combines the benefits of each approach. We demonstrate superior estimation and prediction on challenging tasks, such as visual odometry and sensor fusion for visual-inertial navigation, using DynaNet. Additionally, we show that DynaNet can identify failures by analyzing properties like the rate of innovation (Kalman Gain).",1
"Modeling the hand-object (HO) interaction not only requires estimation of the HO pose, but also pays attention to the contact due to their interaction. Significant progress has been made in estimating hand and object separately with deep learning methods, simultaneous HO pose estimation and contact modeling has not yet been fully explored. In this paper, we present an explicit contact representation namely Contact Potential Field (CPF), and a learning-fitting hybrid framework namely MIHO to Modeling the Interaction of Hand and Object. In CPF, we treat each contacting HO vertex pair as a spring-mass system. Hence the whole system forms a potential field with minimal elastic energy at the grasp position. Extensive experiments on the two commonly used benchmarks have demonstrated that our method can achieve state-of-the-art in several reconstruction metrics, and allow us to produce more physically plausible HO pose even when the ground-truth exhibits severe interpenetration or disjointedness. Our code is available at https://github.com/lixiny/CPF.",0
"Not only is it necessary to estimate the pose of the hand-object (HO) interaction, but attention must also be given to the contact between them. Although there has been significant advancement in estimating the hand and object separately using deep learning methods, simultaneous estimation of the HO pose and contact modeling has not been fully explored. This paper proposes a new approach called Contact Potential Field (CPF) for explicit contact representation and a hybrid framework called MIHO for modeling the interaction between the hand and object. The CPF treats each pair of contacting HO vertices as a spring-mass system, resulting in a potential field with minimal elastic energy at the grasp position. Our experiments on two commonly used benchmarks demonstrate that our method achieves state-of-the-art results in several reconstruction metrics and enables the production of physically plausible HO poses even in cases of severe interpenetration or disjointedness. The code for our approach is available at https://github.com/lixiny/CPF.",1
We present a machine learning method for model reduction which incorporates domain-specific physics through candidate functions. Our method estimates an effective probability distribution and differential equation model from stochastic simulations of a reaction network. The close connection between reduced and fine scale descriptions allows approximations derived from the master equation to be introduced into the learning problem. This representation is shown to improve generalization and allows a large reduction in network size for a classic model of inositol trisphosphate (IP3) dependent calcium oscillations in non-excitable cells.,0
"Our approach utilizes machine learning to reduce model complexity while taking into account the specific physics of the domain. By utilizing candidate functions, we are able to estimate an effective probability distribution and differential equation model based on stochastic simulations of a reaction network. This allows us to introduce approximations from the master equation into the learning process, resulting in improved generalization and a significant reduction in network size for a well-known model of calcium oscillations in non-excitable cells that depends on inositol trisphosphate (IP3). The close relationship between the reduced and fine-scale descriptions is highlighted, demonstrating the efficacy of our representation.",1
"Hybrid modeling, the combination of first principle and machine learning models, is an emerging research field that gathers more and more attention. Even if hybrid models produce formidable results for academic examples, there are still different technical challenges that hinder the use of hybrid modeling in real-world applications. By presenting NeuralFMUs, the fusion of a FMU, a numerical ODE solver and an ANN, we are paving the way for the use of a variety of first principle models from different modeling tools as parts of hybrid models. This contribution handles the hybrid modeling of a complex, real-world example: Starting with a simplified 1D-fluid model of the human cardiovascular system (arterial side), the aim is to learn neglected physical effects like arterial elasticity from data. We will show that the hybrid modeling process is more comfortable, needs less system knowledge and is therefore less error-prone compared to modeling solely based on first principle. Further, the resulting hybrid model has improved in computation performance, compared to a pure first principle white-box model, while still fulfilling the requirements regarding accuracy of the considered hemodynamic quantities. The use of the presented techniques is explained in a general manner and the considered use-case can serve as example for other modeling and simulation applications in and beyond the medical domain.",0
"The emerging research field of hybrid modeling, combining first principle and machine learning models, is gaining attention despite facing various technical challenges in real-world applications. To address this, we introduce NeuralFMUs, which fuse a FMU, a numerical ODE solver, and an ANN, allowing for the use of first principle models from different modeling tools in hybrid models. Our contribution demonstrates the hybrid modeling of a complex real-world example, learning physical effects like arterial elasticity in a simplified 1D-fluid model of the human cardiovascular system. Our approach is shown to be more efficient, requiring less system knowledge and being less error-prone compared to solely first principle modeling. Additionally, the resulting hybrid model improves computation performance while maintaining accuracy requirements. Our techniques can be applied to other modeling and simulation applications beyond the medical domain.",1
"In this work we continue studies of the uncertainty quantification problem in emission tomographies such as PET or SPECT. In particular, we consider a scenario when additional multimodal data (e.g., anatomical MRI images) are available. To solve the aforementioned problem we adapt the recently proposed nonparametric posterior learning technique to the context of Poisson-type data in emission tomography. Using this approach we derive sampling algorithms which are trivially parallelizable, scalable and very easy to implement. In addition, we prove conditional consistency and tightness for the distribution of produced samples in the small noise limit (i.e., when the acquisition time tends to infinity) and derive new geometrical and necessary condition on how MRI images must be used. This condition arises naturally in the context of misspecified generalized Poisson models. We also contrast our approach with bayesian MCMC sampling based on one data augmentation scheme which is very popular in the context of EM-type algorithms for PET or SPECT. We show theoretically and also numerically that such data augmentation significantly increases mixing times for the Markov chain. In view of this, our algorithms seem to give a reasonable trade-off between design complexity, scalability, numerical load and asessement for the uncertainty quantification.",0
"Our study focuses on the problem of uncertainty quantification in emission tomographies like PET or SPECT. We explore the scenario where supplemental multimodal data, such as anatomical MRI images, are available. To solve this issue, we adapt a nonparametric posterior learning technique to the context of Poisson-type data in emission tomography. This approach yields sampling algorithms that are simple to implement, scalable, and parallelizable. We also prove conditional consistency and tightness for the distribution of generated samples in the small noise limit. We derive new geometrical and necessary conditions on how MRI images should be used, which arise naturally in the context of misspecified generalized Poisson models. Additionally, we compare our approach with a Bayesian MCMC sampling method based on a popular data augmentation scheme for PET or SPECT. We show, both theoretically and numerically, that this data augmentation significantly increases mixing times for the Markov chain. Therefore, our algorithm provides a reasonable trade-off between design complexity, scalability, numerical load, and uncertainty quantification assessment.",1
"The rational design of molecules with desired properties is a long-standing challenge in chemistry. Generative neural networks have emerged as a powerful approach to sample novel molecules from a learned distribution. Here, we propose a conditional generative neural network for 3d molecular structures with specified structural and chemical properties. This approach is agnostic to chemical bonding and enables targeted sampling of novel molecules from conditional distributions, even in domains where reference calculations are sparse. We demonstrate the utility of our method for inverse design by generating molecules with specified composition or motifs, discovering particularly stable molecules, and jointly targeting multiple electronic properties beyond the training regime.",0
"In the field of chemistry, the creation of molecules with specific characteristics has been a difficult task to achieve. However, generative neural networks have emerged as a viable solution to generate new molecules based on learned distributions. In this study, we introduce a conditional generative neural network that can produce 3D molecular structures with predetermined chemical and structural properties. This approach is not limited by chemical bonding and allows for targeted sampling of new molecules from conditional distributions, even in areas where there is limited reference data. Our method demonstrates its usefulness in inverse design by generating molecules with specified composition or motifs, discovering highly stable molecules, and targeting multiple electronic properties beyond the training data.",1
"To align advanced artificial intelligence (AI) with human values and promote safe AI, it is important for AI to predict the outcome of physical interactions. Even with the ongoing debates on how humans predict the outcomes of physical interactions among objects in the real world, there are works attempting to tackle this task via cognitive-inspired AI approaches. However, there is still a lack of AI approaches that mimic the mental imagery humans use to predict physical interactions in the real world. In this work, we propose a novel PIP scheme: Physical Interaction Prediction via Mental Imagery with Span Selection. PIP utilizes a deep generative model to output future frames of physical interactions among objects before extracting crucial information for predicting physical interactions by focusing on salient frames using span selection. To evaluate our model, we propose a large-scale SPACE+ dataset of synthetic video frames, including three physical interaction events in a 3D environment. Our experiments show that PIP outperforms baselines and human performance in physical interaction prediction for both seen and unseen objects. Furthermore, PIP's span selection scheme can effectively identify the frames where physical interactions among objects occur within the generated frames, allowing for added interpretability.",0
"In order to ensure that advanced artificial intelligence (AI) adheres to human values and is safe, it is crucial that AI is able to predict the outcomes of physical interactions. Although there is an ongoing debate on how humans predict physical interactions, some researchers are attempting to address this task through cognitive-inspired AI methods. However, there is still a lack of AI approaches that replicate the mental imagery humans use to predict physical interactions in the real world. This paper proposes a new scheme called Physical Interaction Prediction via Mental Imagery with Span Selection (PIP), which employs a deep generative model to generate future frames of physical interactions among objects. PIP then utilizes span selection to extract key information for predicting physical interactions by focusing on relevant frames. To assess the effectiveness of our model, we introduce a large-scale dataset called SPACE+, which contains synthetic video frames of three physical interaction events in a 3D environment. Our findings demonstrate that PIP outperforms both baseline models and human performance in predicting physical interactions for both known and unknown objects. Additionally, PIP's span selection method is effective in identifying frames where physical interactions among objects occur, providing added interpretability.",1
"We present the first method capable of photorealistically reconstructing deformable scenes using photos/videos captured casually from mobile phones. Our approach augments neural radiance fields (NeRF) by optimizing an additional continuous volumetric deformation field that warps each observed point into a canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone to local minima, and propose a coarse-to-fine optimization method for coordinate-based models that allows for more robust optimization. By adapting principles from geometry processing and physical simulation to NeRF-like models, we propose an elastic regularization of the deformation field that further improves robustness. We show that our method can turn casually captured selfie photos/videos into deformable NeRF models that allow for photorealistic renderings of the subject from arbitrary viewpoints, which we dub ""nerfies."" We evaluate our method by collecting time-synchronized data using a rig with two mobile phones, yielding train/validation images of the same pose at different viewpoints. We show that our method faithfully reconstructs non-rigidly deforming scenes and reproduces unseen views with high fidelity.",0
"Our study introduces a novel technique that can realistically reconstruct deformable environments using photos or videos captured on mobile devices. Our approach enhances neural radiance fields (NeRF) by optimizing a continuous volumetric deformation field that transforms each observed point into a canonical 5D NeRF. We discovered that these NeRF-like deformation fields are susceptible to local minima, and therefore, we propose a coarse-to-fine optimization method for coordinate-based models to enhance optimization robustness. By merging principles from geometry processing and physical simulation to NeRF-like models, we suggest an elastic regularization of the deformation field to further improve robustness. Our method transforms casual selfie photos/videos into deformable NeRF models, enabling photorealistic renderings of the subject from any angle, which we label ""nerfies."" We evaluated our method by collecting time-synchronized data using two mobile phones, generating train/validation images of the same pose from various perspectives. Our technique effectively reconstructs non-rigid scenes and produces unseen views with exceptional accuracy.",1
"We present a differentiable soft-body physics simulator that can be composed with neural networks as a differentiable layer. In contrast to other differentiable physics approaches that use explicit forward models to define state transitions, we focus on implicit state transitions defined via function minimization. Implicit state transitions appear in implicit numerical integration methods, which offer the benefits of large time steps and excellent numerical stability, but require a special treatment to achieve differentiability due to the absence of an explicit differentiable forward pass. In contrast to other implicit differentiation approaches that require explicit formulas for the force function and the force Jacobian matrix, we present an energy-based approach that allows us to compute these derivatives automatically and in a matrix-free fashion via reverse-mode automatic differentiation. This allows for more flexibility and productivity when defining physical models and is particularly important in the context of neural network training, which often relies on reverse-mode automatic differentiation (backpropagation). We demonstrate the effectiveness of our differentiable simulator in policy optimization for locomotion tasks and show that it achieves better sample efficiency than model-free reinforcement learning.",0
"Our study introduces a differentiable soft-body physics simulator that can be integrated with neural networks as a differentiable layer. Unlike other differentiable physics methodologies that employ explicit forward models to define state transitions, we concentrate on implicit state transitions that are determined by function minimization. Implicit state transitions are found in implicit numerical integration methods, which offer advantages such as large time steps and outstanding numerical stability, but necessitate specific treatment to achieve differentiability owing to the lack of an explicit differentiable forward pass. Rather than other implicit differentiation techniques that require explicit formulas for the force function and Jacobian matrix, we propose an energy-based strategy that allows us to automatically compute these derivatives in a matrix-free manner via reverse-mode automatic differentiation. This provides more flexibility and efficiency when developing physical models and is especially critical in the context of neural network training, which frequently relies on reverse-mode automatic differentiation (backpropagation). We demonstrate the efficacy of our differentiable simulator in policy optimization for locomotion tasks and establish that it outperforms model-free reinforcement learning in terms of sample efficiency.",1
"Modern vehicles have multiple electronic control units (ECUs) that are connected together as part of a complex distributed cyber-physical system (CPS). The ever-increasing communication between ECUs and external electronic systems has made these vehicles particularly susceptible to a variety of cyber-attacks. In this work, we present a novel anomaly detection framework called TENET to detect anomalies induced by cyber-attacks on vehicles. TENET uses temporal convolutional neural networks with an integrated attention mechanism to detect anomalous attack patterns. TENET is able to achieve an improvement of 32.70% in False Negative Rate, 19.14% in the Mathews Correlation Coefficient, and 17.25% in the ROC-AUC metric, with 94.62% fewer model parameters, 86.95% decrease in memory footprint, and 48.14% lower inference time when compared to the best performing prior work on automotive anomaly detection.",0
"The complex distributed cyber-physical system (CPS) in modern vehicles comprises multiple electronic control units (ECUs) that are interconnected. Unfortunately, the increasing communication between these ECUs and external electronic systems exposes the vehicles to various cyber-attacks. To combat this, we introduce a new framework called TENET that utilizes temporal convolutional neural networks with an integrated attention mechanism to identify anomalous attack patterns. Compared to the best prior work on automotive anomaly detection, TENET achieves significant improvements in the False Negative Rate, Mathews Correlation Coefficient, and ROC-AUC metric, while requiring fewer model parameters, less memory, and lower inference time.",1
"This paper covers two major subjects: First, the presentation of a new open-source library called FMI.jl for integrating FMI into the Julia programming environment by providing the possibility to load, parameterize and simulate FMUs. Further, an extension to this library called FMIFlux.jl is introduced, that allows the integration of FMUs into a neural network topology to obtain a NeuralFMU. This structural combination of an industry typical black-box model and a data-driven machine learning model combines the different advantages of both modeling approaches in one single development environment. This allows for the usage of advanced data driven modeling techniques for physical effects that are difficult to model based on first principles.",0
"The article discusses two primary topics. Firstly, it introduces FMI.jl, an open-source library that integrates FMI with the Julia programming environment. This library enables the loading, parameterization, and simulation of FMUs. Secondly, the article presents an extension to FMI.jl called FMIFlux.jl, which enables the integration of FMUs into a neural network topology to create a NeuralFMU. By combining a black-box model and a data-driven machine learning model, this approach offers the benefits of both modeling techniques in a single development environment. This facilitates the use of advanced data-driven modeling techniques for physical effects that are challenging to model using first principles.",1
"Deep learning-based surrogate modeling is becoming a promising approach for learning and simulating dynamical systems. Deep-learning methods, however, find very challenging learning stiff dynamics. In this paper, we develop DAE-PINN, the first effective deep-learning framework for learning and simulating the solution trajectories of nonlinear differential-algebraic equations (DAE), which present a form of infinite stiffness and describe, for example, the dynamics of power networks. Our DAE-PINN bases its effectiveness on the synergy between implicit Runge-Kutta time-stepping schemes (designed specifically for solving DAEs) and physics-informed neural networks (PINN) (deep neural networks that we train to satisfy the dynamics of the underlying problem). Furthermore, our framework (i) enforces the neural network to satisfy the DAEs as (approximate) hard constraints using a penalty-based method and (ii) enables simulating DAEs for long-time horizons. We showcase the effectiveness and accuracy of DAE-PINN by learning and simulating the solution trajectories of a three-bus power network.",0
"The use of deep learning-based surrogate modeling is showing promise in the learning and simulation of dynamical systems. However, deep-learning methods face difficulties in learning stiff dynamics. In this study, we introduce DAE-PINN, a deep-learning framework that effectively learns and simulates the solution trajectories of nonlinear differential-algebraic equations (DAE), which describe systems such as power networks that exhibit infinite stiffness. Our approach combines implicit Runge-Kutta time-stepping schemes, designed specifically for solving DAEs, and physics-informed neural networks (PINN), which are deep neural networks trained to satisfy the dynamics of the underlying problem. Additionally, our approach enforces the neural network to satisfy the DAEs as approximate hard constraints using a penalty-based method and enables long-time horizon simulations. We demonstrate the effectiveness and accuracy of DAE-PINN by learning and simulating the solution trajectories of a three-bus power network.",1
"Ising models are a simple generative approach to describing interacting binary variables. They have proven useful in a number of biological settings because they enable one to represent observed many-body correlations as the separable consequence of many direct, pairwise statistical interactions. The inference of Ising models from data can be computationally very challenging and often one must be satisfied with numerical approximations or limited precision. In this paper we present a novel method for the determination of Ising parameters from data, called GNisi, which uses a Graph Neural network trained on known Ising models in order to construct the parameters for unseen data. We show that GNisi is more accurate than the existing state of the art software, and we illustrate our method by applying GNisi to gene expression data.",0
"Describing interacting binary variables, Ising models are a straightforward generative approach that has proved useful in biological settings. These models are capable of representing observed many-body correlations as the separable consequence of many direct, pairwise statistical interactions. However, the inference of Ising models from data can be computationally challenging, often requiring numerical approximations or limited precision. In this paper, we introduce GNisi, a novel approach to determine Ising parameters from data. GNisi leverages a Graph Neural network trained on known Ising models to construct parameters for unseen data. Our experiments demonstrate that GNisi is more accurate than existing software, and we showcase its effectiveness in gene expression data analysis.",1
"This paper discusses an application of the singular spectrum analysis method (SSA) in the context of electroluminescence (EL) images of thin-film photovoltaic (PV) modules. We propose an EL image decomposition as a sum of three components: global intensity, cell, and aperiodic components. A parametric model of the extracted signal is used to perform several image processing tasks. The cell component is used to identify interconnection lines between PV cells at sub-pixel accuracy, as well as to correct incorrect stitching of EL images. Furthermore, an explicit expression of the cell component signal is used to estimate the inverse characteristic length, a physical parameter related to the resistances in a PV module.",0
"In this paper, the use of the singular spectrum analysis (SSA) method is explored in relation to electroluminescence (EL) images of thin-film photovoltaic (PV) modules. The approach involves breaking down the EL image into three distinct components: global intensity, cell, and aperiodic. A parametric model of the extracted signal is then utilized to carry out various image processing tasks. The cell component is particularly useful in identifying the interconnection lines between PV cells with high precision and correcting any errors in the stitching of EL images. Additionally, by utilizing an explicit expression of the cell component signal, it is possible to estimate the inverse characteristic length, which is a physical parameter associated with the resistances within a PV module.",1
"Bayesian optimization (BO) is an approach to globally optimizing black-box objective functions that are expensive to evaluate. BO-powered experimental design has found wide application in materials science, chemistry, experimental physics, drug development, etc. This work aims to bring attention to the benefits of applying BO in designing experiments and to provide a BO manual, covering both methodology and software, for the convenience of anyone who wants to apply or learn BO. In particular, we briefly explain the BO technique, review all the applications of BO in additive manufacturing, compare and exemplify the features of different open BO libraries, unlock new potential applications of BO to other types of data (e.g., preferential output). This article is aimed at readers with some understanding of Bayesian methods, but not necessarily with knowledge of additive manufacturing; the software performance overview and implementation instructions are instrumental for any experimental-design practitioner. Moreover, our review in the field of additive manufacturing highlights the current knowledge and technological trends of BO.",0
"The use of Bayesian optimization (BO) is a recommended method for globally optimizing black-box objective functions that are costly to evaluate. BO-powered experimental design has been widely applied in various fields such as materials science, drug development, experimental physics, and chemistry. The purpose of this work is to promote the advantages of applying BO in experimental design and to provide a manual that covers the methodology and software for anyone interested in learning or using BO. Specifically, the BO technique is briefly explained, and all the applications of BO in additive manufacturing are reviewed. The article also compares and provides examples of the features of different open BO libraries, and explores potential applications of BO to other types of data such as preferential output. While this article assumes some understanding of Bayesian methods, it is written to benefit any experimental-design practitioner with the overview of software performance and implementation instructions. Additionally, the review of BO in additive manufacturing highlights the current knowledge and technological trends.",1
"With the advent of advances in self-supervised learning, paired clean-noisy data are no longer required in deep learning-based image denoising. However, existing blind denoising methods still require the assumption with regard to noise characteristics, such as zero-mean noise distribution and pixel-wise noise-signal independence; this hinders wide adaptation of the method in the medical domain. On the other hand, unpaired learning can overcome limitations related to the assumption on noise characteristics, which makes it more feasible for collecting the training data in real-world scenarios. In this paper, we propose a novel image denoising scheme, Interdependent Self-Cooperative Learning (ISCL), that leverages unpaired learning by combining cyclic adversarial learning with self-supervised residual learning. Unlike the existing unpaired image denoising methods relying on matching data distributions in different domains, the two architectures in ISCL, designed for different tasks, complement each other and boost the learning process. To assess the performance of the proposed method, we conducted extensive experiments in various biomedical image degradation scenarios, such as noise caused by physical characteristics of electron microscopy (EM) devices (film and charging noise), and structural noise found in low-dose computer tomography (CT). We demonstrate that the image quality of our method is superior to conventional and current state-of-the-art deep learning-based image denoising methods, including supervised learning.",0
"Deep learning-based image denoising no longer requires paired clean-noisy data due to advances in self-supervised learning. However, existing blind denoising methods still rely on assumptions regarding noise characteristics, which limits their applicability in the medical field. Unpaired learning can overcome these limitations and is more feasible for collecting training data in real-world scenarios. This paper proposes a novel image denoising scheme called Interdependent Self-Cooperative Learning (ISCL), which leverages unpaired learning by combining cyclic adversarial learning with self-supervised residual learning. Unlike existing methods, ISCL's two architectures complement each other and boost the learning process. The proposed method was tested in various biomedical image degradation scenarios and outperformed conventional and current state-of-the-art deep learning-based image denoising methods, including supervised learning.",1
"Particle physics simulations are the cornerstone of nuclear engineering applications. Among them radiotherapy (RT) is crucial for society, with 50% of cancer patients receiving radiation treatments. For the most precise targeting of tumors, next generation RT treatments aim for real-time correction during radiation delivery, necessitating particle transport algorithms that yield precise dose distributions in sub-second times even in highly heterogeneous patient geometries. This is infeasible with currently available, purely physics based simulations. In this study, we present a data-driven dose calculation algorithm predicting the dose deposited by mono-energetic proton beams for arbitrary energies and patient geometries. Our approach frames particle transport as sequence modeling, where convolutional layers extract important spatial features into tokens and the transformer self-attention mechanism routes information between such tokens in the sequence and a beam energy token. We train our network and evaluate prediction accuracy using computationally expensive but accurate Monte Carlo (MC) simulations, considered the gold standard in particle physics. Our proposed model is 33 times faster than current clinical analytic pencil beam algorithms, improving upon their accuracy in the most heterogeneous and challenging geometries. With a relative error of 0.34% and very high gamma pass rate of 99.59% (1%, 3 mm), it also greatly outperforms the only published similar data-driven proton dose algorithm, even at a finer grid resolution. Offering MC precision 400 times faster, our model could overcome a major obstacle that has so far prohibited real-time adaptive proton treatments and significantly increase cancer treatment efficacy. Its potential to model physics interactions of other particles could also boost heavy ion treatment planning procedures limited by the speed of traditional methods.",0
"Nuclear engineering applications rely heavily on simulations of particle physics. Radiotherapy is a critical aspect of nuclear engineering, as it is responsible for treating 50% of cancer patients. To ensure the most precise targeting of tumors, next-generation radiotherapy treatments require real-time corrections during radiation delivery. However, current simulations based solely on physics are inadequate in providing sub-second precise dose distributions, especially in highly heterogeneous patient geometries. In this study, a data-driven dose calculation algorithm is proposed that predicts the dose deposited by mono-energetic proton beams for arbitrary energies and patient geometries. The approach frames particle transport as sequence modeling, where convolutional layers extract important spatial features into tokens, and the transformer self-attention mechanism routes information between such tokens in the sequence and a beam energy token. The network is trained and evaluated using Monte Carlo simulations, which are considered the gold standard in particle physics. The proposed model is 33 times faster than current clinical analytic pencil beam algorithms and outperforms the only published similar data-driven proton dose algorithm. It offers Monte Carlo precision 400 times faster and could overcome the obstacle of real-time adaptive proton treatments, significantly increasing cancer treatment efficacy. Furthermore, the potential to model physics interactions of other particles could also boost heavy ion treatment planning procedures limited by the speed of traditional methods.",1
"With the recent success of representation learning methods, which includes deep learning as a special case, there has been considerable interest in developing representation learning techniques that can incorporate known physical constraints into the learned representation. As one example, in many applications that involve a signal propagating through physical media (e.g., optics, acoustics, fluid dynamics, etc), it is known that the dynamics of the signal must satisfy constraints imposed by the wave equation. Here we propose a matrix factorization technique that decomposes such signals into a sum of components, where each component is regularized to ensure that it satisfies wave equation constraints. Although our proposed formulation is non-convex, we prove that our model can be efficiently solved to global optimality in polynomial time. We demonstrate the benefits of our work by applications in structural health monitoring, where prior work has attempted to solve this problem using sparse dictionary learning approaches that do not come with any theoretical guarantees regarding convergence to global optimality and employ heuristics to capture desired physical constraints.",0
"Representation learning methods, including deep learning, have achieved recent success, prompting interest in developing techniques that can incorporate physical constraints into learned representations. In applications involving signal propagation through physical media, such as optics, acoustics, and fluid dynamics, it is necessary to satisfy constraints imposed by the wave equation. To address this, we propose a matrix factorization method that decomposes signals into components that are regularized to meet wave equation constraints. Despite our non-convex formulation, we prove that our model can be efficiently solved to global optimality in polynomial time. Our approach outperforms prior sparse dictionary learning approaches in structural health monitoring, which lack theoretical guarantees and rely on heuristics to capture physical constraints.",1
"Recent works report that increasing the learning rate or decreasing the minibatch size in stochastic gradient descent (SGD) can improve test set performance. We argue this is expected under some conditions in models with a loss function with multiple local minima. Our main contribution is an approximate but analytical approach inspired by methods in Physics to study the role of the SGD learning rate and batch size in generalization. We characterize test set performance under a shift between the training and test data distributions for loss functions with multiple minima. The shift can simply be due to sampling, and is therefore typically present in practical applications. We show that the resulting shift in local minima worsens test performance by picking up curvature, implying that generalization improves by selecting wide and/or little-shifted local minima. We then specialize to SGD, and study its test performance under stationarity. Because obtaining the exact stationary distribution of SGD is intractable, we derive a Fokker-Planck approximation of SGD and obtain its stationary distribution instead. This process shows that the learning rate divided by the minibatch size plays a role analogous to temperature in statistical mechanics, and implies that SGD, including its stationary distribution, is largely invariant to changes in learning rate or batch size that leave its temperature constant. We show that increasing SGD temperature encourages the selection of local minima with lower curvature, and can enable better generalization. We provide experiments on CIFAR10 demonstrating the temperature invariance of SGD, improvement of the test loss as SGD temperature increases, and quantifying the impact of sampling versus domain shift in driving this effect. Finally, we present synthetic experiments showing how our theory applies in a simplified loss with two local minima.",0
"According to recent studies, altering the learning rate or reducing the minibatch size in stochastic gradient descent (SGD) could lead to better test set performance. This outcome is not surprising in models that have a loss function with multiple local minima. Our contribution is an analytical method inspired by Physics, which approximates the effects of the SGD learning rate and batch size on generalization. We examine test set performance in loss functions with multiple minima, which experience a shift between training and test data distributions, commonly due to sampling in practical applications. Our results indicate that the shift in local minima negatively impacts test performance by picking up curvature. Thus, generalization improves by selecting wide and/or little-shifted local minima. Specializing in SGD, we analyze its test performance under stationarity. Due to the infeasibility of obtaining its exact stationary distribution, we derive a Fokker-Planck approximation of SGD and obtain its stationary distribution. We observe that the learning rate divided by the minibatch size functions like temperature in statistical mechanics. This implies that SGD, including its stationary distribution, is mostly invariant to changes in learning rate or batch size, as long as its temperature remains the same. We demonstrate that increasing SGD temperature encourages the selection of local minima with lower curvature and improves generalization. Furthermore, we conduct experiments on CIFAR10 that illustrate the temperature invariance of SGD, the enhancement of test loss as SGD temperature increases, and the quantification of the impact of sampling versus domain shift in driving this effect. Finally, we present synthetic experiments that demonstrate how our theory applies in a simplified loss with two local minima.",1
"The bias-variance trade-off is a central concept in supervised learning. In classical statistics, increasing the complexity of a model (e.g., number of parameters) reduces bias but also increases variance. Until recently, it was commonly believed that optimal performance is achieved at intermediate model complexities which strike a balance between bias and variance. Modern Deep Learning methods flout this dogma, achieving state-of-the-art performance using ""over-parameterized models"" where the number of fit parameters is large enough to perfectly fit the training data. As a result, understanding bias and variance in over-parameterized models has emerged as a fundamental problem in machine learning. Here, we use methods from statistical physics to derive analytic expressions for bias and variance in two minimal models of over-parameterization (linear regression and two-layer neural networks with nonlinear data distributions), allowing us to disentangle properties stemming from the model architecture and random sampling of data. In both models, increasing the number of fit parameters leads to a phase transition where the training error goes to zero and the test error diverges as a result of the variance (while the bias remains finite). Beyond this threshold in the interpolation regime, the training error remains zero while the test error decreases. We also show that in contrast with classical intuition, over-parameterized models can overfit even in the absence of noise and exhibit bias even if the student and teacher models match. We synthesize these results to construct a holistic understanding of generalization error and the bias-variance trade-off in over-parameterized models and relate our results to random matrix theory.",0
"The trade-off between bias and variance is a critical concept in supervised learning. Traditionally, increasing model complexity, such as the number of parameters, reduced bias but increased variance. However, recent advances in Deep Learning challenge this notion by using ""over-parameterized models"" that perfectly fit the training data. Therefore, understanding bias and variance in over-parameterized models has become a vital issue in machine learning. By utilizing statistical physics methods, we derive analytical expressions for bias and variance in two minimal models of over-parameterization and distinguish features arising from model architecture and random data sampling. In both models, increasing the number of fit parameters results in a phase transition where the training error is zero, and the test error diverges due to variance, while bias remains constant. Moreover, we demonstrate that contrary to classical beliefs, over-parameterized models can overfit even without noise and exhibit bias despite the matching of student and teacher models. By combining these findings, we develop a comprehensive understanding of generalization error and the bias-variance trade-off in over-parameterized models and connect our results to random matrix theory.",1
"Neuroimaging biomarkers that distinguish between typical brain aging and Alzheimer's disease (AD) are valuable for determining how much each contributes to cognitive decline. Machine learning models can derive multi-variate brain change patterns related to the two processes, including the SPARE-AD (Spatial Patterns of Atrophy for Recognition of Alzheimer's Disease) and SPARE-BA (of Brain Aging) investigated herein. However, substantial overlap between brain regions affected in the two processes confounds measuring them independently. We present a methodology toward disentangling the two. T1-weighted MRI images of 4,054 participants (48-95 years) with AD, mild cognitive impairment (MCI), or cognitively normal (CN) diagnoses from the iSTAGING (Imaging-based coordinate SysTem for AGIng and NeurodeGenerative diseases) consortium were analyzed. First, a subset of AD patients and CN adults were selected based purely on clinical diagnoses to train SPARE-BA1 (regression of age using CN individuals) and SPARE-AD1 (classification of CN versus AD). Second, analogous groups were selected based on clinical and molecular markers to train SPARE-BA2 and SPARE-AD2: amyloid-positive (A+) AD continuum group (consisting of A+AD, A+MCI, and A+ and tau-positive CN individuals) and amyloid-negative (A-) CN group. Finally, the combined group of the AD continuum and A-/CN individuals was used to train SPARE-BA3, with the intention to estimate brain age regardless of AD-related brain changes. Disentangled SPARE models derived brain patterns that were more specific to the two types of the brain changes. Correlation between the SPARE-BA and SPARE-AD was significantly reduced. Correlation of disentangled SPARE-AD was non-inferior to the molecular measurements and to the number of APOE4 alleles, but was less to AD-related psychometric test scores, suggesting contribution of advanced brain aging to these scores.",0
"Distinguishing between typical brain aging and Alzheimer's disease (AD) using neuroimaging biomarkers is crucial for understanding their respective contributions to cognitive decline. Machine learning models can identify patterns of brain change related to each process, such as Spatial Patterns of Atrophy for Recognition of Alzheimer's Disease (SPARE-AD) and SPARE-BA (of Brain Aging). However, because there is significant overlap between the brain regions affected in both processes, it is difficult to measure them independently. To address this challenge, the authors developed a methodology to disentangle the two processes. They analyzed T1-weighted MRI images of 4,054 participants with AD, mild cognitive impairment (MCI), or cognitively normal (CN) diagnoses from the iSTAGING consortium. First, they trained SPARE-BA1 and SPARE-AD1 on a subset of AD patients and CN individuals based purely on clinical diagnoses. Then, they trained analogous groups based on both clinical and molecular markers to develop SPARE-BA2 and SPARE-AD2. Finally, they combined the AD continuum and A-/CN individuals to train SPARE-BA3, which estimated brain age regardless of AD-related brain changes. The disentangled SPARE models identified brain patterns specific to each process, reducing correlation between SPARE-BA and SPARE-AD. Disentangled SPARE-AD was comparable to molecular measurements and APOE4 alleles, but less correlated with AD-related psychometric test scores, indicating that advanced brain aging contributes to these scores.",1
"Recent studies have shown that it is possible to combine machine learning methods with data assimilation to reconstruct a dynamical system using only sparse and noisy observations of that system. The same approach can be used to correct the error of a knowledge-based model. The resulting surrogate model is hybrid, with a statistical part supplementing a physical part. In practice, the correction can be added as an integrated term (i.e. in the model resolvent) or directly inside the tendencies of the physical model. The resolvent correction is easy to implement. The tendency correction is more technical, in particular it requires the adjoint of the physical model, but also more flexible. We use the two-scale Lorenz model to compare the two methods. The accuracy in long-range forecast experiments is somewhat similar between the surrogate models using the resolvent correction and the tendency correction. By contrast, the surrogate models using the tendency correction significantly outperform the surrogate models using the resolvent correction in data assimilation experiments. Finally, we show that the tendency correction opens the possibility to make online model error correction, i.e. improving the model progressively as new observations become available. The resulting algorithm can be seen as a new formulation of weak-constraint 4D-Var. We compare online and offline learning using the same framework with the two-scale Lorenz system, and show that with online learning, it is possible to extract all the information from sparse and noisy observations.",0
"Recent research has demonstrated that combining machine learning techniques with data assimilation can be used to reconstruct a dynamic system using only limited and imprecise observations. This approach can also correct errors in a knowledge-based model, creating a hybrid model composed of both a statistical and physical component. The correction can be integrated within the model resolvent or directly within the tendencies of the physical model. The resolvent correction is straightforward to implement, while the tendency correction is more complex, requiring the adjoint of the physical model, but offers greater flexibility. By testing with the two-scale Lorenz model, the accuracy of long-range forecasting was similar between the surrogate models using resolvent and tendency correction. However, in data assimilation experiments, the surrogate models using the tendency correction outperformed those using the resolvent correction. Additionally, the tendency correction enables online model error correction, allowing for continuous improvement as new observations become available. This algorithm is a new formulation of weak-constraint 4D-Var. Comparing online and offline learning with the same framework using the two-scale Lorenz system revealed that online learning can extract all relevant information from limited and imprecise observations.",1
"Object manipulation from 3D visual inputs poses many challenges on building generalizable perception and policy models. However, 3D assets in existing benchmarks mostly lack the diversity of 3D shapes that align with real-world intra-class complexity in topology and geometry. Here we propose SAPIEN Manipulation Skill Benchmark (ManiSkill) to benchmark manipulation skills over diverse objects in a full-physics simulator. 3D assets in ManiSkill include large intra-class topological and geometric variations. Tasks are carefully chosen to cover distinct types of manipulation challenges. Latest progress in 3D vision also makes us believe that we should customize the benchmark so that the challenge is inviting to researchers working on 3D deep learning. To this end, we simulate a moving panoramic camera that returns ego-centric point clouds or RGB-D images. In addition, we would like ManiSkill to serve a broad set of researchers interested in manipulation research. Besides supporting the learning of policies from interactions, we also support learning-from-demonstrations (LfD) methods, by providing a large number of high-quality demonstrations (~36,000 successful trajectories, ~1.5M point cloud/RGB-D frames in total). We provide baselines using 3D deep learning and LfD algorithms. All code of our benchmark (simulator, environment, SDK, and baselines) is open-sourced, and a challenge facing interdisciplinary researchers will be held based on the benchmark.",0
"Developing perception and policy models for object manipulation using 3D visual inputs is challenging due to the lack of diversity in 3D shapes in existing benchmarks. These shapes do not align with the intra-class complexity found in real-world topology and geometry. In response, we introduce the SAPIEN Manipulation Skill Benchmark (ManiSkill). This benchmark includes a full-physics simulator with diverse 3D assets that exhibit large intra-class topological and geometric variations. The tasks are carefully chosen to cover various manipulation challenges. We believe that the latest advances in 3D vision require customization of the benchmark to invite researchers working on 3D deep learning. As such, we simulate a moving panoramic camera that returns ego-centric point clouds or RGB-D images. Furthermore, we aim to support a broad range of researchers interested in manipulation research by providing high-quality demonstrations and baselines using 3D deep learning and learning-from-demonstrations (LfD) algorithms. Our benchmark code, including the simulator, environment, SDK, and baselines, is open-source, and we will hold a challenge for interdisciplinary researchers based on the benchmark.",1
"Backdoor attacks embed hidden malicious behaviors into deep learning models, which only activate and cause misclassifications on model inputs containing a specific trigger. Existing works on backdoor attacks and defenses, however, mostly focus on digital attacks that use digitally generated patterns as triggers. A critical question remains unanswered: can backdoor attacks succeed using physical objects as triggers, thus making them a credible threat against deep learning systems in the real world? We conduct a detailed empirical study to explore this question for facial recognition, a critical deep learning task. Using seven physical objects as triggers, we collect a custom dataset of 3205 images of ten volunteers and use it to study the feasibility of physical backdoor attacks under a variety of real-world conditions. Our study reveals two key findings. First, physical backdoor attacks can be highly successful if they are carefully configured to overcome the constraints imposed by physical objects. In particular, the placement of successful triggers is largely constrained by the target model's dependence on key facial features. Second, four of today's state-of-the-art defenses against (digital) backdoors are ineffective against physical backdoors, because the use of physical objects breaks core assumptions used to construct these defenses. Our study confirms that (physical) backdoor attacks are not a hypothetical phenomenon but rather pose a serious real-world threat to critical classification tasks. We need new and more robust defenses against backdoors in the physical world.",0
"The embedding of hidden malicious behaviors into deep learning models, known as backdoor attacks, is only activated when a specific trigger is present in model inputs, causing misclassifications. While current research on backdoor attacks and defenses focuses on digital attacks using generated patterns, there is a crucial question regarding the feasibility of physical objects as triggers. This raises concerns about the credibility of backdoor attacks against deep learning systems in real-world scenarios. To explore this question, we conducted an empirical study on facial recognition, a critical deep learning task, using seven physical objects as triggers. Our study revealed that physical backdoor attacks can be highly successful if configured carefully to overcome constraints imposed by physical objects. Moreover, four state-of-the-art defenses against digital backdoors are ineffective against physical backdoors, as physical objects break core assumptions used to construct these defenses. These findings confirm that physical backdoor attacks pose a serious real-world threat to critical classification tasks, and we need new and robust defenses to deal with them.",1
"Quantum Machine Learning (QML) is considered to be one of the most promising applications of near term quantum devices. However, the optimization of quantum machine learning models presents numerous challenges arising from the imperfections of hardware and the fundamental obstacles in navigating an exponentially scaling Hilbert space. In this work, we evaluate the potential of contemporary methods in deep reinforcement learning to augment gradient based optimization routines in quantum variational circuits. We find that reinforcement learning augmented optimizers consistently outperform gradient descent in noisy environments. All code and pretrained weights are available to replicate the results or deploy the models at https://github.com/lockwo/rl_qvc_opt.",0
"Near term quantum devices hold great promise for Quantum Machine Learning (QML), but the optimization of QML models is a challenging task due to hardware imperfections and navigating an exponentially scaling Hilbert space. In this study, we explore the potential of deep reinforcement learning methods to enhance gradient-based optimization routines in quantum variational circuits. Our findings demonstrate that reinforcement learning augmented optimizers perform better than gradient descent in noisy environments. We provide access to the code and pretrained weights for replication of the results or deployment of the models at https://github.com/lockwo/rl_qvc_opt.",1
"We introduce in this work the normalizing field flows (NFF) for learning random fields from scattered measurements. More precisely, we construct a bijective transformation (a normalizing flow characterizing by neural networks) between a Gaussian random field with the Karhunen-Lo\`eve (KL) expansion structure and the target stochastic field, where the KL expansion coefficients and the invertible networks are trained by maximizing the sum of the log-likelihood on scattered measurements. This NFF model can be used to solve data-driven forward, inverse, and mixed forward/inverse stochastic partial differential equations in a unified framework. We demonstrate the capability of the proposed NFF model for learning Non Gaussian processes and different types of stochastic partial differential equations.",0
"In this study, we present the normalizing field flows (NFF) as a means of learning random fields from sparsely distributed measurements. Our approach involves creating a bijective transformation, which is characterized by neural networks and normalizing flow, between a Gaussian random field that follows the Karhunen-Lo\`eve (KL) expansion structure and the target stochastic field. To achieve this, we train the KL expansion coefficients and the invertible networks by maximizing the sum of the log-likelihood on scattered measurements. Using this NFF model, we can solve data-driven forward, inverse, and mixed forward/inverse stochastic partial differential equations in a unified framework. Additionally, we demonstrate the effectiveness of the proposed NFF model in learning Non Gaussian processes and various types of stochastic partial differential equations.",1
"We train an object detector built from convolutional neural networks to count interference fringes in elliptical antinode regions in frames of high-speed video recordings of transient oscillations in Caribbean steelpan drums illuminated by electronic speckle pattern interferometry (ESPI). The annotations provided by our model aim to contribute to the understanding of time-dependent behavior in such drums by tracking the development of sympathetic vibration modes. The system is trained on a dataset of crowdsourced human-annotated images obtained from the Zooniverse Steelpan Vibrations Project. Due to the small number of human-annotated images and the ambiguity of the annotation task, we also evaluate the model on a large corpus of synthetic images whose properties have been matched to the real images by style transfer using a Generative Adversarial Network. Applying the model to thousands of unlabeled video frames, we measure oscillations consistent with audio recordings of these drum strikes. One unanticipated result is that sympathetic oscillations of higher-octave notes significantly precede the rise in sound intensity of the corresponding second harmonic tones; the mechanism responsible for this remains unidentified. This paper primarily concerns the development of the predictive model; further exploration of the steelpan images and deeper physical insights await its further application.",0
"We use convolutional neural networks to build an object detector that counts interference fringes in elliptical antinode regions of high-speed video recordings of transient oscillations in Caribbean steelpan drums illuminated by electronic speckle pattern interferometry (ESPI). Our aim is to understand time-dependent behavior in these drums by tracking the development of sympathetic vibration modes through the annotations provided by our model. To train the system, we use a dataset of human-annotated images obtained from the Zooniverse Steelpan Vibrations Project. However, due to the limited number of human-annotated images and the ambiguity of the annotation task, we also evaluate the model on a large corpus of synthetic images. Using this model, we measure oscillations consistent with audio recordings of the drum strikes in thousands of unlabeled video frames. One interesting finding is that the rise in sound intensity of the corresponding second harmonic tones lags significantly behind the sympathetic oscillations of higher-octave notes, and the mechanism behind this remains unknown. This paper focuses on the development of the predictive model, and further exploration of the steelpan images and deeper physical insights require its further application.",1
"In physical design, human designers typically place macros via trial and error, which is a Markov decision process. Reinforcement learning (RL) methods have demonstrated superhuman performance on the macro placement. In this paper, we propose an extension to this prior work (Mirhoseini et al., 2020). We first describe the details of the policy and value network architecture. We replace the force-directed method with DREAMPlace for placing standard cells in the RL environment. We also compare our improved method with other academic placers on public benchmarks.",0
"Traditionally, human designers use a Markov decision process to position macros in physical design through trial and error. However, reinforcement learning (RL) methods have shown exceptional performance in macro placement, surpassing human capabilities. Our paper introduces an extension to the previous work done by Mirhoseini et al. (2020), outlining the policy and value network architecture in detail. We enhance the method by substituting the force-directed approach with DREAMPlace for standard cell placement in the RL environment. Additionally, we compare our upgraded technique to other academic placers on public benchmarks.",1
"Attention mechanisms are developing into a viable alternative to convolutional layers as elementary building block of NNs. Their main advantage is that they are not restricted to capture local dependencies in the input, but can draw arbitrary connections. This unprecedented capability coincides with the long-standing problem of modeling global atomic interactions in molecular force fields and other many-body problems. In its original formulation, however, attention is not applicable to the continuous domains in which the atoms live. For this purpose we propose a variant to describe geometric relations for arbitrary atomic configurations in Euclidean space that also respects all relevant physical symmetries. We furthermore demonstrate, how the successive application of our learned attention matrices effectively translates the molecular geometry into a set of individual atomic contributions on-the-fly.",0
"Alternative building blocks for NNs are being developed with attention mechanisms, which have the advantage of not being limited to capturing only local dependencies in the input, but can establish arbitrary connections. This exceptional capability addresses the challenge of modeling global atomic interactions in molecular force fields and other many-body problems. However, the original formulation of attention does not work for the continuous domains in which atoms exist. To overcome this, we propose a variant that describes geometric relations for arbitrary atomic configurations in Euclidean space while respecting all relevant physical symmetries. Additionally, we demonstrate how our learned attention matrices can effectively translate molecular geometry into individual atomic contributions on-the-fly through successive application.",1
"Wind farm design primarily depends on the variability of the wind turbine wake flows to the atmospheric wind conditions, and the interaction between wakes. Physics-based models that capture the wake flow-field with high-fidelity are computationally very expensive to perform layout optimization of wind farms, and, thus, data-driven reduced order models can represent an efficient alternative for simulating wind farms. In this work, we use real-world light detection and ranging (LiDAR) measurements of wind-turbine wakes to construct predictive surrogate models using machine learning. Specifically, we first demonstrate the use of deep autoencoders to find a low-dimensional \emph{latent} space that gives a computationally tractable approximation of the wake LiDAR measurements. Then, we learn the mapping between the parameter space and the (latent space) wake flow-fields using a deep neural network. Additionally, we also demonstrate the use of a probabilistic machine learning technique, namely, Gaussian process modeling, to learn the parameter-space-latent-space mapping in addition to the epistemic and aleatoric uncertainty in the data. Finally, to cope with training large datasets, we demonstrate the use of variational Gaussian process models that provide a tractable alternative to the conventional Gaussian process models for large datasets. Furthermore, we introduce the use of active learning to adaptively build and improve a conventional Gaussian process model predictive capability. Overall, we find that our approach provides accurate approximations of the wind-turbine wake flow field that can be queried at an orders-of-magnitude cheaper cost than those generated with high-fidelity physics-based simulations.",0
"Wind farm design relies heavily on the variability of wind turbine wake flows in relation to atmospheric wind conditions and the interaction between wakes. However, physics-based models that accurately capture wake flow-fields are computationally expensive for wind farm layout optimization. An alternative approach is to use data-driven reduced order models, which can efficiently simulate wind farms. In this study, we utilized real-world light detection and ranging (LiDAR) measurements of wind turbine wakes to construct predictive surrogate models using machine learning. Specifically, we used deep autoencoders to find a low-dimensional latent space that approximates the wake LiDAR measurements. We then used a deep neural network to learn the mapping between the parameter space and the wake flow-fields in the latent space. Additionally, we employed Gaussian process modeling to learn the parameter-space-latent-space mapping and account for the uncertainty in the data. To handle large datasets, we used variational Gaussian process models and active learning to improve our predictive capabilities. Overall, our approach provides accurate approximations of wind-turbine wake flow fields at a significantly lower cost than high-fidelity physics-based simulations.",1
"Quantum physics experiments produce interesting phenomena such as interference or entanglement, which is a core property of numerous future quantum technologies. The complex relationship between a quantum experiment's structure and its entanglement properties is essential to fundamental research in quantum optics but is difficult to intuitively understand. We present the first deep generative model of quantum optics experiments where a variational autoencoder (QOVAE) is trained on a dataset of experimental setups. In a series of computational experiments, we investigate the learned representation of the QOVAE and its internal understanding of the quantum optics world. We demonstrate that the QOVAE learns an intrepretable representation of quantum optics experiments and the relationship between experiment structure and entanglement. We show the QOVAE is able to generate novel experiments for highly entangled quantum states with specific distributions that match its training data. Importantly, we are able to fully interpret how the QOVAE structures its latent space, finding curious patterns that we can entirely explain in terms of quantum physics. The results demonstrate how we can successfully use and understand the internal representations of deep generative models in a complex scientific domain. The QOVAE and the insights from our investigations can be immediately applied to other physical systems throughout fundamental scientific research.",0
"Experiments in quantum physics exhibit fascinating phenomena, including interference and entanglement, which are critical properties of many upcoming quantum technologies. Although the intricate relationship between a quantum experiment's structure and its entanglement properties is fundamental to quantum optics research, it is not easy to understand intuitively. This study introduces the first deep generative model of quantum optics experiments, using a variational autoencoder (QOVAE) trained on a dataset of experimental setups. Through a series of computational experiments, the researchers explore the QOVAE's learned representation and its internal understanding of quantum optics. The findings show that the QOVAE can learn an interpretable representation of quantum optics experiments, including the relationship between experiment structure and entanglement. The QOVAE can also generate novel experiments for highly entangled quantum states with specific distributions that match its training data. Furthermore, the researchers can interpret how the QOVAE structures its latent space, uncovering intriguing patterns that can be entirely explained in terms of quantum physics. These results demonstrate the potential of using and understanding deep generative models' internal representations in complex scientific domains. The QOVAE and the insights from this research can be applied immediately to other physical systems in fundamental scientific research.",1
"Considerable research has been devoted to deep learning-based predictive models for system prognostics and health management in the reliability and safety community. However, there is limited study on the utilization of deep learning for system reliability assessment. This paper aims to bridge this gap and explore this new interface between deep learning and system reliability assessment by exploiting the recent advances of physics-informed deep learning. Particularly, we present an approach to frame system reliability assessment in the context of physics-informed deep learning and discuss the potential value of physics-informed generative adversarial networks for the uncertainty quantification and measurement data incorporation in system reliability assessment. The proposed approach is demonstrated by three numerical examples involving a dual-processor computing system. The results indicate the potential value of physics-informed deep learning to alleviate computational challenges and combine measurement data and mathematical models for system reliability assessment.",0
"There has been a significant amount of research conducted on deep learning-based predictive models for system prognostics and health management within the reliability and safety community. However, the utilization of deep learning for system reliability assessment has received limited attention. This paper seeks to address this gap by exploring the intersection between deep learning and system reliability assessment through the use of physics-informed deep learning. Specifically, we present an approach that frames system reliability assessment within the context of physics-informed deep learning and discuss the potential use of physics-informed generative adversarial networks for uncertainty quantification and the incorporation of measurement data in system reliability assessment. We demonstrate the proposed approach with three numerical examples involving a dual-processor computing system. The results suggest that physics-informed deep learning has the potential to address computational challenges and integrate measurement data and mathematical models for system reliability assessment.",1
"Data augmentation is a widely used technique in classification to increase data used in training. It improves generalization and reduces amount of annotated human activity data needed for training which reduces labour and time needed with the dataset. Sensor time-series data, unlike images, cannot be augmented by computationally simple transformation algorithms. State of the art models like Recurrent Generative Adversarial Networks (RGAN) are used to generate realistic synthetic data. In this paper, transformer based generative adversarial networks which have global attention on data, are compared on PAMAP2 and Real World Human Activity Recognition data sets with RGAN. The newer approach provides improvements in time and savings in computational resources needed for data augmentation than previous approach.",0
"The method of data augmentation is frequently employed in classification to expand the amount of data available for training. It has been shown to enhance generalization and minimize the quantity of annotated human activity data required, thus reducing the labor and time required to work with the dataset. Unlike images, computational methods that are simple transformations cannot be used to augment sensor time-series data. Advanced models such as Recurrent Generative Adversarial Networks (RGAN) are employed to produce realistic synthetic data. This study compares transformer-based generative adversarial networks, which employ global attention on data, with RGAN on PAMAP2 and Real World Human Activity Recognition datasets. The latest approach offers improvements in time and savings in computational resources needed for data augmentation over the previous method.",1
"In recent years, convolutional neural networks (CNNs) have experienced an increasing interest for their ability to perform fast approximation of effective hydrodynamic parameters in porous media research and applications. This paper presents a novel methodology for permeability prediction from micro-CT scans of geological rock samples. The training data set for CNNs dedicated to permeability prediction consists of permeability labels that are typically generated by classical lattice Boltzmann methods (LBM) that simulate the flow through the pore space of the segmented image data. We instead perform direct numerical simulation (DNS) by solving the stationary Stokes equation in an efficient and distributed-parallel manner. As such, we circumvent the convergence issues of LBM that frequently are observed on complex pore geometries, and therefore, improve on the generality and accuracy of our training data set. Using the DNS-computed permeabilities, a physics-informed CNN PhyCNN) is trained by additionally providing a tailored characteristic quantity of the pore space. More precisely, by exploiting the connection to flow problems on a graph representation of the pore space, additional information about confined structures is provided to the network in terms of the maximum flow value, which is the key innovative component of our workflow. As a result, unprecedented prediction accuracy and robustness are observed for a variety of sandstone samples from archetypal rock formations.",0
"Convolutional neural networks (CNNs) have become increasingly popular for approximating hydrodynamic parameters in porous media research and applications. This study introduces a new approach for predicting permeability from micro-CT scans of geological rock samples. Instead of generating permeability labels using classical lattice Boltzmann methods (LBM), which can be problematic for complex pore geometries, the authors utilize direct numerical simulation (DNS) to solve the stationary Stokes equation in an efficient and distributed-parallel manner. This results in a more accurate and general training data set for the CNNs. To further improve accuracy, the authors incorporate a physics-informed CNN (PhyCNN) that includes additional information about confined structures in the pore space. This is achieved by exploiting the connection to flow problems on a graph representation of the pore space and providing the network with the maximum flow value. This approach yields unprecedented prediction accuracy and robustness for a variety of sandstone samples from typical rock formations.",1
"Wildfires have increased in frequency and severity over the past two decades, especially in the Western United States. Beyond physical infrastructure damage caused by these wildfire events, researchers have increasingly identified harmful impacts of particulate matter generated by wildfire smoke on respiratory, cardiovascular, and cognitive health. This inference is difficult due to the spatial and temporal uncertainty regarding how much particulate matter is specifically attributable to wildfire smoke. One factor contributing to this challenge is the reliance on manually drawn smoke plume annotations, which are often noisy representations limited to the United States. This work uses deep convolutional neural networks to segment smoke plumes from geostationary satellite imagery. We compare the performance of predicted plume segmentations versus the noisy annotations using causal inference methods to estimate the amount of variation each explains in Environmental Protection Agency (EPA) measured surface level particulate matter <2.5um in diameter ($\textrm{PM}_{2.5}$).",0
"Over the past 20 years, wildfires have become more frequent and severe, particularly in the Western US. In addition to physical infrastructure damage, research has shown that the particulate matter generated by wildfire smoke has harmful effects on respiratory, cardiovascular, and cognitive health. However, determining the specific amount of particulate matter caused by wildfire smoke is challenging due to spatial and temporal uncertainty. This is partly due to the reliance on manually drawn smoke plume annotations, which are often limited to the US and can be noisy. This study uses deep convolutional neural networks to segment smoke plumes from geostationary satellite imagery and compares the performance of predicted plume segmentations with noisy annotations. Using causal inference methods, the study estimates the amount of variation each explains in Environmental Protection Agency (EPA) measured surface level particulate matter with a diameter of less than 2.5 micrometers ($\textrm{PM}_{2.5}$).",1
"Stochastic differential equations (SDEs) are used to describe a wide variety of complex stochastic dynamical systems. Learning the hidden physics within SDEs is crucial for unraveling fundamental understanding of the stochastic and nonlinear behavior of these systems. We propose a flexible and scalable framework for training deep neural networks to learn constitutive equations that represent hidden physics within SDEs. The proposed stochastic physics-informed neural network framework (SPINN) relies on uncertainty propagation and moment-matching techniques along with state-of-the-art deep learning strategies. SPINN first propagates stochasticity through the known structure of the SDE (i.e., the known physics) to predict the time evolution of statistical moments of the stochastic states. SPINN learns (deep) neural network representations of the hidden physics by matching the predicted moments to those estimated from data. Recent advances in automatic differentiation and mini-batch gradient descent are leveraged to establish the unknown parameters of the neural networks. We demonstrate SPINN on three benchmark in-silico case studies and analyze the framework's robustness and numerical stability. SPINN provides a promising new direction for systematically unraveling the hidden physics of multivariate stochastic dynamical systems with multiplicative noise.",0
"Stochastic differential equations (SDEs) have a wide range of applications in describing complex stochastic dynamical systems. Understanding the hidden physics within SDEs is important for comprehending the stochastic and nonlinear behavior of these systems. Our proposed framework, the stochastic physics-informed neural network (SPINN), is both flexible and scalable for training deep neural networks to learn the constitutive equations that represent the hidden physics within SDEs. SPINN relies on uncertainty propagation and moment-matching techniques, together with state-of-the-art deep learning strategies. SPINN first propagates stochasticity through the known structure of the SDE, predicting the time evolution of statistical moments of the stochastic states. Then, SPINN learns neural network representations of the hidden physics by matching the predicted moments to those estimated from data. We leverage recent advances in automatic differentiation and mini-batch gradient descent to establish the unknown parameters of the neural networks. We demonstrate the robustness and numerical stability of SPINN on three benchmark in-silico case studies. SPINN is a promising new approach for systematically uncovering the hidden physics of multivariate stochastic dynamical systems with multiplicative noise.",1
"Computer vision and machine learning tools offer an exciting new way for automatically analyzing and categorizing information from complex computer simulations. Here we design an ensemble machine learning framework that can independently and robustly categorize and dissect simulation data output contents of turbulent flow patterns into distinct structure catalogues. The segmentation is performed using an unsupervised clustering algorithm, which segments physical structures by grouping together similar pixels in simulation images. The accuracy and robustness of the resulting segment region boundaries are enhanced by combining information from multiple simultaneously-evaluated clustering operations. The stacking of object segmentation evaluations is performed using image mask combination operations. This statistically-combined ensemble (SCE) of different cluster masks allows us to construct cluster reliability metrics for each pixel and for the associated segments without any prior user input. By comparing the similarity of different cluster occurrences in the ensemble, we can also assess the optimal number of clusters needed to describe the data. Furthermore, by relying on ensemble-averaged spatial segment region boundaries, the SCE method enables reconstruction of more accurate and robust region of interest (ROI) boundaries for the different image data clusters. We apply the SCE algorithm to 2-dimensional simulation data snapshots of magnetically-dominated fully-kinetic turbulent plasma flows where accurate ROI boundaries are needed for geometrical measurements of intermittent flow structures known as current sheets.",0
"The use of computer vision and machine learning tools offers an exciting opportunity to automatically analyze and categorize information from complex computer simulations. In this study, we have designed an ensemble machine learning framework that can independently and robustly categorize and analyze simulation data output contents of turbulent flow patterns into distinct structure catalogues. We used an unsupervised clustering algorithm to segment physical structures by grouping together similar pixels in simulation images. The accuracy and robustness of the resulting segment region boundaries were improved by combining information from multiple simultaneously-evaluated clustering operations using image mask combination operations. This statistically-combined ensemble (SCE) of different cluster masks allowed us to construct cluster reliability metrics for each pixel and segment without any prior user input. By comparing the similarity of different cluster occurrences in the ensemble, we could also assess the optimal number of clusters required to describe the data. By relying on ensemble-averaged spatial segment region boundaries, the SCE method enabled the reconstruction of more accurate and robust region of interest (ROI) boundaries for the different image data clusters. We applied the SCE algorithm to 2-dimensional simulation data snapshots of magnetically-dominated fully-kinetic turbulent plasma flows, where accurate ROI boundaries were necessary for geometrical measurements of intermittent flow structures known as current sheets.",1
"Recent work in scientific machine learning has developed so-called physics-informed neural network (PINN) models. The typical approach is to incorporate physical domain knowledge as soft constraints on an empirical loss function and use existing machine learning methodologies to train the model. We demonstrate that, while existing PINN methodologies can learn good models for relatively trivial problems, they can easily fail to learn relevant physical phenomena even for simple PDEs. In particular, we analyze several distinct situations of widespread physical interest, including learning differential equations with convection, reaction, and diffusion operators. We provide evidence that the soft regularization in PINNs, which involves differential operators, can introduce a number of subtle problems, including making the problem ill-conditioned. Importantly, we show that these possible failure modes are not due to the lack of expressivity in the NN architecture, but that the PINN's setup makes the loss landscape very hard to optimize. We then describe two promising solutions to address these failure modes. The first approach is to use curriculum regularization, where the PINN's loss term starts from a simple PDE regularization, and becomes progressively more complex as the NN gets trained. The second approach is to pose the problem as a sequence-to-sequence learning task, rather than learning to predict the entire space-time at once. Extensive testing shows that we can achieve up to 1-2 orders of magnitude lower error with these methods as compared to regular PINN training.",0
"Scientific machine learning has developed physics-informed neural network (PINN) models that incorporate physical domain knowledge as soft constraints on an empirical loss function. Although existing PINN methodologies can learn good models for simple problems, they can fail to learn relevant physical phenomena for more complex ones, such as differential equations with convection, reaction, and diffusion operators. The soft regularization in PINNs, which involves differential operators, can introduce subtle problems that make the problem ill-conditioned. However, these possible failure modes are not due to the lack of expressivity in the NN architecture but rather the PINN's setup, which makes the loss landscape challenging to optimize. To address these issues, we propose two solutions: curriculum regularization, where the PINN's loss term starts from a simple PDE regularization and becomes increasingly complex as the NN is trained, and posing the problem as a sequence-to-sequence learning task. Our extensive testing shows that these methods can achieve up to 1-2 orders of magnitude lower error compared to regular PINN training.",1
"Reinforcement learning has been found useful in solving optimal power flow (OPF) problems in electric power distribution systems. However, the use of largely model-free reinforcement learning algorithms that completely ignore the physics-based modeling of the power grid compromises the optimizer performance and poses scalability challenges. This paper proposes a novel approach to synergistically combine the physics-based models with learning-based algorithms using imitation learning to solve distribution-level OPF problems. Specifically, we propose imitation learning based improvements in deep reinforcement learning (DRL) methods to solve the OPF problem for a specific case of battery storage dispatch in the power distribution systems. The proposed imitation learning algorithm uses the approximate optimal solutions obtained from a linearized model-based OPF solver to provide a good initial policy for the DRL algorithms while improving the training efficiency. The effectiveness of the proposed approach is demonstrated using IEEE 34-bus and 123-bus distribution feeders with numerous distribution-level battery storage systems.",0
"The efficacy of reinforcement learning in solving optimal power flow (OPF) issues in electric power distribution systems has been established. Nonetheless, relying solely on model-free algorithms that disregard the power grid's physics-based modeling may weaken optimizer performance and create scalability obstacles. This article puts forth a new strategy that blends physics-based models with learning-based algorithms utilizing imitation learning to tackle distribution-level OPF problems. Specifically, the authors suggest improving deep reinforcement learning (DRL) methods through imitation learning to solve the OPF problem for battery storage dispatch in power distribution systems. The suggested imitation learning approach uses near-optimal solutions achieved from a linearized model-based OPF solver to create a strong initial policy for DRL algorithms while increasing training efficiency. The authors demonstrate the effectiveness of their approach using IEEE 34-bus and 123-bus distribution feeders with multiple distribution-level battery storage systems.",1
"In this paper, the aim is multi-illumination color constancy. However, most of the existing color constancy methods are designed for single light sources. Furthermore, datasets for learning multiple illumination color constancy are largely missing. We propose a seed (physics driven) based multi-illumination color constancy method. GANs are exploited to model the illumination estimation problem as an image-to-image domain translation problem. Additionally, a novel multi-illumination data augmentation method is proposed. Experiments on single and multi-illumination datasets show that our methods outperform sota methods.",0
"This paper focuses on achieving multi-illumination color constancy, which poses a challenge as most existing methods are designed for single light sources and there is a lack of datasets for learning multi-illumination color constancy. To address this, we present a physics-driven approach based on seed and utilize GANs to model the illumination estimation problem as an image-to-image domain translation task. We also introduce a new multi-illumination data augmentation method. Our experiments on both single and multi-illumination datasets demonstrate that our methods outperform state-of-the-art techniques.",1
"The implementation of current deep learning training algorithms is power-hungry, owing to data transfer between memory and logic units. Oxide-based RRAMs are outstanding candidates to implement in-memory computing, which is less power-intensive. Their weak RESET regime, is particularly attractive for learning, as it allows tuning the resistance of the devices with remarkable endurance. However, the resistive change behavior in this regime suffers many fluctuations and is particularly challenging to model, especially in a way compatible with tools used for simulating deep learning. In this work, we present a model of the weak RESET process in hafnium oxide RRAM and integrate this model within the PyTorch deep learning framework. Validated on experiments on a hybrid CMOS/RRAM technology, our model reproduces both the noisy progressive behavior and the device-to-device (D2D) variability. We use this tool to train Binarized Neural Networks for the MNIST handwritten digit recognition task and the CIFAR-10 object classification task. We simulate our model with and without various aspects of device imperfections to understand their impact on the training process and identify that the D2D variability is the most detrimental aspect. The framework can be used in the same manner for other types of memories to identify the device imperfections that cause the most degradation, which can, in turn, be used to optimize the devices to reduce the impact of these imperfections.",0
"The current deep learning training algorithms consume a lot of power due to the need to transfer data between memory and logic units. To address this issue, oxide-based RRAMs are promising alternatives that can be used for in-memory computing, which is less energy-intensive. The weak RESET regime of RRAMs is particularly attractive for learning because it allows for the tuning of device resistance with remarkable endurance. However, the resistive change behavior in this regime is prone to fluctuations, making it difficult to model, especially in a manner compatible with deep learning simulation tools. In this study, we present a model of the weak RESET process in hafnium oxide RRAM and integrate it into the PyTorch deep learning framework. Our model is validated through experiments on hybrid CMOS/RRAM technology and reproduces both the noisy progressive behavior and device-to-device variability. We use this model to train Binarized Neural Networks for the MNIST handwritten digit recognition task and the CIFAR-10 object classification task. By simulating our model with and without various device imperfections, we identify that D2D variability has the most significant impact on the training process. This model can be used to optimize other types of memories by identifying the device imperfections that cause the most degradation, reducing their impact on the performance of the devices.",1
"With many frameworks based on message passing neural networks proposed to predict molecular and bulk properties, machine learning methods have tremendously shifted the paradigms of computational sciences underpinning physics, material science, chemistry, and biology. While existing machine learning models have yielded superior performances in many occasions, most of them model and process molecular systems in terms of homogeneous graph, which severely limits the expressive power for representing diverse interactions. In practice, graph data with multiple node and edge types is ubiquitous and more appropriate for molecular systems. Thus, we propose the heterogeneous relational message passing network (HermNet), an end-to-end heterogeneous graph neural networks, to efficiently express multiple interactions in a single model with {\it ab initio} accuracy. HermNet performs impressively against many top-performing models on both molecular and extended systems. Specifically, HermNet outperforms other tested models in nearly 75\%, 83\% and 94\% of tasks on MD17, QM9 and extended systems datasets, respectively. Finally, we elucidate how the design of HermNet is compatible with quantum mechanics from the perspective of the density functional theory. Besides, HermNet is a universal framework, whose sub-networks could be replaced by other advanced models.",0
"The introduction of machine learning methods has had a significant impact on various fields such as physics, material science, chemistry, and biology. Many frameworks based on message passing neural networks have been developed to predict molecular and bulk properties, resulting in superior performance in numerous instances. However, these models often use a homogeneous graph to model and process molecular systems, which limits their ability to represent diverse interactions adequately. In contrast, heterogeneous graph data with multiple node and edge types is more appropriate for molecular systems. To address this limitation, we propose the use of the heterogeneous relational message passing network (HermNet), an end-to-end graph neural network that can efficiently express multiple interactions in a single model with high accuracy. HermNet outperforms other tested models in the majority of tasks on MD17, QM9, and extended systems datasets. Additionally, we demonstrate how HermNet's design is compatible with quantum mechanics based on the density functional theory. HermNet is a universal framework that allows for the replacement of its sub-networks with other advanced models.",1
"We present a supervised learning method to learn the propagator map of a dynamical system from partial and noisy observations. In our computationally cheap and easy-to-implement framework a neural network consisting of random feature maps is trained sequentially by incoming observations within a data assimilation procedure. By employing Takens' embedding theorem, the network is trained on delay coordinates. We show that the combination of random feature maps and data assimilation, called RAFDA, outperforms standard random feature maps for which the dynamics is learned using batch data.",0
"A technique for supervised learning is introduced to acquire the propagator map of a dynamic system from incomplete and imprecise observations. This process utilizes a neural network constructed from random feature maps in a cost-effective and straightforward structure. The network is trained sequentially through a data assimilation strategy using delay coordinates based on Takens' embedding theorem. The approach, named RAFDA, shows superior performance compared to the conventional random feature map approach that learns the dynamics with batch data.",1
"For real-life nonlinear systems, the exact form of nonlinearity is often not known and the known governing equations are often based on certain assumptions and approximations. Such representation introduced model-form error into the system. In this paper, we propose a novel gray-box modeling approach that not only identifies the model-form error but also utilizes it to improve the predictive capability of the known but approximate governing equation. The primary idea is to treat the unknown model-form error as a residual force and estimate it using duel Bayesian filter based joint input-state estimation algorithms. For improving the predictive capability of the underlying physics, we first use machine learning algorithm to learn a mapping between the estimated state and the input (model-form error) and then introduce it into the governing equation as an additional term. This helps in improving the predictive capability of the governing physics and allows the model to generalize to unseen environment. Although in theory, any machine learning algorithm can be used within the proposed framework, we use Gaussian process in this work. To test the performance of proposed framework, case studies discussing four different dynamical systems are discussed; results for which indicate that the framework is applicable to a wide variety of systems and can produce reliable estimates of original system's states.",0
"In real-life nonlinear systems, the precise nature of nonlinearity is often ambiguous and the governing equations are frequently based on certain approximations and assumptions, leading to model-form error. This paper presents a new gray-box modeling technique that not only identifies the model-form error, but also employs it to enhance the predictive capability of the known approximate governing equation. The main approach involves treating the unknown model-form error as a residual force and estimating it using dual Bayesian filter-based joint input-state estimation algorithms. To enhance the predictive capability of the underlying physics, a machine learning algorithm is initially utilized to learn a mapping between the estimated state and the input (model-form error), which is then included into the governing equation as an additional term. This improves the predictive power of the governing physics and enables the model to generalize to unseen environments. Although any machine learning algorithm can theoretically be used in the proposed framework, the Gaussian process is used in this study. Four different dynamical systems are discussed to evaluate the proposed framework's performance, and results show that it is applicable to a wide range of systems and can provide reliable estimates of the original system's states.",1
"We introduce RP2K, a new large-scale retail product dataset for fine-grained image classification. Unlike previous datasets focusing on relatively few products, we collect more than 500,000 images of retail products on shelves belonging to 2000 different products. Our dataset aims to advance the research in retail object recognition, which has massive applications such as automatic shelf auditing and image-based product information retrieval. Our dataset enjoys following properties: (1) It is by far the largest scale dataset in terms of product categories. (2) All images are captured manually in physical retail stores with natural lightings, matching the scenario of real applications. (3) We provide rich annotations to each object, including the sizes, shapes and flavors/scents. We believe our dataset could benefit both computer vision research and retail industry. Our dataset is publicly available at https://www.pinlandata.com/rp2k_dataset.",0
"RP2K is a novel dataset for fine-grained image classification in the retail industry. Unlike previous datasets that focused on a limited number of products, RP2K contains over 500,000 images of 2000 different products displayed on shelves. Our goal is to advance research in retail object recognition, which has numerous applications such as automatic shelf auditing and image-based product information retrieval. RP2K is unique in three ways: (1) it is the largest dataset with the most number of product categories, (2) all images were manually captured in physical retail stores with natural lighting, simulating real-world scenarios, and (3) we provide detailed annotations for each object, including size, shape, and flavor/scent. We believe our dataset can benefit both computer vision research and the retail industry. RP2K is available for public use at https://www.pinlandata.com/rp2k_dataset.",1
"Machine Learning (ML) based algorithms have found significant impact in many fields of engineering and sciences, where datasets are available from experiments and high fidelity numerical simulations. Those datasets are generally utilized in a machine learning model to extract information about the underlying physics and derive functional relationships mapping input variables to target quantities of interest. Commonplace machine learning algorithms utilized in Scientific Machine Learning (SciML) include neural networks, regression trees, random forests, support vector machines, etc. The focus of this article is to review the applications of ML in naval architecture, ocean, and marine engineering problems; and identify priority directions of research. We discuss the applications of machine learning algorithms for different problems such as wave height prediction, calculation of wind loads on ships, damage detection of offshore platforms, calculation of ship added resistance, and various other applications in coastal and marine environments. The details of the data sets including the source of data-sets utilized in the ML model development are included. The features used as the inputs to the ML models are presented in detail and finally, the methods employed in optimization of the ML models were also discussed. Based on this comprehensive analysis we point out future directions of research that may be fruitful for the application of ML to the ocean and marine engineering problems.",0
"Machine Learning algorithms have made a significant impact in various fields of engineering and sciences. These algorithms are used to extract information from datasets obtained from experiments and high fidelity numerical simulations, to derive functional relationships between input and target variables. Popular algorithms used in Scientific Machine Learning (SciML) include regression trees, neural networks, random forests, and support vector machines. This article focuses on reviewing the applications of ML in naval architecture, ocean, and marine engineering problems, and identifying priority research areas. The discussed applications include wave height prediction, wind loads on ships, offshore platform damage detection, ship added resistance calculation, and various other coastal and marine environment applications. The article provides details about the data sets, input features, and optimization methods used in the development of the ML models. Based on this analysis, future research directions to apply ML to ocean and marine engineering problems are identified.",1
"An optical neural network (ONN) is a promising system due to its high-speed and low-power operation. Its linear unit performs a multiplication of an input vector and a weight matrix in optical analog circuits. Among them, a circuit with a multiple-layered structure of programmable Mach-Zehnder interferometers (MZIs) can realize a specific class of unitary matrices with a limited number of MZIs as its weight matrix. The circuit is effective for balancing the number of programmable MZIs and ONN performance. However, it takes a lot of time to learn MZI parameters of the circuit with a conventional automatic differentiation (AD), which machine learning platforms are equipped with. To solve the time-consuming problem, we propose an acceleration method for learning MZI parameters. We create customized complex-valued derivatives for an MZI, exploiting Wirtinger derivatives and a chain rule. They are incorporated into our newly developed function module implemented in C++ to collectively calculate their values in a multi-layered structure. Our method is simple, fast, and versatile as well as compatible with the conventional AD. We demonstrate that our method works 20 times faster than the conventional AD when a pixel-by-pixel MNIST task is performed in a complex-valued recurrent neural network with an MZI-based hidden unit.",0
"The high-speed and low-power operation of an optical neural network (ONN) make it a promising system. Its linear unit utilizes optical analog circuits to multiply an input vector and a weight matrix. A circuit with a multiple-layered structure of programmable Mach-Zehnder interferometers (MZIs) can achieve a specific class of unitary matrices with a reduced number of MZIs. This circuit balances the number of programmable MZIs and ONN performance, but the learning of MZI parameters using conventional automatic differentiation (AD) is time-consuming. To overcome this issue, we propose an acceleration method that employs customized complex-valued derivatives for an MZI, using Wirtinger derivatives and a chain rule. Our approach is compatible with the conventional AD, fast, versatile, and simple. We demonstrate that our method can perform a pixel-by-pixel MNIST task in a complex-valued recurrent neural network with an MZI-based hidden unit 20 times faster than conventional AD.",1
"Deep face recognition (FR) has achieved significantly high accuracy on several challenging datasets and fosters successful real-world applications, even showing high robustness to the illumination variation that is usually regarded as a main threat to the FR system. However, in the real world, illumination variation caused by diverse lighting conditions cannot be fully covered by the limited face dataset. In this paper, we study the threat of lighting against FR from a new angle, i.e., adversarial attack, and identify a new task, i.e., adversarial relighting. Given a face image, adversarial relighting aims to produce a naturally relighted counterpart while fooling the state-of-the-art deep FR methods. To this end, we first propose the physical model-based adversarial relighting attack (ARA) denoted as albedo-quotient-based adversarial relighting attack (AQ-ARA). It generates natural adversarial light under the physical lighting model and guidance of FR systems and synthesizes adversarially relighted face images. Moreover, we propose the auto-predictive adversarial relighting attack (AP-ARA) by training an adversarial relighting network (ARNet) to automatically predict the adversarial light in a one-step manner according to different input faces, allowing efficiency-sensitive applications. More importantly, we propose to transfer the above digital attacks to physical ARA (Phy-ARA) through a precise relighting device, making the estimated adversarial lighting condition reproducible in the real world. We validate our methods on three state-of-the-art deep FR methods, i.e., FaceNet, ArcFace, and CosFace, on two public datasets. The extensive and insightful results demonstrate our work can generate realistic adversarial relighted face images fooling FR easily, revealing the threat of specific light directions and strengths.",0
"Several challenging datasets have shown that deep face recognition (FR) has achieved high accuracy and has been used in successful real-world applications. Despite this success, illumination variation is a significant threat to the FR system, and it cannot be fully covered by the limited face dataset. In this study, we adopt an adversarial attack approach to examine the threat of lighting against FR. We introduce a new task, adversarial relighting, which aims to produce a naturally relighted face image while fooling state-of-the-art deep FR methods. To achieve this, we propose two methods: albedo-quotient-based adversarial relighting attack (AQ-ARA) and auto-predictive adversarial relighting attack (AP-ARA). AQ-ARA generates natural adversarial light under the physical lighting model and guidance of FR systems, while AP-ARA trains an adversarial relighting network (ARNet) to predict the adversarial light in a one-step manner. We also propose transferring these digital attacks to physical ARA (Phy-ARA) using a precise relighting device. Our methods were validated on three state-of-the-art deep FR methods using two public datasets. The results demonstrate that our work generates realistic adversarial relighted face images, revealing the threat of specific light directions and strengths.",1
"Adversarial attacks are feasible in the real world for object detection. However, most of the previous works have tried to learn ""patches"" applied to an object to fool detectors, which become less effective or even ineffective in squint view angles. To address this issue, we propose the Dense Proposals Attack (DPA) to learn robust, physical and targeted adversarial camouflages for detectors. The camouflages are robust because they remain adversarial when filmed under arbitrary viewpoint and different illumination conditions, physical because they function well both in the 3D virtual scene and the real world, and targeted because they can cause detectors to misidentify an object as a specific target class. In order to make the generated camouflages robust in the physical world, we introduce a combination of viewpoint shifts, lighting and other natural transformations to model the physical phenomena. In addition, to improve the attacks, DPA substantially attacks all the classifications in the fixed region proposals. Moreover, we build a virtual 3D scene using the Unity simulation engine to fairly and reproducibly evaluate different physical attacks. Extensive experiments demonstrate that DPA outperforms the state-of-the-art methods significantly, and generalizes well to the real world, posing a potential threat to the security-critical computer vision systems.",0
"While previous research has focused on learning ""patches"" to deceive object detection systems, these techniques have proven ineffective when viewed from different angles. To address this issue, we propose the Dense Proposals Attack (DPA) which generates robust, physical, and targeted adversarial camouflages that can fool detectors regardless of viewpoint or illumination conditions. To ensure the effectiveness of the camouflages in the physical world, we incorporate natural transformations such as viewpoint shifts and lighting changes. DPA targets all classifications in fixed region proposals and is evaluated in a virtual 3D scene using the Unity simulation engine. Our experiments demonstrate that DPA outperforms previous methods and is a significant threat to security-critical computer vision systems.",1
"The CycleGAN framework allows for unsupervised image-to-image translation of unpaired data. In a scenario of surgical training on a physical surgical simulator, this method can be used to transform endoscopic images of phantoms into images which more closely resemble the intra-operative appearance of the same surgical target structure. This can be viewed as a novel augmented reality approach, which we coined Hyperrealism in previous work. In this use case, it is of paramount importance to display objects like needles, sutures or instruments consistent in both domains while altering the style to a more tissue-like appearance. Segmentation of these objects would allow for a direct transfer, however, contouring of these, partly tiny and thin foreground objects is cumbersome and perhaps inaccurate. Instead, we propose to use landmark detection on the points when sutures pass into the tissue. This objective is directly incorporated into a CycleGAN framework by treating the performance of pre-trained detector models as an additional optimization goal. We show that a task defined on these sparse landmark labels improves consistency of synthesis by the generator network in both domains. Comparing a baseline CycleGAN architecture to our proposed extension (DetCycleGAN), mean precision (PPV) improved by +61.32, mean sensitivity (TPR) by +37.91, and mean F1 score by +0.4743. Furthermore, it could be shown that by dataset fusion, generated intra-operative images can be leveraged as additional training data for the detection network itself. The data is released within the scope of the AdaptOR MICCAI Challenge 2021 at https://adaptor2021.github.io/, and code at https://github.com/Cardio-AI/detcyclegan_pytorch.",0
"The CycleGAN framework facilitates unsupervised image-to-image translation of unpaired data. In the context of surgical training on a physical surgical simulator, this technique can be utilized to transform endoscopic images of phantoms into images that resemble the intra-operative appearance of the same surgical target structure. This innovative approach, which we previously referred to as Hyperrealism, can be considered as an augmented reality method. Ensuring consistency in objects such as needles, sutures, or instruments is crucial in this application, while also presenting a more tissue-like style. Direct transfer can be achieved through segmentation of these objects, but contouring tiny and thin foreground objects can be challenging and potentially inaccurate. Instead, we propose using landmark detection on the points where sutures penetrate the tissue. To accomplish this objective, a CycleGAN framework can be modified by integrating pre-trained detector models' performance as an additional optimization objective. We have demonstrated that defining a task based on these sparse landmark labels enhances the consistency of synthesis by the generator network in both domains. By comparing our proposed extension (DetCycleGAN) to a baseline CycleGAN architecture, we observed an improvement in mean precision (PPV) by +61.32, mean sensitivity (TPR) by +37.91, and mean F1 score by +0.4743. Additionally, we discovered that generated intra-operative images can be utilized as supplementary training data for the detection network itself by combining datasets. The data has been released as part of the AdaptOR MICCAI Challenge 2021 at https://adaptor2021.github.io/, and the code is available at https://github.com/Cardio-AI/detcyclegan_pytorch.",1
"We propose a deep videorealistic 3D human character model displaying highly realistic shape, motion, and dynamic appearance learned in a new weakly supervised way from multi-view imagery. In contrast to previous work, our controllable 3D character displays dynamics, e.g., the swing of the skirt, dependent on skeletal body motion in an efficient data-driven way, without requiring complex physics simulation. Our character model also features a learned dynamic texture model that accounts for photo-realistic motion-dependent appearance details, as well as view-dependent lighting effects. During training, we do not need to resort to difficult dynamic 3D capture of the human; instead we can train our model entirely from multi-view video in a weakly supervised manner. To this end, we propose a parametric and differentiable character representation which allows us to model coarse and fine dynamic deformations, e.g., garment wrinkles, as explicit space-time coherent mesh geometry that is augmented with high-quality dynamic textures dependent on motion and view point. As input to the model, only an arbitrary 3D skeleton motion is required, making it directly compatible with the established 3D animation pipeline. We use a novel graph convolutional network architecture to enable motion-dependent deformation learning of body and clothing, including dynamics, and a neural generative dynamic texture model creates corresponding dynamic texture maps. We show that by merely providing new skeletal motions, our model creates motion-dependent surface deformations, physically plausible dynamic clothing deformations, as well as video-realistic surface textures at a much higher level of detail than previous state of the art approaches, and even in real-time.",0
"Our team has developed a cutting-edge videorealistic 3D human character model that boasts a high level of realism in shape, motion, and dynamic appearance. Unlike previous efforts, our model is capable of displaying controllable dynamics, such as the swing of a skirt, that are dependent on skeletal body motion. This is achieved through an efficient data-driven approach that bypasses the need for complex physics simulations. Our model also includes a learned dynamic texture feature that accounts for motion-dependent appearance details and view-dependent lighting effects. Importantly, we can train our model entirely from multi-view video in a weakly supervised manner, without the need for dynamic 3D capture. We have developed a parametric and differentiable character representation that enables us to model coarse and fine dynamic deformations, such as garment wrinkles, as explicit space-time coherent mesh geometry. Our model only requires an arbitrary 3D skeleton motion as input, making it directly compatible with the established 3D animation pipeline. We have implemented a novel graph convolutional network architecture that enables motion-dependent deformation learning of body and clothing, including dynamics, while a neural generative dynamic texture model creates corresponding dynamic texture maps. Our approach creates motion-dependent surface deformations, physically plausible dynamic clothing deformations, and video-realistic surface textures at a much higher level of detail than previous state of the art approaches, and even in real-time, by merely providing new skeletal motions.",1
"Spatio-temporal dynamics of physical processes are generally modeled using partial differential equations (PDEs). Though the core dynamics follows some principles of physics, real-world physical processes are often driven by unknown external sources. In such cases, developing a purely analytical model becomes very difficult and data-driven modeling can be of assistance. In this paper, we present a hybrid framework combining physics-based numerical models with deep learning for source identification and forecasting of spatio-temporal dynamical systems with unobservable time-varying external sources. We formulate our model PhICNet as a convolutional recurrent neural network (RNN) which is end-to-end trainable for spatio-temporal evolution prediction of dynamical systems and learns the source behavior as an internal state of the RNN. Experimental results show that the proposed model can forecast the dynamics for a relatively long time and identify the sources as well.",0
"Partial differential equations (PDEs) are commonly used to model the spatio-temporal dynamics of physical processes. However, real-world physical processes can be influenced by unknown external sources, making it challenging to develop purely analytical models. Data-driven modeling can be beneficial in such cases. This study proposes a hybrid framework, named PhICNet, which combines physics-based numerical models with deep learning to identify and forecast spatio-temporal dynamical systems with unobservable time-varying external sources. PhICNet is an end-to-end trainable convolutional recurrent neural network (RNN) that predicts spatio-temporal evolution and learns the source behavior as an internal state of the RNN. Experimental outcomes demonstrate the model's ability to forecast dynamics for a relatively long time and identify the sources.",1
"In current clinical practice, noisy and artifact-ridden weekly cone-beam computed tomography (CBCT) images are only used for patient setup during radiotherapy. Treatment planning is done once at the beginning of the treatment using high-quality planning CT (pCT) images and manual contours for organs-at-risk (OARs) structures. If the quality of the weekly CBCT images can be improved while simultaneously segmenting OAR structures, this can provide critical information for adapting radiotherapy mid-treatment as well as for deriving biomarkers for treatment response. Using a novel physics-based data augmentation strategy, we synthesize a large dataset of perfectly/inherently registered planning CT and synthetic-CBCT pairs for locally advanced lung cancer patient cohort, which are then used in a multitask 3D deep learning framework to simultaneously segment and translate real weekly CBCT images to high-quality planning CT-like images. We compared the synthetic CT and OAR segmentations generated by the model to real planning CT and manual OAR segmentations and showed promising results. The real week 1 (baseline) CBCT images which had an average MAE of 162.77 HU compared to pCT images are translated to synthetic CT images that exhibit a drastically improved average MAE of 29.31 HU and average structural similarity of 92% with the pCT images. The average DICE scores of the 3D organs-at-risk segmentations are: lungs 0.96, heart 0.88, spinal cord 0.83 and esophagus 0.66. This approach could allow clinicians to adjust treatment plans using only the routine low-quality CBCT images, potentially improving patient outcomes. Our code, data, and pre-trained models will be made available via our physics-based data augmentation library, Physics-ArX, at https://github.com/nadeemlab/Physics-ArX.",0
"Currently, noisy and artifact-ridden weekly cone-beam computed tomography (CBCT) images are solely utilized for patient positioning during radiotherapy, while high-quality planning CT (pCT) images and manual contours for organs-at-risk (OARs) structures are used for treatment planning at the beginning of treatment. However, if weekly CBCT images can be improved and OAR structures segmented, this can provide vital information for adapting radiotherapy mid-treatment and deriving biomarkers for treatment response. To achieve this, we utilized a novel physics-based data augmentation strategy to synthesize a large dataset of perfectly registered planning CT and synthetic-CBCT pairs for a locally advanced lung cancer patient cohort. These pairs were then used in a multitask 3D deep learning framework to simultaneously segment and translate real weekly CBCT images to high-quality planning CT-like images. Our results showed promising outcomes, with synthetic CT images exhibiting a drastically improved average MAE of 29.31 HU and average structural similarity of 92% with the pCT images. Additionally, the average DICE scores of the 3D organs-at-risk segmentations were satisfactory. This approach could potentially improve patient outcomes by allowing clinicians to adjust treatment plans using only the routine low-quality CBCT images. Our code, data, and pre-trained models will be available via our physics-based data augmentation library, Physics-ArX, at https://github.com/nadeemlab/Physics-ArX.",1
"Purpose: Radiotherapy presents unique challenges and clinical requirements for longitudinal tumor and organ-at-risk (OAR) prediction during treatment. The challenges include tumor inflammation/edema and radiation-induced changes in organ geometry, whereas the clinical requirements demand flexibility in input/output sequence timepoints to update the predictions on rolling basis and the grounding of all predictions in relationship to the pre-treatment imaging information for response and toxicity assessment in adaptive radiotherapy. Methods: To deal with the aforementioned challenges and to comply with the clinical requirements, we present a novel 3D sequence-to-sequence model based on Convolution Long Short Term Memory (ConvLSTM) that makes use of series of deformation vector fields (DVF) between individual timepoints and reference pre-treatment/planning CTs to predict future anatomical deformations and changes in gross tumor volume as well as critical OARs. High-quality DVF training data is created by employing hyper-parameter optimization on the subset of the training data with DICE coefficient and mutual information metric. We validated our model on two radiotherapy datasets: a publicly available head-and-neck dataset (28 patients with manually contoured pre-, mid-, and post-treatment CTs), and an internal non-small cell lung cancer dataset (63 patients with manually contoured planning CT and 6 weekly CBCTs). Results: The use of DVF representation and skip connections overcomes the blurring issue of ConvLSTM prediction with the traditional image representation. The mean and standard deviation of DICE for predictions of lung GTV at week 4, 5, and 6 were 0.83$\pm$0.09, 0.82$\pm$0.08, and 0.81$\pm$0.10, respectively, and for post-treatment ipsilateral and contralateral parotids, were 0.81$\pm$0.06 and 0.85$\pm$0.02.",0
"The purpose of this study is to address the unique challenges and clinical requirements associated with predicting longitudinal tumor and organ-at-risk (OAR) changes during radiotherapy treatment. These challenges include tumor inflammation/edema and radiation-induced changes in organ geometry, and clinical requirements demand flexibility in input/output sequence timepoints to update predictions on a rolling basis and the grounding of all predictions in relationship to pre-treatment imaging information for response and toxicity assessment in adaptive radiotherapy. To overcome these challenges and comply with clinical requirements, a novel 3D sequence-to-sequence model based on Convolution Long Short Term Memory (ConvLSTM) was developed, which uses deformation vector fields (DVF) between individual timepoints and reference pre-treatment/planning CTs to predict future anatomical deformations and changes in gross tumor volume as well as critical OARs. The model was validated on two radiotherapy datasets, demonstrating improved prediction accuracy with mean DICE coefficients of 0.83$\pm$0.09, 0.82$\pm$0.08, and 0.81$\pm$0.10 for predictions of lung GTV at week 4, 5, and 6, and 0.81$\pm$0.06 and 0.85$\pm$0.02 for post-treatment ipsilateral and contralateral parotids, respectively. The use of DVF representation and skip connections overcomes the blurring issue of ConvLSTM prediction with traditional image representation.",1
"Brain strain and strain rate are effective in predicting traumatic brain injury (TBI) caused by head impacts. However, state-of-the-art finite element modeling (FEM) demands considerable computational time in the computation, limiting its application in real-time TBI risk monitoring. To accelerate, machine learning head models (MLHMs) were developed, and the model accuracy was found to decrease when the training/test datasets were from different head impacts types. However, the size of dataset for specific impact types may not be enough for model training. To address the computational cost of FEM, the limited strain rate prediction, and the generalizability of MLHMs to on-field datasets, we propose data fusion and transfer learning to develop a series of MLHMs to predict the maximum principal strain (MPS) and maximum principal strain rate (MPSR). We trained and tested the MLHMs on 13,623 head impacts from simulations, American football, mixed martial arts, car crash, and compared against the models trained on only simulations or only on-field impacts. The MLHMs developed with transfer learning are significantly more accurate in estimating MPS and MPSR than other models, with a mean absolute error (MAE) smaller than 0.03 in predicting MPS and smaller than 7 (1/s) in predicting MPSR on all impact datasets. The MLHMs can be applied to various head impact types for rapidly and accurately calculating brain strain and strain rate. Besides the clinical applications in real-time brain strain and strain rate monitoring, this model helps researchers estimate the brain strain and strain rate caused by head impacts more efficiently than FEM.",0
"The ability to predict traumatic brain injury (TBI) from head impacts can be achieved through brain strain and strain rate analysis, but the computational demands of state-of-the-art finite element modeling (FEM) limit its use in real-time TBI risk monitoring. To address this, researchers developed machine learning head models (MLHMs), but found that accuracy decreased when using training/test data from different impact types due to a lack of sufficient data for specific impact types. To improve MLHM accuracy and address FEM's computational cost, limited strain rate prediction, and generalizability to on-field datasets, the researchers propose data fusion and transfer learning to develop a series of MLHMs to predict maximum principal strain (MPS) and maximum principal strain rate (MPSR). These MLHMs were trained and tested on 13,623 head impacts from simulations, American football, mixed martial arts, and car crashes, and were found to be significantly more accurate than other models, with a mean absolute error (MAE) smaller than 0.03 in predicting MPS and smaller than 7 (1/s) in predicting MPSR across all impact datasets. The MLHMs can be applied to various head impact types for rapid and accurate brain strain and strain rate calculation, aiding in real-time TBI risk monitoring and more efficient brain strain and strain rate estimation than FEM.",1
"Cardiovascular disease, especially heart failure is one of the major health hazard issues of our time and is a leading cause of death worldwide. Advancement in data mining techniques using machine learning (ML) models is paving promising prediction approaches. Data mining is the process of converting massive volumes of raw data created by the healthcare institutions into meaningful information that can aid in making predictions and crucial decisions. Collecting various follow-up data from patients who have had heart failures, analyzing those data, and utilizing several ML models to predict the survival possibility of cardiovascular patients is the key aim of this study. Due to the imbalance of the classes in the dataset, Synthetic Minority Oversampling Technique (SMOTE) has been implemented. Two unsupervised models (K-Means and Fuzzy C-Means clustering) and three supervised classifiers (Random Forest, XGBoost and Decision Tree) have been used in our study. After thorough investigation, our results demonstrate a superior performance of the supervised ML algorithms over unsupervised models. Moreover, we designed and propose a supervised stacked ensemble learning model that can achieve an accuracy, precision, recall and F1 score of 99.98%. Our study shows that only certain attributes collected from the patients are imperative to successfully predict the surviving possibility post heart failure, using supervised ML algorithms.",0
"Cardiovascular disease, specifically heart failure, is a significant contemporary health hazard and a primary cause of death worldwide. The use of machine learning (ML) models for data mining has shown promising predictive methods. Data mining involves translating large amounts of raw data from healthcare institutions into meaningful information to aid in decision-making and predictions. This study aims to collect follow-up data from heart failure patients, analyze the information, and use various ML models to predict the likelihood of survival. The Synthetic Minority Oversampling Technique (SMOTE) has been implemented to account for the dataset's class imbalance. Our study utilizes two unsupervised models (K-Means and Fuzzy C-Means clustering) and three supervised classifiers (Random Forest, XGBoost, and Decision Tree). After thorough analysis, our results demonstrate that the supervised ML algorithms outperform the unsupervised models. Additionally, we propose a supervised stacked ensemble learning model that can achieve an accuracy, precision, recall, and F1 score of 99.98%. Our study shows that specific patient attributes are crucial to successfully predict the possibility of survival after heart failure using supervised ML algorithms.",1
"Face recognition is greatly improved by deep convolutional neural networks (CNNs). Recently, these face recognition models have been used for identity authentication in security sensitive applications. However, deep CNNs are vulnerable to adversarial patches, which are physically realizable and stealthy, raising new security concerns on the real-world applications of these models. In this paper, we evaluate the robustness of face recognition models using adversarial patches based on transferability, where the attacker has limited accessibility to the target models. First, we extend the existing transfer-based attack techniques to generate transferable adversarial patches. However, we observe that the transferability is sensitive to initialization and degrades when the perturbation magnitude is large, indicating the overfitting to the substitute models. Second, we propose to regularize the adversarial patches on the low dimensional data manifold. The manifold is represented by generative models pre-trained on legitimate human face images. Using face-like features as adversarial perturbations through optimization on the manifold, we show that the gaps between the responses of substitute models and the target models dramatically decrease, exhibiting a better transferability. Extensive digital world experiments are conducted to demonstrate the superiority of the proposed method in the black-box setting. We apply the proposed method in the physical world as well.",0
"The use of deep convolutional neural networks (CNNs) has greatly enhanced face recognition. These models have been adopted for identity authentication in sensitive security applications. However, they are susceptible to adversarial patches, which are physically realizable and undetectable, resulting in new security concerns for the practical application of these models. The purpose of this study is to assess the robustness of face recognition models against adversarial patches based on transferability, where the attacker has limited access to the target models. Firstly, we expand the existing transfer-based attack techniques to create transferable adversarial patches. However, we notice that transferability is influenced by initialization and declines as the perturbation magnitude increases, indicating overfitting to substitute models. Secondly, we suggest regularizing the adversarial patches on the low-dimensional data manifold represented by generative models pre-trained on legitimate human face images. Through optimization on the manifold, we use face-like features as adversarial perturbations and demonstrate a significant decrease in the gaps between the responses of substitute and target models, resulting in better transferability. We conduct extensive digital and physical world experiments to prove the effectiveness of our approach in the black-box setting.",1
"For dealing with traffic bottlenecks at airports, aircraft object detection is insufficient. Every airport generally has a variety of planes with various physical and technological requirements as well as diverse service requirements. Detecting the presence of new planes will not address all traffic congestion issues. Identifying the type of airplane, on the other hand, will entirely fix the problem because it will offer important information about the plane's technical specifications (i.e., the time it needs to be served and its appropriate place in the airport). Several studies have provided various contributions to address airport traffic jams; however, their ultimate goal was to determine the existence of airplane objects. This paper provides a practical approach to identify the type of airplane in airports depending on the results provided by the airplane detection process using mask region convolution neural network. The key feature employed to identify the type of airplane is the surface area calculated based on the results of airplane detection. The surface area is used to assess the estimated cabin length which is considered as an additional key feature for identifying the airplane type. The length of any detected plane may be calculated by measuring the distance between the detected plane's two furthest points. The suggested approach's performance is assessed using average accuracies and a confusion matrix. The findings show that this method is dependable. This method will greatly aid in the management of airport traffic congestion.",0
"Aircraft object detection is not enough to handle traffic congestion at airports. Airports have a variety of planes with different physical and technological requirements, which detecting only new planes cannot solve. Identifying the type of airplane is the key to addressing this problem, as it provides important technical specifications for appropriate service and location. While previous studies have focused on detecting airplane objects, this paper proposes a practical approach to identify the type of airplane using a mask region convolution neural network. The surface area is used to estimate the cabin length, which is a crucial feature for airplane identification. The performance of this approach is evaluated using average accuracies and a confusion matrix, indicating its reliability. This method will be highly beneficial in managing airport traffic congestion.",1
"Anomaly detection has been a challenging task given high-dimensional multivariate time series data generated by networked sensors and actuators in Cyber-Physical Systems (CPS). Besides the highly nonlinear, complex, and dynamic natures of such time series, the lack of labeled data impedes data exploitation in a supervised manner and thus prevents an accurate detection of abnormal phenomenons. On the other hand, the collected data at the edge of the network is often privacy sensitive and large in quantity, which may hinder the centralized training at the main server. To tackle these issues, we propose an unsupervised time series anomaly detection framework in a federated fashion to continuously monitor the behaviors of interconnected devices within a network and alerts for abnormal incidents so that countermeasures can be taken before undesired consequences occur. To be specific, we leave the training data distributed at the edge to learn a shared Variational Autoencoder (VAE) based on Convolutional Gated Recurrent Unit (ConvGRU) model, which jointly captures feature and temporal dependencies in the multivariate time series data for representation learning and downstream anomaly detection tasks. Experiments on three real-world networked sensor datasets illustrate the advantage of our approach over other state-of-the-art models. We also conduct extensive experiments to demonstrate the effectiveness of our detection framework under non-federated and federated settings in terms of overall performance and detection latency.",0
"Detecting anomalies in high-dimensional multivariate time series data generated by networked sensors and actuators in Cyber-Physical Systems (CPS) has been a challenging task. The complex, highly nonlinear, and dynamic nature of such time series, along with the lack of labeled data, makes it difficult to accurately identify abnormal incidents in a supervised manner. Moreover, the large quantity and privacy sensitivity of the collected data at the edge of the network may hinder centralized training at the main server. To address these issues, we propose an unsupervised time series anomaly detection framework in a federated fashion. This framework continuously monitors the behaviors of interconnected devices within a network and alerts for abnormal incidents, enabling countermeasures to be taken to prevent undesired consequences. Specifically, we distribute the training data at the edge to learn a shared Variational Autoencoder (VAE) based on Convolutional Gated Recurrent Unit (ConvGRU) model that captures feature and temporal dependencies in the multivariate time series data for representation learning and downstream anomaly detection tasks. We conducted experiments on three real-world networked sensor datasets to demonstrate the superiority of our approach over other state-of-the-art models. We also performed extensive experiments to demonstrate the effectiveness of our detection framework under non-federated and federated settings in terms of overall performance and detection latency.",1
"The reinforcement learning (RL) research area is very active, with several important applications. However, certain challenges still need to be addressed, amongst which one can mention the ability to find policies that achieve sufficient exploration and coordination while solving a given task. In this work, we present an algorithmic framework of two RL agents each with a different objective. We introduce a novel function approximation approach to assess the influence $F$ of a certain policy on others. While optimizing $F$ as a regularizer of $\pi$'s objective, agents learn to coordinate team behavior while exploiting high-reward regions of the solution space. Additionally, both agents use prediction error as intrinsic motivation to learn policies that behave as differently as possible, thus achieving the exploration criterion. Our method was evaluated on the suite of OpenAI gym tasks as well as cooperative and mixed scenarios, where agent populations are able to discover various physical and informational coordination strategies, showing state-of-the-art performance when compared to famous baselines.",0
"The research on reinforcement learning (RL) is a lively field with many important applications. However, there are certain challenges that need to be addressed, including the ability to find policies that can explore and coordinate effectively while solving a task. This paper presents an algorithmic framework with two RL agents, each having a different objective. A novel function approximation approach is introduced to evaluate the influence of a policy on others. By optimizing this function as a regularizer of the objective, the agents learn to work as a team while exploiting high-reward areas of the solution space. Furthermore, both agents use prediction error as intrinsic motivation to learn policies that behave differently from one another, thereby meeting the exploration criterion. We evaluated our method on OpenAI gym tasks, cooperative and mixed scenarios, and found that our approach outperformed well-known baselines, allowing agent populations to discover various coordination strategies.",1
"Gaussian processes are an effective model class for learning unknown functions, particularly in settings where accurately representing predictive uncertainty is of key importance. Motivated by applications in the physical sciences, the widely-used Mat\'ern class of Gaussian processes has recently been generalized to model functions whose domains are Riemannian manifolds, by re-expressing said processes as solutions of stochastic partial differential equations. In this work, we propose techniques for computing the kernels of these processes on compact Riemannian manifolds via spectral theory of the Laplace-Beltrami operator in a fully constructive manner, thereby allowing them to be trained via standard scalable techniques such as inducing point methods. We also extend the generalization from the Mat\'ern to the widely-used squared exponential Gaussian process. By allowing Riemannian Mat\'ern Gaussian processes to be trained using well-understood techniques, our work enables their use in mini-batch, online, and non-conjugate settings, and makes them more accessible to machine learning practitioners.",0
"When it comes to accurately representing predictive uncertainty, Gaussian processes are a highly effective model class for learning unknown functions. Recently, the Mat\'ern class of Gaussian processes has been generalized to model functions on Riemannian manifolds, which is useful for applications in the physical sciences. This has been accomplished by expressing these processes as solutions of stochastic partial differential equations. Our work proposes techniques for computing the kernels of these processes on compact Riemannian manifolds using spectral theory of the Laplace-Beltrami operator. This fully constructive approach enables these processes to be trained using standard scalable techniques, such as inducing point methods, which is useful for mini-batch, online, and non-conjugate settings. Additionally, we extend this generalization from the Mat\'ern to the widely-used squared exponential Gaussian process, making it more accessible to machine learning practitioners.",1
"Reduced order modeling (ROM) is a field of techniques that approximates complex physics-based models of real-world processes by inexpensive surrogates that capture important dynamical characteristics with a smaller number of degrees of freedom. Traditional ROM techniques such as proper orthogonal decomposition (POD) focus on linear projections of the dynamics onto a set of spectral features. In this paper we explore the construction of ROM using autoencoders (AE) that perform nonlinear projections of the system dynamics onto a low dimensional manifold learned from data. The approach uses convolutional neural networks (CNN) to learn spatial features as opposed to spectral, and utilize a physics informed (PI) cost function in order to capture temporal features as well. Our investigation using the quasi-geostrophic equations reveals that while the PI cost function helps with spatial reconstruction, spatial features are less powerful than spectral features, and that construction of ROMs through machine learning-based methods requires significant investigation into novel non-standard methodologies.",0
"Reduced order modeling (ROM) involves approximating complex physics-based models of real-world processes with cheaper surrogates that capture important dynamical features using fewer degrees of freedom. The traditional method of ROM, known as proper orthogonal decomposition (POD), focuses on linear projections of dynamics onto a set of spectral features. This study, however, explores the use of autoencoders (AE) to construct ROM, which perform nonlinear projections of system dynamics onto a low dimensional manifold learned from data. The approach uses convolutional neural networks (CNN) to learn spatial features instead of spectral ones, and incorporates a physics informed (PI) cost function to capture temporal features. Our investigation using the quasi-geostrophic equations reveals that while the PI cost function enhances spatial reconstruction, spatial features are less effective than spectral features. Therefore, novel non-standard methodologies are required to construct ROMs through machine learning-based methods, which necessitates significant investigation.",1
"This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.   In many applications, the spatial distribution of a field needs to be carefully monitored to detect spikes, discontinuities or dangerous heterogeneities, but invasive monitoring approaches cannot be used. Besides, technical specifications about the process might not be available by preventing the adoption of an accurate model of the system. In this work, a physics-informed, data-driven algorithm that allows addressing these requirements is presented. The approach is based on the implementation of a boundary element method (BEM)-scheme within a convolutional neural network. Thanks to the capability of representing any continuous mathematical function with a reduced number of parameters, the network allows predicting the field value in any point of the domain, given the boundary conditions and few measurements within the domain. The proposed approach was applied to reconstruct a field described by the Helmholtz equation over a three-dimensional domain. A sensitivity analysis was also performed by investigating different physical conditions and different network configurations. Since the only assumption is the applicability of BEM, the current approach can be applied to the monitoring of a wide range of processes, from the localization of the source of pollutant within a water reservoir to the monitoring of the neutron flux in a nuclear reactor.",0
"The work has been submitted to IEEE for potential publication, and copyright transfer may occur without notice, rendering this version inaccessible. In certain situations, monitoring the spatial distribution of a field is necessary to detect spikes, discontinuities, or dangerous heterogeneities, but invasive monitoring methods are unsuitable. Moreover, the absence of technical specifications about the process may prevent the adoption of an accurate system model. This study presents a physics-informed, data-driven algorithm that addresses these requirements using a boundary element method (BEM)-scheme within a convolutional neural network. With the network's ability to represent any continuous mathematical function with minimal parameters, the field value can be predicted at any point in the domain with a few measurements and boundary conditions. The Helmholtz equation's field reconstruction on a three-dimensional domain was accomplished using this approach, and a sensitivity analysis was conducted with different physical conditions and network configurations. Since the BEM applicability is the only assumption, this approach can be used to monitor a wide range of processes, from identifying the source of pollutants in water reservoirs to tracking neutron flux in nuclear reactors.",1
"This paper addresses the problem of building an affordable easy-to-setup synchronized multi-view camera system, which is in demand for many Computer Vision and Robotics applications in high-dynamic environments. In our work, we propose a solution for this problem -- a publicly-available Android application for synchronized video recording on multiple smartphones with sub-millisecond accuracy. We present a generalized mathematical model of timestamping for Android smartphones and prove its applicability on 47 different physical devices. Also, we estimate the time drift parameter for those smartphones, which is less than 1.2 msec per minute for most of the considered devices, that makes smartphones' camera system a worthy analog for professional multi-view systems. Finally, we demonstrate Android-app performance on the camera system built from Android smartphones quantitatively on setup with lights and qualitatively -- on panorama stitching task.",0
"This article discusses the challenge of creating an inexpensive, simple-to-use multi-view camera system that can operate effectively in high-dynamic settings, a need that arises in many Computer Vision and Robotics applications. To tackle this issue, we have developed a free Android app that enables synchronized video recording on multiple smartphones, with a precision of sub-milliseconds. We have also formulated a universal mathematical model for timestamping Android smartphones, which has been verified on 47 different physical devices. Additionally, we have calculated the time drift parameter for these devices, which is under 1.2 msec per minute for most of them, making smartphones a credible alternative to specialized multi-view systems. Finally, we have quantitatively evaluated the Android app's performance on a camera system constructed from Android smartphones, both in a light setup and for a panorama stitching task.",1
"This paper presents a novel spatio-temporal LSTM (SPATIAL) architecture for time series forecasting applied to environmental datasets. The framework was evaluated across multiple sensors and for three different oceanic variables: current speed, temperature, and dissolved oxygen. Network implementation proceeded in two directions that are nominally separated but connected as part of a natural environmental system -- across the spatial (between individual sensors) and temporal components of the sensor data. Data from four sensors sampling current speed, and eight measuring both temperature and dissolved oxygen evaluated the framework. Results were compared against RF and XGB baseline models that learned on the temporal signal of each sensor independently by extracting the date-time features together with the past history of data using sliding window matrix. Results demonstrated ability to accurately replicate complex signals and provide comparable performance to state-of-the-art benchmarks. Notably, the novel framework provided a simpler pre-processing and training pipeline that handles missing values via a simple masking layer. Enabling learning across the spatial and temporal directions, this paper addresses two fundamental challenges of ML applications to environmental science: 1) data sparsity and the challenges and costs of collecting measurements of environmental conditions such as ocean dynamics, and 2) environmental datasets are inherently connected in the spatial and temporal directions while classical ML approaches only consider one of these directions. Furthermore, sharing of parameters across all input steps makes SPATIAL a fast, scalable, and easily-parameterized forecasting framework.",0
"In this paper, a new method called SPATIAL is introduced for predicting time series in environmental datasets. The approach was tested on data from multiple sensors measuring different oceanic variables, including current speed, temperature, and dissolved oxygen. The SPATIAL model works by analyzing both the spatial (between sensors) and temporal aspects of the data, which are connected as part of the natural environmental system. The results of the experiments were compared to existing baseline models, and the SPATIAL model showed comparable accuracy and provided a simpler training pipeline that can handle missing data. By addressing the challenges of data sparsity and considering both spatial and temporal aspects, this approach provides a fast and scalable forecasting framework that can be easily parameterized.",1
"Decomposing a scene into its shape, reflectance, and illumination is a challenging but important problem in computer vision and graphics. This problem is inherently more challenging when the illumination is not a single light source under laboratory conditions but is instead an unconstrained environmental illumination. Though recent work has shown that implicit representations can be used to model the radiance field of an object, most of these techniques only enable view synthesis and not relighting. Additionally, evaluating these radiance fields is resource and time-intensive. We propose a neural reflectance decomposition (NeRD) technique that uses physically-based rendering to decompose the scene into spatially varying BRDF material properties. In contrast to existing techniques, our input images can be captured under different illumination conditions. In addition, we also propose techniques to convert the learned reflectance volume into a relightable textured mesh enabling fast real-time rendering with novel illuminations. We demonstrate the potential of the proposed approach with experiments on both synthetic and real datasets, where we are able to obtain high-quality relightable 3D assets from image collections. The datasets and code is available on the project page: https://markboss.me/publication/2021-nerd/",0
"The task of breaking down a scene into its shape, reflectance, and illumination is a complex yet crucial issue in the fields of computer vision and graphics. This task becomes more difficult when dealing with environmental illumination, as opposed to controlled laboratory settings with a single light source. Although some recent research has developed implicit representations to model an object's radiance field, these methods are primarily limited to view synthesis and are slow and resource-intensive. Our proposed solution, called neural reflectance decomposition (NeRD), uses physically-based rendering to decompose the scene into varying BRDF material properties. Unlike existing techniques, our approach can handle input images captured under different illumination conditions. Furthermore, we offer techniques to transform the learned reflectance volume into a relightable textured mesh, allowing for rapid real-time rendering with novel illuminations. We showcase the potential of our approach through experiments on both real and synthetic datasets, demonstrating our ability to obtain high-quality, relightable 3D assets from image collections. Interested parties can find our datasets and code on our project page: https://markboss.me/publication/2021-nerd/.",1
"Deep Neural Networks are actively being used in the design of autonomous Cyber-Physical Systems (CPSs). The advantage of these models is their ability to handle high-dimensional state-space and learn compact surrogate representations of the operational state spaces. However, the problem is that the sampled observations used for training the model may never cover the entire state space of the physical environment, and as a result, the system will likely operate in conditions that do not belong to the training distribution. These conditions that do not belong to training distribution are referred to as Out-of-Distribution (OOD). Detecting OOD conditions at runtime is critical for the safety of CPS. In addition, it is also desirable to identify the context or the feature(s) that are the source of OOD to select an appropriate control action to mitigate the consequences that may arise because of the OOD condition. In this paper, we study this problem as a multi-labeled time series OOD detection problem over images, where the OOD is defined both sequentially across short time windows (change points) as well as across the training data distribution. A common approach to solving this problem is the use of multi-chained one-class classifiers. However, this approach is expensive for CPSs that have limited computational resources and require short inference times. Our contribution is an approach to design and train a single $\beta$-Variational Autoencoder detector with a partially disentangled latent space sensitive to variations in image features. We use the feature sensitive latent variables in the latent space to detect OOD images and identify the most likely feature(s) responsible for the OOD. We demonstrate our approach using an Autonomous Vehicle in the CARLA simulator and a real-world automotive dataset called nuImages.",0
"Autonomous Cyber-Physical Systems (CPSs) employ Deep Neural Networks, which can effectively handle high-dimensional state spaces and learn compact surrogate representations of operational state spaces. However, the challenge is that the sampled observations used for model training may not fully cover the physical environment's entire state space. Consequently, the system may operate in conditions that do not belong to the training distribution, known as Out-of-Distribution (OOD) conditions. Detecting OOD conditions in real-time is crucial for CPS safety. Furthermore, identifying the source feature(s) of OOD is desirable to mitigate the consequences. This paper proposes a multi-labeled time series OOD detection approach using a $\beta$-Variational Autoencoder detector with a partially disentangled latent space sensitive to image feature variations. The proposed approach detects OOD images and identifies the most likely feature(s) responsible for OOD. The approach is demonstrated using the nuImages dataset and an Autonomous Vehicle in the CARLA simulator. The proposed approach is computationally efficient and suitable for CPSs with limited computational resources and short inference times compared to the expensive multi-chained one-class classifiers.",1
"Deep networks have become increasingly of interest in dynamical system prediction, but generalization remains elusive. In this work, we consider the physical parameters of ODEs as factors of variation of the data generating process. By leveraging ideas from supervised disentanglement in VAEs, we aim to separate the ODE parameters from the dynamics in the latent space. Experiments show that supervised disentanglement allows VAEs to capture the variability in the dynamics and extrapolate better to ODE parameter spaces that were not present in the training data.",0
"The use of deep networks for dynamical system prediction has sparked interest, but achieving generalization has proven difficult. This study focuses on the physical parameters of ODEs as factors that affect the data generation process. By incorporating concepts from supervised disentanglement found in VAEs, the goal is to differentiate the ODE parameters from the dynamics within the latent space. Results from experiments indicate that supervised disentanglement enables VAEs to grasp the variability in the dynamics and make better predictions in ODE parameter spaces not included in the training data.",1
"There is a wave of interest in using unsupervised neural networks for solving differential equations. The existing methods are based on feed-forward networks, {while} recurrent neural network differential equation solvers have not yet been reported. We introduce an unsupervised reservoir computing (RC), an echo-state recurrent neural network capable of discovering approximate solutions that satisfy ordinary differential equations (ODEs). We suggest an approach to calculate time derivatives of recurrent neural network outputs without using backpropagation. The internal weights of an RC are fixed, while only a linear output layer is trained, yielding efficient training. However, RC performance strongly depends on finding the optimal hyper-parameters, which is a computationally expensive process. We use Bayesian optimization to efficiently discover optimal sets in a high-dimensional hyper-parameter space and numerically show that one set is robust and can be used to solve an ODE for different initial conditions and time ranges. A closed-form formula for the optimal output weights is derived to solve first order linear equations in a backpropagation-free learning process. We extend the RC approach by solving nonlinear system of ODEs using a hybrid optimization method consisting of gradient descent and Bayesian optimization. Evaluation of linear and nonlinear systems of equations demonstrates the efficiency of the RC ODE solver.",0
"Unsupervised neural networks for solving differential equations have sparked a growing interest, but current methods only rely on feed-forward networks. Recurrent neural network differential equation solvers have not yet been reported. To address this, we introduce an unsupervised reservoir computing (RC) approach that uses an echo-state recurrent neural network to discover approximate solutions for ordinary differential equations (ODEs). Our approach involves calculating time derivatives of recurrent neural network outputs without backpropagation, making it computationally efficient. However, finding optimal hyper-parameters is crucial for RC performance, and this process can be expensive. To address this, we use Bayesian optimization to efficiently discover optimal sets in a high-dimensional hyper-parameter space. We show numerically that one set is robust enough to solve ODEs for different initial conditions and time ranges. We also derive a closed-form formula for optimal output weights to solve first order linear equations in a backpropagation-free learning process. Lastly, we extend the RC approach to solve nonlinear systems of ODEs using a hybrid optimization method consisting of gradient descent and Bayesian optimization. Our evaluation of both linear and nonlinear systems of equations demonstrates the efficiency of the RC ODE solver.",1
"All hand-object interaction is controlled by forces that the two bodies exert on each other, but little work has been done in modeling these underlying forces when doing pose and contact estimation from RGB/RGB-D data. Given the pose of the hand and object from any pose estimation system, we propose an end-to-end differentiable model that refines pose estimates by learning the forces experienced by the object at each vertex in its mesh. By matching the learned net force to an estimate of net force based on finite differences of position, this model is able to find forces that accurately describe the movement of the object, while resolving issues like mesh interpenetration and lack of contact. Evaluating on the ContactPose dataset, we show this model successfully corrects poses and finds contact maps that better match the ground truth, despite not using any RGB or depth image data.",0
"Although hand-object interaction is governed by the forces exerted by both bodies, little research has been conducted on modeling these forces during pose and contact estimation from RGB/RGB-D data. Our proposal introduces an end-to-end differentiable model that refines pose estimates by learning the forces encountered by the object at each vertex in its mesh, given the hand and object pose from any pose estimation system. By matching the predicted net force to an estimate based on finite differences of position, our model can identify forces that accurately explain the object's movement and address issues such as mesh interpenetration and lack of contact. We evaluate our model on the ContactPose dataset and demonstrate that it can successfully correct poses and generate contact maps that closely align with the ground truth, without relying on RGB or depth image data.",1
"This book develops an effective theory approach to understanding deep neural networks of practical relevance. Beginning from a first-principles component-level picture of networks, we explain how to determine an accurate description of the output of trained networks by solving layer-to-layer iteration equations and nonlinear learning dynamics. A main result is that the predictions of networks are described by nearly-Gaussian distributions, with the depth-to-width aspect ratio of the network controlling the deviations from the infinite-width Gaussian description. We explain how these effectively-deep networks learn nontrivial representations from training and more broadly analyze the mechanism of representation learning for nonlinear models. From a nearly-kernel-methods perspective, we find that the dependence of such models' predictions on the underlying learning algorithm can be expressed in a simple and universal way. To obtain these results, we develop the notion of representation group flow (RG flow) to characterize the propagation of signals through the network. By tuning networks to criticality, we give a practical solution to the exploding and vanishing gradient problem. We further explain how RG flow leads to near-universal behavior and lets us categorize networks built from different activation functions into universality classes. Altogether, we show that the depth-to-width ratio governs the effective model complexity of the ensemble of trained networks. By using information-theoretic techniques, we estimate the optimal aspect ratio at which we expect the network to be practically most useful and show how residual connections can be used to push this scale to arbitrary depths. With these tools, we can learn in detail about the inductive bias of architectures, hyperparameters, and optimizers.",0
"This book presents an effective theoretical approach to comprehending deep neural networks that are relevant in practical applications. It starts with a fundamental component-level perspective of networks and explains how to obtain an accurate description of trained network outputs by solving layer-to-layer iteration equations and nonlinear learning dynamics. One of the main findings is that network predictions can be described by nearly-Gaussian distributions, where the depth-to-width ratio of the network controls the deviations from the infinite-width Gaussian description. The book also explains how effectively-deep networks learn non-trivial representations during training and analyzes the mechanism of representation learning for nonlinear models. The authors use the concept of representation group flow (RG flow) to characterize the propagation of signals through the network and provide a solution to the exploding and vanishing gradient problem by tuning networks to criticality. They further classify networks based on different activation functions into universality classes. The book demonstrates that the depth-to-width ratio determines the effective model complexity of the ensemble of trained networks. By using information-theoretic techniques, the authors estimate the optimal aspect ratio at which the network is expected to be the most useful and explain how residual connections can be used to push this scale to arbitrary depths. These tools enable a detailed understanding of the inductive bias of architectures, hyperparameters, and optimizers.",1
"In this paper, pelage pattern matching is considered to solve the individual re-identification of the Saimaa ringed seals. Animal re-identification together with the access to large amount of image material through camera traps and crowd-sourcing provide novel possibilities for animal monitoring and conservation. We propose a novel feature pooling approach that allow aggregating the local pattern features to get a fixed size embedding vector that incorporate global features by taking into account the spatial distribution of features. This is obtained by eigen decomposition of covariances computed for probability mass functions representing feature maps. Embedding vectors can then be used to find the best match in the database of known individuals allowing animal re-identification. The results show that the proposed pooling method outperforms the existing methods on the challenging Saimaa ringed seal image data.",0
"This article focuses on the use of pelage pattern matching to address the issue of re-identifying individual Saimaa ringed seals. The availability of large amounts of image data through camera traps and crowd-sourcing, combined with animal re-identification, presents new opportunities for animal monitoring and conservation. To address this, we propose a unique feature pooling approach that allows for the aggregation of local pattern features into a fixed-size embedding vector that incorporates global features by taking into account their spatial distribution. This is achieved by performing an eigen decomposition of covariances calculated for probability mass functions that represent feature maps. The resulting embedding vectors are then used to identify the best match in a database of known individuals, enabling animal re-identification. Our findings indicate that the proposed pooling method surpasses existing approaches when applied to the challenging Saimaa ringed seal image data.",1
"The physical world is governed by the laws of physics, often represented in form of nonlinear partial differential equations (PDEs). Unfortunately, solution of PDEs is non-trivial and often involves significant computational time. With recent developments in the field of artificial intelligence and machine learning, the solution of PDEs using neural network has emerged as a domain with huge potential. However, most of the developments in this field are based on either fully connected neural networks (FNN) or convolutional neural networks (CNN). While FNN is computationally inefficient as the number of network parameters can be potentially huge, CNN necessitates regular grid and simpler domain. In this work, we propose a novel framework referred to as the Graph Attention Differential Equation (GrADE) for solving time dependent nonlinear PDEs. The proposed approach couples FNN, graph neural network, and recently developed Neural ODE framework. The primary idea is to use graph neural network for modeling the spatial domain, and Neural ODE for modeling the temporal domain. The attention mechanism identifies important inputs/features and assign more weightage to the same; this enhances the performance of the proposed framework. Neural ODE, on the other hand, results in constant memory cost and allows trading of numerical precision for speed. We also propose depth refinement as an effective technique for training the proposed architecture in lesser time with better accuracy. The effectiveness of the proposed framework is illustrated using 1D and 2D Burgers' equations. Results obtained illustrate the capability of the proposed framework in modeling PDE and its scalability to larger domains without the need for retraining.",0
"The laws of physics govern the physical world and are typically represented through nonlinear partial differential equations (PDEs). Unfortunately, finding solutions to PDEs can be challenging and computationally time-consuming. Recent advances in artificial intelligence and machine learning have shown potential for solving PDEs using neural networks. However, current approaches rely on fully connected neural networks (FNN) or convolutional neural networks (CNN), which have limitations. FNN can be computationally inefficient, while CNN requires a regular grid and simpler domain. This study proposes a new method called Graph Attention Differential Equation (GrADE) that combines FNN, graph neural network, and Neural ODE framework to solve time-dependent nonlinear PDEs. The proposed approach utilizes a graph neural network to model the spatial domain and Neural ODE to model the temporal domain. An attention mechanism assigns more weight to important features, enhancing the model's performance. Neural ODE results in constant memory cost and allows for trading numerical precision for speed. Additionally, depth refinement is proposed as an effective technique for training the architecture with better accuracy and in less time. The effectiveness of the proposed framework is tested on 1D and 2D Burgers' equations, demonstrating its scalability to larger domains without retraining.",1
"We present MVSNeRF, a novel neural rendering approach that can efficiently reconstruct neural radiance fields for view synthesis. Unlike prior works on neural radiance fields that consider per-scene optimization on densely captured images, we propose a generic deep neural network that can reconstruct radiance fields from only three nearby input views via fast network inference. Our approach leverages plane-swept cost volumes (widely used in multi-view stereo) for geometry-aware scene reasoning, and combines this with physically based volume rendering for neural radiance field reconstruction. We train our network on real objects in the DTU dataset, and test it on three different datasets to evaluate its effectiveness and generalizability. Our approach can generalize across scenes (even indoor scenes, completely different from our training scenes of objects) and generate realistic view synthesis results using only three input images, significantly outperforming concurrent works on generalizable radiance field reconstruction. Moreover, if dense images are captured, our estimated radiance field representation can be easily fine-tuned; this leads to fast per-scene reconstruction with higher rendering quality and substantially less optimization time than NeRF.",0
"Introducing MVSNeRF, an innovative method for neural rendering that efficiently creates neural radiance fields for view synthesis. Unlike previous methods that focus on per-scene optimization using numerous images, we propose a comprehensive deep neural network that can reconstruct radiance fields using just three neighboring views through rapid network inference. Our approach utilizes plane-swept cost volumes for geometry-aware scene reasoning, combining it with physically based volume rendering for neural radiance field reconstruction. We trained our network on the DTU dataset's real objects and tested it on three different datasets to assess its efficacy and generalizability. Our approach can generate realistic view synthesis outcomes using only three input images, surpassing concurrent works on generalizable radiance field reconstruction across scenes, including indoor scenes that differ entirely from our object-based training scenes. Additionally, if dense images are available, our estimated radiance field representation can be easily fine-tuned, resulting in fast per-scene reconstruction with improved rendering quality and significantly less optimization time than NeRF.",1
"Physics-guided Neural Networks (PGNNs) represent an emerging class of neural networks that are trained using physics-guided (PG) loss functions (capturing violations in network outputs with known physics), along with the supervision contained in data. Existing work in PGNNs have demonstrated the efficacy of adding single PG loss functions in the neural network objectives, using constant trade-off parameters, to ensure better generalizability. However, in the presence of multiple physics loss functions with competing gradient directions, there is a need to adaptively tune the contribution of competing PG loss functions during the course of training to arrive at generalizable solutions. We demonstrate the presence of competing PG losses in the generic neural network problem of solving for the lowest (or highest) eigenvector of a physics-based eigenvalue equation, common to many scientific problems. We present a novel approach to handle competing PG losses and demonstrate its efficacy in learning generalizable solutions in two motivating applications of quantum mechanics and electromagnetic propagation. All the code and data used in this work is available at https://github.com/jayroxis/Cophy-PGNN.",0
"Physics-guided Neural Networks (PGNNs) are a new type of neural network that use physics-guided (PG) loss functions to train. These functions capture any violations in network outputs that contradict known physics, combined with data supervision. Previous research has shown that adding a single PG loss function to the neural network objectives, with a constant trade-off parameter, can improve generalizability. However, when there are multiple physics loss functions with competing gradient directions, it is necessary to adaptively adjust the contribution of each loss function during training to achieve generalizable solutions. In this study, we show that there are competing PG losses in solving for the lowest (or highest) eigenvector of a physics-based eigenvalue equation, a common problem in many scientific fields. We propose a new method for handling competing PG losses and demonstrate its effectiveness in learning generalizable solutions for two applications in quantum mechanics and electromagnetic propagation. All of the code and data used in this study can be found at https://github.com/jayroxis/Cophy-PGNN.",1
"Deep Neural Networks (DNNs) are popularly used for implementing autonomy related tasks in automotive Cyber-Physical Systems (CPSs). However, these networks have been shown to make erroneous predictions to anomalous inputs, which manifests either due to Out-of-Distribution (OOD) data or adversarial attacks. To detect these anomalies, a separate DNN called assurance monitor is often trained and used in parallel to the controller DNN, increasing the resource burden and latency. We hypothesize that a single network that can perform controller predictions and anomaly detection is necessary to reduce the resource requirements. Deep-Radial Basis Function (RBF) networks provide a rejection class alongside the class predictions, which can be utilized for detecting anomalies at runtime. However, the use of RBF activation functions limits the applicability of these networks to only classification tasks. In this paper, we show how the deep-RBF network can be used for detecting anomalies in CPS regression tasks such as continuous steering predictions. Further, we design deep-RBF networks using popular DNNs such as NVIDIA DAVE-II, and ResNet20, and then use the resulting rejection class for detecting adversarial attacks such as a physical attack and data poison attack. Finally, we evaluate these attacks and the trained deep-RBF networks using a hardware CPS testbed called DeepNNCar and a real-world German Traffic Sign Benchmark (GTSB) dataset. Our results show that the deep-RBF networks can robustly detect these attacks in a short time without additional resource requirements.",0
"Automotive Cyber-Physical Systems (CPSs) often use Deep Neural Networks (DNNs) for autonomy related tasks. However, these networks are prone to making incorrect predictions when faced with Out-of-Distribution (OOD) data or adversarial attacks. To address this issue, an assurance monitor is usually trained alongside the controller DNN to detect anomalies, leading to increased resource burden and latency. This paper proposes a solution by using a single network that can perform both controller predictions and anomaly detection, reducing resource requirements. Deep-Radial Basis Function (RBF) networks provide a rejection class that can be utilized for anomaly detection at runtime. While RBF activation functions limit these networks to classification tasks, this paper demonstrates how deep-RBF networks can be used for detecting anomalies in CPS regression tasks such as continuous steering predictions. The authors design deep-RBF networks using popular DNNs and evaluate them using a hardware CPS testbed and a real-world dataset. The results indicate that the deep-RBF networks can detect adversarial attacks without additional resource requirements.",1
"This paper proposes a cascading failure mitigation strategy based on Reinforcement Learning (RL). The motivation of the Multi-Stage Cascading Failure (MSCF) problem and its connection with the challenge of climate change are introduced. The bottom-level corrective control of the MCSF problem is formulated based on DCOPF (Direct Current Optimal Power Flow). Then, to mitigate the MSCF issue by a high-level RL-based strategy, physics-informed reward, action, and state are devised. Besides, both shallow and deep neural network architectures are tested. Experiments on the IEEE 118-bus system by the proposed mitigation strategy demonstrate a promising performance in reducing system collapses.",0
"In this paper, a strategy to mitigate cascading failures using Reinforcement Learning (RL) is proposed. The article presents the Multi-Stage Cascading Failure (MSCF) problem and its relationship with the issue of climate change. The corrective control for the bottom-level of the MSCF problem is formulated using Direct Current Optimal Power Flow (DCOPF). Furthermore, to address the MSCF problem using a high-level RL-based strategy, the authors devise physics-informed reward, action, and state. Both shallow and deep neural network architectures are tested. The proposed mitigation strategy is tested on the IEEE 118-bus system, and the results show a promising reduction in system collapses.",1
"Recovering high-quality 3D human motion in complex scenes from monocular videos is important for many applications, ranging from AR/VR to robotics. However, capturing realistic human-scene interactions, while dealing with occlusions and partial views, is challenging; current approaches are still far from achieving compelling results. We address this problem by proposing LEMO: LEarning human MOtion priors for 4D human body capture. By leveraging the large-scale motion capture dataset AMASS, we introduce a novel motion smoothness prior, which strongly reduces the jitters exhibited by poses recovered over a sequence. Furthermore, to handle contacts and occlusions occurring frequently in body-scene interactions, we design a contact friction term and a contact-aware motion infiller obtained via per-instance self-supervised training. To prove the effectiveness of the proposed motion priors, we combine them into a novel pipeline for 4D human body capture in 3D scenes. With our pipeline, we demonstrate high-quality 4D human body capture, reconstructing smooth motions and physically plausible body-scene interactions. The code and data are available at https://sanweiliti.github.io/LEMO/LEMO.html.",0
"The recovery of high-quality 3D human motion from monocular videos in complex scenes has numerous applications, from AR/VR to robotics. However, capturing realistic human-scene interactions while dealing with occlusions and partial views is a challenging task, and current approaches have not yet achieved satisfactory results. To address this issue, we propose LEMO, which stands for LEarning human MOtion priors for 4D human body capture. By utilizing the large-scale motion capture dataset AMASS, we introduce a new motion smoothness prior that significantly reduces the jitters observed in sequence poses. In addition, we design a contact friction term and a contact-aware motion infiller to handle frequent contacts and occlusions in body-scene interactions, obtained through per-instance self-supervised training. To demonstrate the effectiveness of the proposed motion priors, we incorporate them into a novel pipeline for 4D human body capture in 3D scenes. Our pipeline produces high-quality 4D human body captures with smooth motions and realistic body-scene interactions. Code and data are accessible at https://sanweiliti.github.io/LEMO/LEMO.html.",1
"Sensors with digital outputs require software conversion routines to transform the unitless ADC samples to physical quantities with the correct units. These conversion routines are computationally complex given the limited computational resources of low-power embedded systems. This article presents a set of machine learning methods to learn new, less-complex conversion routines that do not sacrifice accuracy for the BME680 environmental sensor. We present a Pareto analysis of the tradeoff between accuracy and computational overhead for the models and present models that reduce the computational overhead of the existing industry-standard conversion routines for temperature, pressure, and humidity by 62 %, 71 %, and 18 % respectively. The corresponding RMS errors for these methods are 0.0114 $^\circ$C, 0.0280 KPa, and 0.0337 %. These results show that machine learning methods for learning conversion routines can produce conversion routines with reduced computational overhead while maintaining good accuracy.",0
"Conversion of unitless ADC samples to physical quantities with proper units for sensors with digital outputs requires software conversion routines. However, given the limited computational resources of low-power embedded systems, these routines are computationally complex. This article proposes a set of machine learning techniques to learn new conversion routines that are less complex but maintain the accuracy of the BME680 environmental sensor. A Pareto analysis is conducted to determine the tradeoff between accuracy and computational overhead for the models. The presented models reduce the computational overhead of the temperature, pressure, and humidity industry-standard conversion routines by 62%, 71%, and 18%, respectively, while the corresponding RMS errors are 0.0114°C, 0.0280 KPa, and 0.0337%. These results demonstrate that machine learning methods can produce conversion routines with reduced computational overhead and high accuracy.",1
"We present an approach for constructing a surrogate from ensembles of information sources of varying cost and accuracy. The multifidelity surrogate encodes connections between information sources as a directed acyclic graph, and is trained via gradient-based minimization of a nonlinear least squares objective. While the vast majority of state-of-the-art assumes hierarchical connections between information sources, our approach works with flexibly structured information sources that may not admit a strict hierarchy. The formulation has two advantages: (1) increased data efficiency due to parsimonious multifidelity networks that can be tailored to the application; and (2) no constraints on the training data -- we can combine noisy, non-nested evaluations of the information sources. Numerical examples ranging from synthetic to physics-based computational mechanics simulations indicate the error in our approach can be orders-of-magnitude smaller, particularly in the low-data regime, than single-fidelity and hierarchical multifidelity approaches.",0
"Our proposed method involves creating a multifidelity surrogate using different information sources with varying levels of accuracy and cost. The surrogate is constructed as a directed acyclic graph, and is trained by minimizing a nonlinear least squares objective using a gradient-based approach. Unlike other methods that assume a hierarchical structure between information sources, our approach can handle flexible structures that do not have a strict hierarchy. This approach has two benefits: it increases data efficiency by creating parsimonious multifidelity networks that can be customized to the task, and it allows for noisy and non-nested evaluations of information sources during training. Numerical examples illustrate that our method produces significantly lower error, especially in situations with limited data, compared to single-fidelity and hierarchical multifidelity approaches, ranging from synthetic to physics-based computational mechanics simulations.",1
"Methods for designing organic materials with desired properties have high potential impact across fields such as medicine, renewable energy, petrochemical engineering, and agriculture. However, using generative modeling to design substances with desired properties is difficult because candidate compounds must satisfy multiple constraints, including synthetic accessibility and other metrics that are intuitive to domain experts but challenging to quantify. We propose C5T5, a novel self-supervised pretraining method that enables transformers to make zero-shot select-and-replace edits, altering organic substances towards desired property values. C5T5 operates on IUPAC names -- a standardized molecular representation that intuitively encodes rich structural information for organic chemists but that has been largely ignored by the ML community. Our technique requires no edited molecule pairs to train and only a rough estimate of molecular properties, and it has the potential to model long-range dependencies and symmetric molecular structures more easily than graph-based methods. C5T5 also provides a powerful interface to domain experts: it grants users fine-grained control over the generative process by selecting and replacing IUPAC name fragments, which enables experts to leverage their intuitions about structure-activity relationships. We demonstrate C5T5's effectiveness on four physical properties relevant for drug discovery, showing that it learns successful and chemically intuitive strategies for altering molecules towards desired property values.",0
"Developing organic materials with specific properties can have a significant impact on various fields, such as medicine, renewable energy, petrochemical engineering, and agriculture. However, creating such substances using generative modeling is challenging due to the need to satisfy multiple constraints, including synthetic accessibility and other metrics that are difficult to quantify. To address this issue, we propose C5T5, a new self-supervised pretraining method that allows transformers to make zero-shot select-and-replace edits to organic substances to achieve desired property values. C5T5 operates on IUPAC names, a standardized molecular representation that encodes rich structural information for organic chemists but has been overlooked by the ML community. Our technique requires no edited molecule pairs to train and only a rough estimate of molecular properties, making it a more efficient way to model long-range dependencies and symmetric molecular structures than graph-based methods. Additionally, C5T5 provides domain experts with fine-grained control over the generative process, allowing them to leverage their knowledge of structure-activity relationships. Our experiments show that C5T5 is effective in altering molecules towards desired property values for four physical properties relevant to drug discovery, using successful and chemically intuitive strategies.",1
"A novel framework has recently been proposed for designing the molecular structure of chemical compounds with a desired chemical property using both artificial neural networks and mixed integer linear programming. In the framework, a chemical graph with a target chemical value is inferred as a feasible solution of a mixed integer linear program that represents a prediction function and other requirements on the structure of graphs. In this paper, we propose a procedure for generating other feasible solutions of the mixed integer linear program by searching the neighbor of output chemical graph in a search space. The procedure is combined in the framework as a new building block. The results of our computational experiments suggest that the proposed method can generate an additional number of new chemical graphs with up to 50 non-hydrogen atoms.",0
"A recently introduced approach utilizes artificial neural networks and mixed integer linear programming to design the molecular structure of chemical compounds with specific chemical properties. The method involves predicting a chemical graph with the desired value through a mixed integer linear program that considers various structural requirements. This paper presents a method for generating additional feasible solutions to the mixed integer linear program by exploring the neighborhood of the output chemical graph within a search space. This approach is incorporated as a new building block into the framework, and the results of computational experiments demonstrate that it can produce additional chemical graphs with up to 50 non-hydrogen atoms.",1
"Deep learning has become an integral part of various computer vision systems in recent years due to its outstanding achievements for object recognition, facial recognition, and scene understanding. However, deep neural networks (DNNs) are susceptible to be fooled with nearly high confidence by an adversary. In practice, the vulnerability of deep learning systems against carefully perturbed images, known as adversarial examples, poses a dire security threat in the physical world applications. To address this phenomenon, we present, what to our knowledge, is the first ever image set based adversarial defence approach. Image set classification has shown an exceptional performance for object and face recognition, owing to its intrinsic property of handling appearance variability. We propose a robust deep Bayesian image set classification as a defence framework against a broad range of adversarial attacks. We extensively experiment the performance of the proposed technique with several voting strategies. We further analyse the effects of image size, perturbation magnitude, along with the ratio of perturbed images in each image set. We also evaluate our technique with the recent state-of-the-art defence methods, and single-shot recognition task. The empirical results demonstrate superior performance on CIFAR-10, MNIST, ETH-80, and Tiny ImageNet datasets.",0
"In recent years, deep learning has played a crucial role in various computer vision systems due to its remarkable success in object recognition, facial recognition, and scene interpretation. However, deep neural networks (DNNs) are highly susceptible to manipulation by an adversary, which increases the security risks of physical world applications. This vulnerability is caused by adversarial examples, carefully manipulated images that can easily deceive deep learning systems. To combat this issue, we propose the first-ever image set-based adversarial defense approach. Image set classification has proven to be effective in handling appearance variability for object and face recognition, and our approach utilizes robust deep Bayesian image set classification to defend against a broad range of adversarial attacks. We conducted extensive experiments to evaluate our technique's performance with several voting strategies, by analyzing the effects of image size, perturbation magnitude, and the ratio of perturbed images in each image set. We also compared our approach with state-of-the-art defense methods and single-shot recognition tasks. Our empirical results demonstrate superior performance on CIFAR-10, MNIST, ETH-80, and Tiny ImageNet datasets.",1
"A rainfall-runoff model predicts surface runoff either using a physically-based approach or using a systems-based approach. Takagi-Sugeno (TS) Fuzzy models are systems-based approaches and a popular modeling choice for hydrologists in recent decades due to several advantages and improved accuracy in prediction over other existing models. In this paper, we propose a new rainfall-runoff model developed using Gustafson-Kessel (GK) clustering-based TS Fuzzy model. We present comparative performance measures of GK algorithms with two other clustering algorithms: (i) Fuzzy C-Means (FCM), and (ii)Subtractive Clustering (SC). Our proposed TS Fuzzy model predicts surface runoff using: (i) observed rainfall in a drainage basin and (ii) previously observed precipitation flow in the basin outlet. The proposed model is validated using the rainfall-runoff data collected from the sensors installed on the campus of the Indian Institute of Technology, Kharagpur. The optimal number of rules of the proposed model is obtained by different validation indices. A comparative study of four performance criteria: RootMean Square Error (RMSE), Coefficient of Efficiency (CE), Volumetric Error (VE), and Correlation Coefficient of Determination(R) have been quantitatively demonstrated for each clustering algorithm.",0
"A surface runoff forecast can be made by a rainfall-runoff model based on either a physical or systemic approach. Hydrologists have increasingly favored the Takagi-Sugeno (TS) Fuzzy models, which are a systemic approach, due to their accuracy and advantages over other models. The paper proposes a new rainfall-runoff model that utilizes the Gustafson-Kessel (GK) clustering-based TS Fuzzy model. The study compares the performance of the GK algorithm with two other clustering algorithms: Fuzzy C-Means (FCM) and Subtractive Clustering (SC). The proposed TS Fuzzy model uses observed rainfall and previously recorded precipitation flow in a drainage basin to predict surface runoff. The model is validated using rainfall-runoff data from sensors installed on the Indian Institute of Technology, Kharagpur campus. The optimal number of rules for the model is determined using various validation indices, and a comparative study of four performance criteria is presented for each clustering algorithm: Root Mean Square Error (RMSE), Coefficient of Efficiency (CE), Volumetric Error (VE), and Correlation Coefficient of Determination (R).",1
"We present a new pipeline for holistic 3D scene understanding from a single image, which could predict object shapes, object poses, and scene layout. As it is a highly ill-posed problem, existing methods usually suffer from inaccurate estimation of both shapes and layout especially for the cluttered scene due to the heavy occlusion between objects. We propose to utilize the latest deep implicit representation to solve this challenge. We not only propose an image-based local structured implicit network to improve the object shape estimation, but also refine the 3D object pose and scene layout via a novel implicit scene graph neural network that exploits the implicit local object features. A novel physical violation loss is also proposed to avoid incorrect context between objects. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods in terms of object shape, scene layout estimation, and 3D object detection.",0
"Our team has developed a novel pipeline that can provide a comprehensive understanding of a 3D scene based on a single image. This pipeline can accurately predict object shapes, object poses, and the layout of the scene. However, since this is a complex problem, previous methods have struggled to accurately estimate shapes and layout, particularly in cluttered scenes with heavy occlusion between objects. To overcome this challenge, we have implemented the latest deep implicit representation technology. Our approach includes an image-based local structured implicit network to enhance object shape estimation and a novel implicit scene graph neural network to refine 3D object pose and scene layout by utilizing implicit local object features. We have also introduced a new physical violation loss that prevents incorrect context between objects. Our experiments have shown that our method significantly outperforms current state-of-the-art methods in terms of object shape, scene layout estimation, and 3D object detection.",1
"Monocular 3D object detection is of great significance for autonomous driving but remains challenging. The core challenge is to predict the distance of objects in the absence of explicit depth information. Unlike regressing the distance as a single variable in most existing methods, we propose a novel geometry-based distance decomposition to recover the distance by its factors. The decomposition factors the distance of objects into the most representative and stable variables, i.e. the physical height and the projected visual height in the image plane. Moreover, the decomposition maintains the self-consistency between the two heights, leading to robust distance prediction when both predicted heights are inaccurate. The decomposition also enables us to trace the causes of the distance uncertainty for different scenarios. Such decomposition makes the distance prediction interpretable, accurate, and robust. Our method directly predicts 3D bounding boxes from RGB images with a compact architecture, making the training and inference simple and efficient. The experimental results show that our method achieves the state-of-the-art performance on the monocular 3D Object Detection and Birds Eye View tasks of the KITTI dataset, and can generalize to images with different camera intrinsics.",0
"Autonomous driving relies heavily on monocular 3D object detection, which is a difficult task. The main challenge is to estimate object distances without explicit depth information. Most existing methods use a single variable to measure distance, but we propose a new approach that decomposes distance into its factors based on geometry. The two most stable and representative variables are the physical height and the projected visual height in the image plane. Our method maintains consistency between these two heights, resulting in accurate distance estimation even when the predicted heights are imprecise. Our decomposition also allows us to identify the causes of distance uncertainty in various scenarios, making our predictions interpretable, precise, and robust. Our compact architecture enables direct 3D bounding box prediction from RGB images, simplifying training and inference. Our approach outperforms state-of-the-art methods on the KITTI dataset's monocular 3D Object Detection and Birds Eye View tasks and generalizes to images with different camera intrinsics.",1
"Machine learning approaches have recently been leveraged as a substitute or an aid for physical/mathematical modeling approaches to dynamical systems. To develop an efficient machine learning method dedicated to modeling and prediction of multiscale dynamics, we propose a reservoir computing model with diverse timescales by using a recurrent network of heterogeneous leaky integrator neurons. In prediction tasks with fast-slow chaotic dynamical systems including a large gap in timescales of their subsystems dynamics, we demonstrate that the proposed model has a higher potential than the existing standard model and yields a performance comparable to the best one of the standard model even without an optimization of the leak rate parameter. Our analysis reveals that the timescales required for producing each component of target dynamics are appropriately and flexibly selected from the reservoir dynamics by model training.",0
"Recently, machine learning approaches have been utilized as a replacement or a support for physical and mathematical modeling techniques for dynamical systems. Our aim is to create an effective machine learning method dedicated to modeling and predicting multiscale dynamics. To achieve this, we propose a reservoir computing model consisting of a recurrent network of leaky integrator neurons with varying timescales. Through our experiments on fast-slow chaotic dynamical systems with a significant difference in timescales, we demonstrate that our model outperforms the existing standard model and performs similarly to the best version of the standard model, even without optimizing the leak rate parameter. Our analysis indicates that during model training, the timescales necessary for generating each component of the target dynamics are appropriately and flexibly selected from the reservoir dynamics.",1
"Specularity prediction is essential to many computer vision applications by giving important visual cues that could be used in Augmented Reality (AR), Simultaneous Localisation and Mapping (SLAM), 3D reconstruction and material modeling, thus improving scene understanding. However, it is a challenging task requiring numerous information from the scene including the camera pose, the geometry of the scene, the light sources and the material properties. Our previous work have addressed this task by creating an explicit model using an ellipsoid whose projection fits the specularity image contours for a given camera pose. These ellipsoid-based approaches belong to a family of models called JOint-LIght MAterial Specularity (JOLIMAS), where we have attempted to gradually remove assumptions on the scene such as the geometry of the specular surfaces. However, our most recent approach is still limited to uniformly curved surfaces. This paper builds upon these methods by generalising JOLIMAS to any surface geometry while improving the quality of specularity prediction, without sacrificing computation performances. The proposed method establishes a link between surface curvature and specularity shape in order to lift the geometric assumptions from previous work. Contrary to previous work, our new model is built from a physics-based local illumination model namely Torrance-Sparrow, providing a better model reconstruction. Specularity prediction using our new model is tested against the most recent JOLIMAS version on both synthetic and real sequences with objects of varying shape curvatures. Our method outperforms previous approaches in specularity prediction, including the real-time setup, as shown in the supplementary material using videos.",0
"Predicting specularity is crucial for various computer vision applications, such as AR, SLAM, 3D reconstruction, and material modeling, as it provides significant visual cues to enhance scene understanding. However, this task is complex and requires extensive information from the scene, such as camera pose, scene geometry, light sources, and material properties. Previously, we developed an explicit model using an ellipsoid that fits specularity image contours for a given camera pose. This approach is part of a model family called JOLIMAS, which aims to eliminate scene assumptions gradually. However, our latest approach is limited to uniformly curved surfaces. In this paper, we build on our previous work by introducing a generalized JOLIMAS that can handle any surface geometry while providing better specularity prediction quality without compromising computational performance. Our model establishes a link between surface curvature and specularity shape to eliminate previous geometric assumptions. Unlike prior work, we use a physics-based local illumination model called Torrance-Sparrow, which offers superior model reconstruction. We evaluated our approach against the most recent JOLIMAS version on synthetic and real sequences with objects of varying shape curvatures. Our method outperforms previous approaches in specularity prediction, including real-time setups, as demonstrated in the supplementary material using videos.",1
"This work studies the linear approximation of high-dimensional dynamical systems using low-rank dynamic mode decomposition (DMD). Searching this approximation in a data-driven approach is formalised as attempting to solve a low-rank constrained optimisation problem. This problem is non-convex and state-of-the-art algorithms are all sub-optimal. This paper shows that there exists a closed-form solution, which is computed in polynomial time, and characterises the l2-norm of the optimal approximation error. The paper also proposes low-complexity algorithms building reduced models from this optimal solution, based on singular value decomposition or eigen value decomposition. The algorithms are evaluated by numerical simulations using synthetic and physical data benchmarks.",0
"The goal of this work is to examine how to approximate high-dimensional dynamical systems using low-rank dynamic mode decomposition (DMD) in a data-driven approach. This approach involves attempting to solve a low-rank constrained optimization problem, which is non-convex and has sub-optimal algorithms. However, this paper presents a closed-form solution that can be computed in polynomial time. Additionally, the paper characterizes the l2-norm of the optimal approximation error. To build reduced models from this optimal solution, the paper proposes low-complexity algorithms based on singular value decomposition or eigenvalue decomposition. Finally, numerical simulations using synthetic and physical data benchmarks are used to evaluate the algorithms.",1
"The adversarial patch attack against image classification models aims to inject adversarially crafted pixels within a localized restricted image region (i.e., a patch) for inducing model misclassification. This attack can be realized in the physical world by printing and attaching the patch to the victim object and thus imposes a real-world threat to computer vision systems. To counter this threat, we propose PatchCleanser as a certifiably robust defense against adversarial patches that is compatible with any image classifier. In PatchCleanser, we perform two rounds of pixel masking on the input image to neutralize the effect of the adversarial patch. In the first round of masking, we apply a set of carefully generated masks to the input image and evaluate the model prediction on every masked image. If model predictions on all one-masked images reach a unanimous agreement, we output the agreed prediction label. Otherwise, we perform a second round of masking to settle the disagreement, in which we evaluate model predictions on two-masked images to robustly recover the correct prediction label. Notably, we can prove that our defense will always make correct predictions on certain images against any adaptive white-box attacker within our threat model, achieving certified robustness. We extensively evaluate our defense on the ImageNet, ImageNette, CIFAR-10, CIFAR-100, SVHN, and Flowers-102 datasets and demonstrate that our defense achieves similar clean accuracy as state-of-the-art classification models and also significantly improves certified robustness from prior works. Notably, our defense can achieve 83.8% top-1 clean accuracy and 60.4% top-1 certified robust accuracy against a 2%-pixel square patch anywhere on the 1000-class ImageNet dataset.",0
"The adversarial patch attack is a technique used to cause image classification models to misclassify images by inserting adversarial pixels into a small, localized area of the image known as a patch. This type of attack can pose a real-world threat as it can be physically applied to objects by printing and attaching the patch. To address this issue, we propose a defense mechanism called PatchCleanser, which is compatible with any image classifier. Our approach involves two rounds of pixel masking on the input image to counteract the effects of the adversarial patch. During the first round, we apply a set of carefully generated masks to the input image and evaluate the model prediction for each masked image. If the model predictions on all one-masked images are in agreement, we output the agreed-upon prediction label. Otherwise, a second round of masking is performed to settle the disagreement by evaluating model predictions on two-masked images to recover the correct prediction label in a robust manner. Importantly, we can prove that our defense mechanism is always accurate on certain images against any adaptive white-box attacker within our threat model, achieving certified robustness. We evaluate our approach extensively on several datasets and demonstrate that our defense achieves comparable clean accuracy to state-of-the-art classification models while significantly improving certified robustness compared to previous works. Notably, our defense is able to achieve 83.8% top-1 clean accuracy and 60.4% top-1 certified robust accuracy against a 2%-pixel square patch anywhere on the 1000-class ImageNet dataset.",1
"In spin based quantum dot arrays, a leading technology for quantum computation applications, material or fabrication imprecisions affect the behaviour of the device, which is compensated via tuning parameters. Automatic tuning of these device parameters constitutes a formidable challenge for machine-learning. Here, we present the first practical algorithm for controlling the transition of electrons in a spin qubit array. We exploit a connection to computational geometry and phrase the task as estimating a convex polytope from measurements.   Our proposed algorithm uses active learning, to find the count, shapes and sizes of all facets of a given polytope. We test our algorithm on artifical polytopes as well as a real 2x2 spin qubit array. Our results show that we can reliably find the facets of the polytope, including small facets with sizes on the order of the measurement precision. We discuss the implications of the NP-hardness of the underlying estimation problem and outline design considerations, limitations and tuning strategies for controlling future large-scale spin qubit devices.",0
"Quantum computation applications rely on spin based quantum dot arrays, but imprecisions in material or fabrication can impact device behavior. To compensate for this, tuning parameters are used, but machine-learning faces difficulties in automatically controlling these parameters. Our algorithm solves this challenge by controlling the transition of electrons in a spin qubit array through estimating a convex polytope using computational geometry. Active learning is used to find the count, shapes, and sizes of all facets of the polytope. Our algorithm is tested on both artificial and real 2x2 spin qubit arrays, successfully identifying even small facets with precision. However, the underlying estimation problem is NP-hard, and thus design considerations, limitations, and tuning strategies must be considered for future large-scale spin qubit devices.",1
"Nowadays, with the rapid development of IoT (Internet of Things) and CPS (Cyber-Physical Systems) technologies, big spatiotemporal data are being generated from mobile phones, car navigation systems, and traffic sensors. By leveraging state-of-the-art deep learning technologies on such data, urban traffic prediction has drawn a lot of attention in AI and Intelligent Transportation System community. The problem can be uniformly modeled with a 3D tensor (T, N, C), where T denotes the total time steps, N denotes the size of the spatial domain (i.e., mesh-grids or graph-nodes), and C denotes the channels of information. According to the specific modeling strategy, the state-of-the-art deep learning models can be divided into three categories: grid-based, graph-based, and multivariate time-series models. In this study, we first synthetically review the deep traffic models as well as the widely used datasets, then build a standard benchmark to comprehensively evaluate their performances with the same settings and metrics. Our study named DL-Traff is implemented with two most popular deep learning frameworks, i.e., TensorFlow and PyTorch, which is already publicly available as two GitHub repositories https://github.com/deepkashiwa20/DL-Traff-Grid and https://github.com/deepkashiwa20/DL-Traff-Graph. With DL-Traff, we hope to deliver a useful resource to researchers who are interested in spatiotemporal data analysis.",0
"The emergence of IoT and CPS technologies has led to the generation of vast amounts of spatiotemporal data from various sources such as mobile phones, car navigation systems, and traffic sensors. As a result, predicting urban traffic has become a popular topic in the AI and Intelligent Transportation System community. To model the problem, a 3D tensor (T, N, C) can be used, where T represents time steps, N represents the spatial domain size, and C represents the channels of information. Deep learning models used for this problem can be categorized into grid-based, graph-based, and multivariate time-series models. This study comprehensively evaluates the performance of these models using a standard benchmark named DL-Traff. DL-Traff is implemented with TensorFlow and PyTorch and is available on GitHub as two repositories. The aim of this study is to provide researchers interested in spatiotemporal data analysis with a valuable resource.",1
"Fingerprints are one of the most widely explored biometric traits. Specifically, contact-based fingerprint recognition systems reign supreme due to their robustness, portability and the extensive research work done in the field. However, these systems suffer from issues such as hygiene, sensor degradation due to constant physical contact, and latent fingerprint threats. In this paper, we propose an approach for developing a contactless fingerprint recognition system that captures finger photo from a distance using an image sensor in a suitable environment. The captured finger photos are then processed further to obtain global and local (minutiae-based) features. Specifically, a Siamese convolutional neural network (CNN) is designed to extract global features from a given finger photo. The proposed system computes matching scores from CNN-based features and minutiae-based features. Finally, the two scores are fused to obtain the final matching score between the probe and reference fingerprint templates. Most importantly, the proposed system is developed using the Nvidia Jetson Nano development kit, which allows us to perform contactless fingerprint recognition in real-time with minimum latency and acceptable matching accuracy. The performance of the proposed system is evaluated on an in-house IITI contactless fingerprint dataset (IITI-CFD) containing 105train and 100 test subjects. The proposed system achieves an equal-error-rate of 2.19% on IITI-CFD.",0
"Fingerprinting is a commonly studied biometric trait, with contact-based recognition systems being the most popular due to their durability, portability, and extensive research. However, these systems face challenges such as cleanliness, sensor deterioration from constant contact, and latent fingerprint risks. This study proposes a method for creating a contactless fingerprint recognition system that captures finger images from a distance using a suitable environment's image sensor. The resulting finger images are processed to acquire global and local features. A Siamese convolutional neural network is used to extract global features, and matching scores are computed from both CNN-based and minutiae-based features. These scores are then combined to obtain the final matching score. The proposed system was created using the Nvidia Jetson Nano development kit, allowing for real-time, contactless fingerprint recognition with minimal latency and acceptable matching accuracy. The performance of the system was tested on an in-house dataset containing 105 train and 100 test subjects, achieving an equal-error-rate of 2.19%.",1
"This work aims to assess the reality and feasibility of the adversarial attack against cardiac diagnosis system powered by machine learning algorithms. To this end, we introduce adversarial beats, which are adversarial perturbations tailored specifically against electrocardiograms (ECGs) beat-by-beat classification system. We first formulate an algorithm to generate adversarial examples for the ECG classification neural network model, and study its attack success rate. Next, to evaluate its feasibility in a physical environment, we mount a hardware attack by designing a malicious signal generator which injects adversarial beats into ECG sensor readings. To the best of our knowledge, our work is the first in evaluating the proficiency of adversarial examples for ECGs in a physical setup. Our real-world experiments demonstrate that adversarial beats successfully manipulated the diagnosis results 3-5 times out of 40 attempts throughout the course of 2 minutes. Finally, we discuss the overall feasibility and impact of the attack, by clearly defining motives and constraints of expected attackers along with our experimental results.",0
"The goal of this study is to examine the viability of an adversarial attack on a machine learning-based cardiac diagnosis system. Specifically, we introduce adversarial beats, which are designed to disrupt the electrocardiogram (ECG) beat-by-beat classification system. We begin by developing an algorithm to create adversarial examples for the ECG classification neural network model and evaluate its effectiveness. Additionally, we test the feasibility of this attack in a physical environment by creating a malicious signal generator that injects adversarial beats into ECG sensor readings. Our study is the first to evaluate the proficiency of adversarial examples for ECGs in a physical setting. Our experiments reveal that adversarial beats were able to manipulate diagnosis results 3-5 times out of 40 attempts over a 2-minute period. We also discuss the feasibility and potential impact of this attack, taking into account the motives and limitations of expected attackers, and present our experimental findings.",1
"Inertial Measurement Unit (IMU) sensors are present in everyday devices such as smartphones and fitness watches. As a result, the array of health-related research and applications that tap onto this data has been growing, but little attention has been devoted to the prediction of an individual's heart rate (HR) from IMU data, when undergoing a physical activity. Would that be even possible? If so, this could be used to design personalized sets of aerobic exercises, for instance. In this work, we show that it is viable to obtain accurate HR predictions from IMU data using Recurrent Neural Networks, provided only access to HR and IMU data from a short-lived, previously executed activity. We propose a novel method for initializing an RNN's hidden state vectors, using a specialized network that attempts to extract an embedding of the physical conditioning (PCE) of a subject. We show that using a discriminator in the training phase to help the model learn whether two PCEs belong to the same individual further reduces the prediction error. We evaluate the proposed model when predicting the HR of 23 subjects performing a variety of physical activities from IMU data available in public datasets (PAMAP2, PPG-DaLiA). For comparison, we use as baselines the only model specifically proposed for this task and an adapted state-of-the-art model for Human Activity Recognition (HAR), a closely related task. Our method, PCE-LSTM, yields over 10% lower mean absolute error. We demonstrate empirically that this error reduction is in part due to the use of the PCE. Last, we use the two datasets (PPG-DaLiA, WESAD) to show that PCE-LSTM can also be successfully applied when photoplethysmography (PPG) sensors are available, outperforming the state-of-the-art deep learning baselines by more than 30%.",0
"IMU sensors can be found in everyday devices, like smartphones and fitness watches. Although the use of this data for health-related research and applications has been increasing, little attention has been given to predicting an individual's heart rate (HR) from IMU data during physical activity. However, if this is possible, it could be used to create personalized aerobic exercise plans. This study shows that accurate HR predictions can be achieved using Recurrent Neural Networks, with access to HR and IMU data from a short-lived, previously executed activity. A new method for initializing an RNN's hidden state vectors is proposed, using a specialized network to extract an embedding of a subject's physical conditioning (PCE). A discriminator is used during training to help the model learn whether two PCEs belong to the same individual, which further reduces the prediction error. The proposed model, PCE-LSTM, is evaluated on 23 subjects performing various physical activities using IMU data from public datasets (PAMAP2, PPG-DaLiA). PCE-LSTM yields over 10% lower mean absolute error compared to only model specifically proposed for this task and an adapted state-of-the-art model for Human Activity Recognition (HAR). The use of PCE is shown to contribute to the error reduction. Furthermore, PCE-LSTM is successfully applied to datasets with PPG sensors, and outperforms state-of-the-art deep learning baselines by more than 30%.",1
"Floods wreak havoc throughout the world, causing billions of dollars in damages, and uprooting communities, ecosystems and economies. Accurate and robust flood detection including delineating open water flood areas and identifying flood levels can aid in disaster response and mitigation. However, estimating flood levels remotely is of essence as physical access to flooded areas is limited and the ability to deploy instruments in potential flood zones can be dangerous. Aligning flood extent mapping with local topography can provide a plan-of-action that the disaster response team can consider. Thus, remote flood level estimation via satellites like Sentinel-1 can prove to be remedial. The Emerging Techniques in Computational Intelligence (ETCI) competition on Flood Detection tasked participants with predicting flooded pixels after training with synthetic aperture radar (SAR) images in a supervised setting. We use a cyclical approach involving two stages (1) training an ensemble model of multiple UNet architectures with available high and low confidence labeled data and, generating pseudo labels or low confidence labels on the entire unlabeled test dataset, and then, (2) filter out quality generated labels and, (3) combining the generated labels with the previously available high confidence labeled dataset. This assimilated dataset is used for the next round of training ensemble models. This cyclical process is repeated until the performance improvement plateaus. Additionally, we post process our results with Conditional Random Fields. Our approach sets the second highest score on the public hold-out test leaderboard for the ETCI competition with 0.7654 IoU. To the best of our knowledge we believe this is one of the first works to try out semi-supervised learning to improve flood segmentation models.",0
"Floods cause extensive damage worldwide, with devastating effects on communities, ecosystems, and economies, resulting in billions of dollars in losses. To aid in disaster response and mitigation, it is crucial to accurately detect and delineate flooded areas and identify flood levels. However, it is challenging to estimate flood levels remotely, as access to flooded areas is often restricted, and deploying instruments in potential flood zones is dangerous. Aligning flood extent mapping with local topography can provide an action plan for disaster response teams. Therefore, remote flood level estimation using satellites like Sentinel-1 can be beneficial. The Flood Detection competition at Emerging Techniques in Computational Intelligence (ETCI) required participants to predict flooded pixels using synthetic aperture radar (SAR) images in a supervised setting. Our approach involves a cyclical process that includes training an ensemble model of multiple UNet architectures with labeled data, generating pseudo labels on the unlabeled test dataset, filtering out low-quality labels, and combining them with high confidence labeled data for the next round of training. We repeat this process until performance improvement plateaus and post-process our results with Conditional Random Fields. Our approach achieved the second-highest score on the public hold-out test leaderboard for the ETCI competition, with 0.7654 IoU. We believe this is one of the first works to explore semi-supervised learning to improve flood segmentation models.",1
"Optimizing the reliability and the robustness of a design is important but often unaffordable due to high sample requirements. Surrogate models based on statistical and machine learning methods are used to increase the sample efficiency. However, for higher dimensional or multi-modal systems, surrogate models may also require a large amount of samples to achieve good results. We propose a sequential sampling strategy for the surrogate based solution of multi-objective reliability based robust design optimization problems. Proposed local Latin hypercube refinement (LoLHR) strategy is model-agnostic and can be combined with any surrogate model because there is no free lunch but possibly a budget one. The proposed method is compared to stationary sampling as well as other proposed strategies from the literature. Gaussian process and support vector regression are both used as surrogate models. Empirical evidence is presented, showing that LoLHR achieves on average better results compared to other surrogate based strategies on the tested examples.",0
"Design reliability and robustness optimization is crucial, but often costly due to high sample requirements. To improve sample efficiency, surrogate models based on statistical and machine learning methods are commonly utilized. However, for more complex systems with higher dimensions or multi-modalities, surrogate models may still require a substantial number of samples to obtain satisfactory outcomes. We present a sequential sampling approach for solving multi-objective reliability-based robust design optimization problems using surrogate models. Our proposed local Latin hypercube refinement (LoLHR) strategy is versatile in that it is model-agnostic and can be combined with any surrogate model, given budget constraints. We compare the LoLHR method with stationary sampling and other strategies proposed in the literature, utilizing Gaussian process and support vector regression as surrogate models. Empirical findings demonstrate that on average, LoLHR outperforms other surrogate-based approaches on the tested examples.",1
"This paper proposes GraviCap, i.e., a new approach for joint markerless 3D human motion capture and object trajectory estimation from monocular RGB videos. We focus on scenes with objects partially observed during a free flight. In contrast to existing monocular methods, we can recover scale, object trajectories as well as human bone lengths in meters and the ground plane's orientation, thanks to the awareness of the gravity constraining object motions. Our objective function is parametrised by the object's initial velocity and position, gravity direction and focal length, and jointly optimised for one or several free flight episodes. The proposed human-object interaction constraints ensure geometric consistency of the 3D reconstructions and improved physical plausibility of human poses compared to the unconstrained case. We evaluate GraviCap on a new dataset with ground-truth annotations for persons and different objects undergoing free flights. In the experiments, our approach achieves state-of-the-art accuracy in 3D human motion capture on various metrics. We urge the reader to watch our supplementary video. Both the source code and the dataset are released; see http://4dqv.mpi-inf.mpg.de/GraviCap/.",0
"The GraviCap method is introduced in this paper as a novel approach to jointly capture markerless 3D human motion and object trajectory estimation from monocular RGB videos. Its focus is on scenes where objects are only partly visible during free flights. This approach differs from existing monocular methods by recovering the scale, object trajectories, human bone lengths, and ground plane orientation in meters, thanks to the knowledge of gravity's effect on object movements. The objective function is defined by parameters such as the object's initial velocity and position, gravity direction, and focal length, which are all optimized in one or several free flight episodes. The proposed human-object interaction constraints ensure the consistent geometry of the 3D reconstructions and improve the physical plausibility of human poses. The performance of GraviCap is evaluated on a new dataset with ground-truth annotations for persons and various objects undergoing free flights, achieving state-of-the-art accuracy in 3D human motion capture across multiple metrics. The source code and dataset are publicly available at http://4dqv.mpi-inf.mpg.de/GraviCap/. The supplementary video is also recommended for further understanding.",1
"We apply adversarial domain adaptation in unsupervised setting to reduce sample bias in a supervised high energy physics events classifier training. We make use of a neural network containing event and domain classifier with a gradient reversal layer to simultaneously enable signal versus background events classification on the one hand, while on the other hand minimising the difference in response of the network to background samples originating from different MC models via adversarial domain classification loss. We show the successful bias removal on the example of simulated events at the LHC with $t\bar{t}H$ signal versus $t\bar{t}b\bar{b}$ background classification and discuss implications and limitations of the method",0
"In order to address sample bias in a supervised high energy physics events classifier training, we employ adversarial domain adaptation in an unsupervised setting. Our approach involves using a neural network that includes an event and domain classifier along with a gradient reversal layer. This allows us to achieve both signal versus background events classification and minimize the difference in network response to background samples from different MC models through adversarial domain classification loss. We demonstrate the effectiveness of our approach using simulated events at the LHC with $t\bar{t}H$ signal versus $t\bar{t}b\bar{b}$ background classification, and we also explore the limitations and implications of our method.",1
"Deeper and wider CNNs are known to provide improved performance for deep learning tasks. However, most such networks have poor performance gain per parameter increase. In this paper, we investigate whether the gain observed in deeper models is purely due to the addition of more optimization parameters or whether the physical size of the network as well plays a role. Further, we present a novel rescaling strategy for CNNs based on learnable repetition of its parameters. Based on this strategy, we rescale CNNs without changing their parameter count, and show that learnable sharing of weights itself can provide significant boost in the performance of any given model without changing its parameter count. We show that small base networks when rescaled, can provide performance comparable to deeper networks with as low as 6% of optimization parameters of the deeper one.   The relevance of weight sharing is further highlighted through the example of group-equivariant CNNs. We show that the significant improvements obtained with group-equivariant CNNs over the regular CNNs on classification problems are only partly due to the added equivariance property, and part of it comes from the learnable repetition of network weights. For rot-MNIST dataset, we show that up to 40% of the relative gain reported by state-of-the-art methods for rotation equivariance could actually be due to just the learnt repetition of weights.",0
"Improved performance for deep learning tasks can be achieved by using deeper and wider CNNs. However, the performance gain per parameter increase in most such networks is poor. This study aims to determine whether the performance improvement observed in deeper models is solely due to the addition of more optimization parameters or if the physical size of the network also plays a role. A new rescaling strategy for CNNs based on learnable repetition of its parameters is presented. By using this strategy, CNNs can be rescaled without changing their parameter count, and it is demonstrated that learnable sharing of weights can significantly enhance the performance of any given model without changing its parameter count. Small base networks, when rescaled, can provide performance comparable to deeper networks with as little as 6% of optimization parameters of the deeper one. The importance of weight sharing is further demonstrated through the example of group-equivariant CNNs. It is shown that the significant improvements obtained with group-equivariant CNNs over regular CNNs on classification problems are partly due to the added equivariance property, and partly due to the learnable repetition of network weights. For the rot-MNIST dataset, it is demonstrated that up to 40% of the relative gain reported by state-of-the-art methods for rotation equivariance could be due to just the learnt repetition of weights.",1
"We present a general learning-based solution for restoring images suffering from spatially-varying degradations. Prior approaches are typically degradation-specific and employ the same processing across different images and different pixels within. However, we hypothesize that such spatially rigid processing is suboptimal for simultaneously restoring the degraded pixels as well as reconstructing the clean regions of the image. To overcome this limitation, we propose SPAIR, a network design that harnesses distortion-localization information and dynamically adjusts computation to difficult regions in the image. SPAIR comprises of two components, (1) a localization network that identifies degraded pixels, and (2) a restoration network that exploits knowledge from the localization network in filter and feature domain to selectively and adaptively restore degraded pixels. Our key idea is to exploit the non-uniformity of heavy degradations in spatial-domain and suitably embed this knowledge within distortion-guided modules performing sparse normalization, feature extraction and attention. Our architecture is agnostic to physical formation model and generalizes across several types of spatially-varying degradations. We demonstrate the efficacy of SPAIR individually on four restoration tasks-removal of rain-streaks, raindrops, shadows and motion blur. Extensive qualitative and quantitative comparisons with prior art on 11 benchmark datasets demonstrate that our degradation-agnostic network design offers significant performance gains over state-of-the-art degradation-specific architectures. Code available at https://github.com/human-analysis/spatially-adaptive-image-restoration.",0
"Our study proposes a learning-based solution to restore images affected by spatially-varying degradations. Current approaches are limited by their degradation-specific and inflexible processing across different images and pixels. We believe that this approach is not optimal for restoring degraded pixels and reconstructing clean regions of the image simultaneously. To address this issue, we introduce SPAIR, a network design that uses distortion-localization information to adjust computation to challenging areas in the image. SPAIR consists of a localization network that identifies degraded pixels and a restoration network that selectively and adaptively restores them using knowledge from the localization network. Our approach utilizes the non-uniformity of heavy degradations and incorporates it into distortion-guided modules that perform sparse normalization, feature extraction, and attention. Our architecture is versatile and works across various types of spatially-varying degradations. We demonstrate the effectiveness of SPAIR in four restoration tasks: removal of rain-streaks, raindrops, shadows, and motion blur. Our results show that SPAIR outperforms previous degradation-specific architectures on 11 benchmark datasets. We provide the code for SPAIR on https://github.com/human-analysis/spatially-adaptive-image-restoration.",1
"The Turing mechanism describes the emergence of spatial patterns due to spontaneous symmetry breaking in reaction-diffusion processes and underlies many developmental processes. Identifying Turing mechanisms in biological systems defines a challenging problem. This paper introduces an approach to the prediction of Turing parameter values from observed Turing patterns. The parameter values correspond to a parametrized system of reaction-diffusion equations that generate Turing patterns as steady state. The Gierer-Meinhardt model with four parameters is chosen as a case study. A novel invariant pattern representation based on resistance distance histograms is employed, along with Wasserstein kernels, in order to cope with the highly variable arrangement of local pattern structure that depends on the initial conditions which are assumed to be unknown. This enables to compute physically plausible distances between patterns, to compute clusters of patterns and, above all, model parameter prediction: for small training sets, classical state-of-the-art methods including operator-valued kernels outperform neural networks that are applied to raw pattern data, whereas for large training sets the latter are more accurate. Excellent predictions are obtained for single parameter values and reasonably accurate results for jointly predicting all parameter values.",0
"The Turing mechanism is responsible for the formation of spatial patterns through spontaneous symmetry breaking in reaction-diffusion processes, which is crucial in many developmental processes. However, identifying Turing mechanisms in biological systems poses a challenging problem. This paper presents an approach to predict Turing parameter values from observed Turing patterns. The parameter values correspond to a parametrized system of reaction-diffusion equations that generate Turing patterns as steady state. The study chooses the Gierer-Meinhardt model with four parameters as a case study. To address the highly variable arrangement of local pattern structure, the paper employs a novel invariant pattern representation based on resistance distance histograms along with Wasserstein kernels. This approach enables the computation of physically plausible distances between patterns, clusters of patterns, and model parameter prediction. The study finds that for small training sets, classical state-of-the-art methods such as operator-valued kernels outperform neural networks applied to raw pattern data, whereas for large training sets, the latter are more accurate. The study obtains excellent predictions for single parameter values and reasonably accurate results for jointly predicting all parameter values.",1
"Although LEGO sets have entertained generations of children and adults, the challenge of designing customized builds matching the complexity of real-world or imagined scenes remains too great for the average enthusiast. In order to make this feat possible, we implement a system that generates a LEGO brick model from 2D images. We design a novel solution to this problem that uses an octree-structured autoencoder trained on 3D voxelized models to obtain a feasible latent representation for model reconstruction, and a separate network trained to predict this latent representation from 2D images. LEGO models are obtained by algorithmic conversion of the 3D voxelized model to bricks. We demonstrate first-of-its-kind conversion of photographs to 3D LEGO models. An octree architecture enables the flexibility to produce multiple resolutions to best fit a user's creative vision or design needs. In order to demonstrate the broad applicability of our system, we generate step-by-step building instructions and animations for LEGO models of objects and human faces. Finally, we test these automatically generated LEGO sets by constructing physical builds using real LEGO bricks.",0
"Despite the enduring popularity of LEGO sets among children and adults alike, creating customized models that match the complexity of real or imagined scenes is still a daunting task for most enthusiasts. To overcome this challenge, we have developed a system that can generate LEGO brick models from 2D images. Our solution involves an innovative approach that utilizes an octree-structured autoencoder for obtaining a feasible latent representation of the model, which is then reconstructed by a separate network trained to predict this representation from 2D images. By algorithmically converting 3D voxelized models to bricks, we have achieved a unique conversion of photographs to 3D LEGO models. Our system also allows for multiple resolutions to accommodate users' creative vision or design needs, and we have successfully generated step-by-step building instructions and animations for LEGO models of objects and human faces. Finally, we have tested our system by constructing physical builds using real LEGO bricks.",1
"Urban rail transit (URT) system plays a dominating role in many megacities like Beijing and Hong Kong. Due to its important role and complex nature, it is always in great need for public agencies to better understand the performance of the URT system. This paper focuses on an essential and hard problem to estimate the network-wide link travel time and station waiting time using the automatic fare collection (AFC) data in the URT system, which is beneficial to better understand the system-wide real-time operation state. The emerging data-driven techniques, such as computational graph (CG) models in the machine learning field, provide a new solution for solving this problem. In this study, we first formulate a data-driven estimation optimization framework to estimate the link travel time and station waiting time. Then, we cast the estimation optimization model into a CG framework to solve the optimization problem and obtain the estimation results. The methodology is verified on a synthetic URT network and applied to a real-world URT network using the synthetic and real-world AFC data, respectively. Results show the robustness and effectiveness of the CG-based framework. To the best of our knowledge, this is the first time that the CG is applied to the URT. This study can provide critical insights to better understand the operational state in URT.",0
"The URT system plays a dominant role in megacities such as Beijing and Hong Kong. Due to its complexity and importance, public agencies require a better understanding of the system's performance. This paper focuses on estimating network-wide link travel time and station waiting time using AFC data to gain insight into the system's real-time operation state. Data-driven techniques, such as computational graph models, offer a new solution to this problem. This study formulates a data-driven estimation optimization framework and applies it to both a synthetic and real-world URT network. The CG-based framework proves to be robust and effective, providing critical insights into the operational state of URT systems. This is the first time that CG has been applied to URT, making this study particularly noteworthy.",1
"Increased drone proliferation in civilian and professional settings has created new threat vectors for airports and national infrastructures. The economic damage for a single major airport from drone incursions is estimated to be millions per day. Due to the lack of diverse drone training data, accurate training of deep learning detection algorithms under scarce data is an open challenge. Existing methods largely rely on collecting diverse and comprehensive experimental drone footage data, artificially induced data augmentation, transfer and meta-learning, as well as physics-informed learning. However, these methods cannot guarantee capturing diverse drone designs and fully understanding the deep feature space of drones. Here, we show how understanding the general distribution of the drone data via a Generative Adversarial Network (GAN) and explaining the missing features using Topological Data Analysis (TDA) - can allow us to acquire missing data to achieve rapid and more accurate learning. We demonstrate our results on a drone image dataset, which contains both real drone images as well as simulated images from computer-aided design. When compared to random data collection (usual practice - discriminator accuracy of 94.67\% after 200 epochs), our proposed GAN-TDA informed data collection method offers a significant 4\% improvement (99.42\% after 200 epochs). We believe that this approach of exploiting general data distribution knowledge form neural networks can be applied to a wide range of scarce data open challenges.",0
"The proliferation of drones in civilian and professional settings has created new risks for airports and national infrastructure. The economic impact of drone incursions on a single major airport is estimated to be millions of dollars per day. However, accurately training deep learning detection algorithms with diverse drone data is a challenge. Current methods rely on experimental footage, artificial data augmentation, transfer and meta-learning, and physics-informed learning. These approaches do not guarantee capturing diverse drone designs or fully understanding the deep feature space of drones. To address this issue, we propose using a Generative Adversarial Network (GAN) and Topological Data Analysis (TDA) to understand the general distribution of drone data and acquire missing data for more accurate learning. Our results on a drone image dataset demonstrated a 4% improvement in accuracy compared to random data collection. We believe this approach can be applied to various scarce data challenges.",1
"Physical photographs now can be conveniently scanned by smartphones and stored forever as a digital version, yet the scanned photos are not restored well. One solution is to train a supervised deep neural network on many digital photos and the corresponding scanned photos. However, it requires a high labor cost, leading to limited training data. Previous works create training pairs by simulating degradation using image processing techniques. Their synthetic images are formed with perfectly scanned photos in latent space. Even so, the real-world degradation in smartphone photo scanning remains unsolved since it is more complicated due to lens defocus, lighting conditions, losing details via printing. Besides, locally structural misalignment still occurs in data due to distorted shapes captured in a 3-D world, reducing restoration performance and the reliability of the quantitative evaluation. To solve these problems, we propose a semi-supervised Deep Photo Scan (DPScan). First, we present a way of producing real-world degradation and provide the DIV2K-SCAN dataset for smartphone-scanned photo restoration. Also, Local Alignment is proposed to reduce the minor misalignment remaining in data. Second, we simulate many different variants of the real-world degradation using low-level image transformation to gain a generalization in smartphone-scanned image properties, then train a degradation network to generalize all styles of degradation and provide pseudo-scanned photos for unscanned images as if they were scanned by a smartphone. Finally, we propose a Semi-Supervised Learning that allows our restoration network to be trained on both scanned and unscanned images, diversifying training image content. As a result, the proposed DPScan quantitatively and qualitatively outperforms its baseline architecture, state-of-the-art academic research, and industrial products in smartphone photo scanning.",0
"Nowadays, physical photographs can be scanned using smartphones and saved as digital copies, but the resulting scanned photos do not always look good. One possible solution is to use a supervised deep neural network to learn from digital and scanned photos, but this is costly and limited by the amount of training data available. Some researchers have tried to create training pairs by artificially degrading digital images, but this does not address the real-world degradation caused by lens defocus, lighting, and printing. In addition, the 3-D nature of the original objects can cause misalignment in the scanned images, which affects the restoration quality and reliability of evaluation. To overcome these challenges, we propose a semi-supervised approach called DPScan. We introduce a dataset that includes real-world degradation for smartphone-scanned photos and a local alignment technique to improve data quality. We also use low-level image transformation to simulate different types of degradation, which allows our network to generalize better. Finally, we use semi-supervised learning to train on both scanned and unscanned images, which enhances the diversity of our training data. Our approach outperforms existing methods in terms of both quantitative and qualitative measures.",1
"One of the most promising approaches for data analysis and exploration of large data sets is Machine Learning techniques that are inspired by brain models. Such methods use alternative learning rules potentially more efficiently than established learning rules. In this work, we focus on the potential of brain-inspired ML for exploiting High-Performance Computing (HPC) resources to solve ML problems: we discuss the BCPNN and an HPC implementation, called StreamBrain, its computational cost, suitability to HPC systems. As an example, we use StreamBrain to analyze the Higgs Boson dataset from High Energy Physics and discriminate between background and signal classes in collisions of high-energy particle colliders. Overall, we reach up to 69.15% accuracy and 76.4% Area Under the Curve (AUC) performance.",0
"Machine Learning techniques inspired by brain models are a promising approach for analyzing and exploring large data sets. These methods use alternative learning rules, which have the potential to be more efficient than established rules. This study focuses on the potential of brain-inspired ML to exploit High-Performance Computing (HPC) resources for solving ML problems. The BCPNN and StreamBrain HPC implementation are discussed, including their computational costs and suitability for HPC systems. The Higgs Boson dataset from High Energy Physics is used as an example to demonstrate StreamBrain's ability to discriminate between background and signal classes in collisions of high-energy particle colliders. The results show an overall accuracy of up to 69.15% and a performance of 76.4% Area Under the Curve (AUC).",1
"It is well known that Neural Network (network) performance often degrades when a network is used in novel operating domains that differ from its training and testing domains. This is a major limitation, as networks are being integrated into safety critical, cyber-physical systems that must work in unconstrained environments, e.g., perception for autonomous vehicles. Training networks that generalize to novel operating domains and that extract robust features is an active area of research, but previous work fails to predict what the network performance will be in novel operating domains. We propose the task Network Generalization Prediction: predicting the expected network performance in novel operating domains. We describe the network performance in terms of an interpretable Context Subspace, and we propose a methodology for selecting the features of the Context Subspace that provide the most information about the network performance. We identify the Context Subspace for a pretrained Faster RCNN network performing pedestrian detection on the Berkeley Deep Drive (BDD) Dataset, and demonstrate Network Generalization Prediction accuracy within 5% or less of observed performance. We also demonstrate that the Context Subspace from the BDD Dataset is informative for completely unseen datasets, JAAD and Cityscapes, where predictions have a bias of 10% or less.",0
"Neural Networks (networks) struggle to maintain their performance when utilized in new operating domains that differ from their training and testing domains. This is a significant drawback as these networks are integrated into systems that require safety and reliability in unconstrained environments, such as autonomous vehicles. Although researchers are actively working on developing networks that can generalize to new operating domains and extract resilient features, previous studies have failed to anticipate network performance in these novel domains. To address this issue, we propose the Network Generalization Prediction task, which predicts the expected network performance in new operating domains. Our approach involves describing the network performance in an interpretable Context Subspace and utilizing a methodology to select Context Subspace features that provide the most information about network performance. We have identified the Context Subspace for a pretrained Faster RCNN network used for pedestrian detection on the Berkeley Deep Drive (BDD) Dataset and have demonstrated Network Generalization Prediction accuracy within 5% or less of actual performance. Furthermore, we have shown that the Context Subspace from the BDD Dataset is effective for predicting network performance on completely unseen datasets, such as JAAD and Cityscapes, with a bias of 10% or less.",1
"We explore an application of the Physics Informed Neural Networks (PINNs) in conjunction with Airy stress functions and Fourier series to find optimal solutions to a few reference biharmonic problems of elasticity and elastic plate theory. Biharmonic relations are fourth-order partial differential equations (PDEs) that are challenging to solve using classical numerical methods, and have not been addressed using PINNs. Our work highlights a novel application of classical analytical methods to guide the construction of efficient neural networks with the minimal number of parameters that are very accurate and fast to evaluate. In particular, we find that enriching feature space using Airy stress functions can significantly improve the accuracy of PINN solutions for biharmonic PDEs.",0
"The utilization of Physics Informed Neural Networks (PINNs) in combination with Airy stress functions and Fourier series is examined to determine optimal solutions for a few reference biharmonic problems in elasticity and elastic plate theory. Classical numerical methods encounter difficulties when solving fourth-order partial differential equations (PDEs) known as biharmonic relations, which have yet to be tackled by PINNs. Our study showcases a new approach where classical analytical methods guide the development of efficient neural networks with minimal parameters that are both accurate and quick to evaluate. Specifically, we discovered that including Airy stress functions in the feature space can notably enhance the precision of PINN solutions for biharmonic PDEs.",1
"Perturbation-based attacks, while not physically realizable, have been the main emphasis of adversarial machine learning (ML) research. Patch-based attacks by contrast are physically realizable, yet most work has focused on 2D domain with recent forays into 3D. Characterizing the robustness properties of patch attacks and their invariance to 3D pose is important, yet not fully elucidated, and is the focus of this paper. To this end, several contributions are made here: A) we develop a new metric called mean Attack Success over Transformations (mAST) to evaluate patch attack robustness and invariance; and B), we systematically assess robustness of patch attacks to 3D position and orientation for various conditions; in particular, we conduct a sensitivity analysis which provides important qualitative insights into attack effectiveness as a function of the 3D pose of a patch relative to the camera (rotation, translation) and sets forth some properties for patch attack 3D invariance; and C), we draw novel qualitative conclusions including: 1) we demonstrate that for some 3D transformations, namely rotation and loom, increasing the training distribution support yields an increase in patch success over the full range at test time. 2) We provide new insights into the existence of a fundamental cutoff limit in patch attack effectiveness that depends on the extent of out-of-plane rotation angles. These findings should collectively guide future design of 3D patch attacks and defenses.",0
"Adversarial machine learning (ML) research has primarily focused on perturbation-based attacks, which are not physically possible. However, patch-based attacks are physically realizable, but most research has been limited to the 2D domain. This paper aims to characterize the robustness properties of patch attacks and their invariance to 3D pose. The authors introduce a new metric called mean Attack Success over Transformations (mAST) to evaluate patch attack robustness and conduct a sensitivity analysis to assess the robustness of patch attacks to 3D position and orientation for various conditions. The authors draw novel conclusions, including an increase in patch success over the full range at test time for some 3D transformations with an increase in the training distribution support. The authors also provide insights into the fundamental cutoff limit in patch attack effectiveness, which depends on the extent of out-of-plane rotation angles. These findings will guide the future design of 3D patch attacks and defenses.",1
"This work addresses the challenging task of LiDAR-based 3D object detection in foggy weather. Collecting and annotating data in such a scenario is very time, labor and cost intensive. In this paper, we tackle this problem by simulating physically accurate fog into clear-weather scenes, so that the abundant existing real datasets captured in clear weather can be repurposed for our task. Our contributions are twofold: 1) We develop a physically valid fog simulation method that is applicable to any LiDAR dataset. This unleashes the acquisition of large-scale foggy training data at no extra cost. These partially synthetic data can be used to improve the robustness of several perception methods, such as 3D object detection and tracking or simultaneous localization and mapping, on real foggy data. 2) Through extensive experiments with several state-of-the-art detection approaches, we show that our fog simulation can be leveraged to significantly improve the performance for 3D object detection in the presence of fog. Thus, we are the first to provide strong 3D object detection baselines on the Seeing Through Fog dataset. Our code is available at www.trace.ethz.ch/lidar_fog_simulation.",0
"The aim of this study is to overcome the difficulty of detecting 3D objects using LiDAR technology in foggy conditions, which involves a time-consuming and costly process of collecting and annotating data. To address this, the authors propose a method of simulating realistic fog in clear-weather scenes, allowing existing datasets captured in clear weather to be repurposed for the task. The authors make two contributions: firstly, they develop a physically accurate fog simulation method that can be applied to any LiDAR dataset, enabling the acquisition of large-scale foggy training data at no extra cost. Secondly, they demonstrate through extensive experiments that their fog simulation can significantly improve the performance of 3D object detection in the presence of fog, providing strong baselines for the Seeing Through Fog dataset. The authors have made their code available at www.trace.ethz.ch/lidar_fog_simulation.",1
"Generative models in molecular design tend to be richly parameterized, data-hungry neural models, as they must create complex structured objects as outputs. Estimating such models from data may be challenging due to the lack of sufficient training data. In this paper, we propose a surprisingly effective self-training approach for iteratively creating additional molecular targets. We first pre-train the generative model together with a simple property predictor. The property predictor is then used as a likelihood model for filtering candidate structures from the generative model. Additional targets are iteratively produced and used in the course of stochastic EM iterations to maximize the log-likelihood that the candidate structures are accepted. A simple rejection (re-weighting) sampler suffices to draw posterior samples since the generative model is already reasonable after pre-training. We demonstrate significant gains over strong baselines for both unconditional and conditional molecular design. In particular, our approach outperforms the previous state-of-the-art in conditional molecular design by over 10% in absolute gain. Finally, we show that our approach is useful in other domains as well, such as program synthesis.",0
"Molecular design generative models are typically complex neural models that require large amounts of data to create intricate structured objects. However, it can be challenging to estimate these models from data due to insufficient training data. This paper proposes a self-training approach that effectively creates additional molecular targets for iterative use in stochastic EM iterations to maximize the likelihood of accepting candidate structures. The generative model is pre-trained with a simple property predictor, which is then used as a likelihood model to filter candidate structures. A straightforward rejection sampler can draw posterior samples since the generative model is reasonable after pre-training. Our approach shows significant improvements over strong baselines for both unconditional and conditional molecular design. In particular, it outperforms the previous state-of-the-art in conditional molecular design by over 10%. Additionally, we demonstrate the usefulness of our approach in other domains, such as program synthesis.",1
"Knowledge about the hidden factors that determine particular system dynamics is crucial for both explaining them and pursuing goal-directed interventions. Inferring these factors from time series data without supervision remains an open challenge. Here, we focus on spatiotemporal processes, including wave propagation and weather dynamics, for which we assume that universal causes (e.g. physics) apply throughout space and time. A recently introduced DIstributed SpatioTemporal graph Artificial Neural network Architecture (DISTANA) is used and enhanced to learn such processes, requiring fewer parameters and achieving significantly more accurate predictions compared to temporal convolutional neural networks and other related approaches. We show that DISTANA, when combined with a retrospective latent state inference principle called active tuning, can reliably derive location-respective hidden causal factors. In a current weather prediction benchmark, DISTANA infers our planet's land-sea mask solely by observing temperature dynamics and, meanwhile, uses the self inferred information to improve its own future temperature predictions.",0
"Understanding the underlying factors that impact a system's behavior is essential for comprehending it and developing targeted interventions. However, determining these factors from unsupervised time series data remains a challenge. Our focus is on spatiotemporal processes, such as weather dynamics and wave propagation, which are subject to universal causes like physics throughout space and time. To overcome this challenge, we have developed a DIstributed SpatioTemporal graph Artificial Neural network Architecture (DISTANA) that requires fewer parameters and produces more accurate predictions than other methods. We demonstrate that DISTANA, combined with active tuning, can identify location-specific causal factors. In a weather prediction test, DISTANA correctly inferred our planet's land-sea mask from temperature data and used this information to improve future temperature forecasts.",1
"We study utilizing auxiliary information in training data to improve the trustworthiness of machine learning models. Specifically, in the context of image classification, we propose to optimize a training objective that incorporates bounding box information, which is available in many image classification datasets. Preliminary experimental results show that the proposed algorithm achieves better performance in accuracy, robustness, and interpretability compared with baselines.",0
"Our research focuses on enhancing the reliability of machine learning models by incorporating auxiliary data during training. In particular, we suggest refining the training objective for image classification by integrating bounding box information, frequently found in image classification datasets. Our initial experiments indicate that this approach outperforms existing methods in accuracy, resilience, and comprehensibility.",1
"Artificial neural networks use a lot of coefficients that take a great deal of computing power for their adjustment, especially if deep learning networks are employed. However, there exist coefficients-free extremely fast indexing-based technologies that work, for instance, in Google search engines, in genome sequencing, etc. The paper discusses the use of indexing-based methods for pattern recognition. It is shown that for pattern recognition applications such indexing methods replace with inverse patterns the fully inverted files, which are typically employed in search engines. Not only such inversion provide automatic feature extraction, which is a distinguishing mark of deep learning, but, unlike deep learning, pattern inversion supports almost instantaneous learning, which is a consequence of absence of coefficients. The paper discusses a pattern inversion formalism that makes use on a novel pattern transform and its application for unsupervised instant learning. Examples demonstrate a view-angle independent recognition of three-dimensional objects, such as cars, against arbitrary background, prediction of remaining useful life of aircraft engines, and other applications. In conclusion, it is noted that, in neurophysiology, the function of the neocortical mini-column has been widely debated since 1957. This paper hypothesize that, mathematically, the cortical mini-column can be described as an inverse pattern, which physically serves as a connection multiplier expanding associations of inputs with relevant pattern classes.",0
"Artificial neural networks require numerous coefficients that demand substantial computing power for adjustment, particularly if deep learning networks are utilized. Nevertheless, there are indexing-based technologies that do not require coefficients and operate rapidly, like those used in Google search engines and genome sequencing. This article investigates the use of indexing-based procedures for pattern recognition and shows that these methods substitute fully inverted files with inverse patterns, commonly used in search engines, for pattern recognition applications. Inversion offers automatic feature extraction, similar to deep learning, but with almost instantaneous learning due to the absence of coefficients. The paper presents a pattern inversion formalism that uses a new pattern transform and applies it for unsupervised instant learning. Several examples demonstrate the recognition of three-dimensional objects, prediction of remaining useful life of aircraft engines, and other applications. The paper concludes by proposing that the cortical mini-column can be mathematically described as an inverse pattern that acts as a connection multiplier in expanding associations of inputs with relevant pattern classes. The function of the neocortical mini-column in neurophysiology has been the subject of debate since 1957.",1
"Deep learning and convolutional neural networks allow achieving impressive performance in computer vision tasks, such as object detection and semantic segmentation (SS). However, recent studies have shown evident weaknesses of such models against adversarial perturbations. In a real-world scenario instead, like autonomous driving, more attention should be devoted to real-world adversarial examples (RWAEs), which are physical objects (e.g., billboards and printable patches) optimized to be adversarial to the entire perception pipeline. This paper presents an in-depth evaluation of the robustness of popular SS models by testing the effects of both digital and real-world adversarial patches. These patches are crafted with powerful attacks enriched with a novel loss function. Firstly, an investigation on the Cityscapes dataset is conducted by extending the Expectation Over Transformation (EOT) paradigm to cope with SS. Then, a novel attack optimization, called scene-specific attack, is proposed. Such an attack leverages the CARLA driving simulator to improve the transferability of the proposed EOT-based attack to a real 3D environment. Finally, a printed physical billboard containing an adversarial patch was tested in an outdoor driving scenario to assess the feasibility of the studied attacks in the real world. Exhaustive experiments revealed that the proposed attack formulations outperform previous work to craft both digital and real-world adversarial patches for SS. At the same time, the experimental results showed how these attacks are notably less effective in the real world, hence questioning the practical relevance of adversarial attacks to SS models for autonomous/assisted driving.",0
"Computer vision tasks, including object detection and semantic segmentation (SS), can achieve impressive performance through deep learning and convolutional neural networks. However, these models have been shown to have weaknesses against adversarial perturbations. Real-world adversarial examples (RWAEs), physical objects optimized to be adversarial to the entire perception pipeline, require more attention in real-world scenarios like autonomous driving. This paper presents an evaluation of popular SS models' robustness by testing digital and real-world adversarial patches created with powerful attacks enriched with a novel loss function. An investigation on the Cityscapes dataset is conducted, followed by proposing a scene-specific attack that leverages the CARLA driving simulator to improve the transferability to a real 3D environment. Finally, a physical billboard with an adversarial patch is tested in an outdoor driving scenario. The proposed attack formulations outperform previous work for crafting both digital and real-world adversarial patches for SS. However, the experimental results reveal that these attacks are notably less effective in the real world, questioning the practical relevance of adversarial attacks to SS models for autonomous/assisted driving.",1
"Real world traffic sign recognition is an important step towards building autonomous vehicles, most of which highly dependent on Deep Neural Networks (DNNs). Recent studies demonstrated that DNNs are surprisingly susceptible to adversarial examples. Many attack methods have been proposed to understand and generate adversarial examples, such as gradient based attack, score based attack, decision based attack, and transfer based attacks. However, most of these algorithms are ineffective in real-world road sign attack, because (1) iteratively learning perturbations for each frame is not realistic for a fast moving car and (2) most optimization algorithms traverse all pixels equally without considering their diverse contribution. To alleviate these problems, this paper proposes the targeted attention attack (TAA) method for real world road sign attack. Specifically, we have made the following contributions: (1) we leverage the soft attention map to highlight those important pixels and skip those zero-contributed areas - this also helps to generate natural perturbations, (2) we design an efficient universal attack that optimizes a single perturbation/noise based on a set of training images under the guidance of the pre-trained attention map, (3) we design a simple objective function that can be easily optimized, (4) we evaluate the effectiveness of TAA on real world data sets. Experimental results validate that the TAA method improves the attack successful rate (nearly 10%) and reduces the perturbation loss (about a quarter) compared with the popular RP2 method. Additionally, our TAA also provides good properties, e.g., transferability and generalization capability. We provide code and data to ensure the reproducibility: https://github.com/AdvAttack/RoadSignAttack.",0
"Developing autonomous vehicles heavily relies on Deep Neural Networks (DNNs), making traffic sign recognition a crucial step. However, recent studies have shown that DNNs are highly vulnerable to adversarial examples. Despite several proposed attack methods such as gradient, score, decision, and transfer-based attacks, they prove ineffective in real-world road sign attack due to the unrealistic iterative perturbation learning process and optimization algorithms that treat all pixels equally. To address these issues, this paper introduces the targeted attention attack (TAA) method. TAA highlights important pixels using soft attention maps to generate natural perturbations, optimizes a single perturbation based on a set of training images, and evaluates the method's efficiency on real-world data sets. The results show that TAA improves the attack success rate by almost 10% and reduces perturbation loss by a quarter compared to the RP2 method. The TAA also exhibits transferability and generalization capability, and the code and data are available for reproducibility.",1
"Computer-aided design (CAD) programs are essential to engineering as they allow for better designs through low-cost iterations. While CAD programs are typically taught to undergraduate students as a job skill, such software can also help students learn engineering concepts. A current limitation of CAD programs (even those that are specifically designed for educational purposes) is that they are not capable of providing automated real-time help to students. To encourage CAD programs to build in assistance to students, we used data generated from students using a free, open source CAD software called Aladdin to demonstrate how student data combined with machine learning techniques can predict how well a particular student will perform in a design task. We challenged students to design a house that consumed zero net energy as part of an introductory engineering technology undergraduate course. Using data from 128 students, along with the scikit-learn Python machine learning library, we tested our models using both total counts of design actions and sequences of design actions as inputs. We found that our models using early design sequence actions are particularly valuable for prediction. Our logistic regression model achieved a >60% chance of predicting if a student would succeed in designing a zero net energy house. Our results suggest that it would be feasible for Aladdin to provide useful feedback to students when they are approximately halfway through their design. Further improvements to these models could lead to earlier predictions and thus provide students feedback sooner to enhance their learning.",0
"CAD programs play a crucial role in engineering by enabling cost-effective iterations for better designs. Although typically taught as a job skill to undergraduate students, CAD software can also aid in teaching engineering concepts. However, the current limitation of CAD programs, even those designed for education, is the lack of real-time automated assistance for students. To address this, we utilized data from students using the free CAD software Aladdin to demonstrate how combining student data with machine learning techniques can predict a student's performance in a design task. In an introductory engineering technology course, we challenged students to design a zero net energy house and used data from 128 students to test our models. We found that early design sequence actions were particularly useful for prediction, with our logistic regression model achieving over a 60% chance of predicting a student's success. Our results suggest that with further improvements to the models, Aladdin could provide valuable feedback to students earlier in their design process, enhancing their learning.",1
"Ocean current, fluid mechanics, and many other spatio-temporal physical dynamical systems are essential components of the universe. One key characteristic of such systems is that certain physics laws -- represented as ordinary/partial differential equations (ODEs/PDEs) -- largely dominate the whole process, irrespective of time or location. Physics-informed learning has recently emerged to learn physics for accurate prediction, but they often lack a mechanism to leverage localized spatial and temporal correlation or rely on hard-coded physics parameters. In this paper, we advocate a physics-coupled neural network model to learn parameters governing the physics of the system, and further couple the learned physics to assist the learning of recurring dynamics. A spatio-temporal physics-coupled neural network (ST-PCNN) model is proposed to achieve three goals: (1) learning the underlying physics parameters, (2) transition of local information between spatio-temporal regions, and (3) forecasting future values for the dynamical system. The physics-coupled learning ensures that the proposed model can be tremendously improved by using learned physics parameters, and can achieve good long-range forecasting (e.g., more than 30-steps). Experiments, using simulated and field-collected ocean current data, validate that ST-PCNN outperforms existing physics-informed models.",0
"Spatio-temporal physical dynamical systems, such as ocean currents and fluid mechanics, are crucial components of the universe. They are largely governed by certain physics laws, which are represented as ordinary/partial differential equations (ODEs/PDEs) that dominate the entire process, regardless of time or location. Although physics-informed learning has been developed to predict accurately, these methods often lack the capacity to take into account localized spatial and temporal correlation or depend on hard-coded physics parameters. This article advocates for a physics-coupled neural network model that can learn the parameters governing the physics of the system and couple the learned physics to assist in the learning of recurring dynamics. The spatio-temporal physics-coupled neural network (ST-PCNN) model achieves three objectives: (1) learning the underlying physics parameters, (2) transferring local information between spatio-temporal regions, and (3) forecasting future values for the dynamical system. The incorporation of physics-coupled learning ensures that the proposed model can be significantly improved by using learned physics parameters and can achieve good long-range forecasting, exceeding 30-steps. Experiments using simulated and field-collected ocean current data demonstrate that ST-PCNN outperforms existing physics-informed models.",1
"This paper has proposed a new baseline deep learning model of more benefits for image classification. Different from the convolutional neural network(CNN) practice where filters are trained by back propagation to represent different patterns of an image, we are inspired by a method called ""PCANet"" in ""PCANet: A Simple Deep Learning Baseline for Image Classification?"" to choose filter vectors from basis vectors in frequency domain like Fourier coefficients or wavelets without back propagation. Researchers have demonstrated that those basis in frequency domain can usually provide physical insights, which adds to the interpretability of the model by analyzing the frequencies selected. Besides, the training process will also be more time efficient, mathematically clear and interpretable compared with the ""black-box"" training process of CNN.",0
"A new deep learning model with more advantageous features for image classification has been proposed in this paper. Unlike the standard convolutional neural network (CNN) approach which involves training filters through back propagation to identify various patterns in an image, this model is inspired by the ""PCANet"" method presented in ""PCANet: A Simple Deep Learning Baseline for Image Classification?"". This approach selects filter vectors from basis vectors in the frequency domain such as Fourier coefficients or wavelets, without relying on back propagation. Previous studies have shown that these frequency domain basis vectors offer physical insights, which enable better analysis of the frequencies selected and enhance the model's interpretability. Furthermore, the training process is more efficient, mathematically transparent, and interpretable compared to the ""black-box"" training process used in CNNs.",1
"The physical and clinical constraints surrounding diffusion-weighted imaging (DWI) often limit the spatial resolution of the produced images to voxels up to 8 times larger than those of T1w images. Thus, the detailed information contained in T1w imagescould help in the synthesis of diffusion images in higher resolution. However, the non-Euclidean nature of diffusion imaging hinders current deep generative models from synthesizing physically plausible images. In this work, we propose the first Riemannian network architecture for the direct generation of diffusion tensors (DT) and diffusion orientation distribution functions (dODFs) from high-resolution T1w images. Our integration of the Log-Euclidean Metric into a learning objective guarantees, unlike standard Euclidean networks, the mathematically-valid synthesis of diffusion. Furthermore, our approach improves the fractional anisotropy mean squared error (FA MSE) between the synthesized diffusion and the ground-truth by more than 23% and the cosine similarity between principal directions by almost 5% when compared to our baselines. We validate our generated diffusion by comparing the resulting tractograms to our expected real data. We observe similar fiber bundles with streamlines having less than 3% difference in length, less than 1% difference in volume, and a visually close shape. While our method is able to generate high-resolution diffusion images from structural inputs in less than 15 seconds, we acknowledge and discuss the limits of diffusion inference solely relying on T1w images. Our results nonetheless suggest a relationship between the high-level geometry of the brain and the overall white matter architecture.",0
"Due to physical and clinical limitations, the resolution of diffusion-weighted imaging (DWI) images is often constrained to voxels that are up to eight times larger than those of T1-weighted (T1w) images. As a result, information contained in T1w images could be useful in creating higher-resolution diffusion images. However, the non-Euclidean nature of diffusion imaging poses challenges for current deep generative models. To address this issue, we propose a Riemannian network architecture that can generate diffusion tensors (DT) and diffusion orientation distribution functions (dODFs) directly from high-resolution T1w images. Our approach uses the Log-Euclidean Metric in the learning objective to ensure mathematically valid synthesis of diffusion. Compared to our baselines, our method improves fractional anisotropy mean squared error (FA MSE) by more than 23% and cosine similarity between principal directions by almost 5%. We verify the accuracy of our generated diffusion by comparing the resulting tractograms to expected real data. Our findings suggest a link between the brain's high-level geometry and its white matter architecture. While our method can create high-resolution diffusion images from structural inputs in less than 15 seconds, we acknowledge the limitations of relying solely on T1w images for diffusion inference.",1
"Spatio-temporal forecasting is of great importance in a wide range of dynamical systems applications from atmospheric science, to recent COVID-19 spread modeling. These applications rely on accurate predictions of spatio-temporal structured data reflecting real-world phenomena. A stunning characteristic is that the dynamical system is not only driven by some physics laws but also impacted by the localized factor in spatial and temporal regions. One of the major challenges is to infer the underlying causes, which generate the perceived data stream and propagate the involved causal dynamics through the distributed observing units. Another challenge is that the success of machine learning based predictive models requires massive annotated data for model training. However, the acquisition of high-quality annotated data is objectively manual and tedious as it needs a considerable amount of human intervention, making it infeasible in fields that require high levels of expertise. To tackle these challenges, we advocate a spatio-temporal physics-coupled neural networks (ST-PCNN) model to learn the underlying physics of the dynamical system and further couple the learned physics to assist the learning of the recurring dynamics. To deal with data-acquisition constraints, an active learning mechanism with Kriging for actively acquiring the most informative data is proposed for ST-PCNN training in a partially observable environment. Our experiments on both synthetic and real-world datasets exhibit that the proposed ST-PCNN with active learning converges to near optimal accuracy with substantially fewer instances.",0
"Accurate predictions of spatio-temporal structured data are crucial in various dynamic systems applications, such as COVID-19 spread modeling and atmospheric science. These systems are not only influenced by physics laws but also by localized factors in temporal and spatial regions. However, inferring the underlying causes of the observed data and propagating causal dynamics through distributed observing units pose significant challenges. Additionally, the acquisition of annotated data for machine learning-based predictive models is labor-intensive and requires high levels of expertise. To overcome these challenges, we propose a spatio-temporal physics-coupled neural networks (ST-PCNN) model that learns the underlying physics of the dynamical system and couples it to assist in learning recurring dynamics. To address data-acquisition constraints, we introduce an active learning mechanism with Kriging for acquiring the most informative data in a partially observable environment. Our experiments demonstrate that the proposed model with active learning achieves near-optimal accuracy with fewer instances on both synthetic and real-world datasets.",1
"We explore the zero-shot setting for day-night domain adaptation. The traditional domain adaptation setting is to train on one domain and adapt to the target domain by exploiting unlabeled data samples from the test set. As gathering relevant test data is expensive and sometimes even impossible, we remove any reliance on test data imagery and instead exploit a visual inductive prior derived from physics-based reflection models for domain adaptation. We cast a number of color invariant edge detectors as trainable layers in a convolutional neural network and evaluate their robustness to illumination changes. We show that the color invariant layer reduces the day-night distribution shift in feature map activations throughout the network. We demonstrate improved performance for zero-shot day to night domain adaptation on both synthetic as well as natural datasets in various tasks, including classification, segmentation and place recognition.",0
"Our study delves into the zero-shot approach to day-night domain adaptation. Typically, domain adaptation involves training on one domain and adjusting to the target domain by utilizing unlabeled data samples from the test set. However, collecting relevant test data can be costly and sometimes unfeasible. To avoid relying on test data imagery, we employ a visual inductive prior derived from physics-based reflection models for domain adaptation. We incorporate several color invariant edge detectors as trainable layers in a convolutional neural network and assess their ability to withstand changes in illumination. Our findings reveal that the color invariant layer minimizes the day-night distribution shift in feature map activations throughout the network. Furthermore, we demonstrate improved performance for zero-shot day to night domain adaptation in various tasks, including classification, segmentation, and place recognition, on both synthetic and natural datasets.",1
"Context is of fundamental importance to both human and machine vision; e.g., an object in the air is more likely to be an airplane than a pig. The rich notion of context incorporates several aspects including physics rules, statistical co-occurrences, and relative object sizes, among others. While previous work has focused on crowd-sourced out-of-context photographs from the web to study scene context, controlling the nature and extent of contextual violations has been a daunting task. Here we introduce a diverse, synthetic Out-of-Context Dataset (OCD) with fine-grained control over scene context. By leveraging a 3D simulation engine, we systematically control the gravity, object co-occurrences and relative sizes across 36 object categories in a virtual household environment. We conducted a series of experiments to gain insights into the impact of contextual cues on both human and machine vision using OCD. We conducted psychophysics experiments to establish a human benchmark for out-of-context recognition, and then compared it with state-of-the-art computer vision models to quantify the gap between the two. We propose a context-aware recognition transformer model, fusing object and contextual information via multi-head attention. Our model captures useful information for contextual reasoning, enabling human-level performance and better robustness in out-of-context conditions compared to baseline models across OCD and other out-of-context datasets. All source code and data are publicly available at https://github.com/kreimanlab/WhenPigsFlyContext",0
"The importance of context cannot be overstated when it comes to both human and machine vision. For example, if an object is seen in the sky, it is more likely to be an airplane than a pig. Context includes various factors such as physical laws, statistical co-occurrences, and relative object sizes. While previous studies have focused on analyzing out-of-context images sourced from the internet, controlling the nature and extent of contextual violations has been a challenging task. To address this, we have created a synthetic Out-of-Context Dataset (OCD) that allows for precise control over the scene context. Using a 3D simulation engine, we manipulated gravity, object co-occurrences, and relative sizes across 36 object categories in a virtual household environment. Through a series of experiments, we explored the impact of contextual cues on human and machine vision. We established a human benchmark for out-of-context recognition through psychophysics experiments and compared it with state-of-the-art computer vision models. We also proposed a context-aware recognition transformer model that integrates object and contextual information using multi-head attention. Our model outperformed baseline models across OCD and other out-of-context datasets, achieving human-level performance and increased robustness in out-of-context conditions. All data and source code can be accessed at https://github.com/kreimanlab/WhenPigsFlyContext.",1
"Deep generative models aim to learn underlying distributions that generate the observed data. Given the fact that the generative distribution may be complex and intractable, deep latent variable models use probabilistic frameworks to learn more expressive joint probability distributions over the data and their low-dimensional hidden variables. Learning complex probability distributions over sequential data without any supervision is a difficult task for deep generative models. Ordinary Differential Equation Variational Auto-Encoder (ODE2VAE) is a deep latent variable model that aims to learn complex distributions over high-dimensional sequential data and their low-dimensional representations. ODE2VAE infers continuous latent dynamics of the high-dimensional input in a low-dimensional hierarchical latent space. The hierarchical organization of the continuous latent space embeds a physics-guided inductive bias in the model. In this paper, we analyze the latent representations inferred by the ODE2VAE model over three different physical motion datasets: bouncing balls, projectile motion, and simple pendulum. Through our experiments, we explore the effects of the physics-guided inductive bias of the ODE2VAE model over the learned dynamical latent representations. We show that the model is able to learn meaningful latent representations to an extent without any supervision.",0
"The objective of deep generative models is to comprehend the underlying distributions that produce the observed data. As the generative distribution can be intricate and not easily manageable, deep latent variable models utilize probabilistic frameworks to acquire more expressive joint probability distributions of the data and their low-dimensional hidden variables. The task of learning complex probability distributions over sequential data without any guidance is a challenging one for deep generative models. The Ordinary Differential Equation Variational Auto-Encoder (ODE2VAE) is a deep latent variable model that intends to learn intricate distributions over high-dimensional sequential data and their low-dimensional representations. ODE2VAE deduces the continuous latent dynamics of the high-dimensional input in a low-dimensional hierarchical latent space. The hierarchical arrangement of the continuous latent space incorporates a physics-guided inductive bias into the model. In this paper, we examine the latent representations inferred by the ODE2VAE model across three distinct physical motion datasets: bouncing balls, projectile motion, and simple pendulum. Throughout our experiments, we analyze the effects of the physics-guided inductive bias of the ODE2VAE model concerning the learned dynamical latent representations. We demonstrate that the model can achieve meaningful latent representations to some extent without any guidance.",1
"In this article, we perform a review of the state-of-the-art of hybrid machine learning in medical imaging. We start with a short summary of the general developments of the past in machine learning and how general and specialized approaches have been in competition in the past decades. A particular focus will be the theoretical and experimental evidence pro and contra hybrid modelling. Next, we inspect several new developments regarding hybrid machine learning with a particular focus on so-called known operator learning and how hybrid approaches gain more and more momentum across essentially all applications in medical imaging and medical image analysis. As we will point out by numerous examples, hybrid models are taking over in image reconstruction and analysis. Even domains such as physical simulation and scanner and acquisition design are being addressed using machine learning grey box modelling approaches. Towards the end of the article, we will investigate a few future directions and point out relevant areas in which hybrid modelling, meta learning, and other domains will likely be able to drive the state-of-the-art ahead.",0
"This article presents an overview of the current state of hybrid machine learning in medical imaging. The history of machine learning is briefly summarized, with a focus on the competition between general and specialized approaches. The discussion then turns to the advantages and disadvantages of hybrid modeling, followed by exploration of recent developments in this field, specifically known operator learning. The article highlights the growing popularity of hybrid approaches across all applications in medical imaging and analysis, with numerous examples of their success in image reconstruction and analysis, and even in physical simulation and scanner and acquisition design. Finally, the article looks to the future, identifying potential areas where hybrid modeling, meta learning, and other domains could push the boundaries of medical imaging research.",1
