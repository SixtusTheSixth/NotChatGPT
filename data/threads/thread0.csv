"This paper shows how a machine, which observes stimuli through an uncharacterized, uncalibrated channel and sensor, can glean machine-independent information (i.e., channel- and sensor-independent information) about the stimuli. First, we demonstrate that a machine defines a specific coordinate system on the stimulus state space, with the nature of that coordinate system depending on the device's channel and sensor. Thus, machines with different channels and sensors ""see"" the same stimulus trajectory through state space, but in different machine-specific coordinate systems. For a large variety of physical stimuli, statistical properties of that trajectory endow the stimulus configuration space with differential geometric structure (a metric and parallel transfer procedure), which can then be used to represent relative stimulus configurations in a coordinate-system-independent manner (and, therefore, in a channel- and sensor-independent manner). The resulting description is an ""inner"" property of the stimulus time series in the sense that it does not depend on extrinsic factors like the observer's choice of a coordinate system in which the stimulus is viewed (i.e., the observer's choice of channel and sensor). This methodology is illustrated with analytic examples and with a numerically simulated experiment. In an intelligent sensory device, this kind of representation ""engine"" could function as a ""front-end"" that passes channel/sensor-independent stimulus representations to a pattern recognition module. After a pattern recognizer has been trained in one of these devices, it could be used without change in other devices having different channels and sensors.",0
"This paper presents a method for a machine to extract information about stimuli independently from the channel and sensor used to observe them. The machine establishes a specific coordinate system for the stimulus state space, which varies depending on the channel and sensor employed. However, the statistical properties of the stimulus trajectory endow the stimulus configuration space with a differential geometric structure, allowing for a channel- and sensor-independent representation of relative stimulus configurations. This representation is an ""inner"" property of the stimulus time series and does not depend on extrinsic factors such as the observer's choice of channel and sensor. The method is demonstrated through analytic examples and a simulated experiment and could be utilized as a ""front-end"" for an intelligent sensory device's pattern recognition module. Once trained, this module could be used across different devices with varying channels and sensors without any modifications.",1
"For the last years, time-series mining has become a challenging issue for researchers. An important application lies in most monitoring purposes, which require analyzing large sets of time-series for learning usual patterns. Any deviation from this learned profile is then considered as an unexpected situation. Moreover, complex applications may involve the temporal study of several heterogeneous parameters. In that paper, we propose a method for mining heterogeneous multivariate time-series for learning meaningful patterns. The proposed approach allows for mixed time-series -- containing both pattern and non-pattern data -- such as for imprecise matches, outliers, stretching and global translating of patterns instances in time. We present the early results of our approach in the context of monitoring the health status of a person at home. The purpose is to build a behavioral profile of a person by analyzing the time variations of several quantitative or qualitative parameters recorded through a provision of sensors installed in the home.",0
"Researchers have faced challenges in time-series mining in recent years, particularly in applications requiring the analysis of large sets of time-series for identifying usual patterns and detecting unexpected situations. Furthermore, complex applications may involve the temporal study of multiple heterogeneous parameters. This paper proposes a method for mining heterogeneous multivariate time-series to identify meaningful patterns, including mixed time-series with both pattern and non-pattern data, imprecise matches, outliers, stretching, and global translating of pattern instances in time. Early results of the approach are presented in the context of monitoring a person's health at home by analyzing time variations of various quantitative or qualitative parameters recorded through sensors installed in the home to create a behavioral profile.",1
"In the computational-mechanics structural analysis of one-dimensional cellular automata the following automata-theoretic analogue of the \emph{change-point problem} from time series analysis arises: \emph{Given a string $\sigma$ and a collection $\{\mc{D}_i\}$ of finite automata, identify the regions of $\sigma$ that belong to each $\mc{D}_i$ and, in particular, the boundaries separating them.} We present two methods for solving this \emph{multi-regular language filtering problem}. The first, although providing the ideal solution, requires a stack, has a worst-case compute time that grows quadratically in $\sigma$'s length and conditions its output at any point on arbitrarily long windows of future input. The second method is to algorithmically construct a transducer that approximates the first algorithm. In contrast to the stack-based algorithm, however, the transducer requires only a finite amount of memory, runs in linear time, and gives immediate output for each letter read; it is, moreover, the best possible finite-state approximation with these three features.",0
"The analysis of one-dimensional cellular automata using computational mechanics involves a problem similar to the change-point problem in time series analysis. This problem asks for the identification of regions belonging to a given collection of finite automata, including the boundaries that separate them, within a given string. We offer two solutions to this multi-regular language filtering problem. The first solution, while optimal, relies on a stack and has a worst-case compute time that grows quadratically with the length of the string. Additionally, it requires knowledge of arbitrarily long windows of future input to produce output. The second solution involves constructing a transducer that approximates the first algorithm. Unlike the stack-based algorithm, the transducer only requires finite memory and runs in linear time, producing immediate output for each letter read. This second solution is the best possible finite-state approximation with these three features.",1
"Emergent behaviors are in the focus of recent research interest. It is then of considerable importance to investigate what optimizations suit the learning and prediction of chaotic systems, the putative candidates for emergence. We have compared L1 and L2 regularizations on predicting chaotic time series using linear recurrent neural networks. The internal representation and the weights of the networks were optimized in a unifying framework. Computational tests on different problems indicate considerable advantages for the L1 regularization: It had considerably better learning time and better interpolating capabilities. We shall argue that optimization viewed as a maximum likelihood estimation justifies our results, because L1 regularization fits heavy-tailed distributions -- an apparently general feature of emergent systems -- better.",0
"Recent research has focused on emergent behaviors, making it important to investigate which optimizations are best suited for learning and predicting chaotic systems, which are considered potential candidates for emergence. Our study compared the effectiveness of L1 and L2 regularizations in predicting chaotic time series using linear recurrent neural networks. We optimized the internal representation and weights of the networks using a unified framework. Our computational tests on various problems demonstrated that L1 regularization had significant advantages, including faster learning time and better interpolating capabilities. We argue that our results are justified by viewing optimization as a maximum likelihood estimation, as L1 regularization is better suited for fitting heavy-tailed distributions - a common characteristic of emergent systems.",1
"We present a new algorithm for discovering patterns in time series and other sequential data. We exhibit a reliable procedure for building the minimal set of hidden, Markovian states that is statistically capable of producing the behavior exhibited in the data -- the underlying process's causal states. Unlike conventional methods for fitting hidden Markov models (HMMs) to data, our algorithm makes no assumptions about the process's causal architecture (the number of hidden states and their transition structure), but rather infers it from the data. It starts with assumptions of minimal structure and introduces complexity only when the data demand it. Moreover, the causal states it infers have important predictive optimality properties that conventional HMM states lack. We introduce the algorithm, review the theory behind it, prove its asymptotic reliability, use large deviation theory to estimate its rate of convergence, and compare it to other algorithms which also construct HMMs from data. We also illustrate its behavior on an example process, and report selected numerical results from an implementation.",0
"A novel algorithm is presented for detecting patterns in sequential data, including time series. The algorithm demonstrates a dependable approach to establishing the smallest set of hidden, Markovian states that can statistically generate the behavior demonstrated in the data, known as the causal states of the underlying process. Unlike conventional methods used for fitting hidden Markov models (HMMs) to data, this algorithm does not make assumptions about the causal architecture of the process, such as the number of hidden states and their transition structure, but instead infers it from the data. It begins with minimal structure assumptions and only introduces complexity when necessary based on the data. Additionally, the causal states inferred by the algorithm possess predictive optimality properties that are absent in conventional HMM states. The algorithm is introduced, and its theory is reviewed, including proof of its asymptotic reliability and estimation of its rate of convergence using large deviation theory. Comparisons are made with other algorithms that construct HMMs from data, and an example process is used to illustrate its behavior. The implementation of the algorithm also includes selected numerical results.",1
"This paper proposes a fusion method of modalities extracted from video through a three-stream network with spatio-temporal and temporal convolutions for fine-grained action classification in sport. It is applied to TTStroke-21 dataset which consists of untrimmed videos of table tennis games. The goal is to detect and classify table tennis strokes in the videos, the first step of a bigger scheme aiming at giving feedback to the players for improving their performance. The three modalities are raw RGB data, the computed optical flow and the estimated pose of the player. The network consists of three branches with attention blocks. Features are fused at the latest stage of the network using bilinear layers. Compared to previous approaches, the use of three modalities allows faster convergence and better performances on both tasks: classification of strokes with known temporal boundaries and joint segmentation and classification. The pose is also further investigated in order to offer richer feedback to the athletes.",0
"In this article, a novel technique for fine-grained action classification in sports is proposed. The approach involves fusing modalities extracted from video data through a three-stream network with spatio-temporal and temporal convolutions. The proposed method is applied to the TTStroke-21 dataset, which comprises of untrimmed table tennis game videos, with the aim of detecting and classifying table tennis strokes. This is the first step towards a larger scheme aimed at providing players with feedback to improve their performance. The three modalities used in the network are raw RGB data, computed optical flow, and estimated player pose. The network comprises of three branches with attention blocks, and features are fused using bilinear layers at the final stage. Compared to previous methods, the use of three modalities results in faster convergence and better performance on both stroke classification and joint segmentation and classification. Moreover, the player's pose is analyzed to provide more comprehensive feedback to athletes.",1
"Establishing robust and accurate correspondences between a pair of images is a long-standing computer vision problem with numerous applications. While classically dominated by sparse methods, emerging dense approaches offer a compelling alternative paradigm that avoids the keypoint detection step. However, dense flow estimation is often inaccurate in the case of large displacements, occlusions, or homogeneous regions. In order to apply dense methods to real-world applications, such as pose estimation, image manipulation, or 3D reconstruction, it is therefore crucial to estimate the confidence of the predicted matches.   We propose the Enhanced Probabilistic Dense Correspondence Network, PDC-Net+, capable of estimating accurate dense correspondences along with a reliable confidence map. We develop a flexible probabilistic approach that jointly learns the flow prediction and its uncertainty. In particular, we parametrize the predictive distribution as a constrained mixture model, ensuring better modelling of both accurate flow predictions and outliers. Moreover, we develop an architecture and an enhanced training strategy tailored for robust and generalizable uncertainty prediction in the context of self-supervised training. Our approach obtains state-of-the-art results on multiple challenging geometric matching and optical flow datasets. We further validate the usefulness of our probabilistic confidence estimation for the tasks of pose estimation, 3D reconstruction, image-based localization, and image retrieval. Code and models are available at https://github.com/PruneTruong/DenseMatching.",0
"The problem of establishing accurate and robust correspondences between two images has been a longstanding issue in computer vision, with many practical applications. Although sparse methods have traditionally dominated this field, newer dense approaches offer an alternative paradigm that eliminates the need for keypoint detection. However, these methods can be unreliable when dealing with large displacements, occlusions, or homogeneous regions. Therefore, in order to utilize dense methods in real-world applications such as image manipulation, pose estimation, or 3D reconstruction, it is crucial to accurately estimate the confidence of the predicted matches. To address this issue, we have developed the PDC-Net+ (Enhanced Probabilistic Dense Correspondence Network), which can estimate precise dense correspondences and a dependable confidence map. Our approach involves a flexible, probabilistic method that learns the flow prediction and its uncertainty jointly. We have employed a constrained mixture model to parametrize the predictive distribution, which provides better modeling of both accurate flow predictions and outliers. Additionally, we have developed an architecture and an improved training strategy tailored for robust and generalizable uncertainty prediction during self-supervised training. Our approach has achieved state-of-the-art results on several challenging optical flow and geometric matching datasets. We have also validated the usefulness of our probabilistic confidence estimation for tasks such as image-based localization, pose estimation, 3D reconstruction, and image retrieval. The code and models for our method are available at https://github.com/PruneTruong/DenseMatching.",1
"In this paper, we propose an end-to-end learning framework for event-based motion deblurring in a self-supervised manner, where real-world events are exploited to alleviate the performance degradation caused by data inconsistency. To achieve this end, optical flows are predicted from events, with which the blurry consistency and photometric consistency are exploited to enable self-supervision on the deblurring network with real-world data. Furthermore, a piece-wise linear motion model is proposed to take into account motion non-linearities and thus leads to an accurate model for the physical formation of motion blurs in the real-world scenario. Extensive evaluation on both synthetic and real motion blur datasets demonstrates that the proposed algorithm bridges the gap between simulated and real-world motion blurs and shows remarkable performance for event-based motion deblurring in real-world scenarios.",0
"Our paper presents a self-supervised approach to event-based motion deblurring using an end-to-end learning framework. Through the exploitation of real-world events, we address the issue of performance degradation caused by data inconsistency. To achieve this, we predict optical flows from events and use blurry consistency and photometric consistency to enable self-supervision on the deblurring network with real-world data. Additionally, we propose a piece-wise linear motion model that accounts for motion non-linearities and accurately models the physical formation of motion blurs in real-world scenarios. Our proposed algorithm is extensively evaluated on both synthetic and real motion blur datasets, showing remarkable performance and bridging the gap between simulated and real-world motion blurs for event-based motion deblurring in real-world scenarios.",1
"Performing a real-time and accurate instrument segmentation from videos is of great significance for improving the performance of robotic-assisted surgery. We identify two important clues for surgical instrument perception, including local temporal dependency from adjacent frames and global semantic correlation in long-range duration. However, most existing works perform segmentation purely using visual cues in a single frame. Optical flow is just used to model the motion between only two frames and brings heavy computational cost. We propose a novel dual-memory network (DMNet) to wisely relate both global and local spatio-temporal knowledge to augment the current features, boosting the segmentation performance and retaining the real-time prediction capability. We propose, on the one hand, an efficient local memory by taking the complementary advantages of convolutional LSTM and non-local mechanisms towards the relating reception field. On the other hand, we develop an active global memory to gather the global semantic correlation in long temporal range to current one, in which we gather the most informative frames derived from model uncertainty and frame similarity. We have extensively validated our method on two public benchmark surgical video datasets. Experimental results demonstrate that our method largely outperforms the state-of-the-art works on segmentation accuracy while maintaining a real-time speed.",0
"Improving the performance of robotic-assisted surgery requires accurately segmenting instruments in real-time from videos. To achieve this, we have identified two crucial factors for surgical instrument perception - local temporal dependency from adjacent frames and global semantic correlation in long-range duration. However, most existing methods use visual cues from a single frame for segmentation, which is insufficient. Optical flow is also not ideal as it models motion between only two frames and requires heavy computational power. To address these limitations, we propose a new approach called Dual-Memory Network (DMNet) that combines both global and local spatio-temporal knowledge for real-time prediction with enhanced segmentation accuracy. Our approach employs an efficient local memory that combines convolutional LSTM and non-local mechanisms to improve reception field and an active global memory that gathers the most informative frames from model uncertainty and frame similarity to augment the current features. Our method has been extensively validated on two public benchmark surgical video datasets, demonstrating superior segmentation accuracy and real-time speed compared to state-of-the-art techniques.",1
"Online action detection (OAD) is a task that receives video segments within a streaming video as inputs and identifies ongoing actions within them. It is important to retain past information associated with a current action. However, long short-term memory (LSTM), a popular recurrent unit for modeling temporal information from videos, accumulates past information from the previous hidden and cell states and the extracted visual features at each timestep without considering the relationships between the past and current information. Consequently, the forget gate of the original LSTM can lose the accumulated information relevant to the current action because it determines which information to forget without considering the current action. We introduce a novel information elevation unit (IEU) that lifts up and accumulate the past information relevant to the current action in order to model the past information that is especially relevant to the current action. To the best of our knowledge, our IEN is the first attempt that considers the computational overhead for the practical use of OAD. Through ablation studies, we design an efficient and effective OAD network using IEUs, called an information elevation network (IEN). Our IEN uses visual features extracted by a fast action recognition network taking only RGB frames because extracting optical flows requires heavy computation overhead. On two OAD benchmark datasets, THUMOS-14 and TVSeries, our IEN outperforms state-of-the-art OAD methods using only RGB frames. Furthermore, on the THUMOS-14 dataset, our IEN outperforms the state-of-the-art OAD methods using two-stream features based on RGB frames and optical flows.",0
"The task of online action detection (OAD) involves identifying ongoing actions within video segments of a streaming video. To effectively accomplish this task, it is important to retain past information associated with a current action. However, the commonly used long short-term memory (LSTM) recurrent unit does not consider the relationships between past and current information, resulting in the forget gate losing accumulated information relevant to the current action. To address this issue, we propose a novel information elevation unit (IEU) that accumulates past information relevant to the current action to model the information most pertinent to the task. Our IEU is the first attempt to consider computational overhead for practical use of OAD. Using visual features extracted by a fast action recognition network from only RGB frames, we design an efficient and effective OAD network called the information elevation network (IEN) using IEUs. Our IEN outperforms state-of-the-art OAD methods on two benchmark datasets, THUMOS-14 and TVSeries, using only RGB frames. Additionally, on the THUMOS-14 dataset, our IEN outperforms state-of-the-art methods using two-stream features based on both RGB frames and optical flows.",1
"Video anomaly detection has gained significant attention due to the increasing requirements of automatic monitoring for surveillance videos. Especially, the prediction based approach is one of the most studied methods to detect anomalies by predicting frames that include abnormal events in the test set after learning with the normal frames of the training set. However, a lot of prediction networks are computationally expensive owing to the use of pre-trained optical flow networks, or fail to detect abnormal situations because of their strong generative ability to predict even the anomalies. To address these shortcomings, we propose spatial rotation transformation (SRT) and temporal mixing transformation (TMT) to generate irregular patch cuboids within normal frame cuboids in order to enhance the learning of normal features. Additionally, the proposed patch transformation is used only during the training phase, allowing our model to detect abnormal frames at fast speed during inference. Our model is evaluated on three anomaly detection benchmarks, achieving competitive accuracy and surpassing all the previous works in terms of speed.",0
"The need for automatic monitoring of surveillance videos has led to a surge in interest in video anomaly detection. Among the various methods studied, the prediction-based approach is one of the most popular ways to detect anomalies. This approach involves predicting frames with abnormal events in the test set after training with normal frames from the training set. However, many prediction networks are expensive in terms of computation due to the use of pre-trained optical flow networks. Additionally, some networks fail to detect abnormal situations because of their strong generative ability to predict even the anomalies. To address these issues, we propose the use of spatial rotation transformation (SRT) and temporal mixing transformation (TMT) to generate irregular patch cuboids within normal frame cuboids to improve the learning of normal features. Our approach only uses patch transformation during the training phase, enabling our model to detect abnormal frames quickly during inference. We evaluated our model on three anomaly detection benchmarks and achieved competitive accuracy, surpassing all previous works in terms of speed.",1
"Dense disparities among multiple views is essential for estimating the 3D architecture of a scene based on the geometrical relationship among the scene and the views or cameras. Scenes with larger extents of heterogeneous textures, differing scene illumination among the multiple views and with occluding objects affect the accuracy of the estimated disparities. Markov random fields (MRF) based methods for disparity estimation address these limitations using spatial dependencies among the observations and among the disparity estimates. These methods, however, are limited by spatially fixed and smaller neighborhood systems or cliques. In this work, we present a new factor graph-based probabilistic graphical model for disparity estimation that allows a larger and a spatially variable neighborhood structure determined based on the local scene characteristics. We evaluated our method using the Middlebury benchmark stereo datasets and the Middlebury evaluation dataset version 3.0 and compared its performance with recent state-of-the-art disparity estimation algorithms. The new factor graph-based method provided disparity estimates with higher accuracy when compared to the recent non-learning- and learning-based disparity estimation algorithms. In addition to disparity estimation, our factor graph formulation can be useful for obtaining maximum a posteriori solution to optimization problems with complex and variable dependency structures as well as for other dense estimation problems such as optical flow estimation.",0
"To estimate the 3D architecture of a scene based on its relationship with multiple views or cameras, it is crucial to have dense disparities among those views. However, scenes with heterogeneous textures, varying illumination, and occluding objects can affect the accuracy of estimated disparities. Markov random fields (MRF) are commonly used to address these limitations by considering spatial dependencies among observations and disparity estimates. However, MRF-based methods are limited by smaller, fixed neighborhood systems or cliques. In this study, we introduce a novel factor graph-based probabilistic graphical model for disparity estimation that allows for a larger and spatially variable neighborhood structure determined by local scene characteristics. Our method was evaluated using the Middlebury benchmark stereo datasets and the Middlebury evaluation dataset version 3.0, and was found to provide more accurate disparity estimates than recent non-learning- and learning-based disparity estimation algorithms. Additionally, our factor graph formulation has the potential to be useful for solving optimization problems with complex and variable dependency structures, as well as for other dense estimation problems like optical flow estimation.",1
"Facial expression spotting is the preliminary step for micro- and macro-expression analysis. The task of reliably spotting such expressions in video sequences is currently unsolved. The current best systems depend upon optical flow methods to extract regional motion features, before categorisation of that motion into a specific class of facial movement. Optical flow is susceptible to drift error, which introduces a serious problem for motions with long-term dependencies, such as high frame-rate macro-expression. We propose a purely deep learning solution which, rather than tracking frame differential motion, compares via a convolutional model, each frame with two temporally local reference frames. Reference frames are sampled according to calculated micro- and macro-expression durations. We show that our solution achieves state-of-the-art performance (F1-score of 0.105) in a dataset of high frame-rate (200 fps) long video sequences (SAMM-LV) and is competitive in a low frame-rate (30 fps) dataset (CAS(ME)2). In this paper, we document our deep learning model and parameters, including how we use local contrast normalisation, which we show is critical for optimal results. We surpass a limitation in existing methods, and advance the state of deep learning in the domain of facial expression spotting.",0
"The initial step for analyzing micro- and macro-expressions is to spot facial expressions, but reliably identifying these expressions in video sequences is currently a problem that has not been solved. The best systems currently rely on optical flow methods to extract regional motion features, which are then categorized into specific classes of facial movement. However, optical flow is susceptible to drift error, making it problematic for analyzing long-term dependencies. To address this issue, we propose a purely deep learning solution that utilizes a convolutional model to compare each frame with two temporally local reference frames, which are sampled based on calculated micro- and macro-expression durations. Our approach achieves state-of-the-art performance on a high frame-rate dataset (SAMM-LV) and competitive performance on a low frame-rate dataset (CAS(ME)2). The paper details our deep learning model and parameters, including the use of local contrast normalization, which we demonstrate is critical for optimal results. Our solution surpasses existing methods and advances the state of deep learning in facial expression spotting.",1
"The temporal and spatial resolution of rainfall data is crucial for climate change modeling studies in which its variability in space and time is considered as a primary factor. Rainfall products from different remote sensing instruments (e.g., radar or satellite) provide different space-time resolutions because of the differences in their sensing capabilities. We developed an approach that augments rainfall data with increased time resolutions to complement relatively lower resolution products. This study proposes a neural network architecture based on Convolutional Neural Networks (CNNs) to improve temporal resolution of radar-based rainfall products and compares the proposed model with an optical flow-based interpolation method.",0
"The accuracy of rainfall data's temporal and spatial resolution is vital in climate change modeling studies, where its variability in space and time is a significant factor. Rainfall data from remote sensing instruments, such as radar or satellite, offers various space-time resolutions due to differing sensing capabilities. To improve rainfall data's temporal resolution, we devised a method to supplement lower resolution products with higher time resolutions. This study introduces a neural network architecture that employs Convolutional Neural Networks (CNNs) to enhance the temporal resolution of radar-based rainfall data and compares it with an optical flow-based interpolation method.",1
"Lines provide the significantly richer geometric structural information about the environment than points, so lines are widely used in recent Visual Odometry (VO) works. Since VO with lines use line tracking results to locate and map, line tracking is a crucial component in VO. Although the state-of-the-art line tracking methods have made great progress, they are still heavily dependent on line detection or the predicted line segments. In order to relieve the dependencies described above to track line segments completely, accurately, and robustly at higher computational efficiency, we propose a structure-aware Line tracking algorithm based entirely on Optical Flow (LOF). Firstly, we propose a gradient-based strategy to sample pixels on lines that are suitable for line optical flow calculation. Then, in order to align the lines by fully using the structural relationship between the sampled points on it and effectively removing the influence of sampled points on it occluded by other objects, we propose a two-step structure-aware line segment alignment method. Furthermore, we propose a line refinement method to refine the orientation, position, and endpoints of the aligned line segments. Extensive experimental results demonstrate that the proposed LOF outperforms the state-of-the-art performance in line tracking accuracy, robustness, and efficiency, which also improves the location accuracy and robustness of VO system with lines.",0
"Recent Visual Odometry (VO) works heavily rely on lines as they offer more geometric structural information about the environment than points. Line tracking is therefore a crucial component in VO. However, current line tracking methods still heavily depend on line detection or predicted line segments. To overcome this, we propose a structure-aware Line tracking algorithm based solely on Optical Flow (LOF). Our algorithm uses a gradient-based strategy to sample pixels on lines suitable for line optical flow calculation. We also propose a two-step structure-aware line segment alignment method to align lines and remove occlusion. Additionally, we introduce a line refinement method to refine line orientation, position, and endpoints. Our experimental results show that LOF outperforms state-of-the-art methods in accuracy, robustness, and efficiency, improving the location accuracy and robustness of VO systems using lines.",1
"Although deep neural networks (DNNs) enable great progress in video abnormal event detection (VAD), existing solutions typically suffer from two issues: (1) The localization of video events cannot be both precious and comprehensive. (2) The semantics and temporal context are under-explored. To tackle those issues, we are motivated by the prevalent cloze test in education and propose a novel approach named Visual Cloze Completion (VCC), which conducts VAD by learning to complete ""visual cloze tests"" (VCTs). Specifically, VCC first localizes each video event and encloses it into a spatio-temporal cube (STC). To achieve both precise and comprehensive localization, appearance and motion are used as complementary cues to mark the object region associated with each event. For each marked region, a normalized patch sequence is extracted from current and adjacent frames and stacked into a STC. With each patch and the patch sequence of a STC compared to a visual ""word"" and ""sentence"" respectively, we deliberately erase a certain ""word"" (patch) to yield a VCT. Then, the VCT is completed by training DNNs to infer the erased patch and its optical flow via video semantics. Meanwhile, VCC fully exploits temporal context by alternatively erasing each patch in temporal context and creating multiple VCTs. Furthermore, we propose localization-level, event-level, model-level and decision-level solutions to enhance VCC, which can further exploit VCC's potential and produce significant performance improvement gain. Extensive experiments demonstrate that VCC achieves state-of-the-art VAD performance. Our codes and results are open at https://github.com/yuguangnudt/VEC_VAD/tree/VCC.",0
"Video abnormal event detection (VAD) using deep neural networks (DNNs) has made great progress, but current solutions have two main issues: (1) The localization of video events is not precise and comprehensive. (2) There is limited exploration of semantics and temporal context. In order to address these issues, we propose a novel approach called Visual Cloze Completion (VCC) inspired by the cloze test in education. VCC localizes each video event and encloses it into a spatio-temporal cube (STC) using appearance and motion as complementary cues. We extract a normalized patch sequence from current and adjacent frames for each marked region and stack them into an STC. We then erase a certain ""word"" (patch) and infer it through video semantics using DNNs to complete a ""visual cloze test"" (VCT). To fully exploit temporal context, we create multiple VCTs by alternatively erasing each patch. We also propose localization-level, event-level, model-level, and decision-level solutions to enhance VCC. Our experiments show that VCC achieves state-of-the-art VAD performance. Our codes and results are open at https://github.com/yuguangnudt/VEC_VAD/tree/VCC.",1
"We introduce RAFT-Stereo, a new deep architecture for rectified stereo based on the optical flow network RAFT. We introduce multi-level convolutional GRUs, which more efficiently propagate information across the image. A modified version of RAFT-Stereo can perform accurate real-time inference. RAFT-stereo ranks first on the Middlebury leaderboard, outperforming the next best method on 1px error by 29% and outperforms all published work on the ETH3D two-view stereo benchmark. Code is available at https://github.com/princeton-vl/RAFT-Stereo.",0
"Introducing RAFT-Stereo, a novel deep architecture for rectified stereo that is based on the optical flow network RAFT. Our approach features multi-level convolutional GRUs that enable more efficient propagation of information across the image. We have also developed a modified version of RAFT-Stereo that is capable of performing accurate real-time inference. Notably, RAFT-Stereo has achieved the top ranking on the Middlebury leaderboard, surpassing the next best method on 1px error by 29% and outperforming all published work on the ETH3D two-view stereo benchmark. Interested parties can access the code at https://github.com/princeton-vl/RAFT-Stereo.",1
"Automatic portrait video matting is an under-constrained problem. Most state-of-the-art methods only exploit the semantic information and process each frame individually. Their performance is compromised due to the lack of temporal information between the frames. To solve this problem, we propose the context motion network to leverage semantic information and motion information. To capture the motion information, we estimate the optical flow and design a context-motion updating operator to integrate features between frames recurrently. Our experiments show that our network outperforms state-of-the-art matting methods significantly on the Video240K SD dataset.",0
"The issue of automatic portrait video matting is not fully resolved, as it is an under-constrained problem. Although many of the latest techniques depend solely on semantic information and tackle one frame at a time, their effectiveness is hindered due to the absence of temporal information across the frames. We suggest a solution to this problem by introducing the context motion network, which utilizes both semantic and motion information. We have devised a context-motion updating operator to combine features recurrently between frames, and to capture motion information, we have estimated the optical flow. Our findings demonstrate that our network surpasses the most advanced matting methods on the Video240K SD dataset.",1
"Video semantic segmentation requires to utilize the complex temporal relations between frames of the video sequence. Previous works usually exploit accurate optical flow to leverage the temporal relations, which suffer much from heavy computational cost. In this paper, we propose a Temporal Memory Attention Network (TMANet) to adaptively integrate the long-range temporal relations over the video sequence based on the self-attention mechanism without exhaustive optical flow prediction. Specially, we construct a memory using several past frames to store the temporal information of the current frame. We then propose a temporal memory attention module to capture the relation between the current frame and the memory to enhance the representation of the current frame. Our method achieves new state-of-the-art performances on two challenging video semantic segmentation datasets, particularly 80.3% mIoU on Cityscapes and 76.5% mIoU on CamVid with ResNet-50.",0
"To perform video semantic segmentation, it is necessary to consider the intricate temporal relationships between the frames in the video sequence. Previous research has often utilized accurate optical flow to exploit these temporal relations, but this method is computationally expensive. In this study, we present the Temporal Memory Attention Network (TMANet), which utilizes a self-attention mechanism to adaptively integrate long-range temporal relations without the need for exhaustive optical flow prediction. Our approach involves constructing a memory that stores temporal information from several past frames to aid in the segmentation of the current frame. We then use a temporal memory attention module to capture the relationship between the current frame and the memory and enhance the current frame's representation. Our method outperforms previous state-of-the-art techniques on two challenging video semantic segmentation datasets, achieving a mIoU of 80.3% on Cityscapes and 76.5% on CamVid with ResNet-50.",1
"One of the most common problems encountered in human-computer interaction is automatic facial expression recognition. Although it is easy for human observer to recognize facial expressions, automatic recognition remains difficult for machines. One of the methods that machines can recognize facial expression is analyzing the changes in face during facial expression presentation. In this paper, optical flow algorithm was used to extract deformation or motion vectors created in the face because of facial expressions. Then, these extracted motion vectors are used to be analyzed. Their positions and directions were exploited for automatic facial expression recognition using different data mining techniques. It means that by employing motion vector features used as our data, facial expressions were recognized. Some of the most state-of-the-art classification algorithms such as C5.0, CRT, QUEST, CHAID, Deep Learning (DL), SVM and Discriminant algorithms were used to classify the extracted motion vectors. Using 10-fold cross validation, their performances were calculated. To compare their performance more precisely, the test was repeated 50 times. Meanwhile, the deformation of face was also analyzed in this research. For example, what exactly happened in each part of face when a person showed fear? Experimental results on Extended Cohen-Kanade (CK+) facial expression dataset demonstrated that the best methods were DL, SVM and C5.0, with the accuracy of 95.3%, 92.8% and 90.2% respectively.",0
"Facial expression recognition is a common challenge in human-computer interaction. While humans can easily identify facial expressions, machines struggle with this task. One approach for machines is to analyze changes in the face during expression presentation. This study used an optical flow algorithm to extract motion vectors from facial deformations caused by expressions. These vectors were analyzed and used for automatic facial expression recognition through various data mining techniques. State-of-the-art algorithms, such as C5.0, CRT, QUEST, CHAID, DL, SVM, and Discriminant, were employed for classification. To evaluate their performance, 10-fold cross-validation was used, and the test was repeated 50 times. Additionally, this research analyzed facial deformation during expressions, such as how fear manifests in different parts of the face. Results from the Extended Cohen-Kanade dataset showed that DL, SVM, and C5.0 were the most accurate methods, achieving 95.3%, 92.8%, and 90.2% accuracy, respectively.",1
"Event camera has offered promising alternative for visual perception, especially in high speed and high dynamic range scenes. Recently, many deep learning methods have shown great success in providing model-free solutions to many event-based problems, such as optical flow estimation. However, existing deep learning methods did not address the importance of temporal information well from the perspective of architecture design and cannot effectively extract spatio-temporal features. Another line of research that utilizes Spiking Neural Network suffers from training issues for deeper architecture. To address these points, a novel input representation is proposed that captures the events temporal distribution for signal enhancement. Moreover, we introduce a spatio-temporal recurrent encoding-decoding neural network architecture for event-based optical flow estimation, which utilizes Convolutional Gated Recurrent Units to extract feature maps from a series of event images. Besides, our architecture allows some traditional frame-based core modules, such as correlation layer and iterative residual refine scheme, to be incorporated. The network is end-to-end trained with self-supervised learning on the Multi-Vehicle Stereo Event Camera dataset. We have shown that it outperforms all the existing state-of-the-art methods by a large margin.",0
"The event camera presents a promising alternative to visual perception, especially for high-speed and high-dynamic-range scenes. Although deep learning methods have shown success in offering model-free solutions to event-based problems like optical flow estimation, they fail to address the importance of temporal information and struggle to extract spatio-temporal features. Spiking Neural Networks also pose training issues for deeper architectures. To tackle these challenges, a novel input representation that captures the temporal distribution of events is proposed for signal enhancement. Additionally, a spatio-temporal recurrent encoding-decoding neural network architecture is introduced for event-based optical flow estimation, utilizing Convolutional Gated Recurrent Units to extract feature maps from event images. Traditional frame-based core modules like correlation layer and iterative residual refine scheme are incorporated into the architecture. The network is self-supervisedly trained end-to-end on the Multi-Vehicle Stereo Event Camera dataset, surpassing all existing state-of-the-art methods by a significant margin.",1
"We propose a novel neural-network-based method to perform matting of videos depicting people that does not require additional user input such as trimaps. Our architecture achieves temporal stability of the resulting alpha mattes by using motion-estimation-based smoothing of image-segmentation algorithm outputs, combined with convolutional-LSTM modules on U-Net skip connections.   We also propose a fake-motion algorithm that generates training clips for the video-matting network given photos with ground-truth alpha mattes and background videos. We apply random motion to photos and their mattes to simulate movement one would find in real videos and composite the result with the background clips. It lets us train a deep neural network operating on videos in an absence of a large annotated video dataset and provides ground-truth training-clip foreground optical flow for use in loss functions.",0
"Our innovative approach utilizes a neural network to conduct video matting of individuals without requiring additional user input such as trimaps. Our design guarantees stable alpha matte outcomes through motion-based smoothing of image segmentation algorithm outputs and convolutional-LSTM modules on U-Net skip connections. In addition, we propose a false motion algorithm that creates training clips for the video-matting network using photos that possess ground-truth alpha mattes and background videos. By applying random motion to both the photos and their mattes, we simulate the movement that exists in actual videos and combine it with the background clips. This approach enables us to train a deep neural network to operate on videos even in the absence of a large annotated video dataset, providing ground-truth training-clip foreground optical flow for use in loss functions.",1
"Self-supervised Multi-view stereo (MVS) with a pretext task of image reconstruction has achieved significant progress recently. However, previous methods are built upon intuitions, lacking comprehensive explanations about the effectiveness of the pretext task in self-supervised MVS. To this end, we propose to estimate epistemic uncertainty in self-supervised MVS, accounting for what the model ignores. Specially, the limitations can be categorized into two types: ambiguious supervision in foreground and invalid supervision in background. To address these issues, we propose a novel Uncertainty reduction Multi-view Stereo (UMVS) framework for self-supervised learning. To alleviate ambiguous supervision in foreground, we involve extra correspondence prior with a flow-depth consistency loss. The dense 2D correspondence of optical flows is used to regularize the 3D stereo correspondence in MVS. To handle the invalid supervision in background, we use Monte-Carlo Dropout to acquire the uncertainty map and further filter the unreliable supervision signals on invalid regions. Extensive experiments on DTU and Tank&Temples benchmark show that our U-MVS framework achieves the best performance among unsupervised MVS methods, with competitive performance with its supervised opponents.",0
"Recently, there has been significant progress in self-supervised Multi-view stereo (MVS) using an image reconstruction pretext task. However, previous methods lack comprehensive explanations for the effectiveness of this approach, as they are based on intuition. To address this issue, we propose an Uncertainty reduction Multi-view Stereo (UMVS) framework for self-supervised learning that estimates epistemic uncertainty in MVS to account for what the model ignores. We identify two limiting factors: ambiguous supervision in the foreground and invalid supervision in the background. To overcome these limitations, we integrate an extra correspondence prior with a flow-depth consistency loss to alleviate ambiguous supervision in the foreground and use Monte-Carlo Dropout to obtain the uncertainty map and filter unreliable supervision signals on invalid regions to handle invalid supervision in the background. Our U-MVS framework achieves the best performance among unsupervised MVS methods and has competitive performance with its supervised counterparts, as demonstrated through extensive experiments on the DTU and Tank&Temples benchmarks.",1
"There hardly exists any large-scale datasets with dense optical flow of non-rigid motion from real-world imagery as of today. The reason lies mainly in the required setup to derive ground truth optical flows: a series of images with known camera poses along its trajectory, and an accurate 3D model from a textured scene. Human annotation is not only too tedious for large databases, it can simply hardly contribute to accurate optical flow. To circumvent the need for manual annotation, we propose a framework to automatically generate optical flow from real-world videos. The method extracts and matches objects from video frames to compute initial constraints, and applies a deformation over the objects of interest to obtain dense optical flow fields. We propose several ways to augment the optical flow variations. Extensive experimental results show that training on our automatically generated optical flow outperforms methods that are trained on rigid synthetic data using FlowNet-S, LiteFlowNet, PWC-Net, and RAFT. Datasets and implementation of our optical flow generation framework are released at https://github.com/lhoangan/arap_flow",0
"At present, there is a scarcity of comprehensive datasets containing dense optical flow for non-rigid motion captured in real-world imagery. The primary factor behind this shortage is the difficulty in obtaining ground truth optical flows, which necessitates a sequence of images with known camera poses and an accurate 3D model of the textured scene. Manual annotation is impractical for large databases and is not very effective in achieving precise optical flow. To address this issue, we propose a framework for automatically generating optical flow from real-world videos. The method involves object extraction and matching in video frames to compute initial constraints, followed by the application of deformation to objects of interest to obtain dense optical flow fields. We offer various ways to enhance optical flow variations. Our extensive experimental results demonstrate that our automatically generated optical flow outperforms methods trained on rigid synthetic data, including FlowNet-S, LiteFlowNet, PWC-Net, and RAFT. We have made available the datasets and implementation of our optical flow generation framework at https://github.com/lhoangan/arap_flow.",1
"As moving objects always draw more attention of human eyes, the temporal motive information is always exploited complementarily with spatial information to detect salient objects in videos. Although efficient tools such as optical flow have been proposed to extract temporal motive information, it often encounters difficulties when used for saliency detection due to the movement of camera or the partial movement of salient objects. In this paper, we investigate the complimentary roles of spatial and temporal information and propose a novel dynamic spatiotemporal network (DS-Net) for more effective fusion of spatiotemporal information. We construct a symmetric two-bypass network to explicitly extract spatial and temporal features. A dynamic weight generator (DWG) is designed to automatically learn the reliability of corresponding saliency branch. And a top-down cross attentive aggregation (CAA) procedure is designed so as to facilitate dynamic complementary aggregation of spatiotemporal features. Finally, the features are modified by spatial attention with the guidance of coarse saliency map and then go through decoder part for final saliency map. Experimental results on five benchmarks VOS, DAVIS, FBMS, SegTrack-v2, and ViSal demonstrate that the proposed method achieves superior performance than state-of-the-art algorithms. The source code is available at https://github.com/TJUMMG/DS-Net.",0
"Temporal motive information is used in conjunction with spatial information to detect salient objects in videos as moving objects tend to attract more attention. Optical flow is an efficient tool for extracting temporal motive information, but its use in saliency detection is challenged by camera movement or partial movement of salient objects. To address this, we introduce a new approach that combines spatial and temporal information more effectively through a dynamic spatiotemporal network (DS-Net). Our approach includes a symmetric two-bypass network that explicitly extracts spatial and temporal features, a dynamic weight generator (DWG) that learns the reliability of the saliency branch, and a top-down cross attentive aggregation (CAA) procedure for dynamic complementary aggregation of spatiotemporal features. Spatial attention, guided by a coarse saliency map, is then used to modify the features, which are finally processed by a decoder for the saliency map. Results on five benchmarks show that our method outperforms state-of-the-art algorithms. The source code is available at https://github.com/TJUMMG/DS-Net.",1
"In this work, we tackle the essential problem of scale inconsistency for self-supervised joint depth-pose learning. Most existing methods assume that a consistent scale of depth and pose can be learned across all input samples, which makes the learning problem harder, resulting in degraded performance and limited generalization in indoor environments and long-sequence visual odometry application. To address this issue, we propose a novel system that explicitly disentangles scale from the network estimation. Instead of relying on PoseNet architecture, our method recovers relative pose by directly solving fundamental matrix from dense optical flow correspondence and makes use of a two-view triangulation module to recover an up-to-scale 3D structure. Then, we align the scale of the depth prediction with the triangulated point cloud and use the transformed depth map for depth error computation and dense reprojection check. Our whole system can be jointly trained end-to-end. Extensive experiments show that our system not only reaches state-of-the-art performance on KITTI depth and flow estimation, but also significantly improves the generalization ability of existing self-supervised depth-pose learning methods under a variety of challenging scenarios, and achieves state-of-the-art results among self-supervised learning-based methods on KITTI Odometry and NYUv2 dataset. Furthermore, we present some interesting findings on the limitation of PoseNet-based relative pose estimation methods in terms of generalization ability. Code is available at https://github.com/B1ueber2y/TrianFlow.",0
"The focus of our work is on addressing scale inconsistency in self-supervised joint depth-pose learning. The current methods assume that a uniform scale of depth and pose can be learned, which results in reduced performance and limited generalization for the long-sequence visual odometry application and indoor environments. We propose a novel system that separates scale from network estimation. Instead of using PoseNet architecture, we recover relative pose by solving fundamental matrix from dense optical flow correspondence and use a two-view triangulation module to determine an up-to-scale 3D structure. We align the scale of the depth prediction with the triangulated point cloud and use the transformed depth map for depth error computation and dense reprojection check. Our entire system can be trained end-to-end. Our experiments indicate that our system not only achieves state-of-the-art performance on KITTI depth and flow estimation but also enhances the generalization ability of existing self-supervised depth-pose learning methods in challenging scenarios. Our method also outperforms other self-supervised learning-based methods on KITTI Odometry and NYUv2 datasets. We also discovered some interesting limitations of PoseNet-based relative pose estimation methods in terms of generalization ability. The code is available at https://github.com/B1ueber2y/TrianFlow.",1
"Recent efforts towards video anomaly detection (VAD) try to learn a deep autoencoder to describe normal event patterns with small reconstruction errors. The video inputs with large reconstruction errors are regarded as anomalies at the test time. However, these methods sometimes reconstruct abnormal inputs well because of the powerful generalization ability of deep autoencoder. To address this problem, we present a novel approach for anomaly detection, which utilizes discriminative prototypes of normal data to reconstruct video frames. In this way, the model will favor the reconstruction of normal events and distort the reconstruction of abnormal events. Specifically, we use a prototype-guided memory module to perform discriminative latent embedding. We introduce a new discriminative criterion for the memory module, as well as a loss function correspondingly, which can encourage memory items to record the representative embeddings of normal data, i.e. prototypes. Besides, we design a novel two-branch autoencoder, which is composed of a future frame prediction network and an RGB difference generation network that share the same encoder. The stacked RGB difference contains motion information just like optical flow, so our model can learn temporal regularity. We evaluate the effectiveness of our method on three benchmark datasets and experimental results demonstrate the proposed method outperforms the state-of-the-art.",0
"The current trend in video anomaly detection (VAD) is to utilize deep autoencoders to learn normal event patterns that have small reconstruction errors. During testing, inputs with large reconstruction errors are considered anomalies. However, the generalization ability of deep autoencoders sometimes allows abnormal inputs to be reconstructed properly, creating a problem. To address this issue, we propose a new approach that employs discriminative prototypes of normal data to reconstruct video frames. This way, the model prioritizes the reconstruction of normal events and distorts the reconstruction of abnormal ones. We use a prototype-guided memory module for discriminative latent embedding, introducing a new discriminative criterion and loss function to encourage memory items to record the representative embeddings of normal data. We also design a novel two-branch autoencoder consisting of a future frame prediction network and an RGB difference generation network sharing the same encoder. The stacked RGB difference contains motion information, allowing our model to learn temporal regularity. We evaluate our method on three benchmark datasets and demonstrate its superiority over the state-of-the-art.",1
"Action anticipation in egocentric videos is a difficult task due to the inherently multi-modal nature of human actions. Additionally, some actions happen faster or slower than others depending on the actor or surrounding context which could vary each time and lead to different predictions. Based on this idea, we build upon RULSTM architecture, which is specifically designed for anticipating human actions, and propose a novel attention-based technique to evaluate, simultaneously, slow and fast features extracted from three different modalities, namely RGB, optical flow, and extracted objects. Two branches process information at different time scales, i.e., frame-rates, and several fusion schemes are considered to improve prediction accuracy. We perform extensive experiments on EpicKitchens-55 and EGTEA Gaze+ datasets, and demonstrate that our technique systematically improves the results of RULSTM architecture for Top-5 accuracy metric at different anticipation times.",0
"Anticipating actions in egocentric videos is a challenging task due to the complex nature of human actions, which involve multiple modalities. Moreover, the speed of actions may vary depending on the actor and context, leading to diverse predictions. To address this issue, we enhance the RULSTM architecture, which is designed for action anticipation, by introducing an innovative attention-based technique that evaluates fast and slow features extracted from three modalities: RGB, optical flow, and object extraction. The technique employs two branches that process information at different time scales and considers several fusion schemes to enhance prediction accuracy. We conduct extensive experiments on the EpicKitchens-55 and EGTEA Gaze+ datasets and demonstrate that our approach significantly improves the Top-5 accuracy metric of the RULSTM architecture across various anticipation times.",1
"We propose FlowReg, a deep learning-based framework for unsupervised image registration for neuroimaging applications. The system is composed of two architectures that are trained sequentially: FlowReg-A which affinely corrects for gross differences between moving and fixed volumes in 3D followed by FlowReg-O which performs pixel-wise deformations on a slice-by-slice basis for fine tuning in 2D. The affine network regresses the 3D affine matrix based on a correlation loss function that enforces global similarity. The deformable network operates on 2D image slices based on the optical flow network FlowNet-Simple but with three loss components. The photometric loss minimizes pixel intensity differences differences, the smoothness loss encourages similar magnitudes between neighbouring vectors, and a correlation loss that is used to maintain the intensity similarity between fixed and moving image slices. The proposed method is compared to four open source registration techniques ANTs, Demons, SE, and Voxelmorph. In total, 4643 FLAIR MR imaging volumes are used from dementia and vascular disease cohorts, acquired from over 60 international centres with varying acquisition parameters. A battery of quantitative novel registration validation metrics are proposed that focus on the structural integrity of tissues, spatial alignment, and intensity similarity. Experimental results show FlowReg (FlowReg-A+O) performs better than iterative-based registration algorithms for intensity and spatial alignment metrics with a Pixelwise Agreement of 0.65, correlation coefficient of 0.80, and Mutual Information of 0.29. Among the deep learning frameworks, FlowReg-A or FlowReg-A+O provided the highest performance over all but one of the metrics. Results show that FlowReg is able to obtain high intensity and spatial similarity while maintaining the shape and structure of anatomy and pathology.",0
"Our proposition is FlowReg, a framework for unsupervised image registration for neuroimaging applications, which is based on deep learning. FlowReg consists of two architectures that are trained sequentially. First, FlowReg-A affinely corrects for gross differences between the moving and fixed volumes in 3D. Then, FlowReg-O performs pixel-wise deformations on a slice-by-slice basis to fine-tune the alignment in 2D. The affine network regresses the 3D affine matrix, using a correlation loss function that enforces global similarity. The deformable network operates on 2D image slices, using three loss components: photometric loss, smoothness loss, and correlation loss. We compare our proposed method to four open source registration techniques (ANTs, Demons, SE, and Voxelmorph), using 4643 FLAIR MR imaging volumes from dementia and vascular disease cohorts, acquired from over 60 international centres with varying acquisition parameters. We propose a battery of quantitative novel registration validation metrics that focus on the structural integrity of tissues, spatial alignment, and intensity similarity. Experimental results show that FlowReg performs better than iterative-based registration algorithms for intensity and spatial alignment metrics, with a Pixelwise Agreement of 0.65, correlation coefficient of 0.80, and Mutual Information of 0.29. Among the deep learning frameworks, FlowReg-A or FlowReg-A+O provided the highest performance over all but one of the metrics. Results show that FlowReg is able to obtain high intensity and spatial similarity while maintaining the shape and structure of anatomy and pathology.",1
"Recent years have seen considerable research activities devoted to video enhancement that simultaneously increases temporal frame rate and spatial resolution. However, the existing methods either fail to explore the intrinsic relationship between temporal and spatial information or lack flexibility in the choice of final temporal/spatial resolution. In this work, we propose an unconstrained space-time video super-resolution network, which can effectively exploit space-time correlation to boost performance. Moreover, it has complete freedom in adjusting the temporal frame rate and spatial resolution through the use of the optical flow technique and a generalized pixelshuffle operation. Our extensive experiments demonstrate that the proposed method not only outperforms the state-of-the-art, but also requires far fewer parameters and less running time.",0
"In recent years, research has focused on enhancing video quality by increasing both temporal frame rate and spatial resolution. However, current methods either do not consider the connection between temporal and spatial information or limit the options for final resolution. Our solution is an unconstrained video super-resolution network that leverages space-time correlation to improve performance. This network can adjust temporal frame rate and spatial resolution using the optical flow technique and a generalized pixelshuffle operation. Our experiments show that this method surpasses the state-of-the-art with fewer parameters and less running time.",1
"We propose to incorporate feature correlation and sequential processing into dense optical flow estimation from event cameras. Modern frame-based optical flow methods heavily rely on matching costs computed from feature correlation. In contrast, there exists no optical flow method for event cameras that explicitly computes matching costs. Instead, learning-based approaches using events usually resort to the U-Net architecture to estimate optical flow sparsely. Our key finding is that the introduction of correlation features significantly improves results compared to previous methods that solely rely on convolution layers. Compared to the state-of-the-art, our proposed approach computes dense optical flow and reduces the end-point error by 23% on MVSEC. Furthermore, we show that all existing optical flow methods developed so far for event cameras have been evaluated on datasets with very small displacement fields with a maximum flow magnitude of 10 pixels. Based on this observation, we introduce a new real-world dataset that exhibits displacement fields with magnitudes up to 210 pixels and 3 times higher camera resolution. Our proposed approach reduces the end-point error on this dataset by 66%.",0
"Our proposal aims to enhance dense optical flow estimation from event cameras by incorporating feature correlation and sequential processing. While traditional frame-based optical flow methods rely on matching costs from feature correlation, no existing method for event cameras explicitly computes matching costs. Instead, learning-based approaches using events typically use the U-Net architecture to estimate optical flow sparsely. Our research reveals that incorporating correlation features significantly improves results compared to previous methods that solely rely on convolution layers. Our proposed approach computes dense optical flow and reduces end-point error by 23% on MVSEC, outperforming state-of-the-art methods. Additionally, we noticed that all existing optical flow methods for event cameras have only been tested on datasets with small displacement fields. To address this, we introduce a new real-world dataset with magnitudes up to 210 pixels and 3 times higher camera resolution. Our approach reduces the end-point error on this dataset by 66%.",1
"Optical flow is inherently a 2D search problem, and thus the computational complexity grows quadratically with respect to the search window, making large displacements matching infeasible for high-resolution images. In this paper, we take inspiration from Transformers and propose a new method for high-resolution optical flow estimation with significantly less computation. Specifically, a 1D attention operation is first applied in the vertical direction of the target image, and then a simple 1D correlation in the horizontal direction of the attended image is able to achieve 2D correspondence modeling effect. The directions of attention and correlation can also be exchanged, resulting in two 3D cost volumes that are concatenated for optical flow estimation. The novel 1D formulation empowers our method to scale to very high-resolution input images while maintaining competitive performance. Extensive experiments on Sintel, KITTI and real-world 4K ($2160 \times 3840$) resolution images demonstrated the effectiveness and superiority of our proposed method. Code and models are available at \url{https://github.com/haofeixu/flow1d}.",0
"The complexity of optical flow increases quadratically as the search window expands, which limits the feasibility of matching large displacements for high-resolution images. To address this issue, we draw inspiration from Transformers and present a novel approach for high-resolution optical flow estimation that requires less computation. Our method involves applying a 1D attention operation vertically on the target image, followed by a simple 1D correlation horizontally on the attended image to achieve 2D correspondence modeling. The direction of attention and correlation can be interchanged, resulting in two 3D cost volumes that are concatenated for optical flow estimation. This 1D formulation enables our method to scale to very high-resolution input images while maintaining competitive performance. Extensive experiments on Sintel, KITTI, and real-world 4K images (2160 x 3840 resolution) demonstrate the effectiveness and superiority of our proposed method. Code and models are available at https://github.com/haofeixu/flow1d.",1
"In this paper, we propose to learn an Unsupervised Single Object Tracker (USOT) from scratch. We identify that three major challenges, i.e., moving object discovery, rich temporal variation exploitation, and online update, are the central causes of the performance bottleneck of existing unsupervised trackers. To narrow the gap between unsupervised trackers and supervised counterparts, we propose an effective unsupervised learning approach composed of three stages. First, we sample sequentially moving objects with unsupervised optical flow and dynamic programming, instead of random cropping. Second, we train a naive Siamese tracker from scratch using single-frame pairs. Third, we continue training the tracker with a novel cycle memory learning scheme, which is conducted in longer temporal spans and also enables our tracker to update online. Extensive experiments show that the proposed USOT learned from unlabeled videos performs well over the state-of-the-art unsupervised trackers by large margins, and on par with recent supervised deep trackers. Code is available at https://github.com/VISION-SJTU/USOT.",0
"This paper presents a proposal for learning an Unsupervised Single Object Tracker (USOT) from scratch. The authors have identified three major challenges that cause performance issues in existing unsupervised trackers: moving object discovery, rich temporal variation exploitation, and online update. To address these challenges, the authors propose an effective unsupervised learning approach that involves three stages. Firstly, the authors use unsupervised optical flow and dynamic programming to sequentially sample moving objects instead of random cropping. Secondly, they train a naive Siamese tracker from scratch using single-frame pairs. Thirdly, they continue training the tracker with a cycle memory learning scheme that allows for longer temporal spans and online updates. Extensive experiments demonstrate that the proposed USOT outperforms existing unsupervised trackers by significant margins and performs comparably to recent supervised deep trackers. The code for this approach is available at https://github.com/VISION-SJTU/USOT.",1
"This paper presents a novel method for pedestrian detection and tracking by fusing camera and LiDAR sensor data. To deal with the challenges associated with the autonomous driving scenarios, an integrated tracking and detection framework is proposed. The detection phase is performed by converting LiDAR streams to computationally tractable depth images, and then, a deep neural network is developed to identify pedestrian candidates both in RGB and depth images. To provide accurate information, the detection phase is further enhanced by fusing multi-modal sensor information using the Kalman filter. The tracking phase is a combination of the Kalman filter prediction and an optical flow algorithm to track multiple pedestrians in a scene. We evaluate our framework on a real public driving dataset. Experimental results demonstrate that the proposed method achieves significant performance improvement over a baseline method that solely uses image-based pedestrian detection.",0
"A new technique for detecting and tracking pedestrians through the combination of camera and LiDAR sensor data is introduced. This method addresses the difficulties that arise in autonomous driving situations through the use of an integrated tracking and detection framework. LiDAR streams are converted into depth images to facilitate the detection process, which is carried out by a deep neural network that identifies pedestrian candidates in both RGB and depth images. To enhance accuracy, multi-modal sensor data is fused by means of a Kalman filter. The tracking phase employs a combination of Kalman filter prediction and an optical flow algorithm to track multiple pedestrians in a given scene. The effectiveness of the proposed approach is demonstrated through experimentation with a real-world driving dataset, with results showing significant performance gains over a baseline method that relies solely on image-based pedestrian detection.",1
"Learning transferable and domain adaptive feature representations from videos is important for video-relevant tasks such as action recognition. Existing video domain adaptation methods mainly rely on adversarial feature alignment, which has been derived from the RGB image space. However, video data is usually associated with multi-modal information, e.g., RGB and optical flow, and thus it remains a challenge to design a better method that considers the cross-modal inputs under the cross-domain adaptation setting. To this end, we propose a unified framework for video domain adaptation, which simultaneously regularizes cross-modal and cross-domain feature representations. Specifically, we treat each modality in a domain as a view and leverage the contrastive learning technique with properly designed sampling strategies. As a result, our objectives regularize feature spaces, which originally lack the connection across modalities or have less alignment across domains. We conduct experiments on domain adaptive action recognition benchmark datasets, i.e., UCF, HMDB, and EPIC-Kitchens, and demonstrate the effectiveness of our components against state-of-the-art algorithms.",0
"Developing transferable and adaptable feature representations from videos is crucial for tasks that involve videos, such as recognizing actions. Existing methods for video domain adaptation primarily depend on adversarial feature alignment, which is based on the RGB image space. However, video data includes multi-modal information, such as RGB and optical flow, making it challenging to design a superior method that considers cross-modal inputs under the cross-domain adaptation setting. To address this issue, we propose a comprehensive framework for video domain adaptation that simultaneously regulates cross-modal and cross-domain feature representations. We consider each modality in a domain as a view and employ contrastive learning with well-designed sampling strategies. Thus, our objectives regulate feature spaces that lack connections across modalities or have less alignment across domains. We evaluate our components on benchmark datasets for domain adaptive action recognition, including UCF, HMDB, and EPIC-Kitchens, and demonstrate their effectiveness compared to state-of-the-art algorithms.",1
"We propose a framework for aligning and fusing multiple images into a single coordinate-based neural representations. Our framework targets burst images that have misalignment due to camera ego motion and small changes in the scene. We describe different strategies for alignment depending on the assumption of the scene motion, namely, perspective planar (i.e., homography), optical flow with minimal scene change, and optical flow with notable occlusion and disocclusion. Our framework effectively combines the multiple inputs into a single neural implicit function without the need for selecting one of the images as a reference frame. We demonstrate how to use this multi-frame fusion framework for various layer separation tasks.",0
"A framework designed to coordinate and merge numerous images into a single neural representation based on the same coordinates is proposed. The framework is intended to fix misalignments in burst images caused by camera ego motion and slight variations in the scenery. Our framework caters to various alignment strategies, contingent on the motion assumptions of the scenery, including perspective planar (homography), optical flow with minimal scene change, and optical flow with notable occlusion and disocclusion. The framework efficiently combines multiple inputs into a single neural implicit function, eliminating the necessity of selecting one image as a reference frame. We demonstrate the use of this multi-frame fusion framework for various layer separation tasks.",1
"A distinctive feature of Doppler radar is the measurement of velocity in the radial direction for radar points. However, the missing tangential velocity component hampers object velocity estimation as well as temporal integration of radar sweeps in dynamic scenes. Recognizing that fusing camera with radar provides complementary information to radar, in this paper we present a closed-form solution for the point-wise, full-velocity estimate of Doppler returns using the corresponding optical flow from camera images. Additionally, we address the association problem between radar returns and camera images with a neural network that is trained to estimate radar-camera correspondences. Experimental results on the nuScenes dataset verify the validity of the method and show significant improvements over the state-of-the-art in velocity estimation and accumulation of radar points.",0
"Doppler radar has the ability to measure velocity in the radial direction for radar points, but the absence of the tangential velocity component poses problems for estimating object velocity and integrating radar sweeps in dynamic scenes. To overcome this limitation, we propose a solution in this paper that combines camera and radar data to provide complementary information. Specifically, we present a closed-form solution that allows for the estimation of full-velocity using optical flow from camera images. Additionally, we tackle the challenge of associating radar returns with camera images by developing a neural network that is trained to estimate radar-camera correspondences. Our experimental results using the nuScenes dataset demonstrate the effectiveness of our approach, which outperforms existing methods in both velocity estimation and accumulation of radar points.",1
"Existing video stabilization methods often generate visible distortion or require aggressive cropping of frame boundaries, resulting in smaller field of views. In this work, we present a frame synthesis algorithm to achieve full-frame video stabilization. We first estimate dense warp fields from neighboring frames and then synthesize the stabilized frame by fusing the warped contents. Our core technical novelty lies in the learning-based hybrid-space fusion that alleviates artifacts caused by optical flow inaccuracy and fast-moving objects. We validate the effectiveness of our method on the NUS, selfie, and DeepStab video datasets. Extensive experiment results demonstrate the merits of our approach over prior video stabilization methods.",0
"Many video stabilization methods currently in use can create noticeable distortions or need to crop frame boundaries severely, which results in a smaller field of view. This study introduces a frame synthesis algorithm that achieves full-frame video stabilization. To accomplish this, we first estimate dense warp fields from neighbouring frames, and then we fuse the warped contents to create the stabilized frame. The primary technical innovation of our method is the learning-based hybrid-space fusion, which mitigates the effects of optical flow inaccuracies and fast-moving objects. We tested our approach on the NUS, selfie, and DeepStab video datasets, and the results of extensive experiments indicate that our method has significant advantages over prior video stabilization methods.",1
"Fingerspelling in sign language has been the means of communicating technical terms and proper nouns when they do not have dedicated sign language gestures. Automatic recognition of fingerspelling can help resolve communication barriers when interacting with deaf people. The main challenges prevalent in fingerspelling recognition are the ambiguity in the gestures and strong articulation of the hands. The automatic recognition model should address high inter-class visual similarity and high intra-class variation in the gestures. Most of the existing research in fingerspelling recognition has focused on the dataset collected in a controlled environment. The recent collection of a large-scale annotated fingerspelling dataset in the wild, from social media and online platforms, captures the challenges in a real-world scenario. In this work, we propose a fine-grained visual attention mechanism using the Transformer model for the sequence-to-sequence prediction task in the wild dataset. The fine-grained attention is achieved by utilizing the change in motion of the video frames (optical flow) in sequential context-based attention along with a Transformer encoder model. The unsegmented continuous video dataset is jointly trained by balancing the Connectionist Temporal Classification (CTC) loss and the maximum-entropy loss. The proposed approach can capture better fine-grained attention in a single iteration. Experiment evaluations show that it outperforms the state-of-the-art approaches.",0
"When sign language lacks specific gestures for technical terms and proper nouns, fingerspelling is used for communication. Automated recognition of fingerspelling can assist in overcoming communication barriers when interacting with the deaf. However, the challenges of ambiguity in gestures and strong articulation of hands must be addressed for successful recognition. Previous research has focused on controlled environments, but a new dataset of fingerspelling in the wild has been collected from social media and online platforms. This presents real-world challenges that require a fine-grained visual attention mechanism using the Transformer model for sequence-to-sequence prediction. By utilizing optical flow and a Transformer encoder model, better fine-grained attention can be achieved. The proposed approach combines Connectionist Temporal Classification (CTC) loss and maximum-entropy loss for joint training of unsegmented continuous video datasets. Experiment evaluations demonstrate that this approach outperforms state-of-the-art methods.",1
"High dynamic range (HDR) video reconstruction from sequences captured with alternating exposures is a very challenging problem. Existing methods often align low dynamic range (LDR) input sequence in the image space using optical flow, and then merge the aligned images to produce HDR output. However, accurate alignment and fusion in the image space are difficult due to the missing details in the over-exposed regions and noise in the under-exposed regions, resulting in unpleasing ghosting artifacts. To enable more accurate alignment and HDR fusion, we introduce a coarse-to-fine deep learning framework for HDR video reconstruction. Firstly, we perform coarse alignment and pixel blending in the image space to estimate the coarse HDR video. Secondly, we conduct more sophisticated alignment and temporal fusion in the feature space of the coarse HDR video to produce better reconstruction. Considering the fact that there is no publicly available dataset for quantitative and comprehensive evaluation of HDR video reconstruction methods, we collect such a benchmark dataset, which contains $97$ sequences of static scenes and 184 testing pairs of dynamic scenes. Extensive experiments show that our method outperforms previous state-of-the-art methods. Our dataset, code and model will be made publicly available.",0
"Reconstructing high dynamic range (HDR) video from sequences of alternating exposures is a difficult task, and current methods struggle to align low dynamic range (LDR) input sequences in the image space using optical flow. This results in unsightly ghosting artifacts due to missing details in over-exposed areas and noise in under-exposed regions. To address this issue, we introduce a deep learning framework for HDR video reconstruction that uses a coarse-to-fine approach. We first perform coarse alignment and pixel blending in the image space to estimate the coarse HDR video. Then, we use a more sophisticated alignment and temporal fusion technique in the feature space of the coarse HDR video to produce better reconstruction. To evaluate our method and others, we have created a benchmark dataset containing 97 sequences of static scenes and 184 testing pairs of dynamic scenes. Our experiments show that our method outperforms previous state-of-the-art methods, and we will make our dataset, code, and model publicly available.",1
"This paper presents a pure transformer-based approach, dubbed the Multi-Modal Video Transformer (MM-ViT), for video action recognition. Different from other schemes which solely utilize the decoded RGB frames, MM-ViT operates exclusively in the compressed video domain and exploits all readily available modalities, i.e., I-frames, motion vectors, residuals and audio waveform. In order to handle the large number of spatiotemporal tokens extracted from multiple modalities, we develop several scalable model variants which factorize self-attention across the space, time and modality dimensions. In addition, to further explore the rich inter-modal interactions and their effects, we develop and compare three distinct cross-modal attention mechanisms that can be seamlessly integrated into the transformer building block. Extensive experiments on three public action recognition benchmarks (UCF-101, Something-Something-v2, Kinetics-600) demonstrate that MM-ViT outperforms the state-of-the-art video transformers in both efficiency and accuracy, and performs better or equally well to the state-of-the-art CNN counterparts with computationally-heavy optical flow.",0
"The Multi-Modal Video Transformer (MM-ViT), a transformer-based method for video action recognition, is presented in this paper. Unlike other methods that only use decoded RGB frames, MM-ViT operates solely in the compressed video domain and utilizes all available modalities, including I-frames, motion vectors, residuals, and audio waveform. To handle the large number of spatiotemporal tokens from multiple modalities, scalable model variants are developed that factorize self-attention across space, time, and modality. Additionally, three cross-modal attention mechanisms are developed and compared to explore the rich inter-modal interactions and their effects. Extensive experiments on three public action recognition benchmarks demonstrate that MM-ViT outperforms state-of-the-art video transformers in both efficiency and accuracy, and performs as well as or better than state-of-the-art CNN counterparts with computationally-heavy optical flow.",1
"Warping-based video stabilizers smooth camera trajectory by constraining each pixel's displacement and warp stabilized frames from unstable ones accordingly. However, since the view outside the boundary is not available during warping, the resulting holes around the boundary of the stabilized frame must be discarded (i.e., cropping) to maintain visual consistency, and thus does leads to a tradeoff between stability and cropping ratio. In this paper, we make a first attempt to address this issue by proposing a new Out-of-boundary View Synthesis (OVS) method. By the nature of spatial coherence between adjacent frames and within each frame, OVS extrapolates the out-of-boundary view by aligning adjacent frames to each reference one. Technically, it first calculates the optical flow and propagates it to the outer boundary region according to the affinity, and then warps pixels accordingly. OVS can be integrated into existing warping-based stabilizers as a plug-and-play module to significantly improve the cropping ratio of the stabilized results. In addition, stability is improved because the jitter amplification effect caused by cropping and resizing is reduced. Experimental results on the NUS benchmark show that OVS can improve the performance of five representative state-of-the-art methods in terms of objective metrics and subjective visual quality. The code is publicly available at https://github.com/Annbless/OVS_Stabilization.",0
"Video stabilizers that rely on warping techniques are effective in smoothing camera trajectory by limiting each pixel's displacement and stabilizing frames that are unstable. However, one downside to this approach is that the view beyond the boundary is not available, resulting in holes around the stabilized frame's boundary that must be cropped. This tradeoff between stability and cropping ratio is addressed in this paper by introducing a new Out-of-boundary View Synthesis (OVS) method. OVS leverages spatial coherence between frames and within each frame to extrapolate the out-of-boundary view by aligning adjacent frames to a reference frame. First, OVS calculates optical flow and propagates it to the outer boundary region based on affinity. Then, it warps pixels accordingly. OVS can be easily integrated into existing warping-based stabilizers as a plug-and-play module, significantly improving the cropping ratio of the stabilized results. Additionally, OVS reduces the jitter amplification effect caused by cropping and resizing, resulting in improved stability. The NUS benchmark shows that OVS can enhance the performance of five state-of-the-art methods in terms of both objective metrics and subjective visual quality. The code for OVS is publicly available at https://github.com/Annbless/OVS_Stabilization.",1
"Learning RAW-to-sRGB mapping has drawn increasing attention in recent years, wherein an input raw image is trained to imitate the target sRGB image captured by another camera. However, the severe color inconsistency makes it very challenging to generate well-aligned training pairs of input raw and target sRGB images. While learning with inaccurately aligned supervision is prone to causing pixel shift and producing blurry results. In this paper, we circumvent such issue by presenting a joint learning model for image alignment and RAW-to-sRGB mapping. To diminish the effect of color inconsistency in image alignment, we introduce to use a global color mapping (GCM) module to generate an initial sRGB image given the input raw image, which can keep the spatial location of the pixels unchanged, and the target sRGB image is utilized to guide GCM for converting the color towards it. Then a pre-trained optical flow estimation network (e.g., PWC-Net) is deployed to warp the target sRGB image to align with the GCM output. To alleviate the effect of inaccurately aligned supervision, the warped target sRGB image is leveraged to learn RAW-to-sRGB mapping. When training is done, the GCM module and optical flow network can be detached, thereby bringing no extra computation cost for inference. Experiments show that our method performs favorably against state-of-the-arts on ZRR and SR-RAW datasets. With our joint learning model, a light-weight backbone can achieve better quantitative and qualitative performance on ZRR dataset. Codes are available at https://github.com/cszhilu1998/RAW-to-sRGB.",0
"In recent years, there has been a growing interest in learning how to map RAW-to-sRGB, which involves training an input raw image to resemble the target sRGB image captured by a different camera. However, due to the significant color inconsistency, it is challenging to generate well-aligned training pairs of input raw and target sRGB images. This can result in pixel shift and blurry outcomes when learning with inaccurately aligned supervision. To address this issue, we propose a joint learning model for image alignment and RAW-to-sRGB mapping. Our approach uses a global color mapping (GCM) module to create an initial sRGB image from the input raw image. The GCM module is guided by the target sRGB image to reduce color inconsistency during image alignment. We then use a pre-trained optical flow estimation network to warp the target sRGB image to align with the GCM output. This warped target sRGB image is then used to learn RAW-to-sRGB mapping. Our experiments show that our method outperforms state-of-the-art approaches on ZRR and SR-RAW datasets. Furthermore, our joint learning model allows a lightweight backbone to achieve superior quantitative and qualitative performance on the ZRR dataset. The code for our method is available at https://github.com/cszhilu1998/RAW-to-sRGB.",1
"Existing optical flow methods are erroneous in challenging scenes, such as fog, rain, and night because the basic optical flow assumptions such as brightness and gradient constancy are broken. To address this problem, we present an unsupervised learning approach that fuses gyroscope into optical flow learning. Specifically, we first convert gyroscope readings into motion fields named gyro field. Second, we design a self-guided fusion module to fuse the background motion extracted from the gyro field with the optical flow and guide the network to focus on motion details. To the best of our knowledge, this is the first deep learning-based framework that fuses gyroscope data and image content for optical flow learning. To validate our method, we propose a new dataset that covers regular and challenging scenes. Experiments show that our method outperforms the state-of-art methods in both regular and challenging scenes. Code and dataset are available at https://github.com/megvii-research/GyroFlow.",0
"In difficult environmental conditions like fog, rain, and darkness, current optical flow techniques are flawed due to the violation of basic assumptions such as brightness and gradient constancy. As a solution, we introduce an unsupervised learning method that integrates gyroscopes into optical flow learning. Our approach involves converting gyroscope data into motion fields named gyro fields and merging background motion extracted from the gyro field with optical flow via a self-guided fusion module to emphasize motion details. This is the first deep learning-based framework that incorporates gyro data and image content for optical flow learning. We have also developed a new dataset that covers a range of scene types to validate our approach. Our experiments demonstrate that our approach surpasses existing methods in both regular and challenging scenes. Code and dataset are accessible at https://github.com/megvii-research/GyroFlow.",1
"We propose a novel framework for video inpainting by adopting an internal learning strategy. Unlike previous methods that use optical flow for cross-frame context propagation to inpaint unknown regions, we show that this can be achieved implicitly by fitting a convolutional neural network to known regions. Moreover, to handle challenging sequences with ambiguous backgrounds or long-term occlusion, we design two regularization terms to preserve high-frequency details and long-term temporal consistency. Extensive experiments on the DAVIS dataset demonstrate that the proposed method achieves state-of-the-art inpainting quality quantitatively and qualitatively. We further extend the proposed method to another challenging task: learning to remove an object from a video giving a single object mask in only one frame in a 4K video.",0
"Our proposition presents a new framework for video inpainting that involves an internal learning approach. Our method differs from previous techniques that utilize optical flow to propagate context across frames in order to fill in unknown areas. Instead, we demonstrate that this can be accomplished implicitly by training a convolutional neural network on known regions. Additionally, we have developed two regularization terms to address problematic sequences that involve ambiguous backgrounds or long-term occlusion. These regularization terms ensure the preservation of high-frequency details and long-term temporal consistency. Our experiments on the DAVIS dataset show that our proposed method achieves superior quantitative and qualitative inpainting quality. We have also extended our approach to address the challenge of removing an object from a 4K video with only one object mask provided in a single frame.",1
"The existing particle image velocimetry (PIV) do not consider the curvature effect of the non-straight particle trajectory, because it seems to be impossible to obtain the curvature information from a pair of particle images. As a result, the computed vector underestimates the real velocity due to the straight-line approximation, that further causes a systematic error for the PIV instrument. In this work, the particle curved trajectory between two recordings is firstly explained with the streamline segment of a steady flow (diffeomorphic transformation) instead of a single vector, and this idea is termed as diffeomorphic PIV. Specifically, a deformation field is introduced to describe the particle displacement, i.e., we try to find the optimal velocity field, of which the corresponding deformation vector field agrees with the particle displacement. Because the variation of the deformation function can be approximated with the variation of the velocity function, the diffeomorphic PIV can be implemented as iterative PIV. That says, the diffeomorphic PIV warps the images with deformation vector field instead of the velocity, and keeps the rest as same as iterative PIVs. Two diffeomorphic deformation schemes -- forward diffeomorphic deformation interrogation (FDDI) and central diffeomorphic deformation interrogation (CDDI) -- are proposed. Tested on synthetic images, the FDDI achieves significant accuracy improvement across different one-pass displacement estimators (cross-correlation, optical flow, deep learning flow). Besides, the results on three real PIV image pairs demonstrate the non-negligible curvature effect for CDI-based PIV, and our FDDI provides larger velocity estimation (more accurate) in the fast curvy streamline areas. The accuracy improvement of the combination of FDDI and accurate dense estimator means that our diffeomorphic PIV paves a new way for complex flow measurement.",0
"The current particle image velocimetry (PIV) methods do not take into account the curvature effect of non-straight particle trajectories. This is because it is difficult to obtain curvature information from a pair of particle images. Consequently, the computed velocity vector is underestimated due to the straight-line approximation, leading to systematic errors in the PIV instrument. In this study, we propose a new approach called diffeomorphic PIV, which explains the curved trajectory between two recordings using the streamline segment of a steady flow. This is achieved by introducing a deformation field to describe particle displacement, with the optimal velocity field being found such that the corresponding deformation vector field matches the particle displacement. Diffeomorphic PIV is implemented as an iterative PIV, with image warping done using the deformation vector field instead of velocity. Two diffeomorphic deformation schemes, forward diffeomorphic deformation interrogation (FDDI) and central diffeomorphic deformation interrogation (CDDI), are proposed. FDDI achieves significant accuracy improvement across different one-pass displacement estimators, as demonstrated through testing on synthetic and real PIV image pairs. The results show that curvature effect is non-negligible for CDI-based PIV, and our FDDI provides more accurate velocity estimation in fast curvy streamline areas. The combination of FDDI and accurate dense estimator paves a new way for complex flow measurement.",1
"We propose a novel framework for finding correspondences in images based on a deep neural network that, given two images and a query point in one of them, finds its correspondence in the other. By doing so, one has the option to query only the points of interest and retrieve sparse correspondences, or to query all points in an image and obtain dense mappings. Importantly, in order to capture both local and global priors, and to let our model relate between image regions using the most relevant among said priors, we realize our network using a transformer. At inference time, we apply our correspondence network by recursively zooming in around the estimates, yielding a multiscale pipeline able to provide highly-accurate correspondences. Our method significantly outperforms the state of the art on both sparse and dense correspondence problems on multiple datasets and tasks, ranging from wide-baseline stereo to optical flow, without any retraining for a specific dataset. We commit to releasing data, code, and all the tools necessary to train from scratch and ensure reproducibility.",0
"Our innovative approach proposes a framework for discovering correspondences in images that utilizes a deep neural network. The network is designed to locate the correspondence of a query point in one image to the corresponding point in the other image. This allows for the retrieval of sparse correspondences when only points of interest are queried, or dense mappings when all points in an image are queried. To capture both local and global priors and enable the model to relate between image regions using the most relevant priors, we utilize a transformer. During inference, our correspondence network is applied by recursively zooming in around the estimates, creating a multiscale pipeline that delivers highly accurate correspondences. Our method surpasses the state of the art in both sparse and dense correspondence problems across various datasets and tasks, such as wide-baseline stereo or optical flow, without requiring retraining for a particular dataset. We pledge to provide data, code, and all necessary tools for training from scratch and ensuring reproducibility.",1
"In this paper, we propose $\text{HF}^2$-VAD, a Hybrid framework that integrates Flow reconstruction and Frame prediction seamlessly to handle Video Anomaly Detection. Firstly, we design the network of ML-MemAE-SC (Multi-Level Memory modules in an Autoencoder with Skip Connections) to memorize normal patterns for optical flow reconstruction so that abnormal events can be sensitively identified with larger flow reconstruction errors. More importantly, conditioned on the reconstructed flows, we then employ a Conditional Variational Autoencoder (CVAE), which captures the high correlation between video frame and optical flow, to predict the next frame given several previous frames. By CVAE, the quality of flow reconstruction essentially influences that of frame prediction. Therefore, poorly reconstructed optical flows of abnormal events further deteriorate the quality of the final predicted future frame, making the anomalies more detectable. Experimental results demonstrate the effectiveness of the proposed method. Code is available at \href{https://github.com/LiUzHiAn/hf2vad}{https://github.com/LiUzHiAn/hf2vad}.",0
"The paper introduces $\text{HF}^2$-VAD, a Hybrid approach for Video Anomaly Detection that seamlessly integrates Flow reconstruction and Frame prediction. The proposed method utilizes ML-MemAE-SC, a network with Multi-Level Memory modules in an Autoencoder with Skip Connections, to memorize normal patterns for optical flow reconstruction, enabling the identification of abnormal events with larger flow reconstruction errors. Additionally, a Conditional Variational Autoencoder (CVAE) is employed to predict the next frame based on several previous frames, conditioned on the reconstructed flows. The quality of flow reconstruction crucially affects that of frame prediction, making anomalies more detectable by deteriorating the quality of the predicted future frame. Experimental results demonstrate the effectiveness of the proposed method, and the code is available at \href{https://github.com/LiUzHiAn/hf2vad}{https://github.com/LiUzHiAn/hf2vad}.",1
"Event camera is an emerging imaging sensor for capturing dynamics of moving objects as events, which motivates our work in estimating 3D human pose and shape from the event signals. Events, on the other hand, have their unique challenges: rather than capturing static body postures, the event signals are best at capturing local motions. This leads us to propose a two-stage deep learning approach, called EventHPE. The first-stage, FlowNet, is trained by unsupervised learning to infer optical flow from events. Both events and optical flow are closely related to human body dynamics, which are fed as input to the ShapeNet in the second stage, to estimate 3D human shapes. To mitigate the discrepancy between image-based flow (optical flow) and shape-based flow (vertices movement of human body shape), a novel flow coherence loss is introduced by exploiting the fact that both flows are originated from the identical human motion. An in-house event-based 3D human dataset is curated that comes with 3D pose and shape annotations, which is by far the largest one to our knowledge. Empirical evaluations on DHP19 dataset and our in-house dataset demonstrate the effectiveness of our approach.",0
"Our research focuses on using the event camera, a novel imaging sensor, to capture the movements of objects and estimate the 3D pose and shape of humans. However, events present unique challenges as they are best suited for capturing local motions rather than static body postures. To address this, we propose a two-stage deep learning approach called EventHPE. The first stage, FlowNet, uses unsupervised learning to infer optical flow from events, which are both related to human body dynamics. The second stage, ShapeNet, estimates 3D human shapes using both events and optical flow as inputs. To overcome the discrepancy between image-based flow and shape-based flow, we introduce a novel flow coherence loss that exploits the fact that both flows originate from the same human motion. We curated an in-house event-based 3D human dataset that is currently the largest of its kind and evaluated our approach on both the DHP19 and our in-house dataset, demonstrating its effectiveness.",1
"Weakly-Supervised Temporal Action Localization (WSTAL) aims to localize actions in untrimmed videos with only video-level labels. Currently, most state-of-the-art WSTAL methods follow a Multi-Instance Learning (MIL) pipeline: producing snippet-level predictions first and then aggregating to the video-level prediction. However, we argue that existing methods have overlooked two important drawbacks: 1) inadequate use of motion information and 2) the incompatibility of prevailing cross-entropy training loss. In this paper, we analyze that the motion cues behind the optical flow features are complementary informative. Inspired by this, we propose to build a context-dependent motion prior, termed as motionness. Specifically, a motion graph is introduced to model motionness based on the local motion carrier (e.g., optical flow). In addition, to highlight more informative video snippets, a motion-guided loss is proposed to modulate the network training conditioned on motionness scores. Extensive ablation studies confirm that motionness efficaciously models action-of-interest, and the motion-guided loss leads to more accurate results. Besides, our motion-guided loss is a plug-and-play loss function and is applicable with existing WSTAL methods. Without loss of generality, based on the standard MIL pipeline, our method achieves new state-of-the-art performance on three challenging benchmarks, including THUMOS'14, ActivityNet v1.2 and v1.3.",0
"The objective of Weakly-Supervised Temporal Action Localization (WSTAL) is to identify actions in untrimmed videos using video-level labels only. The current state-of-the-art WSTAL techniques follow a Multi-Instance Learning (MIL) approach, where they initially make predictions at the snippet-level and then combine them to make a prediction at the video-level. However, this approach has neglected two crucial concerns: 1) insufficient utilization of motion information and 2) the incompatibility of the prevalent cross-entropy training loss. This research examines that the motion cues present in the optical flow features are highly complementary and informative. Therefore, a context-dependent motion prior, referred to as motionness, was created by using a motion graph to model motionness based on the local motion carrier (i.e., optical flow). Furthermore, a motion-guided loss was proposed to train the network based on the motionness scores and to highlight the most informative video snippets. A thorough analysis demonstrates that motionness models the action-of-interest effectively, and the motion-guided loss yields more precise results. Additionally, the motion-guided loss can be utilized with existing WSTAL methods as it is a plug-and-play loss function. Our method, based on the standard MIL pipeline, achieves state-of-the-art performance on three challenging benchmarks, including THUMOS'14, ActivityNet v1.2 and v1.3.",1
"We propose RIFE, a Real-time Intermediate Flow Estimation algorithm for Video Frame Interpolation (VFI). Many recent flow-based VFI methods first estimate the bi-directional optical flows, then scale and reverse them to approximate intermediate flows, leading to artifacts on motion boundaries. RIFE uses a neural network named IFNet that can directly estimate the intermediate flows from coarse-to-fine with much better speed. We design a privileged distillation scheme for training intermediate flow model, which leads to a large performance improvement. Experiments demonstrate that RIFE is flexible and can achieve state-of-the-art performance on several public benchmarks. The code is available at \url{https://github.com/hzwer/arXiv2020-RIFE}",0
"Our proposal is RIFE, an algorithm for Real-time Intermediate Flow Estimation for Video Frame Interpolation (VFI). Current flow-based VFI methods estimate bi-directional optical flows, then approximate intermediate flows by scaling and reversing them, which results in artifacts at motion boundaries. RIFE uses IFNet, a neural network that directly estimates intermediate flows from coarse-to-fine, providing better speed. To train the intermediate flow model, we employ a privileged distillation scheme, which results in a significant improvement in performance. Our experiments show that RIFE is adaptable and can achieve state-of-the-art performance on multiple public benchmarks. The code is available at \url{https://github.com/hzwer/arXiv2020-RIFE}.",1
"Animals have evolved highly functional visual systems to understand motion, assisting perception even under complex environments. In this paper, we work towards developing a computer vision system able to segment objects by exploiting motion cues, i.e. motion segmentation. We make the following contributions: First, we introduce a simple variant of the Transformer to segment optical flow frames into primary objects and the background. Second, we train the architecture in a self-supervised manner, i.e. without using any manual annotations. Third, we analyze several critical components of our method and conduct thorough ablation studies to validate their necessity. Fourth, we evaluate the proposed architecture on public benchmarks (DAVIS2016, SegTrackv2, and FBMS59). Despite using only optical flow as input, our approach achieves superior or comparable results to previous state-of-the-art self-supervised methods, while being an order of magnitude faster. We additionally evaluate on a challenging camouflage dataset (MoCA), significantly outperforming the other self-supervised approaches, and comparing favourably to the top supervised approach, highlighting the importance of motion cues, and the potential bias towards visual appearance in existing video segmentation models.",0
"The visual systems of animals have evolved to understand motion, enabling them to perceive even in complex environments. This study aims to develop a computer vision system that can segment objects by utilizing motion cues, specifically motion segmentation. The study presents several contributions: firstly, a simple variant of the Transformer is introduced to segment optical flow frames into primary objects and the background. Secondly, the architecture is trained in a self-supervised manner without manual annotations. Thirdly, critical components of the method are analyzed, and thorough ablation studies are conducted to confirm their necessity. Finally, the proposed architecture is evaluated on various public benchmarks, including DAVIS2016, SegTrackv2, and FBMS59. Despite using only optical flow as input, the approach achieves superior or comparable results to previous state-of-the-art self-supervised methods and is an order of magnitude faster. The study also evaluates the proposed architecture on a challenging camouflage dataset (MoCA), outperforming other self-supervised approaches and comparing favorably to the top supervised approach, highlighting the importance of motion cues and the potential bias towards visual appearance in existing video segmentation models.",1
"Location and appearance are the key cues for video object segmentation. Many sources such as RGB, depth, optical flow and static saliency can provide useful information about the objects. However, existing approaches only utilize the RGB or RGB and optical flow. In this paper, we propose a novel multi-source fusion network for zero-shot video object segmentation. With the help of interoceptive spatial attention module (ISAM), spatial importance of each source is highlighted. Furthermore, we design a feature purification module (FPM) to filter the inter-source incompatible features. By the ISAM and FPM, the multi-source features are effectively fused. In addition, we put forward an automatic predictor selection network (APS) to select the better prediction of either the static saliency predictor or the moving object predictor in order to prevent over-reliance on the failed results caused by low-quality optical flow maps. Extensive experiments on three challenging public benchmarks (i.e. DAVIS$_{16}$, Youtube-Objects and FBMS) show that the proposed model achieves compelling performance against the state-of-the-arts. The source code will be publicly available at \textcolor{red}{\url{https://github.com/Xiaoqi-Zhao-DLUT/Multi-Source-APS-ZVOS}}.",0
"The primary indicators for video object segmentation are location and appearance. Multiple sources, including RGB, depth, optical flow, and static saliency, can provide valuable insights into objects. However, existing methods only utilize RGB or RGB and optical flow. This study introduces a novel multi-source fusion network for zero-shot video object segmentation that highlights the spatial importance of each source using an interoceptive spatial attention module (ISAM). Additionally, a feature purification module (FPM) is designed to filter inter-source incompatible features. The multi-source features are effectively fused using ISAM and FPM. Furthermore, an automatic predictor selection network (APS) selects the better of the static saliency predictor or the moving object predictor to avoid over-reliance on low-quality optical flow maps that result in failed outcomes. Extensive experiments on three challenging public benchmarks, including DAVIS$_{16}$, Youtube-Objects, and FBMS, demonstrate that the proposed model outperforms existing state-of-the-art methods. The source code will be available for public access at \textcolor{red}{\url{https://github.com/Xiaoqi-Zhao-DLUT/Multi-Source-APS-ZVOS}}.",1
"Self-supervised deep learning-based 3D scene understanding methods can overcome the difficulty of acquiring the densely labeled ground-truth and have made a lot of advances. However, occlusions and moving objects are still some of the major limitations. In this paper, we explore the learnable occlusion aware optical flow guided self-supervised depth and camera pose estimation by an adaptive cross weighted loss to address the above limitations. Firstly, we explore to train the learnable occlusion mask fused optical flow network by an occlusion-aware photometric loss with the temporally supplemental information and backward-forward consistency of adjacent views. And then, we design an adaptive cross-weighted loss between the depth-pose and optical flow loss of the geometric and photometric error to distinguish the moving objects which violate the static scene assumption. Our method shows promising results on KITTI, Make3D, and Cityscapes datasets under multiple tasks. We also show good generalization ability under a variety of challenging scenarios.",0
"Advanced 3D scene understanding methods based on self-supervised deep learning have made significant progress in overcoming the challenge of obtaining densely labeled ground-truth. However, occlusions and moving objects still present major limitations to these methods. This study explores the use of a learnable occlusion-aware optical flow guided self-supervised depth and camera pose estimation method, which utilizes an adaptive cross-weighted loss to address these limitations. The approach involves training a learnable occlusion mask fused optical flow network using an occlusion-aware photometric loss with temporal supplemental information and backward-forward consistency of adjacent views. An adaptive cross-weighted loss is then designed to distinguish moving objects that violate the static scene assumption between the depth-pose and optical flow loss of the geometric and photometric error. Our method demonstrates promising results on various datasets including KITTI, Make3D, and Cityscapes under multiple tasks, and showcases good generalization ability across various challenging scenarios.",1
"This paper strives for action recognition and detection in video modalities like RGB, depth maps or 3D-skeleton sequences when only limited modality-specific labeled examples are available. For the RGB, and derived optical-flow, modality many large-scale labeled datasets have been made available. They have become the de facto pre-training choice when recognizing or detecting new actions from RGB datasets that have limited amounts of labeled examples available. Unfortunately, large-scale labeled action datasets for other modalities are unavailable for pre-training. In this paper, our goal is to recognize actions from limited examples in non-RGB video modalities, by learning from large-scale labeled RGB data. To this end, we propose a two-step training process: (i) we extract action representation knowledge from an RGB-trained teacher network and adapt it to a non-RGB student network. (ii) we then fine-tune the transfer model with available labeled examples of the target modality. For the knowledge transfer we introduce feature-supervision strategies, which rely on unlabeled pairs of two modalities (the RGB and the target modality) to transfer feature level representations from the teacher to the student network. Ablations and generalizations with two RGB source datasets and two non-RGB target datasets demonstrate that an optical-flow teacher provides better action transfer features than RGB for both depth maps and 3D-skeletons, even when evaluated on a different target domain, or for a different task. Compared to alternative cross-modal action transfer methods we show a good improvement in performance especially when labeled non-RGB examples to learn from are scarce",0
"The main objective of this paper is to achieve action recognition and detection in different video modalities such as RGB, depth maps, and 3D-skeleton sequences, even when the availability of modality-specific labeled examples is limited. Although there are many large-scale labeled datasets available for RGB and derived optical-flow modalities, other modalities lack such datasets for pre-training. Therefore, the paper proposes a two-step training process to recognize actions from limited examples in non-RGB video modalities by learning from a large-scale labeled RGB dataset. Firstly, the paper extracts knowledge regarding action representation from an RGB-trained teacher network and then adapts it to a non-RGB student network. Secondly, the transfer model is fine-tuned with available labeled examples of the target modality. To transfer knowledge, the paper introduces feature-supervision strategies based on unlabeled pairs of two modalities to transfer feature level representations from the teacher to the student network. The paper shows that an optical-flow teacher provides better action transfer features than RGB for both depth maps and 3D-skeletons, even when evaluated on a different target domain or for a different task. Compared to other cross-modal action transfer methods, the paper demonstrates a good improvement in performance, particularly when labeled non-RGB examples to learn from are scarce.",1
"Estimating the states of surrounding traffic participants stays at the core of autonomous driving. In this paper, we study a novel setting of this problem: model-free single-object tracking (SOT), which takes the object state in the first frame as input, and jointly solves state estimation and tracking in subsequent frames. The main purpose for this new setting is to break the strong limitation of the popular ""detection and tracking"" scheme in multi-object tracking. Moreover, we notice that shape completion by overlaying the point clouds, which is a by-product of our proposed task, not only improves the performance of state estimation but also has numerous applications. As no benchmark for this task is available so far, we construct a new dataset LiDAR-SOT and corresponding evaluation protocols based on the Waymo Open dataset. We then propose an optimization-based algorithm called SOTracker involving point cloud registration, vehicle shapes, correspondence, and motion priors. Our quantitative and qualitative results prove the effectiveness of our SOTracker and reveal the challenging cases for SOT in point clouds, including the sparsity of LiDAR data, abrupt motion variation, etc. Finally, we also explore how the proposed task and algorithm may benefit other autonomous driving applications, including simulating LiDAR scans, generating motion data, and annotating optical flow. The code and protocols for our benchmark and algorithm are available at https://github.com/TuSimple/LiDAR_SOT/. A video demonstration is at https://www.youtube.com/watch?v=BpHixKs91i8.",0
"The primary focus of autonomous driving is to accurately estimate the states of traffic participants in the surrounding area. This paper examines a new approach to this problem, which involves single-object tracking (SOT) without the use of models. This approach takes the object state in the first frame as input and solves state estimation and tracking in subsequent frames. The aim of this new method is to overcome the limitations of the popular ""detection and tracking"" scheme in multi-object tracking. Additionally, we observe that overlaying point clouds can improve the performance of state estimation and have multiple applications. As there is no benchmark for this task, we construct a new dataset called LiDAR-SOT and corresponding evaluation protocols based on the Waymo Open dataset. We propose an optimization-based algorithm called SOTracker that involves point cloud registration, vehicle shapes, correspondence, and motion priors. Our results show that SOTracker is effective, but SOT in point clouds is challenging due to the sparsity of LiDAR data and abrupt motion variation. Finally, we explore how this task and algorithm can benefit other autonomous driving applications, such as simulating LiDAR scans, generating motion data, and annotating optical flow. The code and protocols for our benchmark and algorithm are available at https://github.com/TuSimple/LiDAR_SOT/. A video demonstration can be viewed at https://www.youtube.com/watch?v=BpHixKs91i8.",1
"In this paper, we firstly present a dataset (X4K1000FPS) of 4K videos of 1000 fps with the extreme motion to the research community for video frame interpolation (VFI), and propose an extreme VFI network, called XVFI-Net, that first handles the VFI for 4K videos with large motion. The XVFI-Net is based on a recursive multi-scale shared structure that consists of two cascaded modules for bidirectional optical flow learning between two input frames (BiOF-I) and for bidirectional optical flow learning from target to input frames (BiOF-T). The optical flows are stably approximated by a complementary flow reversal (CFR) proposed in BiOF-T module. During inference, the BiOF-I module can start at any scale of input while the BiOF-T module only operates at the original input scale so that the inference can be accelerated while maintaining highly accurate VFI performance. Extensive experimental results show that our XVFI-Net can successfully capture the essential information of objects with extremely large motions and complex textures while the state-of-the-art methods exhibit poor performance. Furthermore, our XVFI-Net framework also performs comparably on the previous lower resolution benchmark dataset, which shows a robustness of our algorithm as well. All source codes, pre-trained models, and proposed X4K1000FPS datasets are publicly available at https://github.com/JihyongOh/XVFI.",0
"This paper introduces the X4K1000FPS dataset comprising 4K videos of 1000 fps with extreme motion for video frame interpolation (VFI). We also propose a new VFI network, XVFI-Net, which is specifically designed to handle VFI for 4K videos with large motion. The XVFI-Net is based on a recursive multi-scale shared structure consisting of two cascaded modules for bidirectional optical flow learning. The BiOF-I module can start at any scale of input, while the BiOF-T module only operates at the original input scale to accelerate inference while maintaining high VFI accuracy. Our XVFI-Net successfully captures the essential information of objects with extremely large motions and complex textures while outperforming state-of-the-art methods. We also demonstrate the robustness of our algorithm on a previous lower resolution benchmark dataset. All source codes, pre-trained models, and the proposed X4K1000FPS datasets are publicly available at https://github.com/JihyongOh/XVFI.",1
"Modern methods for counting people in crowded scenes rely on deep networks to estimate people densities in individual images. As such, only very few take advantage of temporal consistency in video sequences, and those that do only impose weak smoothness constraints across consecutive frames. In this paper, we advocate estimating people flows across image locations between consecutive images and inferring the people densities from these flows instead of directly regressing them. This enables us to impose much stronger constraints encoding the conservation of the number of people. As a result, it significantly boosts performance without requiring a more complex architecture. Furthermore, it allows us to exploit the correlation between people flow and optical flow to further improve the results. We also show that leveraging people conservation constraints in both a spatial and temporal manner makes it possible to train a deep crowd counting model in an active learning setting with much fewer annotations. This significantly reduces the annotation cost while still leading to similar performance to the full supervision case.",0
"Counting people in crowded scenes nowadays involves the use of deep networks to determine people densities in individual images. However, only a few methods utilize temporal consistency in video sequences, and those that do only impose weak smoothness constraints across consecutive frames. In this study, we propose estimating people flows across image locations between consecutive images and inferring the people densities from these flows instead of directly regressing them. This approach allows us to impose much stronger constraints that ensure the conservation of the number of people, resulting in a significant improvement in performance without the need for a more complex architecture. Additionally, we can leverage the correlation between people flow and optical flow to further enhance the results. Our approach can also reduce the annotation cost by training a deep crowd counting model in an active learning setting with fewer annotations, while still achieving comparable performance to the full supervision case.",1
"The recently-proposed Perceiver model obtains good results on several domains (images, audio, multimodal, point clouds) while scaling linearly in compute and memory with the input size. While the Perceiver supports many kinds of inputs, it can only produce very simple outputs such as class scores. Perceiver IO overcomes this limitation without sacrificing the original's appealing properties by learning to flexibly query the model's latent space to produce outputs of arbitrary size and semantics. Perceiver IO still decouples model depth from data size and still scales linearly with data size, but now with respect to both input and output sizes. The full Perceiver IO model achieves strong results on tasks with highly structured output spaces, such as natural language and visual understanding, StarCraft II, and multi-task and multi-modal domains. As highlights, Perceiver IO matches a Transformer-based BERT baseline on the GLUE language benchmark without the need for input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation.",0
"The Perceiver model, which has been proposed recently, has shown excellent results across different domains, including images, audio, multimodal, and point clouds. It has the advantage of scaling linearly in compute and memory with the input size, but it can only produce simple outputs like class scores. However, Perceiver IO has been developed to overcome this limitation and can produce outputs of any size and semantics by flexibly querying the model's latent space. Perceiver IO still maintains the original model's desirable properties, such as decoupling model depth from data size and scaling linearly with data size. The full Perceiver IO model has achieved impressive results on tasks with highly structured output spaces, such as natural language and visual understanding, StarCraft II, and multi-task and multi-modal domains. Notably, Perceiver IO has matched a Transformer-based BERT baseline on the GLUE language benchmark without the need for input tokenization and has achieved state-of-the-art performance on Sintel optical flow estimation.",1
"In Computer Vision,object tracking is a very old and complex problem.Though there are several existing algorithms for object tracking, still there are several challenges remain to be solved. For instance, variation of illumination of light, noise, occlusion, sudden start and stop of moving object, shading etc,make the object tracking a complex problem not only for dynamic background but also for static background. In this paper we propose a dual approach for object tracking based on optical flow and swarm Intelligence.The optical flow based KLT(Kanade-Lucas-Tomasi) tracker, tracks the dominant points of the target object from first frame to last frame of a video sequence;whereas swarm Intelligence based PSO (Particle Swarm Optimization) tracker simultaneously tracks the boundary information of the target object from second frame to last frame of the same video sequence.This dual function of tracking makes the trackers very much robust with respect to the above stated problems. The flexibility of our approach is that it can be successfully applicable in variable background as well as static background.We compare the performance of the proposed dual tracking algorithm with several benchmark datasets and obtain very competitive results in general and in most of the cases we obtained superior results using dual tracking algorithm. We also compare the performance of the proposed dual tracker with some existing PSO based algorithms for tracking and achieved better results.",0
"The problem of object tracking in Computer Vision is a complex and long-standing issue, despite the existence of various algorithms. There are several challenges that have yet to be overcome, such as changes in lighting, noise, occlusion, sudden movements, and shading. These factors make object tracking difficult not only for dynamic but also for static backgrounds. To address this, our paper presents a dual approach to object tracking based on optical flow and swarm intelligence. The optical flow-based KLT tracker tracks the main points of the target object from the first to the last frame of the video sequence. Meanwhile, the swarm intelligence-based PSO tracker simultaneously tracks the boundary information of the target object from the second frame to the last frame of the same video sequence. This dual function of tracking makes our trackers more robust to the aforementioned issues. Our approach is flexible and can be applied successfully in variable and static backgrounds. We compared the performance of our proposed dual tracking algorithm with various benchmark datasets and achieved highly competitive results, with superior performance in most cases. Moreover, we compared the performance of our proposed dual tracker with some existing PSO-based algorithms and achieved better results.",1
"Automatic facial action unit (AU) recognition is a challenging task due to the scarcity of manual annotations. To alleviate this problem, a large amount of efforts has been dedicated to exploiting various methods which leverage numerous unlabeled data. However, many aspects with regard to some unique properties of AUs, such as the regional and relational characteristics, are not sufficiently explored in previous works. Motivated by this, we take the AU properties into consideration and propose two auxiliary AU related tasks to bridge the gap between limited annotations and the model performance in a self-supervised manner via the unlabeled data. Specifically, to enhance the discrimination of regional features with AU relation embedding, we design a task of RoI inpainting to recover the randomly cropped AU patches. Meanwhile, a single image based optical flow estimation task is proposed to leverage the dynamic change of facial muscles and encode the motion information into the global feature representation. Based on these two self-supervised auxiliary tasks, local features, mutual relation and motion cues of AUs are better captured in the backbone network with the proposed regional and temporal based auxiliary task learning (RTATL) framework. Extensive experiments on BP4D and DISFA demonstrate the superiority of our method and new state-of-the-art performances are achieved.",0
"Recognizing automatic facial action units (AU) is difficult due to the limited availability of manual annotations. Researchers have tried to overcome this obstacle by using various methods to exploit unlabeled data. However, previous works have not adequately explored the regional and relational characteristics of AUs, leading to suboptimal performance. To address this, we propose two auxiliary tasks that consider AU properties and use self-supervised learning to improve model performance using unlabeled data. The first task is RoI inpainting, which recovers randomly cropped AU patches to enhance the discrimination of regional features with AU relation embedding. The second task is single image-based optical flow estimation, which captures the dynamic change of facial muscles and encodes the motion information into the global feature representation. Our proposed framework, the Regional and Temporal-based Auxiliary Task Learning (RTATL) framework, better captures local features, mutual relations, and motion cues of AUs in the backbone network. Our experiments on BP4D and DISFA datasets demonstrate the superiority of our method, achieving new state-of-the-art performances.",1
"Occlusions pose a significant challenge to optical flow algorithms that rely on local evidences. We consider an occluded point to be one that is imaged in the first frame but not in the next, a slight overloading of the standard definition since it also includes points that move out-of-frame. Estimating the motion of these points is extremely difficult, particularly in the two-frame setting. Previous work relies on CNNs to learn occlusions, without much success, or requires multiple frames to reason about occlusions using temporal smoothness. In this paper, we argue that the occlusion problem can be better solved in the two-frame case by modelling image self-similarities. We introduce a global motion aggregation module, a transformer-based approach to find long-range dependencies between pixels in the first image, and perform global aggregation on the corresponding motion features. We demonstrate that the optical flow estimates in the occluded regions can be significantly improved without damaging the performance in non-occluded regions. This approach obtains new state-of-the-art results on the challenging Sintel dataset, improving the average end-point error by 13.6% on Sintel Final and 13.7% on Sintel Clean. At the time of submission, our method ranks first on these benchmarks among all published and unpublished approaches. Code is available at https://github.com/zacjiang/GMA",0
"Optical flow algorithms that depend on local evidences face a major challenge with occlusions. An occluded point is defined as one that appears in the first frame but not in the next, including points that move out-of-frame. Estimating motion for these points is difficult, especially in the two-frame setting. Previous approaches have relied on CNNs to learn occlusions, with limited success, or require multiple frames to reason about occlusions through temporal smoothness. However, this paper proposes that image self-similarities can be used to solve the occlusion problem better in the two-frame case. The proposed method involves introducing a global motion aggregation module, which uses a transformer-based approach to identify long-range dependencies between pixels in the first image and perform global aggregation on corresponding motion features. This approach significantly improves optical flow estimates in occluded regions without compromising non-occluded regions. The proposed method achieves state-of-the-art results on the challenging Sintel dataset, improving the average end-point error by 13.6% on Sintel Final and 13.7% on Sintel Clean. Code for the proposed method is available at https://github.com/zacjiang/GMA.",1
"While single image shadow detection has been improving rapidly in recent years, video shadow detection remains a challenging task due to data scarcity and the difficulty in modelling temporal consistency. The current video shadow detection method achieves this goal via co-attention, which mostly exploits information that is temporally coherent but is not robust in detecting moving shadows and small shadow regions. In this paper, we propose a simple but powerful method to better aggregate information temporally. We use an optical flow based warping module to align and then combine features between frames. We apply this warping module across multiple deep-network layers to retrieve information from neighboring frames including both local details and high-level semantic information. We train and test our framework on the ViSha dataset. Experimental results show that our model outperforms the state-of-the-art video shadow detection method by 28%, reducing BER from 16.7 to 12.0.",0
"Despite the rapid improvement in single image shadow detection, video shadow detection remains a difficult task due to a shortage of data and the challenge of modeling temporal consistency. The current approach to video shadow detection relies on co-attention, which predominantly utilizes temporally coherent information but is not effective in detecting moving shadows or small shadow regions. This paper proposes a straightforward yet effective method of aggregating information temporally by using an optical flow-based warping module to align and combine features between frames. The warping module is applied across multiple deep-network layers to retrieve information from neighboring frames, including local details and high-level semantic information. The proposed framework is trained and tested on the ViSha dataset, and experimental results demonstrate that it outperforms the state-of-the-art video shadow detection method by 28%, reducing BER from 16.7 to 12.0.",1
