"We propose a light-weight video frame interpolation algorithm. Our key innovation is an instance-level supervision that allows information to be learned from the high-resolution version of similar objects. Our experiment shows that the proposed method can generate state-of-the-art results across different datasets, with fractional computation resources (time and memory) of competing methods. Given two image frames, a cascade network creates an intermediate frame with 1) a flow-warping module that computes coarse bi-directional optical flow and creates an interpolated image via flow-based warping, followed by 2) an image synthesis module to make fine-scale corrections. In the learning stage, object detection proposals are generated on the interpolated image.Lower resolution objects are zoomed into, and the learning algorithms using an adversarial loss trained on high-resolution objects to guide the system towards the instance-level refinement corrects details of object shape and boundaries.",0
"Our proposed video frame interpolation algorithm is lightweight and features an instance-level supervision method that allows for learning from high-resolution versions of similar objects. Through experimentation, we have demonstrated that our approach achieves state-of-the-art results across multiple datasets while utilizing only a fraction of the computational resources required by competing methods. Our cascade network generates an intermediate frame by first employing a flow-warping module to compute bi-directional optical flow and create an interpolated image through flow-based warping. This is followed by an image synthesis module that makes fine-scale corrections. During the learning stage, object detection proposals are produced on the interpolated image. The learning algorithms employ an adversarial loss and train on high-resolution objects to guide the system towards refining object details and boundaries at the instance level, with lower resolution objects being zoomed into for this purpose.",1
"This paper presents a novel approach for segmenting moving objects in unconstrained environments using guided convolutional neural networks. This guiding process relies on foreground masks from independent algorithms (i.e. state-of-the-art algorithms) to implement an attention mechanism that incorporates the spatial location of foreground and background to compute their separated representations. Our approach initially extracts two kinds of features for each frame using colour and optical flow information. Such features are combined following a multiplicative scheme to benefit from their complementarity. These unified colour and motion features are later processed to obtain the separated foreground and background representations. Then, both independent representations are concatenated and decoded to perform foreground segmentation. Experiments conducted on the challenging DAVIS 2016 dataset demonstrate that our guided representations not only outperform non-guided, but also recent and top-performing video object segmentation algorithms.",0
"A new technique for segmenting moving objects in unconstrained environments is presented in this paper, which utilizes guided convolutional neural networks. This approach involves incorporating the spatial location of foreground and background through an attention mechanism that uses foreground masks generated by independent, state-of-the-art algorithms. The method first extracts two types of features from each frame, namely color and optical flow information, which are combined using a multiplicative scheme to take advantage of their complementary nature. These unified features are then processed to obtain separate representations of the foreground and background. The independent representations are subsequently concatenated and decoded to perform foreground segmentation. Results from experiments conducted on the challenging DAVIS 2016 dataset demonstrate that our guided approach outperforms non-guided as well as recent and top-performing video object segmentation algorithms.",1
"The problem of Scene flow estimation in depth videos has been attracting attention of researchers of robot vision, due to its potential application in various areas of robotics. The conventional scene flow methods are difficult to use in reallife applications due to their long computational overhead. We propose a conditional adversarial network SceneFlowGAN for scene flow estimation. The proposed SceneFlowGAN uses loss function at two ends: both generator and descriptor ends. The proposed network is the first attempt to estimate scene flow using generative adversarial networks, and is able to estimate both the optical flow and disparity from the input stereo images simultaneously. The proposed method is experimented on a large RGB-D benchmark sceneflow dataset.",0
"Researchers in robot vision have been focused on the issue of Scene flow estimation in depth videos, as it has potential applications in various fields of robotics. However, traditional scene flow methods have limited practical use due to their lengthy computational processing time. In response, our team has developed a solution called SceneFlowGAN, which utilizes a conditional adversarial network. The SceneFlowGAN includes loss functions at both the generator and descriptor ends, making it the first generative adversarial network to estimate scene flow. This innovative approach is capable of estimating both optical flow and disparity from input stereo images simultaneously. We tested our proposed method on a comprehensive RGB-D benchmark sceneflow dataset.",1
"We present an implementation of a new approach to diffeomorphic non-rigid registration of medical images. The method is based on optical flow and warps images via gradient flow with the standard $L^2$ inner product. To compute the transformation, we rely on accelerated optimisation on the manifold of diffeomorphisms. We achieve regularity properties of Sobolev gradient flows, which are expensive to compute, owing to a novel method of averaging the gradients in time rather than space. We successfully register brain MRI and challenging abdominal CT scans at speeds orders of magnitude faster than previous approaches. We make our code available in a public repository: https://github.com/dgrzech/fastreg",0
"Our study proposes a novel approach to non-rigid registration of medical images using diffeomorphisms. The technique utilizes optical flow to warp images through gradient flow with the standard $L^2$ inner product. Optimal transformation is computed through accelerated optimization on the manifold of diffeomorphisms. We attain Sobolev gradient flow's regularity properties, which are typically costly to compute, by averaging gradients in time instead of space. We successfully registered brain MRI and complex abdominal CT scans, achieving speeds many times faster than previous methods. Our code is available in a public repository at https://github.com/dgrzech/fastreg.",1
"Ghosting artifacts caused by moving objects or misalignments is a key challenge in high dynamic range (HDR) imaging for dynamic scenes. Previous methods first register the input low dynamic range (LDR) images using optical flow before merging them, which are error-prone and cause ghosts in results. A very recent work tries to bypass optical flows via a deep network with skip-connections, however, which still suffers from ghosting artifacts for severe movement. To avoid the ghosting from the source, we propose a novel attention-guided end-to-end deep neural network (AHDRNet) to produce high-quality ghost-free HDR images. Unlike previous methods directly stacking the LDR images or features for merging, we use attention modules to guide the merging according to the reference image. The attention modules automatically suppress undesired components caused by misalignments and saturation and enhance desirable fine details in the non-reference images. In addition to the attention model, we use dilated residual dense block (DRDB) to make full use of the hierarchical features and increase the receptive field for hallucinating the missing details. The proposed AHDRNet is a non-flow-based method, which can also avoid the artifacts generated by optical-flow estimation error. Experiments on different datasets show that the proposed AHDRNet can achieve state-of-the-art quantitative and qualitative results.",0
"A major obstacle in dynamic high dynamic range (HDR) imaging is the appearance of ghosting artifacts caused by either moving objects or misalignments. Prior methods attempted to address this issue by registering the input low dynamic range (LDR) images using optical flow before merging them. However, this approach is prone to errors and often results in ghosting. More recently, a deep network with skip-connections was introduced to bypass the use of optical flows. Nevertheless, this method remains susceptible to ghosting artifacts in the presence of severe movement. To counteract ghosting at the source, we propose a novel attention-guided end-to-end deep neural network (AHDRNet) that produces high-quality HDR images that are free of ghosting. Rather than directly stacking LDR images or features for merging, we use attention modules to guide the merging process based on the reference image. These attention modules automatically suppress undesired components resulting from misalignments and saturation while enhancing desirable fine details in the non-reference images. Additionally, we employ dilated residual dense blocks (DRDB) to make full use of hierarchical features and increase the receptive field to fill in missing details. The proposed AHDRNet is a non-flow-based approach that also avoids artifacts caused by optical-flow estimation errors. Our experiments on various datasets demonstrate that the proposed AHDRNet achieves state-of-the-art quantitative and qualitative results.",1
"We present a self-supervised learning approach for optical flow. Our method distills reliable flow estimations from non-occluded pixels, and uses these predictions as ground truth to learn optical flow for hallucinated occlusions. We further design a simple CNN to utilize temporal information from multiple frames for better flow estimation. These two principles lead to an approach that yields the best performance for unsupervised optical flow learning on the challenging benchmarks including MPI Sintel, KITTI 2012 and 2015. More notably, our self-supervised pre-trained model provides an excellent initialization for supervised fine-tuning. Our fine-tuned models achieve state-of-the-art results on all three datasets. At the time of writing, we achieve EPE=4.26 on the Sintel benchmark, outperforming all submitted methods.",0
"Our optical flow self-supervised learning approach employs two key principles. Firstly, it extracts dependable flow estimations from non-occluded pixels to develop ground truth for learning optical flow in hallucinated occlusions. Secondly, we introduce a basic CNN that utilizes temporal information from multiple frames to enhance flow estimation. Our approach offers the best performance for unsupervised optical flow learning on MPI Sintel, KITTI 2012, and 2015 challenging benchmarks. Furthermore, our self-supervised pre-trained model serves as a superior initialization for supervised fine-tuning, resulting in state-of-the-art performance on all three datasets. Currently, we outperform all submitted methods with an EPE=4.26 on the Sintel benchmark.",1
"We present a self-supervised approach to estimate flow in camera image and top-view grid map sequences using fully convolutional neural networks in the domain of automated driving. We extend existing approaches for self-supervised optical flow estimation by adding a regularizer expressing motion consistency assuming a static environment. However, as this assumption is violated for other moving traffic participants we also estimate a mask to scale this regularization. Adding a regularization towards motion consistency improves convergence and flow estimation accuracy. Furthermore, we scale the errors due to spatial flow inconsistency by a mask that we derive from the motion mask. This improves accuracy in regions where the flow drastically changes due to a better separation between static and dynamic environment. We apply our approach to optical flow estimation from camera image sequences, validate on odometry estimation and suggest a method to iteratively increase optical flow estimation accuracy using the generated motion masks. Finally, we provide quantitative and qualitative results based on the KITTI odometry and tracking benchmark for scene flow estimation based on grid map sequences. We show that we can improve accuracy and convergence when applying motion and spatial consistency regularization.",0
"Our study presents a self-supervised method for estimating flow in camera image and top-view grid map sequences for automated driving using fully convolutional neural networks. To improve upon existing self-supervised optical flow estimation approaches, we incorporate a regularizer that assumes motion consistency in a static environment. However, since this assumption is not applicable for other moving traffic participants, we estimate a mask to adjust this regularization. The addition of a regularization towards motion consistency enhances convergence and flow estimation accuracy. Additionally, we utilize a mask derived from the motion mask to scale errors caused by spatial flow inconsistency, which further improves accuracy in regions where the flow changes rapidly due to a better differentiation between static and dynamic environments. Our method is applied to optical flow estimation from camera image sequences, and its effectiveness is validated through odometry estimation. Moreover, we suggest a technique to enhance optical flow estimation accuracy iteratively using the generated motion masks. Finally, based on the KITTI odometry and tracking benchmark for scene flow estimation based on grid map sequences, we present quantitative and qualitative results that demonstrate the benefits of applying motion and spatial consistency regularization in improving accuracy and convergence.",1
"Explicit representations of the global match distributions of pixel-wise correspondences between pairs of images are desirable for uncertainty estimation and downstream applications. However, the computation of the match density for each pixel may be prohibitively expensive due to the large number of candidates. In this paper, we propose Hierarchical Discrete Distribution Decomposition (HD^3), a framework suitable for learning probabilistic pixel correspondences in both optical flow and stereo matching. We decompose the full match density into multiple scales hierarchically, and estimate the local matching distributions at each scale conditioned on the matching and warping at coarser scales. The local distributions can then be composed together to form the global match density. Despite its simplicity, our probabilistic method achieves state-of-the-art results for both optical flow and stereo matching on established benchmarks. We also find the estimated uncertainty is a good indication of the reliability of the predicted correspondences.",0
"To estimate uncertainty and facilitate downstream applications, it is advantageous to have explicit representations of the global match distributions of pixel-wise correspondences between image pairs. However, calculating the match density for each pixel can be prohibitively expensive due to the numerous candidates involved. This paper introduces the Hierarchical Discrete Distribution Decomposition (HD^3) framework, which is suitable for learning probabilistic pixel correspondences in optical flow and stereo matching. The approach involves hierarchically breaking down the full match density into multiple scales and estimating the local matching distributions at each scale based on matching and warping at coarser scales. The local distributions can then be combined to form the global match density. Despite its simplicity, our probabilistic method is highly effective for both optical flow and stereo matching on established benchmarks. Moreover, we observe that the estimated uncertainty is a reliable indicator of the predicted correspondences' accuracy.",1
"Event cameras are novel vision sensors that output pixel-level brightness changes (""events"") instead of traditional video frames. These asynchronous sensors offer several advantages over traditional cameras, such as, high temporal resolution, very high dynamic range, and no motion blur. To unlock the potential of such sensors, motion compensation methods have been recently proposed. We present a collection and taxonomy of twenty two objective functions to analyze event alignment in motion compensation approaches (Fig. 1). We call them Focus Loss Functions since they have strong connections with functions used in traditional shape-from-focus applications. The proposed loss functions allow bringing mature computer vision tools to the realm of event cameras. We compare the accuracy and runtime performance of all loss functions on a publicly available dataset, and conclude that the variance, the gradient and the Laplacian magnitudes are among the best loss functions. The applicability of the loss functions is shown on multiple tasks: rotational motion, depth and optical flow estimation. The proposed focus loss functions allow to unlock the outstanding properties of event cameras.",0
"Instead of traditional video frames, event cameras output pixel-level brightness changes known as ""events"". These asynchronous sensors have advantages over traditional cameras including high temporal resolution, high dynamic range, and no motion blur. Recent motion compensation methods have been proposed to take advantage of these benefits. In this study, we present a collection of twenty-two objective functions, named Focus Loss Functions, to analyze event alignment in motion compensation approaches. These loss functions have strong connections with traditional shape-from-focus applications and allow for the use of mature computer vision tools with event cameras. Our comparison of the accuracy and runtime performance of all loss functions on a publicly available dataset shows that the variance, gradient, and Laplacian magnitudes are among the best. The proposed focus loss functions are applicable to multiple tasks, including rotational motion, depth, and optical flow estimation. By using these focus loss functions, we can unlock the full potential of event cameras.",1
"Crowd gatherings at social and cultural events are increasing in leaps and bounds with the increase in population. Surveillance through computer vision and expert decision making systems can help to understand the crowd phenomena at large gatherings. Understanding crowd phenomena can be helpful in early identification of unwanted incidents and their prevention. Motion flow is one of the important crowd phenomena that can be instrumental in describing the crowd behavior. Flows can be useful in understanding instabilities in the crowd. However, extracting motion flows is a challenging task due to randomness in crowd movement and limitations of the sensing device. Moreover, low-level features such as optical flow can be misleading if the randomness is high. In this paper, we propose a new model based on Langevin equation to analyze the linear dominant flows in videos of densely crowded scenarios. We assume a force model with three components, namely external force, confinement/drift force, and disturbance force. These forces are found to be sufficient to describe the linear or near-linear motion in dense crowd videos. The method is significantly faster as compared to existing popular crowd segmentation methods. The evaluation of the proposed model has been carried out on publicly available datasets as well as using our dataset. It has been observed that the proposed method is able to estimate and segment the linear flows in the dense crowd with better accuracy as compared to state-of-the-art techniques with substantial decrease in the computational overhead.",0
"With the rise in population, crowd gatherings at social and cultural events have seen a significant increase. To gain a better understanding of the crowd behavior at such events, computer vision and expert decision-making systems are being employed for surveillance. This can aid in identifying and preventing unwanted incidents. Motion flow is a crucial aspect of crowd behavior, and can help in identifying crowd instabilities. However, due to the unpredictable nature of crowd movement and limitations of sensing devices, extracting motion flows can be challenging. Optical flow, a low-level feature, can be misleading in highly random crowds. In this paper, we propose a new model based on Langevin equation to analyze linear dominant flows in videos of densely crowded scenarios. Our force model with external, confinement/drift, and disturbance forces can describe linear or near-linear motion in dense crowd videos. Our proposed method is faster than existing crowd segmentation techniques and has been evaluated on publicly available datasets, showing better accuracy with a substantial decrease in computational overhead.",1
"Video object removal is a challenging task in video processing that often requires massive human efforts. Given the mask of the foreground object in each frame, the goal is to complete (inpaint) the object region and generate a video without the target object. While recently deep learning based methods have achieved great success on the image inpainting task, they often lead to inconsistent results between frames when applied to videos. In this work, we propose a novel learning-based Video Object Removal Network (VORNet) to solve the video object removal task in a spatio-temporally consistent manner, by combining the optical flow warping and image-based inpainting model. Experiments are done on our Synthesized Video Object Removal (SVOR) dataset based on the YouTube-VOS video segmentation dataset, and both the objective and subjective evaluation demonstrate that our VORNet generates more spatially and temporally consistent videos compared with existing methods.",0
"Removing objects from videos is a difficult task that typically requires a lot of human effort. The aim is to create a video without the target object by completing the object region using a mask of the foreground object in each frame. Although deep learning-based methods have had great success with image inpainting, inconsistency between frames can occur when applied to videos. This study proposes a Video Object Removal Network (VORNet) that uses optical flow warping and image-based inpainting to solve the video object removal task consistently in space and time. The Synthesized Video Object Removal (SVOR) dataset based on the YouTube-VOS video segmentation dataset was used for experiments, and the results show that VORNet generates more consistent videos both objectively and subjectively compared to existing methods.",1
"We propose an adversarial contextual model for detecting moving objects in images. A deep neural network is trained to predict the optical flow in a region using information from everywhere else but that region (context), while another network attempts to make such context as uninformative as possible. The result is a model where hypotheses naturally compete with no need for explicit regularization or hyper-parameter tuning. Although our method requires no supervision whatsoever, it outperforms several methods that are pre-trained on large annotated datasets. Our model can be thought of as a generalization of classical variational generative region-based segmentation, but in a way that avoids explicit regularization or solution of partial differential equations at run-time.",0
"Our proposed approach for identifying moving objects in images involves an adversarial contextual model. The model uses a deep neural network to forecast the optical flow in a specific region by drawing on information from all other areas except that region (context). Simultaneously, another network endeavors to render the context as uninformative as possible. This process yields a model where hypotheses naturally compete, without the need for explicit regularization or hyper-parameter tuning. Despite being unsupervised, our technique outperforms several methods that are pre-trained on large annotated datasets. Our model is akin to classical variational generative region-based segmentation, but without explicit regularization or solving partial differential equations during run-time.",1
"In the last few years, convolutional neural networks (CNNs) have demonstrated increasing success at learning many computer vision tasks including dense estimation problems such as optical flow and stereo matching. However, the joint prediction of these tasks, called scene flow, has traditionally been tackled using slow classical methods based on primitive assumptions which fail to generalize. The work presented in this paper overcomes these drawbacks efficiently (in terms of speed and accuracy) by proposing PWOC-3D, a compact CNN architecture to predict scene flow from stereo image sequences in an end-to-end supervised setting. Further, large motion and occlusions are well-known problems in scene flow estimation. PWOC-3D employs specialized design decisions to explicitly model these challenges. In this regard, we propose a novel self-supervised strategy to predict occlusions from images (learned without any labeled occlusion data). Leveraging several such constructs, our network achieves competitive results on the KITTI benchmark and the challenging FlyingThings3D dataset. Especially on KITTI, PWOC-3D achieves the second place among end-to-end deep learning methods with 48 times fewer parameters than the top-performing method.",0
"Over the past few years, CNNs have demonstrated significant progress in computer vision tasks, specifically those related to dense estimation, like optical flow and stereo matching. However, when it comes to joint prediction of these tasks, known as scene flow, traditional methods based on primitive assumptions fail to generalize, and are slow. To overcome these challenges, this paper proposes PWOC-3D, a compact CNN architecture for scene flow prediction from stereo image sequences in an end-to-end supervised setting. The network also addresses the problems of large motion and occlusions typical of scene flow estimation through specialized design decisions. The paper also introduces a novel self-supervised strategy for predicting occlusions from images without any labeled occlusion data. The network achieves competitive results on the KITTI benchmark and FlyingThings3D dataset, ranking second in end-to-end deep learning methods for KITTI with 48 times fewer parameters than the top-performing method.",1
"This paper digs deeper into factors that influence egocentric gaze. Instead of training deep models for this purpose in a blind manner, we propose to inspect factors that contribute to gaze guidance during daily tasks. Bottom-up saliency and optical flow are assessed versus strong spatial prior baselines. Task-specific cues such as vanishing point, manipulation point, and hand regions are analyzed as representatives of top-down information. We also look into the contribution of these factors by investigating a simple recurrent neural model for ego-centric gaze prediction. First, deep features are extracted for all input video frames. Then, a gated recurrent unit is employed to integrate information over time and to predict the next fixation. We also propose an integrated model that combines the recurrent model with several top-down and bottom-up cues. Extensive experiments over multiple datasets reveal that (1) spatial biases are strong in egocentric videos, (2) bottom-up saliency models perform poorly in predicting gaze and underperform spatial biases, (3) deep features perform better compared to traditional features, (4) as opposed to hand regions, the manipulation point is a strong influential cue for gaze prediction, (5) combining the proposed recurrent model with bottom-up cues, vanishing points and, in particular, manipulation point results in the best gaze prediction accuracy over egocentric videos, (6) the knowledge transfer works best for cases where the tasks or sequences are similar, and (7) task and activity recognition can benefit from gaze prediction. Our findings suggest that (1) there should be more emphasis on hand-object interaction and (2) the egocentric vision community should consider larger datasets including diverse stimuli and more subjects.",0
"The objective of this paper is to investigate the factors that affect egocentric gaze. Instead of blindly training deep models, the study proposes an examination of the factors that contribute to gaze guidance during daily activities. The study evaluates bottom-up saliency and optical flow against strong spatial prior baselines and analyzes task-specific cues such as vanishing point, manipulation point, and hand regions as representatives of top-down information. The study also explores the contribution of these factors by using a simple recurrent neural model for ego-centric gaze prediction. Deep features are extracted and a gated recurrent unit is used to integrate information over time and predict the next fixation. The study proposes an integrated model that combines the recurrent model with several top-down and bottom-up cues. Extensive experiments over multiple datasets reveal that spatial biases are strong in egocentric videos, bottom-up saliency models perform poorly, and deep features perform better compared to traditional features. The manipulation point is a strong influential cue for gaze prediction, and combining the proposed recurrent model with bottom-up cues and vanishing points results in the best gaze prediction accuracy over egocentric videos. The study also suggests that there should be more emphasis on hand-object interaction and the egocentric vision community should consider larger datasets including diverse stimuli and more subjects. Additionally, the study concludes that task and activity recognition can benefit from gaze prediction.",1
"Effective spatiotemporal feature representation is crucial to the video-based action recognition task. Focusing on discriminate spatiotemporal feature learning, we propose Information Fused Temporal Transformation Network (IF-TTN) for action recognition on top of popular Temporal Segment Network (TSN) framework. In the network, Information Fusion Module (IFM) is designed to fuse the appearance and motion features at multiple ConvNet levels for each video snippet, forming a short-term video descriptor. With fused features as inputs, Temporal Transformation Networks (TTN) are employed to model middle-term temporal transformation between the neighboring snippets following a sequential order. As TSN itself depicts long-term temporal structure by segmental consensus, the proposed network comprehensively considers multiple granularity temporal features. Our IF-TTN achieves the state-of-the-art results on two most popular action recognition datasets: UCF101 and HMDB51. Empirical investigation reveals that our architecture is robust to the input motion map quality. Replacing optical flow with the motion vectors from compressed video stream, the performance is still comparable to the flow-based methods while the testing speed is 10x faster.",0
"The task of recognizing actions in videos requires an effective representation of spatiotemporal features. Our approach, called the Information Fused Temporal Transformation Network (IF-TTN), focuses on discriminative spatiotemporal feature learning and builds upon the Temporal Segment Network (TSN) framework. In the IF-TTN, we use an Information Fusion Module (IFM) to combine appearance and motion features at multiple levels of the ConvNet for each video snippet, creating a short-term video descriptor. We then apply Temporal Transformation Networks (TTN) to model middle-term temporal transformation between neighboring snippets in sequential order, while the TSN framework captures long-term temporal structure through segmental consensus. By considering multiple granularity temporal features, our IF-TTN achieves state-of-the-art results on the UCF101 and HMDB51 action recognition datasets. Additionally, our architecture is robust to input motion map quality and can achieve comparable performance to flow-based methods while being 10x faster by replacing optical flow with motion vectors from compressed video streams.",1
"Deep learning approaches to optical flow estimation have seen rapid progress over the recent years. One common trait of many networks is that they refine an initial flow estimate either through multiple stages or across the levels of a coarse-to-fine representation. While leading to more accurate results, the downside of this is an increased number of parameters. Taking inspiration from both classical energy minimization approaches as well as residual networks, we propose an iterative residual refinement (IRR) scheme based on weight sharing that can be combined with several backbone networks. It reduces the number of parameters, improves the accuracy, or even achieves both. Moreover, we show that integrating occlusion prediction and bi-directional flow estimation into our IRR scheme can further boost the accuracy. Our full network achieves state-of-the-art results for both optical flow and occlusion estimation across several standard datasets.",0
"Over the years, there has been a rapid advancement in deep learning methods for optical flow estimation. One common characteristic of many networks is that they refine an initial flow estimate either across different levels of a coarse-to-fine representation or through multiple stages. Although this leads to more precise results, it also results in a higher number of parameters. To address this issue, we have developed an iterative residual refinement (IRR) scheme that is based on weight sharing and draws inspiration from classical energy minimization approaches and residual networks. This scheme can be combined with various backbone networks and reduces the number of parameters while enhancing accuracy or achieving both. Additionally, we demonstrate that integrating occlusion prediction and bi-directional flow estimation into our IRR scheme can further improve accuracy. Our complete network outperforms others for both optical flow and occlusion estimation across multiple standard datasets.",1
"Recent advances in 3D human shape estimation build upon parametric representations that model very well the shape of the naked body, but are not appropriate to represent the clothing geometry. In this paper, we present an approach to model dressed humans and predict their geometry from single images. We contribute in three fundamental aspects of the problem, namely, a new dataset, a novel shape parameterization algorithm and an end-to-end deep generative network for predicting shape.   First, we present 3DPeople, a large-scale synthetic dataset with 2.5 Million photo-realistic images of 80 subjects performing 70 activities and wearing diverse outfits. Besides providing textured 3D meshes for clothes and body, we annotate the dataset with segmentation masks, skeletons, depth, normal maps and optical flow. All this together makes 3DPeople suitable for a plethora of tasks.   We then represent the 3D shapes using 2D geometry images. To build these images we propose a novel spherical area-preserving parameterization algorithm based on the optimal mass transportation method. We show this approach to improve existing spherical maps which tend to shrink the elongated parts of the full body models such as the arms and legs, making the geometry images incomplete.   Finally, we design a multi-resolution deep generative network that, given an input image of a dressed human, predicts his/her geometry image (and thus the clothed body shape) in an end-to-end manner. We obtain very promising results in jointly capturing body pose and clothing shape, both for synthetic validation and on the wild images.",0
"Recent advancements in 3D human shape estimation have been relying on parametric models that effectively represent the shape of the bare body, but are not suitable for depicting clothing geometry. In this article, we present an approach to model clothed humans and anticipate their geometry from a single image. We contribute to three fundamental aspects of the issue, namely, a new dataset, an innovative shape parameterization algorithm, and a deep generative network that works end-to-end for predicting shape. Initially, we introduce 3DPeople, a large-scale synthetic dataset comprising 2.5 million photorealistic images of 80 subjects wearing various outfits and performing 70 activities. We annotate the dataset with segmentation masks, skeletons, depth, normal maps, and optical flow, alongside providing textured 3D meshes for clothing and body, making it useful for multiple tasks. We then use 2D geometry images to represent the 3D shapes. To accomplish this, we propose a novel spherical area-preserving parameterization algorithm based on the optimal mass transportation method, which improves current spherical maps that tend to shrink elongated areas of full body models like arms and legs, rendering the geometry images incomplete. Finally, we design a multi-resolution deep generative network that predicts the geometry image (and thus the clothed body shape) of a dressed human from an input image in an end-to-end manner. We achieve promising results in capturing body pose and clothing shape for synthetic validation and wild images.",1
"We address the problem of recovering the 3D geometry of a human face from a set of facial images in multiple views. While recent studies have shown impressive progress in 3D Morphable Model (3DMM) based facial reconstruction, the settings are mostly restricted to a single view. There is an inherent drawback in the single-view setting: the lack of reliable 3D constraints can cause unresolvable ambiguities. We in this paper explore 3DMM-based shape recovery in a different setting, where a set of multi-view facial images are given as input. A novel approach is proposed to regress 3DMM parameters from multi-view inputs with an end-to-end trainable Convolutional Neural Network (CNN). Multiview geometric constraints are incorporated into the network by establishing dense correspondences between different views leveraging a novel self-supervised view alignment loss. The main ingredient of the view alignment loss is a differentiable dense optical flow estimator that can backpropagate the alignment errors between an input view and a synthetic rendering from another input view, which is projected to the target view through the 3D shape to be inferred. Through minimizing the view alignment loss, better 3D shapes can be recovered such that the synthetic projections from one view to another can better align with the observed image. Extensive experiments demonstrate the superiority of the proposed method over other 3DMM methods.",0
"This paper focuses on the challenge of reconstructing a human face's 3D geometry from various facial images captured from multiple viewpoints. Although studies have made impressive strides in facial reconstruction using 3D Morphable Model (3DMM) techniques, they have been limited to a single perspective, which presents significant limitations. The lack of dependable 3D constraints in the single-view setting can lead to impractical ambiguities. Therefore, we propose a different approach for 3DMM-based shape recovery, where multiple facial images are utilized as input. We developed a Convolutional Neural Network (CNN) that can learn to regress 3DMM parameters while incorporating multi-view geometric constraints. Our approach establishes dense correspondences between the different views, and we use a novel self-supervised view alignment loss to incorporate them into the network. This loss function uses a differentiable dense optical flow estimator to backpropagate alignment errors between the input and the synthetic rendering from another view. By minimizing the view alignment loss, our approach generates improved 3D shapes that better align with the observed images. Our experiments demonstrate that our method outperforms other 3DMM methods.",1
"Unsupervised deep learning for optical flow computation has achieved promising results. Most existing deep-net based methods rely on image brightness consistency and local smoothness constraint to train the networks. Their performance degrades at regions where repetitive textures or occlusions occur. In this paper, we propose Deep Epipolar Flow, an unsupervised optical flow method which incorporates global geometric constraints into network learning. In particular, we investigate multiple ways of enforcing the epipolar constraint in flow estimation. To alleviate a ``chicken-and-egg'' type of problem encountered in dynamic scenes where multiple motions may be present, we propose a low-rank constraint as well as a union-of-subspaces constraint for training. Experimental results on various benchmarking datasets show that our method achieves competitive performance compared with supervised methods and outperforms state-of-the-art unsupervised deep-learning methods.",0
"Encouraging outcomes have been achieved with unsupervised deep learning in the computation of optical flow. However, current deep-net based approaches rely on image brightness consistency and local smoothness constraint to train the networks, which results in degraded performance in areas with repetitive textures or occlusions. This study introduces Deep Epipolar Flow, an unsupervised optical flow approach that involves global geometric constraints during network learning. The paper explores multiple ways of enforcing the epipolar constraint in flow estimation. To address the challenge encountered in dynamic scenes with multiple motions, a low-rank constraint and a union-of-subspaces constraint are proposed for training. The experimental results from various benchmarking datasets demonstrate that our method achieves competitive performance compared to supervised methods and surpasses state-of-the-art unsupervised deep-learning methods.",1
"Dense pixel matching is important for many computer vision tasks such as disparity and flow estimation. We present a robust, unified descriptor network that considers a large context region with high spatial variance. Our network has a very large receptive field and avoids striding layers to maintain spatial resolution. These properties are achieved by creating a novel neural network layer that consists of multiple, parallel, stacked dilated convolutions (SDC). Several of these layers are combined to form our SDC descriptor network. In our experiments, we show that our SDC features outperform state-of-the-art feature descriptors in terms of accuracy and robustness. In addition, we demonstrate the superior performance of SDC in state-of-the-art stereo matching, optical flow and scene flow algorithms on several famous public benchmarks.",0
"For various computer vision tasks like flow estimation and disparity, it is crucial to have dense pixel matching. We have developed a descriptor network that is robust and unified, taking into account a large context region with high spatial variance. Our network boasts a large receptive field and avoids striding layers to retain spatial resolution. We have accomplished this by creating a novel neural network layer that consists of multiple, parallel, stacked dilated convolutions (SDC), which are combined to form our SDC descriptor network. Our experiments have shown that SDC features outperform state-of-the-art feature descriptors in terms of accuracy and robustness. Additionally, we have demonstrated SDC's superior performance in state-of-the-art stereo matching, optical flow, and scene flow algorithms on several renowned public benchmarks.",1
"Recently, deep image compression has shown a big progress in terms of coding efficiency and image quality improvement. However, relatively less attention has been put on video compression using deep learning networks. In the paper, we first propose a deep learning based bi-predictive coding network, called BP-DVC Net, for video compression. Learned from the lesson of the conventional video coding, a B-frame coding structure is incorporated in our BP-DVC Net. While the bi-predictive coding in the conventional video codecs requires to transmit to decoder sides the motion vectors for block motion and the residues from prediction, our BP-DVC Net incorporates optical flow estimation networks in both encoder and decoder sides so as not to transmit the motion information to the decoder sides for coding efficiency improvement. Also, a bi-prediction network in the BP-DVC Net is proposed and used to precisely predict the current frame and to yield the resulting residues as small as possible. Furthermore, our BP-DVC Net allows for the compressive feature maps to be entropy-coded using the temporal context among the feature maps of adjacent frames. The BP-DVC Net has an end-to-end video compression architecture with newly designed flow and prediction losses. Experimental results show that the compression performance of our proposed method is comparable to those of H.264, HEVC in terms of PSNR and MS-SSIM.",0
"Significant advancements have been made in deep image compression in terms of coding efficiency and image quality enhancement. However, not enough attention has been given to video compression using deep learning networks. The BP-DVC Net is a deep learning-based bi-predictive coding network proposed in this paper for video compression. Incorporating a B-frame coding structure based on conventional video coding, the BP-DVC Net utilizes optical flow estimation networks in both encoder and decoder sides to enhance coding efficiency by eliminating the need to transmit motion information to the decoder sides. The proposed bi-prediction network accurately predicts the current frame and reduces the resulting residues. Additionally, the BP-DVC Net allows for the compressive feature maps to be entropy-coded using the temporal context among the feature maps of adjacent frames. With newly designed flow and prediction losses, the BP-DVC Net has an end-to-end video compression architecture. Experimental results indicate that our proposed method's compression performance is comparable to that of H.264 and HEVC in terms of PSNR and MS-SSIM.",1
"Unsupervised learning for geometric perception (depth, optical flow, etc.) is of great interest to autonomous systems. Recent works on unsupervised learning have made considerable progress on perceiving geometry; however, they usually ignore the coherence of objects and perform poorly under scenarios with dark and noisy environments. In contrast, supervised learning algorithms, which are robust, require large labeled geometric dataset. This paper introduces SIGNet, a novel framework that provides robust geometry perception without requiring geometrically informative labels. Specifically, SIGNet integrates semantic information to make depth and flow predictions consistent with objects and robust to low lighting conditions. SIGNet is shown to improve upon the state-of-the-art unsupervised learning for depth prediction by 30% (in squared relative error). In particular, SIGNet improves the dynamic object class performance by 39% in depth prediction and 29% in flow prediction. Our code will be made available at https://github.com/mengyuest/SIGNet",0
"Autonomous systems are keenly interested in unsupervised learning for geometric perception, including depth and optical flow. Although recent studies have made significant progress in perceiving geometry through unsupervised learning, they tend to overlook object coherence and perform poorly in low light and noisy environments. In contrast, supervised learning algorithms require a large labeled geometric dataset, but they are robust. This paper introduces SIGNet, a new framework that offers robust geometry perception without the need for geometrically informative labels. SIGNet integrates semantic information to ensure that depth and flow predictions are consistent with objects and can withstand low lighting conditions. SIGNet outperforms the state-of-the-art unsupervised learning for depth prediction by 30% (in squared relative error). Moreover, SIGNet improves the dynamic object class performance by 39% in depth prediction and 29% in flow prediction. Our code will be available at https://github.com/mengyuest/SIGNet.",1
"In semantic video segmentation the goal is to acquire consistent dense semantic labelling across image frames. To this end, recent approaches have been reliant on manually arranged operations applied on top of static semantic segmentation networks - with the most prominent building block being the optical flow able to provide information about scene dynamics. Related to that is the line of research concerned with speeding up static networks by approximating expensive parts of them with cheaper alternatives, while propagating information from previous frames. In this work we attempt to come up with generalisation of those methods, and instead of manually designing contextual blocks that connect per-frame outputs, we propose a neural architecture search solution, where the choice of operations together with their sequential arrangement are being predicted by a separate neural network. We showcase that such generalisation leads to stable and accurate results across common benchmarks, such as CityScapes and CamVid datasets. Importantly, the proposed methodology takes only 2 GPU-days, finds high-performing cells and does not rely on the expensive optical flow computation.",0
"The objective of semantic video segmentation is to obtain consistent semantic labeling that is dense across image frames. Recently, methods have relied on manually arranged operations that are applied on top of static semantic segmentation networks. The most prominent method used is the optical flow, which provides information about the dynamics of the scene. Another line of research aims to speed up static networks by approximating expensive parts with cheaper alternatives while propagating information from previous frames. Our work aims to generalize these methods, proposing a neural architecture search solution. Rather than manually designing contextual blocks that connect per-frame outputs, a separate neural network predicts the choice and sequential arrangement of operations. We demonstrate that this generalization leads to stable and accurate results on common benchmarks such as CityScapes and CamVid datasets. The proposed methodology takes only 2 GPU-days, finds high-performing cells, and does not rely on expensive optical flow computation.",1
"Owing to the development and advancement of artificial intelligence, numerous works were established in the human facial expression recognition system. Meanwhile, the detection and classification of micro-expressions are attracting attentions from various research communities in the recent few years. In this paper, we first review the processes of a conventional optical-flow-based recognition system, which comprised of facial landmarks annotations, optical flow guided images computation, features extraction and emotion class categorization. Secondly, a few approaches have been proposed to improve the feature extraction part, such as exploiting GAN to generate more image samples. Particularly, several variations of optical flow are computed in order to generate optimal images to lead to high recognition accuracy. Next, GAN, a combination of Generator and Discriminator, is utilized to generate new ""fake"" images to increase the sample size. Thirdly, a modified state-of-the-art Convolutional neural networks is proposed. To verify the effectiveness of the the proposed method, the results are evaluated on spontaneous micro-expression databases, namely SMIC, CASME II and SAMM. Both the F1-score and accuracy performance metrics are reported in this paper.",0
"The growing advancements in artificial intelligence have led to the development of several facial expression recognition systems for humans. Recently, there has been a surge of interest in the detection and classification of micro-expressions among various research communities. This article presents a review of the conventional optical-flow-based recognition system, which involves facial landmarks annotations, optical flow guided images computation, features extraction, and emotion class categorization. Various techniques have been proposed to enhance the feature extraction process, such as using GAN to create more image samples and computing different variations of optical flow to generate optimal images for higher recognition accuracy. Additionally, a modified Convolutional neural network has been proposed to further improve the system's performance. The proposed methodology is evaluated on spontaneous micro-expression databases, namely SMIC, CASME II, and SAMM, and the results are reported in terms of F1-score and accuracy performance metrics.",1
"We introduce multigrid Predictive Filter Flow (mgPFF), a framework for unsupervised learning on videos. The mgPFF takes as input a pair of frames and outputs per-pixel filters to warp one frame to the other. Compared to optical flow used for warping frames, mgPFF is more powerful in modeling sub-pixel movement and dealing with corruption (e.g., motion blur). We develop a multigrid coarse-to-fine modeling strategy that avoids the requirement of learning large filters to capture large displacement. This allows us to train an extremely compact model (4.6MB) which operates in a progressive way over multiple resolutions with shared weights. We train mgPFF on unsupervised, free-form videos and show that mgPFF is able to not only estimate long-range flow for frame reconstruction and detect video shot transitions, but also readily amendable for video object segmentation and pose tracking, where it substantially outperforms the published state-of-the-art without bells and whistles. Moreover, owing to mgPFF's nature of per-pixel filter prediction, we have the unique opportunity to visualize how each pixel is evolving during solving these tasks, thus gaining better interpretability.",0
"The framework we present, called multigrid Predictive Filter Flow (mgPFF), enables unsupervised learning on videos. With a pair of frames as input, mgPFF produces per-pixel filters that facilitate warping one frame to the other. In comparison to optical flow, mgPFF is more effective in modeling sub-pixel movement and addressing issues such as motion blur. We have devised a multigrid coarse-to-fine modeling approach that eliminates the need to learn large filters to capture large displacement. This has allowed us to train a compact model (4.6MB) that operates progressively over multiple resolutions using shared weights. We have trained mgPFF on unsupervised videos and demonstrated its ability to estimate long-range flow for frame reconstruction, detect video shot transitions, and perform video object segmentation and pose tracking. It has achieved superior results compared to the current state-of-the-art without the use of additional features. Additionally, since mgPFF predicts per-pixel filters, we can visualize the evolution of each pixel during the tasks, providing better interpretability.",1
"We introduce a self-supervised method for learning visual correspondence from unlabeled video. The main idea is to use cycle-consistency in time as free supervisory signal for learning visual representations from scratch. At training time, our model learns a feature map representation to be useful for performing cycle-consistent tracking. At test time, we use the acquired representation to find nearest neighbors across space and time. We demonstrate the generalizability of the representation -- without finetuning -- across a range of visual correspondence tasks, including video object segmentation, keypoint tracking, and optical flow. Our approach outperforms previous self-supervised methods and performs competitively with strongly supervised methods.",0
"A technique for acquiring knowledge regarding visual correspondence from unlabeled video is presented in this study. The approach is self-supervised and involves utilizing cycle-consistency over time as a supervisory signal to develop visual representations from scratch. During training, the model learns a feature map representation that is useful for undertaking cycle-consistent tracking. During testing, the acquired representation is utilized to locate nearest neighbors across space and time. The acquired representation is demonstrated to be generalizable, without any finetuning, across a variety of visual correspondence tasks, such as video object segmentation, keypoint tracking, and optical flow. Our method demonstrates better performance than prior self-supervised approaches and is competitive with strongly supervised methods.",1
"Video frame interpolation aims to synthesize nonexistent frames in-between the original frames. While significant advances have been made from the recent deep convolutional neural networks, the quality of interpolation is often reduced due to large object motion or occlusion. In this work, we propose a video frame interpolation method which explicitly detects the occlusion by exploring the depth information. Specifically, we develop a depth-aware flow projection layer to synthesize intermediate flows that preferably sample closer objects than farther ones. In addition, we learn hierarchical features to gather contextual information from neighboring pixels. The proposed model then warps the input frames, depth maps, and contextual features based on the optical flow and local interpolation kernels for synthesizing the output frame. Our model is compact, efficient, and fully differentiable. Quantitative and qualitative results demonstrate that the proposed model performs favorably against state-of-the-art frame interpolation methods on a wide variety of datasets.",0
"The objective of video frame interpolation is to create new frames between the original frames. Despite significant advancements in deep convolutional neural networks, the quality of interpolation often suffers when there is significant object motion or occlusion. To address this issue, we have proposed a method that detects occlusion by utilizing depth information. We have created a depth-aware flow projection layer to generate intermediate flows that prioritize closer objects over farther ones. Additionally, we have designed a hierarchical feature learning system to gather contextual information from neighboring pixels. Our model then uses the optical flow and local interpolation kernels to warp the input frames, depth maps, and contextual features to synthesize the output frame. Our model is efficient, compact, and fully differentiable. We have demonstrated through quantitative and qualitative results that our model outperforms the state-of-the-art frame interpolation methods on various datasets.",1
"With superiorities on low cost, portability, and free of radiation, echocardiogram is a widely used imaging modality for left ventricle (LV) function quantification. However, automatic LV segmentation and motion tracking is still a challenging task. In addition to fuzzy border definition, low contrast, and abounding artifacts on typical ultrasound images, the shape and size of the LV change significantly in a cardiac cycle. In this work, we propose a temporal affine network (TAN) to perform image analysis in a warped image space, where the shape and size variations due to the cardiac motion as well as other artifacts are largely compensated. Furthermore, we perform three frequent echocardiogram interpretation tasks simultaneously: standard cardiac plane recognition, LV landmark detection, and LV segmentation. Instead of using three networks with one dedicating to each task, we use a multi-task network to perform three tasks simultaneously. Since three tasks share the same encoder, the compact network improves the segmentation accuracy with more supervision. The network is further finetuned with optical flow adjusted annotations to enhance motion coherence in the segmentation result. Experiments on 1,714 2D echocardiographic sequences demonstrate that the proposed method achieves state-of-the-art segmentation accuracy with real-time efficiency.",0
"Echocardiogram is a widely used imaging technique for quantifying left ventricle (LV) function due to its low cost, portability, and lack of radiation. However, automatic LV segmentation and motion tracking remains a challenging task due to fuzzy border definition, low contrast, and the abundance of artifacts in typical ultrasound images. Additionally, the shape and size of the LV change significantly during a cardiac cycle. To address these issues, we propose a temporal affine network (TAN) that performs image analysis in a warped image space to compensate for shape and size variations caused by cardiac motion and other artifacts. The TAN performs three frequent echocardiogram interpretation tasks simultaneously: standard cardiac plane recognition, LV landmark detection, and LV segmentation, using a multi-task network instead of three separate networks. This compact network improves segmentation accuracy with more supervision, and is fine-tuned with optical flow adjusted annotations to enhance motion coherence in the segmentation result. Our experiments on 1,714 2D echocardiographic sequences demonstrate that the proposed method achieves state-of-the-art segmentation accuracy with real-time efficiency.",1
"It is crucial to reduce natural gas methane emissions, which can potentially offset the climate benefits of replacing coal with gas. Optical gas imaging (OGI) is a widely-used method to detect methane leaks, but is labor-intensive and cannot provide leak detection results without operators' judgment. In this paper, we develop a computer vision approach to OGI-based leak detection using convolutional neural networks (CNN) trained on methane leak images to enable automatic detection. First, we collect ~1 M frames of labeled video of methane leaks from different leaking equipment for building CNN model, covering a wide range of leak sizes (5.3-2051.6 gCH4/h) and imaging distances (4.6-15.6 m). Second, we examine different background subtraction methods to extract the methane plume in the foreground. Third, we then test three CNN model variants, collectively called GasNet, to detect plumes in videos taken at other pieces of leaking equipment. We assess the ability of GasNet to perform leak detection by comparing it to a baseline method that uses optical-flow based change detection algorithm. We explore the sensitivity of results to the CNN structure, with a moderate-complexity variant performing best across distances. We find that the detection accuracy can reach as high as 99%, the overall detection accuracy can exceed 95% for a case across all leak sizes and imaging distances. Binary detection accuracy exceeds 97% for large leaks (~710 gCH4/h) imaged closely (~5-7 m). At closer imaging distances (~5-10 m), CNN-based models have greater than 94% accuracy across all leak sizes. At farthest distances (~13-16 m), performance degrades rapidly, but it can achieve above 95% accuracy to detect large leaks (>950 gCH4/h). The GasNet-based computer vision approach could be deployed in OGI surveys to allow automatic vigilance of methane leak detection with high detection accuracy in the real world.",0
"Reducing natural gas methane emissions is crucial in order to prevent offsetting the climate benefits of replacing coal with gas. While optical gas imaging (OGI) is a widely-used method to detect methane leaks, it is a labor-intensive process that requires operator judgment to provide leak detection results. This paper proposes a computer vision approach to OGI-based leak detection using convolutional neural networks (CNN) trained on methane leak images, which enables automatic detection. The approach involves collecting ~1 M frames of labeled video of methane leaks from various leaking equipment, using different leak sizes (5.3-2051.6 gCH4/h) and imaging distances (4.6-15.6 m) to build the CNN model. Background subtraction methods are examined to extract the methane plume in the foreground, and three CNN model variants, called GasNet, are tested to detect plumes in videos taken at other pieces of leaking equipment. The ability of GasNet to perform leak detection is assessed and compared to a baseline method that uses optical-flow based change detection algorithm. Results show that GasNet achieves high detection accuracy, reaching up to 99%, with an overall detection accuracy of over 95% across all leak sizes and imaging distances. The approach could be deployed in OGI surveys to allow automatic vigilance of methane leak detection with high detection accuracy in the real world.",1
"Learning descriptive spatio-temporal object models from data is paramount for the task of semi-supervised video object segmentation. Most existing approaches mainly rely on models that estimate the segmentation mask based on a reference mask at the first frame (aided sometimes by optical flow or the previous mask). These models, however, are prone to fail under rapid appearance changes or occlusions due to their limitations in modelling the temporal component. On the other hand, very recently, other approaches learned long-term features using a convolutional LSTM to leverage the information from all previous video frames. Even though these models achieve better temporal representations, they still have to be fine-tuned for every new video sequence. In this paper, we present an intermediate solution and devise a novel GAN architecture, FaSTGAN, to learn spatio-temporal object models over finite temporal windows. To achieve this, we concentrate all the heavy computational load to the training phase with two critics that enforce spatial and temporal mask consistency over the last K frames. Then at test time, we only use a relatively light regressor, which reduces the inference time considerably. As a result, our approach combines a high resiliency to sudden geometric and photometric object changes with efficiency at test time (no need for fine-tuning nor post-processing). We demonstrate that the accuracy of our method is on par with state-of-the-art techniques on the challenging YouTube-VOS and DAVIS datasets, while running at 32 fps, about 4x faster than the closest competitor.",0
"Developing descriptive spatio-temporal object models from data is crucial for semi-supervised video object segmentation. However, current methods rely heavily on models that estimate segmentation masks based on a reference mask from the first frame, often with the aid of optical flow or the previous mask. These models are limited in their ability to model the temporal component and are susceptible to failure under rapid appearance changes or occlusions. Recently, other approaches have utilized convolutional LSTM to learn long-term features and improve temporal representations, though they still require fine-tuning for every new video sequence. Our solution, FaSTGAN, presents an intermediate approach by using a novel GAN architecture to learn spatio-temporal object models over finite temporal windows. The training phase is computationally intensive, utilizing two critics to enforce spatial and temporal mask consistency over the last K frames. At test time, a relatively light regressor is used to greatly reduce inference time. Our approach offers high resiliency to sudden geometric and photometric object changes, as well as efficiency at test time without requiring fine-tuning or post-processing. We demonstrate that our method achieves accuracy comparable to state-of-the-art techniques on the challenging YouTube-VOS and DAVIS datasets, running at 32 fps, which is about 4 times faster than the closest competitor.",1
"Successive frames of a video are highly redundant, and the most popular object detection methods do not take advantage of this fact. Using multiple consecutive frames can improve detection of small objects or difficult examples and can improve speed and detection consistency in a video sequence, for instance by interpolating features between frames. In this work, a novel approach is introduced to perform online video object detection using two consecutive frames of video sequences involving road users. Two new models, RetinaNet-Double and RetinaNet-Flow, are proposed, based respectively on the concatenation of a target frame with a preceding frame, and the concatenation of the optical flow with the target frame. The models are trained and evaluated on three public datasets. Experiments show that using a preceding frame improves performance over single frame detectors, but using explicit optical flow usually does not.",0
"The frames of a video are often repetitive, yet common object detection methods fail to utilize this information. Incorporating multiple frames can enhance small object detection and improve the speed and accuracy of detection in video sequences through feature interpolation. This study introduces a new approach to online video object detection using two consecutive frames in road user sequences. Two models, RetinaNet-Double and RetinaNet-Flow, are proposed, one utilizing a target frame concatenated with the preceding frame and the other concatenating optical flow with the target frame. The models are tested on three public datasets, revealing that using a preceding frame enhances performance while explicit optical flow does not.",1
"The combination of spiking neural networks and event-based vision sensors holds the potential of highly efficient and high-bandwidth optical flow estimation. This paper presents the first hierarchical spiking architecture in which motion (direction and speed) selectivity emerges in an unsupervised fashion from the raw stimuli generated with an event-based camera. A novel adaptive neuron model and stable spike-timing-dependent plasticity formulation are at the core of this neural network governing its spike-based processing and learning, respectively. After convergence, the neural architecture exhibits the main properties of biological visual motion systems, namely feature extraction and local and global motion perception. Convolutional layers with input synapses characterized by single and multiple transmission delays are employed for feature and local motion perception, respectively; while global motion selectivity emerges in a final fully-connected layer. The proposed solution is validated using synthetic and real event sequences. Along with this paper, we provide the cuSNN library, a framework that enables GPU-accelerated simulations of large-scale spiking neural networks. Source code and samples are available at https://github.com/tudelft/cuSNN.",0
"This paper introduces a novel approach to optical flow estimation through the use of spiking neural networks and event-based vision sensors. The hierarchical spiking architecture presented in this study allows for motion selectivity to emerge in an unsupervised fashion from the raw stimuli generated by the camera. The neural network is governed by an adaptive neuron model and stable spike-timing-dependent plasticity formulation, enabling effective spike-based processing and learning. The resulting neural architecture exhibits feature extraction and local and global motion perception, resembling biological visual motion systems. The proposed approach is validated using synthetic and real event sequences, and the cuSNN library is provided as a framework for GPU-accelerated simulations of large-scale spiking neural networks. Source code and samples are available at https://github.com/tudelft/cuSNN.",1
"We present a method for human pose tracking that is based on learning spatiotemporal relationships among joints. Beyond generating the heatmap of a joint in a given frame, our system also learns to predict the offset of the joint from a neighboring joint in the frame. Additionally, it is trained to predict the displacement of the joint from its position in the previous frame, in a manner that can account for possibly changing joint appearance, unlike optical flow. These relational cues in the spatial domain and temporal domain are inferred in a robust manner by attending only to relevant areas in the video frames. By explicitly learning and exploiting these joint relationships, our system achieves state-of-the-art performance on standard benchmarks for various pose tracking tasks including 3D body pose tracking in RGB video, 3D hand pose tracking in depth sequences, and 3D hand gesture tracking in RGB video.",0
"Our method for tracking human poses is based on learning relationships between joints in both space and time. In addition to generating joint heatmaps, our system predicts the joint's offset from neighboring joints in the current frame and its displacement from its previous position in a way that accounts for changes in appearance. These relational cues are inferred by attending only to relevant areas in video frames, resulting in robust performance on standard benchmarks for multiple pose tracking tasks, including 3D body and hand pose tracking in RGB and depth sequences. By exploiting joint relationships, our system achieves state-of-the-art performance.",1
"Facial micro-expressions are subtle and involuntary expressions that can reveal concealed emotions. Micro-expressions are an invaluable source of information in application domains such as lie detection, mental health, sentiment analysis and more. One of the biggest challenges in this field of research is the small amount of available spontaneous micro-expression data. However, spontaneous data collection is burdened by time-consuming and expensive annotation. Hence, methods are needed which can reduce the amount of data that annotators have to review. This paper presents a novel micro-expression spotting method using a recurrent neural network (RNN) on optical flow features. We extract Histogram of Oriented Optical Flow (HOOF) features to encode the temporal changes in selected face regions. Finally, the RNN spots short intervals which are likely to contain occurrences of relevant facial micro-movements. The proposed method is evaluated on the SAMM database. Any chance of subject bias is eliminated by training the RNN using Leave-One-Subject-Out cross-validation. Comparing the spotted intervals with the labeled data shows that the method produced 1569 false positives while obtaining a recall of 0.4654. The initial results show that the proposed method would reduce the video length by a factor of 3.5, while still retaining almost half of the relevant micro-movements. Lastly, as the model gets more data, it becomes better at detecting intervals, which makes the proposed method suitable for supporting the annotation process.",0
"Facial micro-expressions are subtle and involuntary expressions that can reveal concealed emotions. They are useful in various application domains such as lie detection, mental health, and sentiment analysis. However, collecting spontaneous data for micro-expression research is challenging due to the time-consuming and expensive annotation process. Thus, the paper proposes a new method for micro-expression spotting using a recurrent neural network (RNN) on optical flow features. The method extracts Histogram of Oriented Optical Flow (HOOF) features to encode the temporal changes in selected face regions. The RNN then spots short intervals likely to contain relevant facial micro-movements. The method is evaluated on the SAMM database and eliminates subject bias by using Leave-One-Subject-Out cross-validation. The results show that the proposed method reduces video length by 3.5 times while retaining almost half of the relevant micro-movements. Furthermore, as the model receives more data, it becomes better at detecting intervals, making it suitable for supporting the annotation process.",1
"Current benchmarks for optical flow algorithms evaluate the estimation quality by comparing their predicted flow field with the ground truth, and additionally may compare interpolated frames, based on these predictions, with the correct frames from the actual image sequences. For the latter comparisons, objective measures such as mean square errors are applied. However, for applications like image interpolation, the expected user's quality of experience cannot be fully deduced from such simple quality measures. Therefore, we conducted a subjective quality assessment study by crowdsourcing for the interpolated images provided in one of the optical flow benchmarks, the Middlebury benchmark. We used paired comparisons with forced choice and reconstructed absolute quality scale values according to Thurstone's model using the classical least squares method. The results give rise to a re-ranking of 141 participating algorithms w.r.t. visual quality of interpolated frames mostly based on optical flow estimation. Our re-ranking result shows the necessity of visual quality assessment as another evaluation metric for optical flow and frame interpolation benchmarks.",0
"Currently, optical flow algorithms are evaluated by comparing their predicted flow field with the ground truth and may also compare interpolated frames with the correct frames from the actual image sequences using objective measures such as mean square errors. However, such simple quality measures cannot fully deduce the expected user's quality of experience in applications like image interpolation. Thus, we conducted a subjective quality assessment study by crowdsourcing for the interpolated images provided in the Middlebury benchmark. Using paired comparisons with forced choice and Thurstone's model, we reconstructed absolute quality scale values and re-ranked 141 participating algorithms based on visual quality of interpolated frames mostly based on optical flow estimation. This re-ranking result highlights the necessity of visual quality assessment as another evaluation metric for optical flow and frame interpolation benchmarks.",1
"Many semantic video analysis tasks can benefit from multiple, heterogenous signals. For example, in addition to the original RGB input sequences, sequences of optical flow are usually used to boost the performance of human action recognition in videos. To learn from these heterogenous input sources, existing methods reply on two-stream architectural designs that contain independent, parallel streams of Recurrent Neural Networks (RNNs). However, two-stream RNNs do not fully exploit the reciprocal information contained in the multiple signals, let alone exploit it in a recurrent manner. To this end, we propose in this paper a novel recurrent architecture, termed Coupled Recurrent Network (CRN), to deal with multiple input sources. In CRN, the parallel streams of RNNs are coupled together. Key design of CRN is a Recurrent Interpretation Block (RIB) that supports learning of reciprocal feature representations from multiple signals in a recurrent manner. Different from RNNs which stack the training loss at each time step or the last time step, we propose an effective and efficient training strategy for CRN. Experiments show the efficacy of the proposed CRN. In particular, we achieve the new state of the art on the benchmark datasets of human action recognition and multi-person pose estimation.",0
"The utilization of multiple and diverse signals can prove advantageous for various semantic video analysis tasks. For instance, along with the initial RGB input sequences, optical flow sequences are commonly incorporated to enhance the performance of recognizing human actions in videos. To learn from these varied input sources, current methods employ two-stream architectural designs that comprise of separate, parallel streams of Recurrent Neural Networks (RNNs). However, the two-stream RNNs do not fully utilize the interdependent information from the multiple signals in a recurrent manner. Therefore, we introduce a new recurrent architecture, named Coupled Recurrent Network (CRN), to address the challenge of dealing with multiple input sources. The CRN integrates the parallel RNN streams to facilitate the learning of reciprocal feature representations from numerous signals in a recurrent manner. Our novel Recurrent Interpretation Block (RIB) is a key feature of the CRN design that supports the learning of these reciprocal feature representations. Unlike RNNs, which stack the training loss at each time step or the final time step, we propose an effective and efficient training strategy for CRN. Our experiments demonstrate the effectiveness of the proposed CRN, which achieves state-of-the-art results on benchmark datasets for human action recognition and multi-person pose estimation.",1
"This paper presents a semi-supervised learning framework to train a keypoint detector using multiview image streams given the limited labeled data (typically $<$4\%). We leverage the complementary relationship between multiview geometry and visual tracking to provide three types of supervisionary signals to utilize the unlabeled data: (1) keypoint detection in one view can be supervised by other views via the epipolar geometry; (2) a keypoint moves smoothly over time where its optical flow can be used to temporally supervise consecutive image frames to each other; (3) visible keypoint in one view is likely to be visible in the adjacent view. We integrate these three signals in a differentiable fashion to design a new end-to-end neural network composed of three pathways. This design allows us to extensively use the unlabeled data to train the keypoint detector. We show that our approach outperforms existing detectors including DeepLabCut tailored to the keypoint detection of non-human species such as monkeys, dogs, and mice.",0
"In this article, a framework for semi-supervised learning is presented for training a keypoint detector using multiview image streams, despite having minimal labeled data (generally less than 4%). The approach utilizes three types of supervisory signals to make the most of the unlabeled data, by leveraging the complementary relationship between multiview geometry and visual tracking. The first signal uses epipolar geometry to supervise keypoint detection in one view based on other views. The second signal uses optical flow to supervise consecutive image frames temporally. The third signal uses the assumption that if a keypoint is visible in one view, it is likely to be visible in the adjacent view. These three signals are integrated in a differentiable manner to create a new neural network consisting of three pathways. This design enables the unlabeled data to be extensively used to train the keypoint detector. The results show that the proposed method outperforms existing detectors, including DeepLabCut, which is designed for keypoint detection in non-human species such as monkeys, dogs, and mice.",1
"Recent geometric methods need reliable estimates of 3D motion parameters to procure accurate dense depth map of a complex dynamic scene from monocular images \cite{kumar2017monocular, ranftl2016dense}. Generally, to estimate \textbf{precise} measurements of relative 3D motion parameters and to validate its accuracy using image data is a challenging task. In this work, we propose an alternative approach that circumvents the 3D motion estimation requirement to obtain a dense depth map of a dynamic scene. Given per-pixel optical flow correspondences between two consecutive frames and, the sparse depth prior for the reference frame, we show that, we can effectively recover the dense depth map for the successive frames without solving for 3D motion parameters. Our method assumes a piece-wise planar model of a dynamic scene, which undergoes rigid transformation locally, and as-rigid-as-possible transformation globally between two successive frames. Under our assumption, we can avoid the explicit estimation of 3D rotation and translation to estimate scene depth. In essence, our formulation provides an unconventional way to think and recover the dense depth map of a complex dynamic scene which is incremental and motion free in nature. Our proposed method does not make object level or any other high-level prior assumption about the dynamic scene, as a result, it is applicable to a wide range of scenarios. Experimental results on the benchmarks dataset show the competence of our approach for multiple frames.",0
"To obtain an accurate dense depth map of a complex dynamic scene from monocular images, recent geometric methods require reliable estimates of 3D motion parameters. However, this task is challenging as it involves obtaining precise measurements of relative 3D motion parameters and validating their accuracy using image data. In this study, we present an alternative approach that does not require 3D motion estimation to obtain a dense depth map of a dynamic scene. Instead, we use per-pixel optical flow correspondences between two consecutive frames and a sparse depth prior for the reference frame to effectively recover the dense depth map for the successive frames. Our method assumes a piece-wise planar model of a dynamic scene, which undergoes rigid transformation locally and as-rigid-as-possible transformation globally between two successive frames. By avoiding the explicit estimation of 3D rotation and translation, our method provides an unconventional and motion-free way to recover the dense depth map of a complex dynamic scene. Our proposed approach does not make any high-level prior assumptions about the dynamic scene, making it applicable to a wide range of scenarios. Experimental results on the benchmark dataset demonstrate the effectiveness of our approach for multiple frames.",1
"In multi-person videos, especially team sport videos, a semantic event is usually represented as a confrontation between two teams of players, which can be represented as collective motion. In broadcast basketball videos, specific camera motions are used to present specific events. Therefore, a semantic event in broadcast basketball videos is closely related to both the global motion (camera motion) and the collective motion. A semantic event in basketball videos can be generally divided into three stages: pre-event, event occurrence (event-occ), and post-event. In this paper, we propose an ontology-based global and collective motion pattern (On_GCMP) algorithm for basketball event classification. First, a two-stage GCMP based event classification scheme is proposed. The GCMP is extracted using optical flow. The two-stage scheme progressively combines a five-class event classification algorithm on event-occs and a two-class event classification algorithm on pre-events. Both algorithms utilize sequential convolutional neural networks (CNNs) and long short-term memory (LSTM) networks to extract the spatial and temporal features of GCMP for event classification. Second, we utilize post-event segments to predict success/failure using deep features of images in the video frames (RGB_DF_VF) based algorithms. Finally the event classification results and success/failure classification results are integrated to obtain the final results. To evaluate the proposed scheme, we collected a new dataset called NCAA+, which is automatically obtained from the NCAA dataset by extending the fixed length of video clips forward and backward of the corresponding semantic events. The experimental results demonstrate that the proposed scheme achieves the mean average precision of 58.10% on NCAA+. It is higher by 6.50% than state-of-the-art on NCAA.",0
"In videos featuring multiple individuals, particularly team sports, a semantic event typically involves a clash between two teams of players, which can be depicted as collective motion. In basketball broadcasts, specific camera movements are utilized to showcase certain occurrences. Therefore, a semantic event in basketball broadcasts is closely linked to both the overall motion (camera movement) and collective motion. Basketball videos' semantic events can be categorized into three stages: pre-event, event occurrence (event-occ), and post-event. This paper proposes an ontology-based global and collective motion pattern (On_GCMP) algorithm for basketball event classification. The GCMP is extracted using optical flow, and the two-stage GCMP-based event classification approach uses sequential convolutional neural networks (CNNs) and long short-term memory (LSTM) networks to extract spatial and temporal features for event classification. Furthermore, the success or failure classification is predicted using deep features of images in the video frames (RGB_DF_VF) based algorithms. Finally, the event classification and success/failure classification results are combined to obtain the final results. To assess the proposed approach, a new dataset called NCAA+ was created, which was automatically derived from the NCAA dataset by extending the fixed length of video clips backward and forward of the corresponding semantic events. The experimental results reveal that the proposed approach achieves a mean average precision of 58.10% on NCAA+, which is 6.50% higher than state-of-the-art on NCAA.",1
"In this paper, several variants of two-stream architectures for temporal action proposal generation in long, untrimmed videos are presented. Inspired by the recent advances in the field of human action recognition utilizing 3D convolutions in combination with two-stream networks and based on the Single-Stream Temporal Action Proposals (SST) architecture, four different two-stream architectures utilizing sequences of images on one stream and sequences of images of optical flow on the other stream are subsequently investigated. The four architectures fuse the two separate streams at different depths in the model; for each of them, a broad range of parameters is investigated systematically as well as an optimal parametrization is empirically determined. The experiments on the THUMOS'14 dataset show that all four two-stream architectures are able to outperform the original single-stream SST and achieve state of the art results. Additional experiments revealed that the improvements are not restricted to a single method of calculating optical flow by exchanging the formerly used method of Brox with FlowNet2 and still achieving improvements.",0
"This paper presents various versions of two-stream architectures for generating temporal action proposals in long, untrimmed videos. These architectures are inspired by recent advancements in human action recognition that use 3D convolutions along with two-stream networks. The study investigates four different two-stream architectures that use image sequences for one stream and optical flow image sequences for the other. These architectures combine the two streams at different depths in the model, and a wide range of parameters is systematically investigated to determine the optimal settings. Results from experiments on the THUMOS'14 dataset show that all four two-stream architectures outperform the original single-stream SST and achieve state-of-the-art outcomes. Further experiments indicate that the improvements are not limited to a specific method of calculating optical flow, as the use of FlowNet2 instead of the previously used Brox method also led to improvements.",1
"Inferring the relations between two images is an important class of tasks in computer vision. Examples of such tasks include computing optical flow and stereo disparity. We treat the relation inference tasks as a machine learning problem and tackle it with neural networks. A key to the problem is learning a representation of relations. We propose a new neural network module, contrast association unit (CAU), which explicitly models the relations between two sets of input variables. Due to the non-negativity of the weights in CAU, we adopt a multiplicative update algorithm for learning these weights. Experiments show that neural networks with CAUs are more effective in learning five fundamental image transformations than conventional neural networks.",0
"The task of inferring relationships between two images is a crucial aspect of computer vision. Tasks such as determining optical flow and stereo disparity fall into this category. Our approach to solving these relation inference tasks is to treat them as a machine learning problem and utilize neural networks. A crucial component of this solution is learning a representation of these relationships. To accomplish this, we introduce a new neural network module called the contrast association unit (CAU), which explicitly models the relationships between two sets of input variables. Because the weights in CAU must be non-negative, we utilize a multiplicative update algorithm to learn these weights. Our experiments demonstrate that neural networks with CAUs are more effective than conventional neural networks in learning five fundamental image transformations.",1
"We address the unsupervised learning of several interconnected problems in low-level vision: single view depth prediction, camera motion estimation, optical flow, and segmentation of a video into the static scene and moving regions. Our key insight is that these four fundamental vision problems are coupled through geometric constraints. Consequently, learning to solve them together simplifies the problem because the solutions can reinforce each other. We go beyond previous work by exploiting geometry more explicitly and segmenting the scene into static and moving regions. To that end, we introduce Competitive Collaboration, a framework that facilitates the coordinated training of multiple specialized neural networks to solve complex problems. Competitive Collaboration works much like expectation-maximization, but with neural networks that act as both competitors to explain pixels that correspond to static or moving regions, and as collaborators through a moderator that assigns pixels to be either static or independently moving. Our novel method integrates all these problems in a common framework and simultaneously reasons about the segmentation of the scene into moving objects and the static background, the camera motion, depth of the static scene structure, and the optical flow of moving objects. Our model is trained without any supervision and achieves state-of-the-art performance among joint unsupervised methods on all sub-problems.",0
"We focus on unsupervised learning for low-level vision tasks, including single view depth prediction, camera motion estimation, optical flow, and video segmentation into static and moving regions. Our approach is based on the understanding that these four problems are interrelated through geometric constraints, and solving them together yields better results. Our framework, Competitive Collaboration, leverages geometry and explicitly segments the scene into static and moving regions. It involves training specialized neural networks to compete for pixels and collaborate through a moderator, similar to expectation-maximization. Our method integrates all these problems and simultaneously addresses the segmentation of the scene into static and moving objects, camera motion, depth estimation of the static scene, and optical flow of moving objects. We achieve state-of-the-art performance among unsupervised methods on all sub-problems.",1
"This paper proposes the novel task of video generation conditioned on a SINGLE semantic label map, which provides a good balance between flexibility and quality in the generation process. Different from typical end-to-end approaches, which model both scene content and dynamics in a single step, we propose to decompose this difficult task into two sub-problems. As current image generation methods do better than video generation in terms of detail, we synthesize high quality content by only generating the first frame. Then we animate the scene based on its semantic meaning to obtain the temporally coherent video, giving us excellent results overall. We employ a cVAE for predicting optical flow as a beneficial intermediate step to generate a video sequence conditioned on the initial single frame. A semantic label map is integrated into the flow prediction module to achieve major improvements in the image-to-video generation process. Extensive experiments on the Cityscapes dataset show that our method outperforms all competing methods.",0
"In this paper, we introduce a new task of generating videos based on a single semantic label map, striking a balance between flexibility and quality. Rather than using a typical end-to-end approach of modeling both scene content and dynamics simultaneously, we divide the task into two sub-problems. Given that current image generation methods excel in detail, we generate the first frame to synthesize high-quality content. We then animate the scene based on its semantic meaning to produce a temporally coherent video, yielding excellent results. To achieve this, we use a cVAE to predict optical flow as an intermediate step, conditioned on the initial single frame. By integrating a semantic label map into the flow prediction module, we achieve significant improvements in the image-to-video generation process. Our experiments on the Cityscapes dataset demonstrate that our approach outperforms all competitors.",1
"Synthetic Aperture Vector Flow Imaging (SA-VFI) can visualize complex cardiac and vascular blood flow patterns at high temporal resolution with a large field of view. Convolutional neural networks (CNNs) are commonly used in image and video recognition and classification. However, most recently presented CNNs also allow for making per-pixel predictions as needed in optical flow velocimetry. To our knowledge we demonstrate here for the first time a CNN architecture to produce 2D full flow field predictions from high frame rate SA ultrasound images using supervised learning. The CNN was initially trained using CFD-generated and augmented noiseless SA ultrasound data of a realistic vessel geometry. Subsequently, a mix of noisy simulated and real \textit{in vivo} acquisitions were added to increase the network's robustness. The resulting flow field of the CNN resembled the ground truth accurately with an endpoint-error percentage between 6.5\% to 14.5\%. Furthermore, when confronted with an unknown geometry of an arterial bifurcation, the CNN was able to predict an accurate flow field indicating its ability for generalization. Remarkably, the CNN also performed well for rotational flows, which usually requires advanced, computationally intensive VFI methods. We have demonstrated that convolutional neural networks can be used to estimate complex multidirectional flow.",0
"High-resolution visualization of intricate cardiac and vascular blood flow patterns can be achieved through Synthetic Aperture Vector Flow Imaging (SA-VFI) with a wide field of view. While Convolutional Neural Networks (CNNs) are commonly used for image and video recognition and classification, they can also make per-pixel predictions required for optical flow velocimetry. In this study, we present, for the first time, a CNN architecture that can produce 2D full flow field predictions from high frame rate SA ultrasound images using supervised learning. Initially trained on noiseless SA ultrasound data of a realistic vessel geometry generated by CFD, we then added a mix of noisy simulated and real \textit{in vivo} acquisitions to increase the network's robustness. The CNN accurately predicted flow fields, with an endpoint-error percentage between 6.5\% to 14.5\%, which closely resembled the ground truth. Notably, the CNN also demonstrated its generalization ability by providing accurate flow field predictions for an unknown arterial bifurcation geometry. Moreover, the CNN was successful in predicting rotational flows, which typically require computationally intensive VFI methods. Our findings demonstrate that CNNs can estimate complex multidirectional flow patterns.",1
"Dynamic scenes that contain both object motion and egomotion are a challenge for monocular visual odometry (VO). Another issue with monocular VO is the scale ambiguity, i.e. these methods cannot estimate scene depth and camera motion in real scale. Here, we propose a learning based approach to predict camera motion parameters directly from optic flow, by marginalizing depthmap variations and outliers. This is achieved by learning a sparse overcomplete basis set of egomotion in an autoencoder network, which is able to eliminate irrelevant components of optic flow for the task of camera parameter or motionfield estimation. The model is trained using a sparsity regularizer and a supervised egomotion loss, and achieves the state-of-the-art performances on trajectory prediction and camera rotation prediction tasks on KITTI and Virtual KITTI datasets, respectively. The sparse latent space egomotion representation learned by the model is robust and requires only 5% of the hidden layer neurons to maintain the best trajectory prediction accuracy on KITTI dataset. Additionally, in presence of depth information, the proposed method demonstrates faithful object velocity prediction for wide range of object sizes and speeds by global compensation of predicted egomotion and a divisive normalization procedure.",0
"Monocular visual odometry (VO) faces challenges when dealing with dynamic scenes that involve object motion and egomotion. Furthermore, the issue of scale ambiguity arises as these methods cannot accurately estimate scene depth and camera motion in real scale. To address these issues, we introduce a learning-based approach that predicts camera motion parameters directly from optic flow while marginalizing depthmap variations and outliers. This is achieved by using an autoencoder network to learn a sparse overcomplete basis set of egomotion, which eliminates irrelevant components of optic flow for the task of camera parameter or motionfield estimation. The model is trained with a sparsity regularizer and a supervised egomotion loss, resulting in state-of-the-art performance on trajectory prediction and camera rotation prediction tasks on the KITTI and Virtual KITTI datasets, respectively. The sparse latent space egomotion representation is robust and requires only 5% of the hidden layer neurons to maintain the best trajectory prediction accuracy on the KITTI dataset. Furthermore, in the presence of depth information, our proposed method accurately predicts object velocity for a wide range of object sizes and speeds through global compensation of predicted egomotion and a divisive normalization procedure.",1
"The goal of video segmentation is to turn video data into a set of concrete motion clusters that can be easily interpreted as building blocks of the video. There are some works on similar topics like detecting scene cuts in a video, but there is few specific research on clustering video data into the desired number of compact segments. It would be more intuitive, and more efficient, to work with perceptually meaningful entity obtained from a low-level grouping process which we call it superframe. This paper presents a new simple and efficient technique to detect superframes of similar content patterns in videos. We calculate the similarity of content-motion to obtain the strength of change between consecutive frames. With the help of existing optical flow technique using deep models, the proposed method is able to perform more accurate motion estimation efficiently. We also propose two criteria for measuring and comparing the performance of different algorithms on various databases. Experimental results on the videos from benchmark databases have demonstrated the effectiveness of the proposed method.",0
"The primary objective of video segmentation is to transform video data into a collection of distinct motion clusters that can be easily comprehended as fundamental components of the video. While there are some studies related to identifying scene transitions in a video, there are not many researches that specifically focus on clustering video data into a desired number of compact segments. It would be more intuitive and efficient to work with perceptually meaningful entities obtained from a low-level grouping process, which we refer to as superframes. This article introduces a new, uncomplicated, and efficient approach to identifying superframes with similar content patterns in videos. The technique calculates the similarity of content-motion to determine the strength of change between consecutive frames. The proposed method utilizes existing optical flow techniques aided by deep models to perform motion estimation more accurately and efficiently. Additionally, two criteria are suggested for measuring and comparing the performance of different algorithms on various databases. The experimental results on benchmark databases have demonstrated the effectiveness of the proposed method.",1
"Identifying human behaviors is a challenging research problem due to the complexity and variation of appearances and postures, the variation of camera settings, and view angles. In this paper, we try to address the problem of human behavior identification by introducing a novel motion descriptor based on statistical features. The method first divide the video into N number of temporal segments. Then for each segment, we compute dense optical flow, which provides instantaneous velocity information for all the pixels. We then compute Histogram of Optical Flow (HOOF) weighted by the norm and quantized into 32 bins. We then compute statistical features from the obtained HOOF forming a descriptor vector of 192- dimensions. We then train a non-linear multi-class SVM that classify different human behaviors with the accuracy of 72.1%. We evaluate our method by using publicly available human action data set. Experimental results shows that our proposed method out performs state of the art methods.",0
"The difficulty of identifying human behaviors stems from the intricate and diverse range of physical appearances and positions, the varying camera settings and angles. Our research paper aims to tackle this issue by introducing a new motion descriptor that relies on statistical features. Firstly, we divide the video into N temporal segments and calculate dense optical flow, which provides velocity information for each pixel. We then use the Histogram of Optical Flow (HOOF) that is weighted by the norm and quantized into 32 bins. Statistical features are derived from the HOOF, creating a 192-dimensional descriptor vector. Finally, we employ a non-linear multi-class SVM to classify human behaviors, with an accuracy of 72.1%. Our method is tested using publicly available human action data sets, and the results demonstrate that our approach surpasses existing methods.",1
"Two-stream architecture have shown strong performance in video classification task. The key idea is to learn spatio-temporal features by fusing convolutional networks spatially and temporally. However, there are some problems within such architecture. First, it relies on optical flow to model temporal information, which are often expensive to compute and store. Second, it has limited ability to capture details and local context information for video data. Third, it lacks explicit semantic guidance that greatly decrease the classification performance. In this paper, we proposed a new two-stream based deep framework for video classification to discover spatial and temporal information only from RGB frames, moreover, the multi-scale pyramid attention (MPA) layer and the semantic adversarial learning (SAL) module is introduced and integrated in our framework. The MPA enables the network capturing global and local feature to generate a comprehensive representation for video, and the SAL can make this representation gradually approximate to the real video semantics in an adversarial manner. Experimental results on two public benchmarks demonstrate our proposed methods achieves state-of-the-art results on standard video datasets.",0
"The performance of two-stream architecture in video classification has been strong, with the main concept being learning spatio-temporal features through the fusion of convolutional networks spatially and temporally. However, this architecture has encountered some issues, including the reliance on optical flow for modeling temporal information, which can be costly to compute and store, limited ability to capture video data's details and local context information, and the lack of explicit semantic guidance, which leads to reduced classification performance. To address these challenges, we have developed a new two-stream based deep framework for video classification that extracts spatial and temporal information solely from RGB frames. Furthermore, our framework incorporates the multi-scale pyramid attention (MPA) layer and the semantic adversarial learning (SAL) module. MPA enables the network to capture global and local features to create a comprehensive representation of the video, while SAL gradually approximates this representation to the true video semantics through an adversarial approach. Our experimental results on two public benchmarks demonstrate that our proposed methods achieve state-of-the-art results on standard video datasets.",1
"Face anti-spoofing is significant to the security of face recognition systems. Previous works on depth supervised learning have proved the effectiveness for face anti-spoofing. Nevertheless, they only considered the depth as an auxiliary supervision in the single frame. Different from these methods, we develop a new method to estimate depth information from multiple RGB frames and propose a depth-supervised architecture which can efficiently encodes spatiotemporal information for presentation attack detection. It includes two novel modules: optical flow guided feature block (OFFB) and convolution gated recurrent units (ConvGRU) module, which are designed to extract short-term and long-term motion to discriminate living and spoofing faces. Extensive experiments demonstrate that the proposed approach achieves state-of-the-art results on four benchmark datasets, namely OULU-NPU, SiW, CASIA-MFSD, and Replay-Attack.",0
"Ensuring the security of face recognition systems requires effective face anti-spoofing measures. Previous studies have shown that depth supervised learning can be effective in this regard, but they have only used depth as an auxiliary supervision in a single frame. Our approach differs from these methods in that we use multiple RGB frames to estimate depth information, and propose a depth-supervised architecture that can efficiently encode spatiotemporal information for presentation attack detection. Our architecture includes two novel modules - the optical flow guided feature block (OFFB) and convolution gated recurrent units (ConvGRU) module - which are designed to extract short-term and long-term motion to distinguish between living and spoofing faces. Our experiments demonstrate that our approach achieves state-of-the-art results on four benchmark datasets, namely OULU-NPU, SiW, CASIA-MFSD, and Replay-Attack.",1
"State-of-the-art neural network models estimate large displacement optical flow in multi-resolution and use warping to propagate the estimation between two resolutions. Despite their impressive results, it is known that there are two problems with the approach. First, the multi-resolution estimation of optical flow fails in situations where small objects move fast. Second, warping creates artifacts when occlusion or dis-occlusion happens. In this paper, we propose a new neural network module, Deformable Cost Volume, which alleviates the two problems. Based on this module, we designed the Deformable Volume Network (Devon) which can estimate multi-scale optical flow in a single high resolution. Experiments show Devon is more suitable in handling small objects moving fast and achieves comparable results to the state-of-the-art methods in public benchmarks.",0
"Cutting-edge neural network models currently utilize warping to propagate estimates of large displacement optical flow between two resolutions in a multi-resolution approach. While these models have produced impressive results, two issues have been identified. Firstly, the optical flow estimation at multiple resolutions fails to accurately track fast-moving small objects. Secondly, warping leads to artifacts in instances of occlusion or dis-occlusion. Our proposed solution to these problems is a novel neural network module called Deformable Cost Volume. Using this module, we have created the Deformable Volume Network (Devon), which can estimate multi-scale optical flow in a single high resolution. Our experiments have shown that Devon is better suited to tracking fast-moving small objects and achieves results comparable to the current state-of-the-art methods in public benchmarks.",1
"Predicting the future location of vehicles is essential for safety-critical applications such as advanced driver assistance systems (ADAS) and autonomous driving. This paper introduces a novel approach to simultaneously predict both the location and scale of target vehicles in the first-person (egocentric) view of an ego-vehicle. We present a multi-stream recurrent neural network (RNN) encoder-decoder model that separately captures both object location and scale and pixel-level observations for future vehicle localization. We show that incorporating dense optical flow improves prediction results significantly since it captures information about motion as well as appearance change. We also find that explicitly modeling future motion of the ego-vehicle improves the prediction accuracy, which could be especially beneficial in intelligent and automated vehicles that have motion planning capability. To evaluate the performance of our approach, we present a new dataset of first-person videos collected from a variety of scenarios at road intersections, which are particularly challenging moments for prediction because vehicle trajectories are diverse and dynamic.",0
"The ability to predict where vehicles will be in the future is crucial for safety-critical applications like advanced driver assistance systems (ADAS) and autonomous driving. This study proposes a new method for predicting the location and size of target vehicles in an ego-vehicle's first-person view. The model is a multi-stream recurrent neural network (RNN) encoder-decoder that accounts for object location, scale, and pixel-level observations for vehicle localization. By incorporating dense optical flow, which tracks motion and appearance changes, the prediction accuracy is significantly improved. Additionally, modeling the future motion of the ego-vehicle enhances the accuracy, which is useful for intelligent and automated vehicles with motion planning capabilities. A new dataset of first-person videos gathered from various scenarios at road intersections is used to evaluate the model's performance, since these moments pose diverse and dynamic vehicle trajectories, making prediction challenging.",1
"In this work we present a lightweight, unsupervised learning pipeline for \textit{dense} depth, optical flow and egomotion estimation from sparse event output of the Dynamic Vision Sensor (DVS). To tackle this low level vision task, we use a novel encoder-decoder neural network architecture - ECN.   Our work is the first monocular pipeline that generates dense depth and optical flow from sparse event data only. The network works in self-supervised mode and has just 150k parameters. We evaluate our pipeline on the MVSEC self driving dataset and present results for depth, optical flow and and egomotion estimation. Due to the lightweight design, the inference part of the network runs at 250 FPS on a single GPU, making the pipeline ready for realtime robotics applications. Our experiments demonstrate significant improvements upon previous works that used deep learning on event data, as well as the ability of our pipeline to perform well during both day and night.",0
"Our work introduces a novel, unsupervised learning pipeline that utilizes the Dynamic Vision Sensor (DVS) to estimate dense depth, optical flow, and egomotion from sparse event output. To accomplish this task, we adopt a lightweight encoder-decoder neural network architecture known as the ECN. This is the first monocular pipeline that can generate dense depth and optical flow solely from sparse event data, operating in a self-supervised manner and with just 150k parameters. Our pipeline is evaluated on the MVSEC dataset for self-driving and produces results for depth, optical flow, and egomotion estimation. The network is designed to be lightweight, enabling it to run at 250 FPS on a single GPU, making it suitable for real-time robotics applications. Our experiments show that our pipeline outperforms previous works that use deep learning on event data and can perform well in both day and night scenarios.",1
"We present DDFlow, a data distillation approach to learning optical flow estimation from unlabeled data. The approach distills reliable predictions from a teacher network, and uses these predictions as annotations to guide a student network to learn optical flow. Unlike existing work relying on hand-crafted energy terms to handle occlusion, our approach is data-driven, and learns optical flow for occluded pixels. This enables us to train our model with a much simpler loss function, and achieve a much higher accuracy. We conduct a rigorous evaluation on the challenging Flying Chairs, MPI Sintel, KITTI 2012 and 2015 benchmarks, and show that our approach significantly outperforms all existing unsupervised learning methods, while running at real time.",0
"DDFlow is a novel technique for training optical flow estimation using unlabeled data. It involves using a teacher network to extract accurate predictions, which are then used as annotations to guide the learning of a student network. Unlike traditional methods that rely on manual energy terms to handle occlusion, DDFlow is data-driven, allowing it to effectively learn optical flow for occluded pixels. This approach simplifies the loss function and allows for higher accuracy. We evaluated our method on challenging benchmarks, including Flying Chairs, MPI Sintel, KITTI 2012 and 2015, and found that it outperforms all existing unsupervised learning methods while maintaining real-time performance.",1
"In this work, we propose a novel transformation for events from an event camera that is equivariant to optical flow under convolutions in the 3-D spatiotemporal domain. Events are generated by changes in the image, which are typically due to motion, either of the camera or the scene. As a result, different motions result in a different set of events. For learning based tasks based on a static scene such as classification which directly use the events, we must either rely on the learning method to learn the underlying object distinct from the motion, or to memorize all possible motions for each object with extensive data augmentation. Instead, we propose a novel transformation of the input event data which normalizes the $x$ and $y$ positions by the timestamp of each event. We show that this transformation generates a representation of the events that is equivariant to this motion when the optical flow is constant, allowing a deep neural network to learn the classification task without the need for expensive data augmentation. We test our method on the event based N-MNIST dataset, as well as a novel dataset N-MOVING-MNIST, with significantly more variety in motion compared to the standard N-MNIST dataset. In all sequences, we demonstrate that our transformed network is able to achieve similar or better performance compared to a network with a standard volumetric event input, and performs significantly better when the test set has a larger set of motions than seen at training.",0
"The aim of this study is to propose a new technique for events captured by an event camera. The proposed transformation is equivariant to optical flow under convolutions in the 3-D spatiotemporal domain. Events are created when changes occur within the image due to motion, either from the camera or the scene. Consequently, different movements generate a distinct set of events. For learning-based tasks like classification, which directly utilize events and are based on a static scene, we must either depend on the learning method to distinguish the object from the motion or memorize all feasible motions for each object with extensive data augmentation. Instead, we propose a transformation of the input event data that normalizes the $x$ and $y$ positions based on the timestamp of each event. We demonstrate that this transformation produces an event representation that is equivariant to motion when there is constant optical flow. This allows a deep neural network to learn the classification task without the need for costly data augmentation. We evaluate our approach on the N-MNIST dataset and introduce a novel dataset, N-MOVING-MNIST, which includes more diverse motion than the conventional N-MNIST dataset. We demonstrate that our transformed network achieves similar or better performance compared to a network with a standard volumetric event input in all sequences and performs notably better when the test set has a larger set of motions than seen at training.",1
"This paper proposes a novel framework to reconstruct the dynamic magnetic resonance images (DMRI) with motion compensation (MC). Due to the inherent motion effects during DMRI acquisition, reconstruction of DMRI using motion estimation/compensation (ME/MC) has been studied under a compressed sensing (CS) scheme. In this paper, by embedding the intensity-based optical flow (OF) constraint into the traditional CS scheme, we are able to couple the DMRI reconstruction with motion field estimation. The formulated optimization problem is solved by a primal-dual algorithm with linesearch due to its efficiency when dealing with non-differentiable problems. With the estimated motion field, the DMRI reconstruction is refined through MC. By employing the multi-scale coarse-to-fine strategy, we are able to update the variables(temporal image sequences and motion vectors) and to refine the image reconstruction alternately. Moreover, the proposed framework is capable of handling a wide class of prior information (regularizations) for DMRI reconstruction, such as sparsity, low rank and total variation. Experiments on various DMRI data, ranging from in vivo lung to cardiac dataset, validate the reconstruction quality improvement using the proposed scheme in comparison to several state-of-the-art algorithms.",0
"A new approach is presented in this paper for reconstructing dynamic magnetic resonance images (DMRI) with motion compensation (MC). DMRI acquisition is known to be affected by motion, which has led to the study of motion estimation/compensation (ME/MC) in a compressed sensing (CS) scheme. In this work, the intensity-based optical flow (OF) constraint is incorporated into the CS scheme to enable the coupling of DMRI reconstruction with motion field estimation. The optimization problem is solved using a primal-dual algorithm with linesearch, known for its efficiency in non-differentiable problems. The estimated motion field is then used to refine the DMRI reconstruction through MC, employing a multi-scale coarse-to-fine strategy to update variables and refine the image reconstruction. The proposed framework accommodates a range of prior information (regularizations) for DMRI reconstruction, including sparsity, low rank, and total variation. The effectiveness of the proposed scheme is validated through experiments on different DMRI datasets, including in vivo lung and cardiac data, demonstrating improved reconstruction quality compared to several state-of-the-art algorithms.",1
"In this work, we propose a method that combines unsupervised deep learning predictions for optical flow and monocular disparity with a model based optimization procedure for instantaneous camera pose. Given the flow and disparity predictions from the network, we apply a RANSAC outlier rejection scheme to find an inlier set of flows and disparities, which we use to solve for the relative camera pose in a least squares fashion. We show that this pipeline is fully differentiable, allowing us to combine the pose with the network outputs as an additional unsupervised training loss to further refine the predicted flows and disparities. This method not only allows us to directly regress relative pose from the network outputs, but also automatically segments away pixels that do not fit the rigid scene assumptions that many unsupervised structure from motion methods apply, such as on independently moving objects. We evaluate our method on the KITTI dataset, and demonstrate state of the art results, even in the presence of challenging independently moving objects.",0
"The proposed work presents a technique that merges unsupervised deep learning predictions for optical flow and monocular disparity with a model-based optimization process for instantaneous camera position. The network's flow and disparity predictions are utilized to locate an inlier set of flows and disparities using a RANSAC outlier rejection scheme, which is then employed to determine the relative camera pose in a least squares approach. This pipeline is entirely differentiable, enabling the combination of pose and network outputs as an extra unsupervised training loss to refine the forecasted flows and disparities. Not only does this method allow for the direct regression of relative pose from network outputs, but it also automatically eliminates pixels that contradict the rigid scene assumptions that numerous unsupervised structure from motion techniques impose, such as on independently moving objects. The KITTI dataset is used to evaluate the approach, and it produces state-of-the-art results, even in the presence of challenging independently moving objects.",1
"State-of-the-art methods for video action recognition commonly use an ensemble of two networks: the spatial stream, which takes RGB frames as input, and the temporal stream, which takes optical flow as input. In recent work, both of these streams consist of 3D Convolutional Neural Networks, which apply spatiotemporal filters to the video clip before performing classification. Conceptually, the temporal filters should allow the spatial stream to learn motion representations, making the temporal stream redundant. However, we still see significant benefits in action recognition performance by including an entirely separate temporal stream, indicating that the spatial stream is ""missing"" some of the signal captured by the temporal stream. In this work, we first investigate whether motion representations are indeed missing in the spatial stream of 3D CNNs. Second, we demonstrate that these motion representations can be improved by distillation, by tuning the spatial stream to predict the outputs of the temporal stream, effectively combining both models into a single stream. Finally, we show that our Distilled 3D Network (D3D) achieves performance on par with two-stream approaches, using only a single model and with no need to compute optical flow.",0
"State-of-the-art techniques used for recognizing actions in videos typically involve combining two networks: the spatial stream, which takes RGB frames as input, and the temporal stream, which takes optical flow as input. Both of these streams utilize 3D Convolutional Neural Networks to apply spatiotemporal filters to the video before classification. The temporal filters should theoretically enable the spatial stream to learn motion representations, rendering the temporal stream unnecessary. However, the inclusion of a separate temporal stream still results in significantly improved action recognition performance, indicating that the spatial stream is missing some information that the temporal stream captures. In this study, we explore whether motion representations are indeed missing from the spatial stream of 3D CNNs and demonstrate that these representations can be enhanced through distillation, which involves tuning the spatial stream to predict the outputs of the temporal stream. This results in a Distilled 3D Network (D3D) that performs just as well as two-stream methods, without requiring optical flow computation and utilizing just one model.",1
"Despite the progress within the last decades, weather forecasting is still a challenging and computationally expensive task. Current satellite-based approaches to predict thunderstorms are usually based on the analysis of the observed brightness temperatures in different spectral channels and emit a warning if a critical threshold is reached. Recent progress in data science however demonstrates that machine learning can be successfully applied to many research fields in science, especially in areas dealing with large datasets. We therefore present a new approach to the problem of predicting thunderstorms based on machine learning. The core idea of our work is to use the error of two-dimensional optical flow algorithms applied to images of meteorological satellites as a feature for machine learning models. We interpret that optical flow error as an indication of convection potentially leading to thunderstorms and lightning. To factor in spatial proximity we use various manual convolution steps. We also consider effects such as the time of day or the geographic location. We train different tree classifier models as well as a neural network to predict lightning within the next few hours (called nowcasting in meteorology) based on these features. In our evaluation section we compare the predictive power of the different models and the impact of different features on the classification result. Our results show a high accuracy of 96% for predictions over the next 15 minutes which slightly decreases with increasing forecast period but still remains above 83% for forecasts of up to five hours. The high false positive rate of nearly 6% however needs further investigation to allow for an operational use of our approach.",0
"Although weather forecasting has made progress in recent decades, it remains a challenging and computationally expensive task. Typically, satellite-based approaches to predict thunderstorms rely on analyzing observed brightness temperatures in various spectral channels and issuing a warning if a critical threshold is met. However, recent developments in data science have shown that machine learning can be successfully applied to many scientific fields, especially those dealing with large datasets. Thus, we propose a new approach to predicting thunderstorms using machine learning. Our approach involves using the error of two-dimensional optical flow algorithms applied to meteorological satellite images as a feature for machine learning models. We interpret this optical flow error as an indication of convection that could potentially lead to thunderstorms and lightning. To account for spatial proximity, we use various manual convolution steps, and we also consider factors such as time of day and geographic location. We train different tree classifier models and a neural network to predict lightning within the next few hours (known as nowcasting in meteorology) based on these features. In the evaluation section, we compare the predictive power of different models and the impact of different features on the classification result. Our results show a high accuracy of 96% for predictions over the next 15 minutes, which slightly decreases with increasing forecast period but still remains above 83% for up to five hours. However, the high false positive rate of nearly 6% requires further investigation before our approach can be used operationally.",1
"In this paper, we propose a data-driven visual rhythm prediction method, which overcomes the previous works' deficiency that predictions are made primarily by human-crafted hard rules. In our approach, we first extract features including original frames and their residuals, optical flow, scene change, and body pose. These visual features will be next taken into an end-to-end neural network as inputs. Here we observe that there are some slight misaligning between features over the timeline and assume that this is due to the distinctions between how different features are computed. To solve this problem, the extracted features are aligned by an elaborately designed layer, which can also be applied to other models suffering from mismatched features, and boost performance. Then these aligned features are fed into sequence labeling layers implemented with BiLSTM and CRF to predict the onsets. Due to the lack of existing public training and evaluation set, we experiment on a dataset constructed by ourselves based on professionally edited Music Videos (MVs), and the F1 score of our approach reaches 79.6.",0
"This paper introduces a novel method for predicting visual rhythm using data-driven techniques instead of relying on human-developed hard rules. The method involves extracting various visual features such as original frames, residuals, optical flow, scene changes, and body pose, which are then fed into an end-to-end neural network. However, slight misalignments between these features over time are observed, likely due to differences in their computation. To address this issue, a layer is designed to align the features, which can also be used to improve the performance of other models with mismatched features. The aligned features are then processed using sequence labeling layers with BiLSTM and CRF to predict onsets. To evaluate the method, a dataset of professionally edited music videos is created, and the F1 score achieved by the proposed approach is 79.6, as there is no existing public training and evaluation set.",1
"Exploration bonus derived from the novelty of the states in an environment has become a popular approach to motivate exploration for deep reinforcement learning agents in the past few years. Recent methods such as curiosity-driven exploration usually estimate the novelty of new observations by the prediction errors of their system dynamics models. Due to the capacity limitation of the models and difficulty of performing next-frame prediction, however, these methods typically fail to balance between exploration and exploitation in high-dimensional observation tasks, resulting in the agents forgetting the visited paths and exploring those states repeatedly. Such inefficient exploration behavior causes significant performance drops, especially in large environments with sparse reward signals. In this paper, we propose to introduce the concept of optical flow estimation from the field of computer vision to deal with the above issue. We propose to employ optical flow estimation errors to examine the novelty of new observations, such that agents are able to memorize and understand the visited states in a more comprehensive fashion. We compare our method against the previous approaches in a number of experimental experiments. Our results indicate that the proposed method appears to deliver superior and long-lasting performance than the previous methods. We further provide a set of comprehensive ablative analysis of the proposed method, and investigate the impact of optical flow estimation on the learning curves of the DRL agents.",0
"In recent years, the exploration bonus that stems from the novelty of states in an environment has become a favored way to motivate deep reinforcement learning agents to explore. However, methods like curiosity-driven exploration that estimate the novelty of new observations by predicting errors in the system dynamics models often fail to balance between exploration and exploitation in high-dimensional observation tasks. This leads to agents forgetting visited paths and exploring those states repeatedly, resulting in inefficient exploration and significant performance drops, particularly in large environments with sparse reward signals. To address this issue, we suggest using optical flow estimation, a concept from computer vision, to examine the novelty of new observations. By employing optical flow estimation errors, agents can memorize and understand the visited states more comprehensively. We compare our method against previous approaches in several experimental experiments and find that our proposed method produces superior and long-lasting performance. Additionally, we conduct a comprehensive ablative analysis of the proposed method and investigate the impact of optical flow estimation on the learning curves of DRL agents.",1
This paper describes the design and implementation of a ground-related odometry sensor suitable for micro aerial vehicles. The sensor is based on a ground-facing camera and a single-board Linux-based embedded computer with a multimedia System on a Chip (SoC). The SoC features a hardware video encoder which is used to estimate the optical flow online. The optical flow is then used in combination with a distance sensor to estimate the vehicle's velocity. The proposed sensor is compared to a similar existing solution and evaluated in both indoor and outdoor environments.,0
The article outlines the development and execution of a ground-related odometry sensor appropriate for micro aerial vehicles. The sensor employs a ground-facing camera and a Linux-based embedded computer with a multimedia System on a Chip (SoC). The SoC contains a hardware video encoder that is utilized for estimating online optical flow. The optical flow is then paired with a distance sensor to approximate the velocity of the vehicle. The effectiveness of the new sensor is measured against an existing solution and tested in both indoor and outdoor settings.,1
"In this paper, we propose the use of a semantic image, an improved representation for video analysis, principally in combination with Inception networks. The semantic image is obtained by applying localized sparse segmentation using global clustering (LSSGC) prior to the approximate rank pooling which summarizes the motion characteristics in single or multiple images. It incorporates the background information by overlaying a static background from the window onto the subsequent segmented frames. The idea is to improve the action-motion dynamics by focusing on the region which is important for action recognition and encoding the temporal variances using the frame ranking method. We also propose the sequential combination of Inception-ResNetv2 and long-short-term memory network (LSTM) to leverage the temporal variances for improved recognition performance. Extensive analysis has been carried out on UCF101 and HMDB51 datasets which are widely used in action recognition studies. We show that (i) the semantic image generates better activations and converges faster than its original variant, (ii) using segmentation prior to approximate rank pooling yields better recognition performance, (iii) The use of LSTM leverages the temporal variance information from approximate rank pooling to model the action behavior better than the base network, (iv) the proposed representations can be adaptive as they can be used with existing methods such as temporal segment networks to improve the recognition performance, and (v) our proposed four-stream network architecture comprising of semantic images and semantic optical flows achieves state-of-the-art performance, 95.9% and 73.5% recognition accuracy on UCF101 and HMDB51, respectively.",0
"This paper suggests a semantic image as an improved representation for video analysis, particularly when combined with Inception networks. To obtain the semantic image, localized sparse segmentation using global clustering (LSSGC) is applied before approximate rank pooling, which summarizes motion characteristics in single or multiple images. The semantic image includes background information by overlaying a static background from the window onto the segmented frames. The aim is to improve action-motion dynamics by focusing on the region essential for action recognition and encoding temporal variances using the frame ranking method. The paper also proposes combining Inception-ResNetv2 and long-short-term memory network (LSTM) sequentially to leverage temporal variances for better recognition performance. The study analyzed the UCF101 and HMDB51 datasets commonly used in action recognition research. The semantic image generates better activations and converges faster than the original variant, and using segmentation before approximate rank pooling improves recognition performance. Additionally, the use of LSTM models action behavior better than the base network by leveraging temporal variance information from approximate rank pooling. The proposed representations can be adaptive and used with existing methods, such as temporal segment networks, to enhance recognition performance. Finally, a four-stream network architecture comprising semantic images and semantic optical flows achieves state-of-the-art performance with 95.9% and 73.5% recognition accuracy on UCF101 and HMDB51, respectively.",1
"Visual SLAM shows significant progress in recent years due to high attention from vision community but still, challenges remain for low-textured environments. Feature based visual SLAMs do not produce reliable camera and structure estimates due to insufficient features in a low-textured environment. Moreover, existing visual SLAMs produce partial reconstruction when the number of 3D-2D correspondences is insufficient for incremental camera estimation using bundle adjustment. This paper presents Edge SLAM, a feature based monocular visual SLAM which mitigates the above mentioned problems. Our proposed Edge SLAM pipeline detects edge points from images and tracks those using optical flow for point correspondence. We further refine these point correspondences using geometrical relationship among three views. Owing to our edge-point tracking, we use a robust method for two-view initialization for bundle adjustment. Our proposed SLAM also identifies the potential situations where estimating a new camera into the existing reconstruction is becoming unreliable and we adopt a novel method to estimate the new camera reliably using a local optimization technique. We present an extensive evaluation of our proposed SLAM pipeline with most popular open datasets and compare with the state-of-the art. Experimental result indicates that our Edge SLAM is robust and works reliably well for both textured and less-textured environment in comparison to existing state-of-the-art SLAMs.",0
"Despite significant progress in recent years and high attention from the vision community, Visual SLAM still faces challenges in low-textured environments. Feature-based visual SLAMs struggle to produce reliable camera and structure estimates due to insufficient features in these environments. Additionally, existing visual SLAMs only produce partial reconstructions when the number of 3D-2D correspondences is insufficient for incremental camera estimation using bundle adjustment. To address these issues, this paper introduces Edge SLAM, a feature-based monocular visual SLAM that detects edge points from images and tracks them using optical flow for point correspondence. The proposed pipeline also refines point correspondences using geometrical relationships among three views and uses a robust method for two-view initialization for bundle adjustment. Furthermore, the proposed SLAM identifies potential situations where estimating a new camera into the existing reconstruction is becoming unreliable and adopts a novel method to estimate the new camera reliably using a local optimization technique. An extensive evaluation of the proposed SLAM pipeline with popular open datasets demonstrates its robustness and reliability in both textured and less-textured environments, outperforming existing state-of-the-art SLAMs.",1
"Convolutional Neural Networks (CNN) are successfully used for various visual perception tasks including bounding box object detection, semantic segmentation, optical flow, depth estimation and visual SLAM. Generally these tasks are independently explored and modeled. In this paper, we present a joint multi-task network design for learning object detection and semantic segmentation simultaneously. The main motivation is to achieve real-time performance on a low power embedded SOC by sharing of encoder for both the tasks. We construct an efficient architecture using a small ResNet10 like encoder which is shared for both decoders. Object detection uses YOLO v2 like decoder and semantic segmentation uses FCN8 like decoder. We evaluate the proposed network in two public datasets (KITTI, Cityscapes) and in our private fisheye camera dataset, and demonstrate that joint network provides the same accuracy as that of separate networks. We further optimize the network to achieve 30 fps for 1280x384 resolution image.",0
"Various visual perception tasks, such as bounding box object detection, semantic segmentation, optical flow, depth estimation, and visual SLAM, have been successfully accomplished using Convolutional Neural Networks (CNN). Typically, these tasks are explored and modeled independently. However, this paper introduces a joint multi-task network design that learns to perform object detection and semantic segmentation simultaneously. The primary aim is to achieve real-time performance on a low power embedded SOC by sharing the encoder for both tasks. By utilizing a small ResNet10-like encoder, we construct an efficient architecture that shares the encoder for both decoders. YOLO v2-like decoder is used for object detection, and FCN8-like decoder is used for semantic segmentation. The proposed network is evaluated on two public datasets, KITTI and Cityscapes, and on our private fisheye camera dataset. The results demonstrate that the joint network provides the same accuracy as separate networks. Furthermore, we optimize the network to attain 30 fps for 1280x384 resolution image.",1
"Motion is a dominant cue in automated driving systems. Optical flow is typically computed to detect moving objects and to estimate depth using triangulation. In this paper, our motivation is to leverage the existing dense optical flow to improve the performance of semantic segmentation. To provide a systematic study, we construct four different architectures which use RGB only, flow only, RGBF concatenated and two-stream RGB + flow. We evaluate these networks on two automotive datasets namely Virtual KITTI and Cityscapes using the state-of-the-art flow estimator FlowNet v2. We also make use of the ground truth optical flow in Virtual KITTI to serve as an ideal estimator and a standard Farneback optical flow algorithm to study the effect of noise. Using the flow ground truth in Virtual KITTI, two-stream architecture achieves the best results with an improvement of 4% IoU. As expected, there is a large improvement for moving objects like trucks, vans and cars with 38%, 28% and 6% increase in IoU. FlowNet produces an improvement of 2.4% in average IoU with larger improvement in the moving objects corresponding to 26%, 11% and 5% in trucks, vans and cars. In Cityscapes, flow augmentation provided an improvement for moving objects like motorcycle and train with an increase of 17% and 7% in IoU.",0
"Automated driving systems rely heavily on motion as a key factor. To detect moving objects and estimate depth through triangulation, optical flow is typically calculated. This study aims to enhance the performance of semantic segmentation by utilizing the existing dense optical flow. Four distinct architectures were created for this purpose: RGB only, flow only, RGBF concatenated, and two-stream RGB + flow. The networks were evaluated on two automotive datasets, Virtual KITTI and Cityscapes, using the advanced flow estimator FlowNet v2. The ground truth optical flow was utilized in Virtual KITTI as an ideal estimator, while the standard Farneback optical flow algorithm was used to investigate the effect of noise. The two-stream architecture produced the best results with a 4% IoU improvement, particularly for moving objects like trucks, vans, and cars with a 38%, 28%, and 6% increase in IoU respectively. FlowNet enhanced the average IoU by 2.4%, with a greater improvement in moving vehicles, corresponding to 26%, 11%, and 5% in trucks, vans, and cars. In Cityscapes, flow augmentation improved the IoU of moving objects such as motorcycles and trains by 17% and 7% respectively.",1
"We propose a new self-supervised approach to image feature learning from motion cue. This new approach leverages recent advances in deep learning in two directions: 1) the success of training deep neural network in estimating optical flow in real data using synthetic flow data; and 2) emerging work in learning image features from motion cues, such as optical flow. Building on these, we demonstrate that image features can be learned in self-supervision by first training an optical flow estimator with synthetic flow data, and then learning image features from the estimated flows in real motion data. We demonstrate and evaluate this approach on an image segmentation task. Using the learned image feature representation, the network performs significantly better than the ones trained from scratch in few-shot segmentation tasks.",0
"Our proposal introduces a novel method for self-supervised image feature learning based on motion cues. This method incorporates recent advancements in deep learning by leveraging two key areas: 1) the successful training of deep neural networks in estimating optical flow using synthetic data, and 2) the emerging research on learning image features from motion cues, particularly optical flow. By building on these areas, we show that image features can be learned through self-supervision by first training an optical flow estimator with synthetic data, and subsequently learning image features from estimated flows in real motion data. Our approach is evaluated through an image segmentation task and demonstrates superior performance compared to networks trained from scratch in few-shot segmentation tasks, thanks to the learned image feature representation.",1
"We present a system for learning motion of independently moving objects from stereo videos. The only human annotation used in our system are 2D object bounding boxes which introduce the notion of objects to our system. Unlike prior learning based work which has focused on predicting dense pixel-wise optical flow field and/or a depth map for each image, we propose to predict object instance specific 3D scene flow maps and instance masks from which we are able to derive the motion direction and speed for each object instance. Our network takes the 3D geometry of the problem into account which allows it to correlate the input images. We present experiments evaluating the accuracy of our 3D flow vectors, as well as depth maps and projected 2D optical flow where our jointly learned system outperforms earlier approaches trained for each task independently.",0
"Our system leverages stereo videos to learn the motion of independently moving objects with minimal human annotation. Instead of predicting dense pixel-wise optical flow or depth maps for each image, we predict object instance specific 3D scene flow maps and instance masks. These maps enable us to determine the motion direction and speed for each object instance. Our network incorporates the 3D geometry of the problem, which allows it to establish correlations between input images. We conducted experiments to assess the accuracy of our 3D flow vectors, depth maps, and projected 2D optical flow. Our jointly learned system outperforms earlier approaches trained for each task independently.",1
"Advanced video classification systems decode video frames to derive the necessary texture and motion representations for ingestion and analysis by spatio-temporal deep convolutional neural networks (CNNs). However, when considering visual Internet-of-Things applications, surveillance systems and semantic crawlers of large video repositories, the video capture and the CNN-based semantic analysis parts do not tend to be co-located. This necessitates the transport of compressed video over networks and incurs significant overhead in bandwidth and energy consumption, thereby significantly undermining the deployment potential of such systems. In this paper, we investigate the trade-off between the encoding bitrate and the achievable accuracy of CNN-based video classification models that directly ingest AVC/H.264 and HEVC encoded videos. Instead of retaining entire compressed video bitstreams and applying complex optical flow calculations prior to CNN processing, we only retain motion vector and select texture information at significantly-reduced bitrates and apply no additional processing prior to CNN ingestion. Based on three CNN architectures and two action recognition datasets, we achieve 11%-94% saving in bitrate with marginal effect on classification accuracy. A model-based selection between multiple CNNs increases these savings further, to the point where, if up to 7% loss of accuracy can be tolerated, video classification can take place with as little as 3 kbps for the transport of the required compressed video information to the system implementing the CNN models.",0
"Sophisticated video classification systems extract texture and motion data from video frames and feed it into spatio-temporal deep convolutional neural networks (CNNs) for analysis. However, for visual Internet-of-Things applications, surveillance systems, and semantic crawlers of large video repositories, the video capture and CNN-based semantic analysis are often not located together. This results in the need to transmit compressed video over networks, which consumes significant bandwidth and energy, and limits deployment potential. This study examines the trade-off between encoding bitrate and CNN-based video classification model accuracy, by directly ingesting AVC/H.264 and HEVC encoded videos. Rather than retaining entire compressed video bitstreams, we only retain motion vector and select texture information at significantly reduced bitrates, with no additional processing prior to CNN ingestion. We achieve 11%-94% bitrate savings with marginal effect on classification accuracy, based on three CNN architectures and two action recognition datasets. By selecting from multiple CNNs, we can further increase savings to the point where, if up to 7% loss of accuracy can be tolerated, video classification can take place with as little as 3 kbps for the transport of the required compressed video information to the system implementing the CNN models.",1
"Dynamic imaging is a recently proposed action description paradigm for simultaneously capturing motion and temporal evolution information, particularly in the context of deep convolutional neural networks (CNNs). Compared with optical flow for motion characterization, dynamic imaging exhibits superior efficiency and compactness. Inspired by the success of dynamic imaging in RGB video, this study extends it to the depth domain. To better exploit three-dimensional (3D) characteristics, multi-view dynamic images are proposed. In particular, the raw depth video is densely projected with respect to different virtual imaging viewpoints by rotating the virtual camera within the 3D space. Subsequently, dynamic images are extracted from the obtained multi-view depth videos and multi-view dynamic images are thus constructed from these images. Accordingly, more view-tolerant visual cues can be involved. A novel CNN model is then proposed to perform feature learning on multi-view dynamic images. Particularly, the dynamic images from different views share the same convolutional layers but correspond to different fully connected layers. This is aimed at enhancing the tuning effectiveness on shallow convolutional layers by alleviating the gradient vanishing problem. Moreover, as the spatial occurrence variation of the actions may impair the CNN, an action proposal approach is also put forth. In experiments, the proposed approach can achieve state-of-the-art performance on three challenging datasets.",0
"Recently, dynamic imaging has been proposed as a method to capture motion and temporal evolution information simultaneously, particularly in deep convolutional neural networks (CNNs). Compared to optical flow, dynamic imaging is more efficient and compact. This study extends dynamic imaging to the depth domain, using multi-view dynamic images to better exploit three-dimensional (3D) characteristics. The raw depth video is projected from different virtual imaging viewpoints, and dynamic images are extracted from the obtained multi-view depth videos. A novel CNN model is proposed for feature learning on multi-view dynamic images, with different fully connected layers for different views. An action proposal approach is also proposed to address spatial occurrence variation. Experiments show that this approach achieves state-of-the-art performance on three challenging datasets.",1
"Recognizing actions in ice hockey using computer vision poses challenges due to bulky equipment and inadequate image quality. A novel two-stream framework has been designed to improve action recognition accuracy for hockey using three main components. First, pose is estimated via the Part Affinity Fields model to extract meaningful cues from the player. Second, optical flow (using LiteFlowNet) is used to extract temporal features. Third, pose and optical flow streams are fused and passed to fully-connected layers to estimate the hockey player's action. A novel publicly available dataset named HARPET (Hockey Action Recognition Pose Estimation, Temporal) was created, composed of sequences of annotated actions and pose of hockey players including their hockey sticks as an extension of human body pose. Three contributions are recognized. (1) The novel two-stream architecture achieves 85% action recognition accuracy, with the inclusion of optical flows increasing accuracy by about 10%. (2) The unique localization of hand-held objects (e.g., hockey sticks) as part of pose increases accuracy by about 13%. (3) For pose estimation, a bigger and more general dataset, MSCOCO, is successfully used for transfer learning to a smaller and more specific dataset, HARPET, achieving a PCKh of 87%.",0
"It is difficult to use computer vision to identify actions in ice hockey due to the players' bulky equipment and poor image quality. To address this issue, a new two-stream framework has been developed to enhance the accuracy of action recognition in hockey using three primary components. Firstly, meaningful cues are extracted from the players by estimating the pose using the Part Affinity Fields model. Secondly, temporal features are extracted using optical flow (LiteFlowNet). Finally, the pose and optical flow streams are combined and passed to fully-connected layers to estimate the player's action. A new publicly available dataset called HARPET (Hockey Action Recognition Pose Estimation, Temporal) has been created, which includes annotated action sequences and player poses, including hockey sticks as an extension of human body pose. Three contributions are recognized: (1) the novel two-stream architecture achieves 85% action recognition accuracy, with the inclusion of optical flows increasing accuracy by about 10%; (2) the unique localization of hand-held objects, such as hockey sticks, as part of pose increases accuracy by about 13%; and (3) transfer learning from a larger and more general dataset, MSCOCO, to a smaller and more specific dataset, HARPET, successfully achieves a PCKh of 87% for pose estimation.",1
"Three-dimensional (3D) biomedical image sets are often acquired with in-plane pixel spacings that are far less than the out-of-plane spacings between images. The resultant anisotropy, which can be detrimental in many applications, can be decreased using image interpolation. Optical flow and/or other registration-based interpolators have proven useful in such interpolation roles in the past. When acquired images are comprised of signals that describe the flow velocity of fluids, additional information is available to guide the interpolation process. In this paper, we present an optical-flow based framework for image interpolation that also minimizes resultant divergence in the interpolated data.",0
"In many cases, 3D biomedical image sets have in-plane pixel spacings much smaller than the out-of-plane spacings between images, resulting in anisotropy that can be problematic. One solution is to use image interpolation, which can be accomplished using optical flow or registration-based interpolators. When the images are made up of signals that describe fluid flow velocity, there is additional information available to aid in the interpolation process. This paper introduces an optical-flow based approach for image interpolation that reduces divergence in the interpolated data.",1
"Optical flow estimation can be formulated as an end-to-end supervised learning problem, which yields estimates with a superior accuracy-runtime tradeoff compared to alternative methodology. In this paper, we make such networks estimate their local uncertainty about the correctness of their prediction, which is vital information when building decisions on top of the estimations. For the first time we compare several strategies and techniques to estimate uncertainty in a large-scale computer vision task like optical flow estimation. Moreover, we introduce a new network architecture utilizing the Winner-Takes-All loss and show that this can provide complementary hypotheses and uncertainty estimates efficiently with a single forward pass and without the need for sampling or ensembles. Finally, we demonstrate the quality of the different uncertainty estimates, which is clearly above previous confidence measures on optical flow and allows for interactive frame rates.",0
"The accuracy-runtime tradeoff of alternative methods is inferior to that of end-to-end supervised learning for optical flow estimation. The estimation of local uncertainty is crucial for decision-making based on predictions. This paper compares various strategies and techniques for estimating uncertainty in large-scale computer vision tasks such as optical flow estimation for the first time. Additionally, a new network architecture is introduced that utilizes the Winner-Takes-All loss and efficiently provides complementary hypotheses and uncertainty estimates with a single forward pass, without the need for sampling or ensembles. Finally, the quality of the uncertainty estimates is demonstrated to be significantly higher than previous confidence measures on optical flow, enabling interactive frame rates.",1
"In this work, we propose a novel framework for unsupervised learning for event cameras that learns motion information from only the event stream. In particular, we propose an input representation of the events in the form of a discretized volume that maintains the temporal distribution of the events, which we pass through a neural network to predict the motion of the events. This motion is used to attempt to remove any motion blur in the event image. We then propose a loss function applied to the motion compensated event image that measures the motion blur in this image. We train two networks with this framework, one to predict optical flow, and one to predict egomotion and depths, and evaluate these networks on the Multi Vehicle Stereo Event Camera dataset, along with qualitative results from a variety of different scenes.",0
"Our work introduces a new approach for unsupervised learning with event cameras that focuses on extracting motion information solely from the event stream. Our proposed method involves representing events as a discretized volume that preserves their temporal distribution. This input representation is then fed into a neural network to predict event motion and eliminate any motion blur in the event image. In addition, we introduce a loss function that assesses the level of motion blur in the motion-compensated event image. We train two networks using this framework: one for predicting optical flow, and the other for predicting egomotion and depths. Finally, we evaluate these networks on the Multi Vehicle Stereo Event Camera dataset and present a range of qualitative results from various scenes.",1
"Image warping is a necessary step in many multimedia applications such as texture mapping, image-based rendering, panorama stitching, image resizing and optical flow computation etc. Traditionally, color image warping interpolation is performed in each color channel independently. In this paper, we show that the warping quality can be significantly enhanced by exploiting the cross-channel correlation. We design a warping scheme that integrates intra-channel interpolation with cross-channel variation at very low computational cost, which is required for interactive multimedia applications on mobile devices. The effectiveness and efficiency of our method are validated by extensive experiments.",0
"Many multimedia applications, including texture mapping, image-based rendering, panorama stitching, image resizing, and optical flow computation, require image warping. Typically, color image warping interpolation is performed in each color channel separately. This article presents a new approach that takes advantage of the cross-channel correlation to improve the quality of warping. We propose a warping scheme that combines intra-channel interpolation with cross-channel variation, which is highly efficient and suitable for interactive multimedia applications on mobile devices. We conducted numerous experiments to demonstrate the effectiveness and efficiency of our method.",1
"A safe and robust on-road navigation system is a crucial component of achieving fully automated vehicles. NVIDIA recently proposed an End-to-End algorithm that can directly learn steering commands from raw pixels of a front camera by using one convolutional neural network. In this paper, we leverage auxiliary information aside from raw images and design a novel network structure, called Auxiliary Task Network (ATN), to help boost the driving performance while maintaining the advantage of minimal training data and an End-to-End training method. In this network, we introduce human prior knowledge into vehicle navigation by transferring features from image recognition tasks. Image semantic segmentation is applied as an auxiliary task for navigation. We consider temporal information by introducing an LSTM module and optical flow to the network. Finally, we combine vehicle kinematics with a sensor fusion step. We discuss the benefits of our method over state-of-the-art visual navigation methods both in the Udacity simulation environment and on the real-world Comma.ai dataset.",0
"To achieve fully automated vehicles, it is essential to have a secure and robust on-road navigation system. NVIDIA has proposed an End-to-End algorithm that allows learning of steering commands directly from raw pixels of a front camera using a convolutional neural network. In this study, we introduce Auxiliary Task Network (ATN), a novel network structure that incorporates additional information besides raw images to enhance driving performance while preserving the advantages of minimal training data and End-to-End learning. Our network utilizes human prior knowledge in vehicle navigation by transferring features from image recognition tasks and incorporating image semantic segmentation as an auxiliary task. We also incorporate temporal information using an LSTM module and optical flow in the network. Lastly, we integrate vehicle kinematics with sensor fusion. We demonstrate the effectiveness of our approach compared to state-of-the-art visual navigation methods in both the Udacity simulation environment and the Comma.ai dataset in real-world scenarios.",1
"We propose a methodology to extend the concept of Two-Stream Convolutional Networks to perform end-to-end learning for self-driving cars with temporal cues. The system has the ability to learn spatiotemporal features by simultaneously mapping raw images and pre-calculated optical flows directly to steering commands. Although optical flows encode temporal-rich information, we found that 2D-CNNs are prone to capturing features only as spatial representations. We show how the use of Multitask Learning favors the learning of temporal features via inductive transfer from a shared spatiotemporal representation. Preliminary results demonstrate a competitive improvement of 30% in prediction accuracy and stability compared to widely used regression methods trained on the Comma.ai dataset.",0
"Our proposed methodology aims to expand the Two-Stream Convolutional Networks' capabilities by introducing end-to-end learning for self-driving cars with temporal cues. By concurrently mapping raw images and pre-calculated optical flows to steering commands, our system can learn spatiotemporal features. However, we discovered that 2D-CNNs tend to capture features as spatial representations, despite optical flows encoding rich temporal information. To address this, we utilized Multitask Learning to promote the learning of temporal features through inductive transfer from a shared spatiotemporal representation. Our preliminary results indicate that our approach outperforms widely used regression methods trained on the Comma.ai dataset, exhibiting a competitive improvement of 30% in prediction accuracy and stability.",1
"Anomaly detection is a challenging problem in intelligent video surveillance. Most existing methods are computation consuming, which cannot satisfy the real-time requirement. In this paper, we propose a real-time anomaly detection framework with low computational complexity and high efficiency. A new feature, named Histogram of Magnitude Optical Flow (HMOF), is proposed to capture the motion of video patches. Compared with existing feature descriptors, HMOF is more sensitive to motion magnitude and more efficient to distinguish anomaly information. The HMOF features are computed for foreground patches, and are reconstructed by the auto-encoder for better clustering. Then, we use Gaussian Mixture Model (GMM) Classifiers to distinguish anomalies from normal activities in videos. Experimental results show that our framework outperforms state-of-the-art methods, and can reliably detect anomalies in real-time.",0
"Detecting anomalies in intelligent video surveillance is a complex task that poses several challenges. Currently available methods are computationally demanding and unable to meet real-time requirements. This study proposes a framework for detecting anomalies in real-time with high efficiency and low computational complexity. A new feature, called Histogram of Magnitude Optical Flow (HMOF), is introduced to capture video patch motion. HMOF is more sensitive to motion magnitude and more effective in distinguishing anomaly information than existing feature descriptors. HMOF features are computed for foreground patches and then reconstructed by the auto-encoder for better clustering. Gaussian Mixture Model (GMM) Classifiers are used to differentiate anomalies from normal activities in videos. Experimental results demonstrate that the proposed framework outperforms state-of-the-art methods and can detect anomalies reliably in real-time.",1
"Video super-resolution (VSR) aims to restore a photo-realistic high-resolution (HR) video frame from both its corresponding low-resolution (LR) frame (reference frame) and multiple neighboring frames (supporting frames). Due to varying motion of cameras or objects, the reference frame and each support frame are not aligned. Therefore, temporal alignment is a challenging yet important problem for VSR. Previous VSR methods usually utilize optical flow between the reference frame and each supporting frame to wrap the supporting frame for temporal alignment. Therefore, the performance of these image-level wrapping-based models will highly depend on the prediction accuracy of optical flow, and inaccurate optical flow will lead to artifacts in the wrapped supporting frames, which also will be propagated into the reconstructed HR video frame. To overcome the limitation, in this paper, we propose a temporal deformable alignment network (TDAN) to adaptively align the reference frame and each supporting frame at the feature level without computing optical flow. The TDAN uses features from both the reference frame and each supporting frame to dynamically predict offsets of sampling convolution kernels. By using the corresponding kernels, TDAN transforms supporting frames to align with the reference frame. To predict the HR video frame, a reconstruction network taking aligned frames and the reference frame is utilized. Experimental results demonstrate the effectiveness of the proposed TDAN-based VSR model.",0
"Video super-resolution (VSR) is a technique that aims to generate a high-resolution (HR) video frame that looks realistic from a low-resolution (LR) reference frame and multiple supporting frames. However, due to the varying motion of cameras or objects, it is challenging to align the reference frame and each supporting frame in time. Prior VSR methods have used optical flow to wrap the supporting frames for temporal alignment. Nevertheless, inaccurate optical flow can cause artifacts in the wrapped supporting frames, which can propagate into the reconstructed HR video frame. To address this issue, we propose a temporal deformable alignment network (TDAN) that adaptively aligns the reference frame and each supporting frame at the feature level without computing optical flow. The TDAN uses features from both the reference frame and each supporting frame to dynamically predict offsets of sampling convolution kernels. These kernels are then used to align the supporting frames with the reference frame. To generate the HR video frame, a reconstruction network that takes the aligned frames and the reference frame is utilized. Our experimental results show that the proposed TDAN-based VSR model is effective.",1
"The field of automatic video generation has received a boost thanks to the recent Generative Adversarial Networks (GANs). However, most existing methods cannot control the contents of the generated video using a text caption, losing their usefulness to a large extent. This particularly affects human videos due to their great variety of actions and appearances. This paper presents Conditional Flow and Texture GAN (CFT-GAN), a GAN-based video generation method from action-appearance captions. We propose a novel way of generating video by encoding a caption (e.g., ""a man in blue jeans is playing golf"") in a two-stage generation pipeline. Our CFT-GAN uses such caption to generate an optical flow (action) and a texture (appearance) for each frame. As a result, the output video reflects the content specified in the caption in a plausible way. Moreover, to train our method, we constructed a new dataset for human video generation with captions. We evaluated the proposed method qualitatively and quantitatively via an ablation study and a user study. The results demonstrate that CFT-GAN is able to successfully generate videos containing the action and appearances indicated in the captions.",0
"Recent advancements in Generative Adversarial Networks (GANs) have greatly improved the field of automatic video generation. However, many existing techniques lack the ability to control the contents of generated videos using text captions, limiting their usefulness, especially for human videos. To address this issue, we introduce Conditional Flow and Texture GAN (CFT-GAN), a GAN-based video generation method that utilizes action-appearance captions. Our method generates videos by encoding captions (e.g., ""a man in blue jeans is playing golf"") in a two-stage generation pipeline. CFT-GAN generates optical flow and texture for each frame based on the caption, resulting in a video that reflects the caption content in a plausible manner. To train our method, we created a new dataset for human video generation with captions. We evaluated our approach using an ablation study and a user study, which showed that CFT-GAN successfully generates videos that accurately reflect the actions and appearances specified in the captions.",1
"Abnormal driving behaviour is one of the leading cause of terrible traffic accidents endangering human life. Therefore, study on driving behaviour surveillance has become essential to traffic security and public management. In this paper, we conduct this promising research and employ a two stream CNN framework for video-based driving behaviour recognition, in which spatial stream CNN captures appearance information from still frames, whilst temporal stream CNN captures motion information with pre-computed optical flow displacement between a few adjacent video frames. We investigate different spatial-temporal fusion strategies to combine the intra frame static clues and inter frame dynamic clues for final behaviour recognition. So as to validate the effectiveness of the designed spatial-temporal deep learning based model, we create a simulated driving behaviour dataset, containing 1237 videos with 6 different driving behavior for recognition. Experiment result shows that our proposed method obtains noticeable performance improvements compared to the existing methods.",0
"The danger of traffic accidents caused by abnormal driving behavior necessitates the need for research on driving behavior surveillance to maintain traffic security and public management. This study explores the use of a two stream CNN framework for recognizing driving behavior based on video footage. The spatial stream CNN captures appearance information from still frames, while the temporal stream CNN identifies motion information using pre-computed optical flow displacement between adjacent video frames. We examine various spatial-temporal fusion techniques to integrate intra frame static and inter frame dynamic clues for accurate recognition of driving behavior. To evaluate the efficacy of the spatial-temporal deep learning-based model, we developed a simulated driving behavior dataset comprising 1237 videos featuring six different driving behaviors. Our proposed method outperformed existing methods according to experimental results.",1
"First-person (egocentric) and third person (exocentric) videos are drastically different in nature. The relationship between these two views have been studied in recent years, however, it has yet to be fully explored. In this work, we introduce two datasets (synthetic and natural/real) containing simultaneously recorded egocentric and exocentric videos. We also explore relating the two domains (egocentric and exocentric) in two aspects. First, we synthesize images in the egocentric domain from the exocentric domain using a conditional generative adversarial network (cGAN). We show that with enough training data, our network is capable of hallucinating how the world would look like from an egocentric perspective, given an exocentric video. Second, we address the cross-view retrieval problem across the two views. Given an egocentric query frame (or its momentary optical flow), we retrieve its corresponding exocentric frame (or optical flow) from a gallery set. We show that using synthetic data could be beneficial in retrieving real data. We show that performing domain adaptation from the synthetic domain to the natural/real domain, is helpful in tasks such as retrieval. We believe that the presented datasets and the proposed baselines offer new opportunities for further research in this direction. The code and dataset are publicly available.",0
"The nature of first-person (egocentric) and third person (exocentric) videos is significantly different, and while their relationship has been studied in recent years, there is still much to explore. This study introduces two datasets, one synthetic and one natural/real, that contain simultaneously recorded egocentric and exocentric videos. The study explores two aspects of relating the two domains: first, synthesizing egocentric images from exocentric ones using a conditional generative adversarial network (cGAN), and second, addressing the cross-view retrieval problem across the two views. The study demonstrates that synthetic data can aid in retrieving real data and that domain adaptation from the synthetic domain to the natural/real domain can help with tasks such as retrieval. The study presents new opportunities for further research in this area, and the code and dataset are publicly available.",1
"To date, top-performing optical flow estimation methods only take pairs of consecutive frames into account. While elegant and appealing, the idea of using more than two frames has not yet produced state-of-the-art results. We present a simple, yet effective fusion approach for multi-frame optical flow that benefits from longer-term temporal cues. Our method first warps the optical flow from previous frames to the current, thereby yielding multiple plausible estimates. It then fuses the complementary information carried by these estimates into a new optical flow field. At the time of writing, our method ranks first among published results in the MPI Sintel and KITTI 2015 benchmarks. Our models will be available on https://github.com/NVlabs/PWC-Net.",0
"Until now, optical flow estimation methods that perform the best only consider consecutive frame pairs. The concept of using more than two frames has yet to produce leading results, despite its attractiveness and sophistication. However, we introduce a straightforward but efficient fusion technique for multi-frame optical flow that exploits long-term temporal cues. Initially, our approach warps the optical flow from prior frames to the present, resulting in several plausible predictions. Then, it combines the complementary information conveyed by these forecasts to generate a new optical flow field. Currently, our approach is ranked first among published results in the MPI Sintel and KITTI 2015 benchmarks. Our models will be accessible on https://github.com/NVlabs/PWC-Net.",1
"The success of deep learning research has catapulted deep models into production systems that our society is becoming increasingly dependent on, especially in the image and video domains. However, recent work has shown that these largely uninterpretable models exhibit glaring security vulnerabilities in the presence of an adversary. In this work, we develop a powerful untargeted adversarial attack for action recognition systems in both white-box and black-box settings. Action recognition models differ from image-classification models in that their inputs contain a temporal dimension, which we explicitly target in the attack. Drawing inspiration from image classifier attacks, we create new attacks which achieve state-of-the-art success rates on a two-stream classifier trained on the UCF-101 dataset. We find that our attacks can significantly degrade a model's performance with sparsely and imperceptibly perturbed examples. We also demonstrate the transferability of our attacks to black-box action recognition systems.",0
"The rise of deep learning research has led to the integration of deep models into essential industrial systems, particularly in the fields of image and video processing. However, these models are largely incomprehensible and have been shown to possess severe security weaknesses when faced with an adversary. This study focuses on developing a potent untargeted adversarial attack for action recognition systems, both in white-box and black-box settings. Action recognition models differ from image classification models in that they contain a temporal dimension, which we specifically target in the attack. Our attacks, inspired by image classifier attacks, produce outstanding success rates on a two-stream classifier trained on the UCF-101 dataset. We discover that our attacks can significantly impair a model's performance by introducing sparsely and inconspicuously perturbed examples. Additionally, we prove the transferability of our attacks to black-box action recognition systems.",1
"Convolutional networks optimized for accuracy on challenging, dense prediction tasks are prohibitively slow to run on each frame in a video. The spatial similarity of nearby video frames, however, suggests opportunity to reuse computation. Existing work has explored basic feature reuse and feature warping based on optical flow, but has encountered limits to the speedup attainable with these techniques. In this paper, we present a new, two part approach to accelerating inference on video. First, we propose a fast feature propagation technique that utilizes the block motion vectors present in compressed video (e.g. H.264 codecs) to cheaply propagate features from frame to frame. Second, we develop a novel feature estimation scheme, termed feature interpolation, that fuses features propagated from enclosing keyframes to render accurate feature estimates, even at sparse keyframe frequencies. We evaluate our system on the Cityscapes and CamVid datasets, comparing to both a frame-by-frame baseline and related work. We find that we are able to substantially accelerate segmentation on video, achieving near real-time frame rates (20.1 frames per second) on large images (960 x 720 pixels), while maintaining competitive accuracy. This represents an improvement of almost 6x over the single-frame baseline and 2.5x over the fastest prior work.",0
"Convolutional networks designed to deliver accurate results on complex, dense prediction tasks are too slow to process each frame of a video. However, the similarity between adjacent video frames presents an opportunity to reuse computations. Previous research has examined basic feature reuse and feature warping based on optical flow, but these techniques have limitations in terms of the achievable speedup. This paper introduces a two-part approach to accelerating inference on video. Firstly, we propose a fast feature propagation technique that uses block motion vectors found in compressed video, such as H.264 codecs, to cheaply propagate features from one frame to the next. Secondly, we develop a novel feature estimation strategy called feature interpolation, which blends features propagated from enclosing keyframes to produce accurate feature estimates, even at sparse keyframe frequencies. We evaluate our system on the Cityscapes and CamVid datasets, comparing it to both a frame-by-frame baseline and related research. Our approach achieves near real-time frame rates (20.1 frames per second) on large images (960 x 720 pixels), while maintaining competitive accuracy, representing a 6x improvement over the single-frame baseline and a 2.5x improvement over the fastest prior work.",1
"Understanding the world around us and making decisions about the future is a critical component to human intelligence. As autonomous systems continue to develop, their ability to reason about the future will be the key to their success. Semantic anticipation is a relatively under-explored area for which autonomous vehicles could take advantage of (e.g., forecasting pedestrian trajectories). Motivated by the need for real-time prediction in autonomous systems, we propose to decompose the challenging semantic forecasting task into two subtasks: current frame segmentation and future optical flow prediction. Through this decomposition, we built an efficient, effective, low overhead model with three main components: flow prediction network, feature-flow aggregation LSTM, and end-to-end learnable warp layer. Our proposed method achieves state-of-the-art accuracy on short-term and moving objects semantic forecasting while simultaneously reducing model parameters by up to 95% and increasing efficiency by greater than 40x.",0
"Human intelligence relies on our ability to comprehend the world and make decisions for the future. In the development of autonomous systems, their success will depend on their capacity to anticipate future events. Semantic anticipation is a relatively unexplored area that could benefit autonomous vehicles, such as predicting the trajectories of pedestrians. To address the need for real-time prediction in autonomous systems, we suggest breaking down the complex semantic forecasting task into two subtasks: current frame segmentation and future optical flow prediction. Our approach involves three main components: flow prediction network, feature-flow aggregation LSTM, and end-to-end learnable warp layer. By decomposing the task and implementing our proposed method, we achieved state-of-the-art accuracy in short-term and moving objects semantic forecasting. Additionally, our model reduced parameters by up to 95% and increased efficiency by over 40x.",1
"Real-time motion detection in non-stationary scenes is a difficult task due to dynamic background, changing foreground appearance and limited computational resource. These challenges degrade the performance of the existing methods in practical applications. In this paper, an optical flow based framework is proposed to address this problem. By applying a novel strategy to utilize optical flow, we enable our method being free of model constructing, training or updating and can be performed efficiently. Besides, a dual judgment mechanism with adaptive intervals and adaptive thresholds is designed to heighten the system's adaptation to different situations. In experiment part, we quantitatively and qualitatively validate the effectiveness and feasibility of our method with videos in various scene conditions. The experimental results show that our method adapts itself to different situations and outperforms the state-of-the-art real-time methods, indicating the advantages of our optical flow based method.",0
"Detecting motion in non-stationary scenes in real-time is a complex task due to various factors such as a dynamic background, changing foreground appearance, and limited computational resources. These obstacles can reduce the effectiveness of existing methods in practical applications. To overcome these challenges, this paper proposes an optical flow based framework. The method utilizes a unique strategy that eliminates the need for model construction, training, or updating, making it highly efficient. Additionally, the framework includes a dual judgment mechanism that employs adaptive intervals and thresholds, enabling the system to adapt to different situations. To validate the effectiveness and feasibility of the method, experiments were conducted using videos in various scene conditions. The results demonstrate that our approach outperforms state-of-the-art real-time methods and adapts to different situations, highlighting the advantages of our optical flow based method.",1
"Obtained by moving object detection, the foreground mask result is unshaped and can not be directly used in most subsequent processes. In this paper, we focus on this problem and address it by constructing an optical flow based moving foreground analysis framework. During the processing procedure, the foreground masks are analyzed and segmented through two complementary clustering algorithms. As a result, we obtain the instance-level information like the number, location and size of moving objects. The experimental result show that our method adapts itself to the problem and performs well enough for practical applications.",0
"The outcome of object detection through movement is a shapeless foreground mask that cannot be utilized directly in most subsequent processes. This issue is the primary focus of this paper, which proposes an optical flow-based moving foreground analysis framework to resolve it. The framework analyzes and segments the foreground masks using two complementary clustering algorithms, resulting in instance-level information such as the count, location, and size of moving objects. Experimental results demonstrate that our method effectively addresses the problem and is suitable for practical applications.",1
"The performance of optical flow algorithms greatly depends on the specifics of the content and the application for which it is used. Existing and well established optical flow datasets are limited to rather particular contents from which none is close to crowd behavior analysis; whereas such applications heavily utilize optical flow. We introduce a new optical flow dataset exploiting the possibilities of a recent video engine to generate sequences with ground-truth optical flow for large crowds in different scenarios. We break with the development of the last decade of introducing ever increasing displacements to pose new difficulties. Instead we focus on real-world surveillance scenarios where numerous small, partly independent, non rigidly moving objects observed over a long temporal range pose a challenge. By evaluating different optical flow algorithms, we find that results of established datasets can not be transferred to these new challenges. In exhaustive experiments we are able to provide new insight into optical flow for crowd analysis. Finally, the results have been validated on the real-world UCF crowd tracking benchmark while achieving competitive results compared to more sophisticated state-of-the-art crowd tracking approaches.",0
"The effectiveness of optical flow algorithms is heavily influenced by the content and application they are used for. While there are established datasets for optical flow, they are limited to specific content and do not include crowd behavior analysis, which heavily relies on optical flow. To address this gap, we have created a new optical flow dataset that utilizes a video engine to generate ground-truth optical flow for large crowds in various scenarios. Unlike previous datasets that increased displacements, we focus on real-world surveillance scenarios where small, non-rigidly moving objects pose a challenge. Our exhaustive experiments show that established datasets cannot be used for these new challenges, and we provide new insights into optical flow for crowd analysis. Our results have been validated on the UCF crowd tracking benchmark, and we achieved competitive results compared to state-of-the-art crowd tracking approaches.",1
"Modern optical flow methods are often composed of a cascade of many independent steps or formulated as a black box neural network that is hard to interpret and analyze. In this work we seek for a plain, interpretable, but learnable solution. We propose a novel inpainting based algorithm that approaches the problem in three steps: feature selection and matching, selection of supporting points and energy based inpainting. To facilitate the inference we propose an optimization layer that allows to backpropagate through 10K iterations of a first-order method without any numerical or memory problems. Compared to recent state-of-the-art networks, our modular CNN is very lightweight and competitive with other, more involved, inpainting based methods.",0
"Contemporary optical flow methods are typically made up of multiple self-sufficient stages or structured as a neural network that is difficult to comprehend and investigate. Our objective is to develop an uncomplicated, understandable, and trainable solution. Our proposed method is an innovative inpainting algorithm that tackles the problem in three stages: identifying and matching features, picking supportive points, and energy-based inpainting. To simplify the inference process, we've introduced an optimization layer that enables the backpropagation of 10K iterations of a first-order method without any computational or memory issues. Our modular CNN is significantly lighter than recent cutting-edge networks and is competitive with other more intricate inpainting methods.",1
"The training of many existing end-to-end steering angle prediction models heavily relies on steering angles as the supervisory signal. Without learning from much richer contexts, these methods are susceptible to the presence of sharp road curves, challenging traffic conditions, strong shadows, and severe lighting changes. In this paper, we considerably improve the accuracy and robustness of predictions through heterogeneous auxiliary networks feature mimicking, a new and effective training method that provides us with much richer contextual signals apart from steering direction. Specifically, we train our steering angle predictive model by distilling multi-layer knowledge from multiple heterogeneous auxiliary networks that perform related but different tasks, e.g., image segmentation or optical flow estimation. As opposed to multi-task learning, our method does not require expensive annotations of related tasks on the target set. This is made possible by applying contemporary off-the-shelf networks on the target set and mimicking their features in different layers after transformation. The auxiliary networks are discarded after training without affecting the runtime efficiency of our model. Our approach achieves a new state-of-the-art on Udacity and Comma.ai, outperforming the previous best by a large margin of 12.8% and 52.1%, respectively. Encouraging results are also shown on Berkeley Deep Drive (BDD) dataset.",0
"Many current end-to-end steering angle prediction models heavily rely on steering angles as the supervisory signal, which can make them vulnerable to sharp road curves, challenging traffic conditions, strong shadows, and severe lighting changes due to the lack of broader contextual information. To address this issue, we propose a new and effective training method called heterogeneous auxiliary networks feature mimicking, which significantly improves the accuracy and robustness of predictions by providing richer contextual signals beyond steering direction. Our method involves training the steering angle predictive model by distilling multi-layer knowledge from multiple heterogeneous auxiliary networks that perform related but different tasks, such as image segmentation or optical flow estimation. Unlike multi-task learning, our approach does not require expensive annotations of related tasks on the target set. We achieve this by using off-the-shelf networks on the target set and mimicking their features in different layers after transformation. After training, the auxiliary networks are discarded without affecting the runtime efficiency of our model. Our approach achieves a new state-of-the-art on Udacity and Comma.ai, outperforming the previous best by a large margin of 12.8% and 52.1%, respectively. We also demonstrate encouraging results on the Berkeley Deep Drive (BDD) dataset.",1
"Video style transfer is a useful component for applications such as augmented reality, non-photorealistic rendering, and interactive games. Many existing methods use optical flow to preserve the temporal smoothness of the synthesized video. However, the estimation of optical flow is sensitive to occlusions and rapid motions. Thus, in this work, we introduce a novel evolve-sync loss computed by evolvements to replace optical flow. Using this evolve-sync loss, we build an adversarial learning framework, termed as Video Style Transfer Generative Adversarial Network (VST-GAN), which improves upon the MGAN method for image style transfer for more efficient video style transfer. We perform extensive experimental evaluations of our method and show quantitative and qualitative improvements over the state-of-the-art methods.",0
"Applications like augmented reality, non-photorealistic rendering, and interactive games benefit greatly from video style transfer. Although many existing methods utilize optical flow to maintain temporal smoothness in the resulting video, the accuracy of optical flow can be affected by occlusions and fast movements. This research proposes a new approach called evolve-sync loss, which employs evolvements instead of optical flow. The Video Style Transfer Generative Adversarial Network (VST-GAN) uses this method within an adversarial learning framework to enhance the efficiency of video style transfer compared to the MGAN method for image style transfer. Our experimental evaluations demonstrate substantial qualitative and quantitative improvements over the current state-of-the-art methods.",1
"Recovering structure and motion parameters given a image pair or a sequence of images is a well studied problem in computer vision. This is often achieved by employing Structure from Motion (SfM) or Simultaneous Localization and Mapping (SLAM) algorithms based on the real-time requirements. Recently, with the advent of Convolutional Neural Networks (CNNs) researchers have explored the possibility of using machine learning techniques to reconstruct the 3D structure of a scene and jointly predict the camera pose. In this work, we present a framework that achieves state-of-the-art performance on single image depth prediction for both indoor and outdoor scenes. The depth prediction system is then extended to predict optical flow and ultimately the camera pose and trained end-to-end. Our motion estimation framework outperforms the previous motion prediction systems and we also demonstrate that the state-of-the-art metric depths can be further improved using the knowledge of pose.",0
"The problem of recovering structure and motion parameters from an image pair or sequence of images has been extensively researched in computer vision. Real-time requirements are often met through the use of Structure from Motion (SfM) or Simultaneous Localization and Mapping (SLAM) algorithms. Recently, researchers have begun exploring the use of machine learning techniques, particularly Convolutional Neural Networks (CNNs), to reconstruct a scene's 3D structure and predict camera pose. In this study, we introduce a framework that achieves exceptional performance in predicting depth from a single image in both indoor and outdoor settings. We extend the depth prediction system to include optical flow prediction and ultimately camera pose prediction, all of which are trained end-to-end. Our motion estimation framework outperforms previous prediction systems, and we also demonstrate that knowledge of pose can further enhance the state-of-the-art metric depths.",1
"Two optical flow estimation problems are addressed: i) occlusion estimation and handling, and ii) estimation from image sequences longer than two frames. The proposed ContinualFlow method estimates occlusions before flow, avoiding the use of flow corrupted by occlusions for their estimation. We show that providing occlusion masks as an additional input to flow estimation improves the standard performance metric by more than 25\% on both KITTI and Sintel. As a second contribution, a novel method for incorporating information from past frames into flow estimation is introduced. The previous frame flow serves as an input to occlusion estimation and as a prior in occluded regions, i.e. those without visual correspondences. By continually using the previous frame flow, ContinualFlow performance improves further by 18\% on KITTI and 7\% on Sintel, achieving top performance on KITTI and Sintel.",0
"The article discusses two issues related to optical flow estimation: occlusion estimation and handling, and estimation from image sequences that are longer than two frames. The ContinualFlow approach proposed in the article addresses these problems by estimating occlusions before flow and using occlusion masks as an additional input to flow estimation. This has been shown to improve the standard performance metric by over 25% on both KITTI and Sintel datasets. Additionally, the article presents a new method for incorporating information from past frames into flow estimation by using the previous frame flow as an input to occlusion estimation and as a prior in occluded regions. By continually using the previous frame flow, the performance of ContinualFlow is further improved by 18% on KITTI and 7% on Sintel datasets, resulting in the top performance on both datasets.",1
We address the problem of motion estimation in images operating in the frequency domain. A method is presented which extends phase correlation to handle multiple motions present in an area. Our scheme is based on a novel Bilateral-Phase Correlation (BLPC) technique that incorporates the concept and principles of Bilateral Filters retaining the motion boundaries by taking into account the difference both in value and distance in a manner very similar to Gaussian convolution. The optical flow is obtained by applying the proposed method at certain locations selected based on the present motion differences and then performing non-uniform interpolation in a multi-scale iterative framework. Experiments with several well-known datasets with and without ground-truth show that our scheme outperforms recently proposed state-of-the-art phase correlation based optical flow methods.,0
"We present a solution to the issue of motion estimation in images that operates in the frequency domain. Our approach expands on phase correlation to handle the presence of multiple motions in a given area. Our method, which we call Bilateral-Phase Correlation (BLPC), is based on the principles of Bilateral Filters. It retains motion boundaries by considering the difference in value and distance, much like Gaussian convolution. To obtain optical flow, we apply our technique at selected locations based on differences in motion and use non-uniform interpolation in a multi-scale iterative framework. Our experiments on various datasets, both with and without ground-truth, demonstrate that our approach outperforms existing state-of-the-art optical flow methods that use phase correlation.",1
"Analyzing videos of human actions involves understanding the temporal relationships among video frames. State-of-the-art action recognition approaches rely on traditional optical flow estimation methods to pre-compute motion information for CNNs. Such a two-stage approach is computationally expensive, storage demanding, and not end-to-end trainable. In this paper, we present a novel CNN architecture that implicitly captures motion information between adjacent frames. We name our approach hidden two-stream CNNs because it only takes raw video frames as input and directly predicts action classes without explicitly computing optical flow. Our end-to-end approach is 10x faster than its two-stage baseline. Experimental results on four challenging action recognition datasets: UCF101, HMDB51, THUMOS14 and ActivityNet v1.2 show that our approach significantly outperforms the previous best real-time approaches.",0
"The analysis of human actions in videos requires an understanding of the temporal relationships between video frames. Current methods for action recognition rely on traditional optical flow estimation techniques to compute motion information for CNNs, which is a two-stage process that is computationally expensive, requires a lot of storage, and is not end-to-end trainable. In this article, we present a new CNN architecture that implicitly captures motion information between consecutive frames. Our approach, called hidden two-stream CNNs, takes raw video frames as input and predicts action classes without explicitly computing optical flow. Our end-to-end method is 10 times faster than the two-stage approach. We conducted experiments on four challenging action recognition datasets, including UCF101, HMDB51, THUMOS14, and ActivityNet v1.2, and our approach outperformed previous real-time methods.",1
"Current state-of-the-art approaches to video understanding adopt temporal jittering to simulate analyzing the video at varying frame rates. However, this does not work well for multirate videos, in which actions or subactions occur at different speeds. The frame sampling rate should vary in accordance with the different motion speeds. In this work, we propose a simple yet effective strategy, termed random temporal skipping, to address this situation. This strategy effectively handles multirate videos by randomizing the sampling rate during training. It is an exhaustive approach, which can potentially cover all motion speed variations. Furthermore, due to the large temporal skipping, our network can see video clips that originally cover over 100 frames. Such a time range is enough to analyze most actions/events. We also introduce an occlusion-aware optical flow learning method that generates improved motion maps for human action recognition. Our framework is end-to-end trainable, runs in real-time, and achieves state-of-the-art performance on six widely adopted video benchmarks.",0
"Video understanding techniques currently use temporal jittering to mimic analyzing videos at varying frame rates. However, this method is ineffective for multirate videos, where actions or subactions occur at different speeds. To account for this, the frame sampling rate should adjust according to the different motion speeds. To address this issue, we propose a straightforward yet efficient strategy called random temporal skipping, which randomizes the sampling rate during training and handles multirate videos comprehensively. Moreover, our network can analyze video clips covering over 100 frames, which is sufficient for most actions/events. Additionally, we introduce an occlusion-aware optical flow learning method that generates improved motion maps for human action recognition. Our framework is end-to-end trainable, runs in real-time, and achieves state-of-the-art performance on six widely used video benchmarks.",1
"Modern large displacement optical flow algorithms usually use an initialization by either sparse descriptor matching techniques or dense approximate nearest neighbor fields. While the latter have the advantage of being dense, they have the major disadvantage of being very outlier-prone as they are not designed to find the optical flow, but the visually most similar correspondence. In this article we present a dense correspondence field approach that is much less outlier-prone and thus much better suited for optical flow estimation than approximate nearest neighbor fields. Our approach does not require explicit regularization, smoothing (like median filtering) or a new data term. Instead we solely rely on patch matching techniques and a novel multi-scale matching strategy. We also present enhancements for outlier filtering. We show that our approach is better suited for large displacement optical flow estimation than modern descriptor matching techniques. We do so by initializing EpicFlow with our approach instead of their originally used state-of-the-art descriptor matching technique. We significantly outperform the original EpicFlow on MPI-Sintel, KITTI 2012, KITTI 2015 and Middlebury. In this extended article of our former conference publication we further improve our approach in matching accuracy as well as runtime and present more experiments and insights.",0
"Contemporary optical flow algorithms with large displacement typically employ initialization through sparse descriptor matching techniques or dense approximate nearest neighbor fields. The latter, while dense, are highly susceptible to outliers as they are not designed to locate optical flow but rather the visually similar correspondence. In this article, we introduce a dense correspondence field method that is less vulnerable to outliers, making it better suited for optical flow estimation than approximate nearest neighbor fields. Our method depends solely on patch matching techniques and a multi-scale matching strategy and does not require explicit regularization, smoothing, or a new data term. We also present improvements for outlier filtering. By implementing our approach instead of a state-of-the-art descriptor matching technique, we demonstrate that our approach is better suited for large displacement optical flow estimation and outperforms the original EpicFlow on MPI-Sintel, KITTI 2012, KITTI 2015, and Middlebury. In this extended article, we enhance our approach in matching accuracy and runtime and provide more experiments and insights.",1
"Video super-resolution (SR) aims to generate a sequence of high-resolution (HR) frames with plausible and temporally consistent details from their low-resolution (LR) counterparts. The generation of accurate correspondence plays a significant role in video SR. It is demonstrated by traditional video SR methods that simultaneous SR of both images and optical flows can provide accurate correspondences and better SR results. However, LR optical flows are used in existing deep learning based methods for correspondence generation. In this paper, we propose an end-to-end trainable video SR framework to super-resolve both images and optical flows. Specifically, we first propose an optical flow reconstruction network (OFRnet) to infer HR optical flows in a coarse-to-fine manner. Then, motion compensation is performed according to the HR optical flows. Finally, compensated LR inputs are fed to a super-resolution network (SRnet) to generate the SR results. Extensive experiments demonstrate that HR optical flows provide more accurate correspondences than their LR counterparts and improve both accuracy and consistency performance. Comparative results on the Vid4 and DAVIS-10 datasets show that our framework achieves the state-of-the-art performance.",0
"The goal of video super-resolution (SR) is to create a series of high-resolution (HR) frames that are visually consistent with their low-resolution (LR) counterparts. Correspondence accuracy is crucial to achieving successful video SR. Traditional methods show that simultaneous SR of images and optical flows can provide better results, but current deep learning-based methods use LR optical flows for correspondence generation. This paper proposes an end-to-end video SR framework that super-resolves both images and optical flows using an optical flow reconstruction network (OFRnet) and motion compensation based on HR optical flows. The LR inputs are then fed to a super-resolution network (SRnet) to generate the SR results. Experiments demonstrate that HR optical flows improve accuracy and consistency performance, leading to state-of-the-art results on the Vid4 and DAVIS-10 datasets.",1
"In this work, we propose a mask propagation network to treat the video segmentation problem as a concept of the guided instance segmentation. Similar to most MaskTrack based video segmentation methods, our method takes the mask probability map of previous frame and the appearance of current frame as inputs, and predicts the mask probability map for the current frame. Specifically, we adopt the Xception backbone based DeepLab v3+ model as the probability map predictor in our prediction pipeline. Besides, instead of the full image and the original mask probability, our network takes the region of interest of the instance, and the new mask probability which warped by the optical flow between the previous and current frames as the inputs. We also ensemble the modified One-Shot Video Segmentation Network to make the final predictions in order to retrieve and segment the missing instance.",0
"Our work proposes a solution to the video segmentation problem by introducing a mask propagation network that utilizes guided instance segmentation. Our method is similar to other video segmentation methods that rely on the mask probability map of the previous frame and the appearance of the current frame. However, we use the Xception-based DeepLab v3+ model as our probability map predictor and input the region of interest of the instance, as well as the new mask probability warped by the optical flow between frames. To improve accuracy, we also ensemble the modified One-Shot Video Segmentation Network for final predictions and to segment any missing instances.",1
"This paper addresses the challenge of dense pixel correspondence estimation between two images. This problem is closely related to optical flow estimation task where ConvNets (CNNs) have recently achieved significant progress. While optical flow methods produce very accurate results for the small pixel translation and limited appearance variation scenarios, they hardly deal with the strong geometric transformations that we consider in this work. In this paper, we propose a coarse-to-fine CNN-based framework that can leverage the advantages of optical flow approaches and extend them to the case of large transformations providing dense and subpixel accurate estimates. It is trained on synthetic transformations and demonstrates very good performance to unseen, realistic, data. Further, we apply our method to the problem of relative camera pose estimation and demonstrate that the model outperforms existing dense approaches.",0
"The primary focus of this study is to tackle the intricate task of estimating dense pixel correspondence between two images. This problem is closely linked to the estimation of optical flow, where Convolutional Neural Networks (CNNs) have made considerable advancements in recent times. Despite optical flow methods producing highly precise results for scenarios involving small pixel translations and limited variations in appearance, they are not as effective in handling the robust geometric transformations that this study takes into account. Hence, this paper introduces a CNN-based framework that uses a coarse-to-fine approach, building on the strengths of optical flow methods and expanding their capabilities to encompass large transformations, thereby providing precise and accurate subpixel estimates. The framework is trained on synthetic transformations and exhibits excellent performance on real-life data. Additionally, the study applies this approach to the problem of relative camera pose estimation and proves that it outperforms existing dense methods.",1
"Quantitative assessment of left ventricle (LV) function from cine MRI has significant diagnostic and prognostic value for cardiovascular disease patients. The temporal movement of LV provides essential information on the contracting/relaxing pattern of heart, which is keenly evaluated by clinical experts in clinical practice. Inspired by the expert way of viewing Cine MRI, we propose a new CNN module that is able to incorporate the temporal information into LV segmentation from cine MRI. In the proposed CNN, the optical flow (OF) between neighboring frames is integrated and aggregated at feature level, such that temporal coherence in cardiac motion can be taken into account during segmentation. The proposed module is integrated into the U-net architecture without need of additional training. Furthermore, dilated convolution is introduced to improve the spatial accuracy of segmentation. Trained and tested on the Cardiac Atlas database, the proposed network resulted in a Dice index of 95% and an average perpendicular distance of 0.9 pixels for the middle LV contour, significantly outperforming the original U-net that processes each frame individually. Notably, the proposed method improved the temporal coherence of LV segmentation results, especially at the LV apex and base where the cardiac motion is difficult to follow.",0
"Cine MRI is a valuable tool for assessing left ventricle (LV) function in cardiovascular disease patients. LV movement over time provides crucial information about the heart's contracting and relaxing patterns, which are carefully evaluated by clinical experts. To improve LV segmentation from cine MRI, we propose a new CNN module that integrates temporal information using optical flow (OF) between neighboring frames. This module is incorporated into the U-net architecture without additional training and is augmented with dilated convolution to enhance spatial accuracy. Tested on the Cardiac Atlas database, our method achieved a Dice index of 95% and an average perpendicular distance of 0.9 pixels, outperforming the original U-net. Our method also improved the temporal coherence of LV segmentation results, particularly at the LV apex and base, where cardiac motion is challenging to follow.",1
"Edge detection has made significant progress with the help of deep Convolutional Networks (ConvNet). These ConvNet based edge detectors have approached human level performance on standard benchmarks. We provide a systematical study of these detectors' outputs. We show that the detection results did not accurately localize edge pixels, which can be adversarial for tasks that require crisp edge inputs. As a remedy, we propose a novel refinement architecture to address the challenging problem of learning a crisp edge detector using ConvNet. Our method leverages a top-down backward refinement pathway, and progressively increases the resolution of feature maps to generate crisp edges. Our results achieve superior performance, surpassing human accuracy when using standard criteria on BSDS500, and largely outperforming state-of-the-art methods when using more strict criteria. More importantly, we demonstrate the benefit of crisp edge maps for several important applications in computer vision, including optical flow estimation, object proposal generation and semantic segmentation.",0
"The development of deep Convolutional Networks (ConvNet) has significantly improved edge detection, leading to ConvNet-based edge detectors that perform at a level comparable to humans on standard benchmarks. However, our study found that these detectors do not accurately localize edge pixels, which could be problematic for tasks that require clear edge inputs. To address this issue, we propose a new refinement architecture that utilizes a top-down backward refinement pathway to increase the resolution of feature maps and generate crisp edges. Our method outperforms state-of-the-art methods and achieves superior performance, surpassing human accuracy when using standard criteria on BSDS500. Furthermore, we demonstrate the advantages of crisp edge maps for various computer vision applications, including optical flow estimation, object proposal generation, and semantic segmentation.",1
"Learning depth and optical flow via deep neural networks by watching videos has made significant progress recently. In this paper, we jointly solve the two tasks by exploiting the underlying geometric rules within stereo videos. Specifically, given two consecutive stereo image pairs from a video, we first estimate depth, camera ego-motion and optical flow from three neural networks. Then the whole scene is decomposed into moving foreground and static background by compar- ing the estimated optical flow and rigid flow derived from the depth and ego-motion. We propose a novel consistency loss to let the optical flow learn from the more accurate rigid flow in static regions. We also design a rigid alignment module which helps refine ego-motion estimation by using the estimated depth and optical flow. Experiments on the KITTI dataset show that our results significantly outperform other state-of- the-art algorithms. Source codes can be found at https: //github.com/baidu-research/UnDepthflow",0
"Recently, there has been significant progress in learning depth and optical flow through deep neural networks by watching videos. This paper presents a joint solution to both tasks, utilizing the underlying geometric rules within stereo videos. Initially, two consecutive stereo image pairs are taken from a video and depth, camera ego-motion, and optical flow are estimated using three neural networks. The scene is then divided into moving foreground and static background by comparing the estimated optical flow with the rigid flow derived from depth and ego-motion. A novel consistency loss is proposed to enable optical flow to learn from the more precise rigid flow in static regions. Additionally, a rigid alignment module is designed to refine ego-motion estimation using the estimated depth and optical flow. The results of experiments on the KITTI dataset show that our approach significantly outperforms other state-of-the-art algorithms. Access to source codes can be found at https://github.com/baidu-research/UnDepthflow.",1
"Models optimized for accuracy on single images are often prohibitively slow to run on each frame in a video. Recent work exploits the use of optical flow to warp image features forward from select keyframes, as a means to conserve computation on video. This approach, however, achieves only limited speedup, even when optimized, due to the accuracy degradation introduced by repeated forward warping, and the inference cost of optical flow estimation. To address these problems, we propose a new scheme that propagates features using the block motion vectors (BMV) present in compressed video (e.g. H.264 codecs), instead of optical flow, and bi-directionally warps and fuses features from enclosing keyframes to capture scene context on each video frame. Our technique, interpolation-BMV, enables us to accurately estimate the features of intermediate frames, while keeping inference costs low. We evaluate our system on the CamVid and Cityscapes datasets, comparing to both a strong single-frame baseline and related work. We find that we are able to substantially accelerate segmentation on video, achieving near real-time frame rates (20+ frames per second) on large images (e.g. 960 x 720 pixels), while maintaining competitive accuracy. This represents an improvement of almost 6x over the single-frame baseline and 2.5x over the fastest prior work.",0
"Models that are designed to attain high levels of accuracy on individual images can be too slow to operate on every frame of a video. Recent studies have utilized optical flow to shift image features forward from particular keyframes in order to save computational power when processing videos. However, this approach has limitations in terms of speed, even when it has been optimized, due to the decline in accuracy caused by repeated forward warping, as well as the inference cost of optical flow estimation. To tackle these issues, we propose a new method that utilizes the block motion vectors (BMV) found in compressed videos, such as H.264 codecs, instead of optical flow to propagate features. Our method bi-directionally warps and merges features from enclosing keyframes to capture the context of the scene in each video frame. Our technique, interpolation-BMV, allows us to precisely estimate the features of intermediate frames while keeping inference costs low. We evaluate our system on the CamVid and Cityscapes datasets, comparing it to a strong single-frame baseline and other related studies. Our results demonstrate that we can substantially accelerate segmentation in videos, achieving frame rates close to real-time (20+ frames per second) on large images (e.g. 960 x 720 pixels), while retaining competitive accuracy. Our method represents an improvement of nearly 6x over the single-frame baseline and 2.5x faster than previous methods.",1
"Deep convolutional neural networks (DCNN) have recently shown promising results in low-level computer vision problems such as optical flow and disparity estimation, but still, have much room to further improve their performance. In this paper, we propose a novel sub-pixel convolution-based encoder-decoder network for optical flow and disparity estimations, which can extend FlowNetS and DispNet by replacing the deconvolution layers with sup-pixel convolution blocks. By using sub-pixel refinement and estimation on the decoder stages instead of deconvolution, we can significantly improve the estimation accuracy for optical flow and disparity, even with reduced numbers of parameters. We show a supervised end-to-end training of our proposed networks for optical flow and disparity estimations, and an unsupervised end-to-end training for monocular depth and pose estimations. In order to verify the effectiveness of our proposed networks, we perform intensive experiments for (i) optical flow and disparity estimations, and (ii) monocular depth and pose estimations. Throughout the extensive experiments, our proposed networks outperform the baselines such as FlowNetS and DispNet in terms of estimation accuracy and training times.",0
"Recent studies have revealed that deep convolutional neural networks (DCNN) hold great potential for low-level computer vision problems like optical flow and disparity estimation, but their performance can still be improved. This paper introduces a novel encoder-decoder network for optical flow and disparity estimations that employs sub-pixel convolution blocks instead of deconvolution layers, thereby extending the capabilities of FlowNetS and DispNet. By implementing sub-pixel refinement and estimation on the decoder stages, our network achieves significantly improved estimation accuracy for optical flow and disparity, with fewer parameters. The proposed networks are trained end-to-end, both supervised for optical flow and disparity estimations, and unsupervised for monocular depth and pose estimations. Our experiments demonstrate the effectiveness of our proposed networks, as they surpass baseline methods like FlowNetS and DispNet in estimation accuracy and training times.",1
"Anticipating future events is an important prerequisite towards intelligent behavior. Video forecasting has been studied as a proxy task towards this goal. Recent work has shown that to predict semantic segmentation of future frames, forecasting at the semantic level is more effective than forecasting RGB frames and then segmenting these. In this paper we consider the more challenging problem of future instance segmentation, which additionally segments out individual objects. To deal with a varying number of output labels per image, we develop a predictive model in the space of fixed-sized convolutional features of the Mask R-CNN instance segmentation model. We apply the ""detection head'"" of Mask R-CNN on the predicted features to produce the instance segmentation of future frames. Experiments show that this approach significantly improves over strong baselines based on optical flow and repurposed instance segmentation architectures.",0
"Intelligent behavior requires the ability to anticipate future events, which can be achieved through video forecasting. Recent research has found that predicting semantic segmentation of future frames is more effective than RGB frame forecasting and subsequent segmentation. This paper tackles the more difficult task of future instance segmentation, which involves identifying individual objects. To address the challenge of varying label numbers per image, we devise a predictive model using fixed-sized convolutional features of the Mask R-CNN instance segmentation model. We apply the Mask R-CNN detection head to the predicted features to produce future instance segmentation. Our experiments demonstrate that this method outperforms optical flow and repurposed instance segmentation architectures.",1
"Plenoptic cameras offer a cost effective solution to capture light fields by multiplexing multiple views on a single image sensor. However, the high angular resolution is achieved at the expense of reducing the spatial resolution of each view by orders of magnitude compared to the raw sensor image. While light field super-resolution is still at an early stage, the field of single image super-resolution (SISR) has recently known significant advances with the use of deep learning techniques. This paper describes a simple framework allowing us to leverage state-of-the-art SISR techniques into light fields, while taking into account specific light field geometrical constraints. The idea is to first compute a representation compacting most of the light field energy into as few components as possible. This is achieved by aligning the light field using optical flows and then by decomposing the aligned light field using singular value decomposition (SVD). The principal basis captures the information that is coherent across all the views, while the other basis contain the high angular frequencies. Super-resolving this principal basis using an SISR method allows us to super-resolve all the information that is coherent across the entire light field. This framework allows the proposed light field super-resolution method to inherit the benefits of the SISR method used. Experimental results show that the proposed method is competitive, and most of the time superior, to recent light field super-resolution methods in terms of both PSNR and SSIM quality metrics, with a lower complexity.",0
"Plenoptic cameras are a cost-effective way of capturing light fields by combining multiple views onto a single image sensor. However, this approach results in a decrease in spatial resolution for each view, which is compensated for by achieving high angular resolution. Although light field super-resolution is in its early stages, recent significant advances in deep learning techniques have been made in the field of single image super-resolution (SISR). This paper presents a simple framework that incorporates state-of-the-art SISR techniques into light fields and takes into account specific light field geometrical constraints. The proposed method first computes a representation that condenses most of the light field energy into as few components as possible, through optical flow alignment and singular value decomposition (SVD). The principal basis captures the information that is coherent across all views, while the other bases contain high angular frequencies. Super-resolving the principal basis using an SISR method enables the resolution of all information that is coherent across the entire light field. This framework allows for the proposed light field super-resolution method to benefit from the SISR method used. Experimental results show that the proposed method is competitive and superior to recent light field super-resolution methods in terms of both PSNR and SSIM quality metrics, with lower complexity.",1
"Detecting the occlusion from stereo images or video frames is important to many computer vision applications. Previous efforts focus on bundling it with the computation of disparity or optical flow, leading to a chicken-and-egg problem. In this paper, we leverage convolutional neural network to liberate the occlusion detection task from the interleaved, traditional calculation framework. We propose a Symmetric Network (SymmNet) to directly exploit information from an image pair, without estimating disparity or motion in advance. The proposed network is structurally left-right symmetric to learn the binocular occlusion simultaneously, aimed at jointly improving both results. The comprehensive experiments show that our model achieves state-of-the-art results on detecting the stereo and motion occlusion.",0
"Many computer vision applications require the detection of occlusion in stereo images or video frames. Previous methods attempted to bundle this task with the computation of disparity or optical flow, which created a problem of not knowing where to start. This paper introduces a new approach that uses convolutional neural networks to separate the occlusion detection task from the traditional calculation framework. The proposed Symmetric Network (SymmNet) can directly extract information from an image pair without needing to estimate disparity or motion beforehand. The structure of SymmNet is left-right symmetric, allowing it to learn binocular occlusion simultaneously and improve both results. The experiments conducted demonstrate that our model produces state-of-the-art results when detecting stereo and motion occlusion.",1
"The difficulty of annotating training data is a major obstacle to using CNNs for low-level tasks in video. Synthetic data often does not generalize to real videos, while unsupervised methods require heuristic losses. Proxy tasks can overcome these issues, and start by training a network for a task for which annotation is easier or which can be trained unsupervised. The trained network is then fine-tuned for the original task using small amounts of ground truth data. Here, we investigate frame interpolation as a proxy task for optical flow. Using real movies, we train a CNN unsupervised for temporal interpolation. Such a network implicitly estimates motion, but cannot handle untextured regions. By fine-tuning on small amounts of ground truth flow, the network can learn to fill in homogeneous regions and compute full optical flow fields. Using this unsupervised pre-training, our network outperforms similar architectures that were trained supervised using synthetic optical flow.",0
"CNNs struggle with low-level tasks in video due to the challenge of annotating training data. Synthetic data is not always reliable, while unsupervised methods have limitations in terms of heuristic losses. Proxy tasks offer a solution to these issues by training a network for an easier or unsupervised task before fine-tuning it for the original task with small amounts of ground truth data. In this study, we explore the use of frame interpolation as a proxy task for optical flow. By training a CNN unsupervised for temporal interpolation using real movies, the network can estimate motion but may not handle untextured regions. However, fine-tuning the network with small amounts of ground truth flow allows it to learn and compute full optical flow fields, outperforming similar architectures that were trained supervised using synthetic optical flow.",1
"We investigate two crucial and closely related aspects of CNNs for optical flow estimation: models and training. First, we design a compact but effective CNN model, called PWC-Net, according to simple and well-established principles: pyramidal processing, warping, and cost volume processing. PWC-Net is 17 times smaller in size, 2 times faster in inference, and 11\% more accurate on Sintel final than the recent FlowNet2 model. It is the winning entry in the optical flow competition of the robust vision challenge. Next, we experimentally analyze the sources of our performance gains. In particular, we use the same training procedure of PWC-Net to retrain FlowNetC, a sub-network of FlowNet2. The retrained FlowNetC is 56\% more accurate on Sintel final than the previously trained one and even 5\% more accurate than the FlowNet2 model. We further improve the training procedure and increase the accuracy of PWC-Net on Sintel by 10\% and on KITTI 2012 and 2015 by 20\%. Our newly trained model parameters and training protocols will be available on https://github.com/NVlabs/PWC-Net",0
"Our investigation focuses on two interdependent components of CNNs for optical flow estimation: models and training. Our first objective is to create a potent yet compact CNN model, known as PWC-Net, based on established principles such as warping, cost volume processing, and pyramidal processing. The resulting model is significantly smaller, faster, and more accurate than the FlowNet2 model, as demonstrated by its victory in the optical flow competition of the robust vision challenge. We then conduct an experimental analysis of the factors contributing to our model's superior performance. Specifically, we employ the same training method as PWC-Net to retrain FlowNetC, a sub-network of FlowNet2, which results in a 56% increase in accuracy on Sintel final. We also enhance the training procedure to improve PWC-Net's accuracy on Sintel by 10% and on KITTI 2012 and 2015 by 20%. Our newly trained model parameters and training protocols will be available on https://github.com/NVlabs/PWC-Net.",1
"Currently, the most common motion representation for action recognition is optical flow. Optical flow is based on particle tracking which adheres to a Lagrangian perspective on dynamics. In contrast to the Lagrangian perspective, the Eulerian model of dynamics does not track, but describes local changes. For video, an Eulerian phase-based motion representation, using complex steerable filters, has been successfully employed recently for motion magnification and video frame interpolation. Inspired by these previous works, here, we proposes learning Eulerian motion representations in a deep architecture for action recognition. We learn filters in the complex domain in an end-to-end manner. We design these complex filters to resemble complex Gabor filters, typically employed for phase-information extraction. We propose a phase-information extraction module, based on these complex filters, that can be used in any network architecture for extracting Eulerian representations. We experimentally analyze the added value of Eulerian motion representations, as extracted by our proposed phase extraction module, and compare with existing motion representations based on optical flow, on the UCF101 dataset.",0
"Optical flow is currently the most widely used motion representation for action recognition. It is based on particle tracking and follows a Lagrangian perspective on dynamics. In contrast, the Eulerian model of dynamics describes local changes without tracking. Recently, an Eulerian phase-based motion representation has been successfully utilized for motion magnification and video frame interpolation, using complex steerable filters. Building on these findings, we propose a deep architecture for action recognition that learns Eulerian motion representations. We train complex filters in the complex domain, which resemble complex Gabor filters used for phase-information extraction. Our proposed phase-information extraction module can be used in any network architecture for extracting Eulerian representations. We evaluate the performance of our approach on the UCF101 dataset and compare it with existing motion representations based on optical flow.",1
"The wide availability of Commercial Off-The-Shelf (COTS) electronics that can withstand Low Earth Orbit conditions has opened avenue for wide deployment of CubeSats and small-satellites. CubeSats thanks to their low developmental and launch costs offer new opportunities for rapidly demonstrating on-orbit surveillance capabilities. In our earlier work, we proposed development of SWIMSat (Space based Wide-angle Imaging of Meteors) a 3U CubeSat demonstrator that is designed to observe illuminated objects entering the Earth's atmosphere. The spacecraft would operate autonomously using a smart camera with vision algorithms to detect, track and report of objects. Several CubeSats can track an object in a coordinated fashion to pinpoint an object's trajectory. An extension of this smart camera capability is to track unilluminated objects utilizing capabilities we have been developing to track and navigate to Near Earth Objects (NEOs). This extension enables detecting and tracking objects that can't readily be detected by humans. The system maintains a dense star map of the night sky and performs round the clock observations. Standard optical flow algorithms are used to obtain trajectories of all moving objects in the camera field of view. Through a process of elimination, certain stars maybe occluded by a transiting unilluminated object which is then used to first detect and obtain a trajectory of the object. Using multiple cameras observing the event from different points of view, it may be possible then to triangulate the position of the object in space and obtain its orbital trajectory. In this work, the performance of our space object detection algorithm coupled with a spacecraft guidance, navigation, and control system is demonstrated.",0
"The widespread availability of electronics designed for use in Low Earth Orbit (LEO) conditions has paved the way for the deployment of CubeSats and small satellites. These low-cost options enable the quick demonstration of on-orbit surveillance capabilities. Our team previously suggested the creation of SWIMSat, a 3U CubeSat that uses a smart camera with vision algorithms to observe illuminated objects entering the Earth's atmosphere. Multiple CubeSats can coordinate to track an object and determine its trajectory. We have also developed the capability to track unilluminated objects using our Near Earth Object (NEO) navigation technology. Our system maintains a star map of the night sky and uses optical flow algorithms to detect moving objects. By observing objects from different angles, we can triangulate their positions and determine their orbital paths. Our work demonstrates the effectiveness of our space object detection algorithm and spacecraft guidance, navigation, and control system.",1
"Learning long-term spatial-temporal features are critical for many video analysis tasks. However, existing video segmentation methods predominantly rely on static image segmentation techniques, and methods capturing temporal dependency for segmentation have to depend on pretrained optical flow models, leading to suboptimal solutions for the problem. End-to-end sequential learning to explore spatialtemporal features for video segmentation is largely limited by the scale of available video segmentation datasets, i.e., even the largest video segmentation dataset only contains 90 short video clips. To solve this problem, we build a new large-scale video object segmentation dataset called YouTube Video Object Segmentation dataset (YouTube-VOS). Our dataset contains 4,453 YouTube video clips and 94 object categories. This is by far the largest video object segmentation dataset to our knowledge and has been released at http://youtube-vos.org. We further evaluate several existing state-of-the-art video object segmentation algorithms on this dataset which aims to establish baselines for the development of new algorithms in the future.",0
"Many video analysis tasks require the acquisition of long-term spatial-temporal features. However, current video segmentation techniques primarily depend on static image segmentation methods, and those that incorporate temporal dependency rely on pretrained optical flow models, resulting in suboptimal outcomes. The scale of existing video segmentation datasets significantly hinders the potential of end-to-end sequential learning to explore spatial-temporal features for video segmentation. The most extensive video segmentation dataset consists of just 90 short video clips. To address this issue, we created the YouTube Video Object Segmentation dataset (YouTube-VOS), which includes 4,453 YouTube video clips and 94 object categories, making it the largest video object segmentation dataset available. The dataset is accessible at http://youtube-vos.org. We also evaluated various state-of-the-art video object segmentation algorithms on this dataset to establish benchmarks for future algorithmic development.",1
"We present an unsupervised learning framework for simultaneously training single-view depth prediction and optical flow estimation models using unlabeled video sequences. Existing unsupervised methods often exploit brightness constancy and spatial smoothness priors to train depth or flow models. In this paper, we propose to leverage geometric consistency as additional supervisory signals. Our core idea is that for rigid regions we can use the predicted scene depth and camera motion to synthesize 2D optical flow by backprojecting the induced 3D scene flow. The discrepancy between the rigid flow (from depth prediction and camera motion) and the estimated flow (from optical flow model) allows us to impose a cross-task consistency loss. While all the networks are jointly optimized during training, they can be applied independently at test time. Extensive experiments demonstrate that our depth and flow models compare favorably with state-of-the-art unsupervised methods.",0
"Our paper introduces a framework for unsupervised learning that can train single-view depth prediction and optical flow estimation models simultaneously, using unlabeled video sequences. Current unsupervised methods rely on brightness constancy and spatial smoothness priors to train depth or flow models. However, we propose an additional supervisory signal based on geometric consistency. Our approach is to leverage the predicted scene depth and camera motion to synthesize 2D optical flow for rigid regions by backprojecting the induced 3D scene flow. By comparing the rigid flow with the estimated flow, we introduce a cross-task consistency loss to the training. Although all networks are optimized jointly during training, they can be used as independent models at test time. Our experiments demonstrate that our depth and flow models outperform the state-of-the-art unsupervised methods.",1
"Compositing is one of the most important editing operations for images and videos. The process of improving the realism of composite results is often called harmonization. Previous approaches for harmonization mainly focus on images. In this work, we take one step further to attack the problem of video harmonization. Specifically, we train a convolutional neural network in an adversarial way, exploiting a pixel-wise disharmony discriminator to achieve more realistic harmonized results and introducing a temporal loss to increase temporal consistency between consecutive harmonized frames. Thanks to the pixel-wise disharmony discriminator, we are also able to relieve the need of input foreground masks. Since existing video datasets which have ground-truth foreground masks and optical flows are not sufficiently large, we propose a simple yet efficient method to build up a synthetic dataset supporting supervised training of the proposed adversarial network. Experiments show that training on our synthetic dataset generalizes well to the real-world composite dataset. Also, our method successfully incorporates temporal consistency during training and achieves more harmonious results than previous methods.",0
"Editing images and videos involves many important operations, including compositing. To improve the realism of composite results, the process of harmonization is often used. However, previous approaches have mostly focused on images alone. This study aims to take the next step towards video harmonization by using a convolutional neural network trained in an adversarial manner. By using a pixel-wise disharmony discriminator, the results are more realistic and harmonious. Additionally, a temporal loss is introduced to ensure consistency between consecutive harmonized frames. This approach also eliminates the need for input foreground masks. However, due to the lack of large video datasets with ground-truth foreground masks and optical flows, a synthetic dataset is proposed for training the adversarial network. Experiments show that the synthetic dataset produces good results and that this method achieves better harmonization than previous methods.",1
"Unsupervised video segmentation plays an important role in a wide variety of applications from object identification to compression. However, to date, fast motion, motion blur and occlusions pose significant challenges. To address these challenges for unsupervised video segmentation, we develop a novel saliency estimation technique as well as a novel neighborhood graph, based on optical flow and edge cues. Our approach leads to significantly better initial foreground-background estimates and their robust as well as accurate diffusion across time. We evaluate our proposed algorithm on the challenging DAVIS, SegTrack v2 and FBMS-59 datasets. Despite the usage of only a standard edge detector trained on 200 images, our method achieves state-of-the-art results outperforming deep learning based methods in the unsupervised setting. We even demonstrate competitive results comparable to deep learning based methods in the semi-supervised setting on the DAVIS dataset.",0
"Unsupervised video segmentation is crucial in various applications, including object identification and compression. However, fast motion, motion blur, and occlusions remain significant obstacles. To overcome these challenges, we introduce a unique saliency estimation technique and neighborhood graph based on optical flow and edge cues for unsupervised video segmentation. Our method provides better foreground-background estimates and more robust and precise diffusion across time. We test our algorithm on challenging DAVIS, SegTrack v2, and FBMS-59 datasets and achieve state-of-the-art results without relying on deep learning techniques. Even in the semi-supervised setting on the DAVIS dataset, our approach demonstrates competitive results compared to deep learning-based methods. Despite using only a standard edge detector trained on 200 images, our algorithm outperforms other methods in the unsupervised setting.",1
"Learning long-term spatial-temporal features are critical for many video analysis tasks. However, existing video segmentation methods predominantly rely on static image segmentation techniques, and methods capturing temporal dependency for segmentation have to depend on pretrained optical flow models, leading to suboptimal solutions for the problem. End-to-end sequential learning to explore spatial-temporal features for video segmentation is largely limited by the scale of available video segmentation datasets, i.e., even the largest video segmentation dataset only contains 90 short video clips. To solve this problem, we build a new large-scale video object segmentation dataset called YouTube Video Object Segmentation dataset (YouTube-VOS). Our dataset contains 3,252 YouTube video clips and 78 categories including common objects and human activities. This is by far the largest video object segmentation dataset to our knowledge and we have released it at https://youtube-vos.org. Based on this dataset, we propose a novel sequence-to-sequence network to fully exploit long-term spatial-temporal information in videos for segmentation. We demonstrate that our method is able to achieve the best results on our YouTube-VOS test set and comparable results on DAVIS 2016 compared to the current state-of-the-art methods. Experiments show that the large scale dataset is indeed a key factor to the success of our model.",0
"Many video analysis tasks require the acquisition of long-term spatial-temporal features, but current video segmentation methods mainly use static image segmentation techniques, and those that incorporate temporal dependency rely on pretrained optical flow models, which often result in suboptimal solutions. The limited availability of datasets for end-to-end sequential learning of spatial-temporal features further compounds this problem. To address this, we have created the YouTube Video Object Segmentation dataset, which is the largest of its kind, comprising 3,252 YouTube video clips across 78 categories. Using this dataset, we have developed a sequence-to-sequence network that fully leverages long-term spatial-temporal information for segmentation. Our method achieves superior results on the YouTube-VOS test set and comparable results on DAVIS 2016, demonstrating the crucial role of the large-scale dataset in our model's success.",1
"In this paper we propose a novel approach to estimate dense optical flow from sparse lidar data acquired on an autonomous vehicle. This is intended to be used as a drop-in replacement of any image-based optical flow system when images are not reliable due to e.g. adverse weather conditions or at night. In order to infer high resolution 2D flows from discrete range data we devise a three-block architecture of multiscale filters that combines multiple intermediate objectives, both in the lidar and image domain. To train this network we introduce a dataset with approximately 20K lidar samples of the Kitti dataset which we have augmented with a pseudo ground-truth image-based optical flow computed using FlowNet2. We demonstrate the effectiveness of our approach on Kitti, and show that despite using the low-resolution and sparse measurements of the lidar, we can regress dense optical flow maps which are at par with those estimated with image-based methods.",0
"Our paper proposes a new method for estimating dense optical flow based on sparse lidar data collected by autonomous vehicles. This method can be used as a substitute for image-based optical flow systems that may not function properly under adverse weather conditions or at night. We have developed a three-block architecture of multiscale filters that combines multiple intermediate objectives in the lidar and image domain to infer high-resolution 2D flows from discrete range data. To train this network, we have created a dataset of approximately 20K lidar samples from the Kitti dataset, which we have augmented with a pseudo ground-truth image-based optical flow computed using FlowNet2. Our results show that despite using low-resolution and sparse measurements of the lidar, our approach can produce dense optical flow maps that are comparable to those generated with image-based methods. We have tested our approach on the Kitti dataset and demonstrated its effectiveness.",1
"Perception technologies in Autonomous Driving are experiencing their golden age due to the advances in Deep Learning. Yet, most of these systems rely on the semantically rich information of RGB images. Deep Learning solutions applied to the data of other sensors typically mounted on autonomous cars (e.g. lidars or radars) are not explored much. In this paper we propose a novel solution to understand the dynamics of moving vehicles of the scene from only lidar information. The main challenge of this problem stems from the fact that we need to disambiguate the proprio-motion of the 'observer' vehicle from that of the external 'observed' vehicles. For this purpose, we devise a CNN architecture which at testing time is fed with pairs of consecutive lidar scans. However, in order to properly learn the parameters of this network, during training we introduce a series of so-called pretext tasks which also leverage on image data. These tasks include semantic information about vehicleness and a novel lidar-flow feature which combines standard image-based optical flow with lidar scans. We obtain very promising results and show that including distilled image information only during training, allows improving the inference results of the network at test time, even when image data is no longer used.",0
"The current era is witnessing a significant development in Perception technologies for Autonomous Driving, thanks to the progress made in Deep Learning. However, most of these systems rely heavily on the semantically rich RGB images. There is a lack of exploration of Deep Learning solutions applied to the data of other sensors like lidars or radars, which are typically mounted on autonomous cars. This paper introduces a new approach to comprehend the movements of vehicles in the scene using only lidar information. The major challenge is to distinguish between the proprio-motion of the observer vehicle and that of the observed vehicles. To address this issue, we propose a CNN architecture that takes pairs of consecutive lidar scans as input at testing time. During the training phase, we employ several pretext tasks that utilize image data, including semantic information about vehicleness and a novel lidar-flow feature that merges standard image-based optical flow with lidar scans. Our results show great promise, as including distilled image information during training improves the inference results of the network at test time, even without any image data.",1
"Scene flow describes 3D motion in a 3D scene. It can either be modeled as a single task, or it can be reconstructed from the auxiliary tasks of stereo depth and optical flow estimation. While the second method can achieve real-time performance by using real-time auxiliary methods, it will typically produce non-dense results. In this representation of a basic combination approach for scene flow estimation, we will tackle the problem of non-density by interpolation.",0
"3D motion in a 3D scene is referred to as scene flow. It can be modeled as a single task or reconstructed from stereo depth and optical flow estimation auxiliary tasks. The latter approach can achieve real-time performance by using real-time auxiliary methods, but it typically produces non-dense results. To address the problem of non-density in scene flow estimation, this basic combination approach involves interpolation.",1
"Anomaly detection through video analysis is of great importance to detect any anomalous vehicle/human behavior at a traffic intersection. While most existing works use neural networks and conventional machine learning methods based on provided dataset, we will use object recognition (Faster R-CNN) to identify objects labels and their corresponding location in the video scene as the first step to implement anomaly detection. Then, the optical flow will be utilized to identify adaptive traffic flows in each region of the frame. Basically, we propose an alternative method for unusual activity detection using an adaptive anomaly detection framework. Compared to the baseline method described in the reference paper, our method is more efficient and yields the comparable accuracy.",0
"Detecting anomalies in vehicles and human behavior at traffic intersections is crucial. While many studies rely on neural networks and traditional machine learning techniques with provided datasets, we suggest using object recognition (Faster R-CNN) to recognize objects and their locations in the video scene, followed by optical flow to identify adaptive traffic flows in each frame region. Essentially, we offer an alternative approach to identifying unusual activity using an adaptive anomaly detection framework. Compared to the baseline method presented in the reference paper, our approach is more effective and produces similar accuracy.",1
"We present a method to reconstruct the three-dimensional trajectory of a moving instance of a known object category using stereo video data. We track the two-dimensional shape of objects on pixel level exploiting instance-aware semantic segmentation techniques and optical flow cues. We apply Structure from Motion (SfM) techniques to object and background images to determine for each frame initial camera poses relative to object instances and background structures. We refine the initial SfM results by integrating stereo camera constraints exploiting factor graphs. We compute the object trajectory by combining object and background camera pose information. In contrast to stereo matching methods, our approach leverages temporal adjacent views for object point triangulation. As opposed to monocular trajectory reconstruction approaches, our method shows no degenerated cases. We evaluate our approach using publicly available video data of vehicles in urban scenes.",0
"Our method utilizes stereo video data to reconstruct the three-dimensional trajectory of a moving instance of a known object category. To track the two-dimensional shape of objects, we employ instance-aware semantic segmentation techniques and optical flow cues on a pixel level. We then use Structure from Motion (SfM) techniques to determine initial camera poses relative to object instances and background structures in each frame. We refine these initial results by integrating stereo camera constraints using factor graphs. By combining object and background camera pose information, we compute the object trajectory. Our approach differs from stereo matching methods as we use temporal adjacent views for object point triangulation, and it also avoids the degenerated cases seen in monocular trajectory reconstruction methods. We evaluate our method using publicly available video data of vehicles in urban scenes.",1
"We propose a self-supervised learning method to jointly reason about spatial and temporal context for video recognition. Recent self-supervised approaches have used spatial context [9, 34] as well as temporal coherency [32] but a combination of the two requires extensive preprocessing such as tracking objects through millions of video frames [59] or computing optical flow to determine frame regions with high motion [30]. We propose to combine spatial and temporal context in one self-supervised framework without any heavy preprocessing. We divide multiple video frames into grids of patches and train a network to solve jigsaw puzzles on these patches from multiple frames. So the network is trained to correctly identify the position of a patch within a video frame as well as the position of a patch over time. We also propose a novel permutation strategy that outperforms random permutations while significantly reducing computational and memory constraints. We use our trained network for transfer learning tasks such as video activity recognition and demonstrate the strength of our approach on two benchmark video action recognition datasets without using a single frame from these datasets for unsupervised pretraining of our proposed video jigsaw network.",0
"Our proposed method combines spatial and temporal context for video recognition through a self-supervised learning approach. While previous approaches have utilized either spatial context or temporal coherency, combining the two has required extensive preprocessing that involves tracking objects or computing optical flow. However, our framework does not require heavy preprocessing. Instead, we divide multiple video frames into grids of patches and train a network to solve jigsaw puzzles on these patches from multiple frames. This allows the network to identify the position of a patch within a video frame and over time. We also introduce a novel permutation strategy that outperforms random permutations while reducing computational and memory constraints. Our trained network can be used for transfer learning tasks such as video activity recognition, as we demonstrate on two benchmark datasets without using a single frame from these datasets for unsupervised pretraining.",1
"We propose a novel representation for dense pixel-wise estimation tasks using CNNs that boosts accuracy and reduces training time, by explicitly exploiting joint coarse-and-fine reasoning. The coarse reasoning is performed over a discrete classification space to obtain a general rough solution, while the fine details of the solution are obtained over a continuous regression space. In our approach both components are jointly estimated, which proved to be beneficial for improving estimation accuracy. Additionally, we propose a new network architecture, which combines coarse and fine components by treating the fine estimation as a refinement built on top of the coarse solution, and therefore adding details to the general prediction. We apply our approach to the challenging problem of optical flow estimation and empirically validate it against state-of-the-art CNN-based solutions trained from scratch and tested on large optical flow datasets.",0
"Our proposal introduces a fresh method to enhance accuracy and minimize training time for dense pixel-wise estimation tasks using CNNs. This is accomplished by explicitly taking advantage of joint coarse-and-fine reasoning. The coarse reasoning is conducted over a discrete classification space to generate an approximate overall solution, while the fine details of the solution are obtained over a continuous regression space. Both components are estimated jointly, resulting in improved estimation accuracy. Also, we present a novel network architecture by treating the fine estimation as a refinement to the coarse solution, thereby adding details to the general prediction. We test our approach on the challenging problem of optical flow estimation and validate it empirically against state-of-the-art CNN-based solutions trained from scratch and tested on large optical flow datasets.",1
"Optical flow refers to the visual motion observed between two consecutive images. Since the degree of freedom is typically much larger than the constraints imposed by the image observations, the straightforward formulation of optical flow as an inverse problem is ill-posed. Standard approaches to determine optical flow rely on formulating and solving an optimization problem that contains both a data fidelity term and a regularization term, the latter effectively resolves the otherwise ill-posedness of the inverse problem. In this work, we depart from the deterministic formalism, and instead treat optical flow as a statistical inverse problem. We discuss how a classical optical flow solution can be interpreted as a point estimate in this more general framework. The statistical approach, whose ""solution"" is a distribution of flow fields, which we refer to as Bayesian optical flow, allows not only ""point"" estimates (e.g., the computation of average flow field), but also statistical estimates (e.g., quantification of uncertainty) that are beyond any standard method for optical flow. As application, we benchmark Bayesian optical flow together with uncertainty quantification using several types of prescribed ground-truth flow fields and images.",0
"The phenomenon of optical flow pertains to the perception of motion between two sequential images. However, due to the abundance of degrees of freedom compared to the constraints imposed by image observations, the conventional approach of defining optical flow as an inverse problem is flawed. Thus, the customary method for establishing optical flow involves devising and resolving an optimization problem, which incorporates a fidelity term and a regularization term that effectively solves the problem of ill-posedness in the inverse problem. In this paper, we deviate from the deterministic formalism by treating optical flow as a statistical inverse problem. We explain how a classical optical flow solution can be seen as a point estimate in this broader framework. Our statistical approach, which generates a distribution of flow fields that we refer to as Bayesian optical flow, enables us to produce not only point estimates, such as computing an average flow field, but also statistical estimates that quantify uncertainty, which is beyond the scope of any standard optical flow technique. We evaluate the efficacy of Bayesian optical flow, together with uncertainty quantification, by benchmarking it against various types of prescribed ground-truth flow fields and images.",1
"Recent work has shown that convolutional neural networks (CNNs) can be used to estimate optical flow with high quality and fast runtime. This makes them preferable for real-world applications. However, such networks require very large training datasets. Engineering the training data is difficult and/or laborious. This paper shows how to augment a network trained on an existing synthetic dataset with large amounts of additional unlabelled data. In particular, we introduce a selection mechanism to assemble from multiple estimates a joint optical flow field, which outperforms that of all input methods. The latter can be used as proxy-ground-truth to train a network on real-world data and to adapt it to specific domains of interest. Our experimental results show that the performance of networks improves considerably, both, in cross-domain and in domain-specific scenarios. As a consequence, we obtain state-of-the-art results on the KITTI benchmarks.",0
"Recent research has demonstrated that convolutional neural networks (CNNs) can rapidly and effectively estimate optical flow, making them ideal for practical applications. However, training these networks necessitates extensive datasets, which can be challenging and time-consuming to create. This research presents a method for supplementing a network trained on synthetic data with additional unlabelled data to improve its performance. Specifically, a selection mechanism is introduced to combine multiple estimates into a joint optical flow field that surpasses all input methods. This joint field can be employed as a proxy-ground-truth to train a network on real-world data and adapt it to specific domains. Experimental results show that this approach significantly improves network performance in both cross-domain and domain-specific situations, resulting in state-of-the-art outcomes on the KITTI benchmarks.",1
"Event-based cameras have shown great promise in a variety of situations where frame based cameras suffer, such as high speed motions and high dynamic range scenes. However, developing algorithms for event measurements requires a new class of hand crafted algorithms. Deep learning has shown great success in providing model free solutions to many problems in the vision community, but existing networks have been developed with frame based images in mind, and there does not exist the wealth of labeled data for events as there does for images for supervised training. To these points, we present EV-FlowNet, a novel self-supervised deep learning pipeline for optical flow estimation for event based cameras. In particular, we introduce an image based representation of a given event stream, which is fed into a self-supervised neural network as the sole input. The corresponding grayscale images captured from the same camera at the same time as the events are then used as a supervisory signal to provide a loss function at training time, given the estimated flow from the network. We show that the resulting network is able to accurately predict optical flow from events only in a variety of different scenes, with performance competitive to image based networks. This method not only allows for accurate estimation of dense optical flow, but also provides a framework for the transfer of other self-supervised methods to the event-based domain.",0
"The use of event-based cameras has proven advantageous over frame based cameras in scenarios involving high speed movements and high dynamic range scenes. However, creating algorithms for event measurements requires a different classification of handcrafted algorithms. While deep learning has been successful in providing model-free solutions for vision-related issues, existing networks have been designed with frame based images in mind. Furthermore, labeled data for events is limited, unlike supervised training for images. To address these challenges, we introduce EV-FlowNet, a novel self-supervised deep learning pipeline for optical flow estimation in event-based cameras. Our approach involves an image-based representation of an event stream, which is fed into a self-supervised neural network as the sole input. Grayscale images captured from the same camera simultaneously with the events serve as a supervisory signal to provide a loss function during training with the estimated flow from the network. Our method accurately predicts optical flow from events in various scenes, with results comparable to image-based networks. Additionally, our approach provides a framework for transferring other self-supervised methods to the event-based domain.",1
"Occlusions play an important role in disparity and optical flow estimation, since matching costs are not available in occluded areas and occlusions indicate depth or motion boundaries. Moreover, occlusions are relevant for motion segmentation and scene flow estimation. In this paper, we present an efficient learning-based approach to estimate occlusion areas jointly with disparities or optical flow. The estimated occlusions and motion boundaries clearly improve over the state-of-the-art. Moreover, we present networks with state-of-the-art performance on the popular KITTI benchmark and good generic performance. Making use of the estimated occlusions, we also show improved results on motion segmentation and scene flow estimation.",0
"The detection of occlusions is crucial in estimating disparity and optical flow as they indicate depth or motion boundaries and matching costs are unavailable in those areas. Additionally, occlusions are significant in motion segmentation and scene flow estimation. In this paper, we introduce a proficient learning-based method that simultaneously estimates occlusion regions and disparities or optical flow. Our approach outperforms the current state-of-the-art in terms of estimated occlusions and motion boundaries. Furthermore, we present networks that exhibit exceptional performance on the popular KITTI benchmark and have good generic performance. By utilizing the estimated occlusions, we also demonstrate improved outcomes in motion segmentation and scene flow estimation.",1
"The electroencephalography classifier is the most important component of brain-computer interface based systems. There are two major problems hindering the improvement of it. First, traditional methods do not fully exploit multimodal information. Second, large-scale annotated EEG datasets are almost impossible to acquire because biological data acquisition is challenging and quality annotation is costly. Herein, we propose a novel deep transfer learning approach to solve these two problems. First, we model cognitive events based on EEG data by characterizing the data using EEG optical flow, which is designed to preserve multimodal EEG information in a uniform representation. Second, we design a deep transfer learning framework which is suitable for transferring knowledge by joint training, which contains a adversarial network and a special loss function. The experiments demonstrate that our approach, when applied to EEG classification tasks, has many advantages, such as robustness and accuracy.",0
"The most crucial element of brain-computer interface systems is the electroencephalography classifier. However, there are two significant obstacles to its improvement. Firstly, conventional methods fail to leverage multimodal information fully. Secondly, obtaining large-scale annotated EEG datasets is nearly impossible due to the difficulty in acquiring biological data and the high cost of annotation. Therefore, our proposed solution is a novel deep transfer learning approach that addresses both problems. We use EEG optical flow to model cognitive events based on EEG data, which preserves multimodal EEG information in a uniform representation. Additionally, we develop a deep transfer learning framework suitable for transferring knowledge by joint training, including an adversarial network and a unique loss function. Our experiments demonstrate that this approach provides several advantages, including robustness and accuracy, when applied to EEG classification tasks.",1
"In interventional radiology, short video sequences of vein structure in motion are captured in order to help medical personnel identify vascular issues or plan intervention. Semantic segmentation can greatly improve the usefulness of these videos by indicating exact position of vessels and instruments, thus reducing the ambiguity. We propose a real-time segmentation method for these tasks, based on U-Net network trained in a Siamese architecture from automatically generated annotations. We make use of noisy low level binary segmentation and optical flow to generate multi class annotations that are successively improved in a multistage segmentation approach. We significantly improve the performance of a state of the art U-Net at the processing speeds of 90fps.",0
"To aid in identifying vascular issues or planning intervention, medical personnel capture short video sequences of vein structure in motion in interventional radiology. Semantic segmentation can enhance the effectiveness of these videos by decreasing ambiguity and indicating the precise position of vessels and instruments. This article presents a real-time segmentation method, utilizing a U-Net network trained in a Siamese architecture with automatically generated annotations. The method employs noisy low-level binary segmentation and optical flow to generate multi-class annotations, which are progressively refined in a multistage segmentation approach. The proposed approach significantly enhances the performance of a state-of-the-art U-Net at speeds of 90fps.",1
"Interest point descriptors have fueled progress on almost every problem in computer vision. Recent advances in deep neural networks have enabled task-specific learned descriptors that outperform hand-crafted descriptors on many problems. We demonstrate that commonly used metric learning approaches do not optimally leverage the feature hierarchies learned in a Convolutional Neural Network (CNN), especially when applied to the task of geometric feature matching. While a metric loss applied to the deepest layer of a CNN, is often expected to yield ideal features irrespective of the task, in fact the growing receptive field as well as striding effects cause shallower features to be better at high precision matching tasks. We leverage this insight together with explicit supervision at multiple levels of the feature hierarchy for better regularization, to learn more effective descriptors in the context of geometric matching tasks. Further, we propose to use activation maps at different layers of a CNN, as an effective and principled replacement for the multi-resolution image pyramids often used for matching tasks. We propose concrete CNN architectures employing these ideas, and evaluate them on multiple datasets for 2D and 3D geometric matching as well as optical flow, demonstrating state-of-the-art results and generalization across datasets.",0
"The progress made in computer vision with interest point descriptors has been significant. Deep neural networks have led to the development of task-specific learned descriptors that surpass hand-crafted descriptors in many cases. However, commonly used metric learning approaches are not effectively utilizing the feature hierarchies learned in a Convolutional Neural Network, particularly in geometric feature matching tasks. While a metric loss applied to the deepest layer of a CNN is expected to yield ideal features, shallower features are often better for high precision matching tasks due to the receptive field and striding effects. To improve regularization and learn more effective descriptors for geometric matching tasks, we use explicit supervision at multiple levels of the feature hierarchy. Additionally, we suggest using activation maps at different layers of a CNN instead of multi-resolution image pyramids for matching tasks. We present concrete CNN architectures using these concepts and demonstrate their effectiveness on multiple datasets for 2D and 3D geometric matching and optical flow, achieving state-of-the-art results and generalization across datasets.",1
"Applying image processing algorithms independently to each frame of a video often leads to undesired inconsistent results over time. Developing temporally consistent video-based extensions, however, requires domain knowledge for individual tasks and is unable to generalize to other applications. In this paper, we present an efficient end-to-end approach based on deep recurrent network for enforcing temporal consistency in a video. Our method takes the original unprocessed and per-frame processed videos as inputs to produce a temporally consistent video. Consequently, our approach is agnostic to specific image processing algorithms applied on the original video. We train the proposed network by minimizing both short-term and long-term temporal losses as well as the perceptual loss to strike a balance between temporal stability and perceptual similarity with the processed frames. At test time, our model does not require computing optical flow and thus achieves real-time speed even for high-resolution videos. We show that our single model can handle multiple and unseen tasks, including but not limited to artistic style transfer, enhancement, colorization, image-to-image translation and intrinsic image decomposition. Extensive objective evaluation and subject study demonstrate that the proposed approach performs favorably against the state-of-the-art methods on various types of videos.",0
"When image processing algorithms are applied independently to each frame of a video, the results are often inconsistent over time. Developing video-based extensions that are temporally consistent requires domain knowledge for each individual task, making it difficult to generalize to other applications. In this paper, we propose an efficient end-to-end approach that uses a deep recurrent network to enforce temporal consistency in videos. Our method takes the original unprocessed and per-frame processed videos as inputs to produce a temporally consistent video, regardless of the specific image processing algorithms applied on the original video. During training, we minimize short-term and long-term temporal losses, as well as perceptual loss, to balance temporal stability and perceptual similarity with the processed frames. At test time, our model achieves real-time speed for high-resolution videos without requiring optical flow computation. Our single model can handle multiple and unseen tasks, such as artistic style transfer, enhancement, colorization, image-to-image translation, and intrinsic image decomposition. Objective evaluations and subject studies show that our approach performs favorably against the state-of-the-art methods for various types of videos.",1
"Spatio-temporal representations in frame sequences play an important role in the task of action recognition. Previously, a method of using optical flow as a temporal information in combination with a set of RGB images that contain spatial information has shown great performance enhancement in the action recognition tasks. However, it has an expensive computational cost and requires two-stream (RGB and optical flow) framework. In this paper, we propose MFNet (Motion Feature Network) containing motion blocks which make it possible to encode spatio-temporal information between adjacent frames in a unified network that can be trained end-to-end. The motion block can be attached to any existing CNN-based action recognition frameworks with only a small additional cost. We evaluated our network on two of the action recognition datasets (Jester and Something-Something) and achieved competitive performances for both datasets by training the networks from scratch.",0
"The representation of space and time in sequences of frames is crucial for recognizing actions. Prior research has shown that combining optical flow as temporal information with a set of RGB images containing spatial information can significantly improve action recognition. However, this approach is computationally expensive and requires a two-stream framework. To address this issue, we introduce MFNet (Motion Feature Network), which includes motion blocks capable of encoding spatio-temporal information between adjacent frames in a single network that can be trained end-to-end. The motion block can be easily integrated into any CNN-based action recognition framework at a minimal cost. We evaluated our network on two action recognition datasets (Jester and Something-Something), and our results were competitive with those obtained by training the networks from scratch.",1
"Despite many advances in deep-learning based semantic segmentation, performance drop due to distribution mismatch is often encountered in the real world. Recently, a few domain adaptation and active learning approaches have been proposed to mitigate the performance drop. However, very little attention has been made toward leveraging information in videos which are naturally captured in most camera systems. In this work, we propose to leverage ""motion prior"" in videos for improving human segmentation in a weakly-supervised active learning setting. By extracting motion information using optical flow in videos, we can extract candidate foreground motion segments (referred to as motion prior) potentially corresponding to human segments. We propose to learn a memory-network-based policy model to select strong candidate segments (referred to as strong motion prior) through reinforcement learning. The selected segments have high precision and are directly used to finetune the model. In a newly collected surveillance camera dataset and a publicly available UrbanStreet dataset, our proposed method improves the performance of human segmentation across multiple scenes and modalities (i.e., RGB to Infrared (IR)). Last but not least, our method is empirically complementary to existing domain adaptation approaches such that additional performance gain is achieved by combining our weakly-supervised active learning approach with domain adaptation approaches.",0
"In spite of advancements in deep-learning based semantic segmentation, real-world scenarios often encounter a decline in performance due to distribution mismatch. To alleviate this issue, a few domain adaptation and active learning methods have been suggested, but minimal attention has been paid to utilizing information in videos that are naturally captured in most camera systems. This study proposes enhancing human segmentation in a weakly-supervised active learning setting by utilizing ""motion prior"" in videos. By extracting motion data using optical flow in videos, candidate foreground motion segments (referred to as motion prior) that potentially correspond to human segments can be extracted. The study suggests learning a memory-network-based policy model for selecting strong candidate segments (referred to as strong motion prior) through reinforcement learning. The selected segments have high precision and are directly used for fine-tuning the model. Across various scenes and modalities (i.e., RGB to Infrared (IR)), the proposed method improves the performance of human segmentation in a newly collected surveillance camera dataset and a publicly available UrbanStreet dataset. Furthermore, the study's method is empirically complementary to existing domain adaptation approaches, resulting in additional performance gain by combining the weakly-supervised active learning approach with domain adaptation approaches.",1
"Estimation of 3D motion in a dynamic scene from a temporal pair of images is a core task in many scene understanding problems. In real world applications, a dynamic scene is commonly captured by a moving camera (i.e., panning, tilting or hand-held), increasing the task complexity because the scene is observed from different view points. The main challenge is the disambiguation of the camera motion from scene motion, which becomes more difficult as the amount of rigidity observed decreases, even with successful estimation of 2D image correspondences. Compared to other state-of-the-art 3D scene flow estimation methods, in this paper we propose to \emph{learn} the rigidity of a scene in a supervised manner from a large collection of dynamic scene data, and directly infer a rigidity mask from two sequential images with depths. With the learned network, we show how we can effectively estimate camera motion and projected scene flow using computed 2D optical flow and the inferred rigidity mask. For training and testing the rigidity network, we also provide a new semi-synthetic dynamic scene dataset (synthetic foreground objects with a real background) and an evaluation split that accounts for the percentage of observed non-rigid pixels. Through our evaluation we show the proposed framework outperforms current state-of-the-art scene flow estimation methods in challenging dynamic scenes.",0
"The task of estimating 3D motion in a dynamic scene from a pair of images over time is a fundamental problem in many scene understanding tasks. When dealing with real-world scenarios, a dynamic scene is typically captured by a moving camera, which increases the complexity of the task as the scene is observed from various viewpoints. The primary challenge lies in distinguishing camera motion from scene motion, which becomes more challenging as the rigidity of the scene decreases, even if 2D image correspondences are successfully estimated. In contrast to other advanced 3D scene flow estimation methods, this paper proposes a supervised learning approach for determining the rigidity of a scene using a vast collection of dynamic scene data. The proposed method directly infers a rigidity mask from two sequential images with depths. By utilizing the learned network, camera motion and projected scene flow can be accurately estimated using computed 2D optical flow and the inferred rigidity mask. Furthermore, a new semi-synthetic dynamic scene dataset is provided for training and testing the rigidity network, along with an evaluation split that considers the percentage of observed non-rigid pixels. The proposed framework outperforms current state-of-the-art scene flow estimation methods in challenging dynamic scenes, as demonstrated by the evaluations.",1
"We use large amounts of unlabeled video to learn models for visual tracking without manual human supervision. We leverage the natural temporal coherency of color to create a model that learns to colorize gray-scale videos by copying colors from a reference frame. Quantitative and qualitative experiments suggest that this task causes the model to automatically learn to track visual regions. Although the model is trained without any ground-truth labels, our method learns to track well enough to outperform the latest methods based on optical flow. Moreover, our results suggest that failures to track are correlated with failures to colorize, indicating that advancing video colorization may further improve self-supervised visual tracking.",0
"Our approach involves utilizing a vast amount of unlabeled video data to develop visual tracking models without any manual human oversight. By exploiting the inherent temporal consistency of color, we have created a model that can colorize monochrome videos by replicating colors from a reference frame. Our experiments, both quantitative and qualitative, indicate that this technique leads the model to learn how to track visual regions on its own, without the need for any ground-truth labels. Remarkably, our method yields better results than the most recent optical flow-based approaches, despite not being trained with any such labels. Additionally, our findings suggest that any issues with tracking are associated with difficulties in colorization, implying that enhancing video colorization could further improve self-supervised visual tracking.",1
"Classical computation of optical flow involves generic priors (regularizers) that capture rudimentary statistics of images, but not long-range correlations or semantics. On the other hand, fully supervised methods learn the regularity in the annotated data, without explicit regularization and with the risk of overfitting. We seek to learn richer priors on the set of possible flows that are statistically compatible with an image. Once the prior is learned in a supervised fashion, one can easily learn the full map to infer optical flow directly from two or more images, without any need for (additional) supervision. We introduce a novel architecture, called Conditional Prior Network (CPN), and show how to train it to yield a conditional prior. When used in conjunction with a simple optical flow architecture, the CPN beats all variational methods and all unsupervised learning-based ones using the same data term. It performs comparably to fully supervised ones, that however are fine-tuned to a particular dataset. Our method, on the other hand, performs well even when transferred between datasets.",0
"The traditional method of computing optical flow involves using regularizers that only capture basic image statistics and not long-range correlations or semantics. In contrast, fully supervised methods learn regularities from labeled data without explicit regularization but with a risk of overfitting. Our goal is to acquire a more detailed prior on potential flows that are statistically consistent with an image. By training our novel Conditional Prior Network (CPN) in a supervised manner, we can learn this prior and then use it to infer optical flow directly from two or more images without additional supervision. Our CPN architecture outperforms all variational methods and unsupervised learning-based methods utilizing the same data term and performs similarly to fully supervised methods that are fine-tuned to a specific dataset. However, our approach performs well even when transferred between datasets.",1
"In this paper our objectives are, first, networks that can embed audio and visual inputs into a common space that is suitable for cross-modal retrieval; and second, a network that can localize the object that sounds in an image, given the audio signal. We achieve both these objectives by training from unlabelled video using only audio-visual correspondence (AVC) as the objective function. This is a form of cross-modal self-supervision from video.   To this end, we design new network architectures that can be trained for cross-modal retrieval and localizing the sound source in an image, by using the AVC task. We make the following contributions: (i) show that audio and visual embeddings can be learnt that enable both within-mode (e.g. audio-to-audio) and between-mode retrieval; (ii) explore various architectures for the AVC task, including those for the visual stream that ingest a single image, or multiple images, or a single image and multi-frame optical flow; (iii) show that the semantic object that sounds within an image can be localized (using only the sound, no motion or flow information); and (iv) give a cautionary tale on how to avoid undesirable shortcuts in the data preparation.",0
"The aim of this study is twofold: firstly, to create networks that can integrate audio and visual inputs into a shared space for cross-modal retrieval; and secondly, to develop a network that can identify the location of a sounding object in an image based solely on the audio signal. To accomplish these objectives, we employ cross-modal self-supervision from video by using audio-visual correspondence (AVC) as the objective function. Thus, we propose novel network architectures for the task of cross-modal retrieval and sound source localization, and our contributions include demonstrating the ability to learn audio and visual embeddings for within-mode and between-mode retrieval, exploring various AVC architectures for the visual stream, showing the capability to localize the semantic object that produces the sound in an image using only sound information, and highlighting the importance of avoiding undesirable data preparation shortcuts.",1
"This paper proposes the first non-flow-based deep framework for high dynamic range (HDR) imaging of dynamic scenes with large-scale foreground motions. In state-of-the-art deep HDR imaging, input images are first aligned using optical flows before merging, which are still error-prone due to occlusion and large motions. In stark contrast to flow-based methods, we formulate HDR imaging as an image translation problem without optical flows. Moreover, our simple translation network can automatically hallucinate plausible HDR details in the presence of total occlusion, saturation and under-exposure, which are otherwise almost impossible to recover by conventional optimization approaches. Our framework can also be extended for different reference images. We performed extensive qualitative and quantitative comparisons to show that our approach produces excellent results where color artifacts and geometric distortions are significantly reduced compared to existing state-of-the-art methods, and is robust across various inputs, including images without radiometric calibration.",0
"The aim of this study is to introduce a novel deep framework for high dynamic range (HDR) imaging of dynamic scenes with large-scale foreground motions that does not rely on optical flows. Although previous deep HDR imaging methods have utilized optical flows to align input images before merging, this approach is susceptible to errors due to occlusion and large motions. Our framework, on the other hand, formulates HDR imaging as an image translation problem without optical flows, and utilizes a simple translation network that can generate plausible HDR details even in the presence of total occlusion, saturation, and under-exposure. Furthermore, our approach is adaptable to different reference images. We conducted extensive qualitative and quantitative comparisons to demonstrate that our method produces superior results with reduced color artifacts and geometric distortions compared to existing state-of-the-art methods. Additionally, our framework is resilient to various inputs, including images without radiometric calibration.",1
"We propose a Spatiotemporal Sampling Network (STSN) that uses deformable convolutions across time for object detection in videos. Our STSN performs object detection in a video frame by learning to spatially sample features from the adjacent frames. This naturally renders the approach robust to occlusion or motion blur in individual frames. Our framework does not require additional supervision, as it optimizes sampling locations directly with respect to object detection performance. Our STSN outperforms the state-of-the-art on the ImageNet VID dataset and compared to prior video object detection methods it uses a simpler design, and does not require optical flow data for training.",0
"To detect objects in videos, we suggest using a Spatiotemporal Sampling Network (STSN) that utilizes deformable convolutions over time. The STSN identifies objects in a video frame by learning how to sample features from the neighboring frames, which makes it resistant to interference such as occlusion or motion blur within individual frames. Our approach does not require extra supervision, as it optimizes the sampling locations directly to enhance object detection performance. Our STSN is superior to the current state-of-the-art on the ImageNet VID dataset and has a simpler design compared to previous video object detection methods. Additionally, it does not need optical flow data for training.",1
"Electroencephalography (EEG) has become the most significant input signal for brain computer interface (BCI) based systems. However, it is very difficult to obtain satisfactory classification accuracy due to traditional methods can not fully exploit multimodal information. Herein, we propose a novel approach to modeling cognitive events from EEG data by reducing it to a video classification problem, which is designed to preserve the multimodal information of EEG. In addition, optical flow is introduced to represent the variant information of EEG. We train a deep neural network (DNN) with convolutional neural network (CNN) and recurrent neural network (RNN) for the EEG classification task by using EEG video and optical flow. The experiments demonstrate that our approach has many advantages, such as more robustness and more accuracy in EEG classification tasks. According to our approach, we designed a mixed BCI-based rehabilitation support system to help stroke patients perform some basic operations.",0
"EEG is the primary input signal for brain computer interface (BCI) systems, but obtaining satisfactory classification accuracy is challenging due to traditional methods not fully utilizing multimodal information. To address this, we propose a novel approach that treats EEG data as a video classification problem, preserving its multimodal information. We introduce optical flow to represent variant information and train a deep neural network (DNN) with convolutional and recurrent neural networks (CNN and RNN) for EEG classification. Our approach offers more robustness and accuracy, as demonstrated through experiments. Using this approach, we developed a mixed BCI-based rehabilitation support system to aid stroke patients in performing basic operations.",1
"The optical flow of humans is well known to be useful for the analysis of human action. Given this, we devise an optical flow algorithm specifically for human motion and show that it is superior to generic flow methods. Designing a method by hand is impractical, so we develop a new training database of image sequences with ground truth optical flow. For this we use a 3D model of the human body and motion capture data to synthesize realistic flow fields. We then train a convolutional neural network to estimate human flow fields from pairs of images. Since many applications in human motion analysis depend on speed, and we anticipate mobile applications, we base our method on SpyNet with several modifications. We demonstrate that our trained network is more accurate than a wide range of top methods on held-out test data and that it generalizes well to real image sequences. When combined with a person detector/tracker, the approach provides a full solution to the problem of 2D human flow estimation. Both the code and the dataset are available for research.",0
"The usefulness of optical flow in analyzing human action is widely recognized. To improve this technique, we developed an optical flow algorithm specifically for human motion, which outperforms generic flow methods. Due to the impracticality of designing a method manually, we created a new training database of image sequences with ground truth optical flow using a 3D model of the human body and motion capture data to synthesize realistic flow fields. A convolutional neural network was then trained to estimate human flow fields from image pairs. As speed is critical in many human motion analysis applications, and we expect to use this algorithm in mobile applications, we based our method on SpyNet with several modifications. Our results demonstrate that our trained network is more accurate than a wide range of top methods on held-out test data and generalizes well to real image sequences. Combining this approach with a person detector/tracker provides a comprehensive solution to the problem of 2D human flow estimation. The code and dataset are both available for research purposes.",1
"The general ability to analyze and classify the 3D kinematics of the human form is an essential step in the development of socially adept humanoid robots. A variety of different types of signals can be used by machines to represent and characterize actions such as RGB videos, infrared maps, and optical flow. In particular, skeleton sequences provide a natural 3D kinematic description of human motions and can be acquired in real time using RGB+D cameras. Moreover, skeleton sequences are generalizable to characterize the motions of both humans and humanoid robots. The Globally Optimal Reparameterization Algorithm (GORA) is a novel, recently proposed algorithm for signal alignment in which signals are reparameterized to a globally optimal universal standard timescale (UST). Here, we introduce a variant of GORA for humanoid action recognition with skeleton sequences, which we call GORA-S. We briefly review the algorithm's mathematical foundations and contextualize them in the problem of action recognition with skeleton sequences. Subsequently, we introduce GORA-S and discuss parameters and numerical techniques for its effective implementation. We then compare its performance with that of the DTW and FastDTW algorithms, in terms of computational efficiency and accuracy in matching skeletons. Our results show that GORA-S attains a complexity that is significantly less than that of any tested DTW method. In addition, it displays a favorable balance between speed and accuracy that remains invariant under changes in skeleton sampling frequency, lending it a degree of versatility that could make it well-suited for a variety of action recognition tasks.",0
"Developing socially skilled humanoid robots requires the ability to analyze and categorize 3D human kinematics. Machines can use various signals, such as RGB videos, infrared maps, and optical flow to represent actions, but skeleton sequences are particularly useful as they provide a natural 3D kinematic description of human motions and can be obtained in real time using RGB+D cameras. Skeleton sequences can also be used to characterize the movements of both humans and humanoid robots. The Globally Optimal Reparameterization Algorithm (GORA) is a newly proposed algorithm for signal alignment that reparameterizes signals to a globally optimal universal standard timescale (UST). In this study, we introduce GORA-S, a variant of GORA designed for humanoid action recognition using skeleton sequences. We provide an overview of the algorithm's mathematical foundations and its application to the problem of action recognition with skeleton sequences. We then discuss the parameters and numerical techniques used to implement GORA-S effectively, and compare its performance with that of the DTW and FastDTW algorithms in terms of accuracy and computational efficiency. Our results show that GORA-S has significantly less complexity than any tested DTW method and offers a favorable balance between speed and accuracy that remains consistent across different skeleton sampling frequencies, making it a versatile tool for various action recognition tasks.",1
"Optical flow, semantic segmentation, and surface normals represent different information modalities, yet together they bring better cues for scene understanding problems. In this paper, we study the influence between the three modalities: how one impacts on the others and their efficiency in combination. We employ a modular approach using a convolutional refinement network which is trained supervised but isolated from RGB images to enforce joint modality features. To assist the training process, we create a large-scale synthetic outdoor dataset that supports dense annotation of semantic segmentation, optical flow, and surface normals. The experimental results show positive influence among the three modalities, especially for objects' boundaries, region consistency, and scene structures.",0
"The combination of optical flow, semantic segmentation, and surface normals provides improved cues for understanding scenes, despite each representing different information. This study aims to investigate the interplay between these modalities, exploring their impact on each other and their effectiveness when combined. Our approach involves a modular system that utilizes a convolutional refinement network trained in isolation from RGB images, with joint modality features enforced. To aid in training, we develop a vast synthetic outdoor dataset with dense annotation of semantic segmentation, optical flow, and surface normals. Our experiments demonstrate a positive correlation between the modalities, particularly regarding object boundaries, region consistency, and scene structures.",1
"We propose a novel method for learning convolutional neural image representations without manual supervision. We use motion cues in the form of optical flow, to supervise representations of static images. The obvious approach of training a network to predict flow from a single image can be needlessly difficult due to intrinsic ambiguities in this prediction task. We instead propose a much simpler learning goal: embed pixels such that the similarity between their embeddings matches that between their optical flow vectors. At test time, the learned deep network can be used without access to video or flow information and transferred to tasks such as image classification, detection, and segmentation. Our method, which significantly simplifies previous attempts at using motion for self-supervision, achieves state-of-the-art results in self-supervision using motion cues, competitive results for self-supervision in general, and is overall state of the art in self-supervised pretraining for semantic image segmentation, as demonstrated on standard benchmarks.",0
"Our method introduces a new way to learn convolutional neural image representations without the need for manual supervision. By utilizing motion cues in the form of optical flow, we can supervise the representations of static images. However, predicting flow from a single image can be difficult due to inherent ambiguities in the task. Instead, we propose a simpler learning objective: embedding pixels in a way that ensures the similarity between their embeddings matches that of their optical flow vectors. This approach allows our deep network to be used in tasks such as image classification, detection, and segmentation without requiring access to video or flow information. Our method simplifies previous attempts at using motion for self-supervision and achieves state-of-the-art results in self-supervision using motion cues, competitive results for self-supervision in general, and is overall state of the art in self-supervised pretraining for semantic image segmentation, as demonstrated on standard benchmarks.",1
"Given two consecutive frames, video interpolation aims at generating intermediate frame(s) to form both spatially and temporally coherent video sequences. While most existing methods focus on single-frame interpolation, we propose an end-to-end convolutional neural network for variable-length multi-frame video interpolation, where the motion interpretation and occlusion reasoning are jointly modeled. We start by computing bi-directional optical flow between the input images using a U-Net architecture. These flows are then linearly combined at each time step to approximate the intermediate bi-directional optical flows. These approximate flows, however, only work well in locally smooth regions and produce artifacts around motion boundaries. To address this shortcoming, we employ another U-Net to refine the approximated flow and also predict soft visibility maps. Finally, the two input images are warped and linearly fused to form each intermediate frame. By applying the visibility maps to the warped images before fusion, we exclude the contribution of occluded pixels to the interpolated intermediate frame to avoid artifacts. Since none of our learned network parameters are time-dependent, our approach is able to produce as many intermediate frames as needed. We use 1,132 video clips with 240-fps, containing 300K individual video frames, to train our network. Experimental results on several datasets, predicting different numbers of interpolated frames, demonstrate that our approach performs consistently better than existing methods.",0
"Video interpolation involves generating intermediate frames between two consecutive frames in a video sequence to create a coherent temporal and spatial flow. While most existing methods deal with single-frame interpolation, we propose an end-to-end convolutional neural network that can handle multi-frame interpolation of varying lengths, taking into account motion interpretation and occlusion reasoning. Our approach begins by using a U-Net architecture to compute bi-directional optical flow between the input images. However, the approximate flows produced by this method are only effective in locally smooth regions and can lead to artifacts around motion boundaries. To address this issue, we use another U-Net to refine the approximated flow and predict soft visibility maps. The input images are then warped and fused to form each intermediate frame, and the visibility maps are applied to exclude occluded pixels from contributing to the interpolated frame. Our approach is able to produce as many intermediate frames as required, as none of our learned network parameters depend on time. We trained our network on 1,132 video clips with 240-fps, containing 300K individual video frames, and our experimental results on various datasets demonstrate that our approach outperforms existing methods in consistently predicting different numbers of interpolated frames.",1
"Real-time moving object detection in unconstrained scenes is a difficult task due to dynamic background, changing foreground appearance and limited computational resource. In this paper, an optical flow based moving object detection framework is proposed to address this problem. We utilize homography matrixes to online construct a background model in the form of optical flow. When judging out moving foregrounds from scenes, a dual-mode judge mechanism is designed to heighten the system's adaptation to challenging situations. In experiment part, two evaluation metrics are redefined for more properly reflecting the performance of methods. We quantitatively and qualitatively validate the effectiveness and feasibility of our method with videos in various scene conditions. The experimental results show that our method adapts itself to different situations and outperforms the state-of-the-art methods, indicating the advantages of optical flow based methods.",0
"Detecting moving objects in real-time in unconstrained scenes is a challenging task due to factors such as dynamic background, changing foreground appearance, and limited computational resources. To address this problem, this paper proposes an optical flow-based framework for moving object detection. Homography matrices are used to construct an online background model in the form of optical flow. To improve the system's adaptation to challenging situations when detecting moving foregrounds, a dual-mode judge mechanism is designed. In the experimental section, two evaluation metrics are redefined to better reflect the performance of the methods. The effectiveness and feasibility of the proposed method are validated quantitatively and qualitatively with videos in various scene conditions. The experimental results demonstrate that the proposed method is adaptable to different situations and performs better than state-of-the-art methods, highlighting the advantages of optical flow-based approaches.",1
"Motion representation plays a vital role in human action recognition in videos. In this study, we introduce a novel compact motion representation for video action recognition, named Optical Flow guided Feature (OFF), which enables the network to distill temporal information through a fast and robust approach. The OFF is derived from the definition of optical flow and is orthogonal to the optical flow. The derivation also provides theoretical support for using the difference between two frames. By directly calculating pixel-wise spatiotemporal gradients of the deep feature maps, the OFF could be embedded in any existing CNN based video action recognition framework with only a slight additional cost. It enables the CNN to extract spatiotemporal information, especially the temporal information between frames simultaneously. This simple but powerful idea is validated by experimental results. The network with OFF fed only by RGB inputs achieves a competitive accuracy of 93.3% on UCF-101, which is comparable with the result obtained by two streams (RGB and optical flow), but is 15 times faster in speed. Experimental results also show that OFF is complementary to other motion modalities such as optical flow. When the proposed method is plugged into the state-of-the-art video action recognition framework, it has 96:0% and 74:2% accuracy on UCF-101 and HMDB-51 respectively. The code for this project is available at https://github.com/kevin-ssy/Optical-Flow-Guided-Feature.",0
"The recognition of human actions in videos depends heavily on motion representation. A new motion representation method called Optical Flow guided Feature (OFF) has been developed for video action recognition. This approach allows the network to extract temporal information in a fast and reliable manner. By using the definition of optical flow, OFF is derived in a way that is orthogonal to optical flow and provides a theoretical basis for using the difference between two frames. The pixel-wise spatiotemporal gradients of the deep feature maps are calculated to embed OFF into any existing CNN-based video action recognition framework with minimal additional cost. The incorporation of OFF allows the CNN to extract spatiotemporal information, particularly the temporal information between frames, simultaneously. This simple yet potent idea has been verified by experimental findings. When fed only by RGB inputs, the network with OFF achieves a competitive accuracy of 93.3% on UCF-101, comparable to that obtained by two streams (RGB and optical flow) but 15 times faster in speed. Experimental results also demonstrate that OFF is complementary to other motion modalities, such as optical flow. When integrated into the state-of-the-art video action recognition framework, the proposed method achieves accuracy rates of 96:0% and 74:2% on UCF-101 and HMDB-51, respectively. The code for this project is available at https://github.com/kevin-ssy/Optical-Flow-Guided-Feature.",1
"In this paper, we present supervision-by-registration, an unsupervised approach to improve the precision of facial landmark detectors on both images and video. Our key observation is that the detections of the same landmark in adjacent frames should be coherent with registration, i.e., optical flow. Interestingly, the coherency of optical flow is a source of supervision that does not require manual labeling, and can be leveraged during detector training. For example, we can enforce in the training loss function that a detected landmark at frame$_{t-1}$ followed by optical flow tracking from frame$_{t-1}$ to frame$_t$ should coincide with the location of the detection at frame$_t$. Essentially, supervision-by-registration augments the training loss function with a registration loss, thus training the detector to have output that is not only close to the annotations in labeled images, but also consistent with registration on large amounts of unlabeled videos. End-to-end training with the registration loss is made possible by a differentiable Lucas-Kanade operation, which computes optical flow registration in the forward pass, and back-propagates gradients that encourage temporal coherency in the detector. The output of our method is a more precise image-based facial landmark detector, which can be applied to single images or video. With supervision-by-registration, we demonstrate (1) improvements in facial landmark detection on both images (300W, ALFW) and video (300VW, Youtube-Celebrities), and (2) significant reduction of jittering in video detections.",0
"This paper introduces supervision-by-registration, an unsupervised technique that enhances the accuracy of facial landmark detectors in images and videos. The approach relies on the coherence of optical flow between adjacent frames, which serves as a form of supervision without the need for manual labeling. By incorporating a registration loss into the training loss function, the detector is trained to produce outputs that are not only close to annotations in labeled images but also consistent with registration on large amounts of unlabeled videos. The differentiable Lucas-Kanade operation enables end-to-end training with the registration loss, resulting in a more precise facial landmark detector applicable to both images and videos. The method's effectiveness is demonstrated by improvements in facial landmark detection in various datasets and significant reduction of jittering in video detections.",1
"This paper addresses spatio-temporal localization of human actions in video. In order to localize actions in time, we propose a recurrent localization network (RecLNet) designed to model the temporal structure of actions on the level of person tracks. Our model is trained to simultaneously recognize and localize action classes in time and is based on two layer gated recurrent units (GRU) applied separately to two streams, i.e. appearance and optical flow streams. When used together with state-of-the-art person detection and tracking, our model is shown to improve substantially spatio-temporal action localization in videos. The gain is shown to be mainly due to improved temporal localization. We evaluate our method on two recent datasets for spatio-temporal action localization, UCF101-24 and DALY, demonstrating a significant improvement of the state of the art.",0
"The focus of this paper is on the spatio-temporal localization of human actions in videos. To achieve this, we introduce a recurrent localization network (RecLNet) that is specifically designed to model the temporal structure of actions at the level of person tracks. Our approach involves training the model to recognize and localize action classes in time, through the use of two layer gated recurrent units (GRU) applied separately to appearance and optical flow streams. When combined with advanced person detection and tracking methods, our model is found to greatly enhance spatio-temporal action localization in videos, primarily by improving the temporal localization aspect. Our approach is evaluated on two recent datasets for spatio-temporal action localization (UCF101-24 and DALY), and we demonstrate a significant advancement over the existing state of the art.",1
"De-fencing is to eliminate the captured fence on an image or a video, providing a clear view of the scene. It has been applied for many purposes including assisting photographers and improving the performance of computer vision algorithms such as object detection and recognition. However, the state-of-the-art de-fencing methods have limited performance caused by the difficulty of fence segmentation and also suffer from the motion of the camera or objects. To overcome these problems, we propose a novel method consisting of segmentation using convolutional neural networks and a fast/robust recovery algorithm. The segmentation algorithm using convolutional neural network achieves significant improvement in the accuracy of fence segmentation. The recovery algorithm using optical flow produces plausible de-fenced images and videos. The proposed method is experimented on both our diverse and complex dataset and publicly available datasets. The experimental results demonstrate that the proposed method achieves the state-of-the-art performance for both segmentation and content recovery.",0
"De-fencing refers to the process of removing a fence from an image or video to obtain an unobstructed view of the scene. This technique has been utilized for various purposes, such as aiding photographers and enhancing computer vision algorithms like object recognition and detection. However, current de-fencing methods face performance limitations due to the challenging task of fence segmentation and the presence of camera or object motion. To address these issues, we present a new approach that employs a convolutional neural network segmentation algorithm and a fast/robust recovery algorithm. The segmentation algorithm significantly improves fence segmentation accuracy, while the recovery algorithm, which utilizes optical flow, generates realistic de-fenced images and videos. Our method was tested on both our diverse, complex dataset and publicly available datasets, and the results show that it outperforms existing methods in both segmentation and content recovery.",1
"We present a compact but effective CNN model for optical flow, called PWC-Net. PWC-Net has been designed according to simple and well-established principles: pyramidal processing, warping, and the use of a cost volume. Cast in a learnable feature pyramid, PWC-Net uses the cur- rent optical flow estimate to warp the CNN features of the second image. It then uses the warped features and features of the first image to construct a cost volume, which is processed by a CNN to estimate the optical flow. PWC-Net is 17 times smaller in size and easier to train than the recent FlowNet2 model. Moreover, it outperforms all published optical flow methods on the MPI Sintel final pass and KITTI 2015 benchmarks, running at about 35 fps on Sintel resolution (1024x436) images. Our models are available on https://github.com/NVlabs/PWC-Net.",0
"Introducing PWC-Net, a CNN model for optical flow that is both compact and effective. PWC-Net utilizes straightforward and established techniques such as pyramidal processing, warping, and the use of a cost volume. The model employs a learnable feature pyramid, where the current optical flow estimate warps the CNN features of the second image. This produces a cost volume, which is processed by a CNN to generate an optical flow estimate. PWC-Net is much smaller and easier to train than FlowNet2, and it surpasses all previously published optical flow methods on the MPI Sintel final pass and KITTI 2015 benchmarks. It also operates at around 35 fps on Sintel resolution (1024x436) images. Our models can be accessed at https://github.com/NVlabs/PWC-Net.",1
"This note describes the details of our solution to the dense-captioning events in videos task of ActivityNet Challenge 2018. Specifically, we solve this problem with a two-stage way, i.e., first temporal event proposal and then sentence generation. For temporal event proposal, we directly leverage the three-stage workflow in [13, 16]. For sentence generation, we capitalize on LSTM-based captioning framework with temporal attention mechanism (dubbed as LSTM-T). Moreover, the input visual sequence to the LSTM-based video captioning model is comprised of RGB and optical flow images. At inference, we adopt a late fusion scheme to fuse the two LSTM-based captioning models for sentence generation.",0
"The focus of this note is on our approach to the dense-captioning events in videos task for ActivityNet Challenge 2018. Our solution to this problem involves a two-stage process. First, we use a three-stage workflow from [13, 16] for temporal event proposal. Second, we utilize an LSTM-based captioning framework with temporal attention mechanism (known as LSTM-T) for sentence generation. In addition, the input visual sequence for the LSTM-based video captioning model consists of both RGB and optical flow images. During inference, we apply a late fusion scheme to combine the outputs of the two LSTM-based captioning models for sentence generation.",1
"Deep learning is ubiquitous across many areas areas of computer vision. It often requires large scale datasets for training before being fine-tuned on small-to-medium scale problems. Activity, or, in other words, action recognition, is one of many application areas of deep learning. While there exist many Convolutional Neural Network architectures that work with the RGB and optical flow frames, training on the time sequences of 3D body skeleton joints is often performed via recurrent networks such as LSTM.   In this paper, we propose a new representation which encodes sequences of 3D body skeleton joints in texture-like representations derived from mathematically rigorous kernel methods. Such a representation becomes the first layer in a standard CNN network e.g., ResNet-50, which is then used in the supervised domain adaptation pipeline to transfer information from the source to target dataset. This lets us leverage the available Kinect-based data beyond training on a single dataset and outperform simple fine-tuning on any two datasets combined in a naive manner. More specifically, in this paper we utilize the overlapping classes between datasets. We associate datapoints of the same class via so-called commonality, known from the supervised domain adaptation. We demonstrate state-of-the-art results on three publicly available benchmarks.",0
"The use of deep learning is widespread in various domains of computer vision, and it typically necessitates extensive datasets for training before being customized for smaller-scale issues. One of the many applications of deep learning is activity recognition, which involves identifying actions. While Convolutional Neural Network structures have been developed to deal with RGB and optical flow frames, recurrent networks like LSTM are frequently employed to train on time sequences of 3D body skeleton joints. In this article, we suggest a new approach that transforms sequences of 3D body skeleton joints into texture-like representations using rigorous mathematical kernel methods. This representation is then utilized as the initial layer in a standard CNN network like ResNet-50, which is subsequently used in the supervised domain adaptation pipeline to transfer knowledge from the source to target datasets. This allows us to make use of the Kinect-based data beyond just training on a single dataset and outperform simple fine-tuning on any two datasets combined in a naive manner. The overlapping classes between datasets are used in this article, and data points of the same class are linked through a process known as commonality, which is used in supervised domain adaptation. We demonstrate state-of-the-art performance on three publicly accessible benchmarks.",1
"We consider the problem of learning to play first-person shooter (FPS) video games using raw screen images as observations and keyboard inputs as actions. The high-dimensionality of the observations in this type of applications leads to prohibitive needs of training data for model-free methods, such as the deep Q-network (DQN), and its recurrent variant DRQN. Thus, recent works focused on learning low-dimensional representations that may reduce the need for data. This paper presents a new and efficient method for learning such representations. Salient segments of consecutive frames are detected from their optical flow, and clustered based on their feature descriptors. The clusters typically correspond to different discovered categories of objects. Segments detected in new frames are then classified based on their nearest clusters. Because only a few categories are relevant to a given task, the importance of a category is defined as the correlation between its occurrence and the agent's performance. The result is encoded as a vector indicating objects that are in the frame and their locations, and used as a side input to DRQN. Experiments on the game Doom provide a good evidence for the benefit of this approach.",0
"This study examines the challenge of mastering first-person shooter (FPS) video games through the use of raw screen images and keyboard inputs. The large volume of observations in this context makes it difficult for model-free methods, such as DQN and DRQN, to be effective without substantial amounts of training data. As a result, recent research has focused on discovering low-dimensional representations that can reduce the need for data. This paper introduces a novel and efficient approach to learning such representations. The method identifies significant segments of successive frames through optical flow analysis and clustering techniques, which often correspond to distinct object categories. New frames are then classified based on their proximity to these clusters. As only a small number of categories are relevant to each task, the importance of a category is determined by its correlation with the agent's performance. This information is encoded as a vector indicating object presence and location and used as a side input to DRQN. Experiments using Doom demonstrate the effectiveness of this approach.",1
"In this paper, we propose to improve the traditional use of RNNs by employing a many to many model for video classification. We analyze the importance of modeling spatial layout and temporal encoding for daily living action recognition. Many RGB methods focus only on short term temporal information obtained from optical flow. Skeleton based methods on the other hand show that modeling long term skeleton evolution improves action recognition accuracy. In this work, we propose a deep-temporal LSTM architecture which extends standard LSTM and allows better encoding of temporal information. In addition, we propose to fuse 3D skeleton geometry with deep static appearance. We validate our approach on public available CAD60, MSRDailyActivity3D and NTU-RGB+D, achieving competitive performance as compared to the state-of-the art.",0
"The aim of this paper is to enhance the conventional use of RNNs by utilizing a many to many model for video classification. Our focus is on assessing the significance of modeling spatial layout and temporal encoding for recognizing daily living actions. While RGB techniques mainly concentrate on short term temporal information from optical flow, skeleton based methods prove that modeling long term skeleton evolution enhances action recognition accuracy. To achieve this, we propose a deep-temporal LSTM architecture that extends standard LSTM and facilitates better encoding of temporal information. Additionally, we suggest fusing 3D skeleton geometry with deep static appearance. Our approach is tested on publicly available CAD60, MSRDailyActivity3D and NTU-RGB+D datasets, and yields competitive performance as compared to the state-of-the art.",1
"In this paper, we introduce our submissions for the tasks of trimmed activity recognition (Kinetics) and trimmed event recognition (Moments in Time) for Activitynet Challenge 2018. In the two tasks, non-local neural networks and temporal segment networks are implemented as our base models. Multi-modal cues such as RGB image, optical flow and acoustic signal have also been used in our method. We also propose new non-local-based models for further improvement on the recognition accuracy. The final submissions after ensembling the models achieve 83.5% top-1 accuracy and 96.8% top-5 accuracy on the Kinetics validation set, 35.81% top-1 accuracy and 62.59% top-5 accuracy on the MIT validation set.",0
"Our paper presents our entries for Activitynet Challenge 2018 in the categories of trimmed activity recognition (Kinetics) and trimmed event recognition (Moments in Time). To accomplish these tasks, we used non-local neural networks and temporal segment networks as our foundational models, along with various multi-modal cues such as RGB image, optical flow, and acoustic signals. Furthermore, we developed new non-local-based models to enhance the accuracy of recognition. As a result, our combined models achieved 83.5% top-1 accuracy and 96.8% top-5 accuracy on the Kinetics validation set, and 35.81% top-1 accuracy and 62.59% top-5 accuracy on the MIT validation set.",1
"Temporal coherence is a valuable source of information in the context of optical flow estimation. However, finding a suitable motion model to leverage this information is a non-trivial task. In this paper we propose an unsupervised online learning approach based on a convolutional neural network (CNN) that estimates such a motion model individually for each frame. By relating forward and backward motion these learned models not only allow to infer valuable motion information based on the backward flow, they also help to improve the performance at occlusions, where a reliable prediction is particularly useful. Moreover, our learned models are spatially variant and hence allow to estimate non-rigid motion per construction. This, in turns, allows to overcome the major limitation of recent rigidity-based approaches that seek to improve the estimation by incorporating additional stereo/SfM constraints. Experiments demonstrate the usefulness of our new approach. They not only show a consistent improvement of up to 27% for all major benchmarks (KITTI 2012, KITTI 2015, MPI Sintel) compared to a baseline without prediction, they also show top results for the MPI Sintel benchmark -- the one of the three benchmarks that contains the largest amount of non-rigid motion.",0
"Optical flow estimation can benefit greatly from temporal coherence. However, finding an appropriate motion model to utilize this information is challenging. This study presents an unsupervised online learning technique that employs a convolutional neural network (CNN) to estimate a motion model for each frame. These models relate forward and backward motion, enabling the inference of valuable motion information based on backward flow and enhancing performance at occlusions where reliable prediction is crucial. Additionally, the learned models are spatially variant, allowing the estimation of non-rigid motion. This overcomes the limitations of recent rigidity-based methods that require additional stereo/SfM constraints. Experiments demonstrate the effectiveness of this approach, with consistent improvements of up to 27% for all major benchmarks (KITTI 2012, KITTI 2015, MPI Sintel) compared to a baseline without prediction. The technique also achieves top results for the MPI Sintel benchmark, which contains a significant amount of non-rigid motion.",1
"We present an algorithm (SOFAS) to estimate the optical flow of events generated by a dynamic vision sensor (DVS). Where traditional cameras produce frames at a fixed rate, DVSs produce asynchronous events in response to intensity changes with a high temporal resolution. Our algorithm uses the fact that events are generated by edges in the scene to not only estimate the optical flow but also to simultaneously segment the image into objects which are travelling at the same velocity. This way it is able to avoid the aperture problem which affects other implementations such as Lucas-Kanade. Finally, we show that SOFAS produces more accurate results than traditional optic flow algorithms.",0
"The optical flow of events produced by a dynamic vision sensor (DVS) can be estimated using our algorithm, SOFAS. Unlike traditional cameras, DVSs generate asynchronous events in response to intensity changes with a high temporal resolution. Our algorithm takes advantage of the fact that events are created by edges in the scene to estimate optical flow and segment the image into objects that are moving at the same speed. This approach overcomes the aperture problem that affects other implementations like Lucas-Kanade. Ultimately, we demonstrate that SOFAS produces more precise results than conventional optic flow algorithms.",1
"Making predictions of future frames is a critical challenge in autonomous driving research. Most of the existing methods for video prediction attempt to generate future frames in simple and fixed scenes. In this paper, we propose a novel and effective optical flow conditioned method for the task of video prediction with an application to complex urban scenes. In contrast with previous work, the prediction model only requires video sequences and optical flow sequences for training and testing. Our method uses the rich spatial-temporal features in video sequences. The method takes advantage of the motion information extracting from optical flow maps between neighbor images as well as previous images. Empirical evaluations on the KITTI dataset and the Cityscapes dataset demonstrate the effectiveness of our method.",0
"The task of forecasting future frames poses a significant challenge in the realm of autonomous driving research. Presently, most of the video prediction techniques aim to produce forthcoming frames in uncomplicated and unchanging scenarios. This paper introduces an innovative and potent approach to video prediction that is dependent on optical flow and is applicable to intricate urban environments. Unlike prior research, our forecasting model solely requires video sequences and optical flow sequences for both training and testing. Our method capitalizes on the abundant spatiotemporal features present in video sequences. It leverages motion data extracted from optical flow maps between adjacent and preceding images. Empirical evaluations carried out on the KITTI and Cityscapes datasets demonstrate the efficacy of our methodology.",1
"Existing methods to recognize actions in static images take the images at their face value, learning the appearances---objects, scenes, and body poses---that distinguish each action class. However, such models are deprived of the rich dynamic structure and motions that also define human activity. We propose an approach that hallucinates the unobserved future motion implied by a single snapshot to help static-image action recognition. The key idea is to learn a prior over short-term dynamics from thousands of unlabeled videos, infer the anticipated optical flow on novel static images, and then train discriminative models that exploit both streams of information. Our main contributions are twofold. First, we devise an encoder-decoder convolutional neural network and a novel optical flow encoding that can translate a static image into an accurate flow map. Second, we show the power of hallucinated flow for recognition, successfully transferring the learned motion into a standard two-stream network for activity recognition. On seven datasets, we demonstrate the power of the approach. It not only achieves state-of-the-art accuracy for dense optical flow prediction, but also consistently enhances recognition of actions and dynamic scenes.",0
"Current methods for identifying actions in still images only rely on the visual appearance of the image, such as objects, scenes, and body poses, to differentiate between different action categories. However, this approach does not account for the dynamic motion and structure that define human activity. Our proposed method involves predicting the future motion in a static image by using a prior learned from unlabeled videos, and then training models to analyze both the visual appearance and predicted motion. We developed a convolutional neural network and optical flow encoding to accurately predict the motion, and demonstrated the efficacy of our approach on seven different datasets. Our approach not only achieved state-of-the-art results in optical flow prediction, but also improved action and dynamic scene recognition.",1
"In crowded scenes, detection and localization of abnormal behaviors is challenging in that high-density people make object segmentation and tracking extremely difficult. We associate the optical flows of multiple frames to capture short-term trajectories and introduce the histogram-based shape descriptor referred to as shape contexts to describe such short-term trajectories. Furthermore, we propose a K-NN similarity-based statistical model to detect anomalies over time and space, which is an unsupervised one-class learning algorithm requiring no clustering nor any prior assumption. Firstly, we retrieve the K-NN samples from the training set in regard to the testing sample, and then use the similarities between every pair of the K-NN samples to construct a Gaussian model. Finally, the probabilities of the similarities from the testing sample to the K-NN samples under the Gaussian model are calculated in the form of a joint probability. Abnormal events can be detected by judging whether the joint probability is below predefined thresholds in terms of time and space, separately. Such a scheme can adapt to the whole scene, since the probability computed as such is not affected by motion distortions arising from perspective distortion. We conduct experiments on real-world surveillance videos, and the results demonstrate that the proposed method can reliably detect and locate the abnormal events in the video sequences, outperforming the state-of-the-art approaches.",0
"The detection and localization of abnormal behaviors in crowded scenes is difficult due to the high-density of people, which makes object segmentation and tracking challenging. To address this, we utilize optical flows of multiple frames to capture short-term trajectories and describe them using the histogram-based shape descriptor known as shape contexts. Additionally, we propose an unsupervised one-class learning algorithm based on a K-NN similarity-based statistical model to detect anomalies over time and space without any prior assumptions or clustering. The algorithm retrieves K-NN samples from the training set in relation to the testing sample and constructs a Gaussian model based on the similarities between every pair of K-NN samples. The joint probability of the similarities from the testing sample to the K-NN samples under the Gaussian model is calculated, and abnormal events are detected by comparing the joint probability to predefined thresholds in terms of time and space. This approach is not affected by motion distortions arising from perspective distortion and can adapt to the entire scene. We conducted experiments on real-world surveillance videos, and the results demonstrate that our proposed method outperforms state-of-the-art approaches in detecting and locating abnormal events in video sequences.",1
"This research mainly emphasizes on traffic detection thus essentially involving object detection and classification. The particular work discussed here is motivated from unsatisfactory attempts of re-using well known pre-trained object detection networks for domain specific data. In this course, some trivial issues leading to prominent performance drop are identified and ways to resolve them are discussed. For example, some simple yet relevant tricks regarding data collection and sampling prove to be very beneficial. Also, introducing a blur net to deal with blurred real time data is another important factor promoting performance elevation. We further study the neural network design issues for beneficial object classification and involve shared, region-independent convolutional features. Adaptive learning rates to deal with saddle points are also investigated and an average covariance matrix based pre-conditioned approach is proposed. We also introduce the use of optical flow features to accommodate orientation information. Experimental results demonstrate that this results in a steady rise in the performance rate.",0
"The focus of this research is primarily on traffic detection, which involves object detection and classification. The study is motivated by unsuccessful attempts to repurpose pre-trained object detection networks for domain-specific data. During the investigation, some minor problems that lead to a significant decline in performance are identified, and solutions to these issues are discussed. For instance, collecting and sampling data using some simple yet relevant techniques prove to be very advantageous. Additionally, introducing a blur net to handle blurred real-time data is another vital factor that promotes performance improvement. The study also examines the neural network design issues for useful object classification and incorporates shared, region-independent convolutional features. To address saddle points, adaptive learning rates are explored, and a pre-conditioned approach based on the average covariance matrix is proposed. The use of optical flow features to accommodate orientation information is also introduced. Experimental results demonstrate a steady improvement in performance.",1
Optical flow estimation with convolutional neural networks (CNNs) has recently solved various tasks of computer vision successfully. In this paper we adapt a state-of-the-art approach for optical flow estimation to omnidirectional images. We investigate CNN architectures to determine high motion variations caused by the geometry of fish-eye images. Further we determine the qualitative influence of texture on the non-rigid object to the motion vectors. For evaluation of the results we create ground truth motion fields synthetically. The ground truth contains cubes with static background. We test variations of pre-trained FlowNet 2.0 architectures by indicating common error metrics. We generate competitive results for the motion of the foreground with inhomogeneous texture on the moving object.,0
"Convolutional neural networks (CNNs) have proven to be successful in various computer vision tasks, including optical flow estimation. This paper aims to apply a cutting-edge optical flow estimation approach using CNNs to omnidirectional images. Our study focuses on determining how different CNN architectures can capture high motion variations caused by the geometry of fish-eye images. Texture is also investigated as a factor affecting the motion vectors of non-rigid objects. To evaluate our findings, we create synthetic ground truth motion fields that feature cubes with a static background. We use common error metrics to test different variations of pre-trained FlowNet 2.0 architectures and generate competitive results for foreground motion with inhomogeneous texture on the moving object.",1
"The recent advances in Deep Convolutional Neural Networks (DCNNs) have shown extremely good results for video human action classification, however, action detection is still a challenging problem. The current action detection approaches follow a complex pipeline which involves multiple tasks such as tube proposals, optical flow, and tube classification. In this work, we present a more elegant solution for action detection based on the recently developed capsule network. We propose a 3D capsule network for videos, called VideoCapsuleNet: a unified network for action detection which can jointly perform pixel-wise action segmentation along with action classification. The proposed network is a generalization of capsule network from 2D to 3D, which takes a sequence of video frames as input. The 3D generalization drastically increases the number of capsules in the network, making capsule routing computationally expensive. We introduce capsule-pooling in the convolutional capsule layer to address this issue which makes the voting algorithm tractable. The routing-by-agreement in the network inherently models the action representations and various action characteristics are captured by the predicted capsules. This inspired us to utilize the capsules for action localization and the class-specific capsules predicted by the network are used to determine a pixel-wise localization of actions. The localization is further improved by parameterized skip connections with the convolutional capsule layers and the network is trained end-to-end with a classification as well as localization loss. The proposed network achieves sate-of-the-art performance on multiple action detection datasets including UCF-Sports, J-HMDB, and UCF-101 (24 classes) with an impressive ~20% improvement on UCF-101 and ~15% improvement on J-HMDB in terms of v-mAP scores.",0
"While Deep Convolutional Neural Networks (DCNNs) have been successful for video human action classification, action detection remains a challenging task. Current methods for action detection involve a complex pipeline that includes tube proposals, optical flow, and tube classification. In this study, we present a more elegant solution for action detection using the recently developed capsule network. We propose a 3D capsule network for videos, called VideoCapsuleNet, which performs pixel-wise action segmentation and action classification simultaneously. This proposed network is a generalization of the 2D capsule network and takes a sequence of video frames as input. However, the 3D generalization increases the number of capsules in the network, making capsule routing computationally expensive. To combat this issue, we introduce capsule-pooling in the convolutional capsule layer to make the voting algorithm tractable. The routing-by-agreement in the network models action representations, and the predicted capsules capture various action characteristics. We utilize the capsules for action localization, and the class-specific capsules predicted by the network determine a pixel-wise localization of actions. We improve localization with parameterized skip connections with the convolutional capsule layers, and the network is trained end-to-end with a classification and localization loss. Our proposed network achieves state-of-the-art performance on multiple action detection datasets, including UCF-Sports, J-HMDB, and UCF-101 (24 classes), with impressive improvements in v-mAP scores of ~20% on UCF-101 and ~15% on J-HMDB.",1
"Good temporal representations are crucial for video understanding, and the state-of-the-art video recognition framework is based on two-stream networks. In such framework, besides the regular ConvNets responsible for RGB frame inputs, a second network is introduced to handle the temporal representation, usually the optical flow (OF). However, OF or other task-oriented flow is computationally costly, and is thus typically pre-computed. Critically, this prevents the two-stream approach from being applied to reinforcement learning (RL) applications such as video game playing, where the next state depends on current state and action choices. Inspired by the early vision systems of mammals and insects, we propose a fast event-driven representation (EDR) that models several major properties of early retinal circuits: (1) logarithmic input response, (2) multi-timescale temporal smoothing to filter noise, and (3) bipolar (ON/OFF) pathways for primitive event detection[12]. Trading off the directional information for fast speed (> 9000 fps), EDR en-ables fast real-time inference/learning in video applications that require interaction between an agent and the world such as game-playing, virtual robotics, and domain adaptation. In this vein, we use EDR to demonstrate performance improvements over state-of-the-art reinforcement learning algorithms for Atari games, something that has not been possible with pre-computed OF. Moreover, with UCF-101 video action recognition experiments, we show that EDR performs near state-of-the-art in accuracy while achieving a 1,500x speedup in input representation processing, as compared to optical flow.",0
"The effectiveness of video understanding relies heavily on having good temporal representations, and the current leading video recognition framework is based on two-stream networks. These networks incorporate ConvNets to handle RGB frame inputs, and a second network to manage temporal representations, usually through optical flow. However, the use of optical flow or other task-specific flows is computationally expensive and typically requires pre-computation, which makes it unsuitable for reinforcement learning applications like video game playing, where the next state depends on the current state and action choices. To address this challenge, we propose an event-driven representation (EDR) inspired by early vision systems in mammals and insects. The EDR model incorporates several key properties of early retinal circuits, including logarithmic input response, multi-timescale temporal smoothing to filter noise, and ON/OFF pathways for primitive event detection. With a trade-off in directional information for faster speed (over 9000 fps), EDR enables real-time inference/learning in video applications that require interaction between an agent and the world, such as game-playing, virtual robotics, and domain adaptation. Our experiments with Atari games show that EDR outperforms state-of-the-art reinforcement learning algorithms, which was not possible with pre-computed optical flow. Additionally, our UCF-101 video action recognition experiments demonstrate that EDR achieves near state-of-the-art accuracy while processing input representations 1,500 times faster than optical flow.",1
"FlowNet2, the state-of-the-art convolutional neural network (CNN) for optical flow estimation, requires over 160M parameters to achieve accurate flow estimation. In this paper we present an alternative network that outperforms FlowNet2 on the challenging Sintel final pass and KITTI benchmarks, while being 30 times smaller in the model size and 1.36 times faster in the running speed. This is made possible by drilling down to architectural details that might have been missed in the current frameworks: (1) We present a more effective flow inference approach at each pyramid level through a lightweight cascaded network. It not only improves flow estimation accuracy through early correction, but also permits seamless incorporation of descriptor matching in our network. (2) We present a novel flow regularization layer to ameliorate the issue of outliers and vague flow boundaries by using a feature-driven local convolution. (3) Our network owns an effective structure for pyramidal feature extraction and embraces feature warping rather than image warping as practiced in FlowNet2. Our code and trained models are available at https://github.com/twhui/LiteFlowNet .",0
"FlowNet2, a convolutional neural network (CNN) used for optical flow estimation, requires more than 160M parameters to attain precise flow estimation. This study introduces an alternative network that surpasses FlowNet2's performance on the challenging Sintel final pass and KITTI benchmarks. Despite being 30 times smaller in model size and 1.36 times faster in running speed, our network's superior performance is attributed to a focus on architectural details that current frameworks may have overlooked. Firstly, we propose a more effective flow inference approach at each pyramid level by using a lightweight cascaded network, which improves flow estimation accuracy through early correction and allows seamless integration of descriptor matching. Secondly, we introduce a novel flow regularization layer that uses feature-driven local convolution to alleviate issues of outliers and unclear flow boundaries. Lastly, our network has an effective structure for pyramidal feature extraction and utilizes feature warping instead of image warping as in FlowNet2. Our code and trained models are available at https://github.com/twhui/LiteFlowNet.",1
"Conventional image motion based structure from motion methods first compute optical flow, then solve for the 3D motion parameters based on the epipolar constraint, and finally recover the 3D geometry of the scene. However, errors in optical flow due to regularization can lead to large errors in 3D motion and structure. This paper investigates whether performance and consistency can be improved by avoiding optical flow estimation in the early stages of the structure from motion pipeline, and it proposes a new direct method based on image gradients (normal flow) only. The main idea lies in a reformulation of the positive-depth constraint, which allows the use of well-known minimization techniques to solve for 3D motion. The 3D motion estimate is then refined and structure estimated adding a regularization based on depth. Experimental comparisons on standard synthetic datasets and the real-world driving benchmark dataset KITTI using three different optic flow algorithms show that the method achieves better accuracy in all but one case. Furthermore, it outperforms existing normal flow based 3D motion estimation techniques. Finally, the recovered 3D geometry is shown to be also very accurate.",0
"The typical approach to structure from motion using image motion relies on optical flow computation, followed by the determination of 3D motion parameters based on the epipolar constraint, and ultimately, the recovery of the scene's 3D geometry. However, inaccuracies in optical flow resulting from regularization may lead to significant errors in 3D motion and structure. This study aims to explore if eliminating optical flow estimation in the initial stages of the structure from motion pipeline and implementing a new direct method based solely on image gradients (normal flow) can enhance performance and consistency. The primary concept involves reformulating the positive-depth constraint to allow the use of established minimization techniques for 3D motion determination. The 3D motion is then refined and the structure is estimated with a regularization based on depth. Comparisons on standard synthetic datasets and the KITTI driving benchmark dataset using three different optic flow algorithms demonstrated that the method improved accuracy in all but one case and outperformed existing normal flow based 3D motion estimation techniques. Additionally, the resulting 3D geometry was shown to be highly precise.",1
"Dynamic Vision Sensors (DVS), which output asynchronous log intensity change events, have potential applications in high-speed robotics, autonomous cars and drones. The precise event timing, sparse output, and wide dynamic range of the events are well suited for optical flow, but conventional optical flow (OF) algorithms are not well matched to the event stream data. This paper proposes an event-driven OF algorithm called adaptive block-matching optical flow (ABMOF). ABMOF uses time slices of accumulated DVS events. The time slices are adaptively rotated based on the input events and OF results. Compared with other methods such as gradient-based OF, ABMOF can efficiently be implemented in compact logic circuits. Results show that ABMOF achieves comparable accuracy to conventional standards such as Lucas-Kanade (LK). The main contributions of our paper are new adaptive time-slice rotation methods that ensure the generated slices have sufficient features for matching,including a feedback mechanism that controls the generated slices to have average slice displacement within the block search range. An LK method using our adapted slices is also implemented. The ABMOF accuracy is compared with this LK method on natural scene data including sparse and dense texture, high dynamic range, and fast motion exceeding 30,000 pixels per second.The paper dataset and source code are available from http://sensors.ini.uzh.ch/databases.html.",0
"Potential applications for Dynamic Vision Sensors (DVS) include high-speed robotics, autonomous cars, and drones due to their ability to output asynchronous log intensity change events with precise event timing, sparse output, and wide dynamic range. However, conventional optical flow (OF) algorithms are not well-suited to the event stream data. This paper proposes an event-driven OF algorithm called adaptive block-matching optical flow (ABMOF), which uses time slices of accumulated DVS events that are adaptively rotated based on input events and OF results. ABMOF can be implemented efficiently in compact logic circuits compared to gradient-based OF. Results demonstrate that ABMOF achieves accuracy similar to conventional standards such as Lucas-Kanade (LK). The paper introduces new adaptive time-slice rotation methods that ensure generated slices have sufficient features for matching, including a feedback mechanism controlling the generated slices to have an average slice displacement within the block search range. An LK method using the adapted slices is also implemented. ABMOF accuracy is compared to the LK method on natural scene data including sparse and dense texture, high dynamic range, and fast motion exceeding 30,000 pixels per second. The paper dataset and source code can be accessed at http://sensors.ini.uzh.ch/databases.html.",1
"When a person attempts to conceal an emotion, the genuine emotion is manifest as a micro-expression. Exploration of automatic facial micro-expression recognition systems is relatively new in the computer vision domain. This is due to the difficulty in implementing optimal feature extraction methods to cope with the subtlety and brief motion characteristics of the expression. Most of the existing approaches extract the subtle facial movements based on hand-crafted features. In this paper, we address the micro-expression recognition task with a convolutional neural network (CNN) architecture, which well integrates the features extracted from each video. A new feature descriptor, Optical Flow Features from Apex frame Network (OFF-ApexNet) is introduced. This feature descriptor combines the optical ow guided context with the CNN. Firstly, we obtain the location of the apex frame from each video sequence as it portrays the highest intensity of facial motion among all frames. Then, the optical ow information are attained from the apex frame and a reference frame (i.e., onset frame). Finally, the optical flow features are fed into a pre-designed CNN model for further feature enhancement as well as to carry out the expression classification. To evaluate the effectiveness of OFF-ApexNet, comprehensive evaluations are conducted on three public spontaneous micro-expression datasets (i.e., SMIC, CASME II and SAMM). The promising recognition result suggests that the proposed method can optimally describe the significant micro-expression details. In particular, we report that, in a multi-database with leave-one-subject-out cross-validation experimental protocol, the recognition performance reaches 74.60% of recognition accuracy and F-measure of 71.04%. We also note that this is the first work that performs cross-dataset validation on three databases in this domain.",0
"The concealment of emotions can be revealed through micro-expressions. However, the development of automatic facial micro-expression recognition systems is a recent advancement in computer vision due to the challenge of extracting optimal features to capture the subtlety and brevity of these expressions. Most current approaches use manually crafted features to detect the slight facial movements. In this study, we utilize a convolutional neural network (CNN) architecture to enhance feature extraction from each video using the Optical Flow Features from Apex frame Network (OFF-ApexNet) descriptor. This descriptor combines optical flow with CNN for better feature enhancement and expression classification. We evaluate the effectiveness of OFF-ApexNet on three public spontaneous micro-expression datasets and achieve a promising recognition result. This study is significant because it is the first to perform cross-dataset validation on three databases in this domain. Our method demonstrates an accuracy of 74.60% and F-measure of 71.04% in a multi-database with leave-one-subject-out cross-validation experimental protocol, which effectively describes significant micro-expression details.",1
"Using a layered representation for motion estimation has the advantage of being able to cope with discontinuities and occlusions. In this paper, we learn to estimate optical flow by combining a layered motion representation with deep learning. Instead of pre-segmenting the image to layers, the proposed approach automatically generates a layered representation of optical flow using the proposed soft-mask module. The essential components of the soft-mask module are maxout and fuse operations, which enable a disjoint layered representation of optical flow and more accurate flow estimation. We show that by using masks the motion estimate results in a quadratic function of input features in the output layer. The proposed soft-mask module can be added to any existing optical flow estimation networks by replacing their flow output layer. In this work, we use FlowNet as the base network to which we add the soft-mask module. The resulting network is tested on three well-known benchmarks with both supervised and unsupervised flow estimation tasks. Evaluation results show that the proposed network achieve better results compared with the original FlowNet.",0
"The utilization of a layered motion representation presents an advantage in handling discontinuities and occlusions during motion estimation. In this study, we integrate deep learning with a layered motion representation to estimate optical flow. Instead of segmenting the image into pre-defined layers, we propose a soft-mask module that automatically generates a layered representation of optical flow. The soft-mask module employs maxout and fuse operations to produce a disjoint layered representation of optical flow, resulting in more accurate flow estimation. We demonstrate that using masks leads to a quadratic function of input features in the output layer for motion estimation. The proposed soft-mask module can be integrated into any existing optical flow estimation networks by replacing their flow output layer. Specifically, we incorporate the soft-mask module into the FlowNet architecture and evaluate the performance on three widely used benchmarks for both supervised and unsupervised flow estimation tasks. The experimental results show that the proposed network outperforms the original FlowNet.",1
"Optical Flow algorithms are of high importance for many applications. Recently, the Flow Field algorithm and its modifications have shown remarkable results, as they have been evaluated with top accuracy on different data sets. In our analysis of the algorithm we have found that it produces accurate sparse matches, but there is room for improvement in the interpolation. Thus, we propose in this paper FlowFields++, where we combine the accurate matches of Flow Fields with a robust interpolation. In addition, we propose improved variational optimization as post-processing. Our new algorithm is evaluated on the challenging KITTI and MPI Sintel data sets with public top results on both benchmarks.",0
"Many applications rely heavily on Optical Flow algorithms. The Flow Field algorithm and its modifications have recently demonstrated outstanding performance, having achieved exceptional accuracy when evaluated on various data sets. Our analysis of the algorithm revealed that it produces precise sparse matches, but its interpolation could be enhanced. Consequently, we present our paper on FlowFields++, which combines the accurate matches of Flow Fields with a robust interpolation. Additionally, we suggest improved variational optimization as post-processing. Our new algorithm was tested on the KITTI and MPI Sintel data sets, both of which are challenging, and achieved top public results on both benchmarks.",1
"Convolutional neural networks (CNNs) handle the case where filters extend beyond the image boundary using several heuristics, such as zero, repeat or mean padding. These schemes are applied in an ad-hoc fashion and, being weakly related to the image content and oblivious of the target task, result in low output quality at the boundary. In this paper, we propose a simple and effective improvement that learns the boundary handling itself. At training-time, the network is provided with a separate set of explicit boundary filters. At testing-time, we use these filters which have learned to extrapolate features at the boundary in an optimal way for the specific task. Our extensive evaluation, over a wide range of architectural changes (variations of layers, feature channels, or both), shows how the explicit filters result in improved boundary handling. Consequently, we demonstrate an improvement of 5% to 20% across the board of typical CNN applications (colorization, de-Bayering, optical flow, and disparity estimation).",0
"Several heuristics such as zero, repeat or mean padding are used by convolutional neural networks (CNNs) to handle the case where filters extend beyond the image boundary. However, these schemes are applied in an ad-hoc manner and have weak correlation with the image content, leading to low output quality at the boundary. This paper proposes a simple and effective solution that teaches the network to handle the boundary itself. During training, a separate set of explicit boundary filters is provided to the network. During testing, these filters are used to extrapolate features at the boundary in an optimal way for the specific task. Our evaluation shows that explicit filters result in improved boundary handling, with an overall improvement of 5% to 20% across a range of typical CNN applications such as colorization, de-Bayering, optical flow, and disparity estimation, even with variations in the network architecture.",1
"In recent years, many publications showed that convolutional neural network based features can have a superior performance to engineered features. However, not much effort was taken so far to extract local features efficiently for a whole image. In this paper, we present an approach to compute patch-based local feature descriptors efficiently in presence of pooling and striding layers for whole images at once. Our approach is generic and can be applied to nearly all existing network architectures. This includes networks for all local feature extraction tasks like camera calibration, Patchmatching, optical flow estimation and stereo matching. In addition, our approach can be applied to other patch-based approaches like sliding window object detection and recognition. We complete our paper with a speed benchmark of popular CNN based feature extraction approaches applied on a whole image, with and without our speedup, and example code (for Torch) that shows how an arbitrary CNN architecture can be easily converted by our approach.",0
"Several publications have demonstrated that convolutional neural network features outperform engineered features. However, little effort has been made to efficiently extract local features for entire images. This study introduces a method for effectively computing patch-based local feature descriptors for whole images in the presence of pooling and striding layers. Our method is versatile and can be applied to various network architectures, including those used for local feature extraction tasks such as camera calibration, Patchmatching, optical flow estimation, and stereo matching. Additionally, our approach can be applied to other patch-based techniques such as sliding window object detection and recognition. We conclude our paper by providing a speed benchmark of popular CNN-based feature extraction methods applied to whole images, with and without our acceleration, and by sharing example code (for Torch) that illustrates how to easily convert any CNN architecture using our technique.",1
"Spatio-temporal contexts are crucial in understanding human actions in videos. Recent state-of-the-art Convolutional Neural Network (ConvNet) based action recognition systems frequently involve 3D spatio-temporal ConvNet filters, chunking videos into fixed length clips and Long Short Term Memory (LSTM) networks. Such architectures are designed to take advantage of both short term and long term temporal contexts, but also requires the accumulation of a predefined number of video frames (e.g., to construct video clips for 3D ConvNet filters, to generate enough inputs for LSTMs). For applications that require low-latency online predictions of fast-changing action scenes, a new action recognition system is proposed in this paper. Termed ""Weighted Multi-Region Convolutional Neural Network"" (WMR ConvNet), the proposed system is LSTM-free, and is based on 2D ConvNet that does not require the accumulation of video frames for 3D ConvNet filtering. Unlike early 2D ConvNets that are based purely on RGB frames and optical flow frames, the WMR ConvNet is designed to simultaneously capture multiple spatial and short term temporal cues (e.g., human poses, occurrences of objects in the background) with both the primary region (foreground) and secondary regions (mostly background). On both the UCF101 and HMDB51 datasets, the proposed WMR ConvNet achieves the state-of-the-art performance among competing low-latency algorithms. Furthermore, WMR ConvNet even outperforms the 3D ConvNet based C3D algorithm that requires video frame accumulation. In an ablation study with the optical flow ConvNet stream removed, the ablated WMR ConvNet nevertheless outperforms competing algorithms.",0
"To comprehend human actions in videos, spatio-temporal contexts are essential. Current advanced action recognition systems based on Convolutional Neural Network (ConvNet) often involve 3D spatio-temporal ConvNet filters, dividing videos into fixed length clips and Long Short Term Memory (LSTM) networks. Although such architectures consider both short and long term temporal contexts, they require a set number of video frames to generate enough inputs for LSTMs and construct video clips for 3D ConvNet filters. This paper proposes a new action recognition system, called ""Weighted Multi-Region Convolutional Neural Network"" (WMR ConvNet), for applications that need low-latency online predictions of fast-changing action scenes. This LSTM-free system is based on 2D ConvNet, which does not demand video frame accumulation for 3D ConvNet filtering. The WMR ConvNet is designed to capture multiple spatial and short term temporal cues simultaneously, including human poses and objects in the background. The proposed WMR ConvNet achieves state-of-the-art performance on both the UCF101 and HMDB51 datasets, outperforming even the 3D ConvNet based C3D algorithm that necessitates video frame accumulation. In addition, the ablated WMR ConvNet outperforms competing algorithms even with the optical flow ConvNet stream removed.",1
"Despite the significant progress that has been made on estimating optical flow recently, most estimation methods, including classical and deep learning approaches, still have difficulty with multi-scale estimation, real-time computation, and/or occlusion reasoning. In this paper, we introduce dilated convolution and occlusion reasoning into unsupervised optical flow estimation to address these issues. The dilated convolution allows our network to avoid upsampling via deconvolution and the resulting gridding artifacts. Dilated convolution also results in a smaller memory footprint which speeds up interference. The occlusion reasoning prevents our network from learning incorrect deformations due to occluded image regions during training. Our proposed method outperforms state-of-the-art unsupervised approaches on the KITTI benchmark. We also demonstrate its generalization capability by applying it to action recognition in video.",0
"Although there has been significant progress in optical flow estimation, many estimation methods, such as classical and deep learning approaches, still struggle with multi-scale estimation, occlusion reasoning, and real-time computation. Our paper proposes a solution by incorporating dilated convolution and occlusion reasoning into unsupervised optical flow estimation to address these issues. With dilated convolution, our network can avoid gridding artifacts and reduce memory usage, which speeds up interference. Additionally, occlusion reasoning prevents our network from learning incorrect deformations due to occluded image regions during training. Our method outperforms state-of-the-art unsupervised approaches on the KITTI benchmark, and we also demonstrate its generalization capability by applying it to action recognition in video.",1
"Object tracking is a hot topic in computer vision. Thanks to the booming of the very high resolution (VHR) remote sensing techniques, it is now possible to track targets of interests in satellite videos. However, since the targets in the satellite videos are usually too small compared with the entire image, and too similar with the background, most state-of-the-art algorithms failed to track the target in satellite videos with a satisfactory accuracy. Due to the fact that optical flow shows the great potential to detect even the slight movement of the targets, we proposed a multi-frame optical flow tracker (MOFT) for object tracking in satellite videos. The Lucas-Kanade optical flow method was fused with the HSV color system and integral image to track the targets in the satellite videos, while multi-frame difference method was utilized in the optical flow tracker for a better interpretation. The experiments with three VHR remote sensing satellite video datasets indicate that compared with state-of-the-art object tracking algorithms, the proposed method can track the target more accurately.",0
"Computer vision has placed a significant emphasis on object tracking, especially in light of the recent advancements in very high resolution (VHR) remote sensing techniques. Though this has made it possible to track targets of interest in satellite videos, it remains a challenging task due to the small size of the targets in comparison to the entire image. Furthermore, their similarity with the background makes it difficult for most state-of-the-art algorithms to track them with satisfactory accuracy. To overcome this, we have proposed a multi-frame optical flow tracker (MOFT) that utilizes the Lucas-Kanade optical flow method fused with the HSV color system and integral image to track the targets in the satellite videos. We have also incorporated the multi-frame difference method in the optical flow tracker for improved interpretation. Our experiments with three VHR remote sensing satellite video datasets have shown that our proposed method is more accurate in tracking targets than existing object tracking algorithms.",1
"From the frame/clip-level feature learning to the video-level representation building, deep learning methods in action recognition have developed rapidly in recent years. However, current methods suffer from the confusion caused by partial observation training, or without end-to-end learning, or restricted to single temporal scale modeling and so on. In this paper, we build upon two-stream ConvNets and propose Deep networks with Temporal Pyramid Pooling (DTPP), an end-to-end video-level representation learning approach, to address these problems. Specifically, at first, RGB images and optical flow stacks are sparsely sampled across the whole video. Then a temporal pyramid pooling layer is used to aggregate the frame-level features which consist of spatial and temporal cues. Lastly, the trained model has compact video-level representation with multiple temporal scales, which is both global and sequence-aware. Experimental results show that DTPP achieves the state-of-the-art performance on two challenging video action datasets: UCF101 and HMDB51, either by ImageNet pre-training or Kinetics pre-training.",0
"Deep learning techniques have made significant progress in action recognition, with advancements in frame/clip-level feature learning and video-level representation building. However, there are still several challenges, such as confusion due to partial observation training, lack of end-to-end learning, and limited temporal scale modeling. To overcome these challenges, we propose a novel approach called Deep networks with Temporal Pyramid Pooling (DTPP), which builds upon two-stream ConvNets. DTPP is an end-to-end video-level representation learning approach that uses a temporal pyramid pooling layer to aggregate frame-level features with spatial and temporal cues. The resulting model has a compact video-level representation with multiple temporal scales that is both global and sequence-aware. Our experimental results demonstrate that DTPP achieves state-of-the-art performance on challenging video action datasets, UCF101 and HMDB51, with either ImageNet pre-training or Kinetics pre-training.",1
"Optimized scene representation is an important characteristic of a framework for detecting abnormalities on live videos. One of the challenges for detecting abnormalities in live videos is real-time detection of objects in a non-parametric way. Another challenge is to efficiently represent the state of objects temporally across frames. In this paper, a Gibbs sampling based heuristic model referred to as Temporal Unknown Incremental Clustering (TUIC) has been proposed to cluster pixels with motion. Pixel motion is first detected using optical flow and a Bayesian algorithm has been applied to associate pixels belonging to similar cluster in subsequent frames. The algorithm is fast and produces accurate results in $\Theta(kn)$ time, where $k$ is the number of clusters and $n$ the number of pixels. Our experimental validation with publicly available datasets reveals that the proposed framework has good potential to open-up new opportunities for real-time traffic analysis.",0
"An essential feature of a framework designed for identifying irregularities in live videos is optimized scene representation. Detecting abnormalities in live videos presents two challenges, namely, real-time detection of objects in a non-parametric way and efficient temporal representation of object states across frames. This study proposes a heuristic model called Temporal Unknown Incremental Clustering (TUIC) that uses Gibbs sampling to cluster pixels with motion. The model first detects pixel motion through optical flow and then applies a Bayesian algorithm to associate pixels belonging to similar clusters in subsequent frames. The algorithm is fast and produces accurate results in $\Theta(kn)$ time, where $k$ is the number of clusters and $n$ is the number of pixels. The experimental validation using publicly available datasets indicates that the proposed framework has the potential to create new opportunities for real-time traffic analysis.",1
"Motion boundary detection is a crucial yet challenging problem. Prior methods focus on analyzing the gradients and distributions of optical flow fields, or use hand-crafted features for motion boundary learning. In this paper, we propose the first dedicated end-to-end deep learning approach for motion boundary detection, which we term as MoBoNet. We introduce a refinement network structure which takes source input images, initial forward and backward optical flows as well as corresponding warping errors as inputs and produces high-resolution motion boundaries. Furthermore, we show that the obtained motion boundaries, through a fusion sub-network we design, can in turn guide the optical flows for removing the artifacts. The proposed MoBoNet is generic and works with any optical flows. Our motion boundary detection and the refined optical flow estimation achieve results superior to the state of the art.",0
"Detecting motion boundaries is a challenging task that holds great significance. Previous methods have focused on analyzing optical flow fields' gradients and distributions or using manually crafted features to learn motion boundaries. In this research, we propose MoBoNet, the first end-to-end deep learning approach dedicated to motion boundary detection. Our model includes a refinement network that takes input images, forward and backward optical flows, and warping errors to produce high-resolution motion boundaries. Additionally, we demonstrate that our motion boundaries can guide optical flows to eliminate artifacts using a fusion sub-network. MoBoNet is a versatile model that works with any optical flow and outperforms existing state-of-the-art methods in both motion boundary detection and refined optical flow estimation.",1
"We introduce a two-stream model for dynamic texture synthesis. Our model is based on pre-trained convolutional networks (ConvNets) that target two independent tasks: (i) object recognition, and (ii) optical flow prediction. Given an input dynamic texture, statistics of filter responses from the object recognition ConvNet encapsulate the per-frame appearance of the input texture, while statistics of filter responses from the optical flow ConvNet model its dynamics. To generate a novel texture, a randomly initialized input sequence is optimized to match the feature statistics from each stream of an example texture. Inspired by recent work on image style transfer and enabled by the two-stream model, we also apply the synthesis approach to combine the texture appearance from one texture with the dynamics of another to generate entirely novel dynamic textures. We show that our approach generates novel, high quality samples that match both the framewise appearance and temporal evolution of input texture. Finally, we quantitatively evaluate our texture synthesis approach with a thorough user study.",0
"Our approach to dynamic texture synthesis involves a two-stream model that utilizes pre-trained ConvNets for object recognition and optical flow prediction. By analyzing the filter responses from each ConvNet, our model can encapsulate the appearance and dynamics of an input dynamic texture. To generate a new texture, we optimize a randomly initialized input sequence to match the feature statistics from each stream of an example texture. Using this approach, we can combine the texture appearance from one source with the dynamics of another to create entirely new dynamic textures. Our method produces high-quality samples that match both the appearance and evolution of the input texture. We also conducted a user study to quantitatively evaluate our texture synthesis approach.",1
"In this paper, a new video classification methodology is proposed which can be applied in both first and third person videos. The main idea behind the proposed strategy is to capture complementary information of appearance and motion efficiently by performing two independent streams on the videos. The first stream is aimed to capture long-term motions from shorter ones by keeping track of how elements in optical flow images have changed over time. Optical flow images are described by pre-trained networks that have been trained on large scale image datasets. A set of multi-channel time series are obtained by aligning descriptions beside each other. For extracting motion features from these time series, PoT representation method plus a novel pooling operator is followed due to several advantages. The second stream is accomplished to extract appearance features which are vital in the case of video classification. The proposed method has been evaluated on both first and third-person datasets and results present that the proposed methodology reaches the state of the art successfully.",0
"This paper proposes a new methodology for video classification that can be utilized in both first and third person videos. The proposed strategy aims to efficiently capture complementary information of appearance and motion by performing two independent streams on the videos. The first stream focuses on capturing long-term motions from shorter ones by monitoring how elements in optical flow images have evolved over time. Optical flow images are described by pre-trained networks that have been trained on large scale image datasets. A set of multi-channel time series are obtained by aligning descriptions beside each other. The PoT representation method plus a novel pooling operator is utilized for extracting motion features from these time series, owing to several advantages. The second stream is designed to extract appearance features that are crucial for video classification. The effectiveness of the proposed method has been assessed on both first and third-person datasets, and the results indicate that the proposed methodology successfully achieves the state of the art.",1
"Using deep learning, this paper addresses the problem of joint object boundary detection and boundary motion estimation in videos, which we named boundary flow estimation. Boundary flow is an important mid-level visual cue as boundaries characterize objects spatial extents, and the flow indicates objects motions and interactions. Yet, most prior work on motion estimation has focused on dense object motion or feature points that may not necessarily reside on boundaries. For boundary flow estimation, we specify a new fully convolutional Siamese network (FCSN) that jointly estimates object-level boundaries in two consecutive frames. Boundary correspondences in the two frames are predicted by the same FCSN with a new, unconventional deconvolution approach. Finally, the boundary flow estimate is improved with an edgelet-based filtering. Evaluation is conducted on three tasks: boundary detection in videos, boundary flow estimation, and optical flow estimation. On boundary detection, we achieve the state-of-the-art performance on the benchmark VSB100 dataset. On boundary flow estimation, we present the first results on the Sintel training dataset. For optical flow estimation, we run the recent approach CPMFlow but on the augmented input with our boundary-flow matches, and achieve significant performance improvement on the Sintel benchmark.",0
"The paper utilizes deep learning to tackle the issue of detecting joint object boundaries and boundary motion estimation in videos, which they have dubbed boundary flow estimation. The flow of boundaries is a crucial visual cue as it characterizes the spatial extent of objects and their movements and interactions. However, previous research on motion estimation has primarily concentrated on dense object motion or feature points that may not reside on boundaries. To address this, the paper proposes a new fully convolutional Siamese network (FCSN) that predicts object-level boundaries between two frames and uses an unconventional deconvolution method to predict boundary correspondences. Finally, edgelet-based filtering is employed to enhance the boundary flow estimate. The performance of the proposed approach is evaluated on three tasks, including boundary detection in videos, boundary flow estimation, and optical flow estimation, and is shown to achieve state-of-the-art results on benchmark datasets.",1
"We present a unifying framework to solve several computer vision problems with event cameras: motion, depth and optical flow estimation. The main idea of our framework is to find the point trajectories on the image plane that are best aligned with the event data by maximizing an objective function: the contrast of an image of warped events. Our method implicitly handles data association between the events, and therefore, does not rely on additional appearance information about the scene. In addition to accurately recovering the motion parameters of the problem, our framework produces motion-corrected edge-like images with high dynamic range that can be used for further scene analysis. The proposed method is not only simple, but more importantly, it is, to the best of our knowledge, the first method that can be successfully applied to such a diverse set of important vision tasks with event cameras.",0
"A framework has been introduced to address various computer vision challenges using event cameras, including depth, optical flow, and motion estimation. The framework's primary concept is to identify the image plane's point trajectories that best align with the event data by maximizing the contrast of a warped events image. Our approach handles data association between events implicitly, requiring no additional scene appearance data. Moreover, our framework produces motion-corrected images with high dynamic range that can be used for further scene analysis. Not only is our method simple, but it is also the first of its kind that can be effectively employed to tackle a wide range of critical vision tasks with event cameras.",1
"It has been recently shown that a convolutional neural network can learn optical flow estimation with unsupervised learning. However, the performance of the unsupervised methods still has a relatively large gap compared to its supervised counterpart. Occlusion and large motion are some of the major factors that limit the current unsupervised learning of optical flow methods. In this work we introduce a new method which models occlusion explicitly and a new warping way that facilitates the learning of large motion. Our method shows promising results on Flying Chairs, MPI-Sintel and KITTI benchmark datasets. Especially on KITTI dataset where abundant unlabeled samples exist, our unsupervised method outperforms its counterpart trained with supervised learning.",0
"Recently, it was demonstrated that unsupervised learning can enable a convolutional neural network to master optical flow estimation. Nevertheless, the unsupervised approaches' performance still lags significantly behind their supervised counterparts. The present unsupervised learning of optical flow techniques is hampered by factors such as occlusion and large motion. This study introduces a novel approach that explicitly models occlusion and a new warping method that aids the learning of large motion. Our method yields encouraging results on the Flying Chairs, MPI-Sintel, and KITTI benchmark datasets. Particularly on the KITTI dataset, which has a plethora of unlabeled examples, our unsupervised approach outperforms the supervised learning counterpart.",1
"Despite the recent success of end-to-end learned representations, hand-crafted optical flow features are still widely used in video analysis tasks. To fill this gap, we propose TVNet, a novel end-to-end trainable neural network, to learn optical-flow-like features from data. TVNet subsumes a specific optical flow solver, the TV-L1 method, and is initialized by unfolding its optimization iterations as neural layers. TVNet can therefore be used directly without any extra learning. Moreover, it can be naturally concatenated with other task-specific networks to formulate an end-to-end architecture, thus making our method more efficient than current multi-stage approaches by avoiding the need to pre-compute and store features on disk. Finally, the parameters of the TVNet can be further fine-tuned by end-to-end training. This enables TVNet to learn richer and task-specific patterns beyond exact optical flow. Extensive experiments on two action recognition benchmarks verify the effectiveness of the proposed approach. Our TVNet achieves better accuracies than all compared methods, while being competitive with the fastest counterpart in terms of features extraction time.",0
"Hand-crafted optical flow features remain a popular choice in video analysis tasks, despite the success of end-to-end learned representations. To address this gap, we introduce TVNet, a novel end-to-end trainable neural network that learns optical-flow-like features from data. TVNet incorporates the TV-L1 method as an optical flow solver and is initialized by unfolding its optimization iterations as neural layers. This allows direct use without additional learning and facilitates concatenation with other task-specific networks to create an efficient end-to-end architecture that eliminates the need for pre-computing and storing features on disk. Furthermore, end-to-end training can fine-tune TVNet parameters to learn richer and task-specific patterns beyond optical flow. Extensive experiments on two action recognition benchmarks demonstrate that our approach outperforms all compared methods and is competitive in terms of feature extraction time.",1
"Video frame interpolation algorithms typically estimate optical flow or its variations and then use it to guide the synthesis of an intermediate frame between two consecutive original frames. To handle challenges like occlusion, bidirectional flow between the two input frames is often estimated and used to warp and blend the input frames. However, how to effectively blend the two warped frames still remains a challenging problem. This paper presents a context-aware synthesis approach that warps not only the input frames but also their pixel-wise contextual information and uses them to interpolate a high-quality intermediate frame. Specifically, we first use a pre-trained neural network to extract per-pixel contextual information for input frames. We then employ a state-of-the-art optical flow algorithm to estimate bidirectional flow between them and pre-warp both input frames and their context maps. Finally, unlike common approaches that blend the pre-warped frames, our method feeds them and their context maps to a video frame synthesis neural network to produce the interpolated frame in a context-aware fashion. Our neural network is fully convolutional and is trained end to end. Our experiments show that our method can handle challenging scenarios such as occlusion and large motion and outperforms representative state-of-the-art approaches.",0
"Commonly, video frame interpolation algorithms estimate optical flow or its variations to create an intermediate frame between two original frames. However, challenges related to occlusion require the estimation of bidirectional flow between input frames to warp and blend them. Despite this, effectively blending the two warped frames remains difficult. This paper introduces a context-aware synthesis technique that warps the input frames and their pixel-wise contextual information to interpolate a high-quality intermediate frame. The approach involves using a pre-trained neural network to extract per-pixel contextual information for input frames and estimating bidirectional flow between them with an optical flow algorithm. Instead of blending the pre-warped frames, the method feeds them and their context maps to a video frame synthesis neural network to produce the interpolated frame in a context-aware manner. The fully convolutional neural network is trained end to end and outperforms representative state-of-the-art approaches, even in challenging scenarios such as occlusion and large motion.",1
"Variational inference has experienced a recent surge in popularity owing to stochastic approaches, which have yielded practical tools for a wide range of model classes. A key benefit is that stochastic variational inference obviates the tedious process of deriving analytical expressions for closed-form variable updates. Instead, one simply needs to derive the gradient of the log-posterior, which is often much easier. Yet for certain model classes, the log-posterior itself is difficult to optimize using standard gradient techniques. One such example are random field models, where optimization based on gradient linearization has proven popular, since it speeds up convergence significantly and can avoid poor local optima. In this paper we propose stochastic variational inference with gradient linearization (SVIGL). It is similarly convenient as standard stochastic variational inference - all that is required is a local linearization of the energy gradient. Its benefit over stochastic variational inference with conventional gradient methods is a clear improvement in convergence speed, while yielding comparable or even better variational approximations in terms of KL divergence. We demonstrate the benefits of SVIGL in three applications: Optical flow estimation, Poisson-Gaussian denoising, and 3D surface reconstruction.",0
"Due to the success of stochastic approaches, variational inference has become increasingly popular for a diverse set of model classes. A major advantage of stochastic variational inference is that it eliminates the need for deriving analytical expressions for variable updates, which can be a tedious process. Instead, one only needs to calculate the gradient of the log-posterior, which is typically simpler. However, for certain model classes like random field models, optimizing the log-posterior using standard gradient techniques can be challenging. To address this issue, many have turned to gradient linearization optimization methods, which can speed up convergence and prevent poor local optima. In this paper, we introduce stochastic variational inference with gradient linearization (SVIGL), which is just as convenient as standard stochastic variational inference but requires local linearization of the energy gradient. SVIGL offers faster convergence while providing comparable or even better variational approximations in terms of KL divergence. We illustrate the advantages of SVIGL through three applications: Optical flow estimation, Poisson-Gaussian denoising, and 3D surface reconstruction.",1
"This paper addresses the problem of video object segmentation, where the initial object mask is given in the first frame of an input video. We propose a novel spatio-temporal Markov Random Field (MRF) model defined over pixels to handle this problem. Unlike conventional MRF models, the spatial dependencies among pixels in our model are encoded by a Convolutional Neural Network (CNN). Specifically, for a given object, the probability of a labeling to a set of spatially neighboring pixels can be predicted by a CNN trained for this specific object. As a result, higher-order, richer dependencies among pixels in the set can be implicitly modeled by the CNN. With temporal dependencies established by optical flow, the resulting MRF model combines both spatial and temporal cues for tackling video object segmentation. However, performing inference in the MRF model is very difficult due to the very high-order dependencies. To this end, we propose a novel CNN-embedded algorithm to perform approximate inference in the MRF. This algorithm proceeds by alternating between a temporal fusion step and a feed-forward CNN step. When initialized with an appearance-based one-shot segmentation CNN, our model outperforms the winning entries of the DAVIS 2017 Challenge, without resorting to model ensembling or any dedicated detectors.",0
"The issue of video object segmentation is tackled in this paper, which involves finding the initial object mask in the first frame of a video input. To solve this problem, a unique spatio-temporal Markov Random Field (MRF) model is introduced, which is based on pixel representation. Unlike traditional MRF models, the spatial connections between pixels in this model are established by a Convolutional Neural Network (CNN). A CNN trained for a specific object can predict the likelihood of a labeling for a group of neighboring pixels that pertain to that object, allowing for higher-order, more complex connections between pixels to be implicitly modeled. By using optical flow to establish temporal dependencies, the MRF model can integrate both spatial and temporal cues for effective video object segmentation. However, performing inference in the MRF model is a challenging task due to the high-order dependencies. Therefore, a novel CNN-embedded algorithm is proposed that alternates between a temporal fusion step and a feed-forward CNN step to perform approximate inference in the MRF. When initialized with an appearance-based one-shot segmentation CNN, the proposed model outperforms the winning entries of the DAVIS 2017 Challenge, even without the need for model ensembling or dedicated detectors.",1
"The finding that very large networks can be trained efficiently and reliably has led to a paradigm shift in computer vision from engineered solutions to learning formulations. As a result, the research challenge shifts from devising algorithms to creating suitable and abundant training data for supervised learning. How to efficiently create such training data? The dominant data acquisition method in visual recognition is based on web data and manual annotation. Yet, for many computer vision problems, such as stereo or optical flow estimation, this approach is not feasible because humans cannot manually enter a pixel-accurate flow field. In this paper, we promote the use of synthetically generated data for the purpose of training deep networks on such tasks.We suggest multiple ways to generate such data and evaluate the influence of dataset properties on the performance and generalization properties of the resulting networks. We also demonstrate the benefit of learning schedules that use different types of data at selected stages of the training process.",0
"The discovery that large networks can be trained with efficiency and reliability has caused a shift in computer vision from engineered solutions to learning formulations. Consequently, the research challenge has changed from developing algorithms to producing sufficient and appropriate training data for supervised learning. However, creating such data efficiently remains a challenge. The most common method of data acquisition in visual recognition is through web data and manual annotation, but for some computer vision problems like stereo or optical flow estimation, this method is not practical because humans cannot manually input a pixel-accurate flow field. This paper advocates for the utilization of synthetically generated data to train deep networks in such tasks. We suggest various methods of generating this data and assess the impact of dataset characteristics on the resulting networks' performance and generalization abilities. We also showcase the benefits of using different types of data in learning schedules at selected stages of the training process.",1
"Traditional approaches to interpolate/extrapolate frames in a video sequence require accurate pixel correspondences between images, e.g., using optical flow. Their results stem on the accuracy of optical flow estimation, and could generate heavy artifacts when flow estimation failed. Recently methods using auto-encoder has shown impressive progress, however they are usually trained for specific interpolation/extrapolation settings and lack of flexibility and In order to reduce these limitations, we propose a unified network to parameterize the interest frame position and therefore infer interpolate/extrapolate frames within the same framework. To achieve this, we introduce a transitive consistency loss to better regularize the network. We adopt a multi-scale structure for the network so that the parameters can be shared across multi-layers. Our approach avoids expensive global optimization of optical flow methods, and is efficient and flexible for video interpolation/extrapolation applications. Experimental results have shown that our method performs favorably against state-of-the-art methods.",0
"Conventional methods for interpolating or extrapolating frames in a video sequence require precise pixel correspondences between images, typically achieved with optical flow. The accuracy of these methods is dependent on the accuracy of the optical flow estimation, and can produce significant artifacts if the estimation fails. While auto-encoder-based approaches have recently made impressive progress, they are typically trained for specific interpolation/extrapolation scenarios and lack flexibility. To address these limitations, we propose a unified network that can parameterize the desired frame position and thus perform interpolation/extrapolation within the same framework. We introduce a transitive consistency loss to better regulate the network and adopt a multi-scale structure to enable parameter sharing across multiple layers. Our approach is more efficient and flexible than costly global optimization of optical flow methods, making it ideal for video interpolation/extrapolation applications. Experimental results demonstrate that our method outperforms current state-of-the-art methods.",1
"Accurate prediction of traffic signal duration for roadway junction is a challenging problem due to the dynamic nature of traffic flows. Though supervised learning can be used, parameters may vary across roadway junctions. In this paper, we present a computer vision guided expert system that can learn the departure rate of a given traffic junction modeled using traditional queuing theory. First, we temporally group the optical flow of the moving vehicles using Dirichlet Process Mixture Model (DPMM). These groups are referred to as tracklets or temporal clusters. Tracklet features are then used to learn the dynamic behavior of a traffic junction, especially during on/off cycles of a signal. The proposed queuing theory based approach can predict the signal open duration for the next cycle with higher accuracy when compared with other popular features used for tracking. The hypothesis has been verified on two publicly available video datasets. The results reveal that the DPMM based features are better than existing tracking frameworks to estimate $\mu$. Thus, signal duration prediction is more accurate when tested on these datasets.The method can be used for designing intelligent operator-independent traffic control systems for roadway junctions at cities and highways.",0
"Due to the constantly changing traffic flow, accurately predicting the duration of traffic signals at roadway junctions is a difficult task. While supervised learning can be utilized, variations in parameters across different junctions can cause issues. This study proposes a computer vision-based expert system that can learn the departure rate of a specific traffic junction using traditional queuing theory. The optical flow of moving vehicles is temporally grouped into tracklets using the Dirichlet Process Mixture Model (DPMM) and these tracklets are used to learn the dynamic behavior of the junction during on/off cycles of a signal. Compared to other popular tracking features, the proposed queuing theory-based approach provides higher accuracy in predicting the signal open duration for the next cycle. The results of testing on two publicly available video datasets show that DPMM-based features are superior to existing tracking frameworks in estimating $\mu$, leading to more accurate signal duration predictions. This method can be applied to develop intelligent, operator-independent traffic control systems for roadway junctions on city streets and highways.",1
"Anomaly detection in videos refers to the identification of events that do not conform to expected behavior. However, almost all existing methods tackle the problem by minimizing the reconstruction errors of training data, which cannot guarantee a larger reconstruction error for an abnormal event. In this paper, we propose to tackle the anomaly detection problem within a video prediction framework. To the best of our knowledge, this is the first work that leverages the difference between a predicted future frame and its ground truth to detect an abnormal event. To predict a future frame with higher quality for normal events, other than the commonly used appearance (spatial) constraints on intensity and gradient, we also introduce a motion (temporal) constraint in video prediction by enforcing the optical flow between predicted frames and ground truth frames to be consistent, and this is the first work that introduces a temporal constraint into the video prediction task. Such spatial and motion constraints facilitate the future frame prediction for normal events, and consequently facilitate to identify those abnormal events that do not conform the expectation. Extensive experiments on both a toy dataset and some publicly available datasets validate the effectiveness of our method in terms of robustness to the uncertainty in normal events and the sensitivity to abnormal events.",0
"The identification of events that deviate from expected behavior in videos is known as anomaly detection. However, current methods primarily focus on minimizing reconstruction errors of training data, which does not guarantee a higher reconstruction error for abnormal events. This paper proposes a video prediction framework to address anomaly detection. It is the first work to use the difference between a predicted future frame and its ground truth for anomaly detection. To improve the quality of predicted frames for normal events, the paper introduces a motion constraint in video prediction by ensuring consistency in optical flow between predicted and ground truth frames. This is the first work to introduce a temporal constraint in video prediction. These constraints aid in future frame prediction for normal events and help identify abnormal events that do not adhere to expectations. The proposed method is validated through extensive experiments on both a toy dataset and publicly available datasets, demonstrating its effectiveness in terms of robustness to uncertainty in normal events and sensitivity to abnormal events.",1
"We propose GeoNet, a jointly unsupervised learning framework for monocular depth, optical flow and ego-motion estimation from videos. The three components are coupled by the nature of 3D scene geometry, jointly learned by our framework in an end-to-end manner. Specifically, geometric relationships are extracted over the predictions of individual modules and then combined as an image reconstruction loss, reasoning about static and dynamic scene parts separately. Furthermore, we propose an adaptive geometric consistency loss to increase robustness towards outliers and non-Lambertian regions, which resolves occlusions and texture ambiguities effectively. Experimentation on the KITTI driving dataset reveals that our scheme achieves state-of-the-art results in all of the three tasks, performing better than previously unsupervised methods and comparably with supervised ones.",0
"GeoNet is a framework we suggest for unsupervised learning of monocular depth, optical flow, and ego-motion estimation from videos. The three elements are interconnected by the 3D scene geometry, which our framework learns in an end-to-end manner. Our approach involves extracting geometric relationships from the predictions of individual modules and combining them as an image reconstruction loss, which distinguishes between static and dynamic scene parts. Additionally, we introduce an adaptive geometric consistency loss that enhances robustness to outliers and non-Lambertian regions, resolving occlusions and texture ambiguities effectively. Our experiments on the KITTI driving dataset demonstrate that our method outperforms previous unsupervised approaches and is comparable to supervised ones, achieving state-of-the-art results in all three tasks.",1
"Visual feature clustering is one of the cost-effective approaches to segment objects in videos. However, the assumptions made for developing the existing algorithms prevent them from being used in situations like segmenting an unknown number of static and moving objects under heavy camera movements. This paper addresses the problem by introducing a clustering approach based on superpixels and short-term Histogram of Oriented Optical Flow (HOOF). Salient Dither Pattern Feature (SDPF) is used as the visual feature to track the flow and Simple Linear Iterative Clustering (SLIC) is used for obtaining the superpixels. This new clustering approach is based on merging superpixels by comparing short term local HOOF and a color cue to form high-level semantic segments. The new approach was compared with one of the latest feature clustering approaches based on K-Means in eight-dimensional space and the results revealed that the new approach is better by means of consistency, completeness, and spatial accuracy. Further, the new approach completely solved the problem of not knowing the number of objects in a scene.",0
"Segmenting objects in videos through visual feature clustering is a cost-effective method. However, the current algorithms have limitations in segmenting static and moving objects in scenarios with heavy camera movements, due to the assumptions made during their development. This paper proposes a new clustering method that uses superpixels and short-term Histogram of Oriented Optical Flow (HOOF) based on Salient Dither Pattern Feature (SDPF) for tracking flow and Simple Linear Iterative Clustering (SLIC) for obtaining superpixels. The method merges superpixels by comparing short term local HOOF and a color cue to form high-level semantic segments. Compared to an existing feature clustering approach based on K-Means in an eight-dimensional space, the new approach is more consistent, complete, and spatially accurate. Additionally, it solves the issue of not knowing the number of objects in a scene.",1
"Discriminative correlation filters (DCF) with deep convolutional features have achieved favorable performance in recent tracking benchmarks. However, most of existing DCF trackers only consider appearance features of current frame, and hardly benefit from motion and inter-frame information. The lack of temporal information degrades the tracking performance during challenges such as partial occlusion and deformation. In this work, we focus on making use of the rich flow information in consecutive frames to improve the feature representation and the tracking accuracy. Firstly, individual components, including optical flow estimation, feature extraction, aggregation and correlation filter tracking are formulated as special layers in network. To the best of our knowledge, this is the first work to jointly train flow and tracking task in a deep learning framework. Then the historical feature maps at predefined intervals are warped and aggregated with current ones by the guiding of flow. For adaptive aggregation, we propose a novel spatial-temporal attention mechanism. Extensive experiments are performed on four challenging tracking datasets: OTB2013, OTB2015, VOT2015 and VOT2016, and the proposed method achieves superior results on these benchmarks.",0
"Recent tracking benchmarks have shown that discriminative correlation filters (DCF) with deep convolutional features are effective. However, most existing DCF trackers only consider appearance features of the current frame, which limits their benefit from motion and inter-frame information. This lack of temporal information results in degraded tracking performance, particularly during challenges such as partial occlusion and deformation. To address this issue, we focus on utilizing the rich flow information in consecutive frames to enhance feature representation and tracking accuracy. We integrate individual components, such as optical flow estimation, feature extraction, aggregation, and correlation filter tracking, as special layers in the network. This is the first work to jointly train flow and tracking tasks in a deep learning framework. We then warp and aggregate historical feature maps at predefined intervals with current ones using flow guidance. To enable adaptive aggregation, we propose a novel spatial-temporal attention mechanism. Our proposed method achieves superior results on four challenging tracking datasets: OTB2013, OTB2015, VOT2015, and VOT2016, demonstrating the effectiveness of our approach.",1
"Designing a scheme that can achieve a good performance in predicting single person activities and group activities is a challenging task. In this paper, we propose a novel robust and efficient human activity recognition scheme called ReHAR, which can be used to handle single person activities and group activities prediction. First, we generate an optical flow image for each video frame. Then, both video frames and their corresponding optical flow images are fed into a Single Frame Representation Model to generate representations. Finally, an LSTM is used to pre- dict the final activities based on the generated representations. The whole model is trained end-to-end to allow meaningful representations to be generated for the final activity recognition. We evaluate ReHAR using two well-known datasets: the NCAA Basketball Dataset and the UCFSports Action Dataset. The experimental results show that the pro- posed ReHAR achieves a higher activity recognition accuracy with an order of magnitude shorter computation time compared to the state-of-the-art methods.",0
"Developing a scheme that can effectively predict single person and group activities is a difficult undertaking. This paper introduces a new and resilient human activity recognition scheme known as ReHAR that can handle both types of activity prediction. The proposed approach involves generating an optical flow image for each video frame, using a Single Frame Representation Model to generate representations for both the video frames and their corresponding optical flow images, and employing an LSTM to predict the final activities based on the generated representations. The entire model is trained end-to-end to generate meaningful representations for the final activity recognition. The performance of ReHAR is evaluated using the NCAA Basketball Dataset and the UCFSports Action Dataset, and the experimental results demonstrate that ReHAR achieves higher activity recognition accuracy with significantly less computation time compared to state-of-the-art methods.",1
"We propose a method for unsupervised video object segmentation by transferring the knowledge encapsulated in image-based instance embedding networks. The instance embedding network produces an embedding vector for each pixel that enables identifying all pixels belonging to the same object. Though trained on static images, the instance embeddings are stable over consecutive video frames, which allows us to link objects together over time. Thus, we adapt the instance networks trained on static images to video object segmentation and incorporate the embeddings with objectness and optical flow features, without model retraining or online fine-tuning. The proposed method outperforms state-of-the-art unsupervised segmentation methods in the DAVIS dataset and the FBMS dataset.",0
"A method is proposed to achieve unsupervised video object segmentation. This involves utilizing the knowledge from image-based instance embedding networks. The instance embedding network creates an embedding vector for every pixel, which helps in identifying pixels that belong to the same object. Although the instance embeddings are trained on static images, they remain stable over consecutive video frames, making it possible to connect objects over time. The instance networks are adjusted to cater to video object segmentation and are combined with objectness and optical flow features without the need for model retraining or online fine-tuning. The proposed method surpasses other unsupervised segmentation methods in both the DAVIS dataset and the FBMS dataset.",1
"Two-stream networks have been very successful for solving the problem of action detection. However, prior work using two-stream networks train both streams separately, which prevents the network from exploiting regularities between the two streams. Moreover, unlike the visual stream, the dominant forms of optical flow computation typically do not maximally exploit GPU parallelism. We present a real-time end-to-end trainable two-stream network for action detection. First, we integrate the optical flow computation in our framework by using Flownet2. Second, we apply early fusion for the two streams and train the whole pipeline jointly end-to-end. Finally, for better network initialization, we transfer from the task of action recognition to action detection by pre-training our framework using the recently released large-scale Kinetics dataset. Our experimental results show that training the pipeline jointly end-to-end with fine-tuning the optical flow for the objective of action detection improves detection performance significantly. Additionally, we observe an improvement when initializing with parameters pre-trained using Kinetics. Last, we show that by integrating the optical flow computation, our framework is more efficient, running at real-time speeds (up to 31 fps).",0
"The problem of action detection has been effectively addressed by two-stream networks. However, previous approaches trained the two streams separately, which prevented the network from taking advantage of the similarities between them. Additionally, the optical flow computation used in these networks did not make optimal use of GPU parallelism. To overcome these issues, we introduce a real-time, end-to-end trainable two-stream network for action detection. Our approach integrates Flownet2 for optical flow computation and applies early fusion for the two streams, enabling joint end-to-end training. To improve network initialization, we pre-train our framework using the Kinetics dataset for action recognition. Our experimental results demonstrate that joint end-to-end training with optical flow fine-tuning significantly enhances detection performance. Pre-training with Kinetics also leads to improvement, and our framework is more efficient with real-time speeds of up to 31 fps due to the integration of optical flow computation.",1
"This paper documents the winning entry at the CVPR2017 vehicle velocity estimation challenge. Velocity estimation is an emerging task in autonomous driving which has not yet been thoroughly explored. The goal is to estimate the relative velocity of a specific vehicle from a sequence of images. In this paper, we present a light-weight approach for directly regressing vehicle velocities from their trajectories using a multilayer perceptron. Another contribution is an explorative study of features for monocular vehicle velocity estimation. We find that light-weight trajectory based features outperform depth and motion cues extracted from deep ConvNets, especially for far-distance predictions where current disparity and optical flow estimators are challenged significantly. Our light-weight approach is real-time capable on a single CPU and outperforms all competing entries in the velocity estimation challenge. On the test set, we report an average error of 1.12 m/s which is comparable to a (ground-truth) system that combines LiDAR and radar techniques to achieve an error of around 0.71 m/s.",0
"The article outlines the victorious submission for the CVPR2017 competition on the estimation of vehicle velocity. Although an emerging task in autonomous driving, velocity estimation has not been thoroughly investigated. The objective is to determine the relative velocity of a specific vehicle through a sequence of images. This paper introduces a lightweight method that utilizes a multilayer perceptron to directly regress vehicle velocities from their trajectories. Additionally, the article presents an exploratory analysis of features for monocular vehicle velocity estimation. The study reveals that lightweight trajectory-based features perform better than depth and motion cues extracted from deep ConvNets, particularly for far-distance predictions where current disparity and optical flow estimators face significant challenges. The lightweight approach operates in real-time on a single CPU and outperforms all other entries in the velocity estimation challenge. The test results show an average error of 1.12 m/s, which is similar to a (ground-truth) system that combines LiDAR and radar techniques to achieve an error of approximately 0.71 m/s.",1
"Even with the recent advances in convolutional neural networks (CNN) in various visual recognition tasks, the state-of-the-art action recognition system still relies on hand crafted motion feature such as optical flow to achieve the best performance. We propose a multitask learning model ActionFlowNet to train a single stream network directly from raw pixels to jointly estimate optical flow while recognizing actions with convolutional neural networks, capturing both appearance and motion in a single model. We additionally provide insights to how the quality of the learned optical flow affects the action recognition. Our model significantly improves action recognition accuracy by a large margin 31% compared to state-of-the-art CNN-based action recognition models trained without external large scale data and additional optical flow input. Without pretraining on large external labeled datasets, our model, by well exploiting the motion information, achieves competitive recognition accuracy to the models trained with large labeled datasets such as ImageNet and Sport-1M.",0
"Despite the progress made in convolutional neural networks (CNN) for visual recognition tasks, the most advanced action recognition system still depends on hand-crafted motion features like optical flow to attain optimal performance. Our proposed ActionFlowNet multitask learning model trains a single stream network directly from raw pixels to jointly estimate optical flow while recognizing actions using convolutional neural networks. This captures both appearance and motion in a single model. We also offer insights into how the quality of the learned optical flow influences action recognition. Our model improves action recognition accuracy by 31% compared to state-of-the-art CNN-based action recognition models that lack external large-scale data and additional optical flow input. Despite not being pre-trained on large external labeled datasets, our model uses motion information efficiently and achieves recognition accuracy on par with models trained with large labeled datasets like ImageNet and Sport-1M.",1
"Despite recent interest and advances in facial micro-expression research, there is still plenty room for improvement in terms of micro-expression recognition. Conventional feature extraction approaches for micro-expression video consider either the whole video sequence or a part of it, for representation. However, with the high-speed video capture of micro-expressions (100-200 fps), are all frames necessary to provide a sufficiently meaningful representation? Is the luxury of data a bane to accurate recognition? A novel proposition is presented in this paper, whereby we utilize only two images per video: the apex frame and the onset frame. The apex frame of a video contains the highest intensity of expression changes among all frames, while the onset is the perfect choice of a reference frame with neutral expression. A new feature extractor, Bi-Weighted Oriented Optical Flow (Bi-WOOF) is proposed to encode essential expressiveness of the apex frame. We evaluated the proposed method on five micro-expression databases: CAS(ME)$^2$, CASME II, SMIC-HS, SMIC-NIR and SMIC-VIS. Our experiments lend credence to our hypothesis, with our proposed technique achieving a state-of-the-art F1-score recognition performance of 61% and 62% in the high frame rate CASME II and SMIC-HS databases respectively.",0
"Although facial micro-expression research has gained recent interest and made advances, there is still much room for improvement in recognizing micro-expressions. Current approaches for micro-expression video feature extraction involve using either the entire video sequence or a portion of it. However, with the high-speed video capture of micro-expressions (100-200 fps), it is unclear whether all frames are necessary to provide a meaningful representation. This paper proposes a novel method that uses only two images per video: the apex frame and the onset frame. The apex frame captures the highest intensity of expression changes, while the onset frame is a neutral reference frame. Our proposed method, Bi-Weighted Oriented Optical Flow (Bi-WOOF), is used to extract essential expressiveness from the apex frame. We evaluated our technique on five micro-expression databases and achieved a state-of-the-art F1-score recognition performance of 61% and 62% in the high frame rate CASME II and SMIC-HS databases, respectively. Our experiments lend credibility to our hypothesis that our approach is effective in improving micro-expression recognition.",1
"Wood-composite materials are widely used today as they homogenize humidity related directional deformations. Quantification of these deformations as coefficients is important for construction and engineering and topic of current research but still a manual process.   This work introduces a novel computer vision approach that automatically extracts these properties directly from scans of the wooden specimens, taken at different humidity levels during the long lasting humidity conditioning process. These scans are used to compute a humidity dependent deformation field for each pixel, from which the desired coefficients can easily be calculated.   The overall method includes automated registration of the wooden blocks, numerical optimization to compute a variational optical flow field which is further used to calculate dense strain fields and finally the engineering coefficients and their variance throughout the wooden blocks. The methods regularization is fully parameterizable which allows to model and suppress artifacts due to surface appearance changes of the specimens from mold, cracks, etc. that typically arise in the conditioning process.",0
"Wood-composite materials are commonly utilized due to their ability to homogenize humidity-induced deformations. However, calculating these deformations as coefficients for construction and engineering purposes is still a manual process and an area of current research. This study presents a new computer vision technique that automatically extracts these properties from scans of wooden specimens at varying humidity levels during extended conditioning. By computing a humidity-dependent deformation field for each pixel, the desired coefficients can be easily calculated. The method involves automated registration of the wooden blocks, numerical optimization to compute a variational optical flow field, determination of dense strain fields, and calculation of engineering coefficients and their variance throughout the wooden blocks. The method's regularization is fully parameterizable, allowing for the modeling and suppression of artifacts arising from surface appearance changes due to mold, cracks, etc. that typically occur during the conditioning process.",1
"In this paper, we present an unsupervised learning framework for analyzing activities and interactions in surveillance videos. In our framework, three levels of video events are connected by Hierarchical Dirichlet Process (HDP) model: low-level visual features, simple atomic activities, and multi-agent interactions. Atomic activities are represented as distribution of low-level features, while complicated interactions are represented as distribution of atomic activities. This learning process is unsupervised. Given a training video sequence, low-level visual features are extracted based on optic flow and then clustered into different atomic activities and video clips are clustered into different interactions. The HDP model automatically decide the number of clusters, i.e. the categories of atomic activities and interactions. Based on the learned atomic activities and interactions, a training dataset is generated to train the Gaussian Process (GP) classifier. Then the trained GP models work in newly captured video to classify interactions and detect abnormal events in real time. Furthermore, the temporal dependencies between video events learned by HDP-Hidden Markov Models (HMM) are effectively integrated into GP classifier to enhance the accuracy of the classification in newly captured videos. Our framework couples the benefits of the generative model (HDP) with the discriminant model (GP). We provide detailed experiments showing that our framework enjoys favorable performance in video event classification in real-time in a crowded traffic scene.",0
"The focus of this paper is on presenting an unsupervised learning framework that can be used to analyze activities and interactions in surveillance videos. The framework operates on three distinct levels of video events, namely low-level visual features, simple atomic activities, and multi-agent interactions, which are connected by the Hierarchical Dirichlet Process (HDP) model. In this model, atomic activities are represented by a distribution of low-level features, while complicated interactions are represented by a distribution of atomic activities. The learning process is unsupervised, meaning that given a training video sequence, low-level visual features are extracted based on optic flow and then clustered into different atomic activities and video clips are clustered into different interactions, with the HDP model automatically deciding the number of clusters. A training dataset is generated based on the learned atomic activities and interactions to train the Gaussian Process (GP) classifier, which then works in newly captured video to classify interactions and detect abnormal events in real-time. To enhance the accuracy of the classification, the temporal dependencies between video events learned by HDP-Hidden Markov Models (HMM) are effectively integrated into the GP classifier. The framework combines the benefits of the generative model (HDP) with the discriminant model (GP) and has been shown to perform well in video event classification in real-time in a crowded traffic scene.",1
"Computer vision researchers have been expecting that neural networks have spatial transformation ability to eliminate the interference caused by geometric distortion for a long time. Emergence of spatial transformer network makes dream come true. Spatial transformer network and its variants can handle global displacement well, but lack the ability to deal with local spatial variance. Hence how to achieve a better manner of deformation in the neural network has become a pressing matter of the moment. To address this issue, we analyze the advantages and disadvantages of approximation theory and optical flow theory, then we combine them to propose a novel way to achieve image deformation and implement it with a hierarchical convolutional neural network. This new approach solves for a linear deformation along with an optical flow field to model image deformation. In the experiments of cluttered MNIST handwritten digits classification and image plane alignment, our method outperforms baseline methods by a large margin.",0
"For quite some time, computer vision researchers have anticipated that neural networks would possess the spatial transformation ability to eliminate interference caused by geometric distortion. With the introduction of the spatial transformer network, this expectation has been fulfilled. The spatial transformer network and its variants are capable of handling global displacement effectively, however, they lack the ability to address local spatial variance. As a result, the development of a better approach to deformation in neural networks has become a pressing concern. To tackle this issue, we have scrutinized both approximation theory and optical flow theory, and merged them to propose a new technique for achieving image deformation. This approach involves solving for a linear deformation and an optical flow field to model image deformation, which is then implemented with a hierarchical convolutional neural network. In experiments involving cluttered MNIST handwritten digits classification and image plane alignment, our method significantly outperforms baseline methods.",1
"In this paper, we present a new method for detecting road users in an urban environment which leads to an improvement in multiple object tracking. Our method takes as an input a foreground image and improves the object detection and segmentation. This new image can be used as an input to trackers that use foreground blobs from background subtraction. The first step is to create foreground images for all the frames in an urban video. Then, starting from the original blobs of the foreground image, we merge the blobs that are close to one another and that have similar optical flow. The next step is extracting the edges of the different objects to detect multiple objects that might be very close (and be merged in the same blob) and to adjust the size of the original blobs. At the same time, we use the optical flow to detect occlusion of objects that are moving in opposite directions. Finally, we make a decision on which information we keep in order to construct a new foreground image with blobs that can be used for tracking. The system is validated on four videos of an urban traffic dataset. Our method improves the recall and precision metrics for the object detection task compared to the vanilla background subtraction method and improves the CLEAR MOT metrics in the tracking tasks for most videos.",0
"The paper introduces a novel approach to identifying road users in urban areas, which enhances the accuracy of multiple object tracking. Our approach utilizes a foreground image as input and enhances object detection and segmentation. This new image can then be employed in trackers that use foreground blobs from background subtraction. The process begins by creating foreground images for all video frames in an urban setting. We then merge nearby blobs with similar optical flow from the original foreground image and extract object edges to detect multiple objects that may have been combined in the same blob and adjust original blob size. Concurrently, we use optical flow to identify obstructed objects moving in opposing directions. The final step involves deciding which data to keep to develop a new foreground image with blobs for tracking. We validate our system using four urban traffic videos and show that our approach outperforms the vanilla background subtraction method in terms of recall and precision metrics for object detection while improving the CLEAR MOT metrics for most videos in tracking tasks.",1
"Most of the crowd abnormal event detection methods rely on complex hand-crafted features to represent the crowd motion and appearance. Convolutional Neural Networks (CNN) have shown to be a powerful tool with excellent representational capacities, which can leverage the need for hand-crafted features. In this paper, we show that keeping track of the changes in the CNN feature across time can facilitate capturing the local abnormality. We specifically propose a novel measure-based method which allows measuring the local abnormality in a video by combining semantic information (inherited from existing CNN models) with low-level Optical-Flow. One of the advantage of this method is that it can be used without the fine-tuning costs. The proposed method is validated on challenging abnormality detection datasets and the results show the superiority of our method compared to the state-of-the-art methods.",0
"The majority of abnormal event detection techniques for crowds rely on intricate, manually-crafted features to represent the movement and appearance of the group. However, Convolutional Neural Networks (CNNs) possess exceptional representational capabilities and can eliminate the need for such features. In this study, we demonstrate that monitoring changes in CNN features over time can aid in identifying local anomalies. Our proposed approach involves a novel measure-based method that combines existing CNN models' semantic information with low-level Optical-Flow to gauge local abnormality in videos. This method offers the advantage of not requiring costly fine-tuning. We evaluated our method on difficult abnormality detection datasets and found that it outperforms state-of-the-art approaches.",1
"In recent years, deep neural network approaches have naturally extended to the video domain, in their simplest case by aggregating per-frame classifications as a baseline for action recognition. A majority of the work in this area extends from the imaging domain, leading to visual-feature heavy approaches on temporal data. To address this issue we introduce ""Let's Dance"", a 1000 video dataset (and growing) comprised of 10 visually overlapping dance categories that require motion for their classification. We stress the important of human motion as a key distinguisher in our work given that, as we show in this work, visual information is not sufficient to classify motion-heavy categories. We compare our datasets' performance using imaging techniques with UCF-101 and demonstrate this inherent difficulty. We present a comparison of numerous state-of-the-art techniques on our dataset using three different representations (video, optical flow and multi-person pose data) in order to analyze these approaches. We discuss the motion parameterization of each of them and their value in learning to categorize online dance videos. Lastly, we release this dataset (and its three representations) for the research community to use.",0
"In recent times, deep neural network methods have extended naturally to the video domain, with the basic approach being the aggregation of per-frame classifications as a baseline for action recognition. However, most of the work in this area has been derived from the imaging domain, resulting in visual-heavy approaches on temporal data. To overcome this limitation, we introduce a dataset called ""Let's Dance"", which comprises 1000 videos (and is still growing) that consist of 10 visually overlapping dance categories that require motion for their classification. We emphasize the significance of human motion as a crucial distinguishing factor in our research, as we demonstrate that visual information alone is insufficient for classifying motion-heavy categories. We compare our dataset's performance using imaging techniques with UCF-101 and highlight this inherent difficulty. To analyze these approaches, we present a comparison of several state-of-the-art techniques on our dataset using three different representations (video, optical flow, and multi-person pose data). We also discuss the motion parameterization of each of them and their usefulness in learning to categorize online dance videos. Finally, we release this dataset (and its three representations) to the research community for their use.",1
"Motion blur is a fundamental problem in computer vision as it impacts image quality and hinders inference. Traditional deblurring algorithms leverage the physics of the image formation model and use hand-crafted priors: they usually produce results that better reflect the underlying scene, but present artifacts. Recent learning-based methods implicitly extract the distribution of natural images directly from the data and use it to synthesize plausible images. Their results are impressive, but they are not always faithful to the content of the latent image. We present an approach that bridges the two. Our method fine-tunes existing deblurring neural networks in a self-supervised fashion by enforcing that the output, when blurred based on the optical flow between subsequent frames, matches the input blurry image. We show that our method significantly improves the performance of existing methods on several datasets both visually and in terms of image quality metrics. The supplementary material is https://goo.gl/nYPjEQ",0
"The blurring of motion poses a significant issue in computer vision as it adversely affects the quality of images and impedes interpretation. Traditional techniques for deblurring rely on the principles of image formation and incorporate prior knowledge, resulting in a more accurate portrayal of the underlying scene, albeit with some flaws. Recently developed machine learning-based methods implicitly capture the distribution of natural images from data, leading to impressive results. However, they may not always accurately reflect the latent image. Our approach reconciles these two methods by refining existing neural networks for deblurring in a self-supervised manner. We achieve this by ensuring that the output, when blurred using the optical flow between frames, matches the input blurry image. Our study demonstrates a significant improvement in the performance of existing methods on various datasets, both visually and in terms of image quality metrics. Supplementary material can be found at https://goo.gl/nYPjEQ.",1
"Scene flow is a description of real world motion in 3D that contains more information than optical flow. Because of its complexity there exists no applicable variant for real-time scene flow estimation in an automotive or commercial vehicle context that is sufficiently robust and accurate. Therefore, many applications estimate the 2D optical flow instead. In this paper, we examine the combination of top-performing state-of-the-art optical flow and stereo disparity algorithms in order to achieve a basic scene flow. On the public KITTI Scene Flow Benchmark we demonstrate the reasonable accuracy of the combination approach and show its speed in computation.",0
"Describing motion in 3D in the real world, scene flow provides more extensive details than optical flow. Due to its intricate nature, there is currently no feasible option for determining real-time scene flow in the context of commercial or automotive vehicles that is both reliable and precise. Hence, many applications opt to estimate 2D optical flow instead. This study investigates the use of cutting-edge optical flow and stereo disparity algorithms to create a fundamental scene flow. Our results show that this combination approach is reasonably accurate and fast in computation, as demonstrated by our performance on the KITTI Scene Flow Benchmark.",1
"Light field imaging has recently known a regain of interest due to the availability of practical light field capturing systems that offer a wide range of applications in the field of computer vision. However, capturing high-resolution light fields remains technologically challenging since the increase in angular resolution is often accompanied by a significant reduction in spatial resolution. This paper describes a learning-based spatial light field super-resolution method that allows the restoration of the entire light field with consistency across all sub-aperture images. The algorithm first uses optical flow to align the light field and then reduces its angular dimension using low-rank approximation. We then consider the linearly independent columns of the resulting low-rank model as an embedding, which is restored using a deep convolutional neural network (DCNN). The super-resolved embedding is then used to reconstruct the remaining sub-aperture images. The original disparities are restored using inverse warping where missing pixels are approximated using a novel light field inpainting algorithm. Experimental results show that the proposed method outperforms existing light field super-resolution algorithms, achieving PSNR gains of 0.23 dB over the second best performing method. This performance can be further improved using iterative back-projection as a post-processing step.",0
"Practical light field capturing systems have sparked renewed interest in light field imaging, offering a variety of computer vision applications. Despite challenges in capturing high-resolution light fields, a new spatial light field super-resolution method has been developed using a learning-based approach. Optical flow is employed to align and reduce the angular dimension of the light field, utilizing linearly independent columns as an embedding that is restored with a deep convolutional neural network. The resulting super-resolved embedding is then used to reconstruct the remaining sub-aperture images. The method outperforms existing light field super-resolution algorithms, with PSNR gains of 0.23 dB over the second best method, and iterative back-projection can be used for further improvement. A novel light field inpainting algorithm is used to approximate missing pixels in the original disparities.",1
"Most of the top performing action recognition methods use optical flow as a ""black box"" input. Here we take a deeper look at the combination of flow and action recognition, and investigate why optical flow is helpful, what makes a flow method good for action recognition, and how we can make it better. In particular, we investigate the impact of different flow algorithms and input transformations to better understand how these affect a state-of-the-art action recognition method. Furthermore, we fine tune two neural-network flow methods end-to-end on the most widely used action recognition dataset (UCF101). Based on these experiments, we make the following five observations: 1) optical flow is useful for action recognition because it is invariant to appearance, 2) optical flow methods are optimized to minimize end-point-error (EPE), but the EPE of current methods is not well correlated with action recognition performance, 3) for the flow methods tested, accuracy at boundaries and at small displacements is most correlated with action recognition performance, 4) training optical flow to minimize classification error instead of minimizing EPE improves recognition performance, and 5) optical flow learned for the task of action recognition differs from traditional optical flow especially inside the human body and at the boundary of the body. These observations may encourage optical flow researchers to look beyond EPE as a goal and guide action recognition researchers to seek better motion cues, leading to a tighter integration of the optical flow and action recognition communities.",0
"The majority of top-performing action recognition methods employ optical flow as a ""black box"" input. However, we delve deeper into the combination of flow and action recognition to examine why optical flow is beneficial, what factors contribute to a good flow method for action recognition, and how to enhance it. Specifically, we analyze the impact of various flow algorithms and input transformations to gain a better understanding of their effects on state-of-the-art action recognition methods. Additionally, we fine-tune two neural-network flow methods end-to-end on the widely used UCF101 action recognition dataset. Based on our experiments, we make five observations: 1) optical flow is advantageous for action recognition due to its appearance invariance, 2) optimizing optical flow methods to minimize end-point-error (EPE) does not necessarily improve action recognition performance, 3) accuracy at boundaries and small displacements is most closely related to action recognition performance among the flow methods tested, 4) training optical flow to minimize classification error instead of EPE enhances recognition performance, and 5) optical flow learned for action recognition differs from traditional optical flow in terms of the human body's interior and boundary. These findings should motivate optical flow researchers to explore goals beyond EPE and encourage action recognition researchers to seek more effective motion cues, thus fostering closer collaboration between the optical flow and action recognition communities.",1
"Video image datasets are playing an essential role in design and evaluation of traffic vision algorithms. Nevertheless, a longstanding inconvenience concerning image datasets is that manually collecting and annotating large-scale diversified datasets from real scenes is time-consuming and prone to error. For that virtual datasets have begun to function as a proxy of real datasets. In this paper, we propose to construct large-scale artificial scenes for traffic vision research and generate a new virtual dataset called ""ParallelEye"". First of all, the street map data is used to build 3D scene model of Zhongguancun Area, Beijing. Then, the computer graphics, virtual reality, and rule modeling technologies are utilized to synthesize large-scale, realistic virtual urban traffic scenes, in which the fidelity and geography match the real world well. Furthermore, the Unity3D platform is used to render the artificial scenes and generate accurate ground-truth labels, e.g., semantic/instance segmentation, object bounding box, object tracking, optical flow, and depth. The environmental conditions in artificial scenes can be controlled completely. As a result, we present a viable implementation pipeline for constructing large-scale artificial scenes for traffic vision research. The experimental results demonstrate that this pipeline is able to generate photorealistic virtual datasets with low modeling time and high accuracy labeling.",0
"The use of video image datasets is crucial for the development and assessment of traffic vision algorithms. However, manually collecting and annotating diverse datasets from real-life scenes is both time-consuming and prone to errors. Therefore, virtual datasets have become a useful substitute for real datasets. This paper proposes the creation of large-scale artificial scenes for traffic vision research, resulting in a new virtual dataset known as ""ParallelEye"". Firstly, 3D scene models of the Zhongguancun Area in Beijing are developed using street map data. Computer graphics, virtual reality and rule modeling technologies are then used to synthesize realistic virtual urban traffic scenes that closely resemble real-world scenarios. The Unity3D platform is used to render the artificial scenes and generate accurate ground-truth labels such as semantic/instance segmentation, object bounding box, object tracking, optical flow and depth. The environmental conditions in the artificial scenes can be fully controlled, resulting in a viable implementation pipeline for constructing large-scale artificial scenes for traffic vision research. Experimental results demonstrate that this pipeline is capable of generating photorealistic virtual datasets with high accuracy labeling and low modeling time.",1
"We investigate video classification via a two-stream convolutional neural network (CNN) design that directly ingests information extracted from compressed video bitstreams. Our approach begins with the observation that all modern video codecs divide the input frames into macroblocks (MBs). We demonstrate that selective access to MB motion vector (MV) information within compressed video bitstreams can also provide for selective, motion-adaptive, MB pixel decoding (a.k.a., MB texture decoding). This in turn allows for the derivation of spatio-temporal video activity regions at extremely high speed in comparison to conventional full-frame decoding followed by optical flow estimation. In order to evaluate the accuracy of a video classification framework based on such activity data, we independently train two CNN architectures on MB texture and MV correspondences and then fuse their scores to derive the final classification of each test video. Evaluation on two standard datasets shows that the proposed approach is competitive to the best two-stream video classification approaches found in the literature. At the same time: (i) a CPU-based realization of our MV extraction is over 977 times faster than GPU-based optical flow methods; (ii) selective decoding is up to 12 times faster than full-frame decoding; (iii) our proposed spatial and temporal CNNs perform inference at 5 to 49 times lower cloud computing cost than the fastest methods from the literature.",0
"Our study explores video classification using a two-stream convolutional neural network (CNN) design that directly utilizes information extracted from compressed video bitstreams. We have observed that all modern video codecs divide input frames into macroblocks (MBs). By selectively accessing MB motion vector (MV) information within compressed video bitstreams, we can achieve motion-adaptive MB pixel decoding, which enables us to derive spatio-temporal video activity regions at extremely high speed compared to conventional full-frame decoding followed by optical flow estimation. We evaluate the accuracy of our video classification framework by independently training two CNN architectures on MB texture and MV correspondences and then fusing their scores to derive the final classification of each test video. Our approach proves to be competitive with the best two-stream video classification approaches found in the literature on two standard datasets. Additionally, our CPU-based realization of MV extraction is over 977 times faster than GPU-based optical flow methods, selective decoding is up to 12 times faster than full-frame decoding, and our proposed spatial and temporal CNNs perform inference at 5 to 49 times lower cloud computing cost than the fastest methods from the literature.",1
"This work proposes a novel deep network architecture to solve the camera Ego-Motion estimation problem. A motion estimation network generally learns features similar to Optical Flow (OF) fields starting from sequences of images. This OF can be described by a lower dimensional latent space. Previous research has shown how to find linear approximations of this space. We propose to use an Auto-Encoder network to find a non-linear representation of the OF manifold. In addition, we propose to learn the latent space jointly with the estimation task, so that the learned OF features become a more robust description of the OF input. We call this novel architecture LS-VO.   The experiments show that LS-VO achieves a considerable increase in performances in respect to baselines, while the number of parameters of the estimation network only slightly increases.",0
"A fresh deep network architecture is introduced in this study to tackle the issue of camera Ego-Motion estimation. Typically, motion estimation networks learn features similar to Optical Flow (OF) fields by analyzing sequences of images. These OF features can be represented by a reduced latent space. Past research has demonstrated finding linear approximations of this space. However, we propose using an Auto-Encoder network to discover a nonlinear representation of the OF manifold. Furthermore, we suggest that the latent space and estimation task be learned together to improve the robustness of the OF features. This new architecture is called LS-VO. The results demonstrate that LS-VO outperforms the baselines significantly, while the estimation network's parameter count only slightly increases.",1
"While deep feature learning has revolutionized techniques for static-image understanding, the same does not quite hold for video processing. Architectures and optimization techniques used for video are largely based off those for static images, potentially underutilizing rich video information. In this work, we rethink both the underlying network architecture and the stochastic learning paradigm for temporal data. To do so, we draw inspiration from classic theory on linear dynamic systems for modeling time series. By extending such models to include nonlinear mappings, we derive a series of novel recurrent neural networks that sequentially make top-down predictions about the future and then correct those predictions with bottom-up observations. Predictive-corrective networks have a number of desirable properties: (1) they can adaptively focus computation on ""surprising"" frames where predictions require large corrections, (2) they simplify learning in that only ""residual-like"" corrective terms need to be learned over time and (3) they naturally decorrelate an input data stream in a hierarchical fashion, producing a more reliable signal for learning at each layer of a network. We provide an extensive analysis of our lightweight and interpretable framework, and demonstrate that our model is competitive with the two-stream network on three challenging datasets without the need for computationally expensive optical flow.",0
"Although deep feature learning has transformed the way static images are analyzed, this is not the case for video processing. The techniques and optimization methods employed for videos are mostly based on those used for static images, which may not fully exploit the abundant information available in videos. This research introduces a new approach to network architecture and stochastic learning for temporal data by drawing on classic theories of linear dynamic systems for modeling time series. By extending these models to include nonlinear mappings, a set of novel recurrent neural networks that sequentially predict the future and then adjust those predictions with observations is derived. Predictive-corrective networks have several benefits, including adaptively focusing computation on ""surprising"" frames, simplifying learning by only requiring ""residual-like"" corrective terms to be learned over time, and naturally decorrelating an input data stream in a hierarchical manner, producing a more dependable signal for learning at each layer of a network. Our lightweight and interpretable framework is thoroughly analyzed, and we demonstrate that our model is competitive with the two-stream network on three challenging datasets without the need for computationally expensive optical flow.",1
"This paper proposes a deep learning model to efficiently detect salient regions in videos. It addresses two important issues: (1) deep video saliency model training with the absence of sufficiently large and pixel-wise annotated video data, and (2) fast video saliency training and detection. The proposed deep video saliency network consists of two modules, for capturing the spatial and temporal saliency information, respectively. The dynamic saliency model, explicitly incorporating saliency estimates from the static saliency model, directly produces spatiotemporal saliency inference without time-consuming optical flow computation. We further propose a novel data augmentation technique that simulates video training data from existing annotated image datasets, which enables our network to learn diverse saliency information and prevents overfitting with the limited number of training videos. Leveraging our synthetic video data (150K video sequences) and real videos, our deep video saliency model successfully learns both spatial and temporal saliency cues, thus producing accurate spatiotemporal saliency estimate. We advance the state-of-the-art on the DAVIS dataset (MAE of .06) and the FBMS dataset (MAE of .07), and do so with much improved speed (2fps with all steps).",0
"A deep learning model is proposed in this paper to efficiently detect significant regions in videos while addressing two crucial concerns: (1) the absence of adequate pixel-wise annotated video data for deep video saliency model training and (2) the need for fast video saliency training and detection. Our proposed deep video saliency network comprises two modules for capturing spatial and temporal saliency information, respectively. The dynamic saliency model directly produces spatiotemporal saliency inference by incorporating saliency estimates from the static saliency model, eliminating the need for time-consuming optical flow computation. Additionally, we present a novel data augmentation technique that simulates video training data from existing annotated image datasets, enabling our network to learn diverse saliency information and prevent overfitting with the limited number of training videos. By leveraging our synthetic video data (150K video sequences) and real videos, our deep video saliency model successfully learns both spatial and temporal saliency cues, resulting in accurate spatiotemporal saliency estimates. We achieve state-of-the-art performance on the DAVIS dataset (MAE of .06) and the FBMS dataset (MAE of .07), with much-improved speed (2fps with all steps).",1
"This paper presents a novel method for detecting scene changes from a pair of images with a difference of camera viewpoints using a dense optical flow based change detection network. In the case that camera poses of input images are fixed or known, such as with surveillance and satellite cameras, the pixel correspondence between the images captured at different times can be known. Hence, it is possible to comparatively accurately detect scene changes between the images by modeling the appearance of the scene. On the other hand, in case of cameras mounted on a moving object, such as ground and aerial vehicles, we must consider the spatial correspondence between the images captured at different times. However, it can be difficult to accurately estimate the camera pose or 3D model of a scene, owing to the scene changes or lack of imagery. To solve this problem, we propose a change detection convolutional neural network utilizing dense optical flow between input images to improve the robustness to the difference between camera viewpoints. Our evaluation based on the panoramic change detection dataset shows that the proposed method outperforms state-of-the-art change detection algorithms.",0
"The paper describes a new technique to identify changes in a scene by analyzing a pair of images taken from different camera angles. If the camera position is fixed or known, detecting changes is relatively easy as the images can be compared based on their appearance. However, if the camera is mounted on a moving object, the challenge is to establish spatial correspondence between the images. This is complicated by changes in the scene or lack of data. To address this problem, the paper proposes a convolutional neural network that uses dense optical flow to increase its resilience to variations in camera viewpoints. The new method was evaluated using a panoramic dataset and was found to outperform existing techniques.",1
"Current state-of-the-art solutions for motion capture from a single camera are optimization driven: they optimize the parameters of a 3D human model so that its re-projection matches measurements in the video (e.g. person segmentation, optical flow, keypoint detections etc.). Optimization models are susceptible to local minima. This has been the bottleneck that forced using clean green-screen like backgrounds at capture time, manual initialization, or switching to multiple cameras as input resource. In this work, we propose a learning based motion capture model for single camera input. Instead of optimizing mesh and skeleton parameters directly, our model optimizes neural network weights that predict 3D shape and skeleton configurations given a monocular RGB video. Our model is trained using a combination of strong supervision from synthetic data, and self-supervision from differentiable rendering of (a) skeletal keypoints, (b) dense 3D mesh motion, and (c) human-background segmentation, in an end-to-end framework. Empirically we show our model combines the best of both worlds of supervised learning and test-time optimization: supervised learning initializes the model parameters in the right regime, ensuring good pose and surface initialization at test time, without manual effort. Self-supervision by back-propagating through differentiable rendering allows (unsupervised) adaptation of the model to the test data, and offers much tighter fit than a pretrained fixed model. We show that the proposed model improves with experience and converges to low-error solutions where previous optimization methods fail.",0
"State-of-the-art methods for motion capture using a single camera involve optimizing the parameters of a 3D human model to match measurements in the video. However, optimization models are prone to getting stuck in local minima, making it necessary to use green-screen backgrounds, manual initialization, or multiple cameras. This paper proposes a motion capture model that uses a neural network to predict 3D shape and skeleton configurations from monocular RGB videos. The model is trained using both strong supervision from synthetic data and self-supervision from differentiable rendering. This approach combines the benefits of supervised learning and test-time optimization, allowing for good pose and surface initialization without manual effort and offering a tighter fit than a pretrained fixed model. The proposed model improves with experience and can converge to low-error solutions where previous optimization methods failed.",1
"We study the problem of segmenting moving objects in unconstrained videos. Given a video, the task is to segment all the objects that exhibit independent motion in at least one frame. We formulate this as a learning problem and design our framework with three cues: (i) independent object motion between a pair of frames, which complements object recognition, (ii) object appearance, which helps to correct errors in motion estimation, and (iii) temporal consistency, which imposes additional constraints on the segmentation. The framework is a two-stream neural network with an explicit memory module. The two streams encode appearance and motion cues in a video sequence respectively, while the memory module captures the evolution of objects over time, exploiting the temporal consistency. The motion stream is a convolutional neural network trained on synthetic videos to segment independently moving objects in the optical flow field. The module to build a 'visual memory' in video, i.e., a joint representation of all the video frames, is realized with a convolutional recurrent unit learned from a small number of training video sequences.   For every pixel in a frame of a test video, our approach assigns an object or background label based on the learned spatio-temporal features as well as the 'visual memory' specific to the video. We evaluate our method extensively on three benchmarks, DAVIS, Freiburg-Berkeley motion segmentation dataset and SegTrack. In addition, we provide an extensive ablation study to investigate both the choice of the training data and the influence of each component in the proposed framework.",0
"Our research focuses on the challenge of identifying moving objects in unrestricted videos. The objective is to segment all objects that demonstrate independent motion in at least one frame. To address this problem, we have created a learning model that incorporates three main cues: (i) independent object motion, in conjunction with object recognition, (ii) object appearance, to correct motion estimation errors, and (iii) temporal consistency, which adds further constraints to the segmentation. Our framework is a two-stream neural network that includes a memory module. The two streams encode appearance and motion cues, while the memory module captures the object's evolution through time, taking advantage of temporal consistency. We have trained a convolutional neural network on synthetic videos to segment independently moving objects in the optical flow field. Furthermore, we have developed a module to generate a 'visual memory' of the video, which is a joint representation of all frames. This is accomplished with a convolutional recurrent unit that is trained on a small set of training video sequences. Using our approach, we assign an object or background label to every pixel in a test video frame based on the learned spatio-temporal features and the 'visual memory' specific to the video. We have extensively evaluated our method using three benchmarks: DAVIS, Freiburg-Berkeley motion segmentation dataset, and SegTrack. Additionally, we have conducted an extensive ablation study to examine the impact of the training data and individual components of our framework.",1
"Learning to represent and generate videos from unlabeled data is a very challenging problem. To generate realistic videos, it is important not only to ensure that the appearance of each frame is real, but also to ensure the plausibility of a video motion and consistency of a video appearance in the time direction. The process of video generation should be divided according to these intrinsic difficulties. In this study, we focus on the motion and appearance information as two important orthogonal components of a video, and propose Flow-and-Texture-Generative Adversarial Networks (FTGAN) consisting of FlowGAN and TextureGAN. In order to avoid a huge annotation cost, we have to explore a way to learn from unlabeled data. Thus, we employ optical flow as motion information to generate videos. FlowGAN generates optical flow, which contains only the edge and motion of the videos to be begerated. On the other hand, TextureGAN specializes in giving a texture to optical flow generated by FlowGAN. This hierarchical approach brings more realistic videos with plausible motion and appearance consistency. Our experiments show that our model generates more plausible motion videos and also achieves significantly improved performance for unsupervised action classification in comparison to previous GAN works. In addition, because our model generates videos from two independent information, our model can generate new combinations of motion and attribute that are not seen in training data, such as a video in which a person is doing sit-up in a baseball ground.",0
"Generating and representing videos from unlabeled data is a complex challenge, requiring both realistic appearance and plausible motion and consistency in time. To address this, we propose the Flow-and-Texture-Generative Adversarial Networks (FTGAN), which divides the video generation process into two components: motion and appearance. We employ optical flow as motion information, generated by FlowGAN, which captures only the edges and motion of the video. TextureGAN then adds texture to the optical flow, resulting in more realistic videos. Our hierarchical approach achieves improved performance for unsupervised action classification and generates new combinations of motion and attributes not seen in training data. For example, our model can generate a video of a person doing sit-ups in a baseball field.",1
"Optical flow estimation in the rainy scenes is challenging due to background degradation introduced by rain streaks and rain accumulation effects in the scene. Rain accumulation effect refers to poor visibility of remote objects due to the intense rainfall. Most existing optical flow methods are erroneous when applied to rain sequences because the conventional brightness constancy constraint (BCC) and gradient constancy constraint (GCC) generally break down in this situation. Based on the observation that the RGB color channels receive raindrop radiance equally, we introduce a residue channel as a new data constraint to reduce the effect of rain streaks. To handle rain accumulation, our method decomposes the image into a piecewise-smooth background layer and a high-frequency detail layer. It also enforces the BCC on the background layer only. Results on both synthetic dataset and real images show that our algorithm outperforms existing methods on different types of rain sequences. To our knowledge, this is the first optical flow method specifically dealing with rain.",0
"The presence of rain streaks and rain accumulation effects in rainy scenes makes optical flow estimation a difficult task. Rain accumulation effect occurs due to heavy rainfall that causes poor visibility of objects in the distance. Most of the existing optical flow methods fail to provide accurate results in such scenarios because the conventional brightness constancy constraint (BCC) and gradient constancy constraint (GCC) do not hold true. However, we propose a new data constraint, which is the residue channel that considers the equal radiance received by the RGB color channels from raindrops. We also deal with rain accumulation by dividing the image into a background layer that is smooth and a high-frequency detail layer. The BCC is enforced only on the background layer. Our algorithm performs better than existing methods on various types of rain sequences, as demonstrated on both synthetic datasets and real images. This is the first optical flow method that is specifically designed to handle rain.",1
"In the era of end-to-end deep learning, many advances in computer vision are driven by large amounts of labeled data. In the optical flow setting, however, obtaining dense per-pixel ground truth for real scenes is difficult and thus such data is rare. Therefore, recent end-to-end convolutional networks for optical flow rely on synthetic datasets for supervision, but the domain mismatch between training and test scenarios continues to be a challenge. Inspired by classical energy-based optical flow methods, we design an unsupervised loss based on occlusion-aware bidirectional flow estimation and the robust census transform to circumvent the need for ground truth flow. On the KITTI benchmarks, our unsupervised approach outperforms previous unsupervised deep networks by a large margin, and is even more accurate than similar supervised methods trained on synthetic datasets alone. By optionally fine-tuning on the KITTI training data, our method achieves competitive optical flow accuracy on the KITTI 2012 and 2015 benchmarks, thus in addition enabling generic pre-training of supervised networks for datasets with limited amounts of ground truth.",0
"The advent of end-to-end deep learning has led to significant advancements in computer vision, particularly with regards to large amounts of labeled data. However, obtaining dense per-pixel ground truth for real scenes in the optical flow setting is challenging, resulting in a scarcity of such data. Consequently, recent end-to-end convolutional networks for optical flow rely on synthetic datasets for supervision, but the domain mismatch between training and test scenarios remains a major issue. Drawing inspiration from traditional energy-based optical flow methods, we have developed an unsupervised loss that uses occlusion-aware bidirectional flow estimation and the robust census transform to eliminate the need for ground truth flow. Our unsupervised approach significantly outperforms previous unsupervised deep networks on the KITTI benchmarks, and even outperforms similar supervised methods trained solely on synthetic datasets. We can achieve competitive optical flow accuracy on the KITTI 2012 and 2015 benchmarks by optionally fine-tuning on the KITTI training data, enabling generic pre-training of supervised networks for datasets with limited amounts of ground truth.",1
"One of the key challenges of visual perception is to extract abstract models of 3D objects and object categories from visual measurements, which are affected by complex nuisance factors such as viewpoint, occlusion, motion, and deformations. Starting from the recent idea of viewpoint factorization, we propose a new approach that, given a large number of images of an object and no other supervision, can extract a dense object-centric coordinate frame. This coordinate frame is invariant to deformations of the images and comes with a dense equivariant labelling neural network that can map image pixels to their corresponding object coordinates. We demonstrate the applicability of this method to simple articulated objects and deformable objects such as human faces, learning embeddings from random synthetic transformations or optical flow correspondences, all without any manual supervision.",0
"The process of visual perception faces a significant challenge in extracting abstract models of 3D objects and object categories from visual measurements. This challenge is complicated by factors like viewpoint, occlusion, motion, and deformations. To address this issue, we propose a novel approach that utilizes the viewpoint factorization concept. With this method, a dense object-centric coordinate frame can be extracted from a vast set of images of an object without any other form of supervision. The coordinate frame is immune to image deformation and has a dense equivariant labelling neural network that can map image pixels to their corresponding object coordinates. The technique can be applied to simple articulated objects and deformable objects, such as human faces. We have demonstrated the effectiveness of this approach in learning embeddings from random synthetic transformations or optical flow correspondences, all without any manual supervision.",1
"We present a method to reconstruct the three-dimensional trajectory of a moving instance of a known object category in monocular video data. We track the two-dimensional shape of objects on pixel level exploiting instance-aware semantic segmentation techniques and optical flow cues. We apply Structure from Motion techniques to object and background images to determine for each frame camera poses relative to object instances and background structures. By combining object and background camera pose information, we restrict the object trajectory to a one-parameter family of possible solutions. We compute a ground representation by fusing background structures and corresponding semantic segmentations. This allows us to determine an object trajectory consistent to image observations and reconstructed environment model. Our method is robust to occlusion and handles temporarily stationary objects. We show qualitative results using drone imagery. Due to the lack of suitable benchmark datasets we present a new dataset to evaluate the quality of reconstructed three-dimensional object trajectories. The video sequences contain vehicles in urban areas and are rendered using the path-tracing render engine Cycles to achieve realistic results. We perform a quantitative evaluation of the presented approach using this dataset. Our algorithm achieves an average reconstruction-to-ground-truth distance of 0.31 meter.",0
"A technique is introduced in this paper for reconstructing the three-dimensional path of a known object category in monocular video data. The method involves tracking the object's two-dimensional shape on a pixel level by utilizing instance-aware semantic segmentation techniques and optical flow cues. Structure from Motion techniques are then applied to both object and background images to determine the camera poses relative to object instances and background structures for each frame. By combining object and background camera pose information, the object trajectory is restricted to a one-parameter family of possible solutions. A ground representation is computed by fusing background structures and corresponding semantic segmentations, which allows for the determination of an object trajectory that is consistent with image observations and reconstructed environment models. The method is capable of handling temporarily stationary objects and is robust to occlusion. The paper showcases qualitative results using drone imagery and introduces a new dataset for evaluating the quality of reconstructed three-dimensional object trajectories. The dataset consists of video sequences of vehicles in urban areas rendered using the path-tracing render engine Cycles to achieve realistic results. The presented approach is quantitatively evaluated using this dataset, achieving an average reconstruction-to-ground-truth distance of 0.31 meter.",1
"We present a no reference (NR) quality assessment algorithm for assessing the perceptual quality of natural stereoscopic 3D (S3D) videos. This work is inspired by our finding that the joint statistics of the subband coefficients of motion (optical flow or motion vector magnitude) and depth (disparity map) of natural S3D videos possess a unique signature. Specifically, we empirically show that the joint statistics of the motion and depth subband coefficients of S3D video frames can be modeled accurately using a Bivariate Generalized Gaussian Distribution (BGGD). We then demonstrate that the parameters of the BGGD model possess the ability to discern quality variations in S3D videos. Therefore, the BGGD model parameters are employed as motion and depth quality features. In addition to these features, we rely on a frame level spatial quality feature that is computed using a robust off the shelf NR image quality assessment (IQA) algorithm. These frame level motion, depth and spatial features are consolidated and used with the corresponding S3D video's difference mean opinion score (DMOS) labels for supervised learning using support vector regression (SVR). The overall quality of an S3D video is computed by averaging the frame level quality predictions of the constituent video frames. The proposed algorithm, dubbed Video QUality Evaluation using MOtion and DEpth Statistics (VQUEMODES) is shown to outperform the state of the art methods when evaluated over the IRCCYN and LFOVIA S3D subjective quality assessment databases.",0
"We have developed an algorithm to assess the quality of natural stereoscopic 3D videos without a reference point. Our approach is based on the unique joint statistics of motion and depth subband coefficients found in S3D videos. We have observed that the Bivariate Generalized Gaussian Distribution (BGGD) can accurately model these coefficients. By using the parameters of the BGGD model as motion and depth quality features, we can identify quality variations in S3D videos. Additionally, we use a frame level spatial quality feature that is computed using an existing NR image quality assessment algorithm. These features are consolidated and used with the respective S3D video's difference mean opinion score (DMOS) labels for supervised learning using support vector regression (SVR). The overall quality of an S3D video is determined by averaging the frame level quality predictions. Our algorithm, Video QUality Evaluation using MOtion and DEpth Statistics (VQUEMODES), outperforms existing methods when evaluated over the IRCCYN and LFOVIA S3D subjective quality assessment databases.",1
"Computational saliency models for still images have gained significant popularity in recent years. Saliency prediction from videos, on the other hand, has received relatively little interest from the community. Motivated by this, in this work, we study the use of deep learning for dynamic saliency prediction and propose the so-called spatio-temporal saliency networks. The key to our models is the architecture of two-stream networks where we investigate different fusion mechanisms to integrate spatial and temporal information. We evaluate our models on the DIEM and UCF-Sports datasets and present highly competitive results against the existing state-of-the-art models. We also carry out some experiments on a number of still images from the MIT300 dataset by exploiting the optical flow maps predicted from these images. Our results show that considering inherent motion information in this way can be helpful for static saliency estimation.",0
"Over the past few years, computational saliency models for still images have gained considerable popularity while saliency prediction from videos has not received much attention from the community. In this study, we aim to address this gap by exploring the use of deep learning for dynamic saliency prediction through the development of spatio-temporal saliency networks. Our models rely on the architecture of two-stream networks and investigate various fusion mechanisms to integrate spatial and temporal information. We evaluate the effectiveness of our models on the DIEM and UCF-Sports datasets and demonstrate competitive results compared to existing state-of-the-art models. Additionally, we conduct experiments using the MIT300 dataset to explore the benefits of incorporating optical flow maps for static saliency estimation. Our results suggest that taking into account inherent motion information can improve static saliency estimation.",1
"Small flying robots can perform landing maneuvers using bio-inspired optical flow by maintaining a constant divergence. However, optical flow is typically estimated from frame sequences recorded by standard miniature cameras. This requires processing full images on-board, limiting the update rate of divergence measurements, and thus the speed of the control loop and the robot. Event-based cameras overcome these limitations by only measuring pixel-level brightness changes at microsecond temporal accuracy, hence providing an efficient mechanism for optical flow estimation. This paper presents, to the best of our knowledge, the first work integrating event-based optical flow estimation into the control loop of a flying robot. We extend an existing 'local plane fitting' algorithm to obtain an improved and more computationally efficient optical flow estimation method, valid for a wide range of optical flow velocities. This method is validated for real event sequences. In addition, a method for estimating the divergence from event-based optical flow is introduced, which accounts for the aperture problem. The developed algorithms are implemented in a constant divergence landing controller on-board of a quadrotor. Experiments show that, using event-based optical flow, accurate divergence estimates can be obtained over a wide range of speeds. This enables the quadrotor to perform very fast landing maneuvers.",0
"Using bio-inspired optical flow, small flying robots can execute landing maneuvers by maintaining a constant divergence. However, the process of estimating optical flow from frame sequences captured by standard miniature cameras involves on-board processing of full images, which limits the speed of the control loop and the robot's movement. To overcome this limitation, event-based cameras measure pixel-level brightness changes at microsecond temporal accuracy, making optical flow estimation efficient. This study presents the first integration of event-based optical flow estimation into the control loop of a flying robot, using an improved and computationally efficient optical flow estimation method that is validated for real event sequences. The study also introduces a method for estimating the divergence from event-based optical flow that accounts for the aperture problem. By implementing the developed algorithms in a constant divergence landing controller on-board a quadrotor, the experiments show that the quadrotor can perform fast landing maneuvers using accurate divergence estimates obtained over a wide range of speeds.",1
"Video classification is highly important with wide applications, such as video search and intelligent surveillance. Video naturally consists of static and motion information, which can be represented by frame and optical flow. Recently, researchers generally adopt the deep networks to capture the static and motion information \textbf{\emph{separately}}, which mainly has two limitations: (1) Ignoring the coexistence relationship between spatial and temporal attention, while they should be jointly modelled as the spatial and temporal evolutions of video, thus discriminative video features can be extracted.(2) Ignoring the strong complementarity between static and motion information coexisted in video, while they should be collaboratively learned to boost each other. For addressing the above two limitations, this paper proposes the approach of two-stream collaborative learning with spatial-temporal attention (TCLSTA), which consists of two models: (1) Spatial-temporal attention model: The spatial-level attention emphasizes the salient regions in frame, and the temporal-level attention exploits the discriminative frames in video. They are jointly learned and mutually boosted to learn the discriminative static and motion features for better classification performance. (2) Static-motion collaborative model: It not only achieves mutual guidance on static and motion information to boost the feature learning, but also adaptively learns the fusion weights of static and motion streams, so as to exploit the strong complementarity between static and motion information to promote video classification. Experiments on 4 widely-used datasets show that our TCLSTA approach achieves the best performance compared with more than 10 state-of-the-art methods.",0
"The classification of videos has numerous applications, such as video search and intelligent surveillance. Videos contain both static and motion information, represented by frame and optical flow respectively. To capture this information, researchers have recently used deep networks. However, this approach has two limitations: it ignores the relationship between spatial and temporal attention and the complementarity between static and motion information. To address these limitations, the author proposes the two-stream collaborative learning with spatial-temporal attention approach (TCLSTA). This approach includes a spatial-temporal attention model and a static-motion collaborative model. The former emphasizes salient regions in frames and discriminative frames in videos, while the latter promotes mutual guidance on static and motion information and adaptively learns fusion weights to exploit the complementarity between static and motion information. The TCLSTA approach outperforms more than 10 state-of-the-art methods on four widely-used datasets.",1
"The ability of predicting the future is important for intelligent systems, e.g. autonomous vehicles and robots to plan early and make decisions accordingly. Future scene parsing and optical flow estimation are two key tasks that help agents better understand their environments as the former provides dense semantic information, i.e. what objects will be present and where they will appear, while the latter provides dense motion information, i.e. how the objects will move. In this paper, we propose a novel model to simultaneously predict scene parsing and optical flow in unobserved future video frames. To our best knowledge, this is the first attempt in jointly predicting scene parsing and motion dynamics. In particular, scene parsing enables structured motion prediction by decomposing optical flow into different groups while optical flow estimation brings reliable pixel-wise correspondence to scene parsing. By exploiting this mutually beneficial relationship, our model shows significantly better parsing and motion prediction results when compared to well-established baselines and individual prediction models on the large-scale Cityscapes dataset. In addition, we also demonstrate that our model can be used to predict the steering angle of the vehicles, which further verifies the ability of our model to learn latent representations of scene dynamics.",0
"Intelligent systems, such as autonomous vehicles and robots, require the ability to predict future events in order to plan ahead and make informed decisions. To achieve this, two important tasks are future scene parsing and optical flow estimation. Future scene parsing provides semantic information about what objects will be present and where they will appear, while optical flow estimation provides information about how these objects will move. This paper presents a novel model that can simultaneously predict scene parsing and optical flow in unobserved future video frames. This is the first attempt to jointly predict these two tasks, and our model shows significantly better results compared to established baselines and individual prediction models on the Cityscapes dataset. Furthermore, our model can predict the steering angle of vehicles, demonstrating its ability to learn latent representations of scene dynamics.",1
"We address unsupervised optical flow estimation for ego-centric motion. We argue that optical flow can be cast as a geometrical warping between two successive video frames and devise a deep architecture to estimate such transformation in two stages. First, a dense pixel-level flow is computed with a geometric prior imposing strong spatial constraints. Such prior is typical of driving scenes, where the point of view is coherent with the vehicle motion. We show how such global transformation can be approximated with an homography and how spatial transformer layers can be employed to compute the flow field implied by such transformation. The second stage then refines the prediction feeding a second deeper network. A final reconstruction loss compares the warping of frame X(t) with the subsequent frame X(t+1) and guides both estimates. The model, which we named TransFlow, performs favorably compared to other unsupervised algorithms, and shows better generalization compared to supervised methods with a 3x reduction in error on unseen data.",0
"Our focus is on unsupervised optical flow estimation for self-motion. We propose that optical flow can be seen as a geometrical transformation between two consecutive video frames. To achieve this, we have developed a two-stage deep architecture. Initially, a dense flow at the pixel level is computed with a geometric constraint that is typically seen in driving scenes where the vehicle motion is consistent with the point of view. We demonstrate how this global transformation can be approximated using an homography and how spatial transformer layers can be utilized to calculate the flow field that this transformation implies. In the second stage, a deeper network refines the prediction. A final reconstruction loss is employed to compare the warping of frame X(t) with the subsequent frame X(t+1) and guide both estimates. Our model, TransFlow, outperforms other unsupervised algorithms and exhibits superior generalization compared to supervised methods, with a 3x reduction in error on unseen data.",1
"This paper describes a fully spike-based neural network for optical flow estimation from Dynamic Vision Sensor data. A low power embedded implementation of the method which combines the Asynchronous Time-based Image Sensor with IBM's TrueNorth Neurosynaptic System is presented. The sensor generates spikes with sub-millisecond resolution in response to scene illumination changes. These spike are processed by a spiking neural network running on TrueNorth with a 1 millisecond resolution to accurately determine the order and time difference of spikes from neighboring pixels, and therefore infer the velocity. The spiking neural network is a variant of the Barlow Levick method for optical flow estimation. The system is evaluated on two recordings for which ground truth motion is available, and achieves an Average Endpoint Error of 11% at an estimated power budget of under 80mW for the sensor and computation.",0
"In this article, a spike-based neural network is discussed for estimating optical flow from data gathered by a Dynamic Vision Sensor. The implementation of the method is designed to be low power and embedded, utilizing the Asynchronous Time-based Image Sensor in conjunction with IBM's TrueNorth Neurosynaptic System. The sensor generates spikes in response to changes in illumination with sub-millisecond resolution. These spikes are then analyzed by a spiking neural network running on TrueNorth with 1 millisecond resolution, which uses the Barlow Levick method for optical flow estimation to accurately determine the order and time difference of spikes from neighboring pixels in order to infer velocity. The system's performance was evaluated on two recordings with known motion, achieving an Average Endpoint Error of 11% while using an estimated power budget of under 80mW for both the sensor and computation.",1
"In the context of online Robust Principle Component Analysis (RPCA) for the video foreground-background separation, we propose a compressive online RPCA with optical flow that separates recursively a sequence of frames into sparse (foreground) and low-rank (background) components. Our method considers a small set of measurements taken per data vector (frame), which is different from conventional batch RPCA, processing all the data directly. The proposed method also incorporates multiple prior information, namely previous foreground and background frames, to improve the separation and then updates the prior information for the next frame. Moreover, the foreground prior frames are improved by estimating motions between the previous foreground frames using optical flow and compensating the motions to achieve higher quality foreground prior. The proposed method is applied to online video foreground and background separation from compressive measurements. The visual and quantitative results show that our method outperforms the existing methods.",0
"We suggest an approach for online Robust Principle Component Analysis (RPCA) that utilizes compressive methods and optical flow to distinguish between foreground and background in a sequence of frames. Our method differs from traditional batch RPCA, which processes all data at once, by only considering a limited number of measurements per data vector. Multiple prior information is also incorporated, including previous foreground and background frames, to enhance separation and update the information for the subsequent frame. Additionally, we use optical flow to estimate motion between previous foreground frames and improve the foreground prior frames by compensating for the motion. Our method is applied to online video foreground and background separation using compressive measurements, and our visual and quantitative results demonstrate superior performance compared to existing methods.",1
"Given two consecutive frames from a pair of stereo cameras, 3D scene flow methods simultaneously estimate the 3D geometry and motion of the observed scene. Many existing approaches use superpixels for regularization, but may predict inconsistent shapes and motions inside rigidly moving objects. We instead assume that scenes consist of foreground objects rigidly moving in front of a static background, and use semantic cues to produce pixel-accurate scene flow estimates. Our cascaded classification framework accurately models 3D scenes by iteratively refining semantic segmentation masks, stereo correspondences, 3D rigid motion estimates, and optical flow fields. We evaluate our method on the challenging KITTI autonomous driving benchmark, and show that accounting for the motion of segmented vehicles leads to state-of-the-art performance.",0
"3D scene flow methods estimate the 3D geometry and motion of a scene using two consecutive frames from a pair of stereo cameras. However, some existing approaches may produce inconsistent shapes and motions within rigidly moving objects when using superpixels for regularization. To address this, we assume that the scene is composed of foreground objects moving rigidly in front of a static background and use semantic cues for pixel-accurate scene flow estimates. Our method involves a cascaded classification framework that iteratively refines semantic segmentation masks, stereo correspondences, 3D rigid motion estimates, and optical flow fields to accurately model 3D scenes. We tested our approach on the KITTI autonomous driving benchmark and found that our method, which accounts for the motion of segmented vehicles, achieves state-of-the-art performance.",1
"With the goal of making high-resolution forecasts of regional rainfall, precipitation nowcasting has become an important and fundamental technology underlying various public services ranging from rainstorm warnings to flight safety. Recently, the Convolutional LSTM (ConvLSTM) model has been shown to outperform traditional optical flow based methods for precipitation nowcasting, suggesting that deep learning models have a huge potential for solving the problem. However, the convolutional recurrence structure in ConvLSTM-based models is location-invariant while natural motion and transformation (e.g., rotation) are location-variant in general. Furthermore, since deep-learning-based precipitation nowcasting is a newly emerging area, clear evaluation protocols have not yet been established. To address these problems, we propose both a new model and a benchmark for precipitation nowcasting. Specifically, we go beyond ConvLSTM and propose the Trajectory GRU (TrajGRU) model that can actively learn the location-variant structure for recurrent connections. Besides, we provide a benchmark that includes a real-world large-scale dataset from the Hong Kong Observatory, a new training loss, and a comprehensive evaluation protocol to facilitate future research and gauge the state of the art.",0
"Precipitation nowcasting is a vital technology for regional rainfall forecasting, which serves as the foundation for various public services like flight safety and rainstorm warnings. Recent studies have found that Convolutional LSTM (ConvLSTM) models have outperformed traditional optical flow based methods, highlighting the potential of deep learning models in addressing this issue. However, ConvLSTM-based models have a location-invariant convolutional recurrence structure, which contrasts with the location-variant nature of natural motion and transformation (e.g., rotation). Additionally, the lack of clear evaluation protocols has been a challenge for this new area of deep-learning-based precipitation nowcasting. To tackle these problems, we propose a new model, the Trajectory GRU (TrajGRU), which can actively learn the location-variant structure for recurrent connections. We also introduce a benchmark that includes a large-scale dataset from the Hong Kong Observatory, a new training loss, and an extensive evaluation protocol to facilitate future research and assess the state of the art.",1
"Convolutional neural networks (CNNs) have been widely used over many areas in compute vision. Especially in classification. Recently, FlowNet and several works on opti- cal estimation using CNNs shows the potential ability of CNNs in doing per-pixel regression. We proposed several CNNs network architectures that can estimate optical flow, and fully unveiled the intrinsic different between these structures.",0
"CNNs have found extensive applications in computer vision, particularly in classification. In recent times, the use of CNNs in per-pixel regression has been demonstrated through works such as FlowNet and other optical estimation studies. Our proposal includes various CNN network architectures which can estimate optical flow and thoroughly highlight the inherent differences between these structures.",1
"Semantic video segmentation is challenging due to the sheer amount of data that needs to be processed and labeled in order to construct accurate models. In this paper we present a deep, end-to-end trainable methodology to video segmentation that is capable of leveraging information present in unlabeled data in order to improve semantic estimates. Our model combines a convolutional architecture and a spatio-temporal transformer recurrent layer that are able to temporally propagate labeling information by means of optical flow, adaptively gated based on its locally estimated uncertainty. The flow, the recognition and the gated temporal propagation modules can be trained jointly, end-to-end. The temporal, gated recurrent flow propagation component of our model can be plugged into any static semantic segmentation architecture and turn it into a weakly supervised video processing one. Our extensive experiments in the challenging CityScapes and Camvid datasets, and based on multiple deep architectures, indicate that the resulting model can leverage unlabeled temporal frames, next to a labeled one, in order to improve both the video segmentation accuracy and the consistency of its temporal labeling, at no additional annotation cost and with little extra computation.",0
"The amount of data required to accurately model semantic video segmentation makes it a difficult task. However, our paper introduces a trainable methodology for video segmentation that uses unlabeled data to improve semantic estimates. Our model combines a convolutional architecture and a spatio-temporal transformer recurrent layer that propagates labeling information through optical flow, with adaptive gating based on locally estimated uncertainty. Our flow, recognition, and gated temporal propagation modules can be jointly trained end-to-end. Additionally, the temporal, gated recurrent flow propagation component of our model can be added to any static semantic segmentation architecture to create a weakly supervised video processing model. Our experiments on CityScapes and Camvid datasets, using multiple deep architectures, show that our model can improve video segmentation accuracy and temporal labeling consistency with no additional annotation cost and minimal extra computation.",1
"We present a benchmark suite for visual perception. The benchmark is based on more than 250K high-resolution video frames, all annotated with ground-truth data for both low-level and high-level vision tasks, including optical flow, semantic instance segmentation, object detection and tracking, object-level 3D scene layout, and visual odometry. Ground-truth data for all tasks is available for every frame. The data was collected while driving, riding, and walking a total of 184 kilometers in diverse ambient conditions in a realistic virtual world. To create the benchmark, we have developed a new approach to collecting ground-truth data from simulated worlds without access to their source code or content. We conduct statistical analyses that show that the composition of the scenes in the benchmark closely matches the composition of corresponding physical environments. The realism of the collected data is further validated via perceptual experiments. We analyze the performance of state-of-the-art methods for multiple tasks, providing reference baselines and highlighting challenges for future research. The supplementary video can be viewed at https://youtu.be/T9OybWv923Y",0
"We have created a benchmark suite for visual perception that includes over 250,000 high-quality video frames. Each frame has been annotated with ground-truth data for low-level and high-level vision tasks, such as optical flow, semantic instance segmentation, object detection and tracking, object-level 3D scene layout, and visual odometry. The data was collected in a virtual world that was designed to mimic real-world environments, covering a distance of 184 kilometers while driving, riding, and walking. Our approach to collecting ground-truth data from simulated worlds does not require access to their source code or content. Statistical analysis shows that the scenes in the benchmark closely match those of corresponding physical environments, and the collected data has been validated through perceptual experiments. We provide reference baselines and highlight challenges for future research by analyzing the performance of state-of-the-art methods for multiple tasks. A supplementary video can be viewed at https://youtu.be/T9OybWv923Y.",1
"This paper proposes an end-to-end trainable network, SegFlow, for simultaneously predicting pixel-wise object segmentation and optical flow in videos. The proposed SegFlow has two branches where useful information of object segmentation and optical flow is propagated bidirectionally in a unified framework. The segmentation branch is based on a fully convolutional network, which has been proved effective in image segmentation task, and the optical flow branch takes advantage of the FlowNet model. The unified framework is trained iteratively offline to learn a generic notion, and fine-tuned online for specific objects. Extensive experiments on both the video object segmentation and optical flow datasets demonstrate that introducing optical flow improves the performance of segmentation and vice versa, against the state-of-the-art algorithms.",0
"In this paper, SegFlow, a network that can predict pixel-wise object segmentation and optical flow in videos simultaneously, is proposed. SegFlow has two branches that allow information about object segmentation and optical flow to be propagated bidirectionally within a unified framework. The segmentation branch uses a fully convolutional network that has proven to be effective in image segmentation, whereas the optical flow branch utilizes the FlowNet model. The unified framework is trained offline iteratively to learn a generic notion and fine-tuned online for specific objects. Through extensive experiments on video object segmentation and optical flow datasets, it is demonstrated that introducing optical flow enhances segmentation performance and vice versa, outperforming state-of-the-art algorithms.",1
"Today's general-purpose deep convolutional neural networks (CNN) for image classification and object detection are trained offline on large static datasets. Some applications, however, will require training in real-time on live video streams with a human-in-the-loop. We refer to this class of problem as Time-ordered Online Training (ToOT) - these problems will require a consideration of not only the quantity of incoming training data, but the human effort required to tag and use it. In this paper, we define training benefit as a metric to measure the effectiveness of a sequence in using each user interaction. We demonstrate and evaluate a system tailored to performing ToOT in the field, capable of training an image classifier on a live video stream through minimal input from a human operator. We show that by exploiting the time-ordered nature of the video stream through optical flow-based object tracking, we can increase the effectiveness of human actions by about 8 times.",0
"Current deep convolutional neural networks (CNN) designed for image classification and object detection are typically trained offline on large, static datasets. However, certain applications may require real-time training on live video streams with human involvement. This category of problems is referred to as Time-ordered Online Training (ToOT), which requires consideration not only of the amount of incoming training data but also the human effort required to tag and utilize it. This paper introduces training benefit as a metric for measuring the efficiency of a sequence in utilizing each user interaction. We present and evaluate a ToOT system tailored for use in the field, which can train an image classifier on a live video stream with minimal human input. By utilizing the time-ordered nature of the video stream through optical flow-based object tracking, we can increase the effectiveness of human actions by approximately 8 times.",1
"For many movement disorders, such as Parkinson's disease and ataxia, disease progression is visually assessed by a clinician using a numerical disease rating scale. These tests are subjective, time-consuming, and must be administered by a professional. This can be problematic where specialists are not available, or when a patient is not consistently evaluated by the same clinician. We present an automated method for quantifying the severity of motion impairment in patients with ataxia, using only video recordings. We consider videos of the finger-to-nose test, a common movement task used as part of the assessment of ataxia progression during the course of routine clinical checkups.   Our method uses neural network-based pose estimation and optical flow techniques to track the motion of the patient's hand in a video recording. We extract features that describe qualities of the motion such as speed and variation in performance. Using labels provided by an expert clinician, we train a supervised learning model that predicts severity according to the Brief Ataxia Rating Scale (BARS). The performance of our system is comparable to that of a group of ataxia specialists in terms of mean error and correlation, and our system's predictions were consistently within the range of inter-rater variability. This work demonstrates the feasibility of using computer vision and machine learning to produce consistent and clinically useful measures of motor impairment.",0
"A clinician typically visually assesses the progression of movement disorders such as Parkinson's disease and ataxia using a numerical rating scale, which is subjective, time-consuming, and requires a professional. This can be challenging in areas without specialists or when a patient is evaluated by different clinicians. Our approach involves an automated method that only uses video recordings to quantify motion impairment severity in patients with ataxia. We use neural networks and optical flow techniques to track hand motion during a finger-to-nose test, a common movement task in clinical checkups for ataxia. We extract motion features, including speed and performance variation, and train a supervised learning model to predict severity based on an expert clinician's ratings. Our system's performance is similar to that of a group of ataxia specialists, and its predictions are consistently within inter-rater variability. This study demonstrates that computer vision and machine learning can provide consistent and clinically valuable measures of motor impairment.",1
"In this paper we address the abnormality detection problem in crowded scenes. We propose to use Generative Adversarial Nets (GANs), which are trained using normal frames and corresponding optical-flow images in order to learn an internal representation of the scene normality. Since our GANs are trained with only normal data, they are not able to generate abnormal events. At testing time the real data are compared with both the appearance and the motion representations reconstructed by our GANs and abnormal areas are detected by computing local differences. Experimental results on challenging abnormality detection datasets show the superiority of the proposed method compared to the state of the art in both frame-level and pixel-level abnormality detection tasks.",0
"This paper tackles the issue of detecting abnormalities in crowded scenes. Our approach involves utilizing Generative Adversarial Nets (GANs) that are trained with normal frames and corresponding optical-flow images in order to develop an internal representation of scene normality. Our GANs are trained exclusively with normal data, meaning they cannot generate abnormal events. During testing, we compare real data with appearance and motion representations reconstructed by our GANs, and detect abnormal areas by calculating local differences. Results from experiments on challenging abnormality detection datasets demonstrate that our method outperforms current state-of-the-art approaches in both frame-level and pixel-level abnormality detection tasks.",1
"This paper proposes a two-stream flow-guided convolutional attention networks for action recognition in videos. The central idea is that optical flows, when properly compensated for the camera motion, can be used to guide attention to the human foreground. We thus develop cross-link layers from the temporal network (trained on flows) to the spatial network (trained on RGB frames). These cross-link layers guide the spatial-stream to pay more attention to the human foreground areas and be less affected by background clutter. We obtain promising performances with our approach on the UCF101, HMDB51 and Hollywood2 datasets.",0
"In this paper, a novel approach for recognizing actions in videos is proposed, which involves the use of a two-stream flow-guided convolutional attention network. The key concept is that optical flows, when accurately adjusted to account for camera motion, can be utilized to direct attention to the human foreground. To achieve this, cross-link layers were introduced from the temporal network (trained on flows) to the spatial network (trained on RGB frames). These cross-link layers guide the spatial-stream to focus more on the human foreground regions and minimize the impact of background clutter. The proposed method demonstrated promising results on the UCF101, HMDB51, and Hollywood2 datasets.",1
"Activity recognition from long unstructured egocentric photo-streams has several applications in assistive technology such as health monitoring and frailty detection, just to name a few. However, one of its main technical challenges is to deal with the low frame rate of wearable photo-cameras, which causes abrupt appearance changes between consecutive frames. In consequence, important discriminatory low-level features from motion such as optical flow cannot be estimated. In this paper, we present a batch-driven approach for training a deep learning architecture that strongly rely on Long short-term units to tackle this problem. We propose two different implementations of the same approach that process a photo-stream sequence using batches of fixed size with the goal of capturing the temporal evolution of high-level features. The main difference between these implementations is that one explicitly models consecutive batches by overlapping them. Experimental results over a public dataset acquired by three users demonstrate the validity of the proposed architectures to exploit the temporal evolution of convolutional features over time without relying on event boundaries.",0
"Several applications in assistive technology, such as health monitoring and frailty detection, can benefit from activity recognition in long and unstructured egocentric photo-streams. However, the primary technical obstacle faced is the low frame rate of wearable photo-cameras, which results in sudden appearance changes between consecutive frames. As a result, essential motion-related low-level features like optical flow cannot be calculated. This paper introduces a batch-driven approach that utilizes Long short-term units, which rely on a deep learning architecture to address this issue. Two different implementations of the same approach are proposed, both of which process a photo-stream sequence using fixed-size batches to capture the evolution of high-level features over time. The main difference between the two is that one explicitly models consecutive batches by overlapping them. The proposed architectures' validity is demonstrated through experimental results on a publicly available dataset acquired by three users, which shows their ability to exploit the temporal evolution of convolutional features without relying on event boundaries.",1
"We propose Stereo Direct Sparse Odometry (Stereo DSO) as a novel method for highly accurate real-time visual odometry estimation of large-scale environments from stereo cameras. It jointly optimizes for all the model parameters within the active window, including the intrinsic/extrinsic camera parameters of all keyframes and the depth values of all selected pixels. In particular, we propose a novel approach to integrate constraints from static stereo into the bundle adjustment pipeline of temporal multi-view stereo. Real-time optimization is realized by sampling pixels uniformly from image regions with sufficient intensity gradient. Fixed-baseline stereo resolves scale drift. It also reduces the sensitivities to large optical flow and to rolling shutter effect which are known shortcomings of direct image alignment methods. Quantitative evaluation demonstrates that the proposed Stereo DSO outperforms existing state-of-the-art visual odometry methods both in terms of tracking accuracy and robustness. Moreover, our method delivers a more precise metric 3D reconstruction than previous dense/semi-dense direct approaches while providing a higher reconstruction density than feature-based methods.",0
"Our proposal is Stereo Direct Sparse Odometry (Stereo DSO), a new technique for accurately estimating visual odometry in real-time for large-scale environments using stereo cameras. This method optimizes all model parameters within the active window, including intrinsic/extrinsic camera parameters of keyframes and depth values of selected pixels. We introduce a unique approach that integrates static stereo constraints into the bundle adjustment pipeline of temporal multi-view stereo. Real-time optimization is achieved by uniformly sampling pixels from image regions with sufficient intensity gradient. Fixed-baseline stereo addresses scale drift, while also reducing sensitivities to large optical flow and rolling shutter effects, which are commonly found in direct image alignment methods. Quantitative evaluation demonstrates that Stereo DSO outperforms existing state-of-the-art visual odometry methods in terms of tracking accuracy and robustness. Additionally, our method produces a more precise metric 3D reconstruction than previous dense/semi-dense direct approaches, while providing a higher reconstruction density than feature-based methods.",1
"Human action recognition involves the characterization of human actions through the automated analysis of video data and is integral in the development of smart computer vision systems. However, several challenges like dynamic backgrounds, camera stabilization, complex actions, occlusions etc. make action recognition in a real time and robust fashion difficult. Several complex approaches exist but are computationally intensive. This paper presents a novel approach of using a combination of good features along with iterative optical flow algorithm to compute feature vectors which are classified using a multilayer perceptron (MLP) network. The use of multiple features for motion descriptors enhances the quality of tracking. Resilient backpropagation algorithm is used for training the feedforward neural network reducing the learning time. The overall system accuracy is improved by optimizing the various parameters of the multilayer perceptron network.",0
"The automated analysis of video data is crucial for characterizing human actions in human action recognition, and it is essential for developing intelligent computer vision systems. However, real-time and robust action recognition is challenging due to dynamic backgrounds, occlusions, camera stabilization, and complex actions. Although there are complex approaches, they are computationally intensive. This study proposes a new method that combines good features with an iterative optical flow algorithm to compute feature vectors. These vectors are then classified using a multilayer perceptron network. Using multiple features for motion descriptors improves tracking quality. The feedforward neural network is trained using the resilient backpropagation algorithm, which reduces learning time. By optimizing the multilayer perceptron network's various parameters, the overall system accuracy is improved.",1
"The temporal component of videos provides an important clue for activity recognition, as a number of activities can be reliably recognized based on the motion information. In view of that, this work proposes a novel temporal stream for two-stream convolutional networks based on images computed from the optical flow magnitude and orientation, named Magnitude-Orientation Stream (MOS), to learn the motion in a better and richer manner. Our method applies simple nonlinear transformations on the vertical and horizontal components of the optical flow to generate input images for the temporal stream. Experimental results, carried on two well-known datasets (HMDB51 and UCF101), demonstrate that using our proposed temporal stream as input to existing neural network architectures can improve their performance for activity recognition. Results demonstrate that our temporal stream provides complementary information able to improve the classical two-stream methods, indicating the suitability of our approach to be used as a temporal video representation.",0
"The motion information in videos is a crucial factor in recognizing activities. To enhance this aspect, a new method called Magnitude-Orientation Stream (MOS) has been proposed in this study. It is a temporal stream that uses images generated from the optical flow magnitude and orientation. The MOS method applies nonlinear transformations to the optical flow's vertical and horizontal components to produce input images for the temporal stream. The results of the experiments conducted on two well-known datasets (HMDB51 and UCF101) indicate that using the MOS method as input to existing neural network architectures can enhance their performance in recognizing activities. This demonstrates that our method provides complementary information to improve the classical two-stream methods and is suitable as a temporal video representation.",1
"Optical flow estimation remains challenging due to untextured areas, motion boundaries, occlusions, and more. Thus, the estimated flow is not equally reliable across the image. To that end, post-hoc confidence measures have been introduced to assess the per-pixel reliability of the flow. We overcome the artificial separation of optical flow and confidence estimation by introducing a method that jointly predicts optical flow and its underlying uncertainty. Starting from common energy-based formulations, we rely on the corresponding posterior distribution of the flow given the images. We derive a variational inference scheme based on mean field, which incorporates best practices from energy minimization. An uncertainty measure is obtained along the flow at every pixel as the (marginal) entropy of the variational distribution. We demonstrate the flexibility of our probabilistic approach by applying it to two different energies and on two benchmarks. We not only obtain flow results that are competitive with the underlying energy minimization approach, but also a reliable uncertainty measure that significantly outperforms existing post-hoc approaches.",0
"The task of estimating optical flow is difficult due to various factors such as untextured regions, motion boundaries, and occlusions. As a result, the accuracy of the estimated flow is not consistent throughout the image. To address this issue, confidence measures have been introduced to evaluate the reliability of flow estimation on a per-pixel basis. In this study, we propose a method that combines optical flow prediction and uncertainty estimation, rather than treating them as separate tasks. We utilize an energy-based formulation and the posterior distribution of flow given the input images to derive a variational inference scheme based on mean field. This approach yields an uncertainty measure for each pixel, calculated as the marginal entropy of the variational distribution. We demonstrate the effectiveness of our probabilistic method on two benchmarks, showing that it produces competitive flow results compared to energy minimization approaches, while also providing a more reliable uncertainty measure than existing post-hoc approaches.",1
"We introduce the concept of ""dynamic image"", a novel compact representation of videos useful for video analysis, particularly in combination with convolutional neural networks (CNNs). A dynamic image encodes temporal data such as RGB or optical flow videos by using the concept of `rank pooling'. The idea is to learn a ranking machine that captures the temporal evolution of the data and to use the parameters of the latter as a representation. When a linear ranking machine is used, the resulting representation is in the form of an image, which we call dynamic because it summarizes the video dynamics in addition of appearance. This is a powerful idea because it allows to convert any video to an image so that existing CNN models pre-trained for the analysis of still images can be immediately extended to videos. We also present an efficient and effective approximate rank pooling operator, accelerating standard rank pooling algorithms by orders of magnitude, and formulate that as a CNN layer. This new layer allows generalizing dynamic images to dynamic feature maps. We demonstrate the power of the new representations on standard benchmarks in action recognition achieving state-of-the-art performance.",0
"We propose a new method called ""dynamic image"" for analyzing videos, especially in combination with convolutional neural networks (CNNs). A dynamic image is a concise representation of temporal data such as RGB or optical flow videos, achieved through ""rank pooling"". This involves training a ranking machine to capture the temporal evolution of the data and using its parameters as a representation. When a linear ranking machine is used, the resulting representation takes the form of an image, which we call dynamic as it summarizes both appearance and video dynamics. This is a game-changing idea as it enables conversion of any video into an image, allowing existing CNN models pre-trained for still image analysis to be extended to videos. We also present an efficient and effective approximate rank pooling operator, which accelerates standard rank pooling algorithms by orders of magnitude and can be formulated as a CNN layer. This new layer facilitates generalization of dynamic images to dynamic feature maps. Our experiments on standard benchmarks in action recognition show that these new representations achieve state-of-the-art performance.",1
"Optical flow estimation is one of the most studied problems in computer vision, yet recent benchmark datasets continue to reveal problem areas of today's approaches. Occlusions have remained one of the key challenges. In this paper, we propose a symmetric optical flow method to address the well-known chicken-and-egg relation between optical flow and occlusions. In contrast to many state-of-the-art methods that consider occlusions as outliers, possibly filtered out during post-processing, we highlight the importance of joint occlusion reasoning in the optimization and show how to utilize occlusion as an important cue for estimating optical flow. The key feature of our model is to fully exploit the symmetry properties that characterize optical flow and occlusions in the two consecutive images. Specifically through utilizing forward-backward consistency and occlusion-disocclusion symmetry in the energy, our model jointly estimates optical flow in both forward and backward direction, as well as consistent occlusion maps in both views. We demonstrate significant performance benefits on standard benchmarks, especially from the occlusion-disocclusion symmetry. On the challenging KITTI dataset we report the most accurate two-frame results to date.",0
"Although optical flow estimation is a well-researched issue in computer vision, recent benchmark datasets continue to expose problem areas in current methods. These problems are exemplified by occlusions, which remain a significant challenge. This study presents a symmetric optical flow method that addresses the chicken-and-egg relationship between optical flow and occlusions. Unlike contemporary approaches that filter out occlusions during post-processing, our model emphasizes the importance of joint occlusion reasoning in the optimization and proposes using occlusion as a key cue for estimating optical flow. Our model fully exploits the symmetry properties of optical flow and occlusions in two consecutive images by utilizing forward-backward consistency and occlusion-disocclusion symmetry in the energy. As a result, our model estimates optical flow in both the forward and backward directions, as well as consistent occlusion maps in both views. We show significant performance improvements on standard benchmarks, particularly due to the occlusion-disocclusion symmetry. On the challenging KITTI dataset, we report the most accurate two-frame results to date.",1
"We introduce an approach to integrate segmentation information within a convolutional neural network (CNN). This counter-acts the tendency of CNNs to smooth information across regions and increases their spatial precision. To obtain segmentation information, we set up a CNN to provide an embedding space where region co-membership can be estimated based on Euclidean distance. We use these embeddings to compute a local attention mask relative to every neuron position. We incorporate such masks in CNNs and replace the convolution operation with a ""segmentation-aware"" variant that allows a neuron to selectively attend to inputs coming from its own region. We call the resulting network a segmentation-aware CNN because it adapts its filters at each image point according to local segmentation cues. We demonstrate the merit of our method on two widely different dense prediction tasks, that involve classification (semantic segmentation) and regression (optical flow). Our results show that in semantic segmentation we can match the performance of DenseCRFs while being faster and simpler, and in optical flow we obtain clearly sharper responses than networks that do not use local attention masks. In both cases, segmentation-aware convolution yields systematic improvements over strong baselines. Source code for this work is available online at http://cs.cmu.edu/~aharley/segaware.",0
"Our approach involves integrating segmentation information into a convolutional neural network (CNN) to increase spatial precision and counteract the tendency of CNNs to smooth information across regions. To acquire segmentation information, we establish a CNN that provides an embedding space where Euclidean distance is used to estimate region co-membership. Using these embeddings, we compute a local attention mask for each neuron position, which is incorporated into the CNN by replacing the convolution operation with a ""segmentation-aware"" variant. This allows each neuron to selectively attend to inputs from its own region, making the resulting network a segmentation-aware CNN that adapts its filters based on local segmentation cues. We demonstrate the effectiveness of our method on two dense prediction tasks, semantic segmentation and optical flow regression, achieving performance comparable to DenseCRFs in semantic segmentation while being faster and simpler, and obtaining sharper responses in optical flow than networks without local attention masks. Our segmentation-aware convolution method consistently improves upon strong baselines. The source code is available online at http://cs.cmu.edu/~aharley/segaware.",1
"Human actions captured in video sequences are three-dimensional signals characterizing visual appearance and motion dynamics. To learn action patterns, existing methods adopt Convolutional and/or Recurrent Neural Networks (CNNs and RNNs). CNN based methods are effective in learning spatial appearances, but are limited in modeling long-term motion dynamics. RNNs, especially Long Short-Term Memory (LSTM), are able to learn temporal motion dynamics. However, naively applying RNNs to video sequences in a convolutional manner implicitly assumes that motions in videos are stationary across different spatial locations. This assumption is valid for short-term motions but invalid when the duration of the motion is long.   In this work, we propose Lattice-LSTM (L2STM), which extends LSTM by learning independent hidden state transitions of memory cells for individual spatial locations. This method effectively enhances the ability to model dynamics across time and addresses the non-stationary issue of long-term motion dynamics without significantly increasing the model complexity. Additionally, we introduce a novel multi-modal training procedure for training our network. Unlike traditional two-stream architectures which use RGB and optical flow information as input, our two-stream model leverages both modalities to jointly train both input gates and both forget gates in the network rather than treating the two streams as separate entities with no information about the other. We apply this end-to-end system to benchmark datasets (UCF-101 and HMDB-51) of human action recognition. Experiments show that on both datasets, our proposed method outperforms all existing ones that are based on LSTM and/or CNNs of similar model complexities.",0
"Video sequences capture human actions as three-dimensional signals that represent visual appearance and motion dynamics. Existing methods for learning action patterns utilize Convolutional and/or Recurrent Neural Networks (CNNs and RNNs). CNNs are effective in learning spatial appearances but cannot model long-term motion dynamics. On the other hand, RNNs, especially Long Short-Term Memory (LSTM), can learn temporal motion dynamics. However, applying RNNs to video sequences in a convolutional manner assumes stationary motions across different spatial locations, which is invalid for long-term motions. In this study, we propose Lattice-LSTM (L2STM), which enhances the ability to model dynamics across time by extending LSTM to learn independent hidden state transitions of memory cells for individual spatial locations. Our method addresses the non-stationary issue of long-term motion dynamics without increasing model complexity significantly. We also introduce a novel multi-modal training procedure for our two-stream model that leverages both RGB and optical flow information to jointly train input and forget gates. We apply our end-to-end system to benchmark datasets of human action recognition (UCF-101 and HMDB-51). The experiments show that our proposed method outperforms all existing ones that are based on LSTM and/or CNNs of similar model complexities on both datasets.",1
"Video deblurring is a challenging problem as the blur is complex and usually caused by the combination of camera shakes, object motions, and depth variations. Optical flow can be used for kernel estimation since it predicts motion trajectories. However, the estimates are often inaccurate in complex scenes at object boundaries, which are crucial in kernel estimation. In this paper, we exploit semantic segmentation in each blurry frame to understand the scene contents and use different motion models for image regions to guide optical flow estimation. While existing pixel-wise blur models assume that the blur kernel is the same as optical flow during the exposure time, this assumption does not hold when the motion blur trajectory at a pixel is different from the estimated linear optical flow. We analyze the relationship between motion blur trajectory and optical flow, and present a novel pixel-wise non-linear kernel model to account for motion blur. The proposed blur model is based on the non-linear optical flow, which describes complex motion blur more effectively. Extensive experiments on challenging blurry videos demonstrate the proposed algorithm performs favorably against the state-of-the-art methods.",0
"The problem of video deblurring is difficult due to the complex nature of the blur, which is often the result of camera movements, object motions, and changes in depth. While optical flow can be used to estimate the blur kernel by predicting motion trajectories, it is often inaccurate in complex scenes at object boundaries, which are important for kernel estimation. This paper proposes the use of semantic segmentation to understand the contents of each blurry frame and the use of different motion models for image regions to guide optical flow estimation. Additionally, the paper presents a novel pixel-wise non-linear kernel model that accounts for the relationship between motion blur trajectory and optical flow. This model is based on non-linear optical flow and is more effective in describing complex motion blur. The proposed algorithm is evaluated on challenging blurry videos and performs favorably against state-of-the-art methods.",1
"In this work, we propose a technique to convert CNN models for semantic segmentation of static images into CNNs for video data. We describe a warping method that can be used to augment existing architectures with very little extra computational cost. This module is called NetWarp and we demonstrate its use for a range of network architectures. The main design principle is to use optical flow of adjacent frames for warping internal network representations across time. A key insight of this work is that fast optical flow methods can be combined with many different CNN architectures for improved performance and end-to-end training. Experiments validate that the proposed approach incurs only little extra computational cost, while improving performance, when video streams are available. We achieve new state-of-the-art results on the CamVid and Cityscapes benchmark datasets and show consistent improvements over different baseline networks. Our code and models will be available at http://segmentation.is.tue.mpg.de",0
"Our work introduces a technique to transform CNN models intended for semantic segmentation of still images into CNNs suitable for video data. We present a warping technique, known as NetWarp, that can be applied to existing architectures with minimal additional computational expense. Our approach employs optical flow from adjacent frames to warp internal network representations across time, and we demonstrate its efficacy with a variety of network architectures. We find that the combination of fast optical flow methods with different CNN architectures yields improved performance and end-to-end training. Our experiments show that our method enhances performance while incurring only slight extra computational cost when video streams are available. We achieve new state-of-the-art results on the CamVid and Cityscapes benchmark datasets, consistently outperforming various baseline networks. Interested parties may access our code and models at http://segmentation.is.tue.mpg.de.",1
"The ability to predict and therefore to anticipate the future is an important attribute of intelligence. It is also of utmost importance in real-time systems, e.g. in robotics or autonomous driving, which depend on visual scene understanding for decision making. While prediction of the raw RGB pixel values in future video frames has been studied in previous work, here we introduce the novel task of predicting semantic segmentations of future frames. Given a sequence of video frames, our goal is to predict segmentation maps of not yet observed video frames that lie up to a second or further in the future. We develop an autoregressive convolutional neural network that learns to iteratively generate multiple frames. Our results on the Cityscapes dataset show that directly predicting future segmentations is substantially better than predicting and then segmenting future RGB frames. Prediction results up to half a second in the future are visually convincing and are much more accurate than those of a baseline based on warping semantic segmentations using optical flow.",0
"Intelligence is enhanced by the ability to predict and anticipate future events. This skill is especially critical in real-time systems such as robotics or autonomous driving, which rely on visual scene comprehension to make decisions. While prior research has explored the prediction of raw RGB pixel values in upcoming video frames, our study introduces a new task of forecasting semantic segmentations for future frames. Our objective is to generate segmentation maps for yet-to-be-seen video frames up to a second or more ahead, based on a sequence of video frames. To achieve this, we have developed an autoregressive convolutional neural network that can learn to generate multiple frames iteratively. Our experiments on the Cityscapes dataset demonstrate that directly predicting future segmentations is significantly more effective than predicting and then segmenting future RGB frames. We achieve visually convincing prediction results up to half a second into the future, which are more precise than those obtained using a baseline method that warps semantic segmentations using optical flow.",1
"We address the problem of synthesizing new video frames in an existing video, either in-between existing frames (interpolation), or subsequent to them (extrapolation). This problem is challenging because video appearance and motion can be highly complex. Traditional optical-flow-based solutions often fail where flow estimation is challenging, while newer neural-network-based methods that hallucinate pixel values directly often produce blurry results. We combine the advantages of these two methods by training a deep network that learns to synthesize video frames by flowing pixel values from existing ones, which we call deep voxel flow. Our method requires no human supervision, and any video can be used as training data by dropping, and then learning to predict, existing frames. The technique is efficient, and can be applied at any video resolution. We demonstrate that our method produces results that both quantitatively and qualitatively improve upon the state-of-the-art.",0
"Our focus is on the challenge of generating novel video frames within an existing video, either by interpolating between existing frames or extrapolating beyond them. This task is complicated by the complexity of video appearance and motion. While traditional optical-flow-based solutions may struggle with flow estimation, newer neural-network-based methods that directly hallucinate pixel values may yield blurry results. To overcome these limitations, we introduce the concept of deep voxel flow, which combines the strengths of both approaches. By training a deep network to flow pixel values from existing frames, our technique can synthesize video frames without any human supervision, using any video as training data. This approach is efficient and scalable to any video resolution, and our results demonstrate significant improvements over existing state-of-the-art techniques.",1
"Standard video frame interpolation methods first estimate optical flow between input frames and then synthesize an intermediate frame guided by motion. Recent approaches merge these two steps into a single convolution process by convolving input frames with spatially adaptive kernels that account for motion and re-sampling simultaneously. These methods require large kernels to handle large motion, which limits the number of pixels whose kernels can be estimated at once due to the large memory demand. To address this problem, this paper formulates frame interpolation as local separable convolution over input frames using pairs of 1D kernels. Compared to regular 2D kernels, the 1D kernels require significantly fewer parameters to be estimated. Our method develops a deep fully convolutional neural network that takes two input frames and estimates pairs of 1D kernels for all pixels simultaneously. Since our method is able to estimate kernels and synthesizes the whole video frame at once, it allows for the incorporation of perceptual loss to train the neural network to produce visually pleasing frames. This deep neural network is trained end-to-end using widely available video data without any human annotation. Both qualitative and quantitative experiments show that our method provides a practical solution to high-quality video frame interpolation.",0
"Traditional methods for video frame interpolation involve estimating optical flow between input frames and then using this information to create an intermediate frame based on motion. However, more recent approaches have combined these steps into a single convolution process using adaptive kernels that account for motion and re-sampling simultaneously. Unfortunately, these methods require large kernels to handle large motion, making it difficult to estimate kernels for all pixels at once due to memory limitations. To overcome this challenge, we propose a new method that formulates frame interpolation as local separable convolution using pairs of 1D kernels. Because 1D kernels require fewer parameters than 2D kernels, our approach can estimate kernels for all pixels simultaneously. We have developed a deep fully convolutional neural network that can take two input frames and estimate pairs of 1D kernels to create visually pleasing frames. Our method can be trained end-to-end using widely available video data without any human annotation. Both qualitative and quantitative experiments demonstrate that our approach provides a practical solution for high-quality video frame interpolation.",1
"Future frame prediction in videos is a promising avenue for unsupervised video representation learning. Video frames are naturally generated by the inherent pixel flows from preceding frames based on the appearance and motion dynamics in the video. However, existing methods focus on directly hallucinating pixel values, resulting in blurry predictions. In this paper, we develop a dual motion Generative Adversarial Net (GAN) architecture, which learns to explicitly enforce future-frame predictions to be consistent with the pixel-wise flows in the video through a dual-learning mechanism. The primal future-frame prediction and dual future-flow prediction form a closed loop, generating informative feedback signals to each other for better video prediction. To make both synthesized future frames and flows indistinguishable from reality, a dual adversarial training method is proposed to ensure that the future-flow prediction is able to help infer realistic future-frames, while the future-frame prediction in turn leads to realistic optical flows. Our dual motion GAN also handles natural motion uncertainty in different pixel locations with a new probabilistic motion encoder, which is based on variational autoencoders. Extensive experiments demonstrate that the proposed dual motion GAN significantly outperforms state-of-the-art approaches on synthesizing new video frames and predicting future flows. Our model generalizes well across diverse visual scenes and shows superiority in unsupervised video representation learning.",0
"The prediction of future frames in videos shows great promise for unsupervised video representation learning. Video frames are produced naturally through the movement and appearance dynamics of previous frames. However, current methods focus on generating pixel values directly, resulting in unclear predictions. This study introduces a dual motion Generative Adversarial Net (GAN) architecture, which enforces the consistency of future-frame predictions with pixel-wise flows using a dual-learning mechanism. The primal future-frame prediction and dual future-flow prediction work together to generate feedback signals for better video prediction. To ensure that both synthesized future frames and flows are realistic, a dual adversarial training method is proposed. Our dual motion GAN also handles motion uncertainty with a new probabilistic motion encoder. Extensive experiments show that our approach outperforms state-of-the-art methods in predicting future flows and synthesizing new video frames. Our model also generalizes well across diverse visual scenes and demonstrates superiority in unsupervised video representation learning.",1
"Real-time occlusion handling is a major problem in outdoor mixed reality system because it requires great computational cost mainly due to the complexity of the scene. Using only segmentation, it is difficult to accurately render a virtual object occluded by complex objects such as trees, bushes etc. In this paper, we propose a novel occlusion handling method for real-time, outdoor, and omni-directional mixed reality system using only the information from a monocular image sequence. We first present a semantic segmentation scheme for predicting the amount of visibility for different type of objects in the scene. We also simultaneously calculate a foreground probability map using depth estimation derived from optical flow. Finally, we combine the segmentation result and the probability map to render the computer generated object and the real scene using a visibility-based rendering method. Our results show great improvement in handling occlusions compared to existing blending based methods.",0
"Real-time occlusion management in outdoor mixed reality systems is a significant challenge due to the high computational cost needed, primarily caused by the intricate nature of the setting. It is problematic to render a virtual object accurately when obstructed by complex objects like trees and bushes with only segmentation. This paper introduces a new method for managing occlusion in real-time, outdoor, and omni-directional mixed reality systems using information from a monocular image sequence. We begin by proposing a semantic segmentation scheme that predicts the amount of visibility for distinct objects in the scene. We also concurrently calculate a foreground probability map using depth estimation derived from optical flow. Finally, we combine the segmentation result and probability map using a visibility-based rendering approach to render the computer-generated object and the real scene. Our results indicate significant improvement in occlusion handling compared to existing blending-based methods.",1
"Shot boundary detection (SBD) is an important pre-processing step for video manipulation. Here, each segment of frames is classified as either sharp, gradual or no transition. Current SBD techniques analyze hand-crafted features and attempt to optimize both detection accuracy and processing speed. However, the heavy computations of optical flow prevents this. To achieve this aim, we present an SBD technique based on spatio-temporal Convolutional Neural Networks (CNN). Since current datasets are not large enough to train an accurate SBD CNN, we present a new dataset containing more than 3.5 million frames of sharp and gradual transitions. The transitions are generated synthetically using image compositing models. Our dataset contain additional 70,000 frames of important hard-negative no transitions. We perform the largest evaluation to date for one SBD algorithm, on real and synthetic data, containing more than 4.85 million frames. In comparison to the state of the art, we outperform dissolve gradual detection, generate competitive performance for sharp detections and produce significant improvement in wipes. In addition, we are up to 11 times faster than the state of the art.",0
"Detecting shot boundaries (SBD) is a crucial initial step in video editing. Each frame segment is classified as either having a sharp, gradual, or no transition. Existing SBD methods use hand-crafted features to optimize detection accuracy and processing speed, but the calculations involved in optical flow make this difficult. To address this challenge, we introduce an SBD technique that relies on spatio-temporal Convolutional Neural Networks (CNN). However, current datasets are too small to train an accurate SBD CNN, so we present a new dataset that consists of over 3.5 million frames of sharp and gradual transitions. We generate transitions synthetically using image compositing models and include an additional 70,000 frames of important hard-negative no transitions. Our evaluation is the largest to date for an SBD algorithm, with real and synthetic data comprising more than 4.85 million frames. In comparison to the state of the art, our model performs better in detecting dissolve gradual transitions, achieves competitive performance in detecting sharp transitions, and significantly improves wipe detection. Furthermore, our model is up to 11 times faster than the current state of the art.",1
"There is an inherent need for autonomous cars, drones, and other robots to have a notion of how their environment behaves and to anticipate changes in the near future. In this work, we focus on anticipating future appearance given the current frame of a video. Existing work focuses on either predicting the future appearance as the next frame of a video, or predicting future motion as optical flow or motion trajectories starting from a single video frame. This work stretches the ability of CNNs (Convolutional Neural Networks) to predict an anticipation of appearance at an arbitrarily given future time, not necessarily the next video frame. We condition our predicted future appearance on a continuous time variable that allows us to anticipate future frames at a given temporal distance, directly from the input video frame. We show that CNNs can learn an intrinsic representation of typical appearance changes over time and successfully generate realistic predictions at a deliberate time difference in the near future.",0
"Autonomous vehicles, drones, and other robots require the ability to understand their environment and predict future changes. Our focus is on predicting the future appearance of a video based on the current frame, rather than just the next frame or future motion. Using Convolutional Neural Networks, we can predict appearance at any future time by conditioning our results on a continuous time variable. By learning typical appearance changes over time, we can generate accurate predictions at a predetermined time in the near future.",1
"As an important and challenging problem in computer vision, learning based optical flow estimation aims to discover the intrinsic correspondence structure between two adjacent video frames through statistical learning. Therefore, a key issue to solve in this area is how to effectively model the multi-scale correspondence structure properties in an adaptive end-to-end learning fashion. Motivated by this observation, we propose an end-to-end multi-scale correspondence structure learning (MSCSL) approach for optical flow estimation. In principle, the proposed MSCSL approach is capable of effectively capturing the multi-scale inter-image-correlation correspondence structures within a multi-level feature space from deep learning. Moreover, the proposed MSCSL approach builds a spatial Conv-GRU neural network model to adaptively model the intrinsic dependency relationships among these multi-scale correspondence structures. Finally, the above procedures for correspondence structure learning and multi-scale dependency modeling are implemented in a unified end-to-end deep learning framework. Experimental results on several benchmark datasets demonstrate the effectiveness of the proposed approach.",0
"Optical flow estimation using learning-based techniques is a complex and significant issue in computer vision. The objective is to establish the internal correspondence structure between two consecutive video frames using statistical learning. The crucial challenge in this domain is to devise a methodology that can model the multi-scale correspondence structure properties efficiently. We have developed an end-to-end multi-scale correspondence structure learning (MSCSL) approach for optical flow estimation, which effectively captures the multi-scale inter-image-correlation correspondence structures within a multi-level feature space from deep learning. The proposed MSCSL approach uses a spatial Conv-GRU neural network model to adaptively model the intrinsic dependency relationships among the multi-scale correspondence structures. The above two steps, i.e., correspondence structure learning and multi-scale dependency modeling, are implemented in a unified end-to-end deep learning framework. The effectiveness of the proposed approach has been demonstrated by conducting experiments on several benchmark datasets.",1
We propose a method for large displacement optical flow in which local matching costs are learned by a convolutional neural network (CNN) and a smoothness prior is imposed by a conditional random field (CRF). We tackle the computation- and memory-intensive operations on the 4D cost volume by a min-projection which reduces memory complexity from quadratic to linear and binary descriptors for efficient matching. This enables evaluation of the cost on the fly and allows to perform learning and CRF inference on high resolution images without ever storing the 4D cost volume. To address the problem of learning binary descriptors we propose a new hybrid learning scheme. In contrast to current state of the art approaches for learning binary CNNs we can compute the exact non-zero gradient within our model. We compare several methods for training binary descriptors and show results on public available benchmarks.,0
"Our proposed technique for optical flow with large displacement involves the use of a convolutional neural network (CNN) to learn local matching costs and a conditional random field (CRF) to impose smoothness. We handle the resource-intensive computations and memory operations of the 4D cost volume through min-projection, which reduces memory complexity from quadratic to linear, and binary descriptors for efficient matching. This allows for on-the-fly cost evaluation and high-resolution image learning and CRF inference without ever storing the 4D cost volume. To overcome the challenge of learning binary descriptors, we introduce a novel hybrid learning scheme that computes the exact non-zero gradient within our model, unlike current binary CNN learning methods. We compare various binary descriptor training methods and showcase our results on publicly available benchmarks.",1
"Classical approaches for estimating optical flow have achieved rapid progress in the last decade. However, most of them are too slow to be applied in real-time video analysis. Due to the great success of deep learning, recent work has focused on using CNNs to solve such dense prediction problems. In this paper, we investigate a new deep architecture, Densely Connected Convolutional Networks (DenseNet), to learn optical flow. This specific architecture is ideal for the problem at hand as it provides shortcut connections throughout the network, which leads to implicit deep supervision. We extend current DenseNet to a fully convolutional network to learn motion estimation in an unsupervised manner. Evaluation results on three standard benchmarks demonstrate that DenseNet is a better fit than other widely adopted CNN architectures for optical flow estimation.",0
"In the past decade, traditional methods for estimating optical flow have made considerable advancements, but their application in real-time video analysis is limited due to their slow speed. To address this issue, recent research has turned to deep learning and using Convolutional Neural Networks (CNNs) for dense prediction problems. This study explores a new deep architecture called Densely Connected Convolutional Networks (DenseNet) for learning optical flow. The architecture is well-suited for the task at hand as it includes shortcut connections throughout the network, leading to implicit deep supervision. The study extends the current DenseNet to a fully convolutional network for unsupervised learning of motion estimation. Evaluation on three standard benchmarks reveals that DenseNet is more suitable for optical flow estimation than other widely used CNN architectures.",1
"We propose a new multi-frame method for efficiently computing scene flow (dense depth and optical flow) and camera ego-motion for a dynamic scene observed from a moving stereo camera rig. Our technique also segments out moving objects from the rigid scene. In our method, we first estimate the disparity map and the 6-DOF camera motion using stereo matching and visual odometry. We then identify regions inconsistent with the estimated camera motion and compute per-pixel optical flow only at these regions. This flow proposal is fused with the camera motion-based flow proposal using fusion moves to obtain the final optical flow and motion segmentation. This unified framework benefits all four tasks - stereo, optical flow, visual odometry and motion segmentation leading to overall higher accuracy and efficiency. Our method is currently ranked third on the KITTI 2015 scene flow benchmark. Furthermore, our CPU implementation runs in 2-3 seconds per frame which is 1-3 orders of magnitude faster than the top six methods. We also report a thorough evaluation on challenging Sintel sequences with fast camera and object motion, where our method consistently outperforms OSF [Menze and Geiger, 2015], which is currently ranked second on the KITTI benchmark.",0
"Our proposal introduces a novel approach to scene flow computation, which includes dense depth and optical flow, as well as camera ego-motion for a dynamic scene viewed through a moving stereo camera rig. Additionally, our technique enables the identification of moving objects within a rigid scene. Initially, we apply stereo matching and visual odometry to compute the disparity map and 6-DOF camera motion. Next, we identify regions that are inconsistent with the estimated camera motion and only compute per-pixel optical flow in these areas. We fuse this flow proposal with the camera motion-based flow proposal using fusion moves to obtain the final optical flow and motion segmentation. This unified framework enhances all four tasks - stereo, optical flow, visual odometry, and motion segmentation - resulting in higher accuracy and efficiency. Our method ranks third on the KITTI 2015 scene flow benchmark and our CPU implementation runs in 2-3 seconds per frame, which is 1-3 orders of magnitude faster than the top six methods. Furthermore, we report a comprehensive evaluation on challenging Sintel sequences with fast camera and object motion, where our method consistently outperforms OSF [Menze and Geiger, 2015], which currently ranks second on the KITTI benchmark.",1
"Learning approaches have shown great success in the task of super-resolving an image given a low resolution input. Video super-resolution aims for exploiting additionally the information from multiple images. Typically, the images are related via optical flow and consecutive image warping. In this paper, we provide an end-to-end video super-resolution network that, in contrast to previous works, includes the estimation of optical flow in the overall network architecture. We analyze the usage of optical flow for video super-resolution and find that common off-the-shelf image warping does not allow video super-resolution to benefit much from optical flow. We rather propose an operation for motion compensation that performs warping from low to high resolution directly. We show that with this network configuration, video super-resolution can benefit from optical flow and we obtain state-of-the-art results on the popular test sets. We also show that the processing of whole images rather than independent patches is responsible for a large increase in accuracy.",0
"The utilization of learning approaches has been successful in enhancing the resolution of low quality images. Video super-resolution seeks to utilize data from multiple images related by optical flow and image warping. This research presents a video super-resolution network that integrates optical flow estimation into the overall architecture, in contrast to previous works. The analysis of optical flow usage indicates that typical image warping does not substantially improve video super-resolution. Instead, a motion compensation operation that directly warps from low to high resolution is suggested. The proposed network configuration demonstrates that video super-resolution can benefit from optical flow, resulting in state-of-the-art outcomes on popular test sets. Utilizing entire images instead of individual patches is also shown to significantly enhance accuracy.",1
"We study the unsupervised learning of CNNs for optical flow estimation using proxy ground truth data. Supervised CNNs, due to their immense learning capacity, have shown superior performance on a range of computer vision problems including optical flow prediction. They however require the ground truth flow which is usually not accessible except on limited synthetic data. Without the guidance of ground truth optical flow, unsupervised CNNs often perform worse as they are naturally ill-conditioned. We therefore propose a novel framework in which proxy ground truth data generated from classical approaches is used to guide the CNN learning. The models are further refined in an unsupervised fashion using an image reconstruction loss. Our guided learning approach is competitive with or superior to state-of-the-art approaches on three standard benchmark datasets yet is completely unsupervised and can run in real time.",0
"Our research focuses on the unsupervised learning of CNNs for optical flow estimation, using alternative ground truth data. While supervised CNNs have demonstrated exceptional performance in a variety of computer vision tasks, including optical flow prediction, they require access to ground truth flow data that is often limited to synthetic data. Since unsupervised CNNs lack the guidance of ground truth optical flow, they can perform poorly due to their inherent limitations. To address this, we propose a novel framework that utilizes proxy ground truth data generated from traditional methods to guide the CNN learning process. Additionally, we refine our models through an unsupervised approach that utilizes an image reconstruction loss. Our guided learning approach is both unsupervised and real-time, and has been shown to be competitive with or superior to state-of-the-art approaches on three standard benchmark datasets.",1
"In this paper, we present YoTube-a novel network fusion framework for searching action proposals in untrimmed videos, where each action proposal corresponds to a spatialtemporal video tube that potentially locates one human action. Our method consists of a recurrent YoTube detector and a static YoTube detector, where the recurrent YoTube explores the regression capability of RNN for candidate bounding boxes predictions using learnt temporal dynamics and the static YoTube produces the bounding boxes using rich appearance cues in a single frame. Both networks are trained using rgb and optical flow in order to fully exploit the rich appearance, motion and temporal context, and their outputs are fused to produce accurate and robust proposal boxes. Action proposals are finally constructed by linking these boxes using dynamic programming with a novel trimming method to handle the untrimmed video effectively and efficiently. Extensive experiments on the challenging UCF-101 and UCF-Sports datasets show that our proposed technique obtains superior performance compared with the state-of-the-art.",0
"In this research, we introduce YoTube, a fresh network fusion system that seeks potential action proposals in untrimmed videos. Each action proposal corresponds to a spatial-temporal video tube that may contain a human action. Our approach comprises of a recurrent YoTube detector and a static YoTube detector. The recurrent YoTube employs RNN for candidate bounding box predictions, utilizing learned temporal dynamics. Meanwhile, the static YoTube employs rich appearance cues in a single frame to produce the bounding boxes. Both networks are trained using rgb and optical flow, exploiting rich appearance, motion, and temporal context. Their outputs are merged to create accurate and robust proposal boxes. Finally, action proposals are formed by linking these boxes using dynamic programming and a novel trimming method that efficiently handles untrimmed video. Our experiments on the challenging UCF-101 and UCF-Sports datasets demonstrate that our proposed method achieves superior performance compared to the state-of-the-art.",1
"We propose a novel method for temporally pooling frames in a video for the task of human action recognition. The method is motivated by the observation that there are only a small number of frames which, together, contain sufficient information to discriminate an action class present in a video, from the rest. The proposed method learns to pool such discriminative and informative frames, while discarding a majority of the non-informative frames in a single temporal scan of the video. Our algorithm does so by continuously predicting the discriminative importance of each video frame and subsequently pooling them in a deep learning framework. We show the effectiveness of our proposed pooling method on standard benchmarks where it consistently improves on baseline pooling methods, with both RGB and optical flow based Convolutional networks. Further, in combination with complementary video representations, we show results that are competitive with respect to the state-of-the-art results on two challenging and publicly available benchmark datasets.",0
"A new approach to temporal frame pooling in videos for human action recognition is presented. The method is based on the observation that only a few frames contain enough information to distinguish an action class from the rest. The approach learns to identify and pool these informative frames while disregarding the majority of non-informative frames in a single pass through the video. The algorithm predicts the discriminative importance of each frame and pools them in a deep learning framework. The proposed method outperforms baseline pooling methods on standard benchmarks using RGB and optical flow-based Convolutional networks. When combined with complementary video representations, the approach delivers competitive results compared to state-of-the-art results on two challenging and publicly available benchmark datasets.",1
"Intra-operative measurements of tissue shape and multi/ hyperspectral information have the potential to provide surgical guidance and decision making support. We report an optical probe based system to combine sparse hyperspectral measurements and spectrally-encoded structured lighting (SL) for surface measurements. The system provides informative signals for navigation with a surgical interface. By rapidly switching between SL and white light (WL) modes, SL information is combined with structure-from-motion (SfM) from white light images, based on SURF feature detection and Lucas-Kanade (LK) optical flow to provide quasi-dense surface shape reconstruction with known scale in real-time. Furthermore, ""super-spectral-resolution"" was realized, whereby the RGB images and sparse hyperspectral data were integrated to recover dense pixel-level hyperspectral stacks, by using convolutional neural networks to upscale the wavelength dimension. Validation and demonstration of this system is reported on ex vivo/in vivo animal/ human experiments.",0
"The use of intra-operative measurements of tissue shape and multi/hyperspectral information can assist with surgical guidance and decision making. Our team has developed an optical probe system that combines sparse hyperspectral measurements with spectrally-encoded structured lighting (SL) for surface measurements. This system provides informative signals that aid with surgical navigation. The SL information is combined with structure-from-motion (SfM) from white light images to provide quasi-dense surface shape reconstruction in real-time. We also achieved ""super-spectral-resolution"" by integrating RGB images and sparse hyperspectral data to recover dense pixel-level hyperspectral stacks. This was accomplished using convolutional neural networks to upscale the wavelength dimension. We have validated and demonstrated the effectiveness of this system in both ex vivo and in vivo animal and human experiments.",1
"Rapid and low power computation of optical flow (OF) is potentially useful in robotics. The dynamic vision sensor (DVS) event camera produces quick and sparse output, and has high dynamic range, but conventional OF algorithms are frame-based and cannot be directly used with event-based cameras. Previous DVS OF methods do not work well with dense textured input and are designed for implementation in logic circuits. This paper proposes a new block-matching based DVS OF algorithm which is inspired by motion estimation methods used for MPEG video compression. The algorithm was implemented both in software and on FPGA. For each event, it computes the motion direction as one of 9 directions. The speed of the motion is set by the sample interval. Results show that the Average Angular Error can be improved by 30\% compared with previous methods. The OF can be calculated on FPGA with 50\,MHz clock in 0.2\,us per event (11 clock cycles), 20 times faster than a Java software implementation running on a desktop PC. Sample data is shown that the method works on scenes dominated by edges, sparse features, and dense texture.",0
"In robotics, the ability to compute optical flow (OF) quickly and with low power consumption is highly advantageous. While the dynamic vision sensor (DVS) event camera provides rapid and sparse output with a wide dynamic range, traditional OF algorithms are based on frames and cannot be directly applied to event-based cameras. Previous DVS OF methods are geared towards implementation in logic circuits and do not perform well with densely textured input. This study introduces a novel block-matching based DVS OF algorithm, inspired by motion estimation techniques used in MPEG video compression. The algorithm has been implemented on both software and FPGA, and determines motion direction as one of nine directions per event, with motion speed set by the sample interval. Results demonstrate a 30% improvement in Average Angular Error over previous methods, and FPGA computation of OF can be achieved at a rate of 0.2μs per event (11 clock cycles) with a 50MHz clock, 20 times faster than a Java software implementation on a desktop PC. Sample data shows the algorithm to be effective in scenes with edges, sparse features, and dense texture.",1
"This work presents a supervised learning based approach to the computer vision problem of frame interpolation. The presented technique could also be used in the cartoon animations since drawing each individual frame consumes a noticeable amount of time. The most existing solutions to this problem use unsupervised methods and focus only on real life videos with already high frame rate. However, the experiments show that such methods do not work as well when the frame rate becomes low and object displacements between frames becomes large. This is due to the fact that interpolation of the large displacement motion requires knowledge of the motion structure thus the simple techniques such as frame averaging start to fail. In this work the deep convolutional neural network is used to solve the frame interpolation problem. In addition, it is shown that incorporating the prior information such as optical flow improves the interpolation quality significantly.",0
"This study introduces a supervised learning approach to address the computer vision issue of frame interpolation. The technique proposed can be beneficial for cartoon animations since creating each frame requires a considerable amount of time. Previous solutions to this problem mainly relied on unsupervised methods and were limited to real-life videos with high frame rates. Nevertheless, experiments have demonstrated that these methods are less effective when the frame rate is low and the object displacements between frames are significant. This is because interpolating large displacement movements necessitates knowledge of motion structure, causing straightforward techniques like frame averaging to falter. This research utilizes a deep convolutional neural network to tackle the frame interpolation problem and demonstrates that including prior knowledge such as optical flow significantly enhances interpolation quality.",1
"Webly-supervised learning has recently emerged as an alternative paradigm to traditional supervised learning based on large-scale datasets with manual annotations. The key idea is that models such as CNNs can be learned from the noisy visual data available on the web. In this work we aim to exploit web data for video understanding tasks such as action recognition and detection. One of the main problems in webly-supervised learning is cleaning the noisy labeled data from the web. The state-of-the-art paradigm relies on training a first classifier on noisy data that is then used to clean the remaining dataset. Our key insight is that this procedure biases the second classifier towards samples that the first one understands. Here we train two independent CNNs, a RGB network on web images and video frames and a second network using temporal information from optical flow. We show that training the networks independently is vastly superior to selecting the frames for the flow classifier by using our RGB network. Moreover, we show benefits in enriching the training set with different data sources from heterogeneous public web databases. We demonstrate that our framework outperforms all other webly-supervised methods on two public benchmarks, UCF-101 and Thumos'14.",0
"Recently, a new approach to supervised learning called webly-supervised learning has emerged. This method differs from traditional supervised learning in that it uses the vast amount of web data with noisy annotations rather than relying on manually annotated large-scale datasets. The main focus of this study is to apply webly-supervised learning to video understanding tasks, such as action recognition and detection. However, one of the challenges with this approach is cleaning the noisy labeled data from the web. The current state-of-the-art method involves training a classifier on the noisy data, which is then used to clean the remaining dataset. However, this approach can bias the second classifier towards samples that the first one understands. To address this, we propose training two independent CNNs - an RGB network for web images and video frames, and a second network using temporal information from optical flow. We demonstrate that training the networks independently is much more effective than selecting frames for the flow classifier using the RGB network. Additionally, we show that enriching the training set with data from different sources improves performance. Our approach outperforms other webly-supervised methods on two public benchmarks - UCF-101 and Thumos'14.",1
"Accurate detection of the myocardial infarction (MI) area is crucial for early diagnosis planning and follow-up management. In this study, we propose an end-to-end deep-learning algorithm framework (OF-RNN ) to accurately detect the MI area at the pixel level. Our OF-RNN consists of three different function layers: the heart localization layers, which can accurately and automatically crop the region-of-interest (ROI) sequences, including the left ventricle, using the whole cardiac magnetic resonance image sequences; the motion statistical layers, which are used to build a time-series architecture to capture two types of motion features (at the pixel-level) by integrating the local motion features generated by long short-term memory-recurrent neural networks and the global motion features generated by deep optical flows from the whole ROI sequence, which can effectively characterize myocardial physiologic function; and the fully connected discriminate layers, which use stacked auto-encoders to further learn these features, and they use a softmax classifier to build the correspondences from the motion features to the tissue identities (infarction or not) for each pixel. Through the seamless connection of each layer, our OF-RNN can obtain the area, position, and shape of the MI for each patient. Our proposed framework yielded an overall classification accuracy of 94.35% at the pixel level, from 114 clinical subjects. These results indicate the potential of our proposed method in aiding standardized MI assessments.",0
"Early diagnosis planning and follow-up management rely heavily on accurately detecting the myocardial infarction (MI) area. Our study proposes the OF-RNN, an end-to-end deep-learning algorithm framework, to precisely identify the MI area at the pixel level. The OF-RNN comprises three function layers: the heart localization layers, which crop the region-of-interest (ROI) sequences, including the left ventricle, using the entire cardiac magnetic resonance image sequences; the motion statistical layers, which capture two types of motion features at the pixel-level using long short-term memory-recurrent neural networks and deep optical flows from the whole ROI sequence; and the fully connected discriminate layers, which use stacked auto-encoders to learn these features and a softmax classifier to build correspondences from motion features to tissue identities (infarction or not) for each pixel. By seamlessly connecting each layer, our OF-RNN can identify the area, position, and shape of the MI for each patient. With 114 clinical subjects, our proposed framework achieved an overall classification accuracy of 94.35% at the pixel level, showcasing its potential in aiding standardized MI assessments.",1
"Predicting an interaction before it is fully executed is very important in applications such as human-robot interaction and video surveillance. In a two-human interaction scenario, there often contextual dependency structure between the global interaction context of the two humans and the local context of the different body parts of each human. In this paper, we propose to learn the structure of the interaction contexts, and combine it with the spatial and temporal information of a video sequence for a better prediction of the interaction class. The structural models, including the spatial and the temporal models, are learned with Long Short Term Memory (LSTM) networks to capture the dependency of the global and local contexts of each RGB frame and each optical flow image, respectively. LSTM networks are also capable of detecting the key information from the global and local interaction contexts. Moreover, to effectively combine the structural models with the spatial and temporal models for interaction prediction, a ranking score fusion method is also introduced to automatically compute the optimal weight of each model for score fusion. Experimental results on the BIT Interaction and the UT-Interaction datasets clearly demonstrate the benefits of the proposed method.",0
"Anticipating interactions before their execution is crucial in various applications, including video surveillance and human-robot interaction. When two humans interact, there is a contextual dependency structure between the global interaction context of the individuals and the local context of their body parts. This paper suggests learning the interaction context structure and combining it with spatial and temporal information from a video sequence to improve interaction prediction. Long Short Term Memory (LSTM) networks are used to learn the structural models, including spatial and temporal models, to capture the dependency of global and local contexts of each RGB frame and optical flow image. LSTM networks can also detect key information from interaction contexts. Additionally, a ranking score fusion method is introduced to effectively combine the structural models with the spatial and temporal models for interaction prediction. This method is experimentally proven to be beneficial on the BIT Interaction and the UT-Interaction datasets.",1
"Typical human actions last several seconds and exhibit characteristic spatio-temporal structure. Recent methods attempt to capture this structure and learn action representations with convolutional neural networks. Such representations, however, are typically learned at the level of a few video frames failing to model actions at their full temporal extent. In this work we learn video representations using neural networks with long-term temporal convolutions (LTC). We demonstrate that LTC-CNN models with increased temporal extents improve the accuracy of action recognition. We also study the impact of different low-level representations, such as raw values of video pixels and optical flow vector fields and demonstrate the importance of high-quality optical flow estimation for learning accurate action models. We report state-of-the-art results on two challenging benchmarks for human action recognition UCF101 (92.7%) and HMDB51 (67.2%).",0
"Actions performed by humans typically last for a few seconds and have a spatio-temporal structure that is unique to them. Recent attempts to capture this structure and develop action representations using convolutional neural networks have been made. However, these representations are usually learned from only a few video frames, which fail to capture the complete temporal extent of actions. To address this, we have developed a method to learn video representations using neural networks with long-term temporal convolutions (LTC). Our research shows that the accuracy of action recognition can be improved by increasing the temporal extent of LTC-CNN models. We have also studied the effect of different low-level representations, such as raw video pixel values and optical flow vector fields, and have highlighted the importance of high-quality optical flow estimation for accurate learning of action models. Our results on two challenging benchmarks for human action recognition, UCF101 (92.7%) and HMDB51 (67.2%), demonstrate state-of-the-art performance.",1
"Infrared (IR) imaging has the potential to enable more robust action recognition systems compared to visible spectrum cameras due to lower sensitivity to lighting conditions and appearance variability. While the action recognition task on videos collected from visible spectrum imaging has received much attention, action recognition in IR videos is significantly less explored. Our objective is to exploit imaging data in this modality for the action recognition task. In this work, we propose a novel two-stream 3D convolutional neural network (CNN) architecture by introducing the discriminative code layer and the corresponding discriminative code loss function. The proposed network processes IR image and the IR-based optical flow field sequences. We pretrain the 3D CNN model on the visible spectrum Sports-1M action dataset and finetune it on the Infrared Action Recognition (InfAR) dataset. To our best knowledge, this is the first application of the 3D CNN to action recognition in the IR domain. We conduct an elaborate analysis of different fusion schemes (weighted average, single and double-layer neural nets) applied to different 3D CNN outputs. Experimental results demonstrate that our approach can achieve state-of-the-art average precision (AP) performances on the InfAR dataset: (1) the proposed two-stream 3D CNN achieves the best reported 77.5% AP, and (2) our 3D CNN model applied to the optical flow fields achieves the best reported single stream 75.42% AP.",0
"Compared to visible spectrum cameras, infrared (IR) imaging has the potential to create more reliable action recognition systems because it is less affected by lighting conditions and appearance variability. Although action recognition using visible spectrum videos has been extensively studied, there is limited research on action recognition in IR videos. Our goal is to utilize IR imaging data for action recognition, and we propose a new two-stream 3D convolutional neural network (CNN) architecture that includes a discriminative code layer and corresponding loss function. The network processes IR images and IR-based optical flow field sequences, and we pretrain it on the Sports-1M dataset and finetune it on the Infrared Action Recognition (InfAR) dataset. Our method achieves the best reported average precision (AP) performances on the InfAR dataset, with the proposed two-stream 3D CNN achieving 77.5% AP and our 3D CNN model applied to the optical flow fields achieving 75.42% AP. We also conduct an in-depth analysis of fusion schemes applied to different 3D CNN outputs. To our knowledge, this is the first application of 3D CNN to action recognition in the IR domain.",1
"We present a method to perform online Multiple Object Tracking (MOT) of known object categories in monocular video data. Current Tracking-by-Detection MOT approaches build on top of 2D bounding box detections. In contrast, we exploit state-of-the-art instance aware semantic segmentation techniques to compute 2D shape representations of target objects in each frame. We predict position and shape of segmented instances in subsequent frames by exploiting optical flow cues. We define an affinity matrix between instances of subsequent frames which reflects locality and visual similarity. The instance association is solved by applying the Hungarian method. We evaluate different configurations of our algorithm using the MOT 2D 2015 train dataset. The evaluation shows that our tracking approach is able to track objects with high relative motions. In addition, we provide results of our approach on the MOT 2D 2015 test set for comparison with previous works. We achieve a MOTA score of 32.1.",0
"Our study introduces a technique for conducting online Multiple Object Tracking (MOT) of familiar object categories within monocular video data. While current Tracking-by-Detection MOT methods rely on 2D bounding box detections, we utilize cutting-edge instance aware semantic segmentation techniques to calculate 2D shape representations of target objects in each frame. By leveraging optical flow cues, we can forecast the position and shape of segmented instances in subsequent frames. We establish an affinity matrix between instances of subsequent frames that considers proximity and visual similarity. The Hungarian method is applied to solve instance association. We assess various configurations of our algorithm using the MOT 2D 2015 train dataset, and the results show that our tracking approach can track objects with significant relative motion. Furthermore, we present our approach's outcomes on the MOT 2D 2015 test set for comparison with prior studies, achieving a MOTA score of 32.1.",1
"We propose a novel approach based on deep Convolutional Neural Networks (CNN) to recognize human actions in still images by predicting the future motion, and detecting the shape and location of the salient parts of the image. We make the following major contributions to this important area of research: (i) We use the predicted future motion in the static image (Walker et al., 2015) as a means of compensating for the missing temporal information, while using the saliency map to represent the the spatial information in the form of location and shape of what is predicted as significant. (ii) We cast action classification in static images as a domain adaptation problem by transfer learning. We first map the input static image to a new domain that we refer to as the Predicted Optical Flow-Saliency Map domain (POF-SM), and then fine-tune the layers of a deep CNN model trained on classifying the ImageNet dataset to perform action classification in the POF-SM domain. (iii) We tested our method on the popular Willow dataset. But unlike existing methods, we also tested on a more realistic and challenging dataset of over 2M still images that we collected and labeled by taking random frames from the UCF-101 video dataset. We call our dataset the UCF Still Image dataset or UCFSI-101 in short. Our results outperform the state of the art.",0
"Our innovative method employs deep Convolutional Neural Networks (CNN) to recognize human actions in still images by predicting future motion and identifying the salient parts of the image. Our contributions to this research include using predicted future motion to compensate for missing temporal information and utilizing the saliency map to represent spatial information. Additionally, we approach action classification as a domain adaptation problem through transfer learning, mapping input static images to the Predicted Optical Flow-Saliency Map domain (POF-SM) and fine-tuning a deep CNN model trained on the ImageNet dataset for action classification in the POF-SM domain. We evaluated our approach on the Willow dataset and a more challenging dataset of over 2 million still images labeled from the UCF-101 video dataset, which we named the UCF Still Image dataset (UCFSI-101). Our results surpass the state-of-the-art.",1
"Recent progress in style transfer on images has focused on improving the quality of stylized images and speed of methods. However, real-time methods are highly unstable resulting in visible flickering when applied to videos. In this work we characterize the instability of these methods by examining the solution set of the style transfer objective. We show that the trace of the Gram matrix representing style is inversely related to the stability of the method. Then, we present a recurrent convolutional network for real-time video style transfer which incorporates a temporal consistency loss and overcomes the instability of prior methods. Our networks can be applied at any resolution, do not re- quire optical flow at test time, and produce high quality, temporally consistent stylized videos in real-time.",0
"The recent advancements in enhancing the quality of stylized images and increasing the speed of style transfer methods have been the primary focus. However, the real-time techniques are prone to instability, leading to noticeable flickering in videos. In this study, we evaluate the instability of these methods by analyzing the solution set of the style transfer objective. Our findings indicate that the trace of the Gram matrix, which represents style, is inversely proportional to method stability. To tackle this issue, we introduce a recurrent convolutional network that incorporates a temporal consistency loss for real-time video style transfer. Our approach surpasses the instability of prior methods and does not require optical flow during testing. Moreover, our networks can be implemented at any resolution and produce high-quality stylized videos that are consistent over time.",1
"Given a visual history, multiple future outcomes for a video scene are equally probable, in other words, the distribution of future outcomes has multiple modes. Multimodality is notoriously hard to handle by standard regressors or classifiers: the former regress to the mean and the latter discretize a continuous high dimensional output space. In this work, we present stochastic neural network architectures that handle such multimodality through stochasticity: future trajectories of objects, body joints or frames are represented as deep, non-linear transformations of random (as opposed to deterministic) variables. Such random variables are sampled from simple Gaussian distributions whose means and variances are parametrized by the output of convolutional encoders over the visual history. We introduce novel convolutional architectures for predicting future body joint trajectories that outperform fully connected alternatives \cite{DBLP:journals/corr/WalkerDGH16}. We introduce stochastic spatial transformers through optical flow warping for predicting future frames, which outperform their deterministic equivalents \cite{DBLP:journals/corr/PatrauceanHC15}. Training stochastic networks involves an intractable marginalization over stochastic variables. We compare various training schemes that handle such marginalization through a) straightforward sampling from the prior, b) conditional variational autoencoders \cite{NIPS2015_5775,DBLP:journals/corr/WalkerDGH16}, and, c) a proposed K-best-sample loss that penalizes the best prediction under a fixed ""prediction budget"". We show experimental results on object trajectory prediction, human body joint trajectory prediction and video prediction under varying future uncertainty, validating quantitatively and qualitatively our architectural choices and training schemes.",0
"The distribution of future outcomes for a video scene is multimodal, meaning that multiple outcomes are equally likely based on the visual history. This presents difficulty for standard regressors and classifiers, as the former tend to average out results and the latter struggle to work with a continuous, high-dimensional output space. To address this issue, stochastic neural network architectures have been developed that incorporate random variables in the prediction of future trajectories for objects, body joints, and frames. These random variables are sampled from Gaussian distributions with means and variances that are determined by the output of convolutional encoders over the visual history. New convolutional architectures have been introduced that outperform fully connected alternatives for predicting future body joint trajectories, as well as stochastic spatial transformers for predicting future frames. Training these stochastic networks involves a challenge in marginalizing over the stochastic variables, which has been addressed through various methods including sampling from the prior, conditional variational autoencoders, and the proposed K-best-sample loss. Experimental results have demonstrated the effectiveness of these approaches for predicting object and human body joint trajectories, as well as video prediction under varying levels of future uncertainty.",1
"The optical flow of natural scenes is a combination of the motion of the observer and the independent motion of objects. Existing algorithms typically focus on either recovering motion and structure under the assumption of a purely static world or optical flow for general unconstrained scenes. We combine these approaches in an optical flow algorithm that estimates an explicit segmentation of moving objects from appearance and physical constraints. In static regions we take advantage of strong constraints to jointly estimate the camera motion and the 3D structure of the scene over multiple frames. This allows us to also regularize the structure instead of the motion. Our formulation uses a Plane+Parallax framework, which works even under small baselines, and reduces the motion estimation to a one-dimensional search problem, resulting in more accurate estimation. In moving regions the flow is treated as unconstrained, and computed with an existing optical flow method. The resulting Mostly-Rigid Flow (MR-Flow) method achieves state-of-the-art results on both the MPI-Sintel and KITTI-2015 benchmarks.",0
"The combination of observer motion and the independent motion of objects creates the optical flow of natural scenes. However, current algorithms typically focus on either motion and structure under the assumption of a static world or optical flow for general unconstrained scenes. Our optical flow algorithm takes a different approach by estimating the moving object segmentation from appearance and physical constraints. For static regions, we use strong constraints to estimate the camera motion and the 3D structure of the scene across multiple frames, allowing us to also regularize the structure. Our Plane+Parallax framework reduces the motion estimation to a one-dimensional search problem, resulting in more accurate estimation. Moving regions are treated as unconstrained and computed with an existing optical flow method. Our Mostly-Rigid Flow (MR-Flow) method achieves state-of-the-art results on both the MPI-Sintel and KITTI-2015 benchmarks.",1
"We propose a novel superpixel-based multi-view convolutional neural network for semantic image segmentation. The proposed network produces a high quality segmentation of a single image by leveraging information from additional views of the same scene. Particularly in indoor videos such as captured by robotic platforms or handheld and bodyworn RGBD cameras, nearby video frames provide diverse viewpoints and additional context of objects and scenes. To leverage such information, we first compute region correspondences by optical flow and image boundary-based superpixels. Given these region correspondences, we propose a novel spatio-temporal pooling layer to aggregate information over space and time. We evaluate our approach on the NYU--Depth--V2 and the SUN3D datasets and compare it to various state-of-the-art single-view and multi-view approaches. Besides a general improvement over the state-of-the-art, we also show the benefits of making use of unlabeled frames during training for multi-view as well as single-view prediction.",0
"Our innovative approach to semantic image segmentation involves a superpixel-based multi-view convolutional neural network. By incorporating information from multiple views of the same scene, our network can generate high-quality segmentations of single images. This is particularly useful in indoor videos captured by robotic platforms or RGBD cameras, as nearby frames provide additional context and viewpoints. To achieve this, we use optical flow and image boundary-based superpixels to establish region correspondences. We also introduce a novel spatio-temporal pooling layer to aggregate information over space and time. Our approach outperforms state-of-the-art single-view and multi-view methods, and we demonstrate the benefits of using unlabeled frames during training for both multi-view and single-view prediction.",1
"We propose SfM-Net, a geometry-aware neural network for motion estimation in videos that decomposes frame-to-frame pixel motion in terms of scene and object depth, camera motion and 3D object rotations and translations. Given a sequence of frames, SfM-Net predicts depth, segmentation, camera and rigid object motions, converts those into a dense frame-to-frame motion field (optical flow), differentiably warps frames in time to match pixels and back-propagates. The model can be trained with various degrees of supervision: 1) self-supervised by the re-projection photometric error (completely unsupervised), 2) supervised by ego-motion (camera motion), or 3) supervised by depth (e.g., as provided by RGBD sensors). SfM-Net extracts meaningful depth estimates and successfully estimates frame-to-frame camera rotations and translations. It often successfully segments the moving objects in the scene, even though such supervision is never provided.",0
"SfM-Net is a neural network that is aware of geometry and is proposed for estimating motion in videos. It breaks down the motion of pixels from one frame to another into scene and object depth, camera motion, and 3D object rotations and translations. SfM-Net can predict depth, segmentation, camera and rigid object motions from a sequence of frames, and convert them into a dense motion field, which can be used to warp frames in time and back-propagate. The model can be trained with varying degrees of supervision: unsupervised, supervised by ego-motion, or supervised by depth. SfM-Net provides meaningful depth estimates, successfully estimates camera rotations and translations, and often segments moving objects in the scene, even when no supervision is given.",1
"Automated Facial Expression Recognition (FER) has been a challenging task for decades. Many of the existing works use hand-crafted features such as LBP, HOG, LPQ, and Histogram of Optical Flow (HOF) combined with classifiers such as Support Vector Machines for expression recognition. These methods often require rigorous hyperparameter tuning to achieve good results. Recently Deep Neural Networks (DNN) have shown to outperform traditional methods in visual object recognition. In this paper, we propose a two-part network consisting of a DNN-based architecture followed by a Conditional Random Field (CRF) module for facial expression recognition in videos. The first part captures the spatial relation within facial images using convolutional layers followed by three Inception-ResNet modules and two fully-connected layers. To capture the temporal relation between the image frames, we use linear chain CRF in the second part of our network. We evaluate our proposed network on three publicly available databases, viz. CK+, MMI, and FERA. Experiments are performed in subject-independent and cross-database manners. Our experimental results show that cascading the deep network architecture with the CRF module considerably increases the recognition of facial expressions in videos and in particular it outperforms the state-of-the-art methods in the cross-database experiments and yields comparable results in the subject-independent experiments.",0
"For decades, the task of Automated Facial Expression Recognition (FER) has posed a significant challenge. Existing methods have typically utilized hand-crafted features such as LBP, HOG, LPQ, and Histogram of Optical Flow (HOF) combined with classifiers like Support Vector Machines for expression recognition. However, these approaches often require extensive hyperparameter tuning to attain satisfactory results. Recently, Deep Neural Networks (DNN) have proven to be more effective than traditional methods in visual object recognition. This paper introduces a two-part network that comprises a DNN-based architecture and a Conditional Random Field (CRF) module for facial expression recognition in videos. The first part of the network employs convolutional layers, three Inception-ResNet modules, and two fully-connected layers to capture the spatial relation within facial images. The second part uses a linear chain CRF to capture the temporal relation between image frames. The proposed network is evaluated on three publicly available databases, namely, CK+, MMI, and FERA, in both subject-independent and cross-database manners. The experimental results demonstrate that integrating the deep network architecture with the CRF module significantly improves the recognition of facial expressions in videos, outperforming state-of-the-art methods in cross-database experiments and producing comparable results in subject-independent experiments.",1
"We present an optical flow estimation approach that operates on the full four-dimensional cost volume. This direct approach shares the structural benefits of leading stereo matching pipelines, which are known to yield high accuracy. To this day, such approaches have been considered impractical due to the size of the cost volume. We show that the full four-dimensional cost volume can be constructed in a fraction of a second due to its regularity. We then exploit this regularity further by adapting semi-global matching to the four-dimensional setting. This yields a pipeline that achieves significantly higher accuracy than state-of-the-art optical flow methods while being faster than most. Our approach outperforms all published general-purpose optical flow methods on both Sintel and KITTI 2015 benchmarks.",0
"Our approach to estimating optical flow involves using the entire four-dimensional cost volume, which shares the advantageous structure of leading stereo matching pipelines and is known for its high accuracy. Although this approach has previously been deemed impractical due to the size of the cost volume, we demonstrate that it can be constructed in a mere fraction of a second thanks to its regularity. We then utilize this regularity to our advantage by adapting semi-global matching to the four-dimensional setting, resulting in a pipeline that surpasses state-of-the-art optical flow methods in accuracy and speed. In fact, our method outperforms all previously published general-purpose optical flow methods on both the Sintel and KITTI 2015 benchmarks.",1
"The ability to amplify or reduce subtle image changes over time is useful in contexts such as video editing, medical video analysis, product quality control and sports. In these contexts there is often large motion present which severely distorts current video amplification methods that magnify change linearly. In this work we propose a method to cope with large motions while still magnifying small changes. We make the following two observations: i) large motions are linear on the temporal scale of the small changes; ii) small changes deviate from this linearity. We ignore linear motion and propose to magnify acceleration. Our method is pure Eulerian and does not require any optical flow, temporal alignment or region annotations. We link temporal second-order derivative filtering to spatial acceleration magnification. We apply our method to moving objects where we show motion magnification and color magnification. We provide quantitative as well as qualitative evidence for our method while comparing to the state-of-the-art.",0
"In various fields such as video editing, medical video analysis, product quality control, and sports, it is essential to enhance or diminish subtle changes in images over time. However, current video amplification methods that increase changes linearly are not suitable for contexts that involve significant motion. To address this issue, we propose a new method that can handle large motions while still amplifying small changes. We have observed that large motions are linear on the temporal scale of small changes, and small changes deviate from linearity. By ignoring linear motion and focusing on acceleration, we have developed a purely Eulerian method that does not require optical flow, temporal alignment, or region annotations. We have linked temporal second-order derivative filtering to spatial acceleration magnification and applied this method to moving objects, resulting in motion and color magnification. Our method has been thoroughly evaluated and compared with the state-of-the-art, providing both quantitative and qualitative evidence of its effectiveness.",1
"It is difficult to recover the motion field from a real-world footage given a mixture of camera shake and other photometric effects. In this paper we propose a hybrid framework by interleaving a Convolutional Neural Network (CNN) and a traditional optical flow energy. We first conduct a CNN architecture using a novel learnable directional filtering layer. Such layer encodes the angle and distance similarity matrix between blur and camera motion, which is able to enhance the blur features of the camera-shake footages. The proposed CNNs are then integrated into an iterative optical flow framework, which enable the capability of modelling and solving both the blind deconvolution and the optical flow estimation problems simultaneously. Our framework is trained end-to-end on a synthetic dataset and yields competitive precision and performance against the state-of-the-art approaches.",0
"Recovering the motion field from real-life footage is challenging due to a combination of camera shake and other photometric effects. To address this issue, our paper proposes a hybrid framework that combines a Convolutional Neural Network (CNN) and a traditional optical flow energy. The first step involves developing a CNN architecture that utilizes a unique learnable directional filtering layer to encode the angle and distance similarity matrix between blur and camera motion. This enhances the blur features of the camera-shake footage. The resulting CNNs are then integrated into an iterative optical flow framework, which can model and solve both the blind deconvolution and optical flow estimation problems simultaneously. Our end-to-end trained framework uses a synthetic dataset and achieves competitive precision and performance compared to state-of-the-art approaches.",1
"Dynamic scene understanding is a challenging problem and motion segmentation plays a crucial role in solving it. Incorporating semantics and motion enhances the overall perception of the dynamic scene. For applications of outdoor robotic navigation, joint learning methods have not been extensively used for extracting spatio-temporal features or adding different priors into the formulation. The task becomes even more challenging without stereo information being incorporated. This paper proposes an approach to fuse semantic features and motion clues using CNNs, to address the problem of monocular semantic motion segmentation. We deduce semantic and motion labels by integrating optical flow as a constraint with semantic features into dilated convolution network. The pipeline consists of three main stages i.e Feature extraction, Feature amplification and Multi Scale Context Aggregation to fuse the semantics and flow features. Our joint formulation shows significant improvements in monocular motion segmentation over the state of the art methods on challenging KITTI tracking dataset.",0
"Solving the problem of dynamic scene understanding is no easy feat, with motion segmentation being a vital aspect of it. Combining semantics and motion leads to a more accurate perception of the dynamic scene - especially in outdoor robotic navigation applications. However, extracting spatio-temporal features or incorporating diverse priors into the formulation has not been widely done using joint learning methods. This task becomes even more difficult without stereo information, which is why our paper proposes a CNN-based approach to fuse semantic features and motion clues. Our method utilizes optical flow as a constraint and integrates it with semantic features into a dilated convolution network to deduce semantic and motion labels. The pipeline consists of three stages: Feature extraction, Feature amplification, and Multi Scale Context Aggregation, which are used to merge the semantics and flow features. Our joint formulation significantly enhances monocular motion segmentation performance compared to state-of-the-art methods on the challenging KITTI tracking dataset.",1
We propose a variational approach to obtain super-resolution images from multiple low-resolution frames extracted from video clips. First the displacement between the low-resolution frames and the reference frame are computed by an optical flow algorithm. Then a low-rank model is used to construct the reference frame in high-resolution by incorporating the information of the low-resolution frames. The model has two terms: a 2-norm data fidelity term and a nuclear-norm regularization term. Alternating direction method of multipliers is used to solve the model. Comparison of our methods with other models on synthetic and real video clips show that our resulting images are more accurate with less artifacts. It also provides much finer and discernable details.,0
"Our proposed method utilizes a variational approach to generate high-resolution images from multiple low-resolution frames extracted from video clips. Initially, an optical flow algorithm computes the displacement between the low-resolution frames and the reference frame. Subsequently, a low-rank model is employed to create the reference frame in high-resolution by integrating the information from the low-resolution frames. The model consists of two terms: a 2-norm data fidelity term and a nuclear-norm regularization term. The model is solved using the alternating direction method of multipliers. In comparison to other models on synthetic and real video clips, our method produces more precise images with fewer artifacts and more refined details.",1
"Initializing optical flow field by either sparse descriptor matching or dense patch matches has been proved to be particularly useful for capturing large displacements. In this paper, we present a pyramidal gradient matching approach that can provide dense matches for highly accurate and efficient optical flow estimation. A novel contribution of our method is that image gradient is used to describe image patches and proved to be able to produce robust matching. Therefore, our method is more efficient than methods that adopt special features (like SIFT) or patch distance metric. Moreover, we find that image gradient is scalable for optical flow estimation, which means we can use different levels of gradient feature (for example, full gradients or only direction information of gradients) to obtain different complexity without dramatic changes in accuracy. Another contribution is that we uncover the secrets of limited PatchMatch through a thorough analysis and design a pyramidal matching framework based these secrets. Our pyramidal matching framework is aimed at robust gradient matching and effective to grow inliers and reject outliers. In this framework, we present some special enhancements for outlier filtering in gradient matching. By initializing EpicFlow with our matches, experimental results show that our method is efficient and robust (ranking 1st on both clean pass and final pass of MPI Sintel dataset among published methods).",0
"To capture large displacements, initializing the optical flow field through sparse descriptor matching or dense patch matches has proven useful. In this research, we introduce a pyramidal gradient matching approach that provides highly accurate and efficient optical flow estimation through dense matches. Our approach utilizes image gradient to describe image patches, which has been proven to produce robust matching, making it more efficient than methods using special features or patch distance metric. We also discovered that image gradient is scalable for optical flow estimation, allowing us to use different levels of gradient feature for varying complexity without compromising accuracy. Additionally, we analyzed the limitations of PatchMatch and designed a pyramidal matching framework based on these findings, aimed at robust gradient matching with special enhancements for outlier filtering. Our method's experimental results show it to be efficient and robust, ranking first on both clean pass and final pass of MPI Sintel dataset among published methods when used to initialize EpicFlow.",1
"In this paper we formulate structure from motion as a learning problem. We train a convolutional network end-to-end to compute depth and camera motion from successive, unconstrained image pairs. The architecture is composed of multiple stacked encoder-decoder networks, the core part being an iterative network that is able to improve its own predictions. The network estimates not only depth and motion, but additionally surface normals, optical flow between the images and confidence of the matching. A crucial component of the approach is a training loss based on spatial relative differences. Compared to traditional two-frame structure from motion methods, results are more accurate and more robust. In contrast to the popular depth-from-single-image networks, DeMoN learns the concept of matching and, thus, better generalizes to structures not seen during training.",0
"This paper proposes a learning-based approach to structure from motion. The method involves training a convolutional network to calculate depth and camera motion using unconstrained image pairs. The network comprises multiple encoder-decoder networks, with the core being an iterative network that can refine its own predictions. In addition to depth and motion, the network also estimates surface normals, optical flow, and matching confidence. The training loss is based on spatial relative differences. Compared to traditional two-frame methods, this approach produces more accurate and robust results. Furthermore, unlike depth-from-single-image networks, DeMoN learns the concept of matching, enabling it to generalize better to novel structures.",1
"The problem of determining whether an object is in motion, irrespective of camera motion, is far from being solved. We address this challenging task by learning motion patterns in videos. The core of our approach is a fully convolutional network, which is learned entirely from synthetic video sequences, and their ground-truth optical flow and motion segmentation. This encoder-decoder style architecture first learns a coarse representation of the optical flow field features, and then refines it iteratively to produce motion labels at the original high-resolution. We further improve this labeling with an objectness map and a conditional random field, to account for errors in optical flow, and also to focus on moving ""things"" rather than ""stuff"". The output label of each pixel denotes whether it has undergone independent motion, i.e., irrespective of camera motion. We demonstrate the benefits of this learning framework on the moving object segmentation task, where the goal is to segment all objects in motion. Our approach outperforms the top method on the recently released DAVIS benchmark dataset, comprising real-world sequences, by 5.6%. We also evaluate on the Berkeley motion segmentation database, achieving state-of-the-art results.",0
"The issue of detecting motion in an object, regardless of camera movement, has not yet been completely resolved. Our solution to this difficult problem involves acquiring knowledge of motion patterns from videos. Our methodology centers on a fully convolutional network that is trained using synthetic video sequences and their corresponding ground-truth optical flow and motion segmentation. The encoder-decoder style architecture initially learns a rough representation of the optical flow field features, which it then progressively improves to generate motion labels of the original high-resolution. To address errors in optical flow and concentrate on moving objects, rather than stationary items, we enhance this labeling process with an objectness map and a conditional random field. The label assigned to each pixel indicates whether it has undergone independent motion, regardless of camera movement. Our approach performs better than the leading method on the recently published DAVIS benchmark dataset, which includes real-life sequences, by 5.6%. We also achieve state-of-the-art results on the Berkeley motion segmentation database.",1
"CNN-based optical flow estimation has attracted attention recently, mainly due to its impressively high frame rates. These networks perform well on synthetic datasets, but they are still far behind the classical methods in real-world videos. This is because there is no ground truth optical flow for training these networks on real data. In this paper, we boost CNN-based optical flow estimation in real scenes with the help of the freely available self-supervised task of next-frame prediction. To this end, we train the network in a hybrid way, providing it with a mixture of synthetic and real videos. With the help of a sample-variant multi-tasking architecture, the network is trained on different tasks depending on the availability of ground-truth. We also experiment with the prediction of ""next-flow"" instead of estimation of the current flow, which is intuitively closer to the task of next-frame prediction and yields favorable results. We demonstrate the improvement in optical flow estimation on the real-world KITTI benchmark. Additionally, we test the optical flow indirectly in an action classification scenario. As a side product of this work, we report significant improvements over state-of-the-art in the task of next-frame prediction.",0
"The use of CNN-based optical flow estimation has gained attention recently due to its high frame rates, but these networks still lag behind classical methods in real-world videos because they lack ground truth optical flow for training. To address this, we enhance CNN-based optical flow estimation in real scenes by using self-supervised next-frame prediction. Our approach involves hybrid training with synthetic and real videos using a sample-variant multi-tasking architecture that trains the network on different tasks depending on ground-truth availability. We also experiment with predicting ""next-flow"" instead of current flow estimation, yielding favorable results. Our approach improves optical flow estimation on the KITTI benchmark and enhances next-frame prediction significantly. Additionally, we indirectly test optical flow in an action classification scenario.",1
"Training of Convolutional Neural Networks (CNNs) on long video sequences is computationally expensive due to the substantial memory requirements and the massive number of parameters that deep architectures demand. Early fusion of video frames is thus a standard technique, in which several consecutive frames are first agglomerated into a compact representation, and then fed into the CNN as an input sample. For this purpose, a summarization approach that represents a set of consecutive RGB frames by a single dynamic image to capture pixel dynamics is proposed recently. In this paper, we introduce a novel ordered representation of consecutive optical flow frames as an alternative and argue that this representation captures the action dynamics more effectively than RGB frames. We provide intuitions on why such a representation is better for action recognition. We validate our claims on standard benchmark datasets and demonstrate that using summaries of flow images lead to significant improvements over RGB frames while achieving accuracy comparable to the state-of-the-art on UCF101 and HMDB datasets.",0
"The computational expense of training Convolutional Neural Networks (CNNs) on lengthy video sequences is due to the extensive memory requirements and numerous parameters required by deep architectures. To mitigate this, early fusion is a common technique that involves combining several consecutive frames into a concise representation that is then fed into the CNN. A summarization approach that creates a dynamic image to capture pixel dynamics was recently proposed for this purpose. In this study, we propose an alternative ordered representation of consecutive optical flow frames that we argue captures action dynamics more effectively than RGB frames. We provide insights into why this representation is superior for action recognition and validate our claims on standard benchmark datasets. We demonstrate that using summaries of flow images leads to significant improvements over RGB frames while achieving accuracy comparable to the state-of-the-art on UCF101 and HMDB datasets.",1
"We propose a framework for Google Map aided UAV navigation in GPS-denied environment. Geo-referenced navigation provides drift-free localization and does not require loop closures. The UAV position is initialized via correlation, which is simple and efficient. We then use optical flow to predict its position in subsequent frames. During pose tracking, we obtain inter-frame translation either by motion field or homography decomposition, and we use HOG features for registration on Google Map. We employ particle filter to conduct a coarse to fine search to localize the UAV. Offline test using aerial images collected by our quadrotor platform shows promising results as our approach eliminates the drift in dead-reckoning, and the small localization error indicates the superiority of our approach as a supplement to GPS.",0
"Our proposal presents a framework that utilizes Google Maps to aid UAV navigation when GPS is not available. By using geo-referenced navigation, we achieve accurate localization without the need for loop closures. Our approach initializes the UAV position through correlation, which is both simple and effective. We then utilize optical flow to predict the UAV's position in consecutive frames. During pose tracking, we obtain inter-frame translation through either motion field or homography decomposition, while registration on Google Maps is facilitated by HOG features. We employ a particle filter to conduct a coarse to fine search to localize the UAV. Offline testing using aerial images collected by our quadrotor platform demonstrates our approach's promising results, as it eliminates drift in dead-reckoning and achieves superior localization accuracy as a GPS supplement.",1
"Video classification is productive in many practical applications, and the recent deep learning has greatly improved its accuracy. However, existing works often model video frames indiscriminately, but from the view of motion, video frames can be decomposed into salient and non-salient areas naturally. Salient and non-salient areas should be modeled with different networks, for the former present both appearance and motion information, and the latter present static background information. To address this problem, in this paper, video saliency is predicted by optical flow without supervision firstly. Then two streams of 3D CNN are trained individually for raw frames and optical flow on salient areas, and another 2D CNN is trained for raw frames on non-salient areas. For the reason that these three streams play different roles for each class, the weights of each stream are adaptively learned for each class. Experimental results show that saliency-guided modeling and adaptively weighted learning can reinforce each other, and we achieve the state-of-the-art results.",0
"The accuracy of video classification has greatly improved with recent developments in deep learning, making it productive in many practical applications. However, current methods model video frames without considering their motion characteristics. It is natural to decompose video frames into salient and non-salient areas based on motion. Salient areas contain both appearance and motion information, while non-salient areas provide static background information. To overcome this issue, this study proposes predicting video saliency through optical flow without supervision. The salient areas are then modeled using two streams of 3D CNN trained on raw frames and optical flow, while non-salient areas are modeled using a separate 2D CNN trained on raw frames. The weights of each stream are adaptively learned for each class since they play different roles. The results show that saliency-guided modeling and adaptively weighted learning can complement each other, resulting in state-of-the-art performance.",1
"Video frame interpolation typically involves two steps: motion estimation and pixel synthesis. Such a two-step approach heavily depends on the quality of motion estimation. This paper presents a robust video frame interpolation method that combines these two steps into a single process. Specifically, our method considers pixel synthesis for the interpolated frame as local convolution over two input frames. The convolution kernel captures both the local motion between the input frames and the coefficients for pixel synthesis. Our method employs a deep fully convolutional neural network to estimate a spatially-adaptive convolution kernel for each pixel. This deep neural network can be directly trained end to end using widely available video data without any difficult-to-obtain ground-truth data like optical flow. Our experiments show that the formulation of video interpolation as a single convolution process allows our method to gracefully handle challenges like occlusion, blur, and abrupt brightness change and enables high-quality video frame interpolation.",0
"The typical process of video frame interpolation involves two steps: motion estimation and pixel synthesis, with the quality of motion estimation being crucial. However, this paper proposes a robust method for video frame interpolation that combines these steps into one process. The method involves using local convolution over two input frames to synthesize the interpolated frame, with the convolution kernel capturing both the local motion and coefficients for pixel synthesis. To achieve this, a deep fully convolutional neural network is employed to estimate a spatially-adaptive convolution kernel for each pixel. This network can be trained end to end using readily available video data, without the need for optical flow ground-truth data. By treating video interpolation as a single convolution process, our method can handle challenges such as occlusion, blur, and abrupt brightness change, resulting in high-quality video frame interpolation.",1
"We present a generative method to estimate 3D human motion and body shape from monocular video. Under the assumption that starting from an initial pose optical flow constrains subsequent human motion, we exploit flow to find temporally coherent human poses of a motion sequence. We estimate human motion by minimizing the difference between computed flow fields and the output of an artificial flow renderer. A single initialization step is required to estimate motion over multiple frames. Several regularization functions enhance robustness over time. Our test scenarios demonstrate that optical flow effectively regularizes the under-constrained problem of human shape and motion estimation from monocular video.",0
"A technique for determining 3D human motion and body shape using monocular video is presented. Our approach utilizes optical flow to identify temporally consistent human poses in a motion sequence, based on the assumption that optical flow constrains subsequent human motion from an initial pose. To estimate human motion, we minimize the disparity between the output of an artificial flow renderer and the computed flow fields. A solitary initialization step is needed to predict motion over several frames, and a number of regularization functions improve stability over time. Our experimental findings demonstrate that optical flow is an effective means of regulating the underdetermined issue of human shape and motion estimation from monocular video.",1
"In computer vision most iterative optimization algorithms, both sparse and dense, rely on a coarse and reliable dense initialization to bootstrap their optimization procedure. For example, dense optical flow algorithms profit massively in speed and robustness if they are initialized well in the basin of convergence of the used loss function. The same holds true for methods as sparse feature tracking when initial flow or depth information for new features at arbitrary positions is needed. This makes it extremely important to have techniques at hand that allow to obtain from only very few available measurements a dense but still approximative sketch of a desired 2D structure (e.g. depth maps, optical flow, disparity maps, etc.). The 2D map is regarded as sample from a 2D random process. The method presented here exploits the complete information given by the principal component analysis (PCA) of that process, the principal basis and its prior distribution. The method is able to determine a dense reconstruction from sparse measurement. When facing situations with only very sparse measurements, typically the number of principal components is further reduced which results in a loss of expressiveness of the basis. We overcome this problem and inject prior knowledge in a maximum a posterior (MAP) approach. We test our approach on the KITTI and the virtual KITTI datasets and focus on the interpolation of depth maps for driving scenes. The evaluation of the results show good agreement to the ground truth and are clearly better than results of interpolation by the nearest neighbor method which disregards statistical information.",0
"In computer vision, iterative optimization algorithms, whether sparse or dense, require a reliable dense initialization to begin the optimization procedure. For instance, dense optical flow algorithms benefit significantly in terms of speed and robustness if they are initialized in the basin of convergence of the loss function. The same applies to sparse feature tracking methods that require initial flow or depth information for new features at arbitrary positions. Therefore, it becomes crucial to have techniques to obtain a dense but approximate sketch of a desired 2D structure, such as depth maps, optical flow, and disparity maps, from very few available measurements. This approach considers the 2D map as a sample from a 2D random process, and it uses the principal component analysis (PCA) of that process, including the principal basis and its prior distribution, to determine a dense reconstruction from sparse measurement. When dealing with merely sparse measurements, the number of principal components usually gets reduced, decreasing the expressiveness of the basis. However, this problem is addressed using a maximum a posterior (MAP) approach that incorporates prior knowledge. The method is evaluated on the KITTI and virtual KITTI datasets, focusing on the interpolation of depth maps for driving scenes. The results show excellent agreement with the ground truth and are significantly better than those obtained by the nearest neighbor method, which ignores statistical information.",1
"Sparse-to-dense interpolation for optical flow is a fundamental phase in the pipeline of most of the leading optical flow estimation algorithms. The current state-of-the-art method for interpolation, EpicFlow, is a local average method based on an edge aware geodesic distance. We propose a new data-driven sparse-to-dense interpolation algorithm based on a fully convolutional network. We draw inspiration from the filling-in process in the visual cortex and introduce lateral dependencies between neurons and multi-layer supervision into our learning process. We also show the importance of the image contour to the learning process. Our method is robust and outperforms EpicFlow on competitive optical flow benchmarks with several underlying matching algorithms. This leads to state-of-the-art performance on the Sintel and KITTI 2012 benchmarks.",0
"The interpolation of optical flow from sparse-to-dense is a crucial step in the majority of top optical flow estimation algorithms. The current leading interpolation method, EpicFlow, employs a local averaging technique based on an edge-aware geodesic distance. In this study, we propose a novel data-driven sparse-to-dense interpolation algorithm that utilizes a fully convolutional network. Our approach is inspired by the visual cortex's filling-in process and incorporates lateral dependencies between neurons and multi-layer supervision into the learning process. We also emphasize the significance of the image contour in the learning process. Our method is robust and surpasses EpicFlow on competitive optical flow benchmarks with various underlying matching algorithms, achieving state-of-the-art performance on both the Sintel and KITTI 2012 benchmarks.",1
"Smile is one of the key elements in identifying emotions and present state of mind of an individual. In this work, we propose a cluster of approaches to classify posed and spontaneous smiles using deep convolutional neural network (CNN) face features, local phase quantization (LPQ), dense optical flow and histogram of gradient (HOG). Eulerian Video Magnification (EVM) is used for micro-expression smile amplification along with three normalization procedures for distinguishing posed and spontaneous smiles. Although the deep CNN face model is trained with large number of face images, HOG features outperforms this model for overall face smile classification task. Using EVM to amplify micro-expressions did not have a significant impact on classification accuracy, while the normalizing facial features improved classification accuracy. Unlike many manual or semi-automatic methodologies, our approach aims to automatically classify all smiles into either `spontaneous' or `posed' categories, by using support vector machines (SVM). Experimental results on large UvA-NEMO smile database show promising results as compared to other relevant methods.",0
"Identifying emotions and the current state of mind of an individual can be done through their smile, which is a crucial element. Our study proposes various techniques for classifying spontaneous and posed smiles using deep convolutional neural network (CNN) face features, local phase quantization (LPQ), dense optical flow, and histogram of gradient (HOG). We use Eulerian Video Magnification (EVM) to enhance micro-expression smiles and three normalization procedures to differentiate between posed and spontaneous smiles. Although the deep CNN face model is trained with a significant number of face images, the HOG features outperform this model in the overall face smile classification task. The use of EVM to amplify micro-expressions did not significantly affect the classification accuracy. However, normalizing facial features improved the classification accuracy. Our approach is different from manual or semi-automatic methodologies as it automatically classifies all smiles as either ""spontaneous"" or ""posed"" using support vector machines (SVM). Our experiments on a large UvA-NEMO smile database show promising results compared to other relevant methods.",1
"Motion detection in video is important for a number of applications and fields. In video surveillance, motion detection is an essential accompaniment to activity recognition for early warning systems. Robotics also has much to gain from motion detection and segmentation, particularly in high speed motion tracking for tactile systems. There are a myriad of techniques for detecting and masking motion in an image. Successful systems have used Gaussian Models to discern background from foreground in an image (motion from static imagery). However, particularly in the case of a moving camera or frame of reference, it is necessary to compensate for the motion of the camera when attempting to discern objects moving in the foreground. For example, it is possible to estimate motion of the camera through optical flow methods or temporal differencing and then compensate for this motion in a background subtraction model. We selection a method by Yi et al. using Dual-Mode Single Gaussian Models which does just this. We implement the technique in Intel's Thread Building Blocks (TBB) and NVIDIA's CUDA libraries. We then compare parallelization improvements with a theoretical analysis of speedups based on the characteristics of our selected model and attributes of both TBB and CUDA. We make our implementation available to the public.",0
"The detection of motion in video is crucial in various fields and applications. For instance, in video surveillance, motion detection is indispensable in conjunction with activity recognition to create early warning systems. Robotics can also benefit greatly from motion detection and segmentation, particularly in high-speed motion tracking for tactile systems. There are numerous techniques for detecting and masking motion in an image, including the use of Gaussian Models to differentiate between background and foreground in an image. However, when the camera or frame of reference is in motion, it becomes necessary to compensate for this motion when attempting to identify objects moving in the foreground. To achieve this, methods such as optical flow or temporal differencing can be used to estimate camera motion, and this motion can be compensated for in a background subtraction model. We have chosen the Dual-Mode Single Gaussian Models method by Yi et al. for this purpose. Our implementation of this technique is done using Intel's Thread Building Blocks (TBB) and NVIDIA's CUDA libraries. We have also analyzed the speedups of our selected model and the attributes of both TBB and CUDA to make parallelization improvements. Our implementation is available to the public.",1
"We show that the matching problem that underlies optical flow requires multiple strategies, depending on the amount of image motion and other factors. We then study the implications of this observation on training a deep neural network for representing image patches in the context of descriptor based optical flow. We propose a metric learning method, which selects suitable negative samples based on the nature of the true match. This type of training produces a network that displays multiple strategies depending on the input and leads to state of the art results on the KITTI 2012 and KITTI 2015 optical flow benchmarks.",0
"Our research demonstrates that the matching problem underlying optical flow necessitates various approaches that rely on factors such as the extent of image motion. We examine the impact of this finding on the development of a deep neural network that can represent image patches within the framework of descriptor-based optical flow. To achieve this, we introduce a metric learning technique that identifies appropriate negative samples based on the characteristics of the actual match. This form of training produces a network with multiple strategies that adapt to the input and yields outstanding outcomes on the KITTI 2012 and KITTI 2015 optical flow benchmarks.",1
"We introduce SceneNet RGB-D, expanding the previous work of SceneNet to enable large scale photorealistic rendering of indoor scene trajectories. It provides pixel-perfect ground truth for scene understanding problems such as semantic segmentation, instance segmentation, and object detection, and also for geometric computer vision problems such as optical flow, depth estimation, camera pose estimation, and 3D reconstruction. Random sampling permits virtually unlimited scene configurations, and here we provide a set of 5M rendered RGB-D images from over 15K trajectories in synthetic layouts with random but physically simulated object poses. Each layout also has random lighting, camera trajectories, and textures. The scale of this dataset is well suited for pre-training data-driven computer vision techniques from scratch with RGB-D inputs, which previously has been limited by relatively small labelled datasets in NYUv2 and SUN RGB-D. It also provides a basis for investigating 3D scene labelling tasks by providing perfect camera poses and depth data as proxy for a SLAM system. We host the dataset at http://robotvault.bitbucket.io/scenenet-rgbd.html",0
"SceneNet RGB-D is an extension of SceneNet that allows for the creation of large-scale, photorealistic indoor scenes. The dataset provides accurate ground truth for semantic segmentation, object detection, optical flow, camera pose estimation, depth estimation, and 3D reconstruction. Through random sampling, it is possible to create an unlimited number of scene configurations. The dataset contains 5 million rendered RGB-D images from over 15,000 trajectories, with physically simulated object poses, random lighting, camera trajectories, and textures. This dataset is ideal for pre-training data-driven computer vision models from scratch with RGB-D inputs, which was previously limited by small datasets such as NYUv2 and SUN RGB-D. It also provides a foundation for exploring 3D scene labeling tasks, as it includes perfect camera poses and depth data as a proxy for a SLAM system. The dataset is available at http://robotvault.bitbucket.io/scenenet-rgbd.html.",1
"In this paper we present a decomposition algorithm for computation of the spatial-temporal optical flow of a dynamic image sequence. We consider several applications, such as the extraction of temporal motion features and motion detection in dynamic sequences under varying illumination conditions, such as they appear for instance in psychological flickering experiments. For the numerical implementation we are solving an integro-differential equation by a fixed point iteration. For comparison purposes we use a standard time dependent optical flow algorithm, which in contrast to our method, constitutes in solving a spatial-temporal differential equation.",0
"The focus of this paper is to introduce an algorithm that breaks down the computation of spatial-temporal optical flow for a dynamic image sequence. The algorithm can be applied to various purposes, including extracting temporal motion features and detecting motion in dynamic sequences that undergo different illumination conditions such as those in psychological flickering experiments. We implement the algorithm numerically by using a fixed point iteration to solve an integro-differential equation. To compare our method, we also utilize a standard time-dependent optical flow algorithm that solves a spatial-temporal differential equation.",1
"This paper describes the development of a novel algorithm to tackle the problem of real-time video stabilization for unmanned aerial vehicles (UAVs). There are two main components in the algorithm: (1) By designing a suitable model for the global motion of UAV, the proposed algorithm avoids the necessity of estimating the most general motion model, projective transformation, and considers simpler motion models, such as rigid transformation and similarity transformation. (2) To achieve a high processing speed, optical-flow based tracking is employed in lieu of conventional tracking and matching methods used by state-of-the-art algorithms. These two new ideas resulted in a real-time stabilization algorithm, developed over two phases. Stage I considers processing the whole sequence of frames in the video while achieving an average processing speed of 50fps on several publicly available benchmark videos. Next, Stage II undertakes the task of real-time video stabilization using a multi-threading implementation of the algorithm designed in Stage I.",0
"The aim of this paper is to present a new algorithm that addresses the issue of real-time video stabilization for unmanned aerial vehicles (UAVs). The algorithm comprises of two main components: (1) Rather than estimating the most general motion model, projective transformation, a suitable global motion model is designed for UAV, which considers simpler motion models like rigid transformation and similarity transformation. (2) The algorithm employs optical-flow based tracking to achieve high processing speed instead of conventional tracking and matching methods used by existing algorithms. These ideas resulted in a real-time stabilization algorithm that was developed in two phases. During Stage I, the entire sequence of frames in the video was processed, achieving an average processing speed of 50fps on various publicly available benchmark videos. In Stage II, a multi-threading implementation of the algorithm designed in Stage I was used to undertake the real-time video stabilization task.",1
"Robust visual tracking is a challenging computer vision problem, with many real-world applications. Most existing approaches employ hand-crafted appearance features, such as HOG or Color Names. Recently, deep RGB features extracted from convolutional neural networks have been successfully applied for tracking. Despite their success, these features only capture appearance information. On the other hand, motion cues provide discriminative and complementary information that can improve tracking performance. Contrary to visual tracking, deep motion features have been successfully applied for action recognition and video classification tasks. Typically, the motion features are learned by training a CNN on optical flow images extracted from large amounts of labeled videos.   This paper presents an investigation of the impact of deep motion features in a tracking-by-detection framework. We further show that hand-crafted, deep RGB, and deep motion features contain complementary information. To the best of our knowledge, we are the first to propose fusing appearance information with deep motion features for visual tracking. Comprehensive experiments clearly suggest that our fusion approach with deep motion features outperforms standard methods relying on appearance information alone.",0
"The task of robust visual tracking poses a significant challenge in the field of computer vision and has numerous practical applications. The current methods mainly employ manually designed appearance features like HOG or Color Names. However, the advent of convolutional neural networks has enabled the successful extraction of deep RGB features for tracking. Although these features have proven useful, they only capture appearance information. In contrast, motion cues offer unique and complementary data that can enhance tracking performance. While deep motion features have been effectively applied to video classification and action recognition, they are yet to be explored in visual tracking. This study investigates the potential impact of deep motion features in a tracking-by-detection framework and establishes that they contain complementary information to both hand-crafted and deep RGB features. Notably, we are the first to propose the fusion of appearance information with deep motion features for visual tracking, and our experimental results demonstrate that our approach surpasses traditional methods that rely solely on appearance information.",1
"Micro-facial expressions are regarded as an important human behavioural event that can highlight emotional deception. Spotting these movements is difficult for humans and machines, however research into using computer vision to detect subtle facial expressions is growing in popularity. This paper proposes an individualised baseline micro-movement detection method using 3D Histogram of Oriented Gradients (3D HOG) temporal difference method. We define a face template consisting of 26 regions based on the Facial Action Coding System (FACS). We extract the temporal features of each region using 3D HOG. Then, we use Chi-square distance to find subtle facial motion in the local regions. Finally, an automatic peak detector is used to detect micro-movements above the newly proposed adaptive baseline threshold. The performance is validated on two FACS coded datasets: SAMM and CASME II. This objective method focuses on the movement of the 26 face regions. When comparing with the ground truth, the best result was an AUC of 0.7512 and 0.7261 on SAMM and CASME II, respectively. The results show that 3D HOG outperformed for micro-movement detection, compared to state-of-the-art feature representations: Local Binary Patterns in Three Orthogonal Planes and Histograms of Oriented Optical Flow.",0
"Detecting micro-facial expressions can be a challenging task for both humans and computers, but it is crucial in identifying emotional deception. As computer vision continues to advance, there is growing interest in using it to detect subtle facial expressions. This study proposes a personalized method for detecting micro-movements using the 3D Histogram of Oriented Gradients (3D HOG) temporal difference approach. The method involves creating a face template consisting of 26 regions based on the Facial Action Coding System (FACS), extracting temporal features of each region using 3D HOG, utilizing Chi-square distance to identify subtle facial motion in local regions, and using an automatic peak detector to detect micro-movements above an adaptive baseline threshold. The study was validated on two FACS coded datasets, SAMM and CASME II, and achieved an AUC of 0.7512 and 0.7261, respectively. The results indicate that the proposed 3D HOG method is superior to other state-of-the-art feature representations such as Local Binary Patterns in Three Orthogonal Planes and Histograms of Oriented Optical Flow.",1
"High dynamic range (HDR) image synthesis from multiple low dynamic range (LDR) exposures continues to be actively researched. The extension to HDR video synthesis is a topic of significant current interest due to potential cost benefits. For HDR video, a stiff practical challenge presents itself in the form of accurate correspondence estimation of objects between video frames. In particular, loss of data resulting from poor exposures and varying intensity make conventional optical flow methods highly inaccurate. We avoid exact correspondence estimation by proposing a statistical approach via maximum a posterior (MAP) estimation, and under appropriate statistical assumptions and choice of priors and models, we reduce it to an optimization problem of solving for the foreground and background of the target frame. We obtain the background through rank minimization and estimate the foreground via a novel multiscale adaptive kernel regression technique, which implicitly captures local structure and temporal motion by solving an unconstrained optimization problem. Extensive experimental results on both real and synthetic datasets demonstrate that our algorithm is more capable of delivering high-quality HDR videos than current state-of-the-art methods, under both subjective and objective assessments. Furthermore, a thorough complexity analysis reveals that our algorithm achieves better complexity-performance trade-off than conventional methods.",0
"Research on synthesizing high dynamic range (HDR) images from multiple low dynamic range (LDR) exposures is ongoing, with a current focus on extending the process to HDR video synthesis in order to achieve potential cost benefits. However, accurately estimating the correspondence of objects between video frames presents a significant challenge due to varying intensity and loss of data resulting from poor exposures. We propose a statistical approach using maximum a posterior (MAP) estimation to avoid the need for exact correspondence estimation. By making appropriate statistical assumptions and selecting priors and models, we reduce the problem to an optimization task of solving for the foreground and background of the target frame. We obtain the background through rank minimization and estimate the foreground using a novel multiscale adaptive kernel regression technique that captures local structure and temporal motion through unconstrained optimization. Our algorithm outperforms current state-of-the-art methods in delivering high-quality HDR videos under subjective and objective assessments, as demonstrated by extensive experimental results on real and synthetic datasets. Additionally, our algorithm achieves a better complexity-performance trade-off than conventional methods, as revealed by thorough complexity analysis.",1
"The FlowNet demonstrated that optical flow estimation can be cast as a learning problem. However, the state of the art with regard to the quality of the flow has still been defined by traditional methods. Particularly on small displacements and real-world data, FlowNet cannot compete with variational methods. In this paper, we advance the concept of end-to-end learning of optical flow and make it work really well. The large improvements in quality and speed are caused by three major contributions: first, we focus on the training data and show that the schedule of presenting data during training is very important. Second, we develop a stacked architecture that includes warping of the second image with intermediate optical flow. Third, we elaborate on small displacements by introducing a sub-network specializing on small motions. FlowNet 2.0 is only marginally slower than the original FlowNet but decreases the estimation error by more than 50%. It performs on par with state-of-the-art methods, while running at interactive frame rates. Moreover, we present faster variants that allow optical flow computation at up to 140fps with accuracy matching the original FlowNet.",0
"The FlowNet approach showed that optical flow estimation can be accomplished through learning. However, traditional methods still define the current standard for flow quality, particularly for small displacements and real-world data, where FlowNet falls short compared to variational methods. In this paper, we improve upon the concept of end-to-end learning of optical flow and achieve remarkable results. Our advancements in quality and speed are due to three main contributions: first, we emphasize the significance of the training data presentation schedule. Second, we devise a stacked architecture that involves warping the second image with intermediate optical flow. Third, we address small displacements by introducing a sub-network specialized in small motions. FlowNet 2.0 is only slightly slower than the original, yet reduces estimation error by over 50%. It performs equally well as state-of-the-art methods and operates at interactive frame rates. Furthermore, we present faster variations that enable optical flow computation at up to 140fps with accuracy that matches that of the original FlowNet.",1
"This paper proposes a novel MAP inference framework for Markov Random Field (MRF) in parallel computing environments. The inference framework, dubbed Swarm Fusion, is a natural generalization of the Fusion Move method. Every thread (in a case of multi-threading environments) maintains and updates a solution. At each iteration, a thread can generate arbitrary number of solution proposals and take arbitrary number of concurrent solutions from the other threads to perform multi-way fusion in updating its solution. The framework is general, making popular existing inference techniques such as alpha-expansion, fusion move, parallel alpha-expansion, and hierarchical fusion, its special cases. We have evaluated the effectiveness of our approach against competing methods on three problems of varying difficulties, in particular, the stereo, the optical flow, and the layered depthmap estimation problems.",0
"In this paper, a new framework for performing MAP inference on Markov Random Field (MRF) in parallel computing environments is proposed. The framework is called Swarm Fusion and is an extension of the Fusion Move technique. In multi-threading environments, each thread maintains and updates a solution. At each iteration, a thread can generate multiple solution proposals and incorporate multiple concurrent solutions from other threads to update its solution. The framework is versatile and can be adapted to popular inference techniques such as alpha-expansion, fusion move, parallel alpha-expansion, and hierarchical fusion. The effectiveness of the Swarm Fusion approach is evaluated against other methods on three problems of varying complexities, including stereo, optical flow, and layered depthmap estimation.",1
"Surveillance video parsing, which segments the video frames into several labels, e.g., face, pants, left-leg, has wide applications. However,pixel-wisely annotating all frames is tedious and inefficient. In this paper, we develop a Single frame Video Parsing (SVP) method which requires only one labeled frame per video in training stage. To parse one particular frame, the video segment preceding the frame is jointly considered. SVP (1) roughly parses the frames within the video segment, (2) estimates the optical flow between frames and (3) fuses the rough parsing results warped by optical flow to produce the refined parsing result. The three components of SVP, namely frame parsing, optical flow estimation and temporal fusion are integrated in an end-to-end manner. Experimental results on two surveillance video datasets show the superiority of SVP over state-of-the-arts.",0
"The process of segmenting surveillance video frames into different labels, such as face, pants, and left-leg, is a useful technique with numerous applications. However, annotating each frame on a pixel-by-pixel basis can be a time-consuming and inefficient task. To address this issue, the authors of this paper have developed a method called Single frame Video Parsing (SVP), which only requires one labeled frame per video during the training stage. SVP works by taking into account the video segment that precedes the frame being parsed, and uses this information to roughly parse the frames within the segment, estimate the optical flow between frames, and fuse the rough parsing results to produce a refined parsing result. All three components of SVP - frame parsing, optical flow estimation, and temporal fusion - are integrated in an end-to-end manner. The authors have tested their method on two surveillance video datasets and found that SVP outperforms state-of-the-art methods.",1
"We learn to compute optical flow by combining a classical spatial-pyramid formulation with deep learning. This estimates large motions in a coarse-to-fine approach by warping one image of a pair at each pyramid level by the current flow estimate and computing an update to the flow. Instead of the standard minimization of an objective function at each pyramid level, we train one deep network per level to compute the flow update. Unlike the recent FlowNet approach, the networks do not need to deal with large motions; these are dealt with by the pyramid. This has several advantages. First, our Spatial Pyramid Network (SPyNet) is much simpler and 96% smaller than FlowNet in terms of model parameters. This makes it more efficient and appropriate for embedded applications. Second, since the flow at each pyramid level is small (< 1 pixel), a convolutional approach applied to pairs of warped images is appropriate. Third, unlike FlowNet, the learned convolution filters appear similar to classical spatio-temporal filters, giving insight into the method and how to improve it. Our results are more accurate than FlowNet on most standard benchmarks, suggesting a new direction of combining classical flow methods with deep learning.",0
"To compute optical flow, we have developed a new method that combines classical spatial-pyramid techniques with deep learning. Our approach involves estimating large motions in a coarse-to-fine manner by warping one image from a pair and computing an update to the flow. Rather than minimizing an objective function at each pyramid level, we train a deep network per level to compute the flow update. Our Spatial Pyramid Network (SPyNet) is much simpler and smaller than previous approaches, making it more efficient and suitable for embedded applications. Additionally, since the flow at each pyramid level is small, a convolutional approach applied to pairs of warped images is appropriate. Our results outperform previous benchmarks, indicating the potential of combining classical flow methods with deep learning.",1
"Finding visual correspondence between local features is key to many computer vision problems. While defining features with larger contextual scales usually implies greater discriminativeness, it could also lead to less spatial accuracy of the features. We propose AutoScaler, a scale-attention network to explicitly optimize this trade-off in visual correspondence tasks. Our network consists of a weight-sharing feature network to compute multi-scale feature maps and an attention network to combine them optimally in the scale space. This allows our network to have adaptive receptive field sizes over different scales of the input. The entire network is trained end-to-end in a siamese framework for visual correspondence tasks. Our method achieves favorable results compared to state-of-the-art methods on challenging optical flow and semantic matching benchmarks, including Sintel, KITTI and CUB-2011. We also show that our method can generalize to improve hand-crafted descriptors (e.g Daisy) on general visual correspondence tasks. Finally, our attention network can generate visually interpretable scale attention maps.",0
"The ability to find correspondence between local features is crucial for many computer vision problems. Although defining features with larger contextual scales may increase their discriminativeness, it may also reduce the spatial accuracy of the features. To address this issue, we introduce AutoScaler, a scale-attention network that optimizes the trade-off between discriminativeness and spatial accuracy in visual correspondence tasks. Our network comprises a weight-sharing feature network that generates multi-scale feature maps and an attention network that combines these maps optimally in the scale space. This enables our network to adaptively adjust its receptive field sizes over different input scales. We trained the entire network end-to-end in a siamese framework for visual correspondence tasks and achieved promising results compared to state-of-the-art methods on challenging optical flow and semantic matching benchmarks, such as Sintel, KITTI, and CUB-2011. Additionally, our method can improve hand-crafted descriptors (e.g., Daisy) on general visual correspondence tasks. Finally, our attention network can generate visually interpretable scale attention maps.",1
"In this paper, two simple principal component regression methods for estimating the optical flow between frames of video sequences according to a pel-recursive manner are introduced. These are easy alternatives to dealing with mixtures of motion vectors in addition to the lack of prior information on spatial-temporal statistics (although they are supposed to be normal in a local sense). The 2D motion vector estimation approaches take into consideration simple image properties and are used to harmonize regularized least square estimates. Their main advantage is that no knowledge of the noise distribution is necessary, although there is an underlying assumption of localized smoothness. Preliminary experiments indicate that this approach provides robust estimates of the optical flow.",0
"The article introduces two uncomplicated principal component regression techniques for calculating the optical flow between consecutive frames of video sequences using a pel-recursive method. These methods provide a straightforward solution for handling motion vector combinations and a lack of prior knowledge about spatial-temporal statistics. While they do assume local smoothness, they consider basic image features and are used to reconcile regularized least square calculations. One significant benefit is that it does not require understanding the noise distribution. Preliminary trials suggest that this approach produces resilient estimates of the optical flow.",1
"The computation of 2-D optical flow by means of regularized pel-recursive algorithms raises a host of issues, which include the treatment of outliers, motion discontinuities and occlusion among other problems. We propose a new approach which allows us to deal with these issues within a common framework. Our approach is based on the use of a technique called Generalized Cross-Validation to estimate the best regularization scheme for a given pixel. In our model, the regularization parameter is a matrix whose entries can account for diverse sources of error. The estimation of the motion vectors takes into consideration local properties of the image following a spatially adaptive approach where each moving pixel is supposed to have its own regularization matrix. Preliminary experiments indicate that this approach provides robust estimates of the optical flow.",0
"A variety of challenges arise when applying regularized pel-recursive algorithms to compute 2-D optical flow, such as how to handle outliers, motion discontinuities, and occlusion. To address these issues, we propose a novel methodology that tackles them all in a unified manner. Our strategy employs Generalized Cross-Validation to identify the optimal regularization scheme for a given pixel. We represent the regularization parameter as a matrix, with entries that can account for different types of errors. The motion vector estimation accounts for local image properties and adapts spatially by assigning a unique regularization matrix to each moving pixel. Our initial tests demonstrate that this approach produces reliable optical flow estimates.",1
"The pel-recursive computation of 2-D optical flow has been extensively studied in computer vision to estimate motion from image sequences, but it still raises a wealth of issues, such as the treatment of outliers, motion discontinuities and occlusion. It relies on spatio-temporal brightness variations due to motion. Our proposed adaptive regularized approach deals with these issues within a common framework. It relies on the use of a data-driven technique called Mixed Norm (MN) to estimate the best motion vector for a given pixel. In our model, various types of noise can be handled, representing different sources of error. The motion vector estimation takes into consideration local image properties and it results from the minimization of a mixed norm functional with a regularization parameter depending on the kurtosis. This parameter determines the relative importance of the fourth norm and makes the functional convex. The main advantage of the developed procedure is that no knowledge of the noise distribution is necessary. Experiments indicate that this approach provides robust estimates of the optical flow.",0
"The computation of 2-D optical flow using pel-recursive methods has been heavily researched in computer vision as a means of motion estimation from image sequences. However, it remains problematic due to issues such as outliers, motion discontinuities, and occlusion. The technique relies on spatio-temporal brightness variations caused by motion. Our proposed method solves these issues within a unified framework utilizing an adaptive regularized approach. This approach employs a data-driven technique called Mixed Norm (MN) to estimate the optimal motion vector for each pixel, thereby handling various types of noise and sources of error. Our model considers local image characteristics and minimizes a mixed norm functional with a regularization parameter that depends on kurtosis. This parameter dictates the importance of the fourth norm and ensures convexity of the functional. An important advantage of our procedure is that it requires no prior knowledge of the noise distribution. Experiments have demonstrated that our approach produces reliable estimates of optical flow.",1
"This article describes the implementation of the joint motion estimation and image reconstruction framework presented by Burger, Dirks and Sch\""onlieb and extends this framework to large-scale motion between consecutive image frames. The variational framework uses displacements between consecutive frames based on the optical flow approach to improve the image reconstruction quality on the one hand and the motion estimation quality on the other. The energy functional consists of a data-fidelity term with a general operator that connects the input sequence to the solution, it has a total variation term for the image sequence and is connected to the underlying flow using an optical flow term. Additional spatial regularity for the flow is modeled by a total variation regularizer for both components of the flow. The numerical minimization is performed in an alternating manner using primal-dual techniques. The resulting schemes are presented as pseudo-code together with a short numerical evaluation.",0
"In this article, the joint motion estimation and image reconstruction framework by Burger, Dirks, and Schönlieb is discussed, and its application is extended to large-scale motion between consecutive image frames. The framework utilizes displacements between consecutive frames using the optical flow approach to enhance the quality of image reconstruction and motion estimation. The energy functional comprises a data-fidelity term with a general operator connecting the input sequence to the solution, a total variation term for the image sequence, and an optical flow term linking it to the underlying flow. A total variation regularizer is used for both components of the flow to model additional spatial regularity. Numerical minimization is carried out alternatively using primal-dual techniques. The resulting schemes are presented as pseudo-code, along with a brief numerical evaluation.",1
"Conventional approaches to image de-fencing use multiple adjacent frames for segmentation of fences in the reference image and are limited to restoring images of static scenes only. In this paper, we propose a de-fencing algorithm for images of dynamic scenes using an occlusion-aware optical flow method. We divide the problem of image de-fencing into the tasks of automated fence segmentation from a single image, motion estimation under known occlusions and fusion of data from multiple frames of a captured video of the scene. Specifically, we use a pre-trained convolutional neural network to segment fence pixels from a single image. The knowledge of spatial locations of fences is used to subsequently estimate optical flow in the occluded frames of the video for the final data fusion step. We cast the fence removal problem in an optimization framework by modeling the formation of the degraded observations. The inverse problem is solved using fast iterative shrinkage thresholding algorithm (FISTA). Experimental results show the effectiveness of proposed algorithm.",0
"The conventional techniques for image de-fencing rely on using several contiguous frames to segment fences in reference images, but they are restricted to restoring pictures of stationary scenes. This article introduces a de-fencing method for dynamic scene images that utilizes an occlusion-aware optical flow technique. We break down the image de-fencing challenge into three tasks: automated fence segmentation from a single picture, motion estimation under known occlusions, and data fusion from several frames of a captured video of the scene. To segment fence pixels from a single image, we employ a pre-trained convolutional neural network. We leverage this knowledge of fence locations to estimate optical flow in the occluded frames of the video for the final data fusion step. We frame the fence removal issue within an optimization framework that models the formation of the degraded observations. To solve the inverse problem, we use the fast iterative shrinkage thresholding algorithm (FISTA). Experimental findings demonstrate the effectiveness of the proposed approach.",1
"Fine-scale short-term cloud motion prediction is needed for several applications, including solar energy generation and satellite communications. In tropical regions such as Singapore, clouds are mostly formed by convection; they are very localized, and evolve quickly. We capture hemispherical images of the sky at regular intervals of time using ground-based cameras. They provide a high resolution and localized cloud images. We use two successive frames to compute optical flow and predict the future location of clouds. We achieve good prediction accuracy for a lead time of up to 5 minutes.",0
"Accurate prediction of cloud motion on a small scale and in the short term is essential for various purposes, such as generating solar energy and facilitating satellite communications. In tropical areas like Singapore, clouds are predominantly generated through convection, which makes them highly localized and rapidly changing. To obtain high-resolution images of these clouds, we employ ground-based cameras to capture hemispherical views of the sky at regular intervals. We then leverage two consecutive frames to calculate optical flow and forecast the direction of cloud movement. Our method achieves a reliable prediction accuracy for up to five minutes in advance.",1
"In the absence of pedestrian crossing lights, finding a safe moment to cross the road is often hazardous and challenging, especially for people with visual impairments. We present a reliable low-cost solution, an Android device attached to a traffic sign or lighting pole near the crossing, indicating whether it is safe to cross the road. The indication can be by sound, display, vibration, and various communication modalities provided by the Android device. The integral system camera is aimed at approaching traffic. Optical flow is computed from the incoming video stream, and projected onto an influx map, automatically acquired during a brief training period. The crossing safety is determined based on a 1-dimensional temporal signal derived from the projection. We implemented the complete system on a Samsung Galaxy K-Zoom Android smartphone, and obtained real-time operation. The system achieves promising experimental results, providing pedestrians with sufficiently early warning of approaching vehicles. The system can serve as a stand-alone safety device, that can be installed where pedestrian crossing lights are ruled out. Requiring no dedicated infrastructure, it can be powered by a solar panel and remotely maintained via the cellular network.",0
"When pedestrian crossing lights are absent, crossing the road can be dangerous and difficult, particularly for individuals with visual impairments. We introduce a cost-effective and dependable solution that involves attaching an Android device to a traffic sign or lighting pole positioned near the crossing to indicate whether it's a safe time to cross. The Android device offers multiple communication modes, including sound, display, and vibration. The device's built-in camera is directed at oncoming traffic, and optical flow is evaluated from the incoming video stream, which is then projected onto an influx map obtained via brief training. The device assesses the safety of the crossing based on a 1-dimensional temporal signal produced from the projection. We developed the entire system on a Samsung Galaxy K-Zoom Android smartphone, and it functions in real-time. The system provides pedestrians with timely warnings of approaching vehicles and can be used as a standalone safety mechanism that can be installed in places where pedestrian crossing lights are not available. It is powered by a solar panel and can be remotely maintained via the cellular network, requiring no dedicated infrastructure.",1
"Convex relaxations of nonconvex multilabel problems have been demonstrated to produce superior (provably optimal or near-optimal) solutions to a variety of classical computer vision problems. Yet, they are of limited practical use as they require a fine discretization of the label space, entailing a huge demand in memory and runtime. In this work, we propose the first sublabel accurate convex relaxation for vectorial multilabel problems. The key idea is that we approximate the dataterm of the vectorial labeling problem in a piecewise convex (rather than piecewise linear) manner. As a result we have a more faithful approximation of the original cost function that provides a meaningful interpretation for the fractional solutions of the relaxed convex problem. In numerous experiments on large-displacement optical flow estimation and on color image denoising we demonstrate that the computed solutions have superior quality while requiring much lower memory and runtime.",0
"Nonconvex multilabel problems can be solved using convex relaxations, which have been proven to produce optimal or near-optimal solutions for various computer vision problems. However, these relaxations are not practical due to the need for a fine label space discretization, leading to significant memory and runtime requirements. In this study, we introduce a sublabel accurate convex relaxation for vectorial multilabel problems. By approximating the dataterm in a piecewise convex manner, we obtain a more accurate representation of the cost function that allows for a meaningful interpretation of fractional solutions. Our experiments on large-displacement optical flow estimation and color image denoising demonstrate that our approach yields superior solutions with much lower memory and runtime requirements.",1
"Egocentric, or first-person vision which became popular in recent years with an emerge in wearable technology, is different than exocentric (third-person) vision in some distinguishable ways, one of which being that the camera wearer is generally not visible in the video frames. Recent work has been done on action and object recognition in egocentric videos, as well as work on biometric extraction from first-person videos. Height estimation can be a useful feature for both soft-biometrics and object tracking. Here, we propose a method of estimating the height of an egocentric camera without any calibration or reference points. We used both traditional computer vision approaches and deep learning in order to determine the visual cues that results in best height estimation. Here, we introduce a framework inspired by two stream networks comprising of two Convolutional Neural Networks, one based on spatial information, and one based on information given by optical flow in a frame. Given an egocentric video as an input to the framework, our model yields a height estimate as an output. We also incorporate late fusion to learn a combination of temporal and spatial cues. Comparing our model with other methods we used as baselines, we achieve height estimates for videos with a Mean Average Error of 14.04 cm over a range of 103 cm of data, and classification accuracy for relative height (tall, medium or short) up to 93.75% where chance level is 33%.",0
"The rise of wearable technology has led to the popularity of egocentric, or first-person perspective, which differs from exocentric, or third-person perspective, as the camera wearer is typically not visible in the video frames. Recent studies have focused on recognizing actions and objects in egocentric videos, as well as extracting biometric data from first-person footage. Height estimation is a useful feature for soft-biometrics and object tracking, and we present a method for estimating the height of an egocentric camera without calibration or reference points. Our approach combines traditional computer vision techniques with deep learning, utilizing two Convolutional Neural Networks to process spatial and optical flow information. We incorporate late fusion to learn from temporal and spatial cues and achieve a Mean Average Error of 14.04 cm and a classification accuracy of up to 93.75% for relative height.",1
"We propose a large displacement optical flow method that introduces a new strategy to compute a good local minimum of any optical flow energy functional. The method requires a given set of discrete matches, which can be extremely sparse, and an energy functional which locally guides the interpolation from those matches. In particular, the matches are used to guide a structured coordinate-descent of the energy functional around these keypoints. It results in a two-step minimization method at the finest scale which is very robust to the inevitable outliers of the sparse matcher and able to capture large displacements of small objects. Its benefits over other variational methods that also rely on a set of sparse matches are its robustness against very few matches, high levels of noise and outliers. We validate our proposal using several optical flow variational models. The results consistently outperform the coarse-to-fine approaches and achieve good qualitative and quantitative performance on the standard optical flow benchmarks.",0
"Our proposed method for large displacement optical flow introduces a novel approach for computing a reliable local minimum for any optical flow energy functional. The approach is based on a set of discrete matches, which can be sparse, and an energy functional that guides the interpolation from those matches. Specifically, we use the matches to facilitate a structured coordinate-descent of the energy functional around these keypoints, resulting in a two-step minimization method that is highly resilient to the sparse matcher's outliers and capable of capturing large displacements of small objects. Unlike other variational methods that rely on sparse matches, our approach is highly robust against very few matches, high levels of noise, and outliers. We validate our method using various optical flow variational models, consistently outperforming coarse-to-fine approaches and achieving excellent qualitative and quantitative performance on standard optical flow benchmarks.",1
"Human actions are comprised of a sequence of poses. This makes videos of humans a rich and dense source of human poses. We propose an unsupervised method to learn pose features from videos that exploits a signal which is complementary to appearance and can be used as supervision: motion. The key idea is that humans go through poses in a predictable manner while performing actions. Hence, given two poses, it should be possible to model the motion that caused the change between them. We represent each of the poses as a feature in a CNN (Appearance ConvNet) and generate a motion encoding from optical flow maps using a separate CNN (Motion ConvNet). The data for this task is automatically generated allowing us to train without human supervision. We demonstrate the strength of the learned representation by finetuning the trained model for Pose Estimation on the FLIC dataset, for static image action recognition on PASCAL and for action recognition in videos on UCF101 and HMDB51.",0
"Videos of humans provide a valuable source of information about human poses, as these actions are made up of a series of poses. To harness this information, we have developed an unsupervised method for learning pose features from videos. Our approach leverages a signal that is different from appearance and can serve as a form of supervision - motion. We believe that as humans transition between poses, there is a predictable pattern of motion that occurs. This idea forms the basis of our method. We use separate convolutional neural networks (CNNs) to represent each pose (Appearance ConvNet) and generate a motion encoding from optical flow maps (Motion ConvNet). The data for training is generated automatically, eliminating the need for human supervision. The resulting learned representation is robust, as demonstrated by our successful use of the trained model for Pose Estimation on the FLIC dataset, for static image action recognition on PASCAL, and for action recognition in videos on UCF101 and HMDB51.",1
"This work advocates Eulerian motion representation learning over the current standard Lagrangian optical flow model. Eulerian motion is well captured by using phase, as obtained by decomposing the image through a complex-steerable pyramid. We discuss the gain of Eulerian motion in a set of practical use cases: (i) action recognition, (ii) motion prediction in static images, (iii) motion transfer in static images and, (iv) motion transfer in video. For each task we motivate the phase-based direction and provide a possible approach.",0
"The paper argues in favor of learning Eulerian motion representation instead of the current Lagrangian optical flow model. The use of phase obtained by breaking down the image through a complex-steerable pyramid is an effective means of capturing Eulerian motion. The benefits of Eulerian motion are explored through a range of use cases, including action recognition, motion prediction in still images, motion transfer in still images, and motion transfer in video. For each task, the direction based on phase is explained and a potential method is suggested.",1
"The video and action classification have extremely evolved by deep neural networks specially with two stream CNN using RGB and optical flow as inputs and they present outstanding performance in terms of video analysis. One of the shortcoming of these methods is handling motion information extraction which is done out side of the CNNs and relatively time consuming also on GPUs. So proposing end-to-end methods which are exploring to learn motion representation, like 3D-CNN can achieve faster and accurate performance. We present some novel deep CNNs using 3D architecture to model actions and motion representation in an efficient way to be accurate and also as fast as real-time. Our new networks learn distinctive models to combine deep motion features into appearance model via learning optical flow features inside the network.",0
"Deep neural networks, particularly two stream CNNs that use RGB and optical flow as inputs, have significantly advanced video and action classification and exhibit exceptional performance in video analysis. However, these methods are limited in their ability to extract motion information, which is conducted outside of the CNNs and can be time-consuming on GPUs. To address this issue, end-to-end methods that aim to learn motion representation, such as 3D-CNN, have been proposed to achieve faster and more precise performance. Our research proposes novel deep CNNs that utilize 3D architecture to effectively model actions and motion representation in real-time with high accuracy. Our new networks learn unique models that integrate deep motion features into appearance models through the learning of optical flow features within the network.",1
"We describe a new spatio-temporal video autoencoder, based on a classic spatial image autoencoder and a novel nested temporal autoencoder. The temporal encoder is represented by a differentiable visual memory composed of convolutional long short-term memory (LSTM) cells that integrate changes over time. Here we target motion changes and use as temporal decoder a robust optical flow prediction module together with an image sampler serving as built-in feedback loop. The architecture is end-to-end differentiable. At each time step, the system receives as input a video frame, predicts the optical flow based on the current observation and the LSTM memory state as a dense transformation map, and applies it to the current frame to generate the next frame. By minimising the reconstruction error between the predicted next frame and the corresponding ground truth next frame, we train the whole system to extract features useful for motion estimation without any supervision effort. We present one direct application of the proposed framework in weakly-supervised semantic segmentation of videos through label propagation using optical flow.",0
"A new spatio-temporal video autoencoder is explained, which is based on a spatial image autoencoder and a nested temporal autoencoder. The temporal encoder is represented by a differentiable visual memory that is made up of convolutional long short-term memory (LSTM) cells that integrate changes over time, with a focus on motion changes. The temporal decoder consists of a robust optical flow prediction module and an image sampler that serves as a feedback loop. The architecture is end-to-end differentiable, and at each time step, the system receives a video frame as input, predicts the optical flow based on the current observation and the LSTM memory state as a dense transformation map, and applies it to the current frame to generate the next frame. The system is trained to extract features useful for motion estimation by minimizing the reconstruction error between the predicted next frame and the corresponding ground truth next frame, without any supervision effort. One direct application of this framework is in weakly-supervised semantic segmentation of videos through label propagation using optical flow.",1
"Information of time differentiation is extremely important cue for a motion representation. We have applied first-order differential velocity from a positional information, moreover we believe that second-order differential acceleration is also a significant feature in a motion representation. However, an acceleration image based on a typical optical flow includes motion noises. We have not employed the acceleration image because the noises are too strong to catch an effective motion feature in an image sequence. On one hand, the recent convolutional neural networks (CNN) are robust against input noises. In this paper, we employ acceleration-stream in addition to the spatial- and temporal-stream based on the two-stream CNN. We clearly show the effectiveness of adding the acceleration stream to the two-stream CNN.",0
"The differentiation of time information is crucial in representing motion. Initially, we used first-order differential velocity from positional data and also recognized the importance of second-order differential acceleration as a motion feature. However, the acceleration image generated from typical optical flow contains motion-related distortions that make it unsuitable for extracting effective motion features from an image sequence. Therefore, we did not use acceleration image in our approach. However, convolutional neural networks (CNN) are known to be robust against input noises. In this study, we incorporated the acceleration stream with the spatial- and temporal-stream in the two-stream CNN. Our results clearly demonstrate the effectiveness of adding the acceleration stream to the two-stream CNN.",1
"We tackle the problem of estimating optical flow from a monocular camera in the context of autonomous driving. We build on the observation that the scene is typically composed of a static background, as well as a relatively small number of traffic participants which move rigidly in 3D. We propose to estimate the traffic participants using instance-level segmentation. For each traffic participant, we use the epipolar constraints that govern each independent motion for faster and more accurate estimation. Our second contribution is a new convolutional net that learns to perform flow matching, and is able to estimate the uncertainty of its matches. This is a core element of our flow estimation pipeline. We demonstrate the effectiveness of our approach in the challenging KITTI 2015 flow benchmark, and show that our approach outperforms published approaches by a large margin.",0
"Our focus is on estimating optical flow from a single camera for self-driving cars. We take into account that the majority of the scene remains static with only a few traffic participants moving in a rigid 3D manner. We suggest using instance-level segmentation to determine the traffic participants and utilize epipolar constraints to enhance the speed and accuracy of estimation. Additionally, we introduce a novel convolutional net that can match flow and determine the confidence of these matches, which is a key component of our flow estimation process. We demonstrate our approach's efficacy in the challenging KITTI 2015 flow benchmark and prove that it surpasses previously published methods by a significant margin.",1
"Recently, convolutional networks (convnets) have proven useful for predicting optical flow. Much of this success is predicated on the availability of large datasets that require expensive and involved data acquisition and laborious la- beling. To bypass these challenges, we propose an unsuper- vised approach (i.e., without leveraging groundtruth flow) to train a convnet end-to-end for predicting optical flow be- tween two images. We use a loss function that combines a data term that measures photometric constancy over time with a spatial term that models the expected variation of flow across the image. Together these losses form a proxy measure for losses based on the groundtruth flow. Empiri- cally, we show that a strong convnet baseline trained with the proposed unsupervised approach outperforms the same network trained with supervision on the KITTI dataset.",0
"Convolutional networks (convnets) have recently demonstrated their effectiveness in predicting optical flow. However, this success largely relies on the availability of large datasets that require costly and labor-intensive data acquisition and labeling. To overcome these difficulties, we suggest an unsupervised method for training a convnet end-to-end to predict optical flow between two images, without relying on groundtruth flow. Our approach employs a loss function that combines a data term measuring photometric constancy over time with a spatial term modeling the expected flow variation across the image. These losses serve as a substitute measure for groundtruth-based losses. Our experimental results show that our proposed unsupervised approach, when used to train a strong convnet baseline, outperforms the same network trained with supervision on the KITTI dataset.",1
"This paper introduces a novel approach to the task of data association within the context of pedestrian tracking, by introducing a two-stage learning scheme to match pairs of detections. First, a Siamese convolutional neural network (CNN) is trained to learn descriptors encoding local spatio-temporal structures between the two input image patches, aggregating pixel values and optical flow information. Second, a set of contextual features derived from the position and size of the compared input patches are combined with the CNN output by means of a gradient boosting classifier to generate the final matching probability. This learning approach is validated by using a linear programming based multi-person tracker showing that even a simple and efficient tracker may outperform much more complex models when fed with our learned matching probabilities. Results on publicly available sequences show that our method meets state-of-the-art standards in multiple people tracking.",0
"A new technique for data association in pedestrian tracking is presented in this article. The approach involves a two-stage learning process for matching pairs of detections. The first stage involves training a Siamese convolutional neural network to learn descriptors that encode local spatio-temporal structures between the two input image patches. These descriptors aggregate pixel values and optical flow information. In the second stage, a gradient boosting classifier combines a set of contextual features derived from the position and size of the compared input patches with the CNN output to generate the final matching probability. The effectiveness of this approach is demonstrated using a linear programming based multi-person tracker and is shown to outperform much more complex models. The results obtained from this technique meet state-of-the-art standards in multiple people tracking, as demonstrated in publicly available sequences.",1
"Event cameras or neuromorphic cameras mimic the human perception system as they measure the per-pixel intensity change rather than the actual intensity level. In contrast to traditional cameras, such cameras capture new information about the scene at MHz frequency in the form of sparse events. The high temporal resolution comes at the cost of losing the familiar per-pixel intensity information. In this work we propose a variational model that accurately models the behaviour of event cameras, enabling reconstruction of intensity images with arbitrary frame rate in real-time. Our method is formulated on a per-event-basis, where we explicitly incorporate information about the asynchronous nature of events via an event manifold induced by the relative timestamps of events. In our experiments we verify that solving the variational model on the manifold produces high-quality images without explicitly estimating optical flow.",0
"Event cameras or neuromorphic cameras imitate the way humans perceive by measuring the change in per-pixel intensity, rather than the actual intensity level. Unlike conventional cameras, these cameras capture sparse events in the scene at a frequency of MHz, providing new information. However, this high temporal resolution comes at the cost of losing per-pixel intensity information. To overcome this, we propose a variational model that accurately models event camera behavior, allowing for reconstruction of intensity images in real-time at any frame rate. Our method is based on a per-event-basis, where we incorporate information about the asynchronous nature of events via an event manifold induced by the relative timestamps of events. Through experiments, we demonstrate that solving the variational model on the manifold produces high-quality images without the need for estimating optical flow.",1
"In this work, we propose an approach to the spatiotemporal localisation (detection) and classification of multiple concurrent actions within temporally untrimmed videos. Our framework is composed of three stages. In stage 1, appearance and motion detection networks are employed to localise and score actions from colour images and optical flow. In stage 2, the appearance network detections are boosted by combining them with the motion detection scores, in proportion to their respective spatial overlap. In stage 3, sequences of detection boxes most likely to be associated with a single action instance, called action tubes, are constructed by solving two energy maximisation problems via dynamic programming. While in the first pass, action paths spanning the whole video are built by linking detection boxes over time using their class-specific scores and their spatial overlap, in the second pass, temporal trimming is performed by ensuring label consistency for all constituting detection boxes. We demonstrate the performance of our algorithm on the challenging UCF101, J-HMDB-21 and LIRIS-HARL datasets, achieving new state-of-the-art results across the board and significantly increasing detection speed at test time. We achieve a huge leap forward in action detection performance and report a 20% and 11% gain in mAP (mean average precision) on UCF-101 and J-HMDB-21 datasets respectively when compared to the state-of-the-art.",0
"Our study presents a novel method for the detection and classification of multiple concurrent actions in temporally untrimmed videos, utilizing spatiotemporal localisation. Our framework comprises three stages. Firstly, we employ appearance and motion detection networks to detect and score actions from colour images and optical flow. Secondly, we enhance the appearance network detections by merging them with the motion detection scores, in proportion to their respective spatial overlap. Finally, we construct action tubes, which are sequences of detection boxes most likely to be associated with a single action instance, by solving two energy maximisation problems through dynamic programming. Our algorithm achieves state-of-the-art results on the challenging UCF101, J-HMDB-21 and LIRIS-HARL datasets, with a significant increase in detection speed at test time. We report a remarkable gain in mAP, with a 20% and 11% improvement on UCF-101 and J-HMDB-21 datasets, respectively, compared to the current state-of-the-art.",1
"The importance and demands of visual scene understanding have been steadily increasing along with the active development of autonomous systems. Consequently, there has been a large amount of research dedicated to semantic segmentation and dense motion estimation. In this paper, we propose a method for jointly estimating optical flow and temporally consistent semantic segmentation, which closely connects these two problem domains and leverages each other. Semantic segmentation provides information on plausible physical motion to its associated pixels, and accurate pixel-level temporal correspondences enhance the accuracy of semantic segmentation in the temporal domain. We demonstrate the benefits of our approach on the KITTI benchmark, where we observe performance gains for flow and segmentation. We achieve state-of-the-art optical flow results, and outperform all published algorithms by a large margin on challenging, but crucial dynamic objects.",0
"As the development of autonomous systems continues, the need for visual scene understanding has become increasingly important. This has led to extensive research on dense motion estimation and semantic segmentation. Our paper proposes a method that combines these two areas by jointly estimating optical flow and consistent semantic segmentation. By doing so, we enhance the accuracy of both domains as semantic segmentation provides physical motion information to its associated pixels, and accurate pixel-level temporal correspondences improve the accuracy of semantic segmentation. We demonstrate the effectiveness of our approach on the KITTI benchmark, where we achieve state-of-the-art optical flow results and outperform all previously published algorithms on challenging dynamic objects.",1
"In this paper we present a dense ground truth dataset of nonrigidly deforming real-world scenes. Our dataset contains both long and short video sequences, and enables the quantitatively evaluation for RGB based tracking and registration methods. To construct ground truth for the RGB sequences, we simultaneously capture Near-Infrared (NIR) image sequences where dense markers - visible only in NIR - represent ground truth positions. This allows for comparison with automatically tracked RGB positions and the formation of error metrics. Most previous datasets containing nonrigidly deforming sequences are based on synthetic data. Our capture protocol enables us to acquire real-world deforming objects with realistic photometric effects - such as blur and illumination change - as well as occlusion and complex deformations. A public evaluation website is constructed to allow for ranking of RGB image based optical flow and other dense tracking algorithms, with various statistical measures. Furthermore, we present an RGB-NIR multispectral optical flow model allowing for energy optimization by adoptively combining featured information from both the RGB and the complementary NIR channels. In our experiments we evaluate eight existing RGB based optical flow methods on our new dataset. We also evaluate our hybrid optical flow algorithm by comparing to two existing multispectral approaches, as well as varying our input channels across RGB, NIR and RGB-NIR.",0
"This article discusses a comprehensive dataset of real-world scenes that deform non-rigidly, both short and long video sequences. The dataset is used for the quantitative evaluation of tracking and registration methods based on RGB. To create the RGB ground truth, NIR image sequences are simultaneously captured, with dense markers that are only visible in NIR representing the ground truth positions. This facilitates comparison with automatically tracked RGB positions and the creation of error metrics. Unlike previous datasets based on synthetic data, our capture protocol enables us to obtain real-world deforming objects with realistic photometric effects, complex deformations, and occlusion. A public evaluation website is developed to rank RGB image-based optical flow and other dense tracking algorithms using various statistical measures. Moreover, we introduce an RGB-NIR multispectral optical flow model that optimizes energy by adaptively combining featured information from both RGB and complementary NIR channels. We assess eight existing RGB-based optical flow methods on our new dataset in our experiments. In addition, we compare our hybrid optical flow algorithm to two existing multispectral approaches and vary our input channels across RGB, NIR, and RGB-NIR.",1
"Representing videos by densely extracted local space-time features has recently become a popular approach for analysing actions. In this paper, we tackle the problem of categorising human actions by devising Bag of Words (BoW) models based on covariance matrices of spatio-temporal features, with the features formed from histograms of optical flow. Since covariance matrices form a special type of Riemannian manifold, the space of Symmetric Positive Definite (SPD) matrices, non-Euclidean geometry should be taken into account while discriminating between covariance matrices. To this end, we propose to embed SPD manifolds to Euclidean spaces via a diffeomorphism and extend the BoW approach to its Riemannian version. The proposed BoW approach takes into account the manifold geometry of SPD matrices during the generation of the codebook and histograms. Experiments on challenging human action datasets show that the proposed method obtains notable improvements in discrimination accuracy, in comparison to several state-of-the-art methods.",0
"Recently, it has become common to utilize densely extracted local space-time features to represent videos for action analysis. This paper addresses the problem of categorizing human actions through the creation of Bag of Words (BoW) models based on covariance matrices of spatio-temporal features. The features are formed from optical flow histograms. As covariance matrices create a Riemannian manifold known as the space of Symmetric Positive Definite (SPD) matrices, discrimination between them must account for non-Euclidean geometry. To address this, we suggest embedding SPD manifolds to Euclidean spaces via a diffeomorphism and extending the BoW approach to its Riemannian version. Our proposed BoW approach considers the manifold geometry of SPD matrices during codebook and histogram generation. Experiments on challenging human action datasets demonstrate that our proposed method significantly improves discrimination accuracy compared to several state-of-the-art methods.",1
"Saliency maps are used to understand human attention and visual fixation. However, while very well established for static images, there is no general agreement on how to compute a saliency map of dynamic scenes. In this paper we propose a mathematically rigorous approach to this prob- lem, including static saliency maps of each video frame for the calculation of the optical flow. Taking into account static saliency maps for calculating the optical flow allows for overcoming the aperture problem. Our ap- proach is able to explain human fixation behavior in situations which pose challenges to standard approaches, such as when a fixated object disappears behind an occlusion and reappears after several frames. In addition, we quantitatively compare our model against alternative solutions using a large eye tracking data set. Together, our results suggest that assessing optical flow information across a series of saliency maps gives a highly accurate and useful account of human overt attention in dynamic scenes.",0
"The use of saliency maps is common for studying human attention and visual fixation. However, there is no consensus on how to generate a saliency map for dynamic scenes, despite their established use for static images. Our paper proposes a precise mathematical approach to this issue, involving static saliency maps for each video frame to calculate optical flow. This method overcomes the aperture problem and explains human fixation behavior in challenging situations, such as when an object disappears behind an occlusion and reappears later. We also compare our model with other solutions, using a large eye tracking data set. Overall, our results demonstrate that the use of optical flow information across multiple saliency maps provides an accurate and valuable explanation of human attention in dynamic scenes.",1
"In this paper, we introduce an end-to-end framework for video analysis focused towards practical scenarios built on theoretical foundations from sparse representation, including a novel descriptor for general purpose video analysis. In our approach, we compute kinematic features from optical flow and first and second-order derivatives of intensities to represent motion and appearance respectively. These features are then used to construct covariance matrices which capture joint statistics of both low-level motion and appearance features extracted from a video. Using an over-complete dictionary of the covariance based descriptors built from labeled training samples, we formulate low-level event recognition as a sparse linear approximation problem. Within this, we pose the sparse decomposition of a covariance matrix, which also conforms to the space of semi-positive definite matrices, as a determinant maximization problem. Also since covariance matrices lie on non-linear Riemannian manifolds, we compare our former approach with a sparse linear approximation alternative that is suitable for equivalent vector spaces of covariance matrices. This is done by searching for the best projection of the query data on a dictionary using an Orthogonal Matching pursuit algorithm. We show the applicability of our video descriptor in two different application domains - namely low-level event recognition in unconstrained scenarios and gesture recognition using one shot learning. Our experiments provide promising insights in large scale video analysis.",0
"This paper presents a comprehensive framework for analyzing videos in practical settings, based on the theoretical foundations of sparse representation. The framework includes a novel descriptor for general-purpose video analysis. Our approach involves computing kinematic features from optical flow and derivatives of intensities to represent motion and appearance, respectively. These features are used to create covariance matrices that capture joint statistics of both low-level motion and appearance features from the video. We formulate low-level event recognition as a sparse linear approximation problem using an over-complete dictionary of the covariance-based descriptors. We pose the sparse decomposition of a covariance matrix, which conforms to the space of semi-positive definite matrices, as a determinant maximization problem. Since covariance matrices lie on non-linear Riemannian manifolds, we also compare our approach with a sparse linear approximation alternative suitable for equivalent vector spaces of covariance matrices. We use an Orthogonal Matching pursuit algorithm to search for the best projection of the query data on a dictionary. We demonstrate the applicability of our video descriptor in two different application domains - low-level event recognition in unconstrained scenarios and gesture recognition using one-shot learning. Our experiments provide promising insights into large-scale video analysis.",1
"We propose a personalized ConvNet pose estimator that automatically adapts itself to the uniqueness of a person's appearance to improve pose estimation in long videos. We make the following contributions: (i) we show that given a few high-precision pose annotations, e.g. from a generic ConvNet pose estimator, additional annotations can be generated throughout the video using a combination of image-based matching for temporally distant frames, and dense optical flow for temporally local frames; (ii) we develop an occlusion aware self-evaluation model that is able to automatically select the high-quality and reject the erroneous additional annotations; and (iii) we demonstrate that these high-quality annotations can be used to fine-tune a ConvNet pose estimator and thereby personalize it to lock on to key discriminative features of the person's appearance. The outcome is a substantial improvement in the pose estimates for the target video using the personalized ConvNet compared to the original generic ConvNet. Our method outperforms the state of the art (including top ConvNet methods) by a large margin on two standard benchmarks, as well as on a new challenging YouTube video dataset. Furthermore, we show that training from the automatically generated annotations can be used to improve the performance of a generic ConvNet on other benchmarks.",0
"Our proposal is a ConvNet pose estimator that can adapt to a person's unique appearance to enhance pose estimation in long videos. We present three main contributions: firstly, we demonstrate that additional annotations can be generated throughout the video by using image-based matching for temporally distant frames and dense optical flow for temporally local frames, given a few high-precision pose annotations from a generic ConvNet pose estimator. Secondly, we develop an occlusion aware self-evaluation model that can automatically identify high-quality annotations and reject erroneous ones. Thirdly, we illustrate that these high-quality annotations can personalize a ConvNet pose estimator by fine-tuning it to focus on key discriminative features of a person's appearance. This results in significantly improved pose estimates for the target video compared to the original generic ConvNet. Our approach outperforms state-of-the-art methods, including top ConvNet methods, on two standard benchmarks and a new challenging YouTube video dataset. Additionally, we demonstrate that training from the automatically generated annotations can enhance the performance of a generic ConvNet on other benchmarks.",1
"This work targets people identification in video based on the way they walk (i.e. gait). While classical methods typically derive gait signatures from sequences of binary silhouettes, in this work we explore the use of convolutional neural networks (CNN) for learning high-level descriptors from low-level motion features (i.e. optical flow components). We carry out a thorough experimental evaluation of the proposed CNN architecture on the challenging TUM-GAID dataset. The experimental results indicate that using spatio-temporal cuboids of optical flow as input data for CNN allows to obtain state-of-the-art results on the gait task with an image resolution eight times lower than the previously reported results (i.e. 80x60 pixels).",0
"The objective of this study is to identify individuals in videos by analyzing their gait. Traditional methods involve analyzing binary silhouettes to derive gait signatures. However, this research utilizes convolutional neural networks (CNN) to learn high-level descriptors from low-level motion features, specifically optical flow components. The proposed CNN architecture was evaluated on the challenging TUM-GAID dataset, and the results showed that using spatio-temporal cuboids of optical flow as input data for CNN achieved superior results compared to previous methods. Notably, image resolution was eight times lower (80x60 pixels) than previously reported results.",1
"Optical strain is an extension of optical flow that is capable of quantifying subtle changes on faces and representing the minute facial motion intensities at the pixel level. This is computationally essential for the relatively new field of spontaneous micro-expression, where subtle expressions can be technically challenging to pinpoint. In this paper, we present a novel method for detecting and recognizing micro-expressions by utilizing facial optical strain magnitudes to construct optical strain features and optical strain weighted features. The two sets of features are then concatenated to form the resultant feature histogram. Experiments were performed on the CASME II and SMIC databases. We demonstrate on both databases, the usefulness of optical strain information and more importantly, that our best approaches are able to outperform the original baseline results for both detection and recognition tasks. A comparison of the proposed method with other existing spatio-temporal feature extraction approaches is also presented.",0
"The concept of optical strain is an extension of optical flow, which has the ability to measure subtle changes in facial expressions and represent them at a pixel level. This is crucial for the relatively new field of spontaneous micro-expression, where identifying these subtle expressions can be a technical challenge. In this study, we introduce a new approach for detecting and recognizing micro-expressions by utilizing facial optical strain magnitudes to create optical strain features and optical strain weighted features. These two sets of features are combined to form the resultant feature histogram. We conducted experiments on the CASME II and SMIC databases and demonstrated the usefulness of optical strain information. Our best approaches were able to surpass the original baseline results for both detection and recognition tasks. Furthermore, we compared our proposed method with other existing spatio-temporal feature extraction approaches.",1
"In this paper, we tackle the problem of temporally consistent boundary detection and hierarchical segmentation in videos. While finding the best high-level reasoning of region assignments in videos is the focus of much recent research, temporal consistency in boundary detection has so far only rarely been tackled. We argue that temporally consistent boundaries are a key component to temporally consistent region assignment. The proposed method is based on the point-wise mutual information (PMI) of spatio-temporal voxels. Temporal consistency is established by an evaluation of PMI-based point affinities in the spectral domain over space and time. Thus, the proposed method is independent of any optical flow computation or previously learned motion models. The proposed low-level video segmentation method outperforms the learning-based state of the art in terms of standard region metrics.",0
"The focus of this paper is on addressing the challenge of detecting consistent boundaries and hierarchical segmentation in videos over time. Although there has been much research on determining the best high-level reasoning for assigning regions in videos, little attention has been given to achieving temporal consistency in boundary detection. We contend that consistency in boundaries is crucial for ensuring consistent region assignment over time. Our proposed approach utilizes the point-wise mutual information (PMI) of spatio-temporal voxels and establishes temporal consistency by evaluating PMI-based point affinities in the spectral domain across space and time. Unlike other methods, our approach does not rely on optical flow computation or previously learned motion models. Our low-level video segmentation method surpasses the learning-based state of the art in terms of standard region metrics.",1
"In a traditional convolutional layer, the learned filters stay fixed after training. In contrast, we introduce a new framework, the Dynamic Filter Network, where filters are generated dynamically conditioned on an input. We show that this architecture is a powerful one, with increased flexibility thanks to its adaptive nature, yet without an excessive increase in the number of model parameters. A wide variety of filtering operations can be learned this way, including local spatial transformations, but also others like selective (de)blurring or adaptive feature extraction. Moreover, multiple such layers can be combined, e.g. in a recurrent architecture. We demonstrate the effectiveness of the dynamic filter network on the tasks of video and stereo prediction, and reach state-of-the-art performance on the moving MNIST dataset with a much smaller model. By visualizing the learned filters, we illustrate that the network has picked up flow information by only looking at unlabelled training data. This suggests that the network can be used to pretrain networks for various supervised tasks in an unsupervised way, like optical flow and depth estimation.",0
"In a traditional convolutional layer, the filters that are learned remain fixed after training. However, we have developed a new framework called the Dynamic Filter Network, where filters are generated on-the-fly based on the input. This architecture offers increased flexibility due to its adaptive nature, without significantly increasing the number of model parameters. A range of filtering operations can be learned, including local spatial transformations, selective (de)blurring, and adaptive feature extraction. Additionally, multiple layers can be combined, such as in a recurrent architecture. We have demonstrated the effectiveness of the dynamic filter network in tasks such as video and stereo prediction, achieving state-of-the-art performance on the moving MNIST dataset with a smaller model. By examining the learned filters, we have observed that the network has captured flow information without any labeled training data. This suggests that the network can be used to pretrain networks for various supervised tasks in an unsupervised manner, such as optical flow and depth estimation.",1
"Compared to other applications in computer vision, convolutional neural networks have under-performed on pedestrian detection. A breakthrough was made very recently by using sophisticated deep CNN models, with a number of hand-crafted features, or explicit occlusion handling mechanism. In this work, we show that by re-using the convolutional feature maps (CFMs) of a deep convolutional neural network (DCNN) model as image features to train an ensemble of boosted decision models, we are able to achieve the best reported accuracy without using specially designed learning algorithms. We empirically identify and disclose important implementation details. We also show that pixel labelling may be simply combined with a detector to boost the detection performance. By adding complementary hand-crafted features such as optical flow, the DCNN based detector can be further improved. We set a new record on the Caltech pedestrian dataset, lowering the log-average miss rate from $11.7\%$ to $8.9\%$, a relative improvement of $24\%$. We also achieve a comparable result to the state-of-the-art approaches on the KITTI dataset.",0
"Pedestrian detection using convolutional neural networks has not been as successful as other computer vision applications. However, recent progress has been made by utilizing advanced deep CNN models, incorporating hand-crafted features or explicit occlusion handling mechanisms. This study demonstrates that by reusing convolutional feature maps (CFMs) from a deep CNN model as image features to train an ensemble of boosted decision models, without requiring specially designed learning algorithms, the best reported accuracy can be achieved. Implementation details are disclosed, and it is shown that combining pixel labelling with a detector can enhance detection performance. Adding complementary hand-crafted features, such as optical flow, can further improve the DCNN-based detector. The study sets a new record on the Caltech pedestrian dataset, with a relative improvement of 24%, and achieves comparable results with state-of-the-art approaches on the KITTI dataset.",1
"A recent paper by Gatys et al. describes a method for rendering an image in the style of another image. First, they use convolutional neural network features to build a statistical model for the style of an image. Then they create a new image with the content of one image but the style statistics of another image. Here, we extend this method to render a movie in a given artistic style. The naive solution that independently renders each frame produces poor results because the features of the style move substantially from one frame to the next. The other naive method that initializes the optimization for the next frame using the rendered version of the previous frame also produces poor results because the features of the texture stay fixed relative to the frame of the movie instead of moving with objects in the scene. The main contribution of this paper is to use optical flow to initialize the style transfer optimization so that the texture features move with the objects in the video. Finally, we suggest a method to incorporate optical flow explicitly into the cost function.",0
"Gatys et al. recently published a paper outlining their process for creating an image in the style of another image. They use convolutional neural network features to establish a statistical model for the style of an image, then apply this model to a new image containing the content of one image and the style statistics of another. We have expanded this process to render a movie in a specific artistic style. However, rendering each frame independently or initializing the optimization for the next frame using the previous render produces unsatisfactory results due to the moving style features. To address this, we utilize optical flow to initialize the style transfer optimization so that the texture features move in conjunction with the objects in the video. In addition, we propose a method for explicitly incorporating optical flow into the cost function.",1
"Manual spatio-temporal annotation of human action in videos is laborious, requires several annotators and contains human biases. In this paper, we present a weakly supervised approach to automatically obtain spatio-temporal annotations of an actor in action videos. We first obtain a large number of action proposals in each video. To capture a few most representative action proposals in each video and evade processing thousands of them, we rank them using optical flow and saliency in a 3D-MRF based framework and select a few proposals using MAP based proposal subset selection method. We demonstrate that this ranking preserves the high quality action proposals. Several such proposals are generated for each video of the same action. Our next challenge is to iteratively select one proposal from each video so that all proposals are globally consistent. We formulate this as Generalized Maximum Clique Graph problem using shape, global and fine grained similarity of proposals across the videos. The output of our method is the most action representative proposals from each video. Our method can also annotate multiple instances of the same action in a video. We have validated our approach on three challenging action datasets: UCF Sport, sub-JHMDB and THUMOS'13 and have obtained promising results compared to several baseline methods. Moreover, on UCF Sports, we demonstrate that action classifiers trained on these automatically obtained spatio-temporal annotations have comparable performance to the classifiers trained on ground truth annotation.",0
"The process of manually annotating human actions in videos with spatio-temporal information is a time-consuming task that involves multiple annotators and can be influenced by human biases. This paper proposes a weakly supervised approach for automatically obtaining spatio-temporal annotations of actors in action videos. The approach involves generating a large number of action proposals in each video and then selecting the most representative ones using optical flow and saliency ranking in a 3D-MRF framework. The selected proposals are further refined using a Maximum Clique Graph method that considers shape, global, and fine-grained similarities across videos. The output of the proposed method is a set of action proposals that are globally consistent and representative of the actions in the videos. The approach is validated on three challenging action datasets and is shown to achieve promising results compared to baseline methods. The proposed method is also capable of annotating multiple instances of the same action in a video, and the resulting annotations can be used to train action classifiers with comparable performance to those trained on ground truth annotations.",1
"Smile is an irrefutable expression that shows the physical state of the mind in both true and deceptive ways. Generally, it shows happy state of the mind, however, `smiles' can be deceptive, for example people can give a smile when they feel happy and sometimes they might also give a smile (in a different way) when they feel pity for others. This work aims to distinguish spontaneous (felt) smile expressions from posed (deliberate) smiles by extracting and analyzing both global (macro) motion of the face and subtle (micro) changes in the facial expression features through both tracking a series of facial fiducial markers as well as using dense optical flow. Specifically the eyes and lips features are captured and used for analysis. It aims to automatically classify all smiles into either `spontaneous' or `posed' categories, by using support vector machines (SVM). Experimental results on large database show promising results as compared to other relevant methods.",0
"The physical state of the mind can be expressed through a smile, but it can also be deceptive. While a smile often indicates happiness, it can also be used to express pity. This study seeks to distinguish between genuine and fake smiles by analyzing the facial expressions and motion of the face, specifically the eyes and lips. The aim is to use support vector machines to classify smiles as either spontaneous or posed. The study shows promising results when compared to other methods.",1
"Modern computer vision algorithms typically require expensive data acquisition and accurate manual labeling. In this work, we instead leverage the recent progress in computer graphics to generate fully labeled, dynamic, and photo-realistic proxy virtual worlds. We propose an efficient real-to-virtual world cloning method, and validate our approach by building and publicly releasing a new video dataset, called Virtual KITTI (see http://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds), automatically labeled with accurate ground truth for object detection, tracking, scene and instance segmentation, depth, and optical flow. We provide quantitative experimental evidence suggesting that (i) modern deep learning algorithms pre-trained on real data behave similarly in real and virtual worlds, and (ii) pre-training on virtual data improves performance. As the gap between real and virtual worlds is small, virtual worlds enable measuring the impact of various weather and imaging conditions on recognition performance, all other things being equal. We show these factors may affect drastically otherwise high-performing deep models for tracking.",0
"Usually, up-to-date computer vision algorithms require a costly process of data acquisition and precise manual labeling. Nevertheless, we have taken advantage of the recent advancements in computer graphics to produce dynamic and photo-realistic virtual worlds that are fully labeled. We suggest a method of cloning real-to-virtual worlds, which we have tested by generating and releasing a new video dataset called Virtual KITTI, featuring automatically labeled ground truth for object detection, tracking, scene and instance segmentation, depth, and optical flow. Our experiments have shown that (i) state-of-the-art deep learning algorithms that are pre-trained using real data behave similarly in both real and virtual worlds, and (ii) pre-training on virtual data enhances performance. Since there is only a slight difference between real and virtual worlds, virtual environments enable us to evaluate the influence of various weather and imaging conditions on recognition performance, with all other factors being equal. We have demonstrated that these factors can significantly impact deep models for tracking, even if their performance is otherwise high.",1
"This paper deals with a challenging, frequently encountered, yet not properly investigated problem in two-frame optical flow estimation. That is, the input frames are compounds of two imaging layers -- one desired background layer of the scene, and one distracting, possibly moving layer due to transparency or reflection. In this situation, the conventional brightness constancy constraint -- the cornerstone of most existing optical flow methods -- will no longer be valid. In this paper, we propose a robust solution to this problem. The proposed method performs both optical flow estimation, and image layer separation. It exploits a generalized double-layer brightness consistency constraint connecting these two tasks, and utilizes the priors for both of them. Experiments on both synthetic data and real images have confirmed the efficacy of the proposed method. To the best of our knowledge, this is the first attempt towards handling generic optical flow fields of two-frame images containing transparency or reflection.",0
"This article addresses a challenging issue that is often encountered in two-frame optical flow estimation. Specifically, the input frames comprise two imaging layers: a desired background layer of the scene and a distracting, potentially moving layer caused by transparency or reflection. Consequently, the conventional brightness constancy constraint, which is fundamental to most existing optical flow methods, is no longer applicable. To overcome this issue, the article proposes a robust solution that performs optical flow estimation and image layer separation. The approach utilizes a generalized double-layer brightness consistency constraint to connect these two tasks and leverages the priors for each. The effectiveness of the proposed method is confirmed through experiments on both synthetic data and real images. This is the first effort to handle generic optical flow fields of two-frame images that include transparency or reflection.",1
"Optical flow estimation is a widely known problem in computer vision introduced by Gibson, J.J(1950) to describe the visual perception of human by stimulus objects. Estimation of optical flow model can be achieved by solving for the motion vectors from region of interest in the the different timeline. In this paper, we assumed slightly uniform change of velocity between two nearby frames, and solve the optical flow problem by traditional method, Lucas-Kanade(1981). This method performs minimization of errors between template and target frame warped back onto the template. Solving minimization steps requires optimization methods which have diverse convergence rate and error. We explored first and second order optimization methods, and compare their results with Gauss-Newton method in Lucas-Kanade. We generated 105 videos with 10,500 frames by synthetic objects, and 10 videos with 1,000 frames from real world footage. Our experimental results could be used as tuning parameters for Lucas-Kanade method.",0
"The problem of estimating optical flow in computer vision was first introduced by Gibson, J.J in 1950 to explain how humans perceive visual stimuli. To estimate an optical flow model, motion vectors can be solved for in different timelines from regions of interest. Our paper assumes a slightly uniform change in velocity between nearby frames and uses the traditional Lucas-Kanade (1981) method to solve the optical flow problem. This involves minimizing the errors between the template and target frame warped back onto the template. Optimization methods with different convergence rates and errors are necessary for solving the minimization steps. We compared the results of first and second order optimization methods with the Gauss-Newton method in Lucas-Kanade. We created 105 videos with 10,500 frames from synthetic objects and 10 videos with 1,000 frames from real-world footage to conduct our experiments. Our findings can be used to fine-tune the Lucas-Kanade method.",1
"The deep two-stream architecture exhibited excellent performance on video based action recognition. The most computationally expensive step in this approach comes from the calculation of optical flow which prevents it to be real-time. This paper accelerates this architecture by replacing optical flow with motion vector which can be obtained directly from compressed videos without extra calculation. However, motion vector lacks fine structures, and contains noisy and inaccurate motion patterns, leading to the evident degradation of recognition performance. Our key insight for relieving this problem is that optical flow and motion vector are inherent correlated. Transferring the knowledge learned with optical flow CNN to motion vector CNN can significantly boost the performance of the latter. Specifically, we introduce three strategies for this, initialization transfer, supervision transfer and their combination. Experimental results show that our method achieves comparable recognition performance to the state-of-the-art, while our method can process 390.7 frames per second, which is 27 times faster than the original two-stream method.",0
"Impressive results were obtained with the deep two-stream architecture in recognizing actions in video, but the computation of optical flow made it impractical for real-time use. To address this, the authors replaced optical flow with motion vector, which can be retrieved directly from compressed videos without extra calculations. However, motion vector lacks fine details and contains noisy and inaccurate motion patterns, causing a decline in recognition performance. To address this issue, the authors proposed leveraging the inherent correlation between optical flow and motion vector. They introduced three strategies to transfer knowledge from optical flow CNN to motion vector CNN: initialization transfer, supervision transfer, and their combination. Their approach achieved comparable recognition performance to the state-of-the-art, with a processing speed of 390.7 frames per second, which is 27 times faster than the original two-stream method.",1
"Motion blur can adversely affect a number of vision tasks, hence it is generally considered a nuisance. We instead treat motion blur as a useful signal that allows to compute the motion of objects from a single image. Drawing on the success of joint segmentation and parametric motion models in the context of optical flow estimation, we propose a parametric object motion model combined with a segmentation mask to exploit localized, non-uniform motion blur. Our parametric image formation model is differentiable w.r.t. the motion parameters, which enables us to generalize marginal-likelihood techniques from uniform blind deblurring to localized, non-uniform blur. A two-stage pipeline, first in derivative space and then in image space, allows to estimate both parametric object motion as well as a motion segmentation from a single image alone. Our experiments demonstrate its ability to cope with very challenging cases of object motion blur.",0
"Though motion blur is typically seen as a hindrance due to its negative impact on various vision-related tasks, we view it as a valuable cue that can be used to determine object motion from a single image. We propose a method that utilizes a parametric object motion model and a segmentation mask to make use of localized, non-uniform motion blur. Our differentiable parametric image formation model allows for generalization of marginal-likelihood techniques from uniform blind deblurring to localized, non-uniform blur. By implementing a two-stage pipeline, we can estimate both parametric object motion and motion segmentation from only one image, even in extremely challenging cases of object motion blur, as demonstrated by our experiments.",1
We present a global optimization approach to optical flow estimation. The approach optimizes a classical optical flow objective over the full space of mappings between discrete grids. No descriptor matching is used. The highly regular structure of the space of mappings enables optimizations that reduce the computational complexity of the algorithm's inner loop from quadratic to linear and support efficient matching of tens of thousands of nodes to tens of thousands of displacements. We show that one-shot global optimization of a classical Horn-Schunck-type objective over regular grids at a single resolution is sufficient to initialize continuous interpolation and achieve state-of-the-art performance on challenging modern benchmarks.,0
"Our study introduces a worldwide optimization method for estimating optical flow. This method optimizes the classical optical flow objective across the complete range of mappings between discrete grids, without relying on descriptor matching. The regular structure of the mapping space allows for optimization that significantly reduces the complexity of the algorithm's inner loop from quadratic to linear, enabling efficient matching of tens of thousands of nodes to tens of thousands of displacements. We demonstrate that performing one-shot global optimization of a classical Horn-Schunck-type objective on regular grids at a single resolution is enough to initiate continuous interpolation and attain top-quality performance on modern benchmarks that are challenging.",1
"Existing optical flow methods make generic, spatially homogeneous, assumptions about the spatial structure of the flow. In reality, optical flow varies across an image depending on object class. Simply put, different objects move differently. Here we exploit recent advances in static semantic scene segmentation to segment the image into objects of different types. We define different models of image motion in these regions depending on the type of object. For example, we model the motion on roads with homographies, vegetation with spatially smooth flow, and independently moving objects like cars and planes with affine motion plus deviations. We then pose the flow estimation problem using a novel formulation of localized layers, which addresses limitations of traditional layered models for dealing with complex scene motion. Our semantic flow method achieves the lowest error of any published monocular method in the KITTI-2015 flow benchmark and produces qualitatively better flow and segmentation than recent top methods on a wide range of natural videos.",0
"Current optical flow techniques rely on generic, uniform assumptions regarding the spatial structure of flow, despite the fact that the flow varies depending on the object class within the image. Our approach addresses this issue by utilizing advancements in static semantic scene segmentation to divide the image into various object types. We establish unique models of image motion for each region based on the object type. For example, we utilize homographies to model motion on roads, spatially smooth flow for vegetation, and affine motion plus deviations for independently moving objects such as cars and planes. Our method uses localized layers to formulate the flow estimation problem, overcoming the limitations of traditional layered models in dealing with complex scene motion. Our semantic flow method outperforms any published monocular method in the KITTI-2015 flow benchmark and produces superior flow and segmentation compared to recent top methods in a variety of natural videos.",1
"Optical flow is typically estimated by minimizing a ""data cost"" and an optional regularizer. While there has been much work on different regularizers many modern algorithms still use a data cost that is not very different from the ones used over 30 years ago: a robust version of brightness constancy or gradient constancy. In this paper we leverage the recent availability of ground-truth optical flow databases in order to learn a data cost. Specifically we take a generative approach in which the data cost models the distribution of noise after warping an image according to the flow and we measure the ""goodness"" of a data cost by how well it matches the true distribution of flow warp error. Consistent with current practice, we find that robust versions of gradient constancy are better models than simple brightness constancy but a learned GMM that models the density of patches of warp error gives a much better fit than any existing assumption of constancy. This significant advantage of the GMM is due to an explicit modeling of the spatial structure of warp errors, a feature which is missing from almost all existing data costs in optical flow. Finally, we show how a good density model of warp error patches can be used for optical flow estimation on whole images. We replace the data cost by the expected patch log-likelihood (EPLL), and show how this cost can be optimized iteratively using an additional step of denoising the warp error image. The results of our experiments are promising and show that patch models with higher likelihood lead to better optical flow estimation.",0
"Typically, optical flow is estimated by minimizing a ""data cost"" and a regularizer, although many modern algorithms still use a data cost that is similar to those used over 30 years ago. In this paper, we use ground-truth optical flow databases to learn a data cost by taking a generative approach. We model the distribution of noise after warping an image according to the flow and measure the ""goodness"" of a data cost by how well it matches the true distribution of flow warp error. Our findings show that robust versions of gradient constancy are better models than simple brightness constancy, but a learned GMM that models the density of patches of warp error gives a much better fit than any existing assumption of constancy. This is because the GMM explicitly models the spatial structure of warp errors, a feature missing from almost all existing data costs in optical flow. We demonstrate how a good density model of warp error patches can be used for optical flow estimation on whole images by replacing the data cost with the expected patch log-likelihood (EPLL). This cost can be optimized iteratively using an additional step of denoising the warp error image. Our experiments show promising results, indicating that patch models with higher likelihood lead to better optical flow estimation.",1
"We propose a new pipeline for optical flow computation, based on Deep Learning techniques. We suggest using a Siamese CNN to independently, and in parallel, compute the descriptors of both images. The learned descriptors are then compared efficiently using the L2 norm and do not require network processing of patch pairs. The success of the method is based on an innovative loss function that computes higher moments of the loss distributions for each training batch. Combined with an Approximate Nearest Neighbor patch matching method and a flow interpolation technique, state of the art performance is obtained on the most challenging and competitive optical flow benchmarks.",0
"Our proposal entails a novel approach to calculate optical flow utilizing Deep Learning methodologies. Our suggestion is to employ a Siamese CNN for the autonomous computation of image descriptors in parallel. The descriptors acquired are compared efficiently through the application of L2 norm, requiring no network processing of patch pairs. The efficacy of this technique is attributed to a pioneering loss function that calculates higher moments of the loss distributions for every training batch. In combination with an Approximate Nearest Neighbor patch matching method and a flow interpolation technique, we achieve top-notch results on the most difficult and competitive optical flow benchmarks.",1
"The human ability to detect and segment moving objects works in the presence of multiple objects, complex background geometry, motion of the observer, and even camouflage. In addition to all of this, the ability to detect motion is nearly instantaneous. While there has been much recent progress in motion segmentation, it still appears we are far from human capabilities. In this work, we derive from first principles a new likelihood function for assessing the probability of an optical flow vector given the 3D motion direction of an object. This likelihood uses a novel combination of the angle and magnitude of the optical flow to maximize the information about the true motions of objects. Using this new likelihood and several innovations in initialization, we develop a motion segmentation algorithm that beats current state-of-the-art methods by a large margin. We compare to five state-of-the-art methods on two established benchmarks, and a third new data set of camouflaged animals, which we introduce to push motion segmentation to the next level.",0
"Despite the challenges posed by multiple objects, complex background geometry, observer motion, and camouflage, humans possess an impressive ability to quickly detect and separate moving objects. While recent advancements have been made in motion segmentation, we still fall short of human capabilities. This study introduces a novel likelihood function based on first principles, which utilizes both angle and magnitude of optical flow to maximize information about object motion. By incorporating this likelihood and implementing several innovations in initialization, we have developed a motion segmentation algorithm that outperforms current state-of-the-art methods by a significant margin. Our approach was evaluated on two established benchmarks and a new dataset of camouflaged animals, which we introduced to further push the boundaries of motion segmentation.",1
"Non-rigid video interpolation is a common computer vision task. In this paper we present an optical flow approach which adopts a Laplacian Cotangent Mesh constraint to enhance the local smoothness. Similar to Li et al., our approach adopts a mesh to the image with a resolution up to one vertex per pixel and uses angle constraints to ensure sensible local deformations between image pairs. The Laplacian Mesh constraints are expressed wholly inside the optical flow optimization, and can be applied in a straightforward manner to a wide range of image tracking and registration problems. We evaluate our approach by testing on several benchmark datasets, including the Middlebury and Garg et al. datasets. In addition, we show application of our method for constructing 3D Morphable Facial Models from dynamic 3D data.",0
"The task of non-rigid video interpolation is a frequent undertaking in computer vision. Our paper introduces an optical flow technique that incorporates a Laplacian Cotangent Mesh constraint to enhance local smoothness. Similar to Li et al., we utilize a mesh with one vertex per pixel and angle constraints to ensure logical local deformations between image pairs. The Laplacian Mesh constraints are entirely integrated within the optical flow optimization and can be easily applied to various image tracking and registration issues. We assessed our method by conducting tests on several benchmark datasets, such as the Middlebury and Garg et al. datasets. Furthermore, we demonstrate how our approach can be utilized to construct 3D Morphable Facial Models from dynamic 3D data.",1
"The proposed method uses live image footage which, based on calculations of pixel motion, decides whether or not an object is in the blind-spot. If found, the driver is notified by a sensory light or noise built into the vehicle's CPU. The new technology incorporates optical vectors and flow fields rather than expensive radar-waves, creating cheaper detection systems that retain the needed accuracy while adapting to the current processor speeds.",0
"The method suggested involves employing real-time video footage that utilizes pixel motion computations to determine the presence of an object in the blind spot. In case of detection, the vehicle's central processing unit (CPU) is equipped with a sensory light or sound to alert the driver. The innovative technology employs optical vectors and flow fields instead of costly radar-waves, resulting in cost-effective detection systems that maintain the necessary precision while adjusting to the processor speeds of the moment.",1
This technical report describes an improved image mosaicking algorithm. It is based on Jain's logarithmic search algorithm [Jain 1981] which is coupled to the method of Kourogi (1999} for matching images in a video sequence. Logarithmic search has a better invariance against illumination changes than the original optical-flow-based method of Kourogi.,0
The technical report explains an upgraded algorithm for image mosaicking that combines Jain's logarithmic search algorithm [Jain 1981] with Kourogi's (1999) technique for matching images in a video sequence. The logarithmic search algorithm provides superior resistance to illumination changes compared to Kourogi's original optical-flow-based method.,1
"State-of-the-art video deblurring methods cannot handle blurry videos recorded in dynamic scenes, since they are built under a strong assumption that the captured scenes are static. Contrary to the existing methods, we propose a video deblurring algorithm that can deal with general blurs inherent in dynamic scenes. To handle general and locally varying blurs caused by various sources, such as moving objects, camera shake, depth variation, and defocus, we estimate pixel-wise non-uniform blur kernels. We infer bidirectional optical flows to handle motion blurs, and also estimate Gaussian blur maps to remove optical blur from defocus in our new blur model. Therefore, we propose a single energy model that jointly estimates optical flows, defocus blur maps and latent frames. We also provide a framework and efficient solvers to minimize the proposed energy model. By optimizing the energy model, we achieve significant improvements in removing general blurs, estimating optical flows, and extending depth-of-field in blurry frames. Moreover, in this work, to evaluate the performance of non-uniform deblurring methods objectively, we have constructed a new realistic dataset with ground truths. In addition, extensive experimental on publicly available challenging video data demonstrate that the proposed method produces qualitatively superior performance than the state-of-the-art methods which often fail in either deblurring or optical flow estimation.",0
"Current video deblurring techniques are limited in their ability to handle videos captured in dynamic environments due to their reliance on the assumption that the scenes being captured are static. As opposed to these existing methods, we propose a new algorithm that can address the general blurs present in dynamic scenes. To accomplish this, we estimate non-uniform blur kernels at the pixel level to handle various sources of blur, including camera shake, depth variation, defocus, and moving objects. Bidirectional optical flows are inferred to handle motion blurs, while Gaussian blur maps are estimated to remove optical blur caused by defocus in our new blur model. Our approach involves a single energy model that jointly estimates optical flows, defocus blur maps, and latent frames, with a corresponding framework and efficient solvers for minimizing the proposed energy model. Our optimization of the energy model results in significant improvements in removing general blurs, estimating optical flows, and extending depth-of-field in blurry frames. We have also created a new realistic dataset with ground truths to objectively evaluate the performance of non-uniform deblurring methods. Extensive experimentation on publicly available video data demonstrates that our proposed method produces superior performance compared to state-of-the-art methods, which often fail in either deblurring or optical flow estimation.",1
"It is hard to estimate optical flow given a realworld video sequence with camera shake and other motion blur. In this paper, we first investigate the blur parameterization for video footage using near linear motion elements. we then combine a commercial 3D pose sensor with an RGB camera, in order to film video footage of interest together with the camera motion. We illustrates that this additional camera motion/trajectory channel can be embedded into a hybrid framework by interleaving an iterative blind deconvolution and warping based optical flow scheme. Our method yields improved accuracy within three other state-of-the-art baselines given our proposed ground truth blurry sequences; and several other realworld sequences filmed by our imaging system.",0
"Estimating optical flow in real-world video sequences with camera shake and motion blur is a challenging task. This study explores the use of near linear motion elements to parameterize blur in video footage. The researchers combined a commercial 3D pose sensor with an RGB camera to capture video footage and camera motion. By integrating an iterative blind deconvolution and warping based optical flow scheme, they were able to incorporate the additional camera motion/trajectory channel into a hybrid framework. The proposed method outperformed three other state-of-the-art baselines when tested on proposed ground truth blurry sequences and real-world sequences recorded by their imaging system.",1
"It is hard to densely track a nonrigid object in long term, which is a fundamental research issue in the computer vision community. This task often relies on estimating pairwise correspondences between images over time where the error is accumulated and leads to a drift issue. In this paper, we introduce a novel optimization framework with an Anchor Patch constraint. It is supposed to significantly reduce overall errors given long sequences containing non-rigidly deformable objects. Our framework can be applied to any dense tracking algorithm, e.g. optical flow. We demonstrate the success of our approach by showing significant error reduction on 6 popular optical flow algorithms applied to a range of real-world nonrigid benchmarks. We also provide quantitative analysis of our approach given synthetic occlusions and image noise.",0
"The computer vision community struggles with the challenge of accurately tracking nonrigid objects over extended periods of time. This is due to the difficulty of establishing pairwise correspondences between images, which can result in accumulated errors and drift. To address this issue, we propose a new optimization framework that incorporates an Anchor Patch constraint. This constraint is designed to reduce errors in long sequences involving non-rigidly deformable objects. Our framework can be utilized with any dense tracking algorithm, such as optical flow. We demonstrate the effectiveness of our approach by reducing errors in six popular optical flow algorithms when applied to various real-world nonrigid benchmarks. Additionally, we provide quantitative analysis of our approach under conditions of synthetic occlusions and image noise.",1
"Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics. This is why pixel-space video prediction may be viewed as a promising avenue for unsupervised feature learning. In addition, while optical flow has been a very studied problem in computer vision for a long time, future frame prediction is rarely approached. Still, many vision applications could benefit from the knowledge of the next frames of videos, that does not require the complexity of tracking every pixel trajectories. In this work, we train a convolutional network to generate future frames given an input sequence. To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset",0
"Constructing an accurate internal representation that models image evolution is necessary for predicting future images from a video sequence. Pixel-space video prediction is seen as a promising avenue for unsupervised feature learning due to its ability to capture content and dynamics. While optical flow has been extensively studied in computer vision, future frame prediction is rarely explored. Knowing the next frames of videos could benefit many vision applications without the need to track every pixel trajectory. In this study, we use a convolutional network to generate future frames from an input sequence. To address the inherent blurriness in predictions from the standard Mean Squared Error (MSE) loss function, we propose three complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. Our predictions are compared to published results based on recurrent neural networks on the UCF101 dataset.",1
"This paper addresses the problem of detecting coherent motions in crowd scenes and presents its two applications in crowd scene understanding: semantic region detection and recurrent activity mining. It processes input motion fields (e.g., optical flow fields) and produces a coherent motion filed, named as thermal energy field. The thermal energy field is able to capture both motion correlation among particles and the motion trends of individual particles which are helpful to discover coherency among them. We further introduce a two-step clustering process to construct stable semantic regions from the extracted time-varying coherent motions. These semantic regions can be used to recognize pre-defined activities in crowd scenes. Finally, we introduce a cluster-and-merge process which automatically discovers recurrent activities in crowd scenes by clustering and merging the extracted coherent motions. Experiments on various videos demonstrate the effectiveness of our approach.",0
"The focus of this paper is the identification of cohesive movements in crowds, and its applications in comprehending crowd scenes, specifically in semantic region recognition and recurrent activity detection. The paper utilizes input motion fields, such as optical flow fields, to generate a thermal energy field that captures both motion correlation among particles and individual particle motion trends, aiding in the identification of coherency. The paper also proposes a two-step clustering process to establish stable semantic regions from the extracted time-varying coherent motions, which can be used to recognize pre-defined activities in crowd scenes. Additionally, the cluster-and-merge process is introduced, which automatically detects recurrent activities by clustering and merging the extracted coherent motions. The effectiveness of this approach is demonstrated through experiments conducted on various videos.",1
"We propose robust methods for estimating camera egomotion in noisy, real-world monocular image sequences in the general case of unknown observer rotation and translation with two views and a small baseline. This is a difficult problem because of the nonconvex cost function of the perspective camera motion equation and because of non-Gaussian noise arising from noisy optical flow estimates and scene non-rigidity. To address this problem, we introduce the expected residual likelihood method (ERL), which estimates confidence weights for noisy optical flow data using likelihood distributions of the residuals of the flow field under a range of counterfactual model parameters. We show that ERL is effective at identifying outliers and recovering appropriate confidence weights in many settings. We compare ERL to a novel formulation of the perspective camera motion equation using a lifted kernel, a recently proposed optimization framework for joint parameter and confidence weight estimation with good empirical properties. We incorporate these strategies into a motion estimation pipeline that avoids falling into local minima. We find that ERL outperforms the lifted kernel method and baseline monocular egomotion estimation strategies on the challenging KITTI dataset, while adding almost no runtime cost over baseline egomotion methods.",0
"In this study, we present robust techniques to estimate camera egomotion in real-world monocular image sequences with two views and a small baseline, where the observer's rotation and translation are unknown. This is a challenging task due to the nonconvex cost function of the perspective camera motion equation and non-Gaussian noise caused by noisy optical flow estimates and non-rigid scenes. To overcome these issues, we introduce the expected residual likelihood method (ERL), which assigns confidence weights to the noisy optical flow data using likelihood distributions of the flow field residuals under various counterfactual model parameters. Our results show that ERL is effective in identifying outliers and recovering appropriate confidence weights in various scenarios. We compare ERL to a novel formulation of the perspective camera motion equation that employs a lifted kernel, an optimization framework for joint parameter and confidence weight estimation with promising empirical properties. We integrate these methods into a motion estimation pipeline that avoids local minima. Our experiments on the challenging KITTI dataset demonstrate that ERL outperforms the lifted kernel approach and baseline monocular egomotion estimation methods, without adding significant runtime costs.",1
"Object segmentation in infant's egocentric videos is a fundamental step in studying how children perceive objects in early stages of development. From the computer vision perspective, object segmentation in such videos pose quite a few challenges because the child's view is unfocused, often with large head movements, effecting in sudden changes in the child's point of view which leads to frequent change in object properties such as size, shape and illumination. In this paper, we develop a semi-automated, domain specific, method to address these concerns and facilitate the object annotation process for cognitive scientists allowing them to select and monitor the object under segmentation. The method starts with an annotation from the user of the desired object and employs graph cut segmentation and optical flow computation to predict the object mask for subsequent video frames automatically. To maintain accuracy, we use domain specific heuristic rules to re-initialize the program with new user input whenever object properties change dramatically. The evaluations demonstrate the high speed and accuracy of the presented method for object segmentation in voluminous egocentric videos. We apply the proposed method to investigate potential patterns in object distribution in child's view at progressive ages.",0
"The segmentation of objects in egocentric videos of infants is a crucial aspect of understanding how children perceive objects during early stages of development. However, this task poses several challenges from a computer vision perspective due to the child's view being unfocused and prone to sudden changes in perspective, resulting in alterations in object properties such as size, shape, and illumination. This paper introduces a semi-automated, domain-specific method to address these issues, making the object annotation process more accessible for cognitive scientists. The method involves the user annotating the desired object and utilizing graph cut segmentation and optical flow computation to automatically predict the object mask for subsequent video frames. Domain-specific heuristic rules are implemented to ensure accuracy, and the program is re-initialized with new user input whenever object properties change significantly. The proposed method is evaluated for its speed and accuracy in object segmentation in voluminous egocentric videos. It is applied to investigate potential patterns in object distribution in a child's view at different ages.",1
"This paper shows how to extract dense optical flow from videos with a convolutional neural network (CNN). The proposed model constitutes a potential building block for deeper architectures to allow using motion without resorting to an external algorithm, \eg for recognition in videos. We derive our network architecture from signal processing principles to provide desired invariances to image contrast, phase and texture. We constrain weights within the network to enforce strict rotation invariance and substantially reduce the number of parameters to learn. We demonstrate end-to-end training on only 8 sequences of the Middlebury dataset, orders of magnitude less than competing CNN-based motion estimation methods, and obtain comparable performance to classical methods on the Middlebury benchmark. Importantly, our method outputs a distributed representation of motion that allows representing multiple, transparent motions, and dynamic textures. Our contributions on network design and rotation invariance offer insights nonspecific to motion estimation.",0
"In this study, a convolutional neural network (CNN) is utilized to extract dense optical flow from videos. The proposed model has the potential to be used as a building block for deeper architectures, enabling motion recognition in videos without the need for an external algorithm. The network architecture is designed based on signal processing principles to ensure desired invariances to image contrast, phase, and texture. Furthermore, weights within the network are constrained to enforce strict rotation invariance, resulting in a significant reduction in the number of parameters to learn. With only 8 sequences from the Middlebury dataset, our method achieves comparable performance to classical methods on the Middlebury benchmark. Moreover, the method outputs a distributed representation of motion, allowing for the representation of multiple, transparent motions and dynamic textures. Our findings on network design and rotation invariance offer insights that are not specific to motion estimation.",1
"Dense image matching is a fundamental low-level problem in Computer Vision, which has received tremendous attention from both discrete and continuous optimization communities. The goal of this paper is to combine the advantages of discrete and continuous optimization in a coherent framework. We devise a model based on energy minimization, to be optimized by both discrete and continuous algorithms in a consistent way. In the discrete setting, we propose a novel optimization algorithm that can be massively parallelized. In the continuous setting we tackle the problem of non-convex regularizers by a formulation based on differences of convex functions. The resulting hybrid discrete-continuous algorithm can be efficiently accelerated by modern GPUs and we demonstrate its real-time performance for the applications of dense stereo matching and optical flow.",0
"The field of Computer Vision places great emphasis on solving the core problem of Dense Image Matching, which has garnered significant interest from both continuous and discrete optimization communities. This paper aims to unify the strengths of both approaches by introducing a model centered on energy minimization that can be optimized using both discrete and continuous algorithms in a consistent manner. To achieve this, we introduce a novel optimization technique that is highly parallelizable in the discrete setting. Additionally, we address the challenge of non-convex regularizers in the continuous setting by implementing a formulation that relies on differences of convex functions. Our resulting algorithm, which combines both discrete and continuous approaches, is highly efficient and can be accelerated using modern GPUs. Finally, we demonstrate the algorithm's real-time capabilities by applying it to problems in dense stereo matching and optical flow.",1
"Traditional methods for motion estimation estimate the motion field F between a pair of images as the one that minimizes a predesigned cost function. In this paper, we propose a direct method and train a Convolutional Neural Network (CNN) that when, at test time, is given a pair of images as input it produces a dense motion field F at its output layer. In the absence of large datasets with ground truth motion that would allow classical supervised training, we propose to train the network in an unsupervised manner. The proposed cost function that is optimized during training, is based on the classical optical flow constraint. The latter is differentiable with respect to the motion field and, therefore, allows backpropagation of the error to previous layers of the network. Our method is tested on both synthetic and real image sequences and performs similarly to the state-of-the-art methods.",0
"The conventional approach to motion estimation involves finding the motion field F between two images that minimizes a predetermined cost function. However, this paper presents a new method using a Convolutional Neural Network (CNN) that can produce a dense motion field F when given a pair of images as input. To accomplish this, we trained the network in an unsupervised manner since there were no large datasets with ground truth motion available for classical supervised training. During training, we optimized a proposed cost function based on the optical flow constraint, which is differentiable with respect to the motion field and allows for backpropagation of the error to previous layers of the network. Our method was tested on both synthetic and real image sequences and achieved similar results to state-of-the-art methods.",1
"An ever increasing number of computer vision and image/video processing challenges are being approached using deep convolutional neural networks, obtaining state-of-the-art results in object recognition and detection, semantic segmentation, action recognition, optical flow and superresolution. Hardware acceleration of these algorithms is essential to adopt these improvements in embedded and mobile computer vision systems. We present a new architecture, design and implementation as well as the first reported silicon measurements of such an accelerator, outperforming previous work in terms of power-, area- and I/O-efficiency. The manufactured device provides up to 196 GOp/s on 3.09 mm^2 of silicon in UMC 65nm technology and can achieve a power efficiency of 803 GOp/s/W. The massively reduced bandwidth requirements make it the first architecture scalable to TOp/s performance.",0
"Deep convolutional neural networks are being increasingly used to tackle various computer vision and image/video processing challenges, resulting in cutting-edge outcomes in object recognition and detection, semantic segmentation, action recognition, optical flow, and superresolution. For embedded and mobile computer vision systems to leverage these advancements, hardware acceleration of these algorithms is crucial. Our study introduces a novel architecture, design, and implementation, along with the initial reported silicon measurements of such an accelerator. Our device surpasses previous work in terms of power, area, and I/O efficiency, providing up to 196 GOp/s on 3.09 mm^2 of silicon in UMC 65nm technology. Furthermore, it can achieve a power efficiency of 803 GOp/s/W, with greatly reduced bandwidth requirements, making it the first architecture scalable to TOp/s performance.",1
"Sparse representation-based classifiers have shown outstanding accuracy and robustness in image classification tasks even with the presence of intense noise and occlusion. However, it has been discovered that the performance degrades significantly either when test image is not aligned with the dictionary atoms or the dictionary atoms themselves are not aligned with each other, in which cases the sparse linear representation assumption fails. In this paper, having both training and test images misaligned, we introduce a novel sparse coding framework that is able to efficiently adapt the dictionary atoms to the test image via large displacement optical flow. In the proposed algorithm, every dictionary atom is automatically aligned with the input image and the sparse code is then recovered using the adapted dictionary atoms. A corresponding supervised dictionary learning algorithm is also developed for the proposed framework. Experimental results on digit datasets recognition verify the efficacy and robustness of the proposed algorithm.",0
"The use of sparse representation-based classifiers has proven to be highly accurate and reliable for image classification tasks, even when faced with significant levels of noise and obstruction. However, it has been discovered that the performance of these classifiers can suffer greatly if the test image is not properly aligned with the dictionary atoms or if the dictionary atoms themselves are not aligned with each other. This is due to the failure of the sparse linear representation assumption. In this paper, we propose a novel sparse coding framework that can adapt the dictionary atoms to the test image, even when both the training and test images are misaligned. This is accomplished using large displacement optical flow. Our algorithm automatically aligns every dictionary atom with the input image, allowing for the recovered sparse code to be accurately obtained using the adapted dictionary atoms. Additionally, we have developed a supervised dictionary learning algorithm for this framework. Experimental results on digit datasets have confirmed the effectiveness and robustness of our proposed algorithm.",1
"Given a scene, what is going to move, and in what direction will it move? Such a question could be considered a non-semantic form of action prediction. In this work, we present a convolutional neural network (CNN) based approach for motion prediction. Given a static image, this CNN predicts the future motion of each and every pixel in the image in terms of optical flow. Our CNN model leverages the data in tens of thousands of realistic videos to train our model. Our method relies on absolutely no human labeling and is able to predict motion based on the context of the scene. Because our CNN model makes no assumptions about the underlying scene, it can predict future optical flow on a diverse set of scenarios. We outperform all previous approaches by large margins.",0
"This article discusses the use of a convolutional neural network (CNN) to predict motion in a given scene. The goal is to determine what will move and in which direction it will move, which is considered a non-semantic form of action prediction. By leveraging data from thousands of videos, the CNN can predict the future motion of each pixel in an image in terms of optical flow. This method requires no human labeling and can predict motion based on the context of the scene. The CNN model makes no assumptions about the underlying scene and can predict optical flow in various scenarios, outperforming previous approaches significantly.",1
"Recent work has shown that optical flow estimation can be formulated as a supervised learning task and can be successfully solved with convolutional networks. Training of the so-called FlowNet was enabled by a large synthetically generated dataset. The present paper extends the concept of optical flow estimation via convolutional networks to disparity and scene flow estimation. To this end, we propose three synthetic stereo video datasets with sufficient realism, variation, and size to successfully train large networks. Our datasets are the first large-scale datasets to enable training and evaluating scene flow methods. Besides the datasets, we present a convolutional network for real-time disparity estimation that provides state-of-the-art results. By combining a flow and disparity estimation network and training it jointly, we demonstrate the first scene flow estimation with a convolutional network.",0
"Recently, it has been discovered that optical flow estimation can be approached as a supervised learning task and solved effectively using convolutional networks. The FlowNet was created by training on a large, artificially generated dataset. This study expands upon this concept by introducing convolutional networks for estimating disparity and scene flow. To achieve this, three synthetic stereo video datasets were created, which are sufficiently realistic, varied, and large enough to train large networks. These datasets are the first to allow for the training and evaluation of scene flow methods on a large scale. In addition to the datasets, a convolutional network for real-time disparity estimation that delivers cutting-edge results is presented. By training a flow and disparity estimation network together, we demonstrate the first scene flow estimation using a convolutional network.",1
"In this work we propose a technique that transfers supervision between images from different modalities. We use learned representations from a large labeled modality as a supervisory signal for training representations for a new unlabeled paired modality. Our method enables learning of rich representations for unlabeled modalities and can be used as a pre-training procedure for new modalities with limited labeled data. We show experimental results where we transfer supervision from labeled RGB images to unlabeled depth and optical flow images and demonstrate large improvements for both these cross modal supervision transfers. Code, data and pre-trained models are available at https://github.com/s-gupta/fast-rcnn/tree/distillation",0
"Our proposed technique involves transferring supervision between images of different modalities. By utilizing representations learned from a large labeled modality, we can train representations for a new unlabeled paired modality. This approach facilitates the acquisition of rich representations for unlabeled modalities and serves as a pre-training method for new modalities with restricted labeled data. Our experiments showcase successful supervision transfers from labeled RGB images to unlabeled depth and optical flow images, resulting in significant improvements. Interested parties can access our code, data, and pre-trained models at https://github.com/s-gupta/fast-rcnn/tree/distillation.",1
"While egocentric video is becoming increasingly popular, browsing it is very difficult. In this paper we present a compact 3D Convolutional Neural Network (CNN) architecture for long-term activity recognition in egocentric videos. Recognizing long-term activities enables us to temporally segment (index) long and unstructured egocentric videos. Existing methods for this task are based on hand tuned features derived from visible objects, location of hands, as well as optical flow.   Given a sparse optical flow volume as input, our CNN classifies the camera wearer's activity. We obtain classification accuracy of 89%, which outperforms the current state-of-the-art by 19%. Additional evaluation is performed on an extended egocentric video dataset, classifying twice the amount of categories than current state-of-the-art. Furthermore, our CNN is able to recognize whether a video is egocentric or not with 99.2% accuracy, up by 24% from current state-of-the-art. To better understand what the network actually learns, we propose a novel visualization of CNN kernels as flow fields.",0
"The popularity of egocentric video is on the rise, but it is not easy to browse. In this paper, we introduce a compact 3D Convolutional Neural Network (CNN) design that can recognize long-term activities in egocentric videos. This recognition ability allows us to index long and unstructured egocentric videos in a temporal manner. Compared to existing methods that depend on handcrafted features extracted from visible objects, hand location, and optical flow, our CNN can classify the activity of the camera wearer using a sparse optical flow volume as input. Our classification accuracy reaches 89%, which is 19% higher than the current state-of-the-art. We also evaluate our CNN on an extended egocentric video dataset and classify twice as many categories as the current state-of-the-art. Moreover, our CNN can determine whether a video is egocentric or not with an accuracy of 99.2%, which is 24% better than the current state-of-the-art. We also introduce a novel visualization of CNN kernels as flow fields to improve our understanding of what the network learns.",1
"Over the last few years deep learning methods have emerged as one of the most prominent approaches for video analysis. However, so far their most successful applications have been in the area of video classification and detection, i.e., problems involving the prediction of a single class label or a handful of output variables per video. Furthermore, while deep networks are commonly recognized as the best models to use in these domains, there is a widespread perception that in order to yield successful results they often require time-consuming architecture search, manual tweaking of parameters and computationally intensive pre-processing or post-processing methods.   In this paper we challenge these views by presenting a deep 3D convolutional architecture trained end to end to perform voxel-level prediction, i.e., to output a variable at every voxel of the video. Most importantly, we show that the same exact architecture can be used to achieve competitive results on three widely different voxel-prediction tasks: video semantic segmentation, optical flow estimation, and video coloring. The three networks learned on these problems are trained from raw video without any form of preprocessing and their outputs do not require post-processing to achieve outstanding performance. Thus, they offer an efficient alternative to traditional and much more computationally expensive methods in these video domains.",0
"In recent years, deep learning methods have become a prominent approach for analyzing videos. However, their most successful applications have been limited to problems involving the prediction of a single class label or a few output variables per video. Additionally, there is a common perception that these methods require time-consuming architecture search, manual parameter tweaking, and computationally intensive pre- or post-processing methods to achieve successful results. This paper challenges these beliefs by presenting a deep 3D convolutional architecture trained end-to-end for voxel-level prediction, meaning it outputs a variable at every voxel of the video. The same architecture achieves competitive results for three different voxel-prediction tasks: video semantic segmentation, optical flow estimation, and video coloring. These networks are trained from raw video without preprocessing and do not require post-processing for outstanding performance. Thus, they offer an efficient alternative to traditional, more computationally expensive methods in these video domains.",1
"Image and video classification research has made great progress through the development of handcrafted local features and learning based features. These two architectures were proposed roughly at the same time and have flourished at overlapping stages of history. However, they are typically viewed as distinct approaches. In this paper, we emphasize their structural similarities and show how such a unified view helps us in designing features that balance efficiency and effectiveness. As an example, we study the problem of designing efficient video feature learning algorithms for action recognition.   We approach this problem by first showing that local handcrafted features and Convolutional Neural Networks (CNNs) share the same convolution-pooling network structure. We then propose a two-stream Convolutional ISA (ConvISA) that adopts the convolution-pooling structure of the state-of-the-art handcrafted video feature with greater modeling capacities and a cost-effective training algorithm. Through custom designed network structures for pixels and optical flow, our method also reflects distinctive characteristics of these two data sources.   Our experimental results on standard action recognition benchmarks show that by focusing on the structure of CNNs, rather than end-to-end training methods, we are able to design an efficient and powerful video feature learning algorithm.",0
"Great strides have been made in image and video classification research through the use of handcrafted local features and learning-based features. While these two approaches were introduced around the same time and have progressed concurrently, they are usually considered separate methods. This paper highlights their structural similarities and demonstrates how a unified perspective can aid in the creation of features that balance efficiency and effectiveness. Specifically, the paper focuses on the challenge of developing efficient video feature learning algorithms for action recognition. The authors begin by revealing that local handcrafted features and Convolutional Neural Networks (CNNs) share a convolution-pooling network structure. They then introduce a two-stream Convolutional ISA (ConvISA) that adopts the convolution-pooling structure of the most advanced handcrafted video feature while simultaneously offering increased modeling capabilities and a cost-effective training algorithm. The authors also incorporate custom designed network structures for pixels and optical flow, reflecting the unique characteristics of these two data sources. Through their experiments on standard action recognition benchmarks, the authors demonstrate that by concentrating on the structure of CNNs, rather than end-to-end training methods, they can design a potent and efficient video feature learning algorithm.",1
"We present a deeply integrated method of exploiting low-cost gyroscopes to improve general purpose feature tracking. Most previous methods use gyroscopes to initialize and bound the search for features. In contrast, we use them to regularize the tracking energy function so that they can directly assist in the tracking of ambiguous and poor-quality features. We demonstrate that our simple technique offers significant improvements in performance over conventional template-based tracking methods, and is in fact competitive with more complex and computationally expensive state-of-the-art trackers, but at a fraction of the computational cost. Additionally, we show that the practice of initializing template-based feature trackers like KLT (Kanade-Lucas-Tomasi) using gyro-predicted optical flow offers no advantage over using a careful optical-only initialization method, suggesting that some deeper level of integration, like the method we propose, is needed in order to realize a genuine improvement in tracking performance from these inertial sensors.",0
"Our approach involves integrating low-cost gyroscopes into feature tracking to improve its overall performance. Unlike previous methods that solely use gyroscopes for feature initialization and bounding, we incorporate them into the tracking energy function to assist in tracking low-quality and ambiguous features. Our straightforward technique yields significant performance improvements compared to traditional template-based tracking methods and competes with complex, expensive state-of-the-art trackers but at a fraction of the computational cost. Furthermore, we found that using gyro-predicted optical flow to initialize feature trackers like KLT does not offer any advantage over an optical-only approach, indicating the need for deeper integration, as proposed in our method, to achieve genuine improvements in tracking performance using inertial sensors.",1
"Modern large displacement optical flow algorithms usually use an initialization by either sparse descriptor matching techniques or dense approximate nearest neighbor fields. While the latter have the advantage of being dense, they have the major disadvantage of being very outlier prone as they are not designed to find the optical flow, but the visually most similar correspondence. In this paper we present a dense correspondence field approach that is much less outlier prone and thus much better suited for optical flow estimation than approximate nearest neighbor fields. Our approach is conceptually novel as it does not require explicit regularization, smoothing (like median filtering) or a new data term, but solely our novel purely data based search strategy that finds most inliers (even for small objects), while it effectively avoids finding outliers. Moreover, we present novel enhancements for outlier filtering. We show that our approach is better suited for large displacement optical flow estimation than state-of-the-art descriptor matching techniques. We do so by initializing EpicFlow (so far the best method on MPI-Sintel) with our Flow Fields instead of their originally used state-of-the-art descriptor matching technique. We significantly outperform the original EpicFlow on MPI-Sintel, KITTI and Middlebury.",0
"Currently, large displacement optical flow algorithms typically use either sparse descriptor matching techniques or dense approximate nearest neighbor fields for initialization. However, while dense fields have the advantage of being dense, they are highly susceptible to outliers as they are not designed to detect optical flow but rather the most visually similar correspondence. To address this issue, we propose a novel approach using a dense correspondence field that is much less prone to outliers and thus better suited for optical flow estimation than approximate nearest neighbor fields. Our approach does not require explicit regularization, smoothing, or a new data term, but instead employs a purely data-based search strategy that effectively avoids detecting outliers while finding the most inliers, even for small objects. Additionally, we present innovative enhancements for outlier filtering. We demonstrate the superiority of our approach in comparison to state-of-the-art descriptor matching techniques by initializing EpicFlow, the current best method on MPI-Sintel, with our Flow Fields rather than their prior descriptor matching technique. Our method significantly outperforms the original EpicFlow on MPI-Sintel, KITTI, and Middlebury.",1
"We introduce a novel matching algorithm, called DeepMatching, to compute dense correspondences between images. DeepMatching relies on a hierarchical, multi-layer, correlational architecture designed for matching images and was inspired by deep convolutional approaches. The proposed matching algorithm can handle non-rigid deformations and repetitive textures and efficiently determines dense correspondences in the presence of significant changes between images. We evaluate the performance of DeepMatching, in comparison with state-of-the-art matching algorithms, on the Mikolajczyk (Mikolajczyk et al 2005), the MPI-Sintel (Butler et al 2012) and the Kitti (Geiger et al 2013) datasets. DeepMatching outperforms the state-of-the-art algorithms and shows excellent results in particular for repetitive textures.We also propose a method for estimating optical flow, called DeepFlow, by integrating DeepMatching in the large displacement optical flow (LDOF) approach of Brox and Malik (2011). Compared to existing matching algorithms, additional robustness to large displacements and complex motion is obtained thanks to our matching approach. DeepFlow obtains competitive performance on public benchmarks for optical flow estimation.",0
"In this article, we present a new algorithm named DeepMatching, which can be used to calculate dense correspondences between images. The algorithm is based on a hierarchical, multi-layer, correlational architecture that has been inspired by deep convolutional approaches. It is capable of handling non-rigid deformations and repetitive textures, and can efficiently determine dense correspondences even when significant changes occur between images. We evaluated the performance of DeepMatching on several datasets and found that it outperformed existing algorithms, especially with repetitive textures. We also developed a method for estimating optical flow, called DeepFlow, by integrating DeepMatching with the large displacement optical flow (LDOF) approach of Brox and Malik. This approach provides additional robustness to large displacements and complex motion, and DeepFlow achieved competitive performance on public benchmarks for optical flow estimation.",1
"We propose the Encoder-Recurrent-Decoder (ERD) model for recognition and prediction of human body pose in videos and motion capture. The ERD model is a recurrent neural network that incorporates nonlinear encoder and decoder networks before and after recurrent layers. We test instantiations of ERD architectures in the tasks of motion capture (mocap) generation, body pose labeling and body pose forecasting in videos. Our model handles mocap training data across multiple subjects and activity domains, and synthesizes novel motions while avoid drifting for long periods of time. For human pose labeling, ERD outperforms a per frame body part detector by resolving left-right body part confusions. For video pose forecasting, ERD predicts body joint displacements across a temporal horizon of 400ms and outperforms a first order motion model based on optical flow. ERDs extend previous Long Short Term Memory (LSTM) models in the literature to jointly learn representations and their dynamics. Our experiments show such representation learning is crucial for both labeling and prediction in space-time. We find this is a distinguishing feature between the spatio-temporal visual domain in comparison to 1D text, speech or handwriting, where straightforward hard coded representations have shown excellent results when directly combined with recurrent units.",0
"Our proposal is the ERD model, a recurrent neural network that utilizes nonlinear encoder and decoder networks before and after recurrent layers for the recognition and prediction of human body pose in videos and motion capture. We evaluate different ERD architectures in motion capture generation, body pose labeling, and body pose forecasting in videos. Our model handles mocap training data from various subjects and activity domains, generating new motions while avoiding drifting for extended periods. ERD outperforms a per frame body part detector for human pose labeling by resolving left-right body part confusions, and it predicts body joint displacements across a 400ms temporal horizon, surpassing a first order motion model based on optical flow for video pose forecasting. ERDs expand on previous LSTM models by jointly learning representations and dynamics, which is crucial for labeling and prediction in space-time. This distinguishes the spatio-temporal visual domain from 1D text, speech, or handwriting, where simple hard-coded representations have yielded excellent results when combined with recurrent units.",1
"Computational neuroscience studies that have examined human visual system through functional magnetic resonance imaging (fMRI) have identified a model where the mammalian brain pursues two distinct pathways (for recognition of biological movement tasks). In the brain, dorsal stream analyzes the information of motion (optical flow), which is the fast features, and ventral stream (form pathway) analyzes form information (through active basis model based incremental slow feature analysis ) as slow features. The proposed approach suggests the motion perception of the human visual system composes of fast and slow feature interactions that identifies biological movements. Form features in the visual system biologically follows the application of active basis model with incremental slow feature analysis for the extraction of the slowest form features of human objects movements in the ventral stream. Applying incremental slow feature analysis provides an opportunity to use the action prototypes. To extract the slowest features episodic observation is required but the fast features updates the processing of motion information in every frames. Experimental results have shown promising accuracy for the proposed model and good performance with two datasets (KTH and Weizmann).",0
"Research in computational neuroscience using fMRI has revealed a model that describes two distinct pathways in the mammalian brain for recognizing biological movement tasks. The dorsal stream analyzes optical flow, which constitutes fast motion features, while the ventral stream (form pathway) analyzes form information through active basis model-based incremental slow feature analysis, which represents slow features. By combining fast and slow feature interactions, the proposed approach can identify biological movements in the human visual system. For the extraction of the slowest form features of human objects movements in the ventral stream, form features in the visual system follow the application of active basis model with incremental slow feature analysis. To extract the slowest features, episodic observation is required, while the fast features update the processing of motion information in every frame. Experimental results have shown promising accuracy for the proposed model, with good performance on two datasets (KTH and Weizmann).",1
"A technique for the enhancement of point targets in clutter is described. The local 3-D spectrum at each pixel is estimated recursively. An optical flow-field for the textured background is then generated using the 3-D autocorrelation function and the local velocity estimates are used to apply high-pass velocity-selective spatiotemporal filters, with finite impulse responses (FIRs), to subtract the background clutter signal, leaving the foreground target signal, plus noise. Parallel software implementations using a multicore central processing unit (CPU) and a graphical processing unit (GPU) are investigated.",0
"The article presents a method to improve the visibility of point targets in the presence of clutter. The process involves recursively estimating the 3-D spectrum at each pixel, generating an optical flow-field for the textured background using the 3-D autocorrelation function, and applying velocity-selective spatiotemporal filters with finite impulse responses (FIRs) to remove the background clutter signal. This leaves behind the foreground target signal and some noise. The study also explores the use of parallel software implementations utilizing both a multicore CPU and a GPU.",1
"Two complementary approaches have been extensively used in signal and image processing leading to novel results, the sparse representation methodology and the variational strategy. Recently, a new sparsity based model has been proposed, the cosparse analysis framework, which may potentially help in bridging sparse approximation based methods to the traditional total-variation minimization. Based on this, we introduce a sparsity based framework for solving overparameterized variational problems. The latter has been used to improve the estimation of optical flow and also for general denoising of signals and images. However, the recovery of the space varying parameters involved was not adequately addressed by traditional variational methods. We first demonstrate the efficiency of the new framework for one dimensional signals in recovering a piecewise linear and polynomial function. Then, we illustrate how the new technique can be used for denoising and segmentation of images.",0
"Signal and image processing have seen significant advancements through the utilization of two distinct approaches, namely, the sparse representation methodology and the variational strategy. A new model called the cosparse analysis framework has recently emerged, which has the potential to bridge the gap between sparse approximation-based methods and the conventional total-variation minimization. Building on this, we propose a sparsity based framework that can effectively solve overparameterized variational problems, leading to improved estimation of optical flow and denoising of signals and images. However, traditional variational methods have not adequately addressed the recovery of space-varying parameters. We first demonstrate the effectiveness of the new framework by recovering a piecewise linear and polynomial function for one-dimensional signals. We then showcase how this technique can be utilized for image denoising and segmentation.",1
"Honey bees use optical flow to avoid obstacles effectively. In this research work similar methodology was tested on a simulated mobile robot. Simulation framework was based on VRML and Simulink in a 3D world. Optical flow vectors were calculated from a video scene captured by a virtual camera which was used as inputs to a fuzzy logic controller. Fuzzy logic controller decided the locomotion of the robot. Different fuzzy logic rules were evaluated. The robot was able to navigate through complex static and dynamic environments effectively, avoiding obstacles on its path.",0
"The use of optical flow by honey bees to effectively evade obstacles was tested on a simulated mobile robot in this study. The simulation framework was constructed in a 3D world using VRML and Simulink. Optical flow vectors were obtained from a video scene taken by a virtual camera, and were utilized as inputs to a fuzzy logic controller. The fuzzy logic controller was responsible for determining the robot's movement, and various fuzzy logic rules were assessed. The robot successfully navigated through intricate static and dynamic surroundings, skillfully avoiding obstacles in its way.",1
"Spatial multiplexing cameras (SMCs) acquire a (typically static) scene through a series of coded projections using a spatial light modulator (e.g., a digital micro-mirror device) and a few optical sensors. This approach finds use in imaging applications where full-frame sensors are either too expensive (e.g., for short-wave infrared wavelengths) or unavailable. Existing SMC systems reconstruct static scenes using techniques from compressive sensing (CS). For videos, however, existing acquisition and recovery methods deliver poor quality. In this paper, we propose the CS multi-scale video (CS-MUVI) sensing and recovery framework for high-quality video acquisition and recovery using SMCs. Our framework features novel sensing matrices that enable the efficient computation of a low-resolution video preview, while enabling high-resolution video recovery using convex optimization. To further improve the quality of the reconstructed videos, we extract optical-flow estimates from the low-resolution previews and impose them as constraints in the recovery procedure. We demonstrate the efficacy of our CS-MUVI framework for a host of synthetic and real measured SMC video data, and we show that high-quality videos can be recovered at roughly $60\times$ compression.",0
"Spatial multiplexing cameras (SMCs) capture a scene through coded projections using a spatial light modulator and a few optical sensors, which is useful in cases where full-frame sensors are expensive or not available. However, existing SMC systems employing compressive sensing (CS) for static scenes do not produce high-quality videos. In this research, we introduce the CS multi-scale video (CS-MUVI) sensing and recovery framework that enables the efficient computation of a low-resolution video preview and high-resolution video recovery using convex optimization. To enhance video quality, we incorporate optical-flow estimates from low-resolution previews as constraints in the recovery process. Our CS-MUVI framework demonstrates its effectiveness in both synthetic and real measured SMC video data and can recover high-quality videos at about $60\times$ compression.",1
"In this work, we introduce a deep-structured conditional random field (DS-CRF) model for the purpose of state-based object silhouette tracking. The proposed DS-CRF model consists of a series of state layers, where each state layer spatially characterizes the object silhouette at a particular point in time. The interactions between adjacent state layers are established by inter-layer connectivity dynamically determined based on inter-frame optical flow. By incorporate both spatial and temporal context in a dynamic fashion within such a deep-structured probabilistic graphical model, the proposed DS-CRF model allows us to develop a framework that can accurately and efficiently track object silhouettes that can change greatly over time, as well as under different situations such as occlusion and multiple targets within the scene. Experiment results using video surveillance datasets containing different scenarios such as occlusion and multiple targets showed that the proposed DS-CRF approach provides strong object silhouette tracking performance when compared to baseline methods such as mean-shift tracking, as well as state-of-the-art methods such as context tracking and boosted particle filtering.",0
"Our study introduces a model called the deep-structured conditional random field (DS-CRF), designed to track state-based object silhouettes. The model comprises state layers that spatially depict object silhouettes at various points in time. The layers are interconnected through inter-layer connectivity, which is dynamically determined based on inter-frame optical flow. The DS-CRF model incorporates both spatial and temporal context within a dynamic deep-structured probabilistic graphical model. This allows for accurate and efficient tracking of object silhouettes in evolving situations such as occlusion and multiple targets within the scene. The DS-CRF approach was tested on video surveillance datasets with various scenarios, and it provided strong object silhouette tracking results when compared to baseline methods such as mean-shift tracking, and state-of-the-art methods such as context tracking and boosted particle filtering.",1
"Several state-of-the-art video deblurring methods are based on a strong assumption that the captured scenes are static. These methods fail to deblur blurry videos in dynamic scenes. We propose a video deblurring method to deal with general blurs inherent in dynamic scenes, contrary to other methods. To handle locally varying and general blurs caused by various sources, such as camera shake, moving objects, and depth variation in a scene, we approximate pixel-wise kernel with bidirectional optical flows. Therefore, we propose a single energy model that simultaneously estimates optical flows and latent frames to solve our deblurring problem. We also provide a framework and efficient solvers to optimize the energy model. By minimizing the proposed energy function, we achieve significant improvements in removing blurs and estimating accurate optical flows in blurry frames. Extensive experimental results demonstrate the superiority of the proposed method in real and challenging videos that state-of-the-art methods fail in either deblurring or optical flow estimation.",0
"Current video deblurring methods rely on the assumption that the scenes captured are static, which means they are not effective in removing blurs from dynamic scenes. We propose a novel video deblurring method that can handle general blurs that occur in dynamic scenes. Our method approximates pixel-wise kernel with bidirectional optical flows to address locally varying and general blurs caused by various sources, such as camera shake, moving objects, and depth variation in a scene. To solve our deblurring problem, we propose a single energy model that simultaneously estimates optical flows and latent frames. We also offer a framework and efficient solvers to optimize the energy model. By minimizing the proposed energy function, we achieve significant improvements in removing blurs and estimating accurate optical flows in blurry frames. Our experimental results demonstrate that our method outperforms state-of-the-art methods in real and challenging videos that fail in either deblurring or optical flow estimation.",1
"Crowd flow segmentation is an important step in many video surveillance tasks. In this work, we propose an algorithm for segmenting flows in H.264 compressed videos in a completely unsupervised manner. Our algorithm works on motion vectors which can be obtained by partially decoding the compressed video without extracting any additional features. Our approach is based on modelling the motion vector field as a Conditional Random Field (CRF) and obtaining oriented motion segments by finding the optimal labelling which minimises the global energy of CRF. These oriented motion segments are recursively merged based on gradient across their boundaries to obtain the final flow segments. This work in compressed domain can be easily extended to pixel domain by substituting motion vectors with motion based features like optical flow. The proposed algorithm is experimentally evaluated on a standard crowd flow dataset and its superior performance in both accuracy and computational time are demonstrated through quantitative results.",0
"Segmenting crowd flow is a crucial step in video surveillance, and we present an unsupervised algorithm for this task in H.264 compressed videos. Our method uses motion vectors from the partially decoded video, without any additional features. The motion vector field is modeled as a Conditional Random Field and optimal labeling is found to obtain oriented motion segments. These segments are merged recursively based on gradient at their boundaries to obtain the final flow segments. We demonstrate superior performance in accuracy and computational time compared to existing methods on a standard crowd flow dataset. Our approach can be extended to pixel domain using motion-based features like optical flow.",1
"We propose a novel approach for optical flow estimation , targeted at large displacements with significant oc-clusions. It consists of two steps: i) dense matching by edge-preserving interpolation from a sparse set of matches; ii) variational energy minimization initialized with the dense matches. The sparse-to-dense interpolation relies on an appropriate choice of the distance, namely an edge-aware geodesic distance. This distance is tailored to handle occlusions and motion boundaries -- two common and difficult issues for optical flow computation. We also propose an approximation scheme for the geodesic distance to allow fast computation without loss of performance. Subsequent to the dense interpolation step, standard one-level variational energy minimization is carried out on the dense matches to obtain the final flow estimation. The proposed approach, called Edge-Preserving Interpolation of Correspondences (EpicFlow) is fast and robust to large displacements. It significantly outperforms the state of the art on MPI-Sintel and performs on par on Kitti and Middlebury.",0
"Our innovative method for estimating optical flow addresses the challenge of large displacements and significant occlusions. It involves two steps: first, dense matching through edge-preserving interpolation from a sparse set of matches, and second, variational energy minimization using the dense matches as the initial input. To handle occlusions and motion boundaries, we use an edge-aware geodesic distance for the sparse-to-dense interpolation. We also propose an approximation scheme for this distance to ensure quick computation without sacrificing performance. Once the dense interpolation is complete, we carry out standard one-level variational energy minimization on the dense matches to obtain the final flow estimation. Our approach, named Edge-Preserving Interpolation of Correspondences (EpicFlow), is both efficient and resilient to large displacements. It surpasses the current state of the art on MPI-Sintel and performs equally well on Kitti and Middlebury.",1
"In this paper, we present a new feature representation for first-person videos. In first-person video understanding (e.g., activity recognition), it is very important to capture both entire scene dynamics (i.e., egomotion) and salient local motion observed in videos. We describe a representation framework based on time series pooling, which is designed to abstract short-term/long-term changes in feature descriptor elements. The idea is to keep track of how descriptor values are changing over time and summarize them to represent motion in the activity video. The framework is general, handling any types of per-frame feature descriptors including conventional motion descriptors like histogram of optical flows (HOF) as well as appearance descriptors from more recent convolutional neural networks (CNN). We experimentally confirm that our approach clearly outperforms previous feature representations including bag-of-visual-words and improved Fisher vector (IFV) when using identical underlying feature descriptors. We also confirm that our feature representation has superior performance to existing state-of-the-art features like local spatio-temporal features and Improved Trajectory Features (originally developed for 3rd-person videos) when handling first-person videos. Multiple first-person activity datasets were tested under various settings to confirm these findings.",0
"The article introduces a novel approach to representing features in first-person videos. In order to effectively recognize activities in these videos, it is crucial to capture both overall scene dynamics and noticeable local motion. The proposed framework utilizes time series pooling to abstract short-term and long-term changes in feature descriptor elements, in order to track the changes in descriptor values over time and summarize them to represent motion in the activity video. This approach can handle various types of per-frame feature descriptors, such as histogram of optical flows and appearance descriptors from convolutional neural networks. Experimental results demonstrate that this new method outperforms previous feature representations, including bag-of-visual-words and improved Fisher vector, when using identical underlying feature descriptors. Furthermore, the proposed feature representation also outperforms existing state-of-the-art features, such as local spatio-temporal features and Improved Trajectory Features developed for 3rd-person videos, when applied to first-person videos. Multiple first-person activity datasets were used to validate these results.",1
"Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks where CNNs were successful. In this paper we construct appropriate CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations.   Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.",0
"Recently, Convolutional neural networks (CNNs) have demonstrated remarkable performance in various computer vision tasks, particularly in those related to recognition. However, optical flow estimation has not been one of the areas where CNNs have excelled. This study aims to develop CNNs that are suitable for supervised learning tasks and capable of solving the optical flow estimation problem. We present two architectures, a generic one, and another that includes a layer that correlates feature vectors at different image locations. As existing datasets are insufficient for training a CNN, we generate a synthetic Flying Chairs dataset. Despite being unrealistic, networks trained on this dataset generalize well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates between 5 and 10 fps.",1
"In this work, we have developed a robust lane detection and departure warning technique. Our system is based on single camera sensor. For lane detection a modified Inverse Perspective Mapping using only a few extrinsic camera parameters and illuminant Invariant techniques is used. Lane markings are represented using a combination of 2nd and 4th order steerable filters, robust to shadowing. Effect of shadowing and extra sun light are removed using Lab color space, and illuminant invariant representation. Lanes are assumed to be cubic curves and fitted using robust RANSAC. This method can reliably detect lanes of the road and its boundary. This method has been experimented in Indian road conditions under different challenging situations and the result obtained were very good. For lane departure angle an optical flow based method were used.",0
"Our study presents a strong approach for detecting lanes and issuing warnings for departure using a single camera sensor. We have employed a modified Inverse Perspective Mapping technique that employs only a few extrinsic camera parameters and illuminant Invariant techniques for lane detection. Lane markings are identified using a combination of 2nd and 4th order steerable filters that are robust to shadowing. Lab color space and illuminant invariant representation are used to remove the impact of shadowing and extra sunlight. Lanes are assumed to be cubic curves and are fitted using robust RANSAC. This technique can detect lanes and their boundaries on the road with high reliability. We performed experiments in various challenging situations on Indian roads, and the results were excellent. Additionally, we utilized an optical flow-based method for determining the lane departure angle.",1
"This paper introduces a state-of-the-art video representation and applies it to efficient action recognition and detection. We first propose to improve the popular dense trajectory features by explicit camera motion estimation. More specifically, we extract feature point matches between frames using SURF descriptors and dense optical flow. The matches are used to estimate a homography with RANSAC. To improve the robustness of homography estimation, a human detector is employed to remove outlier matches from the human body as human motion is not constrained by the camera. Trajectories consistent with the homography are considered as due to camera motion, and thus removed. We also use the homography to cancel out camera motion from the optical flow. This results in significant improvement on motion-based HOF and MBH descriptors. We further explore the recent Fisher vector as an alternative feature encoding approach to the standard bag-of-words histogram, and consider different ways to include spatial layout information in these encodings. We present a large and varied set of evaluations, considering (i) classification of short basic actions on six datasets, (ii) localization of such actions in feature-length movies, and (iii) large-scale recognition of complex events. We find that our improved trajectory features significantly outperform previous dense trajectories, and that Fisher vectors are superior to bag-of-words encodings for video recognition tasks. In all three tasks, we show substantial improvements over the state-of-the-art results.",0
"This paper showcases a cutting-edge video representation and applies it to achieve efficient action recognition and detection. Our approach involves enhancing the widely-used dense trajectory features by incorporating explicit camera motion estimation. Specifically, we leverage SURF descriptors and dense optical flow to extract feature point matches between frames, which are then used to estimate a homography with RANSAC. To ensure the robustness of homography estimation, we use a human detector to eliminate outlier matches from the human body since human motion is not confined by the camera. Trajectories that conform to the homography are considered to be due to camera motion and are therefore removed. We also leverage the homography to eliminate camera motion from the optical flow, which leads to significant improvements on motion-based HOF and MBH descriptors. We further examine the Fisher vector as an alternative feature encoding approach to the standard bag-of-words histogram and explore various ways to incorporate spatial layout information in these encodings. We conduct a diverse set of evaluations, including (i) classification of short basic actions on six datasets, (ii) localization of such actions in feature-length movies, and (iii) large-scale recognition of complex events. Our results demonstrate that our improved trajectory features outperform previous dense trajectories, and that Fisher vectors are more effective than bag-of-words encodings for video recognition tasks. We outperform state-of-the-art results in all three tasks.",1
"We present a method for determining surface flows from solar images based upon optical flow techniques. We apply the method to sets of images obtained by a variety of solar imagers to assess its performance. The {\tt opflow3d} procedure is shown to extract accurate velocity estimates when provided perfect test data and quickly generates results consistent with completely distinct methods when applied on global scales. We also validate it in detail by comparing it to an established method when applied to high-resolution datasets and find that it provides comparable results without the need to tune, filter or otherwise preprocess the images before its application.",0
"Our paper introduces a technique that employs optical flow methods to determine surface flows from solar images. To evaluate its effectiveness, we tested the method on various sets of images captured by different solar imagers. Our results indicate that the {\tt opflow3d} procedure can produce accurate velocity estimates with perfect test data and is capable of generating results that align with other methods on a global scale. Furthermore, we conducted a thorough validation by comparing it to an established method using high-resolution datasets, and found that it delivers comparable results without requiring any image pre-processing, tuning, or filtering.",1
"Convolutional neural networks (CNNs) have been extensively applied for image recognition problems giving state-of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted. We propose two methods capable of handling full length videos. The first method explores various convolutional temporal feature pooling architectures, examining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improvements over previously published results on the Sports 1 million dataset (73.1% vs. 60.9%) and the UCF-101 datasets with (88.6% vs. 88.0%) and without additional optical flow information (82.6% vs. 72.8%).",0
"The use of convolutional neural networks (CNNs) has become widespread in solving image recognition problems, achieving impressive results in tasks such as recognition, detection, segmentation, and retrieval. This study introduces and assesses various deep neural network architectures that can pool image information over extended time periods in videos, unlike previous attempts. Two methods capable of handling full-length videos are proposed. The first method examines different convolutional temporal feature pooling designs to adapt CNNs for this task. The second method models the video explicitly as a sequence of frames and uses a recurrent neural network with Long Short-Term Memory (LSTM) cells connected to the underlying CNN's output. Our best-performing networks demonstrate significant improvements over previous results on the Sports 1 million dataset (73.1% vs. 60.9%) and the UCF-101 datasets with (88.6% vs. 88.0%) and without additional optical flow information (82.6% vs. 72.8%).",1
"In the recent past, the computer vision community has developed centralized benchmarks for the performance evaluation of a variety of tasks, including generic object and pedestrian detection, 3D reconstruction, optical flow, single-object short-term tracking, and stereo estimation. Despite potential pitfalls of such benchmarks, they have proved to be extremely helpful to advance the state of the art in the respective area. Interestingly, there has been rather limited work on the standardization of quantitative benchmarks for multiple target tracking. One of the few exceptions is the well-known PETS dataset, targeted primarily at surveillance applications. Despite being widely used, it is often applied inconsistently, for example involving using different subsets of the available data, different ways of training the models, or differing evaluation scripts. This paper describes our work toward a novel multiple object tracking benchmark aimed to address such issues. We discuss the challenges of creating such a framework, collecting existing and new data, gathering state-of-the-art methods to be tested on the datasets, and finally creating a unified evaluation system. With MOTChallenge we aim to pave the way toward a unified evaluation framework for a more meaningful quantification of multi-target tracking.",0
"Recently, the computer vision community has established centralized benchmarks to evaluate the performance of various tasks such as 3D reconstruction, optical flow, single-object short-term tracking, generic object and pedestrian detection, and stereo estimation. While these benchmarks have drawbacks, they have proven to be advantageous in advancing the field. However, there has been minimal effort to standardize quantitative benchmarks for multiple target tracking, except for the PETS dataset, which is widely used for surveillance applications but inconsistently applied. This paper presents our efforts to create a new multiple object tracking benchmark that addresses these issues. We discuss the challenges of developing such a framework, collecting both existing and new data, gathering state-of-the-art methods for testing on the datasets, and creating a unified evaluation system. Our goal with MOTChallenge is to establish a unified evaluation framework for a more meaningful quantification of multi-target tracking.",1
"Videos contain very rich semantic information. Traditional hand-crafted features are known to be inadequate in analyzing complex video semantics. Inspired by the huge success of the deep learning methods in analyzing image, audio and text data, significant efforts are recently being devoted to the design of deep nets for video analytics. Among the many practical needs, classifying videos (or video clips) based on their major semantic categories (e.g., ""skiing"") is useful in many applications. In this paper, we conduct an in-depth study to investigate important implementation options that may affect the performance of deep nets on video classification. Our evaluations are conducted on top of a recent two-stream convolutional neural network (CNN) pipeline, which uses both static frames and motion optical flows, and has demonstrated competitive performance against the state-of-the-art methods. In order to gain insights and to arrive at a practical guideline, many important options are studied, including network architectures, model fusion, learning parameters and the final prediction methods. Based on the evaluations, very competitive results are attained on two popular video classification benchmarks. We hope that the discussions and conclusions from this work can help researchers in related fields to quickly set up a good basis for further investigations along this very promising direction.",0
"The semantic information contained in videos is very comprehensive, but traditional hand-crafted features are insufficient for analyzing complex video semantics. To overcome this issue, researchers have turned to deep learning methods, which have been successful in analyzing image, audio, and text data. Video classification based on major semantic categories is useful in many applications, and in this study, we investigate implementation options that can affect the performance of deep nets on video classification. Our evaluations are conducted on a two-stream convolutional neural network (CNN) pipeline that uses both static frames and motion optical flows and has shown competitive performance. We explore several important options, including network architectures, model fusion, learning parameters, and final prediction methods, to gain insights and practical guidelines. Our evaluations on two popular video classification benchmarks show very competitive results. We hope that the conclusions from this study will assist researchers in related fields to quickly establish a strong foundation for further investigations in this promising area.",1
"Edge preserving filters preserve the edges and its information while blurring an image. In other words they are used to smooth an image, while reducing the edge blurring effects across the edge like halos, phantom etc. They are nonlinear in nature. Examples are bilateral filter, anisotropic diffusion filter, guided filter, trilateral filter etc. Hence these family of filters are very useful in reducing the noise in an image making it very demanding in computer vision and computational photography applications like denoising, video abstraction, demosaicing, optical-flow estimation, stereo matching, tone mapping, style transfer, relighting etc. This paper provides a concrete introduction to edge preserving filters starting from the heat diffusion equation in olden to recent eras, an overview of its numerous applications, as well as mathematical analysis, various efficient and optimized ways of implementation and their interrelationships, keeping focus on preserving the boundaries, spikes and canyons in presence of noise. Furthermore it provides a realistic notion for efficient implementation with a research scope for hardware realization for further acceleration.",0
"Filters that preserve edges are designed to blur images while maintaining the integrity of the edge information, effectively reducing the blurring effects such as halos and phantoms across the edges. These filters are nonlinear and include examples such as the bilateral filter, anisotropic diffusion filter, guided filter, and trilateral filter. The family of filters is particularly useful in reducing noise in images, making it highly sought after in computer vision and computational photography applications including denoising, video abstraction, demosaicing, optical-flow estimation, stereo matching, tone mapping, style transfer, and relighting. This paper presents an introduction to edge preserving filters, discussing their evolution from the heat diffusion equation in the past to the present, their various applications, mathematical analysis, and efficient implementation methods while focusing on preserving the boundaries, spikes, and canyons in the presence of noise. Additionally, the paper proposes research opportunities for efficient hardware realization to further accelerate the implementation process.",1
"In this paper we propose a novel approach to multi-action recognition that performs joint segmentation and classification. This approach models each action using a Gaussian mixture using robust low-dimensional action features. Segmentation is achieved by performing classification on overlapping temporal windows, which are then merged to produce the final result. This approach is considerably less complicated than previous methods which use dynamic programming or computationally expensive hidden Markov models (HMMs). Initial experiments on a stitched version of the KTH dataset show that the proposed approach achieves an accuracy of 78.3%, outperforming a recent HMM-based approach which obtained 71.2%.",0
"A new technique for recognizing multiple actions is proposed in this paper, which involves combining segmentation and classification. Robust low-dimensional action features are modeled using a Gaussian mixture for each action. Overlapping temporal windows are classified to achieve segmentation, and the results are merged to produce the final outcome. This approach is less complex than previous methods, such as dynamic programming or computationally expensive hidden Markov models (HMMs). In initial tests on a modified version of the KTH dataset, the proposed approach achieved an accuracy of 78.3%, surpassing a recent HMM-based approach that achieved 71.2%.",1
"The matching function for the problem of stereo reconstruction or optical flow has been traditionally designed as a function of the distance between the features describing matched pixels. This approach works under assumption, that the appearance of pixels in two stereo cameras or in two consecutive video frames does not change dramatically. However, this might not be the case, if we try to match pixels over a large interval of time.   In this paper we propose a method, which learns the matching function, that automatically finds the space of allowed changes in visual appearance, such as due to the motion blur, chromatic distortions, different colour calibration or seasonal changes. Furthermore, it automatically learns the importance of matching scores of contextual features at different relative locations and scales. Proposed classifier gives reliable estimations of pixel disparities already without any form of regularization.   We evaluated our method on two standard problems - stereo matching on KITTI outdoor dataset, optical flow on Sintel data set, and on newly introduced TimeLapse change detection dataset. Our algorithm obtained very promising results comparable to the state-of-the-art.",0
"Traditionally, the matching function for stereo reconstruction or optical flow has been based on the distance between features of matched pixels. This approach assumes that the appearance of pixels in two stereo cameras or consecutive video frames remains relatively stable. However, matching pixels over a longer period of time may result in significant changes in visual appearance due to factors such as motion blur, chromatic distortions, different color calibration, or seasonal changes. To address this issue, we propose a method that learns the matching function and identifies the permissible range of visual appearance changes. Additionally, our method learns the significance of matching contextual features at different scales and relative locations. Our proposed classifier can give accurate pixel disparity estimations without regularization. We evaluated our method on three standard problems, including stereo matching on KITTI outdoor dataset, optical flow on Sintel dataset, and TimeLapse change detection dataset. Our algorithm achieved promising results comparable to the state-of-the-art.",1
"In this paper we study the use of convolutional neural networks (convnets) for the task of pedestrian detection. Despite their recent diverse successes, convnets historically underperform compared to other pedestrian detectors. We deliberately omit explicitly modelling the problem into the network (e.g. parts or occlusion modelling) and show that we can reach competitive performance without bells and whistles. In a wide range of experiments we analyse small and big convnets, their architectural choices, parameters, and the influence of different training data, including pre-training on surrogate tasks.   We present the best convnet detectors on the Caltech and KITTI dataset. On Caltech our convnets reach top performance both for the Caltech1x and Caltech10x training setup. Using additional data at training time our strongest convnet model is competitive even to detectors that use additional data (optical flow) at test time.",0
"The aim of this study is to investigate the effectiveness of convolutional neural networks (convnets) in detecting pedestrians. Despite their success in various applications, convnets have historically performed poorly compared to other pedestrian detectors. Our study deliberately avoids explicitly modeling the problem in the network, such as parts or occlusion modeling, and demonstrates that we can achieve competitive performance without additional features. We conduct a range of experiments to analyze the performance of small and large convnets, their architectural choices, parameters, and the impact of different training data, including pre-training on surrogate tasks. We present the best convnet detectors on the Caltech and KITTI datasets. Our convnets outperform other detectors on both Caltech1x and Caltech10x training setups. With additional data during training, our strongest convnet model is even competitive with detectors that use additional data, such as optical flow, during testing.",1
"This paper starts from the observation that multiple top performing pedestrian detectors can be modelled by using an intermediate layer filtering low-level features in combination with a boosted decision forest. Based on this observation we propose a unifying framework and experimentally explore different filter families. We report extensive results enabling a systematic analysis.   Using filtered channel features we obtain top performance on the challenging Caltech and KITTI datasets, while using only HOG+LUV as low-level features. When adding optical flow features we further improve detection quality and report the best known results on the Caltech dataset, reaching 93% recall at 1 FPPI.",0
"The paper begins by noting that various pedestrian detectors that achieve high levels of performance can be created by incorporating a boosted decision forest with an intermediate layer that filters low-level features. Based on this observation, the paper proposes a comprehensive framework and conducts experiments with various filter families. The results are thoroughly documented, allowing for a systematic analysis. By utilizing filtered channel features with HOG+LUV as low-level features, the paper achieves remarkable results on the challenging Caltech and KITTI datasets. The addition of optical flow features further enhances detection quality, with the best-known outcomes on the Caltech dataset being a 93% recall at 1 FPPI.",1
"A 3-D spatiotemporal prediction-error filter (PEF), is used to enhance foreground/background contrast in (real and simulated) sensor image sequences. Relative velocity is utilized to extract point-targets that would otherwise be indistinguishable on spatial frequency alone. An optical-flow field is generated using local estimates of the 3-D autocorrelation function via the application of the fast Fourier transform (FFT) and inverse FFT. Velocity estimates are then used to tune in a background-whitening PEF that is matched to the motion and texture of the local background. Finite-impulse-response (FIR) filters are designed and implemented in the frequency domain. An analytical expression for the frequency response of velocity-tuned FIR filters, of odd or even dimension, with an arbitrary delay in each dimension, is derived.",0
"To enhance the contrast between foreground and background in both real and simulated sensor image sequences, a 3-D spatiotemporal prediction-error filter (PEF) is employed. By using relative velocity, point-targets that would be difficult to differentiate based solely on spatial frequency can be extracted. To generate an optical-flow field, the fast Fourier transform (FFT) and inverse FFT are applied to local estimates of the 3-D autocorrelation function. Velocity estimates are then used to fine-tune a background-whitening PEF that corresponds to the motion and texture of the local background. Finite-impulse-response (FIR) filters are designed and implemented in the frequency domain. An analytical expression for the frequency response of velocity-tuned FIR filters, of odd or even dimension, with an arbitrary delay in each dimension, is derived.",1
"We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework.   Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multi-task learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both.   Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.",0
"Our research focuses on the structures of deep Convolutional Networks (ConvNets) specifically designed for action recognition in video, through discriminative training. The challenge lies in capturing complementary information on appearance from still frames and motion between frames. Our goal is to generalize the most effective hand-crafted features within a data-driven learning framework. Our contribution involves three key aspects. Firstly, we propose a two-stream ConvNet architecture that comprises spatial and temporal networks. Secondly, we demonstrate that a ConvNet trained on multi-frame dense optical flow can achieve excellent performance, despite limited training data. Finally, we prove that multi-task learning can be used to increase the amount of training data and improve the performance on two different action classification datasets. Our architecture is tested and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it competes with the state-of-the-art approaches. Additionally, it surpasses previous attempts to use deep nets for video classification by a significant margin.",1
"In this paper, we describe a simple strategy for mitigating variability in temporal data series by shifting focus onto long-term, frequency domain features that are less susceptible to variability. We apply this method to the human action recognition task and demonstrate how working in the frequency domain can yield good recognition features for commonly used optical flow and articulated pose features, which are highly sensitive to small differences in motion, viewpoint, dynamic backgrounds, occlusion and other sources of variability. We show how these frequency-based features can be used in combination with a simple forest classifier to achieve good and robust results on the popular KTH Actions dataset.",0
"The paper outlines a straightforward approach to reducing fluctuations in time-based data series by emphasizing long-term, frequency-based characteristics that are more resistant to variability. The method is tested on recognizing human actions and proves effective in producing reliable recognition features for optical flow and articulated pose features that are typically vulnerable to slight variations in movement, perspective, background dynamics, and occlusion. By combining these frequency-based features with a basic forest classifier, we demonstrate impressive and resilient outcomes on the widely-used KTH Actions dataset.",1
"In this paper we present a number of methods (manual, semi-automatic and automatic) for tracking individual targets in high density crowd scenes where thousand of people are gathered. The necessary data about the motion of individuals and a lot of other physical information can be extracted from consecutive image sequences in different ways, including optical flow and block motion estimation. One of the famous methods for tracking moving objects is the block matching method. This way to estimate subject motion requires the specification of a comparison window which determines the scale of the estimate. In this work we present a real-time method for pedestrian recognition and tracking in sequences of high resolution images obtained by a stationary (high definition) camera located in different places on the Haram mosque in Mecca. The objective is to estimate pedestrian velocities as a function of the local density.The resulting data of tracking moving pedestrians based on video sequences are presented in the following section. Through the evaluated system the spatio-temporal coordinates of each pedestrian during the Tawaf ritual are established. The pilgrim velocities as function of the local densities in the Mataf area (Haram Mosque Mecca) are illustrated and very precisely documented.",0
"This paper introduces various techniques, including manual, semi-automatic, and automatic methods, for tracking individual targets in high density crowd scenes with large numbers of people. Physical information and data on the motion of individuals can be obtained from consecutive image sequences, whereby optical flow and block motion estimation are two common ways to extract this data. The block matching method is a well-known approach for tracking moving objects, which involves specifying a comparison window to determine the estimate's scale. In this study, we present a real-time method for recognizing and tracking pedestrians in high-resolution image sequences captured by a stationary camera located in different areas of the Haram mosque in Mecca. The goal is to estimate pedestrian velocities as a function of local density. The outcomes of tracking moving pedestrians based on video sequences are presented in the subsequent section. The spatio-temporal coordinates of each pedestrian during the Tawaf ritual are established using the evaluated system. The pilgrim velocities in the Mataf area (Haram Mosque Mecca) as a function of local densities are accurately documented and illustrated.",1
"Handling all together large displacements, motion details and occlusions remains an open issue for reliable computation of optical flow in a video sequence. We propose a two-step aggregation paradigm to address this problem. The idea is to supply local motion candidates at every pixel in a first step, and then to combine them to determine the global optical flow field in a second step. We exploit local parametric estimations combined with patch correspondences and we experimentally demonstrate that they are sufficient to produce highly accurate motion candidates. The aggregation step is designed as the discrete optimization of a global regularized energy. The occlusion map is estimated jointly with the flow field throughout the two steps. We propose a generic exemplar-based approach for occlusion filling with motion vectors. We achieve state-of-the-art results in computer vision benchmarks, with particularly significant improvements in the case of large displacements and occlusions.",0
"The computation of optical flow in a video sequence is still a challenge when dealing with large displacements, motion details, and occlusions. To tackle this problem, we suggest a two-step aggregation methodology. In the first step, we generate local motion candidates for every pixel and in the second step, we combine them to determine the global optical flow field. Our approach employs local parametric estimations combined with patch correspondences, which we prove to be adequate for producing highly precise motion candidates. The aggregation step is formulated as a discrete optimization problem of a global regularized energy. Moreover, we estimate the occlusion map jointly with the flow field in both steps, and we present a generic exemplar-based method for filling in occlusions with motion vectors. Our technique surpasses the state-of-the-art in computer vision benchmarks, particularly in handling large displacements and occlusions.",1
The purpose of this paper is to describe one-shot-learning gesture recognition systems developed on the \textit{ChaLearn Gesture Dataset}. We use RGB and depth images and combine appearance (Histograms of Oriented Gradients) and motion descriptors (Histogram of Optical Flow) for parallel temporal segmentation and recognition. The Quadratic-Chi distance family is used to measure differences between histograms to capture cross-bin relationships. We also propose a new algorithm for trimming videos --- to remove all the unimportant frames from videos. We present two methods that use combination of HOG-HOF descriptors together with variants of Dynamic Time Warping technique. Both methods outperform other published methods and help narrow down the gap between human performance and algorithms on this task. The code has been made publicly available in the MLOSS repository.,0
"The aim of this paper is to elucidate the one-shot-learning gesture recognition systems that have been developed on the \textit{ChaLearn Gesture Dataset}. In order to achieve this, we utilize RGB and depth images and amalgamate appearance (Histograms of Oriented Gradients) and motion descriptors (Histogram of Optical Flow) for simultaneous temporal segmentation and recognition. To capture cross-bin relationships, the Quadratic-Chi distance family is used to gauge differences between histograms. Furthermore, we introduce a novel algorithm for eliminating extraneous frames from videos. We present two methods that employ a combination of HOG-HOF descriptors along with variations of Dynamic Time Warping technique. Both techniques surpass other previously published methods and help to narrow the gap between human performance and algorithms applied to this task. The code has been made publicly accessible via the MLOSS repository.",1
"This paper proposes combining spatio-temporal appearance (STA) descriptors with optical flow for human action recognition. The STA descriptors are local histogram-based descriptors of space-time, suitable for building a partial representation of arbitrary spatio-temporal phenomena. Because of the possibility of iterative refinement, they are interesting in the context of online human action recognition. We investigate the use of dense optical flow as the image function of the STA descriptor for human action recognition, using two different algorithms for computing the flow: the Farneb\""ack algorithm and the TVL1 algorithm. We provide a detailed analysis of the influencing optical flow algorithm parameters on the produced optical flow fields. An extensive experimental validation of optical flow-based STA descriptors in human action recognition is performed on the KTH human action dataset. The encouraging experimental results suggest the potential of our approach in online human action recognition.",0
"The objective of this paper is to propose a novel approach to human action recognition by combining spatio-temporal appearance (STA) descriptors with optical flow. The STA descriptors, which are based on local histograms of space-time, can be used to develop a partial representation of any spatio-temporal phenomenon. Given their iterative refinement capability, they are particularly useful for online human action recognition. In this study, we explore the use of dense optical flow as the image function of the STA descriptor and evaluate two different algorithms for computing the flow: the Farneb\""ack algorithm and the TVL1 algorithm. We also examine the influence of various optical flow algorithm parameters on the resulting optical flow fields. Our approach is validated using the KTH human action dataset, and we present comprehensive experimental results that demonstrate the effectiveness of optical flow-based STA descriptors in human action recognition. The promising outcomes suggest the potential of our technique in online human action recognition.",1
This paper describes a technique of real time head gesture recognition system. The method includes Gaussian mixture model (GMM) accompanied by optical flow algorithm which provided us the required information regarding head movement. The proposed model can be implemented in various control system. We are also presenting the result and implementation of both mentioned method.,0
"In this paper, a method for recognizing head gestures in real-time is presented. The approach utilizes a Gaussian mixture model (GMM) and an optical flow algorithm to obtain the necessary data on head movements. The suggested model has the potential to be used in multiple control systems. Furthermore, the outcomes and application of both techniques are showcased.",1
"Crowd monitoring and analysis in mass events are highly important technologies to support the security of attending persons. Proposed methods based on terrestrial or airborne image/video data often fail in achieving sufficiently accurate results to guarantee a robust service. We present a novel framework for estimating human count, density and motion from video data based on custom tailored object detection techniques, a regression based density estimate and a total variation based optical flow extraction. From the gathered features we present a detailed accuracy analysis versus ground truth measurements. In addition, all information is projected into world coordinates to enable a direct integration with existing geo-information systems. The resulting human counts demonstrate a mean error of 4% to 9% and thus represent a most efficient measure that can be robustly applied in security critical services.",0
"Monitoring and analyzing crowds at mass events is crucial for ensuring attendee safety. Traditional methods using terrestrial or airborne image/video data often fall short in providing accurate results. To address this issue, we introduce a new framework that utilizes custom object detection techniques, regression-based density estimation, and total variation-based optical flow extraction to estimate human count, density, and motion from video data. Our approach achieves high accuracy, with a mean error of only 4% to 9%, as demonstrated through a detailed accuracy analysis versus ground truth measurements. Additionally, all information is transformed into world coordinates for easy integration with existing geo-information systems. Our framework offers a highly effective and reliable solution for security-critical services.",1
"A robust and efficient anomaly detection technique is proposed, capable of dealing with crowded scenes where traditional tracking based approaches tend to fail. Initial foreground segmentation of the input frames confines the analysis to foreground objects and effectively ignores irrelevant background dynamics. Input frames are split into non-overlapping cells, followed by extracting features based on motion, size and texture from each cell. Each feature type is independently analysed for the presence of an anomaly. Unlike most methods, a refined estimate of object motion is achieved by computing the optical flow of only the foreground pixels. The motion and size features are modelled by an approximated version of kernel density estimation, which is computationally efficient even for large training datasets. Texture features are modelled by an adaptively grown codebook, with the number of entries in the codebook selected in an online fashion. Experiments on the recently published UCSD Anomaly Detection dataset show that the proposed method obtains considerably better results than three recent approaches: MPPCA, social force, and mixture of dynamic textures (MDT). The proposed method is also several orders of magnitude faster than MDT, the next best performing method.",0
"An effective technique for detecting anomalies in crowded scenes is introduced, which overcomes the limitations of traditional tracking-based methods. The technique involves segmenting the foreground objects in the input frames to eliminate irrelevant background dynamics and extracting motion, size, and texture features from non-overlapping cells. Each feature type is analyzed independently to detect anomalies. The method uses optical flow to refine object motion estimation and employs an approximated version of kernel density estimation to model motion and size features. Texture features are modeled by an adaptively grown codebook, with the number of entries determined online. Experiments on the UCSD Anomaly Detection dataset demonstrate that the proposed method outperforms three recent approaches, namely MPPCA, social force, and MDT. Furthermore, the proposed method is significantly faster than MDT, which is the second-best performing method.",1
"This paper describes and provides an initial solution to a novel video editing task, i.e., video de-fencing. It targets automatic restoration of the video clips that are corrupted by fence-like occlusions during capture. Our key observation lies in the visual parallax between fences and background scenes, which is caused by the fact that the former are typically closer to the camera. Unlike in traditional image inpainting, fence-occluded pixels in the videos tend to appear later in the temporal dimension and are therefore recoverable via optimized pixel selection from relevant frames. To eventually produce fence-free videos, major challenges include cross-frame sub-pixel image alignment under diverse scene depth, and ""correct"" pixel selection that is robust to dominating fence pixels. Several novel tools are developed in this paper, including soft fence detection, weighted truncated optical flow method and robust temporal median filter. The proposed algorithm is validated on several real-world video clips with fences.",0
"The aim of this paper is to introduce a new form of video editing called video de-fencing, which involves restoring video clips that have been corrupted by fence-like obstructions during filming. The paper identifies a significant visual difference between fences and background scenes due to their relative proximity to the camera, which is the key to restoring the damaged footage. Unlike traditional image inpainting, the occluded pixels in video footage appear later in the temporal dimension, which means they can be recovered through optimized pixel selection from relevant frames. However, the process of producing fence-free videos is complicated by challenges such as aligning images with different depths and selecting the correct pixels, which must be robust against dominant fence pixels. The paper presents several new tools, including soft fence detection, a weighted truncated optical flow method, and a robust temporal median filter, which are used to validate the proposed algorithm on real-world video clips with fences.",1
"Recognizing group activities is challenging due to the difficulties in isolating individual entities, finding the respective roles played by the individuals and representing the complex interactions among the participants. Individual actions and group activities in videos can be represented in a common framework as they share the following common feature: both are composed of a set of low-level features describing motions, e.g., optical flow for each pixel or a trajectory for each feature point, according to a set of composition constraints in both temporal and spatial dimensions. In this paper, we present a unified model to assess the similarity between two given individual or group activities. Our approach avoids explicit extraction of individual actors, identifying and representing the inter-person interactions. With the proposed approach, retrieval from a video database can be performed through Query-by-Example; and activities can be recognized by querying videos containing known activities. The suggested video matching process can be performed in an unsupervised manner. We demonstrate the performance of our approach by recognizing a set of human actions and football plays.",0
"It is difficult to identify group activities because it is hard to separate individuals, determine their specific roles, and capture the complex interactions between participants. However, both individual actions and group activities can be described using low-level motion features, such as optical flow or feature points, and composition constraints that account for spatial and temporal dimensions. Our paper presents a unified model that can assess the similarity between two individual or group activities without explicitly extracting individual actors or representing inter-person interactions. Our approach allows for Query-by-Example retrieval from a video database and unsupervised video matching. We tested our method on recognizing human actions and football plays and achieved successful results.",1
"In this paper we address the problem of tracking non-rigid objects whose local appearance and motion changes as a function of time. This class of objects includes dynamic textures such as steam, fire, smoke, water, etc., as well as articulated objects such as humans performing various actions. We model the temporal evolution of the object's appearance/motion using a Linear Dynamical System (LDS). We learn such models from sample videos and use them as dynamic templates for tracking objects in novel videos. We pose the problem of tracking a dynamic non-rigid object in the current frame as a maximum a-posteriori estimate of the location of the object and the latent state of the dynamical system, given the current image features and the best estimate of the state in the previous frame. The advantage of our approach is that we can specify a-priori the type of texture to be tracked in the scene by using previously trained models for the dynamics of these textures. Our framework naturally generalizes common tracking methods such as SSD and kernel-based tracking from static templates to dynamic templates. We test our algorithm on synthetic as well as real examples of dynamic textures and show that our simple dynamics-based trackers perform at par if not better than the state-of-the-art. Since our approach is general and applicable to any image feature, we also apply it to the problem of human action tracking and build action-specific optical flow trackers that perform better than the state-of-the-art when tracking a human performing a particular action. Finally, since our approach is generative, we can use a-priori trained trackers for different texture or action classes to simultaneously track and recognize the texture or action in the video.",0
"The paper focuses on the tracking of non-rigid objects that undergo changes in appearance and motion over time, such as dynamic textures like steam, smoke, fire, water, and articulated objects like humans in motion. To address this problem, the authors propose using a Linear Dynamical System (LDS) to model the temporal evolution of the object's appearance and motion, which is learned from sample videos and used as a dynamic template for tracking objects in new videos. The authors pose the problem of tracking a dynamic non-rigid object as a maximum a-posteriori estimate of the object's location and latent state, given the current image features and the best estimate of the state in the previous frame. This approach allows for a-priori specification of the texture to be tracked using previously trained models for the dynamics of those textures. The authors demonstrate the effectiveness of their approach on synthetic and real examples of dynamic textures, as well as human action tracking, showing that their dynamics-based trackers perform better than state-of-the-art methods. Furthermore, since their approach is generative, it can be used to simultaneously track and recognize different texture or action classes in a video.",1
"An image articulation manifold (IAM) is the collection of images formed when an object is articulated in front of a camera. IAMs arise in a variety of image processing and computer vision applications, where they provide a natural low-dimensional embedding of the collection of high-dimensional images. To date IAMs have been studied as embedded submanifolds of Euclidean spaces. Unfortunately, their promise has not been realized in practice, because real world imagery typically contains sharp edges that render an IAM non-differentiable and hence non-isometric to the low-dimensional parameter space under the Euclidean metric. As a result, the standard tools from differential geometry, in particular using linear tangent spaces to transport along the IAM, have limited utility. In this paper, we explore a nonlinear transport operator for IAMs based on the optical flow between images and develop new analytical tools reminiscent of those from differential geometry using the idea of optical flow manifolds (OFMs). We define a new metric for IAMs that satisfies certain local isometry conditions, and we show how to use this metric to develop a new tools such as flow fields on IAMs, parallel flow fields, parallel transport, as well as a intuitive notion of curvature. The space of optical flow fields along a path of constant curvature has a natural multi-scale structure via a monoid structure on the space of all flow fields along a path. We also develop lower bounds on approximation errors while approximating non-parallel flow fields by parallel flow fields.",0
"The IAM is a set of images resulting from the movement of an object in front of a camera. It is commonly used in computer vision and image processing applications as it provides a low-dimensional representation of high-dimensional images. However, IAMs have proven challenging to work with due to the non-differentiability of the sharp edges that often appear in real-world imagery. Current methods for studying IAMs rely on linear tangent spaces, which have limited functionality. This paper introduces a nonlinear transport operator based on optical flow, which allows for the development of new analytical tools like optical flow manifolds (OFMs). The OFM enables the definition of a new metric for IAMs that meets local isometry conditions and provides insights into the curvature of the IAM. We also explore the multi-scale structure of the space of optical flow fields along a path of constant curvature and develop lower bounds on approximation errors when approximating non-parallel flow fields using parallel flow fields.",1
"An algorithm for pose and motion estimation using corresponding features in omnidirectional images and a digital terrain map is proposed. In previous paper, such algorithm for regular camera was considered. Using a Digital Terrain (or Digital Elevation) Map (DTM/DEM) as a global reference enables recovering the absolute position and orientation of the camera. In order to do this, the DTM is used to formulate a constraint between corresponding features in two consecutive frames. In this paper, these constraints are extended to handle non-central projection, as is the case with many omnidirectional systems. The utilization of omnidirectional data is shown to improve the robustness and accuracy of the navigation algorithm. The feasibility of this algorithm is established through lab experimentation with two kinds of omnidirectional acquisition systems. The first one is polydioptric cameras while the second is catadioptric camera.",0
"The paper proposes an algorithm that estimates pose and motion using corresponding features in omnidirectional images and a digital terrain map. The algorithm was previously considered for regular cameras, but the use of a Digital Terrain Map (DTM) allows for the recovery of the camera's absolute position and orientation. The DTM is used to create a constraint between corresponding features in consecutive frames, which is extended to handle non-central projection in omnidirectional systems. The algorithm's accuracy and robustness are improved by utilizing omnidirectional data. The feasibility of the algorithm is demonstrated through lab experimentation with two types of omnidirectional acquisition systems: polydioptric and catadioptric cameras.",1
"The paper deals with the error analysis of a navigation algorithm that uses as input a sequence of images acquired by a moving camera and a Digital Terrain Map (DTM) of the region been imaged by the camera during the motion. The main sources of error are more or less straightforward to identify: camera resolution, structure of the observed terrain and DTM accuracy, field of view and camera trajectory. After characterizing and modeling these error sources in the framework of the CDTM algorithm, a closed form expression for their effect on the pose and motion errors of the camera can be found. The analytic expression provides a priori measurements for the accuracy in terms of the parameters mentioned above.",0
"The article concerns the analysis of errors in a navigation algorithm that relies on a sequence of images captured by a moving camera and a Digital Terrain Map (DTM) of the area photographed by the camera during its movement. The primary sources of error include camera resolution, terrain structure, DTM accuracy, field of view, and camera path. Once these error sources are characterized and modeled within the CDTM algorithm, an analytical expression that accounts for their impact on camera pose and motion errors can be derived. This formula yields a priori measurements of accuracy based on the aforementioned parameters.",1
"This work presents a framework for tracking head movements and capturing the movements of the mouth and both the eyebrows in real-time. We present a head tracker which is a combination of a optical flow and a template based tracker. The estimation of the optical flow head tracker is used as starting point for the template tracker which fine-tunes the head estimation. This approach together with re-updating the optical flow points prevents the head tracker from drifting. This combination together with our switching scheme, makes our tracker very robust against fast movement and motion-blur. We also propose a way to reduce the influence of partial occlusion of the head. In both the optical flow and the template based tracker we identify and exclude occluded points.",0
"The article introduces a system that monitors head movements in real-time and records the movements of the mouth and eyebrows. The system includes a head tracker that combines an optical flow and template-based tracker. The optical flow head tracker is used as a starting point for the template tracker, which fine-tunes the head estimation. This approach prevents the head tracker from drifting, and together with the re-updating of optical flow points, makes the tracker resistant to fast movement and motion-blur. The article also proposes a method to reduce the impact of partial occlusion of the head by identifying and excluding occluded points in both the optical flow and template-based tracker.",1
"The problem of the generation of an intermediate image between two given images in an image sequence is considered. The problem is formulated as an optimal control problem governed by a transport equation. This approach bears similarities with the Horn \& Schunck method for optical flow calculation but in fact the model is quite different. The images are modelled in $BV$ and an analysis of solutions of transport equations with values in $BV$ is included. Moreover, the existence of optimal controls is proven and necessary conditions are derived. Finally, two algorithms are given and numerical results are compared with existing methods. The new method is competitive with state-of-the-art methods and even outperforms several existing methods.",0
"This article addresses the challenge of generating an intermediate image between two images in a sequence. The approach involves an optimal control problem governed by a transport equation, which differs from the Horn & Schunck method for optical flow calculation. The images are represented in $BV$, and the solutions of the transport equations with values in $BV$ are analyzed. The study proves the existence of optimal controls and derives necessary conditions. Additionally, two algorithms are presented, and numerical results are compared with other methods. The new approach proves to be competitive, even outperforming some state-of-the-art methods.",1
"This paper proposes a method of gesture recognition with a focus on important actions for distinguishing similar gestures. The method generates a partial action sequence by using optical flow images, expresses the sequence in the eigenspace, and checks the feature vector sequence by applying an optimum path-searching method of weighted graph to focus the important actions. Also presented are the results of an experiment on the recognition of similar sign language words.",0
"The proposed approach in this article deals with recognizing gestures by emphasizing significant actions to differentiate between comparable gestures. The method employs optical flow images to create a partial action sequence, transforms it into the eigenspace, and analyzes the feature vector sequence utilizing a weighted graph's optimal path-searching technique to highlight the important actions. Additionally, the article reports the outcomes of a study conducted on recognizing similar sign language words.",1
"In the field of computer vision, a crucial task is the detection of motion (also called optical flow extraction). This operation allows analysis such as 3D reconstruction, feature tracking, time-to-collision and novelty detection among others. Most of the optical flow extraction techniques work within a finite range of speeds. Usually, the range of detection is extended towards higher speeds by combining some multiscale information in a serial architecture. This serial multi-scale approach suffers from the problem of error propagation related to the number of scales used in the algorithm. On the other hand, biological experiments show that human motion perception seems to follow a parallel multiscale scheme. In this work we present a bio-inspired parallel architecture to perform detection of motion, providing a wide range of operation and avoiding error propagation associated with the serial architecture. To test our algorithm, we perform relative error comparisons between both classical and proposed techniques, showing that the parallel architecture is able to achieve motion detection with results similar to the serial approach.",0
"Detecting motion, also known as optical flow extraction, is a critical task in computer vision that enables various analyses such as feature tracking, 3D reconstruction, time-to-collision, and novelty detection. Optical flow extraction techniques typically have a limited range of speed detection, which can be expanded by combining multiscale information in a serial architecture. However, this approach suffers from error propagation issues. In contrast, human motion perception follows a parallel multiscale scheme, which inspired the development of a bio-inspired parallel architecture for motion detection. This architecture provides a broader range of operation and avoids error propagation associated with the serial approach. To evaluate the algorithm, relative error comparisons were conducted between classical and proposed techniques, demonstrating that the parallel architecture achieves motion detection results similar to the serial approach.",1
"In this paper, we propose a global method for estimating the motion of a camera which films a static scene. Our approach is direct, fast and robust, and deals with adjacent frames of a sequence. It is based on a quadratic approximation of the deformation between two images, in the case of a scene with constant depth in the camera coordinate system. This condition is very restrictive but we show that provided translation and depth inverse variations are small enough, the error on optical flow involved by the approximation of depths by a constant is small. In this context, we propose a new model of camera motion, that allows to separate the image deformation in a similarity and a ``purely'' projective application, due to change of optical axis direction. This model leads to a quadratic approximation of image deformation that we estimate with an M-estimator; we can immediatly deduce camera motion parameters.",0
"The aim of this paper is to introduce an efficient and reliable method for estimating camera motion when filming a stationary scene. Our direct approach involves analyzing adjacent frames in a sequence and is based on a quadratic approximation of the deformation between the images. However, this technique is only suitable for scenes with constant depth in the camera coordinate system. Despite this restriction, we demonstrate that small variations in translation and depth inverse can result in minimal errors on the optical flow. We also present a new camera motion model that allows us to separate the image deformation into a similarity and a purely projective application, caused by changes in the optical axis direction. By using an M-estimator to estimate the quadratic approximation of the image deformation, we are able to obtain the camera motion parameters immediately.",1
"This paper proposes a fusion method of modalities extracted from video through a three-stream network with spatio-temporal and temporal convolutions for fine-grained action classification in sport. It is applied to TTStroke-21 dataset which consists of untrimmed videos of table tennis games. The goal is to detect and classify table tennis strokes in the videos, the first step of a bigger scheme aiming at giving feedback to the players for improving their performance. The three modalities are raw RGB data, the computed optical flow and the estimated pose of the player. The network consists of three branches with attention blocks. Features are fused at the latest stage of the network using bilinear layers. Compared to previous approaches, the use of three modalities allows faster convergence and better performances on both tasks: classification of strokes with known temporal boundaries and joint segmentation and classification. The pose is also further investigated in order to offer richer feedback to the athletes.",0
"In this paper, a fusion technique is proposed for modalities extracted from video using a three-stream network with spatio-temporal and temporal convolutions. The main objective is fine-grained action classification in sports, specifically in detecting and classifying table tennis strokes in untrimmed videos using the TTStroke-21 dataset. This is the first step in a larger scheme to provide feedback to players for improving their performance. The three modalities used in the network are raw RGB data, computed optical flow, and estimated player pose. The network comprises of three branches with attention blocks and features are fused through bilinear layers. Compared to previous approaches, the use of three modalities enables faster convergence and better performances in both stroke classification with known temporal boundaries and joint segmentation and classification. Additionally, the estimated player pose is further investigated to provide more detailed feedback to athletes.",1
"Establishing robust and accurate correspondences between a pair of images is a long-standing computer vision problem with numerous applications. While classically dominated by sparse methods, emerging dense approaches offer a compelling alternative paradigm that avoids the keypoint detection step. However, dense flow estimation is often inaccurate in the case of large displacements, occlusions, or homogeneous regions. In order to apply dense methods to real-world applications, such as pose estimation, image manipulation, or 3D reconstruction, it is therefore crucial to estimate the confidence of the predicted matches.   We propose the Enhanced Probabilistic Dense Correspondence Network, PDC-Net+, capable of estimating accurate dense correspondences along with a reliable confidence map. We develop a flexible probabilistic approach that jointly learns the flow prediction and its uncertainty. In particular, we parametrize the predictive distribution as a constrained mixture model, ensuring better modelling of both accurate flow predictions and outliers. Moreover, we develop an architecture and an enhanced training strategy tailored for robust and generalizable uncertainty prediction in the context of self-supervised training. Our approach obtains state-of-the-art results on multiple challenging geometric matching and optical flow datasets. We further validate the usefulness of our probabilistic confidence estimation for the tasks of pose estimation, 3D reconstruction, image-based localization, and image retrieval. Code and models are available at https://github.com/PruneTruong/DenseMatching.",0
"The problem of establishing accurate correspondences between two images has been a challenge in computer vision for a long time, and has numerous practical applications. Although sparse methods have traditionally dominated this field, dense approaches are becoming more popular as they avoid the need for keypoint detection. However, dense flow estimation is often inaccurate in cases involving large displacements, occlusions, or homogeneous regions. To make dense methods applicable to real-world tasks such as pose estimation, image manipulation, or 3D reconstruction, it is essential to estimate the confidence of predicted matches. We introduce the Enhanced Probabilistic Dense Correspondence Network, or PDC-Net+, which can accurately estimate dense correspondences as well as a reliable confidence map. Our probabilistic approach jointly learns the flow prediction and its uncertainty, and we parametrize the predictive distribution as a constrained mixture model to improve modeling of both accurate flow predictions and outliers. We also develop an architecture and training strategy that are tailored for robust and generalizable uncertainty prediction in self-supervised training. Our approach achieves state-of-the-art performance on multiple challenging geometric matching and optical flow datasets, and we demonstrate the usefulness of our probabilistic confidence estimation for pose estimation, 3D reconstruction, image-based localization, and image retrieval. Our code and models are available at https://github.com/PruneTruong/DenseMatching.",1
"In this paper, we propose an end-to-end learning framework for event-based motion deblurring in a self-supervised manner, where real-world events are exploited to alleviate the performance degradation caused by data inconsistency. To achieve this end, optical flows are predicted from events, with which the blurry consistency and photometric consistency are exploited to enable self-supervision on the deblurring network with real-world data. Furthermore, a piece-wise linear motion model is proposed to take into account motion non-linearities and thus leads to an accurate model for the physical formation of motion blurs in the real-world scenario. Extensive evaluation on both synthetic and real motion blur datasets demonstrates that the proposed algorithm bridges the gap between simulated and real-world motion blurs and shows remarkable performance for event-based motion deblurring in real-world scenarios.",0
"Our paper introduces a self-supervised end-to-end learning framework for event-based motion deblurring. By leveraging real-world events, we address the issue of data inconsistency that can negatively impact performance. We predict optical flows from events, which allow us to exploit blurry and photometric consistency and enable self-supervision on the deblurring network using real-world data. Additionally, we propose a piece-wise linear motion model that accounts for motion non-linearities, resulting in an accurate model for motion blurs in real-world scenarios. Our algorithm is evaluated extensively on synthetic and real motion blur datasets and demonstrates remarkable performance for event-based motion deblurring in real-world scenarios, bridging the gap between simulated and real-world motion blurs.",1
"Performing a real-time and accurate instrument segmentation from videos is of great significance for improving the performance of robotic-assisted surgery. We identify two important clues for surgical instrument perception, including local temporal dependency from adjacent frames and global semantic correlation in long-range duration. However, most existing works perform segmentation purely using visual cues in a single frame. Optical flow is just used to model the motion between only two frames and brings heavy computational cost. We propose a novel dual-memory network (DMNet) to wisely relate both global and local spatio-temporal knowledge to augment the current features, boosting the segmentation performance and retaining the real-time prediction capability. We propose, on the one hand, an efficient local memory by taking the complementary advantages of convolutional LSTM and non-local mechanisms towards the relating reception field. On the other hand, we develop an active global memory to gather the global semantic correlation in long temporal range to current one, in which we gather the most informative frames derived from model uncertainty and frame similarity. We have extensively validated our method on two public benchmark surgical video datasets. Experimental results demonstrate that our method largely outperforms the state-of-the-art works on segmentation accuracy while maintaining a real-time speed.",0
"To enhance the effectiveness of robotic-assisted surgery, it is crucial to accurately segment surgical instruments in real-time from videos. Our research has identified two critical factors that aid in instrument perception: local temporal dependency and global semantic correlation. However, the majority of current segmentation methods rely solely on visual cues from a single frame, while optical flow incurs high computational costs. We propose a new approach called the Dual-Memory Network (DMNet), which combines both global and local spatio-temporal knowledge to improve segmentation accuracy and maintain real-time efficiency. Our solution involves an efficient local memory that utilizes convolutional LSTM and non-local mechanisms to enhance the reception field, and an active global memory that collects informative frames from long temporal ranges. We have tested our method on two publicly available surgical video datasets and demonstrated that our approach outperforms existing methods in terms of segmentation accuracy, while maintaining real-time processing speeds.",1
"Online action detection (OAD) is a task that receives video segments within a streaming video as inputs and identifies ongoing actions within them. It is important to retain past information associated with a current action. However, long short-term memory (LSTM), a popular recurrent unit for modeling temporal information from videos, accumulates past information from the previous hidden and cell states and the extracted visual features at each timestep without considering the relationships between the past and current information. Consequently, the forget gate of the original LSTM can lose the accumulated information relevant to the current action because it determines which information to forget without considering the current action. We introduce a novel information elevation unit (IEU) that lifts up and accumulate the past information relevant to the current action in order to model the past information that is especially relevant to the current action. To the best of our knowledge, our IEN is the first attempt that considers the computational overhead for the practical use of OAD. Through ablation studies, we design an efficient and effective OAD network using IEUs, called an information elevation network (IEN). Our IEN uses visual features extracted by a fast action recognition network taking only RGB frames because extracting optical flows requires heavy computation overhead. On two OAD benchmark datasets, THUMOS-14 and TVSeries, our IEN outperforms state-of-the-art OAD methods using only RGB frames. Furthermore, on the THUMOS-14 dataset, our IEN outperforms the state-of-the-art OAD methods using two-stream features based on RGB frames and optical flows.",0
"The task of online action detection (OAD) involves identifying ongoing actions in video segments within a streaming video. To accomplish this task, it is necessary to consider past information associated with the current action. However, the commonly used long short-term memory (LSTM) recurrent unit for modeling temporal information from videos accumulates past information from previous hidden and cell states and extracted visual features at each timestep without considering the relationship between past and current information. This can result in the forget gate of the original LSTM losing relevant accumulated information for the current action because it determines which information to forget without considering the current action. Our proposed information elevation unit (IEU) addresses this issue by lifting up and accumulating past information relevant to the current action, resulting in a more effective model. Our IEU is the first attempt to consider the computational overhead for the practical use of OAD. We present an efficient and effective OAD network called the information elevation network (IEN) that uses visual features extracted by a fast action recognition network from only RGB frames. Our IEN outperforms state-of-the-art OAD methods using only RGB frames on two OAD benchmark datasets, THUMOS-14 and TVSeries. It also outperforms state-of-the-art OAD methods using two-stream features based on RGB frames and optical flows on the THUMOS-14 dataset.",1
"Video anomaly detection has gained significant attention due to the increasing requirements of automatic monitoring for surveillance videos. Especially, the prediction based approach is one of the most studied methods to detect anomalies by predicting frames that include abnormal events in the test set after learning with the normal frames of the training set. However, a lot of prediction networks are computationally expensive owing to the use of pre-trained optical flow networks, or fail to detect abnormal situations because of their strong generative ability to predict even the anomalies. To address these shortcomings, we propose spatial rotation transformation (SRT) and temporal mixing transformation (TMT) to generate irregular patch cuboids within normal frame cuboids in order to enhance the learning of normal features. Additionally, the proposed patch transformation is used only during the training phase, allowing our model to detect abnormal frames at fast speed during inference. Our model is evaluated on three anomaly detection benchmarks, achieving competitive accuracy and surpassing all the previous works in terms of speed.",0
"Due to the increasing need for automatic monitoring in surveillance videos, video anomaly detection has become a popular topic. One of the most widely researched methods is the prediction-based approach, which involves predicting frames with abnormal events in the test set after learning from normal frames in the training set. However, many prediction networks are computationally expensive due to pre-trained optical flow networks or have limited ability to detect anomalies. To address these issues, we propose using spatial rotation transformation (SRT) and temporal mixing transformation (TMT) to generate irregular patch cuboids within normal frame cuboids, which enhances the learning of normal features. This transformation is only used during training, allowing our model to detect abnormal frames at a faster speed during inference. Our model is evaluated on three anomaly detection benchmarks and achieves competitive accuracy while surpassing previous works in terms of speed.",1
"Dense disparities among multiple views is essential for estimating the 3D architecture of a scene based on the geometrical relationship among the scene and the views or cameras. Scenes with larger extents of heterogeneous textures, differing scene illumination among the multiple views and with occluding objects affect the accuracy of the estimated disparities. Markov random fields (MRF) based methods for disparity estimation address these limitations using spatial dependencies among the observations and among the disparity estimates. These methods, however, are limited by spatially fixed and smaller neighborhood systems or cliques. In this work, we present a new factor graph-based probabilistic graphical model for disparity estimation that allows a larger and a spatially variable neighborhood structure determined based on the local scene characteristics. We evaluated our method using the Middlebury benchmark stereo datasets and the Middlebury evaluation dataset version 3.0 and compared its performance with recent state-of-the-art disparity estimation algorithms. The new factor graph-based method provided disparity estimates with higher accuracy when compared to the recent non-learning- and learning-based disparity estimation algorithms. In addition to disparity estimation, our factor graph formulation can be useful for obtaining maximum a posteriori solution to optimization problems with complex and variable dependency structures as well as for other dense estimation problems such as optical flow estimation.",0
"For accurately estimating the 3D architecture of a scene, it is crucial to have dense disparities among multiple views based on the geometrical relationship between the scene and cameras. Scenes with diverse textures, varying illumination, and occluding objects can affect the accuracy of estimated disparities. Markov random fields (MRF) can address these limitations using spatial dependencies, but they are restricted by fixed and smaller neighborhood systems. In this study, we propose a new probabilistic graphical model for disparity estimation using factor graphs. Our method allows a larger and spatially variable neighborhood structure based on local scene characteristics. We tested our approach using Middlebury benchmark stereo datasets and compared it with recent state-of-the-art algorithms. Our factor graph-based method provided more accurate disparity estimates than both non-learning and learning-based algorithms. Additionally, our factor graph formulation can be useful for solving optimization problems with complex and variable dependency structures and for other dense estimation problems, such as optical flow estimation.",1
"Facial expression spotting is the preliminary step for micro- and macro-expression analysis. The task of reliably spotting such expressions in video sequences is currently unsolved. The current best systems depend upon optical flow methods to extract regional motion features, before categorisation of that motion into a specific class of facial movement. Optical flow is susceptible to drift error, which introduces a serious problem for motions with long-term dependencies, such as high frame-rate macro-expression. We propose a purely deep learning solution which, rather than tracking frame differential motion, compares via a convolutional model, each frame with two temporally local reference frames. Reference frames are sampled according to calculated micro- and macro-expression durations. We show that our solution achieves state-of-the-art performance (F1-score of 0.105) in a dataset of high frame-rate (200 fps) long video sequences (SAMM-LV) and is competitive in a low frame-rate (30 fps) dataset (CAS(ME)2). In this paper, we document our deep learning model and parameters, including how we use local contrast normalisation, which we show is critical for optimal results. We surpass a limitation in existing methods, and advance the state of deep learning in the domain of facial expression spotting.",0
"The initial phase of micro- and macro-expression analysis involves identifying facial expressions through spotting. However, accurately spotting these expressions in video sequences remains a challenge. The present top-performing systems rely on optical flow methods to extract motion features in specific regions, followed by categorizing the motion into facial movements. However, optical flow is prone to drift error, particularly for long-term dependent motions such as high frame-rate macro-expressions. In this study, we propose a deep learning solution that uses a convolutional model to compare each frame with two temporally local reference frames, which are sampled based on the duration of micro- and macro-expressions. Our approach achieves state-of-the-art performance (F1-score of 0.105) in a dataset of high frame-rate videos (200 fps) with long sequences (SAMM-LV) and is competitive in a low frame-rate dataset (CAS(ME)2). We describe our deep learning model and parameters, including the importance of local contrast normalization for optimal results. Our method overcomes a limitation of existing approaches and advances the state of deep learning in facial expression spotting.",1
"The temporal and spatial resolution of rainfall data is crucial for climate change modeling studies in which its variability in space and time is considered as a primary factor. Rainfall products from different remote sensing instruments (e.g., radar or satellite) provide different space-time resolutions because of the differences in their sensing capabilities. We developed an approach that augments rainfall data with increased time resolutions to complement relatively lower resolution products. This study proposes a neural network architecture based on Convolutional Neural Networks (CNNs) to improve temporal resolution of radar-based rainfall products and compares the proposed model with an optical flow-based interpolation method.",0
"The importance of rainfall data's temporal and spatial resolution cannot be understated in climate change modeling studies, where its variability in space and time is a key consideration. Various remote sensing instruments, such as radar or satellite, produce rainfall products with different space-time resolutions due to variations in their sensing capabilities. To supplement lower resolution products, we have devised an approach that enhances rainfall data with increased time resolutions. Our study introduces a neural network architecture that employs Convolutional Neural Networks (CNNs) to enhance the temporal resolution of radar-based rainfall products, and we compare its performance with an optical flow-based interpolation method.",1
"Lines provide the significantly richer geometric structural information about the environment than points, so lines are widely used in recent Visual Odometry (VO) works. Since VO with lines use line tracking results to locate and map, line tracking is a crucial component in VO. Although the state-of-the-art line tracking methods have made great progress, they are still heavily dependent on line detection or the predicted line segments. In order to relieve the dependencies described above to track line segments completely, accurately, and robustly at higher computational efficiency, we propose a structure-aware Line tracking algorithm based entirely on Optical Flow (LOF). Firstly, we propose a gradient-based strategy to sample pixels on lines that are suitable for line optical flow calculation. Then, in order to align the lines by fully using the structural relationship between the sampled points on it and effectively removing the influence of sampled points on it occluded by other objects, we propose a two-step structure-aware line segment alignment method. Furthermore, we propose a line refinement method to refine the orientation, position, and endpoints of the aligned line segments. Extensive experimental results demonstrate that the proposed LOF outperforms the state-of-the-art performance in line tracking accuracy, robustness, and efficiency, which also improves the location accuracy and robustness of VO system with lines.",0
"Recent Visual Odometry (VO) works commonly use lines instead of points in order to obtain richer geometric structural information about the environment. Line tracking is a crucial component in VO as it uses line tracking results to locate and map. Despite the progress made by state-of-the-art line tracking methods, they heavily rely on line detection or predicted line segments. To overcome this dependency and achieve efficient, accurate, and robust line tracking, we propose an entirely Optical Flow (LOF)-based structure-aware Line tracking algorithm. Our algorithm uses a gradient-based strategy to sample pixels on lines suitable for line optical flow calculation. We then propose a two-step structure-aware line segment alignment method to align the lines fully using the structural relationship between the sampled points and effectively remove occluded points. Additionally, we introduce a line refinement method to refine the orientation, position, and endpoints of the aligned line segments. Extensive experimental results demonstrate that our proposed LOF algorithm outperforms the state-of-the-art in line tracking accuracy, robustness, and efficiency, improving the VO system's location accuracy and robustness with lines.",1
"Although deep neural networks (DNNs) enable great progress in video abnormal event detection (VAD), existing solutions typically suffer from two issues: (1) The localization of video events cannot be both precious and comprehensive. (2) The semantics and temporal context are under-explored. To tackle those issues, we are motivated by the prevalent cloze test in education and propose a novel approach named Visual Cloze Completion (VCC), which conducts VAD by learning to complete ""visual cloze tests"" (VCTs). Specifically, VCC first localizes each video event and encloses it into a spatio-temporal cube (STC). To achieve both precise and comprehensive localization, appearance and motion are used as complementary cues to mark the object region associated with each event. For each marked region, a normalized patch sequence is extracted from current and adjacent frames and stacked into a STC. With each patch and the patch sequence of a STC compared to a visual ""word"" and ""sentence"" respectively, we deliberately erase a certain ""word"" (patch) to yield a VCT. Then, the VCT is completed by training DNNs to infer the erased patch and its optical flow via video semantics. Meanwhile, VCC fully exploits temporal context by alternatively erasing each patch in temporal context and creating multiple VCTs. Furthermore, we propose localization-level, event-level, model-level and decision-level solutions to enhance VCC, which can further exploit VCC's potential and produce significant performance improvement gain. Extensive experiments demonstrate that VCC achieves state-of-the-art VAD performance. Our codes and results are open at https://github.com/yuguangnudt/VEC_VAD/tree/VCC.",0
"Video abnormal event detection (VAD) has made substantial progress with the use of deep neural networks (DNNs), but current methods suffer from two issues: imprecise video event localization and underexplored semantics and temporal context. To address these challenges, we propose a new approach called Visual Cloze Completion (VCC), inspired by the cloze test in education. VCC localizes each video event and encloses it in a spatio-temporal cube (STC) using appearance and motion cues for precise and comprehensive localization. A normalized patch sequence is extracted from current and adjacent frames and stacked into a STC, where each patch is compared to a visual ""word"" and ""sentence"" respectively. By erasing a certain patch, we create a visual cloze test (VCT) that is completed by training DNNs to infer the erased patch and its optical flow via video semantics. We also propose solutions at different levels to enhance VCC's performance. Our experiments demonstrate that VCC achieves state-of-the-art VAD performance and our codes and results are available at https://github.com/yuguangnudt/VEC_VAD/tree/VCC.",1
"We introduce RAFT-Stereo, a new deep architecture for rectified stereo based on the optical flow network RAFT. We introduce multi-level convolutional GRUs, which more efficiently propagate information across the image. A modified version of RAFT-Stereo can perform accurate real-time inference. RAFT-stereo ranks first on the Middlebury leaderboard, outperforming the next best method on 1px error by 29% and outperforms all published work on the ETH3D two-view stereo benchmark. Code is available at https://github.com/princeton-vl/RAFT-Stereo.",0
"Introducing RAFT-Stereo, an innovative deep structure for rectified stereo that relies on the RAFT optical flow network. Multi-level convolutional GRUs are used to propagate information across the image in a more efficient manner. With a modified version of RAFT-Stereo, real-time inference can be performed with high accuracy. RAFT-Stereo has achieved the top rank on the Middlebury leaderboard, surpassing the next best method by 29% in 1px error, and has outperformed all previously published works on the ETH3D two-view stereo benchmark. The code for RAFT-Stereo is available at https://github.com/princeton-vl/RAFT-Stereo.",1
"Automatic portrait video matting is an under-constrained problem. Most state-of-the-art methods only exploit the semantic information and process each frame individually. Their performance is compromised due to the lack of temporal information between the frames. To solve this problem, we propose the context motion network to leverage semantic information and motion information. To capture the motion information, we estimate the optical flow and design a context-motion updating operator to integrate features between frames recurrently. Our experiments show that our network outperforms state-of-the-art matting methods significantly on the Video240K SD dataset.",0
"The issue of automatic portrait video matting is one that lacks sufficient constraints. Current methods primarily rely on semantic information and analyze each frame separately, which results in a decrease in performance due to the absence of temporal data between frames. In order to address this problem, our solution is to implement the context motion network, which combines both semantic and motion information. By estimating optical flow and incorporating a context-motion updating operator, we are able to recurrently integrate features between frames to capture motion information. Our experiments demonstrate that our network surpasses other matting methods on the Video240K SD dataset.",1
"Video semantic segmentation requires to utilize the complex temporal relations between frames of the video sequence. Previous works usually exploit accurate optical flow to leverage the temporal relations, which suffer much from heavy computational cost. In this paper, we propose a Temporal Memory Attention Network (TMANet) to adaptively integrate the long-range temporal relations over the video sequence based on the self-attention mechanism without exhaustive optical flow prediction. Specially, we construct a memory using several past frames to store the temporal information of the current frame. We then propose a temporal memory attention module to capture the relation between the current frame and the memory to enhance the representation of the current frame. Our method achieves new state-of-the-art performances on two challenging video semantic segmentation datasets, particularly 80.3% mIoU on Cityscapes and 76.5% mIoU on CamVid with ResNet-50.",0
"To accurately segment videos semantically, it is necessary to consider the complex temporal relationships between frames in the video sequence. However, previous approaches have relied on expensive optical flow algorithms to leverage these relationships. In this study, we introduce a Temporal Memory Attention Network (TMANet) that can integrate long-range temporal relations using a self-attention mechanism without relying on optical flow. We use a memory module to store temporal information from several past frames, and a temporal memory attention module to enhance the current frame's representation by capturing the relationship between the memory and the current frame. Our approach outperforms previous methods, achieving state-of-the-art results on challenging video semantic segmentation datasets, with mIoU scores of 80.3% and 76.5% on Cityscapes and CamVid using ResNet-50.",1
"One of the most common problems encountered in human-computer interaction is automatic facial expression recognition. Although it is easy for human observer to recognize facial expressions, automatic recognition remains difficult for machines. One of the methods that machines can recognize facial expression is analyzing the changes in face during facial expression presentation. In this paper, optical flow algorithm was used to extract deformation or motion vectors created in the face because of facial expressions. Then, these extracted motion vectors are used to be analyzed. Their positions and directions were exploited for automatic facial expression recognition using different data mining techniques. It means that by employing motion vector features used as our data, facial expressions were recognized. Some of the most state-of-the-art classification algorithms such as C5.0, CRT, QUEST, CHAID, Deep Learning (DL), SVM and Discriminant algorithms were used to classify the extracted motion vectors. Using 10-fold cross validation, their performances were calculated. To compare their performance more precisely, the test was repeated 50 times. Meanwhile, the deformation of face was also analyzed in this research. For example, what exactly happened in each part of face when a person showed fear? Experimental results on Extended Cohen-Kanade (CK+) facial expression dataset demonstrated that the best methods were DL, SVM and C5.0, with the accuracy of 95.3%, 92.8% and 90.2% respectively.",0
"Facial expression recognition is a common problem in human-computer interaction. Although it is easy for a human observer to recognize facial expressions, machines still struggle with automatic recognition. One way machines can recognize facial expressions is by analyzing changes in the face during expressions. This study used the optical flow algorithm to extract motion vectors caused by facial expressions. The motion vectors were then analyzed using various data mining techniques to automatically recognize facial expressions. State-of-the-art classification algorithms such as C5.0, CRT, QUEST, CHAID, Deep Learning (DL), SVM and Discriminant algorithms were used to classify the motion vectors. Their performances were calculated using 10-fold cross validation and repeated 50 times. The study also analyzed the deformation of the face during expressions, such as what happened in each part of the face during fear. The experimental results using the Extended Cohen-Kanade (CK+) facial expression dataset showed that the best methods were DL, SVM and C5.0 with an accuracy of 95.3%, 92.8% and 90.2% respectively.",1
"Event camera has offered promising alternative for visual perception, especially in high speed and high dynamic range scenes. Recently, many deep learning methods have shown great success in providing model-free solutions to many event-based problems, such as optical flow estimation. However, existing deep learning methods did not address the importance of temporal information well from the perspective of architecture design and cannot effectively extract spatio-temporal features. Another line of research that utilizes Spiking Neural Network suffers from training issues for deeper architecture. To address these points, a novel input representation is proposed that captures the events temporal distribution for signal enhancement. Moreover, we introduce a spatio-temporal recurrent encoding-decoding neural network architecture for event-based optical flow estimation, which utilizes Convolutional Gated Recurrent Units to extract feature maps from a series of event images. Besides, our architecture allows some traditional frame-based core modules, such as correlation layer and iterative residual refine scheme, to be incorporated. The network is end-to-end trained with self-supervised learning on the Multi-Vehicle Stereo Event Camera dataset. We have shown that it outperforms all the existing state-of-the-art methods by a large margin.",0
"The event camera is a promising option for visual perception in fast-moving and high-contrast environments. Deep learning techniques have been successful in solving event-based problems, such as optical flow estimation, without the need for a pre-defined model. However, these methods have not fully considered the significance of temporal information in their design, resulting in poor spatio-temporal feature extraction. There is also a challenge in training Spiking Neural Networks with deeper architectures. To address these limitations, a novel input representation has been proposed to improve signal enhancement by capturing temporal events distribution. Additionally, a spatio-temporal recurrent encoding-decoding neural network architecture has been developed for event-based optical flow estimation using Convolutional Gated Recurrent Units to extract feature maps from event images. The network also incorporates traditional frame-based core modules, such as correlation layers and iterative residual refine schemes. The network has been end-to-end trained with self-supervised learning on the Multi-Vehicle Stereo Event Camera dataset and has shown significant improvement over existing state-of-the-art methods.",1
"We propose a novel neural-network-based method to perform matting of videos depicting people that does not require additional user input such as trimaps. Our architecture achieves temporal stability of the resulting alpha mattes by using motion-estimation-based smoothing of image-segmentation algorithm outputs, combined with convolutional-LSTM modules on U-Net skip connections.   We also propose a fake-motion algorithm that generates training clips for the video-matting network given photos with ground-truth alpha mattes and background videos. We apply random motion to photos and their mattes to simulate movement one would find in real videos and composite the result with the background clips. It lets us train a deep neural network operating on videos in an absence of a large annotated video dataset and provides ground-truth training-clip foreground optical flow for use in loss functions.",0
"Our innovative approach for video matting of people eliminates the need for additional user input, such as trimaps. Our neural network architecture employs motion-estimation-based smoothing of image-segmentation algorithm outputs, along with convolutional-LSTM modules on U-Net skip connections, to maintain temporal stability in the alpha mattes. Additionally, we introduce a fake-motion algorithm to generate training clips for the video-matting network, using photos with ground-truth alpha mattes and background videos. This allows us to simulate movement found in real videos and train the network without a large annotated video dataset. The resulting foreground optical flow from the training clips is used in loss functions to enhance accuracy.",1
"Self-supervised Multi-view stereo (MVS) with a pretext task of image reconstruction has achieved significant progress recently. However, previous methods are built upon intuitions, lacking comprehensive explanations about the effectiveness of the pretext task in self-supervised MVS. To this end, we propose to estimate epistemic uncertainty in self-supervised MVS, accounting for what the model ignores. Specially, the limitations can be categorized into two types: ambiguious supervision in foreground and invalid supervision in background. To address these issues, we propose a novel Uncertainty reduction Multi-view Stereo (UMVS) framework for self-supervised learning. To alleviate ambiguous supervision in foreground, we involve extra correspondence prior with a flow-depth consistency loss. The dense 2D correspondence of optical flows is used to regularize the 3D stereo correspondence in MVS. To handle the invalid supervision in background, we use Monte-Carlo Dropout to acquire the uncertainty map and further filter the unreliable supervision signals on invalid regions. Extensive experiments on DTU and Tank&Temples benchmark show that our U-MVS framework achieves the best performance among unsupervised MVS methods, with competitive performance with its supervised opponents.",0
"In recent times, Self-supervised Multi-view stereo (MVS) using an image reconstruction pretext task has made significant strides. However, earlier approaches were based on assumptions and lacked thorough explanations about the effectiveness of the pretext task in self-supervised MVS. To address this, we suggest estimating epistemic uncertainty in self-supervised MVS, which accounts for what the model ignores. We categorize the limitations into two types: ambiguous supervision in foreground and invalid supervision in background. To tackle these challenges, we propose a new framework for self-supervised learning called Uncertainty reduction Multi-view Stereo (UMVS). To address ambiguous supervision in foreground, we include additional correspondence prior with a flow-depth consistency loss. We use dense 2D correspondence of optical flows to regulate the 3D stereo correspondence in MVS. To handle invalid supervision in background, we use Monte-Carlo Dropout to obtain the uncertainty map and further eliminate unreliable supervision signals in invalid regions. Our U-MVS framework achieves the best performance among unsupervised MVS methods, with competitive performance with supervised ones, as demonstrated by extensive experiments on DTU and Tank&Temples benchmark.",1
"There hardly exists any large-scale datasets with dense optical flow of non-rigid motion from real-world imagery as of today. The reason lies mainly in the required setup to derive ground truth optical flows: a series of images with known camera poses along its trajectory, and an accurate 3D model from a textured scene. Human annotation is not only too tedious for large databases, it can simply hardly contribute to accurate optical flow. To circumvent the need for manual annotation, we propose a framework to automatically generate optical flow from real-world videos. The method extracts and matches objects from video frames to compute initial constraints, and applies a deformation over the objects of interest to obtain dense optical flow fields. We propose several ways to augment the optical flow variations. Extensive experimental results show that training on our automatically generated optical flow outperforms methods that are trained on rigid synthetic data using FlowNet-S, LiteFlowNet, PWC-Net, and RAFT. Datasets and implementation of our optical flow generation framework are released at https://github.com/lhoangan/arap_flow",0
"Currently, there is a significant scarcity of large-scale datasets with dense optical flow of non-rigid motion from real-world imagery. The principal reason for this is the requisite setup to obtain ground truth optical flows, which involves a sequence of photographs with known camera positions and an accurate 3D model of a textured environment. Human annotation is impractical for extensive databases and does not contribute to precise optical flow. To eliminate the necessity for manual annotation, we propose an automated framework to generate optical flow from real-world videos. Our approach involves object extraction and matching from video frames to calculate initial constraints, followed by object deformation to obtain dense optical flow fields. We present several methods to enhance optical flow variations. Our extensive experimental results demonstrate that our framework outperforms methods that rely on rigid synthetic data by using FlowNet-S, LiteFlowNet, PWC-Net, and RAFT. We have released our optical flow generation framework's datasets and implementation at https://github.com/lhoangan/arap_flow.",1
"As moving objects always draw more attention of human eyes, the temporal motive information is always exploited complementarily with spatial information to detect salient objects in videos. Although efficient tools such as optical flow have been proposed to extract temporal motive information, it often encounters difficulties when used for saliency detection due to the movement of camera or the partial movement of salient objects. In this paper, we investigate the complimentary roles of spatial and temporal information and propose a novel dynamic spatiotemporal network (DS-Net) for more effective fusion of spatiotemporal information. We construct a symmetric two-bypass network to explicitly extract spatial and temporal features. A dynamic weight generator (DWG) is designed to automatically learn the reliability of corresponding saliency branch. And a top-down cross attentive aggregation (CAA) procedure is designed so as to facilitate dynamic complementary aggregation of spatiotemporal features. Finally, the features are modified by spatial attention with the guidance of coarse saliency map and then go through decoder part for final saliency map. Experimental results on five benchmarks VOS, DAVIS, FBMS, SegTrack-v2, and ViSal demonstrate that the proposed method achieves superior performance than state-of-the-art algorithms. The source code is available at https://github.com/TJUMMG/DS-Net.",0
"Human eyes are naturally drawn to moving objects, making temporal motive information a useful complement to spatial information for detecting salient objects in videos. However, optical flow, a common tool for extracting temporal motive information, can face challenges in saliency detection due to camera movements or partial movement of salient objects. To address this, we propose a novel dynamic spatiotemporal network (DS-Net) that leverages both spatial and temporal information more effectively. Our symmetric two-bypass network explicitly extracts spatial and temporal features, while a dynamic weight generator (DWG) learns the reliability of the corresponding saliency branch. A top-down cross attentive aggregation (CAA) procedure facilitates dynamic complementary aggregation of spatiotemporal features. Finally, spatial attention guides the modification of features with a coarse saliency map before the decoder generates the final saliency map. Our experiments demonstrate that DS-Net outperforms state-of-the-art algorithms on five benchmarks, including VOS, DAVIS, FBMS, SegTrack-v2, and ViSal. The source code is available at https://github.com/TJUMMG/DS-Net.",1
"In this work, we tackle the essential problem of scale inconsistency for self-supervised joint depth-pose learning. Most existing methods assume that a consistent scale of depth and pose can be learned across all input samples, which makes the learning problem harder, resulting in degraded performance and limited generalization in indoor environments and long-sequence visual odometry application. To address this issue, we propose a novel system that explicitly disentangles scale from the network estimation. Instead of relying on PoseNet architecture, our method recovers relative pose by directly solving fundamental matrix from dense optical flow correspondence and makes use of a two-view triangulation module to recover an up-to-scale 3D structure. Then, we align the scale of the depth prediction with the triangulated point cloud and use the transformed depth map for depth error computation and dense reprojection check. Our whole system can be jointly trained end-to-end. Extensive experiments show that our system not only reaches state-of-the-art performance on KITTI depth and flow estimation, but also significantly improves the generalization ability of existing self-supervised depth-pose learning methods under a variety of challenging scenarios, and achieves state-of-the-art results among self-supervised learning-based methods on KITTI Odometry and NYUv2 dataset. Furthermore, we present some interesting findings on the limitation of PoseNet-based relative pose estimation methods in terms of generalization ability. Code is available at https://github.com/B1ueber2y/TrianFlow.",0
"The focus of our work is on addressing the problem of scale inconsistency in self-supervised joint depth-pose learning. Most existing methods rely on learning a consistent scale of depth and pose for all input samples, which leads to poor performance and limited generalization in indoor environments and long-sequence visual odometry applications. To overcome this issue, we propose a novel system that disentangles scale from the network estimation by using dense optical flow correspondence to recover relative pose and a two-view triangulation module to recover an up-to-scale 3D structure. We align the scale of the depth prediction with the triangulated point cloud and use the transformed depth map for depth error computation and dense reprojection check. Our system can be jointly trained end-to-end and achieves state-of-the-art results on KITTI depth and flow estimation, KITTI Odometry, and NYUv2 dataset. Additionally, we provide insights into the limitations of PoseNet-based relative pose estimation methods. Our code is available at https://github.com/B1ueber2y/TrianFlow.",1
"Recent efforts towards video anomaly detection (VAD) try to learn a deep autoencoder to describe normal event patterns with small reconstruction errors. The video inputs with large reconstruction errors are regarded as anomalies at the test time. However, these methods sometimes reconstruct abnormal inputs well because of the powerful generalization ability of deep autoencoder. To address this problem, we present a novel approach for anomaly detection, which utilizes discriminative prototypes of normal data to reconstruct video frames. In this way, the model will favor the reconstruction of normal events and distort the reconstruction of abnormal events. Specifically, we use a prototype-guided memory module to perform discriminative latent embedding. We introduce a new discriminative criterion for the memory module, as well as a loss function correspondingly, which can encourage memory items to record the representative embeddings of normal data, i.e. prototypes. Besides, we design a novel two-branch autoencoder, which is composed of a future frame prediction network and an RGB difference generation network that share the same encoder. The stacked RGB difference contains motion information just like optical flow, so our model can learn temporal regularity. We evaluate the effectiveness of our method on three benchmark datasets and experimental results demonstrate the proposed method outperforms the state-of-the-art.",0
"Video anomaly detection (VAD) has been exploring the use of deep autoencoders to identify normal event patterns with minimal reconstruction errors. However, this approach may not always work effectively because deep autoencoders have a potent generalization ability that may reconstruct abnormal inputs well. To mitigate this problem, we propose a new method that leverages discriminative prototypes of normal data to reconstruct video frames. This model favors the reconstruction of normal events and distorts the reconstruction of abnormal ones by using a prototype-guided memory module to perform discriminative latent embedding. To encourage memory items to record representative embeddings of normal data (i.e. prototypes), we introduce a new discriminative criterion and corresponding loss function. Additionally, we design a novel two-branch autoencoder that includes a future frame prediction network and an RGB difference generation network that share the same encoder. The stacked RGB difference captures motion information similar to optical flow, enabling our model to learn temporal regularity. We evaluate our approach on three benchmark datasets and show that it outperforms the state-of-the-art.",1
"Action anticipation in egocentric videos is a difficult task due to the inherently multi-modal nature of human actions. Additionally, some actions happen faster or slower than others depending on the actor or surrounding context which could vary each time and lead to different predictions. Based on this idea, we build upon RULSTM architecture, which is specifically designed for anticipating human actions, and propose a novel attention-based technique to evaluate, simultaneously, slow and fast features extracted from three different modalities, namely RGB, optical flow, and extracted objects. Two branches process information at different time scales, i.e., frame-rates, and several fusion schemes are considered to improve prediction accuracy. We perform extensive experiments on EpicKitchens-55 and EGTEA Gaze+ datasets, and demonstrate that our technique systematically improves the results of RULSTM architecture for Top-5 accuracy metric at different anticipation times.",0
"Predicting actions in egocentric videos is a challenging task due to the complex and varied nature of human actions. The speed of actions can vary based on the actor or context, leading to different outcomes. To address this, we enhanced the RULSTM architecture, designed for human action prediction, with a new attention-based technique to assess both slow and fast features from three different modalities - RGB, optical flow, and extracted objects. Two branches with different frame-rates process information, and multiple fusion schemes are tested to enhance prediction accuracy. Extensive testing on EpicKitchens-55 and EGTEA Gaze+ datasets showed that our approach consistently improved the Top-5 accuracy metric for different anticipation times compared to the RULSTM architecture.",1
"We propose FlowReg, a deep learning-based framework for unsupervised image registration for neuroimaging applications. The system is composed of two architectures that are trained sequentially: FlowReg-A which affinely corrects for gross differences between moving and fixed volumes in 3D followed by FlowReg-O which performs pixel-wise deformations on a slice-by-slice basis for fine tuning in 2D. The affine network regresses the 3D affine matrix based on a correlation loss function that enforces global similarity. The deformable network operates on 2D image slices based on the optical flow network FlowNet-Simple but with three loss components. The photometric loss minimizes pixel intensity differences differences, the smoothness loss encourages similar magnitudes between neighbouring vectors, and a correlation loss that is used to maintain the intensity similarity between fixed and moving image slices. The proposed method is compared to four open source registration techniques ANTs, Demons, SE, and Voxelmorph. In total, 4643 FLAIR MR imaging volumes are used from dementia and vascular disease cohorts, acquired from over 60 international centres with varying acquisition parameters. A battery of quantitative novel registration validation metrics are proposed that focus on the structural integrity of tissues, spatial alignment, and intensity similarity. Experimental results show FlowReg (FlowReg-A+O) performs better than iterative-based registration algorithms for intensity and spatial alignment metrics with a Pixelwise Agreement of 0.65, correlation coefficient of 0.80, and Mutual Information of 0.29. Among the deep learning frameworks, FlowReg-A or FlowReg-A+O provided the highest performance over all but one of the metrics. Results show that FlowReg is able to obtain high intensity and spatial similarity while maintaining the shape and structure of anatomy and pathology.",0
"Our proposition is FlowReg, which is a deep learning framework designed for unsupervised image registration in neuroimaging applications. The system comprises two architectures that are trained in a sequential manner. Firstly, FlowReg-A corrects for significant differences between 3D moving and fixed volumes using affine correction. Secondly, FlowReg-O performs pixel-wise deformations on a slice-by-slice basis to fine-tune in 2D. The affine network utilizes a correlation loss function to enforce global similarity and regress the 3D affine matrix. The deformable network operates on 2D image slices using the optical flow network FlowNet-Simple with three loss components: photometric loss, smoothness loss, and correlation loss. The proposed approach is compared to four open source registration techniques, namely ANTs, Demons, SE, and Voxelmorph, using a total of 4643 FLAIR MR imaging volumes from dementia and vascular disease cohorts. New validation metrics are proposed for spatial alignment, structural integrity of tissues, and intensity similarity. Experimental results indicate that FlowReg (FlowReg-A+O) outperforms iterative-based registration algorithms for intensity and spatial alignment metrics with a Pixelwise Agreement of 0.65, correlation coefficient of 0.80, and Mutual Information of 0.29. Among deep learning frameworks, FlowReg-A or FlowReg-A+O provided the highest performance over all but one of the metrics. The results demonstrate that FlowReg is capable of achieving high intensity and spatial similarity while preserving the shape and structure of anatomy and pathology.",1
"Recent years have seen considerable research activities devoted to video enhancement that simultaneously increases temporal frame rate and spatial resolution. However, the existing methods either fail to explore the intrinsic relationship between temporal and spatial information or lack flexibility in the choice of final temporal/spatial resolution. In this work, we propose an unconstrained space-time video super-resolution network, which can effectively exploit space-time correlation to boost performance. Moreover, it has complete freedom in adjusting the temporal frame rate and spatial resolution through the use of the optical flow technique and a generalized pixelshuffle operation. Our extensive experiments demonstrate that the proposed method not only outperforms the state-of-the-art, but also requires far fewer parameters and less running time.",0
"In recent years, there has been significant research focused on enhancing videos by simultaneously increasing their temporal frame rate and spatial resolution. However, current methods do not effectively consider the relationship between temporal and spatial information or provide flexibility in choosing the final temporal/spatial resolution. To address this, we propose an unconstrained space-time video super-resolution network that leverages space-time correlation to improve performance. Our approach allows for complete control over adjusting the temporal frame rate and spatial resolution using optical flow and a generalized pixelshuffle operation. Our experiments demonstrate that our method outperforms the state-of-the-art while requiring fewer parameters and less running time.",1
"We propose to incorporate feature correlation and sequential processing into dense optical flow estimation from event cameras. Modern frame-based optical flow methods heavily rely on matching costs computed from feature correlation. In contrast, there exists no optical flow method for event cameras that explicitly computes matching costs. Instead, learning-based approaches using events usually resort to the U-Net architecture to estimate optical flow sparsely. Our key finding is that the introduction of correlation features significantly improves results compared to previous methods that solely rely on convolution layers. Compared to the state-of-the-art, our proposed approach computes dense optical flow and reduces the end-point error by 23% on MVSEC. Furthermore, we show that all existing optical flow methods developed so far for event cameras have been evaluated on datasets with very small displacement fields with a maximum flow magnitude of 10 pixels. Based on this observation, we introduce a new real-world dataset that exhibits displacement fields with magnitudes up to 210 pixels and 3 times higher camera resolution. Our proposed approach reduces the end-point error on this dataset by 66%.",0
"Our proposal suggests integrating feature correlation and sequential processing into the estimation of dense optical flow from event cameras. Unlike current frame-based optical flow techniques that rely heavily on matching costs computed from feature correlation, there is currently no optical flow method for event cameras that explicitly computes matching costs. Instead, event-based learning approaches typically utilize the U-Net architecture to sparsely estimate optical flow. Our research has found that incorporating correlation features significantly enhances results compared to previous methods that solely use convolution layers. Our proposed method computes dense optical flow and reduces the end-point error by 23% on MVSEC, surpassing the current state-of-the-art. Additionally, we discovered that all existing optical flow methods for event cameras have only been evaluated on datasets with small displacement fields, with a maximum flow magnitude of 10 pixels. Therefore, we introduce a new real-world dataset that exhibits displacement fields with magnitudes up to 210 pixels and 3 times higher camera resolution. Our proposed approach demonstrates a 66% reduction in the end-point error on this dataset.",1
"Optical flow is inherently a 2D search problem, and thus the computational complexity grows quadratically with respect to the search window, making large displacements matching infeasible for high-resolution images. In this paper, we take inspiration from Transformers and propose a new method for high-resolution optical flow estimation with significantly less computation. Specifically, a 1D attention operation is first applied in the vertical direction of the target image, and then a simple 1D correlation in the horizontal direction of the attended image is able to achieve 2D correspondence modeling effect. The directions of attention and correlation can also be exchanged, resulting in two 3D cost volumes that are concatenated for optical flow estimation. The novel 1D formulation empowers our method to scale to very high-resolution input images while maintaining competitive performance. Extensive experiments on Sintel, KITTI and real-world 4K ($2160 \times 3840$) resolution images demonstrated the effectiveness and superiority of our proposed method. Code and models are available at \url{https://github.com/haofeixu/flow1d}.",0
"The complexity of optical flow increases quadratically as the search window expands, rendering it impractical to match large displacements in high-resolution images. This paper draws inspiration from Transformers to introduce a new technique for high-resolution optical flow estimation that requires significantly less computation. Initially, a 1D attention operation is applied vertically to the target image, followed by a simple 1D correlation horizontally on the attended image, which achieves a 2D correspondence modeling effect. The attention and correlation directions are interchangeable, resulting in two concatenated 3D cost volumes for optical flow estimation. This 1D approach enables the method to scale up to very high-resolution input images while maintaining competitive performance. Experimental tests on Sintel, KITTI, and real-world 4K ($2160 \times 3840$) resolution images validate the effectiveness and superiority of the proposed method. The code and models are accessible at \url{https://github.com/haofeixu/flow1d}.",1
"In this paper, we propose to learn an Unsupervised Single Object Tracker (USOT) from scratch. We identify that three major challenges, i.e., moving object discovery, rich temporal variation exploitation, and online update, are the central causes of the performance bottleneck of existing unsupervised trackers. To narrow the gap between unsupervised trackers and supervised counterparts, we propose an effective unsupervised learning approach composed of three stages. First, we sample sequentially moving objects with unsupervised optical flow and dynamic programming, instead of random cropping. Second, we train a naive Siamese tracker from scratch using single-frame pairs. Third, we continue training the tracker with a novel cycle memory learning scheme, which is conducted in longer temporal spans and also enables our tracker to update online. Extensive experiments show that the proposed USOT learned from unlabeled videos performs well over the state-of-the-art unsupervised trackers by large margins, and on par with recent supervised deep trackers. Code is available at https://github.com/VISION-SJTU/USOT.",0
"This paper introduces the concept of an Unsupervised Single Object Tracker (USOT), which is developed from scratch. Existing unsupervised trackers face three major challenges, including identifying moving objects, exploiting rich temporal variation, and online updating, which lead to performance limitations. To bridge the gap between unsupervised and supervised trackers, we propose a three-stage learning approach. Firstly, we use unsupervised optical flow and dynamic programming to sequentially sample moving objects instead of random cropping. Secondly, we train a basic Siamese tracker from scratch using single-frame pairs. Thirdly, we continue training the tracker with a new cycle memory learning scheme, which spans longer temporal durations and enables our tracker to update online. Extensive experiments demonstrate that our proposed USOT, which is learned from unlabeled videos, outperforms state-of-the-art unsupervised trackers and is comparable to recent supervised deep trackers. The code for this project is available at https://github.com/VISION-SJTU/USOT.",1
"This paper presents a novel method for pedestrian detection and tracking by fusing camera and LiDAR sensor data. To deal with the challenges associated with the autonomous driving scenarios, an integrated tracking and detection framework is proposed. The detection phase is performed by converting LiDAR streams to computationally tractable depth images, and then, a deep neural network is developed to identify pedestrian candidates both in RGB and depth images. To provide accurate information, the detection phase is further enhanced by fusing multi-modal sensor information using the Kalman filter. The tracking phase is a combination of the Kalman filter prediction and an optical flow algorithm to track multiple pedestrians in a scene. We evaluate our framework on a real public driving dataset. Experimental results demonstrate that the proposed method achieves significant performance improvement over a baseline method that solely uses image-based pedestrian detection.",0
"In this paper, a new approach to detecting and tracking pedestrians is presented, which combines data from a camera and LiDAR sensor. In order to address the difficulties of autonomous driving situations, a tracking and detection framework is proposed. The detection process involves converting LiDAR data into depth images, which are then analyzed using a deep neural network to identify potential pedestrians in both RGB and depth images. To ensure accuracy, multi-modal sensor data is fused together using the Kalman filter. In the tracking phase, a combination of the Kalman filter prediction and optical flow algorithm is used to track multiple pedestrians in a scene. The effectiveness of the framework is demonstrated by evaluating it on a real driving dataset. Results show that the proposed method outperforms a baseline method that relies solely on image-based pedestrian detection.",1
"Learning transferable and domain adaptive feature representations from videos is important for video-relevant tasks such as action recognition. Existing video domain adaptation methods mainly rely on adversarial feature alignment, which has been derived from the RGB image space. However, video data is usually associated with multi-modal information, e.g., RGB and optical flow, and thus it remains a challenge to design a better method that considers the cross-modal inputs under the cross-domain adaptation setting. To this end, we propose a unified framework for video domain adaptation, which simultaneously regularizes cross-modal and cross-domain feature representations. Specifically, we treat each modality in a domain as a view and leverage the contrastive learning technique with properly designed sampling strategies. As a result, our objectives regularize feature spaces, which originally lack the connection across modalities or have less alignment across domains. We conduct experiments on domain adaptive action recognition benchmark datasets, i.e., UCF, HMDB, and EPIC-Kitchens, and demonstrate the effectiveness of our components against state-of-the-art algorithms.",0
"Developing transferable and domain adaptive feature representations from videos is crucial for tasks related to video analysis, such as action recognition. Currently, video domain adaptation methods rely heavily on adversarial feature alignment, which only considers the RGB image space. However, video data typically includes multi-modal information, including RGB and optical flow, making it difficult to design an effective method that accounts for cross-modal inputs under the cross-domain adaptation setting. Our proposed solution is a unified framework for video domain adaptation that simultaneously regulates cross-modal and cross-domain feature representations. We view each modality in a domain as a view and employ contrastive learning with well-designed sampling strategies to achieve our objectives. This approach enables us to regulate feature spaces that lack connectivity across modalities or have less alignment across domains. We evaluate our approach on benchmark datasets for domain adaptive action recognition, namely UCF, HMDB, and EPIC-Kitchens, and demonstrate its superiority over state-of-the-art algorithms.",1
"We propose a framework for aligning and fusing multiple images into a single coordinate-based neural representations. Our framework targets burst images that have misalignment due to camera ego motion and small changes in the scene. We describe different strategies for alignment depending on the assumption of the scene motion, namely, perspective planar (i.e., homography), optical flow with minimal scene change, and optical flow with notable occlusion and disocclusion. Our framework effectively combines the multiple inputs into a single neural implicit function without the need for selecting one of the images as a reference frame. We demonstrate how to use this multi-frame fusion framework for various layer separation tasks.",0
"A framework is presented in this study for the purpose of aligning and merging multiple images into a unified neural representation based on coordinates. This framework is designed to address the issue of misalignment in burst images caused by camera ego motion and slight alterations in the environment. Depending on the assumed motion of the scene, several alignment strategies are outlined, including perspective planar alignment (i.e., homography), optical flow with minimal scene change, and optical flow with significant occlusion and disocclusion. By integrating several input images into a single neural implicit function, this framework eliminates the need to choose one image as a reference frame. Various layer separation tasks can be accomplished using this multi-frame fusion framework, as demonstrated.",1
"A distinctive feature of Doppler radar is the measurement of velocity in the radial direction for radar points. However, the missing tangential velocity component hampers object velocity estimation as well as temporal integration of radar sweeps in dynamic scenes. Recognizing that fusing camera with radar provides complementary information to radar, in this paper we present a closed-form solution for the point-wise, full-velocity estimate of Doppler returns using the corresponding optical flow from camera images. Additionally, we address the association problem between radar returns and camera images with a neural network that is trained to estimate radar-camera correspondences. Experimental results on the nuScenes dataset verify the validity of the method and show significant improvements over the state-of-the-art in velocity estimation and accumulation of radar points.",0
"Doppler radar is known for accurately measuring velocity in the radial direction, but the absence of tangential velocity data can impede the estimation of object velocity and the integration of radar sweeps in dynamic environments. To address this issue, we propose a solution that combines the information provided by cameras and radar. By utilizing optical flow from camera images, we can obtain a comprehensive full-velocity estimate for Doppler returns. To overcome the challenge of associating radar returns with camera images, we introduce a neural network that is trained to estimate these correspondences. Our experimental results, using the nuScenes dataset, demonstrate the effectiveness of our approach. We achieve significant improvements in both velocity estimation and the accumulation of radar points compared to the current state-of-the-art methods.",1
"Existing video stabilization methods often generate visible distortion or require aggressive cropping of frame boundaries, resulting in smaller field of views. In this work, we present a frame synthesis algorithm to achieve full-frame video stabilization. We first estimate dense warp fields from neighboring frames and then synthesize the stabilized frame by fusing the warped contents. Our core technical novelty lies in the learning-based hybrid-space fusion that alleviates artifacts caused by optical flow inaccuracy and fast-moving objects. We validate the effectiveness of our method on the NUS, selfie, and DeepStab video datasets. Extensive experiment results demonstrate the merits of our approach over prior video stabilization methods.",0
"Current methods of video stabilization often result in visible distortions or require significant cropping, which reduces the field of view. In this study, we propose a frame synthesis algorithm that achieves full-frame video stabilization. Our approach involves obtaining dense warp fields from neighboring frames and then combining the warped contents to create a stabilized frame. The core innovation of our method lies in the hybrid-space fusion, which is based on machine learning and helps reduce artifacts caused by inaccuracies in optical flow and fast-moving objects. We verified the effectiveness of our approach on several video datasets, including NUS, selfie, and DeepStab, and found that it outperforms existing video stabilization methods.",1
"Fingerspelling in sign language has been the means of communicating technical terms and proper nouns when they do not have dedicated sign language gestures. Automatic recognition of fingerspelling can help resolve communication barriers when interacting with deaf people. The main challenges prevalent in fingerspelling recognition are the ambiguity in the gestures and strong articulation of the hands. The automatic recognition model should address high inter-class visual similarity and high intra-class variation in the gestures. Most of the existing research in fingerspelling recognition has focused on the dataset collected in a controlled environment. The recent collection of a large-scale annotated fingerspelling dataset in the wild, from social media and online platforms, captures the challenges in a real-world scenario. In this work, we propose a fine-grained visual attention mechanism using the Transformer model for the sequence-to-sequence prediction task in the wild dataset. The fine-grained attention is achieved by utilizing the change in motion of the video frames (optical flow) in sequential context-based attention along with a Transformer encoder model. The unsegmented continuous video dataset is jointly trained by balancing the Connectionist Temporal Classification (CTC) loss and the maximum-entropy loss. The proposed approach can capture better fine-grained attention in a single iteration. Experiment evaluations show that it outperforms the state-of-the-art approaches.",0
"Sign language relies on fingerspelling to convey technical terms and proper nouns lacking dedicated gestures. Automatic recognition of fingerspelling can bridge communication gaps with the deaf. However, challenges in recognizing fingerspelling include gesture ambiguity and strong hand articulation. Previous research has focused on controlled datasets, but a new annotated dataset from social media and online platforms depicts real-world challenges. To address these challenges, we propose a Transformer model with fine-grained visual attention using motion changes in video frames, trained jointly with Connectionist Temporal Classification and maximum-entropy losses. Our approach outperforms previous state-of-the-art methods, providing better fine-grained attention in a single iteration.",1
"High dynamic range (HDR) video reconstruction from sequences captured with alternating exposures is a very challenging problem. Existing methods often align low dynamic range (LDR) input sequence in the image space using optical flow, and then merge the aligned images to produce HDR output. However, accurate alignment and fusion in the image space are difficult due to the missing details in the over-exposed regions and noise in the under-exposed regions, resulting in unpleasing ghosting artifacts. To enable more accurate alignment and HDR fusion, we introduce a coarse-to-fine deep learning framework for HDR video reconstruction. Firstly, we perform coarse alignment and pixel blending in the image space to estimate the coarse HDR video. Secondly, we conduct more sophisticated alignment and temporal fusion in the feature space of the coarse HDR video to produce better reconstruction. Considering the fact that there is no publicly available dataset for quantitative and comprehensive evaluation of HDR video reconstruction methods, we collect such a benchmark dataset, which contains $97$ sequences of static scenes and 184 testing pairs of dynamic scenes. Extensive experiments show that our method outperforms previous state-of-the-art methods. Our dataset, code and model will be made publicly available.",0
"Reconstructing high dynamic range (HDR) video from sequences captured with varying exposures is a formidable challenge. Current techniques typically align low dynamic range (LDR) input sequences using optical flow in the image space, then merge the aligned images to generate HDR output. However, precise alignment and fusion in the image space are complicated because of missing details in over-exposed regions and noise in under-exposed regions, leading to unsightly ghosting artifacts. To improve alignment and HDR fusion accuracy, we propose a deep learning framework for HDR video reconstruction, which uses a coarse-to-fine approach. First, we perform coarse alignment and pixel blending in the image space to obtain the coarse HDR video. Next, we carry out more advanced alignment and temporal fusion in the feature space of the coarse HDR video to produce better reconstruction. As there is currently no publicly available dataset for comprehensive evaluation of HDR video reconstruction methods, we have assembled a benchmark dataset featuring 97 sequences of static scenes and 184 testing pairs of dynamic scenes. Our extensive experiments demonstrate that our method surpasses prior state-of-the-art techniques. We intend to publicly release our dataset, code and model.",1
"This paper presents a pure transformer-based approach, dubbed the Multi-Modal Video Transformer (MM-ViT), for video action recognition. Different from other schemes which solely utilize the decoded RGB frames, MM-ViT operates exclusively in the compressed video domain and exploits all readily available modalities, i.e., I-frames, motion vectors, residuals and audio waveform. In order to handle the large number of spatiotemporal tokens extracted from multiple modalities, we develop several scalable model variants which factorize self-attention across the space, time and modality dimensions. In addition, to further explore the rich inter-modal interactions and their effects, we develop and compare three distinct cross-modal attention mechanisms that can be seamlessly integrated into the transformer building block. Extensive experiments on three public action recognition benchmarks (UCF-101, Something-Something-v2, Kinetics-600) demonstrate that MM-ViT outperforms the state-of-the-art video transformers in both efficiency and accuracy, and performs better or equally well to the state-of-the-art CNN counterparts with computationally-heavy optical flow.",0
"The Multi-Modal Video Transformer (MM-ViT) is a transformer-based approach presented in this paper for video action recognition. Unlike other methods that only use decoded RGB frames, MM-ViT operates solely in the compressed video domain and utilizes all available modalities, including I-frames, motion vectors, residuals, and audio waveform. To handle the large number of spatiotemporal tokens from multiple modalities, scalable model variants are developed that factorize self-attention across space, time, and modality dimensions. Additionally, three different cross-modal attention mechanisms are developed and compared to explore inter-modal interactions and their effects. Extensive experiments on three public action recognition benchmarks demonstrate that MM-ViT is more efficient and accurate than state-of-the-art video transformers and performs better or equivalently to computationally-heavy optical flow-based CNN models.",1
"Warping-based video stabilizers smooth camera trajectory by constraining each pixel's displacement and warp stabilized frames from unstable ones accordingly. However, since the view outside the boundary is not available during warping, the resulting holes around the boundary of the stabilized frame must be discarded (i.e., cropping) to maintain visual consistency, and thus does leads to a tradeoff between stability and cropping ratio. In this paper, we make a first attempt to address this issue by proposing a new Out-of-boundary View Synthesis (OVS) method. By the nature of spatial coherence between adjacent frames and within each frame, OVS extrapolates the out-of-boundary view by aligning adjacent frames to each reference one. Technically, it first calculates the optical flow and propagates it to the outer boundary region according to the affinity, and then warps pixels accordingly. OVS can be integrated into existing warping-based stabilizers as a plug-and-play module to significantly improve the cropping ratio of the stabilized results. In addition, stability is improved because the jitter amplification effect caused by cropping and resizing is reduced. Experimental results on the NUS benchmark show that OVS can improve the performance of five representative state-of-the-art methods in terms of objective metrics and subjective visual quality. The code is publicly available at https://github.com/Annbless/OVS_Stabilization.",0
"Video stabilizers that rely on warping are effective in smoothing camera motion by limiting the displacement of each pixel and shifting unstable frames to a more stable position. Nonetheless, warping does not allow for a view of the outside boundary, resulting in gaps around the stabilized frame's edges that must be removed (cropped) to maintain visual consistency. This tradeoff between stability and cropping ratio is addressed in this paper with a novel Out-of-boundary View Synthesis (OVS) method. OVS leverages spatial coherence between adjacent frames and within each frame to extrapolate the out-of-boundary view by aligning adjacent frames with each reference one. The optical flow is calculated and propagated to the outer boundary region according to the affinity, while pixels are warped accordingly. OVS can be integrated into existing warping-based stabilizers as a plug-and-play module to significantly improve the cropping ratio of the stabilized results and reduce the jitter amplification effect caused by cropping and resizing. Experimental results on the NUS benchmark demonstrate that OVS can enhance the performance of five representative state-of-the-art methods in terms of objective metrics and subjective visual quality. The code is available at https://github.com/Annbless/OVS_Stabilization.",1
"Learning RAW-to-sRGB mapping has drawn increasing attention in recent years, wherein an input raw image is trained to imitate the target sRGB image captured by another camera. However, the severe color inconsistency makes it very challenging to generate well-aligned training pairs of input raw and target sRGB images. While learning with inaccurately aligned supervision is prone to causing pixel shift and producing blurry results. In this paper, we circumvent such issue by presenting a joint learning model for image alignment and RAW-to-sRGB mapping. To diminish the effect of color inconsistency in image alignment, we introduce to use a global color mapping (GCM) module to generate an initial sRGB image given the input raw image, which can keep the spatial location of the pixels unchanged, and the target sRGB image is utilized to guide GCM for converting the color towards it. Then a pre-trained optical flow estimation network (e.g., PWC-Net) is deployed to warp the target sRGB image to align with the GCM output. To alleviate the effect of inaccurately aligned supervision, the warped target sRGB image is leveraged to learn RAW-to-sRGB mapping. When training is done, the GCM module and optical flow network can be detached, thereby bringing no extra computation cost for inference. Experiments show that our method performs favorably against state-of-the-arts on ZRR and SR-RAW datasets. With our joint learning model, a light-weight backbone can achieve better quantitative and qualitative performance on ZRR dataset. Codes are available at https://github.com/cszhilu1998/RAW-to-sRGB.",0
"Recently, there has been a growing interest in learning how to map RAW-to-sRGB, which involves training an input raw image to mimic the target sRGB image captured by another camera. However, the challenge lies in the severe color inconsistency that makes it difficult to generate well-aligned training pairs of input raw and target sRGB images. Learning with inaccurately aligned supervision can result in pixel shift and blurry outputs. To address this issue, we propose a joint learning model for image alignment and RAW-to-sRGB mapping. We use a global color mapping (GCM) module to generate an initial sRGB image based on the input raw image, while the target sRGB image guides the GCM to convert the color towards it. A pre-trained optical flow estimation network (such as PWC-Net) is deployed to warp the target sRGB image to align with the GCM output. We leverage the warped target sRGB image to learn RAW-to-sRGB mapping and detach the GCM module and optical flow network after training. Our method performs favorably against state-of-the-art techniques on ZRR and SR-RAW datasets, and our joint learning model enables a light-weight backbone to achieve better quantitative and qualitative performance on the ZRR dataset. Our codes are available at https://github.com/cszhilu1998/RAW-to-sRGB.",1
"Existing optical flow methods are erroneous in challenging scenes, such as fog, rain, and night because the basic optical flow assumptions such as brightness and gradient constancy are broken. To address this problem, we present an unsupervised learning approach that fuses gyroscope into optical flow learning. Specifically, we first convert gyroscope readings into motion fields named gyro field. Second, we design a self-guided fusion module to fuse the background motion extracted from the gyro field with the optical flow and guide the network to focus on motion details. To the best of our knowledge, this is the first deep learning-based framework that fuses gyroscope data and image content for optical flow learning. To validate our method, we propose a new dataset that covers regular and challenging scenes. Experiments show that our method outperforms the state-of-art methods in both regular and challenging scenes. Code and dataset are available at https://github.com/megvii-research/GyroFlow.",0
"The conventional optical flow methods are flawed when dealing with difficult scenarios like fog, rain, and darkness due to the violation of basic assumptions such as gradient and brightness constancy. Our solution to this issue involves an unsupervised learning technique that incorporates gyroscope data into optical flow learning. Firstly, the gyroscope readings are transformed into motion fields called gyro fields. Secondly, we created a self-guided fusion module to merge the background motion from the gyro field with the optical flow and direct the network's focus towards motion details. This is the first deep learning-based framework that combines gyroscope data and image content for optical flow learning. We have also developed a new dataset consisting of both regular and challenging scenes to validate our technique. Our experiments have demonstrated that our method surpasses the current state-of-the-art methods in both types of scenes. The code and dataset are available for access at https://github.com/megvii-research/GyroFlow.",1
"We propose a novel framework for video inpainting by adopting an internal learning strategy. Unlike previous methods that use optical flow for cross-frame context propagation to inpaint unknown regions, we show that this can be achieved implicitly by fitting a convolutional neural network to known regions. Moreover, to handle challenging sequences with ambiguous backgrounds or long-term occlusion, we design two regularization terms to preserve high-frequency details and long-term temporal consistency. Extensive experiments on the DAVIS dataset demonstrate that the proposed method achieves state-of-the-art inpainting quality quantitatively and qualitatively. We further extend the proposed method to another challenging task: learning to remove an object from a video giving a single object mask in only one frame in a 4K video.",0
"Our innovative approach to video inpainting involves utilizing internal learning techniques. Rather than relying on optical flow for cross-frame context propagation to fill in unknown areas, we demonstrate that this can be accomplished implicitly through fitting a convolutional neural network to known regions. Additionally, we have developed two regularization terms to address difficult sequences with unclear backgrounds or long-term obstructions, which help to maintain high-frequency details and long-term temporal consistency. Through extensive experimentation on the DAVIS dataset, we have demonstrated that our method achieves superior inpainting quality both quantitatively and qualitatively. We have also expanded our approach to tackle another challenging task: removing an object from a 4K video using a single object mask in one frame.",1
"The existing particle image velocimetry (PIV) do not consider the curvature effect of the non-straight particle trajectory, because it seems to be impossible to obtain the curvature information from a pair of particle images. As a result, the computed vector underestimates the real velocity due to the straight-line approximation, that further causes a systematic error for the PIV instrument. In this work, the particle curved trajectory between two recordings is firstly explained with the streamline segment of a steady flow (diffeomorphic transformation) instead of a single vector, and this idea is termed as diffeomorphic PIV. Specifically, a deformation field is introduced to describe the particle displacement, i.e., we try to find the optimal velocity field, of which the corresponding deformation vector field agrees with the particle displacement. Because the variation of the deformation function can be approximated with the variation of the velocity function, the diffeomorphic PIV can be implemented as iterative PIV. That says, the diffeomorphic PIV warps the images with deformation vector field instead of the velocity, and keeps the rest as same as iterative PIVs. Two diffeomorphic deformation schemes -- forward diffeomorphic deformation interrogation (FDDI) and central diffeomorphic deformation interrogation (CDDI) -- are proposed. Tested on synthetic images, the FDDI achieves significant accuracy improvement across different one-pass displacement estimators (cross-correlation, optical flow, deep learning flow). Besides, the results on three real PIV image pairs demonstrate the non-negligible curvature effect for CDI-based PIV, and our FDDI provides larger velocity estimation (more accurate) in the fast curvy streamline areas. The accuracy improvement of the combination of FDDI and accurate dense estimator means that our diffeomorphic PIV paves a new way for complex flow measurement.",0
"The current particle image velocimetry (PIV) technique does not take into account the curvature effect of non-linear particle trajectories. This is because it is impossible to obtain curvature information from a pair of particle images, resulting in an underestimation of the computed vector and causing systematic errors in PIV instruments. This study introduces a new concept called diffeomorphic PIV, which explains the particle's curved trajectory between two recordings using a streamline segment of a steady flow. A deformation field is used to describe the particle displacement, finding the optimal velocity field whose corresponding deformation vector field agrees with the particle displacement. The diffeomorphic PIV can be implemented as iterative PIV, warping the images with a deformation vector field instead of the velocity. Two diffeomorphic deformation schemes, forward diffeomorphic deformation interrogation (FDDI) and central diffeomorphic deformation interrogation (CDDI), are proposed. The FDDI improves accuracy across various displacement estimators and demonstrates a non-negligible curvature effect on CDI-based PIV. FDDI provides a more accurate estimation of velocity in fast curvy streamline areas, paving a new way for complex flow measurement.",1
"We propose a novel framework for finding correspondences in images based on a deep neural network that, given two images and a query point in one of them, finds its correspondence in the other. By doing so, one has the option to query only the points of interest and retrieve sparse correspondences, or to query all points in an image and obtain dense mappings. Importantly, in order to capture both local and global priors, and to let our model relate between image regions using the most relevant among said priors, we realize our network using a transformer. At inference time, we apply our correspondence network by recursively zooming in around the estimates, yielding a multiscale pipeline able to provide highly-accurate correspondences. Our method significantly outperforms the state of the art on both sparse and dense correspondence problems on multiple datasets and tasks, ranging from wide-baseline stereo to optical flow, without any retraining for a specific dataset. We commit to releasing data, code, and all the tools necessary to train from scratch and ensure reproducibility.",0
"Our innovative approach proposes a framework that utilizes a deep neural network to identify corresponding features in images. Given two images and a query point in one of them, our system can locate its corresponding feature in the other image. Users can choose to query specific points of interest, resulting in sparse correspondences, or query all points in an image, yielding dense mappings. Our model incorporates both local and global priors and employs a transformer to relate relevant priors between image regions. During inference, we apply our correspondence network by recursively zooming in around the estimates, resulting in a multiscale approach that delivers highly accurate correspondences. Our method surpasses the current state of the art in sparse and dense correspondence problems across various datasets and tasks, ranging from wide-baseline stereo to optical flow, without requiring retraining for a specific dataset. We are committed to sharing our data, code, and tools necessary for training and ensuring reproducibility.",1
"In this paper, we propose $\text{HF}^2$-VAD, a Hybrid framework that integrates Flow reconstruction and Frame prediction seamlessly to handle Video Anomaly Detection. Firstly, we design the network of ML-MemAE-SC (Multi-Level Memory modules in an Autoencoder with Skip Connections) to memorize normal patterns for optical flow reconstruction so that abnormal events can be sensitively identified with larger flow reconstruction errors. More importantly, conditioned on the reconstructed flows, we then employ a Conditional Variational Autoencoder (CVAE), which captures the high correlation between video frame and optical flow, to predict the next frame given several previous frames. By CVAE, the quality of flow reconstruction essentially influences that of frame prediction. Therefore, poorly reconstructed optical flows of abnormal events further deteriorate the quality of the final predicted future frame, making the anomalies more detectable. Experimental results demonstrate the effectiveness of the proposed method. Code is available at \href{https://github.com/LiUzHiAn/hf2vad}{https://github.com/LiUzHiAn/hf2vad}.",0
"We introduce $\text{HF}^2$-VAD, a Hybrid framework for Video Anomaly Detection that seamlessly combines Flow reconstruction and Frame prediction. Our approach utilizes a network called ML-MemAE-SC (Multi-Level Memory modules in an Autoencoder with Skip Connections) to memorize normal patterns for optical flow reconstruction, making it possible to sensitively identify abnormal events with larger flow reconstruction errors. Additionally, we employ a Conditional Variational Autoencoder (CVAE) to predict the next frame given several previous frames, which is conditioned on the reconstructed flows. The quality of flow reconstruction significantly affects the quality of frame prediction, as poorly reconstructed optical flows of abnormal events further degrade the quality of predicted future frames, making the anomalies more easily detectable. Experimental results indicate the effectiveness of our proposed method. The code is available at \href{https://github.com/LiUzHiAn/hf2vad}{https://github.com/LiUzHiAn/hf2vad}.",1
"Event camera is an emerging imaging sensor for capturing dynamics of moving objects as events, which motivates our work in estimating 3D human pose and shape from the event signals. Events, on the other hand, have their unique challenges: rather than capturing static body postures, the event signals are best at capturing local motions. This leads us to propose a two-stage deep learning approach, called EventHPE. The first-stage, FlowNet, is trained by unsupervised learning to infer optical flow from events. Both events and optical flow are closely related to human body dynamics, which are fed as input to the ShapeNet in the second stage, to estimate 3D human shapes. To mitigate the discrepancy between image-based flow (optical flow) and shape-based flow (vertices movement of human body shape), a novel flow coherence loss is introduced by exploiting the fact that both flows are originated from the identical human motion. An in-house event-based 3D human dataset is curated that comes with 3D pose and shape annotations, which is by far the largest one to our knowledge. Empirical evaluations on DHP19 dataset and our in-house dataset demonstrate the effectiveness of our approach.",0
"Our work focuses on utilizing the event camera, a novel imaging sensor that captures the dynamics of moving objects as events, to estimate 3D human pose and shape. However, this presents unique challenges as event signals are ideal for capturing local motions, as opposed to static body postures. To address this, we propose a two-stage deep learning approach called EventHPE. The first stage involves training FlowNet through unsupervised learning to infer optical flow from events. Both optical flow and event signals are related to human body dynamics, which are used as input for the second stage, ShapeNet, to estimate 3D human shapes. To deal with discrepancies between image-based and shape-based flows, we introduce a novel flow coherence loss that exploits the fact that both flows originate from the same human motion. We curated a large in-house event-based 3D human dataset with pose and shape annotations, which is currently the largest of its kind. Our approach was empirically evaluated on the DHP19 dataset and our in-house dataset, and the results demonstrate its effectiveness.",1
"Weakly-Supervised Temporal Action Localization (WSTAL) aims to localize actions in untrimmed videos with only video-level labels. Currently, most state-of-the-art WSTAL methods follow a Multi-Instance Learning (MIL) pipeline: producing snippet-level predictions first and then aggregating to the video-level prediction. However, we argue that existing methods have overlooked two important drawbacks: 1) inadequate use of motion information and 2) the incompatibility of prevailing cross-entropy training loss. In this paper, we analyze that the motion cues behind the optical flow features are complementary informative. Inspired by this, we propose to build a context-dependent motion prior, termed as motionness. Specifically, a motion graph is introduced to model motionness based on the local motion carrier (e.g., optical flow). In addition, to highlight more informative video snippets, a motion-guided loss is proposed to modulate the network training conditioned on motionness scores. Extensive ablation studies confirm that motionness efficaciously models action-of-interest, and the motion-guided loss leads to more accurate results. Besides, our motion-guided loss is a plug-and-play loss function and is applicable with existing WSTAL methods. Without loss of generality, based on the standard MIL pipeline, our method achieves new state-of-the-art performance on three challenging benchmarks, including THUMOS'14, ActivityNet v1.2 and v1.3.",0
"The objective of Weakly-Supervised Temporal Action Localization (WSTAL) is to locate actions in untrimmed videos using only video-level labels. Currently, the majority of state-of-the-art WSTAL techniques employ a Multi-Instance Learning (MIL) strategy, where they first produce predictions at the snippet level and then aggregate them to arrive at a prediction at the video level. However, we posit that existing approaches have disregarded two significant drawbacks: 1) they do not make optimal use of motion information and 2) the prevailing cross-entropy training loss is incompatible. In this study, we analyze that optical flow features contain complementary motion cues that provide valuable information. Based on this insight, we propose a context-dependent motion prior known as motionness. To model motionness, we introduce a motion graph that relies on the local motion carrier (i.e., optical flow). Additionally, we present a motion-guided loss that highlights more informative video snippets and modulates network training based on motionness scores. Through extensive ablation studies, we demonstrate that motionness effectively models the action of interest, and the motion-guided loss results in more accurate outcomes. Furthermore, our motion-guided loss is a plug-and-play loss function that can be used with existing WSTAL methods. Using the standard MIL pipeline, our approach achieves new state-of-the-art results on three challenging benchmarks: THUMOS'14, ActivityNet v1.2, and v1.3, without any loss in generality.",1
"We propose RIFE, a Real-time Intermediate Flow Estimation algorithm for Video Frame Interpolation (VFI). Many recent flow-based VFI methods first estimate the bi-directional optical flows, then scale and reverse them to approximate intermediate flows, leading to artifacts on motion boundaries. RIFE uses a neural network named IFNet that can directly estimate the intermediate flows from coarse-to-fine with much better speed. We design a privileged distillation scheme for training intermediate flow model, which leads to a large performance improvement. Experiments demonstrate that RIFE is flexible and can achieve state-of-the-art performance on several public benchmarks. The code is available at \url{https://github.com/hzwer/arXiv2020-RIFE}",0
"Our proposal is RIFE, an algorithm for Video Frame Interpolation (VFI) called Real-time Intermediate Flow Estimation. Existing flow-based VFI methods estimate bi-directional optical flows, then approximate intermediate flows by scaling and reversing them, which can cause artifacts on motion boundaries. Instead, RIFE employs a neural network called IFNet that can directly estimate intermediate flows from coarse-to-fine with greater speed. We also incorporate a privileged distillation scheme to train the intermediate flow model that leads to a significant performance improvement. Our experiments demonstrate that RIFE is a flexible approach that achieves state-of-the-art results on several public benchmarks. The code is available at \url{https://github.com/hzwer/arXiv2020-RIFE}.",1
"Animals have evolved highly functional visual systems to understand motion, assisting perception even under complex environments. In this paper, we work towards developing a computer vision system able to segment objects by exploiting motion cues, i.e. motion segmentation. We make the following contributions: First, we introduce a simple variant of the Transformer to segment optical flow frames into primary objects and the background. Second, we train the architecture in a self-supervised manner, i.e. without using any manual annotations. Third, we analyze several critical components of our method and conduct thorough ablation studies to validate their necessity. Fourth, we evaluate the proposed architecture on public benchmarks (DAVIS2016, SegTrackv2, and FBMS59). Despite using only optical flow as input, our approach achieves superior or comparable results to previous state-of-the-art self-supervised methods, while being an order of magnitude faster. We additionally evaluate on a challenging camouflage dataset (MoCA), significantly outperforming the other self-supervised approaches, and comparing favourably to the top supervised approach, highlighting the importance of motion cues, and the potential bias towards visual appearance in existing video segmentation models.",0
"The visual systems of animals have developed to comprehend motion, aiding perception even in complex surroundings. This study's objective is to create a computer vision system that can segment objects through motion cues, called motion segmentation. Our contributions include introducing a straightforward variation of the Transformer that segments optical flow frames into primary objects and background, training the architecture in a self-supervised manner, analyzing critical components of our method, and evaluating the proposed architecture on public benchmarks. Despite using only optical flow as input, our approach achieves comparable or superior results to previous state-of-the-art self-supervised methods, and it is much faster. We also evaluate our approach on a challenging camouflage dataset, where it outperforms other self-supervised approaches and compares favorably to the top supervised approach, emphasizing the significance of motion cues and the potential bias towards visual appearance in existing video segmentation models.",1
"Location and appearance are the key cues for video object segmentation. Many sources such as RGB, depth, optical flow and static saliency can provide useful information about the objects. However, existing approaches only utilize the RGB or RGB and optical flow. In this paper, we propose a novel multi-source fusion network for zero-shot video object segmentation. With the help of interoceptive spatial attention module (ISAM), spatial importance of each source is highlighted. Furthermore, we design a feature purification module (FPM) to filter the inter-source incompatible features. By the ISAM and FPM, the multi-source features are effectively fused. In addition, we put forward an automatic predictor selection network (APS) to select the better prediction of either the static saliency predictor or the moving object predictor in order to prevent over-reliance on the failed results caused by low-quality optical flow maps. Extensive experiments on three challenging public benchmarks (i.e. DAVIS$_{16}$, Youtube-Objects and FBMS) show that the proposed model achieves compelling performance against the state-of-the-arts. The source code will be publicly available at \textcolor{red}{\url{https://github.com/Xiaoqi-Zhao-DLUT/Multi-Source-APS-ZVOS}}.",0
"Video object segmentation relies on location and appearance cues, which can be derived from various sources such as RGB, depth, optical flow, and static saliency. However, existing methods only utilize RGB or RGB and optical flow. To address this, we propose a novel multi-source fusion network for zero-shot video object segmentation. Our approach uses an interoceptive spatial attention module (ISAM) to highlight the spatial importance of each source and a feature purification module (FPM) to filter out inter-source incompatible features. By effectively fusing the multi-source features, our model achieves compelling performance on three challenging public benchmarks: DAVIS$_{16}$, Youtube-Objects, and FBMS. Additionally, we introduce an automatic predictor selection network (APS) to select the better prediction between the static saliency predictor and the moving object predictor, preventing over-reliance on low-quality optical flow maps. The source code for our approach will be publicly available at \textcolor{red}{\url{https://github.com/Xiaoqi-Zhao-DLUT/Multi-Source-APS-ZVOS}}.",1
"Self-supervised deep learning-based 3D scene understanding methods can overcome the difficulty of acquiring the densely labeled ground-truth and have made a lot of advances. However, occlusions and moving objects are still some of the major limitations. In this paper, we explore the learnable occlusion aware optical flow guided self-supervised depth and camera pose estimation by an adaptive cross weighted loss to address the above limitations. Firstly, we explore to train the learnable occlusion mask fused optical flow network by an occlusion-aware photometric loss with the temporally supplemental information and backward-forward consistency of adjacent views. And then, we design an adaptive cross-weighted loss between the depth-pose and optical flow loss of the geometric and photometric error to distinguish the moving objects which violate the static scene assumption. Our method shows promising results on KITTI, Make3D, and Cityscapes datasets under multiple tasks. We also show good generalization ability under a variety of challenging scenarios.",0
"The difficulty of obtaining densely labeled ground-truth has been overcome by self-supervised deep learning-based 3D scene understanding methods, which have made significant progress. However, occlusions and moving objects remain major limitations. Our paper proposes a solution to these limitations with a learnable occlusion-aware optical flow guided self-supervised depth and camera pose estimation method using an adaptive cross-weighted loss. We first train the learnable occlusion mask fused optical flow network with an occlusion-aware photometric loss, considering the temporally supplemental information and backward-forward consistency of adjacent views. We then design an adaptive cross-weighted loss to distinguish moving objects that violate the static scene assumption from the depth-pose and optical flow loss of the geometric and photometric error. Our approach demonstrates promising results across multiple tasks on KITTI, Make3D, and Cityscapes datasets, and it performs well under various challenging scenarios, indicating good generalization ability.",1
"This paper strives for action recognition and detection in video modalities like RGB, depth maps or 3D-skeleton sequences when only limited modality-specific labeled examples are available. For the RGB, and derived optical-flow, modality many large-scale labeled datasets have been made available. They have become the de facto pre-training choice when recognizing or detecting new actions from RGB datasets that have limited amounts of labeled examples available. Unfortunately, large-scale labeled action datasets for other modalities are unavailable for pre-training. In this paper, our goal is to recognize actions from limited examples in non-RGB video modalities, by learning from large-scale labeled RGB data. To this end, we propose a two-step training process: (i) we extract action representation knowledge from an RGB-trained teacher network and adapt it to a non-RGB student network. (ii) we then fine-tune the transfer model with available labeled examples of the target modality. For the knowledge transfer we introduce feature-supervision strategies, which rely on unlabeled pairs of two modalities (the RGB and the target modality) to transfer feature level representations from the teacher to the student network. Ablations and generalizations with two RGB source datasets and two non-RGB target datasets demonstrate that an optical-flow teacher provides better action transfer features than RGB for both depth maps and 3D-skeletons, even when evaluated on a different target domain, or for a different task. Compared to alternative cross-modal action transfer methods we show a good improvement in performance especially when labeled non-RGB examples to learn from are scarce",0
"The objective of this study is to detect and recognize actions in video modalities such as RGB, depth maps, or 3D-skeleton sequences, with limited modality-specific labeled examples. While large-scale labeled datasets are available for RGB and derived optical-flow modalities, pre-training datasets for other modalities are unavailable. Therefore, the goal of this paper is to recognize actions in non-RGB video modalities by learning from large-scale labeled RGB data. The proposed two-step training process involves extracting action representation knowledge from an RGB-trained teacher network and adapting it to a non-RGB student network, followed by fine-tuning the transfer model with the available labeled examples of the target modality. Feature-supervision strategies are introduced for knowledge transfer, relying on unlabeled pairs of two modalities to transfer feature level representations from the teacher to the student network. The study demonstrates that an optical-flow teacher provides better action transfer features than RGB for both depth maps and 3D-skeletons, even when evaluated on a different target domain or for a different task. Compared to alternative cross-modal action transfer methods, this study shows a significant improvement in performance, especially when labeled non-RGB examples are scarce.",1
"Estimating the states of surrounding traffic participants stays at the core of autonomous driving. In this paper, we study a novel setting of this problem: model-free single-object tracking (SOT), which takes the object state in the first frame as input, and jointly solves state estimation and tracking in subsequent frames. The main purpose for this new setting is to break the strong limitation of the popular ""detection and tracking"" scheme in multi-object tracking. Moreover, we notice that shape completion by overlaying the point clouds, which is a by-product of our proposed task, not only improves the performance of state estimation but also has numerous applications. As no benchmark for this task is available so far, we construct a new dataset LiDAR-SOT and corresponding evaluation protocols based on the Waymo Open dataset. We then propose an optimization-based algorithm called SOTracker involving point cloud registration, vehicle shapes, correspondence, and motion priors. Our quantitative and qualitative results prove the effectiveness of our SOTracker and reveal the challenging cases for SOT in point clouds, including the sparsity of LiDAR data, abrupt motion variation, etc. Finally, we also explore how the proposed task and algorithm may benefit other autonomous driving applications, including simulating LiDAR scans, generating motion data, and annotating optical flow. The code and protocols for our benchmark and algorithm are available at https://github.com/TuSimple/LiDAR_SOT/. A video demonstration is at https://www.youtube.com/watch?v=BpHixKs91i8.",0
"The core of autonomous driving involves estimating the states of surrounding traffic participants. In this paper, we examine a new approach to this problem: model-free single-object tracking (SOT), which uses the object state in the first frame as input and solves state estimation and tracking in subsequent frames. Our aim with this new approach is to overcome the limitations of the popular ""detection and tracking"" scheme in multi-object tracking. Furthermore, we have discovered that shape completion by overlaying the point clouds, which is a by-product of our proposed task, not only enhances state estimation performance but also has numerous applications. As there is currently no benchmark available for this task, we have created a new dataset called LiDAR-SOT and corresponding evaluation protocols based on the Waymo Open dataset. Our optimization-based algorithm, SOTracker, involves point cloud registration, vehicle shapes, correspondence, and motion priors. Our results demonstrate the effectiveness of our SOTracker and highlight the challenging cases for SOT in point clouds, including the sparsity of LiDAR data and abrupt motion variation. Finally, we explore how the proposed task and algorithm can benefit other autonomous driving applications, such as simulating LiDAR scans, generating motion data, and annotating optical flow. Our benchmark and algorithm protocols are available at https://github.com/TuSimple/LiDAR_SOT/, and a video demonstration can be found at https://www.youtube.com/watch?v=BpHixKs91i8.",1
"In this paper, we firstly present a dataset (X4K1000FPS) of 4K videos of 1000 fps with the extreme motion to the research community for video frame interpolation (VFI), and propose an extreme VFI network, called XVFI-Net, that first handles the VFI for 4K videos with large motion. The XVFI-Net is based on a recursive multi-scale shared structure that consists of two cascaded modules for bidirectional optical flow learning between two input frames (BiOF-I) and for bidirectional optical flow learning from target to input frames (BiOF-T). The optical flows are stably approximated by a complementary flow reversal (CFR) proposed in BiOF-T module. During inference, the BiOF-I module can start at any scale of input while the BiOF-T module only operates at the original input scale so that the inference can be accelerated while maintaining highly accurate VFI performance. Extensive experimental results show that our XVFI-Net can successfully capture the essential information of objects with extremely large motions and complex textures while the state-of-the-art methods exhibit poor performance. Furthermore, our XVFI-Net framework also performs comparably on the previous lower resolution benchmark dataset, which shows a robustness of our algorithm as well. All source codes, pre-trained models, and proposed X4K1000FPS datasets are publicly available at https://github.com/JihyongOh/XVFI.",0
"The focus of this paper is the presentation of the X4K1000FPS dataset, which features 4K videos with extreme motion and a frame rate of 1000 fps. This dataset is intended for use in video frame interpolation (VFI) research. Additionally, we introduce the XVFI-Net, an extreme VFI network that can handle the VFI for 4K videos with large motion. The XVFI-Net is based on a recursive multi-scale shared structure with two cascaded modules for bidirectional optical flow learning. The BiOF-I module can start at any input scale, while the BiOF-T module only operates at the original input scale to accelerate inference while maintaining high VFI accuracy. Our experimental results demonstrate that our XVFI-Net can effectively capture the essential information of objects with extremely large motions and complex textures, outperforming state-of-the-art methods. Our algorithm is also robust, performing comparably on a previous lower resolution benchmark dataset. All source codes, pre-trained models, and proposed X4K1000FPS datasets are publicly available at https://github.com/JihyongOh/XVFI.",1
"Modern methods for counting people in crowded scenes rely on deep networks to estimate people densities in individual images. As such, only very few take advantage of temporal consistency in video sequences, and those that do only impose weak smoothness constraints across consecutive frames. In this paper, we advocate estimating people flows across image locations between consecutive images and inferring the people densities from these flows instead of directly regressing them. This enables us to impose much stronger constraints encoding the conservation of the number of people. As a result, it significantly boosts performance without requiring a more complex architecture. Furthermore, it allows us to exploit the correlation between people flow and optical flow to further improve the results. We also show that leveraging people conservation constraints in both a spatial and temporal manner makes it possible to train a deep crowd counting model in an active learning setting with much fewer annotations. This significantly reduces the annotation cost while still leading to similar performance to the full supervision case.",0
"Current techniques for counting individuals in crowded scenes utilize deep networks to estimate the density of people in each image. However, very few of these methods utilize the temporal consistency of video sequences, and the ones that do only impose weak smoothness constraints between consecutive frames. In this study, we propose a method of estimating people flows between images and inferring their densities from these flows instead of direct regression. This enables us to impose stronger constraints that preserve the number of people, resulting in a significant improvement in performance without the need for a more complex architecture. Additionally, we can use the correlation between people flow and optical flow to further enhance the results. By enforcing conservation constraints in both spatial and temporal dimensions, we can train a deep crowd counting model in an active learning setting with fewer annotations, significantly reducing the annotation cost while maintaining similar performance to fully supervised methods.",1
"The recently-proposed Perceiver model obtains good results on several domains (images, audio, multimodal, point clouds) while scaling linearly in compute and memory with the input size. While the Perceiver supports many kinds of inputs, it can only produce very simple outputs such as class scores. Perceiver IO overcomes this limitation without sacrificing the original's appealing properties by learning to flexibly query the model's latent space to produce outputs of arbitrary size and semantics. Perceiver IO still decouples model depth from data size and still scales linearly with data size, but now with respect to both input and output sizes. The full Perceiver IO model achieves strong results on tasks with highly structured output spaces, such as natural language and visual understanding, StarCraft II, and multi-task and multi-modal domains. As highlights, Perceiver IO matches a Transformer-based BERT baseline on the GLUE language benchmark without the need for input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation.",0
"The Perceiver model, which has been recently proposed, performs well on various domains, including images, audio, multimodal, and point clouds, and its scalability with input size is linear in compute and memory. However, the model can only generate simple outputs like class scores, which limits its functionality. Perceiver IO addresses this limitation by allowing flexible querying of the model's latent space to produce outputs of arbitrary size and semantics, while maintaining the original model's desirable properties. Perceiver IO still maintains its linear scaling with data size, but now with respect to both input and output sizes, while decoupling model depth from data size. The complete Perceiver IO model achieves impressive results on tasks with highly structured output spaces, such as natural language and visual understanding, StarCraft II, and multi-task and multi-modal domains. Notably, Perceiver IO performs as well as a Transformer-based BERT baseline on the GLUE language benchmark without the need for input tokenization and sets a new benchmark for Sintel optical flow estimation.",1
"In Computer Vision,object tracking is a very old and complex problem.Though there are several existing algorithms for object tracking, still there are several challenges remain to be solved. For instance, variation of illumination of light, noise, occlusion, sudden start and stop of moving object, shading etc,make the object tracking a complex problem not only for dynamic background but also for static background. In this paper we propose a dual approach for object tracking based on optical flow and swarm Intelligence.The optical flow based KLT(Kanade-Lucas-Tomasi) tracker, tracks the dominant points of the target object from first frame to last frame of a video sequence;whereas swarm Intelligence based PSO (Particle Swarm Optimization) tracker simultaneously tracks the boundary information of the target object from second frame to last frame of the same video sequence.This dual function of tracking makes the trackers very much robust with respect to the above stated problems. The flexibility of our approach is that it can be successfully applicable in variable background as well as static background.We compare the performance of the proposed dual tracking algorithm with several benchmark datasets and obtain very competitive results in general and in most of the cases we obtained superior results using dual tracking algorithm. We also compare the performance of the proposed dual tracker with some existing PSO based algorithms for tracking and achieved better results.",0
"The problem of object tracking in Computer Vision is both old and complex, with multiple existing algorithms available. However, numerous challenges remain to be addressed, such as lighting variations, noise, occlusion, and sudden changes in motion. This complexity applies to both dynamic and static backgrounds, making object tracking a difficult task. To tackle this issue, our paper proposes a dual approach utilizing optical flow and swarm intelligence. The KLT tracker based on optical flow identifies the target object's dominant points from the first frame to the last frame of a video sequence. Meanwhile, the PSO tracker based on swarm intelligence simultaneously tracks the object's boundary information from the second frame to the last frame of the same video sequence. This dual function enhances the trackers' robustness and makes them suitable for both static and variable backgrounds. We compared our proposed dual tracking algorithm's performance with various benchmark datasets and obtained competitive results, with the majority of cases showing superior performance. Additionally, we compared our dual tracker's performance with existing PSO-based algorithms and achieved better results.",1
"Automatic facial action unit (AU) recognition is a challenging task due to the scarcity of manual annotations. To alleviate this problem, a large amount of efforts has been dedicated to exploiting various methods which leverage numerous unlabeled data. However, many aspects with regard to some unique properties of AUs, such as the regional and relational characteristics, are not sufficiently explored in previous works. Motivated by this, we take the AU properties into consideration and propose two auxiliary AU related tasks to bridge the gap between limited annotations and the model performance in a self-supervised manner via the unlabeled data. Specifically, to enhance the discrimination of regional features with AU relation embedding, we design a task of RoI inpainting to recover the randomly cropped AU patches. Meanwhile, a single image based optical flow estimation task is proposed to leverage the dynamic change of facial muscles and encode the motion information into the global feature representation. Based on these two self-supervised auxiliary tasks, local features, mutual relation and motion cues of AUs are better captured in the backbone network with the proposed regional and temporal based auxiliary task learning (RTATL) framework. Extensive experiments on BP4D and DISFA demonstrate the superiority of our method and new state-of-the-art performances are achieved.",0
"Due to the limited availability of manual annotations, automatic recognition of facial action units (AU) is a difficult task. To address this issue, many methods have been developed to utilize unlabeled data. However, previous works have not adequately explored certain unique properties of AUs, including regional and relational characteristics. To address this gap, we propose two self-supervised auxiliary tasks that consider AU properties. The first task involves RoI inpainting to enhance the discrimination of regional features with AU relation embedding. The second task involves optical flow estimation to capture the dynamic change of facial muscles and encode motion information into the global feature representation. Our proposed regional and temporal based auxiliary task learning (RTATL) framework better captures local features, mutual relation, and motion cues of AUs in the backbone network. Our method outperforms previous works on BP4D and DISFA datasets, demonstrating its superiority.",1
"Occlusions pose a significant challenge to optical flow algorithms that rely on local evidences. We consider an occluded point to be one that is imaged in the first frame but not in the next, a slight overloading of the standard definition since it also includes points that move out-of-frame. Estimating the motion of these points is extremely difficult, particularly in the two-frame setting. Previous work relies on CNNs to learn occlusions, without much success, or requires multiple frames to reason about occlusions using temporal smoothness. In this paper, we argue that the occlusion problem can be better solved in the two-frame case by modelling image self-similarities. We introduce a global motion aggregation module, a transformer-based approach to find long-range dependencies between pixels in the first image, and perform global aggregation on the corresponding motion features. We demonstrate that the optical flow estimates in the occluded regions can be significantly improved without damaging the performance in non-occluded regions. This approach obtains new state-of-the-art results on the challenging Sintel dataset, improving the average end-point error by 13.6% on Sintel Final and 13.7% on Sintel Clean. At the time of submission, our method ranks first on these benchmarks among all published and unpublished approaches. Code is available at https://github.com/zacjiang/GMA",0
"Optical flow algorithms that rely on local evidences face significant challenges when dealing with occlusions. In this context, an occluded point refers to an image point present in the first frame but not in the subsequent frame, including points that move out-of-frame. Estimating the motion of such points is challenging, especially when only two frames are available. Previous approaches to this problem have used CNNs to learn occlusions or required multiple frames to reason about occlusions using temporal smoothness, with limited success. In this paper, we propose a novel approach to the occlusion problem in the two-frame case by using image self-similarities. We introduce a global motion aggregation module based on transformers, which enables the identification of long-range dependencies between pixels in the first image and performs global aggregation on the corresponding motion features. Our method significantly improves the optical flow estimates in occluded regions without affecting the performance in non-occluded regions. Our approach achieves state-of-the-art results on the challenging Sintel dataset, improving the average end-point error by 13.6% on Sintel Final and 13.7% on Sintel Clean. Our method is currently the top-performing approach on these benchmarks among all published and unpublished approaches. The code for our approach is available at https://github.com/zacjiang/GMA.",1
"While single image shadow detection has been improving rapidly in recent years, video shadow detection remains a challenging task due to data scarcity and the difficulty in modelling temporal consistency. The current video shadow detection method achieves this goal via co-attention, which mostly exploits information that is temporally coherent but is not robust in detecting moving shadows and small shadow regions. In this paper, we propose a simple but powerful method to better aggregate information temporally. We use an optical flow based warping module to align and then combine features between frames. We apply this warping module across multiple deep-network layers to retrieve information from neighboring frames including both local details and high-level semantic information. We train and test our framework on the ViSha dataset. Experimental results show that our model outperforms the state-of-the-art video shadow detection method by 28%, reducing BER from 16.7 to 12.0.",0
"Despite recent advancements in single image shadow detection, detecting shadows in videos remains a challenging task due to limited data and the complexity of maintaining temporal consistency. The current video shadow detection method uses co-attention to exploit consistent information, but it struggles to accurately detect moving and small shadow regions. This paper proposes a straightforward yet effective approach to improve temporal information aggregation. By using an optical flow-based warping module, we align and combine features from neighboring frames, including local details and high-level semantic information. Our framework is tested on the ViSha dataset, and our experimental results demonstrate a 28% improvement over the state-of-the-art video shadow detection method, reducing BER from 16.7 to 12.0.",1
"The dominant paradigm in spatiotemporal action detection is to classify actions using spatiotemporal features learned by 2D or 3D Convolutional Networks. We argue that several actions are characterized by their context, such as relevant objects and actors present in the video. To this end, we introduce an architecture based on self-attention and Graph Convolutional Networks in order to model contextual cues, such as actor-actor and actor-object interactions, to improve human action detection in video. We are interested in achieving this in a weakly-supervised setting, i.e. using as less annotations as possible in terms of action bounding boxes. Our model aids explainability by visualizing the learned context as an attention map, even for actions and objects unseen during training. We evaluate how well our model highlights the relevant context by introducing a quantitative metric based on recall of objects retrieved by attention maps. Our model relies on a 3D convolutional RGB stream, and does not require expensive optical flow computation. We evaluate our models on the DALY dataset, which consists of human-object interaction actions. Experimental results show that our contextualized approach outperforms a baseline action detection approach by more than 2 points in Video-mAP. Code is available at \url{https://github.com/micts/acgcn}",0
"In the realm of spatiotemporal action detection, the prevailing approach involves utilizing spatiotemporal features acquired through 2D or 3D Convolutional Networks to classify actions. However, we contend that certain actions are better defined by their surrounding context, such as the presence of relevant objects and actors in the video. Thus, we have developed a self-attention and Graph Convolutional Network-based architecture to model contextual cues like actor-actor and actor-object interactions, with the goal of enhancing human action detection in video. Our primary objective is to achieve this in a weakly-supervised setup, where action bounding boxes are annotated to the least extent possible. Furthermore, our model facilitates interpretability by producing an attention map that visualizes the learned context, even for actions and objects that were not part of the training data. To gauge the effectiveness of our model in highlighting relevant context, we have introduced a quantitative metric based on recall of objects retrieved by attention maps. Our model solely depends on a 3D convolutional RGB stream, and does not necessitate expensive optical flow computation. We have evaluated our models using the DALY dataset, which comprises human-object interaction actions. Experimental results reveal that our contextualized approach outperforms a baseline action detection approach by more than 2 points in Video-mAP. To access the code, please visit \url{https://github.com/micts/acgcn}.",1
"We present a learning-based approach for removing unwanted obstructions, such as window reflections, fence occlusions, or adherent raindrops, from a short sequence of images captured by a moving camera. Our method leverages motion differences between the background and obstructing elements to recover both layers. Specifically, we alternate between estimating dense optical flow fields of the two layers and reconstructing each layer from the flow-warped images via a deep convolutional neural network. This learning-based layer reconstruction module facilitates accommodating potential errors in the flow estimation and brittle assumptions, such as brightness consistency. We show that the proposed approach learned from synthetically generated data performs well to real images. Experimental results on numerous challenging scenarios of reflection and fence removal demonstrate the effectiveness of the proposed method.",0
"A method for removing unwanted obstructions, like reflections, occlusions, or raindrops, from a short sequence of images taken by a moving camera is presented. The approach utilizes the differences in motion between the obstructing elements and the background to recover both layers. The process involves estimating dense optical flow fields of the two layers and reconstructing each layer from the flow-warped images through a deep convolutional neural network. This learning-based layer reconstruction function allows for the accommodation of errors in flow estimation and assumptions that are prone to errors, such as brightness consistency. The proposed approach, trained on synthetic data, performs well on real images. The effectiveness of the method is demonstrated through experimental results on several challenging scenarios involving reflection and fence removal.",1
"Highly complex deep learning models are increasingly integrated into modern cyber-physical systems (CPS), many of which have strict safety requirements. One problem arising from this is that deep learning lacks interpretability, operating as a black box. The reliability of deep learning is heavily impacted by how well the model training data represents runtime test data, especially when the input space dimension is high as natural images. In response, we propose a robust out-of-distribution (OOD) detection framework. Our approach detects unusual movements from driving video in real-time by combining classical optic flow operation with representation learning via variational autoencoder (VAE). We also design a method to locate OOD factors in images. Evaluation on a driving simulation data set shows that our approach is statistically more robust than related works.",0
"Modern cyber-physical systems (CPS) increasingly incorporate highly complex deep learning models, which can be problematic due to their lack of interpretability. This is particularly concerning given the strict safety requirements of many CPS. The reliability of deep learning models is heavily dependent on the accuracy of the model training data and its ability to represent runtime test data, especially when the input space dimension is high, such as with natural images. To address this issue, we propose a robust out-of-distribution (OOD) detection framework to detect unusual movements from driving video in real-time. Our approach combines classical optic flow operation with representation learning via variational autoencoder (VAE) and includes a method for locating OOD factors in images. Our evaluation on a driving simulation data set demonstrates that our approach is significantly more robust than related works.",1
"Feature pyramids and iterative refinement have recently led to great progress in optical flow estimation. However, downsampling in feature pyramids can cause blending of foreground objects with the background, which will mislead subsequent decisions in the iterative processing. The results are missing details especially in the flow of thin and of small structures. We propose a novel Residual Feature Pyramid Module (RFPM) which retains important details in the feature map without changing the overall iterative refinement design of the optical flow estimation. RFPM incorporates a residual structure between multiple feature pyramids into a downsampling module that corrects the blending of objects across boundaries. We demonstrate how to integrate our module with two state-of-the-art iterative refinement architectures. Results show that our RFPM visibly reduces flow errors and improves state-of-art performance in the clean pass of Sintel, and is one of the top-performing methods in KITTI. According to the particular modular structure of RFPM, we introduce a special transfer learning approach that can dramatically decrease the training time compared to a typical full optical flow training schedule on multiple datasets.",0
"Optical flow estimation has seen significant advancements recently due to the implementation of feature pyramids and iterative refinement. However, downsampling in feature pyramids can lead to a loss of details, particularly in thin and small structures, due to the blending of foreground objects with the background. This can mislead subsequent decisions during iterative processing. To address this issue, we introduce the Residual Feature Pyramid Module (RFPM), which preserves important feature map details while maintaining the iterative refinement design of optical flow estimation. RFPM utilizes a residual structure between multiple feature pyramids in a downsampling module to correct object blending across boundaries. We demonstrate the integration of our module with two state-of-the-art iterative refinement architectures, resulting in visibly reduced flow errors and improved performance in the clean pass of Sintel and KITTI. Our modular RFPM structure allows for a special transfer learning approach that significantly decreases training time compared to a typical full optical flow training schedule on multiple datasets.",1
"Learning deformable 3D objects from 2D images is an extremely ill-posed problem. Existing methods rely on explicit supervision to establish multi-view correspondences, such as template shape models and keypoint annotations, which restricts their applicability on objects ""in the wild"". In this paper, we propose to use monocular videos, which naturally provide correspondences across time, allowing us to learn 3D shapes of deformable object categories without explicit keypoints or template shapes. Specifically, we present DOVE, which learns to predict 3D canonical shape, deformation, viewpoint and texture from a single 2D image of a bird, given a bird video collection as well as automatically obtained silhouettes and optical flows as training data. Our method reconstructs temporally consistent 3D shape and deformation, which allows us to animate and re-render the bird from arbitrary viewpoints from a single image.",0
"The challenge of learning deformable 3D objects from 2D images is significant due to its poor definition. The current approaches require explicit supervision to establish multi-view correspondences, like template shape models and keypoint annotations, which limits their use on objects in their natural state. To address this issue, this study proposes using monocular videos that provide correspondences across time to learn 3D shapes of flexible object categories without explicit keypoints or template shapes. The proposed method, called DOVE, predicts 3D canonical shape, deformation, viewpoint, and texture from a single 2D image of a bird, utilizing a bird video collection and automatically obtained silhouettes and optical flows as training data. Our method produces temporally consistent 3D shape and deformation, enabling us to animate and re-render the bird from any viewpoint using a single image.",1
"Deep Learning-based 2D/3D registration methods are highly robust but often lack the necessary registration accuracy for clinical application. A refinement step using the classical optimization-based 2D/3D registration method applied in combination with Deep Learning-based techniques can provide the required accuracy. However, it also increases the runtime. In this work, we propose a novel Deep Learning driven 2D/3D registration framework that can be used end-to-end for iterative registration tasks without relying on any further refinement step. We accomplish this by learning the update step of the 2D/3D registration framework using Point-to-Plane Correspondences. The update step is learned using iterative residual refinement-based optical flow estimation, in combination with the Point-to-Plane correspondence solver embedded as a known operator. Our proposed method achieves an average runtime of around 8s, a mean re-projection distance error of 0.60 $\pm$ 0.40 mm with a success ratio of 97 percent and a capture range of 60 mm. The combination of high registration accuracy, high robustness, and fast runtime makes our solution ideal for clinical applications.",0
"Although Deep Learning-based 2D/3D registration methods are typically very robust, they often lack the necessary accuracy for clinical use. To address this issue, a refinement step utilizing classical optimization-based 2D/3D registration methods in combination with Deep Learning techniques can be added to increase the accuracy, but this also increases the runtime. This paper proposes a new Deep Learning-driven 2D/3D registration framework that can be used end-to-end for iterative registration tasks without requiring additional refinement steps. The proposed method learns the update step of the 2D/3D registration framework using Point-to-Plane Correspondences and iterative residual refinement-based optical flow estimation. The Point-to-Plane correspondence solver is embedded as a known operator. The proposed method achieves an average runtime of 8 seconds, a mean re-projection distance error of 0.60 $\pm$ 0.40 mm, a success ratio of 97 percent, and a capture range of 60 mm. This approach combines high registration accuracy, high robustness, and fast runtime, making it ideal for clinical applications.",1
"Research on group activity recognition mostly leans on the standard two-stream approach (RGB and Optical Flow) as their input features. Few have explored explicit pose information, with none using it directly to reason about the persons interactions. In this paper, we leverage the skeleton information to learn the interactions between the individuals straight from it. With our proposed method GIRN, multiple relationship types are inferred from independent modules, that describe the relations between the body joints pair-by-pair. Additionally to the joints relations, we also experiment with the previously unexplored relationship between individuals and relevant objects (e.g. volleyball). The individuals distinct relations are then merged through an attention mechanism, that gives more importance to those individuals more relevant for distinguishing the group activity. We evaluate our method in the Volleyball dataset, obtaining competitive results to the state-of-the-art. Our experiments demonstrate the potential of skeleton-based approaches for modeling multi-person interactions.",0
"Group activity recognition research often relies on the standard two-stream approach, utilizing RGB and Optical Flow as input features. However, few studies have explored explicit pose information and none have used it directly to reason about individuals' interactions. This paper proposes a method called GIRN that leverages skeleton information to learn interactions between individuals. GIRN uses multiple independent modules to infer various relationship types between body joint pairs. Additionally, the method experiments with relationships between individuals and relevant objects, such as a volleyball. An attention mechanism merges distinct relationships to give more importance to individuals relevant for distinguishing group activity. The proposed method is evaluated using the Volleyball dataset and achieves competitive results compared to the state-of-the-art. The experiments demonstrate the potential of skeleton-based approaches for modeling multi-person interactions.",1
"This presentation introduces a self-supervised learning approach to the synthesis of new video clips from old ones, with several new key elements for improved spatial resolution and realism: It conditions the synthesis process on contextual information for temporal continuity and ancillary information for fine control. The prediction model is doubly autoregressive, in the latent space of an autoencoder for forecasting, and in image space for updating contextual information, which is also used to enforce spatio-temporal consistency through a learnable optical flow module. Adversarial training of the autoencoder in the appearance and temporal domains is used to further improve the realism of its output. A quantizer inserted between the encoder and the transformer in charge of forecasting future frames in latent space (and its inverse inserted between the transformer and the decoder) adds even more flexibility by affording simple mechanisms for handling multimodal ancillary information for controlling the synthesis process (eg, a few sample frames, an audio track, a trajectory in image space) and taking into account the intrinsically uncertain nature of the future by allowing multiple predictions. Experiments with an implementation of the proposed approach give very good qualitative and quantitative results on multiple tasks and standard benchmarks.",0
"In this presentation, a new approach to self-supervised learning for synthesizing new video clips from old ones is introduced. The method includes several new features that improve spatial resolution and realism. The process is conditioned on contextual information for temporal continuity and ancillary information for fine control. The prediction model is doubly autoregressive, using an autoencoder for forecasting in the latent space and image space for updating contextual information. In addition, a learnable optical flow module enforces spatio-temporal consistency. Adversarial training is used to improve the realism of the output, and a quantizer between the encoder and transformer allows for handling multimodal ancillary information and multiple predictions. Implementation of this approach yielded excellent qualitative and quantitative results on multiple tasks and standard benchmarks.",1
"To solve the issue of video dehazing, there are two main tasks to attain: how to align adjacent frames to the reference frame; how to restore the reference frame. Some papers adopt explicit approaches (e.g., the Markov random field, optical flow, deformable convolution, 3D convolution) to align neighboring frames with the reference frame in feature space or image space, they then use various restoration methods to achieve the final dehazing results. In this paper, we propose a progressive alignment and restoration method for video dehazing. The alignment process aligns consecutive neighboring frames stage by stage without using the optical flow estimation. The restoration process is not only implemented under the alignment process but also uses a refinement network to improve the dehazing performance of the whole network. The proposed networks include four fusion networks and one refinement network. To decrease the parameters of networks, three fusion networks in the first fusion stage share the same parameters. Extensive experiments demonstrate that the proposed video dehazing method achieves outstanding performance against the-state-of-art methods.",0
"There are two main tasks involved in addressing the issue of video dehazing. The first is aligning adjacent frames with the reference frame, while the second is restoring the reference frame. Some research papers have used explicit approaches such as optical flow, deformable convolution, 3D convolution, and Markov random field to align neighboring frames and then applied various restoration methods to achieve the final dehazing results. In this study, we propose a progressive alignment and restoration method for video dehazing. The alignment process stages consecutive neighboring frames without relying on optical flow estimation, while the restoration process is implemented under the alignment process and uses a refinement network to enhance the dehazing performance of the whole network. The proposed network comprises four fusion networks and one refinement network, with three fusion networks in the first fusion stage sharing the same parameters to reduce network parameters. The experimental results demonstrate that our proposed video dehazing method outperforms state-of-the-art methods.",1
"We propose a method for multi-object tracking and segmentation (MOTS) that does not require fine-tuning or per benchmark hyperparameter selection. The proposed method addresses particularly the data association problem. Indeed, the recently introduced HOTA metric, that has a better alignment with the human visual assessment by evenly balancing detections and associations quality, has shown that improvements are still needed for data association. After creating tracklets using instance segmentation and optical flow, the proposed method relies on a space-time memory network (STM) developed for one-shot video object segmentation to improve the association of tracklets with temporal gaps. To the best of our knowledge, our method, named MeNToS, is the first to use the STM network to track object masks for MOTS. We took the 4th place in the RobMOTS challenge. The project page is https://mehdimiah.com/mentos.html.",0
"Our proposed method for multi-object tracking and segmentation (MOTS) eliminates the need for fine-tuning or selecting per benchmark hyperparameters. Specifically, our method focuses on solving the data association problem, which previous metrics such as HOTA have shown still requires improvement. Our approach involves utilizing instance segmentation and optical flow to create tracklets, which are then enhanced using a space-time memory network (STM) designed for one-shot video object segmentation. Our method, named MeNToS, is the first to utilize the STM network for object mask tracking in MOTS. We achieved the 4th place in the RobMOTS challenge, and further details can be found on our project page at https://mehdimiah.com/mentos.html.",1
"We address the problem of text-guided video temporal grounding, which aims to identify the time interval of certain event based on a natural language description. Different from most existing methods that only consider RGB images as visual features, we propose a multi-modal framework to extract complementary information from videos. Specifically, we adopt RGB images for appearance, optical flow for motion, and depth maps for image structure. While RGB images provide abundant visual cues of certain event, the performance may be affected by background clutters. Therefore, we use optical flow to focus on large motion and depth maps to infer the scene configuration when the action is related to objects recognizable with their shapes. To integrate the three modalities more effectively and enable inter-modal learning, we design a dynamic fusion scheme with transformers to model the interactions between modalities. Furthermore, we apply intra-modal self-supervised learning to enhance feature representations across videos for each modality, which also facilitates multi-modal learning. We conduct extensive experiments on the Charades-STA and ActivityNet Captions datasets, and show that the proposed method performs favorably against state-of-the-art approaches.",0
"Our focus is on solving the issue of text-based temporal grounding in videos, where the aim is to identify a specific event's time interval based on a natural language description. Unlike existing methods that only consider RGB images as visual features, we propose a multi-modal framework to extract additional information from videos. Our approach involves using RGB images to capture appearance, optical flow to identify large motion, and depth maps to infer the scene configuration when the action relates to recognizable objects' shapes. To optimize the interactions between these modalities, we employ a dynamic fusion scheme with transformers. Additionally, we utilize intra-modal self-supervised learning to enhance feature representations across videos for each modality and enable multi-modal learning. Our experiments on Charades-STA and ActivityNet Captions datasets demonstrate that our proposed method outperforms state-of-the-art approaches.",1
"Dense optical flow estimation is challenging when there are large displacements in a scene with heterogeneous motion dynamics, occlusion, and scene homogeneity. Traditional approaches to handle these challenges include hierarchical and multiresolution processing methods. Learning-based optical flow methods typically use a multiresolution approach with image warping when a broad range of flow velocities and heterogeneous motion is present. Accuracy of such coarse-to-fine methods is affected by the ghosting artifacts when images are warped across multiple resolutions and by the vanishing problem in smaller scene extents with higher motion contrast. Previously, we devised strategies for building compact dense prediction networks guided by the effective receptive field (ERF) characteristics of the network (DDCNet). The DDCNet design was intentionally simple and compact allowing it to be used as a building block for designing more complex yet compact networks. In this work, we extend the DDCNet strategies to handle heterogeneous motion dynamics by cascading DDCNet based sub-nets with decreasing extents of their ERF. Our DDCNet with multiresolution capability (DDCNet-Multires) is compact without any specialized network layers. We evaluate the performance of the DDCNet-Multires network using standard optical flow benchmark datasets. Our experiments demonstrate that DDCNet-Multires improves over the DDCNet-B0 and -B1 and provides optical flow estimates with accuracy comparable to similar lightweight learning-based methods.",0
"The estimation of dense optical flow can be difficult in scenes with diverse motion dynamics, occlusion, and homogeneity. Traditional methods address these challenges through hierarchical and multiresolution processing. However, learning-based approaches using multiresolution methods with image warping are more effective in handling a range of flow velocities and heterogeneous motion. Yet, these methods can produce ghosting artifacts and vanishing problems. To address these issues, we previously developed the DDCNet, a compact network that uses effective receptive field (ERF) characteristics. In this study, we extend the DDCNet to handle heterogeneous motion dynamics by cascading sub-nets with decreasing ERF. The resulting DDCNet-Multires is compact and does not require specialized network layers. We evaluated the performance of DDCNet-Multires on standard optical flow benchmark datasets and found that it outperformed DDCNet-B0 and -B1, and compared favorably to other lightweight learning-based methods.",1
"This technical report presents our solution to the HACS Temporal Action Localization Challenge 2021, Weakly-Supervised Learning Track. The goal of weakly-supervised temporal action localization is to temporally locate and classify action of interest in untrimmed videos given only video-level labels. We adopt the two-stream consensus network (TSCN) as the main framework in this challenge. The TSCN consists of a two-stream base model training procedure and a pseudo ground truth learning procedure. The base model training encourages the model to predict reliable predictions based on single modality (i.e., RGB or optical flow), based on the fusion of which a pseudo ground truth is generated and in turn used as supervision to train the base models. On the HACS v1.1.1 dataset, without fine-tuning the feature-extraction I3D models, our method achieves 22.20% on the validation set and 21.68% on the testing set in terms of average mAP. Our solution ranked the 2rd in this challenge, and we hope our method can serve as a baseline for future academic research.",0
"We present our approach to the HACS Temporal Action Localization Challenge 2021, for the Weakly-Supervised Learning Track. The objective is to locate and classify actions of interest in untrimmed videos using only video-level labels. Our main framework for this challenge is the two-stream consensus network (TSCN), which comprises a two-stream base model training procedure and a pseudo ground truth learning procedure. The base model is trained to make accurate predictions based on a single modality (RGB or optical flow), and the fusion of these predictions generates a pseudo ground truth used for model supervision. We achieved an average mAP of 22.20% on the validation set and 21.68% on the testing set of the HACS v1.1.1 dataset without fine-tuning the feature-extraction I3D models. Our method ranked 2nd in the challenge, and we hope it will serve as a benchmark for future research in this area.",1
"Detection of moving objects is a very important task in autonomous driving systems. After the perception phase, motion planning is typically performed in Bird's Eye View (BEV) space. This would require projection of objects detected on the image plane to top view BEV plane. Such a projection is prone to errors due to lack of depth information and noisy mapping in far away areas. CNNs can leverage the global context in the scene to project better. In this work, we explore end-to-end Moving Object Detection (MOD) on the BEV map directly using monocular images as input. To the best of our knowledge, such a dataset does not exist and we create an extended KITTI-raw dataset consisting of 12.9k images with annotations of moving object masks in BEV space for five classes. The dataset is intended to be used for class agnostic motion cue based object detection and classes are provided as meta-data for better tuning. We design and implement a two-stream RGB and optical flow fusion architecture which outputs motion segmentation directly in BEV space. We compare it with inverse perspective mapping of state-of-the-art motion segmentation predictions on the image plane. We observe a significant improvement of 13% in mIoU using the simple baseline implementation. This demonstrates the ability to directly learn motion segmentation output in BEV space. Qualitative results of our baseline and the dataset annotations can be found in https://sites.google.com/view/bev-modnet.",0
"The identification of moving objects is a crucial aspect of self-driving systems. Following the perception phase, motion planning is typically executed in Bird's Eye View (BEV) space, which necessitates the transformation of detected objects in the image plane to the top view BEV plane. However, such a projection can be erroneous due to the absence of depth information and distorted mapping in distant regions. Convolutional Neural Networks (CNNs) can enhance the projection process by utilizing the global context of the scene. In this study, we investigate end-to-end Moving Object Detection (MOD) using monocular images as input and directly on the BEV map. We produce an extended KITTI-raw dataset with annotations of moving object masks in BEV space for five classes, which is intended for class-agnostic motion cue-based object detection with classes provided as meta-data. We construct a two-stream RGB and optical flow fusion architecture that generates motion segmentation in BEV space. We compare our approach to state-of-the-art motion segmentation predictions on the image plane using inverse perspective mapping and observe a significant 13% improvement in mIoU with our simple baseline implementation. Our results demonstrate the ability to learn motion segmentation output directly in BEV space. Access to our dataset annotations and a qualitative evaluation of our baseline can be found at https://sites.google.com/view/bev-modnet.",1
"Dense pixel matching problems such as optical flow and disparity estimation are among the most challenging tasks in computer vision. Recently, several deep learning methods designed for these problems have been successful. A sufficiently larger effective receptive field (ERF) and a higher resolution of spatial features within a network are essential for providing higher-resolution dense estimates. In this work, we present a systemic approach to design network architectures that can provide a larger receptive field while maintaining a higher spatial feature resolution. To achieve a larger ERF, we utilized dilated convolutional layers. By aggressively increasing dilation rates in the deeper layers, we were able to achieve a sufficiently larger ERF with a significantly fewer number of trainable parameters. We used optical flow estimation problem as the primary benchmark to illustrate our network design strategy. The benchmark results (Sintel, KITTI, and Middlebury) indicate that our compact networks can achieve comparable performance in the class of lightweight networks.",0
"One of the most difficult challenges in computer vision involves solving complex problems related to dense pixel matching, such as optical flow and disparity estimation. However, recent breakthroughs in deep learning have led to successful methods for addressing these problems. To improve the accuracy of dense estimates, it is crucial to have a network with a larger effective receptive field (ERF) and higher spatial feature resolution. In this study, we present a comprehensive approach for creating network architectures that achieve a larger ERF while maintaining high spatial feature resolution. Our technique involves using dilated convolutional layers and increasing dilation rates in deeper layers. By doing so, we can achieve a larger ERF with fewer trainable parameters. We tested our network design approach using the optical flow estimation problem and found that our compact networks can achieve comparable performance to lightweight networks, as demonstrated by benchmark results from Sintel, KITTI, and Middlebury.",1
"State-of-the-art temporal action detectors to date are based on two-stream input including RGB frames and optical flow. Although combining RGB frames and optical flow boosts performance significantly, optical flow is a hand-designed representation which not only requires heavy computation, but also makes it methodologically unsatisfactory that two-stream methods are often not learned end-to-end jointly with the flow. In this paper, we argue that optical flow is dispensable in high-accuracy temporal action detection and image level data augmentation (ILDA) is the key solution to avoid performance degradation when optical flow is removed. To evaluate the effectiveness of ILDA, we design a simple yet efficient one-stage temporal action detector based on single RGB stream named DaoTAD. Our results show that when trained with ILDA, DaoTAD has comparable accuracy with all existing state-of-the-art two-stream detectors while surpassing the inference speed of previous methods by a large margin and the inference speed is astounding 6668 fps on GeForce GTX 1080 Ti. Code is available at \url{https://github.com/Media-Smart/vedatad}.",0
"Temporal action detectors that are currently considered state-of-the-art utilize two-stream input, combining RGB frames and optical flow. However, the use of optical flow, a hand-designed representation, results in heavy computation and often prevents two-stream methods from being learned end-to-end jointly with the flow, making the approach methodologically unsatisfactory. This paper argues that optical flow is not necessary for high-accuracy temporal action detection and proposes image level data augmentation (ILDA) as a key solution to prevent performance degradation when optical flow is not used. To evaluate the effectiveness of ILDA, a simple yet efficient one-stage temporal action detector called DaoTAD is designed based on single RGB stream. Results show that DaoTAD trained with ILDA achieves comparable accuracy with existing two-stream detectors while surpassing previous methods in terms of inference speed, which is an impressive 6668 fps on GeForce GTX 1080 Ti. The code for DaoTAD is available at \url{https://github.com/Media-Smart/vedatad}.",1
"Optical flow estimation is a fundamental problem of computer vision and has many applications in the fields of robot learning and autonomous driving. This paper reveals novel geometric laws of optical flow based on the insight and detailed definition of non-occlusion. Then, two novel loss functions are proposed for the unsupervised learning of optical flow based on the geometric laws of non-occlusion. Specifically, after the occlusion part of the images are masked, the flowing process of pixels is carefully considered and geometric constraints are conducted based on the geometric laws of optical flow. First, neighboring pixels in the first frame will not intersect during the pixel displacement to the second frame. Secondly, when the cluster containing adjacent four pixels in the first frame moves to the second frame, no other pixels will flow into the quadrilateral formed by them. According to the two geometrical constraints, the optical flow non-intersection loss and the optical flow non-blocking loss in the non-occlusion regions are proposed. Two loss functions punish the irregular and inexact optical flows in the non-occlusion regions. The experiments on datasets demonstrated that the proposed unsupervised losses of optical flow based on the geometric laws in non-occlusion regions make the estimated optical flow more refined in detail, and improve the performance of unsupervised learning of optical flow. In addition, the experiments training on synthetic data and evaluating on real data show that the generalization ability of optical flow network is improved by our proposed unsupervised approach.",0
"The estimation of optical flow is a crucial aspect of computer vision that finds numerous applications in autonomous driving and robot learning. This study uncovers new geometric principles of optical flow with the help of a comprehensive understanding of non-occlusion. It further introduces two fresh loss functions for unsupervised optical flow learning based on these geometric principles. The first loss function ensures that neighboring pixels in the first frame do not intersect while moving to the second frame, and the second loss function ensures that no other pixels flow into the quadrilateral formed by the cluster containing adjacent four pixels in the first frame. These two loss functions penalize inaccurate and irregular optical flows in the non-occlusion regions. The experiments confirm that the proposed unsupervised losses of optical flow, based on the geometric laws in non-occlusion regions, refine the estimated optical flow and improve the performance of unsupervised learning of optical flow. Additionally, the experiments carried out on synthetic data and evaluated on real data demonstrate that our proposed unsupervised approach enhances the generalization ability of the optical flow network.",1
"Different types of spectroscopies, such as X-ray absorption near edge structure (XANES) and Raman spectroscopy, play a very important role in analyzing the characteristics of different materials. In scientific literature, XANES/Raman data are usually plotted in line graphs which is a visually appropriate way to represent the information when the end-user is a human reader. However, such graphs are not conducive to direct programmatic analysis due to the lack of automatic tools. In this paper, we develop a plot digitizer, named Plot2Spectra, to extract data points from spectroscopy graph images in an automatic fashion, which makes it possible for large scale data acquisition and analysis. Specifically, the plot digitizer is a two-stage framework. In the first axis alignment stage, we adopt an anchor-free detector to detect the plot region and then refine the detected bounding boxes with an edge-based constraint to locate the position of two axes. We also apply scene text detector to extract and interpret all tick information below the x-axis. In the second plot data extraction stage, we first employ semantic segmentation to separate pixels belonging to plot lines from the background, and from there, incorporate optical flow constraints to the plot line pixels to assign them to the appropriate line (data instance) they encode. Extensive experiments are conducted to validate the effectiveness of the proposed plot digitizer, which shows that such a tool could help accelerate the discovery and machine learning of materials properties.",0
"The analysis of different materials is facilitated by various types of spectroscopies, including Raman spectroscopy and X-ray absorption near edge structure (XANES). While line graphs are visually appropriate for human readers to represent XANES/Raman data in scientific literature, they lack automatic tools for direct programmatic analysis. This paper introduces a plot digitizer, called Plot2Spectra, to automatically extract data points from spectroscopy graph images. The proposed plot digitizer is a two-stage framework that aligns the axes and extracts plot data. In the first stage, an anchor-free detector locates the plot region and edge-based constraints refine the detected bounding boxes to locate the position of the two axes. The scene text detector extracts and interprets all tick information below the x-axis. In the second stage, semantic segmentation separates pixels belonging to plot lines from the background, and optical flow constraints assign them to the appropriate line (data instance) they encode. Extensive experiments confirm the effectiveness of the plot digitizer, which can accelerate the discovery and machine learning of materials properties.",1
"This article presents a novel approach to incorporate visual cues from video-data from a wide-angle stereo camera system mounted at an urban intersection into the forecast of cyclist trajectories. We extract features from image and optical flow (OF) sequences using 3D convolutional neural networks (3D-ConvNet) and combine them with features extracted from the cyclist's past trajectory to forecast future cyclist positions. By the use of additional information, we are able to improve positional accuracy by about 7.5 % for our test dataset and by up to 22 % for specific motion types compared to a method solely based on past trajectories. Furthermore, we compare the use of image sequences to the use of OF sequences as additional information, showing that OF alone leads to significant improvements in positional accuracy. By training and testing our methods using a real-world dataset recorded at a heavily frequented public intersection and evaluating the methods' runtimes, we demonstrate the applicability in real traffic scenarios. Our code and parts of our dataset are made publicly available.",0
"In this article, we introduce a fresh approach to enhance the prediction of cyclist trajectories by incorporating visual cues from video-data taken by a wide-angle stereo camera system installed at an urban intersection. We utilize 3D convolutional neural networks (3D-ConvNet) to extract features from image and optical flow (OF) sequences, which are combined with cyclist trajectory features to forecast future cyclist positions. The use of additional information improves positional accuracy by approximately 7.5 % for the test dataset and up to 22 % for specific motion types, compared to solely using past trajectories. Additionally, we compare the effectiveness of image and OF sequences as additional information and find that OF alone leads to significant improvements in positional accuracy. We evaluate our methods using a real-world dataset recorded at a heavily frequented public intersection and demonstrate their practicality in real traffic scenarios by assessing their runtimes. Our code and some of our dataset are publicly accessible.",1
"Video frame interpolation is the task of creating an interframe between two adjacent frames along the time axis. So, instead of simply averaging two adjacent frames to create an intermediate image, this operation should maintain semantic continuity with the adjacent frames. Most conventional methods use optical flow, and various tools such as occlusion handling and object smoothing are indispensable. Since the use of these various tools leads to complex problems, we tried to tackle the video interframe generation problem without using problematic optical flow . To enable this , we have tried to use a deep neural network with an invertible structure, and developed an U-Net based Generative Flow which is a modified normalizing flow. In addition, we propose a learning method with a new consistency loss in the latent space to maintain semantic temporal consistency between frames. The resolution of the generated image is guaranteed to be identical to that of the original images by using an invertible network. Furthermore, as it is not a random image like the ones by generative models, our network guarantees stable outputs without flicker. Through experiments, we \sam {confirmed the feasibility of the proposed algorithm and would like to suggest the U-Net based Generative Flow as a new possibility for baseline in video frame interpolation. This paper is meaningful in that it is the world's first attempt to use invertible networks instead of optical flows for video interpolation.",0
"The task of video frame interpolation involves creating an intermediate frame between two adjacent frames along the time axis that maintains semantic continuity with the adjacent frames, rather than simply averaging the frames. Most conventional methods use optical flow and require tools such as occlusion handling and object smoothing, which can lead to complex problems. To avoid these issues, we developed a deep neural network with an invertible structure called the U-Net based Generative Flow, which is a modified normalizing flow. We also propose a learning method with a new consistency loss in the latent space to maintain semantic temporal consistency between frames. Our network guarantees stable outputs without flicker and maintains the identical resolution as the original images. Through experiments, we confirmed the feasibility of our approach and propose it as a new possibility for baseline in video frame interpolation. This paper is significant as it is the first attempt to use invertible networks instead of optical flows for video interpolation.",1
"Multiple human tracking is a fundamental problem for scene understanding. Although both accuracy and speed are required in real-world applications, recent tracking methods based on deep learning have focused on accuracy and require substantial running time. This study aims to improve running speed by performing human detection at a certain frame interval because it accounts for most of the running time. The question is how to maintain accuracy while skipping human detection. In this paper, we propose a method that complements the detection results with optical flow, based on the fact that someone's appearance does not change much between adjacent frames. To maintain the tracking accuracy, we introduce robust interest point selection within human regions and a tracking termination metric calculated by the distribution of the interest points. On the MOT20 dataset in the MOTChallenge, the proposed SDOF-Tracker achieved the best performance in terms of the total running speed while maintaining the MOTA metric. Our code is available at https://anonymous.4open.science/r/sdof-tracker-75AE.",0
"The problem of tracking multiple humans is essential for comprehending a scene. While real-world applications require both accuracy and speed, recent deep learning-based tracking methods prioritize accuracy and consume significant processing time. This research focuses on enhancing running speed by detecting humans at specific frame intervals, which consume most of the processing time. However, the challenge is to maintain accuracy while skipping human detection. To address this issue, this paper presents a method that supplements detection outcomes with optical flow by exploiting the fact that an individual's appearance rarely changes between adjacent frames. To sustain tracking accuracy, the proposed approach features robust interest point selection within human regions and a tracking termination metric based on the distribution of interest points. On the MOT20 dataset in the MOTChallenge, our SDOF-Tracker method outperformed other approaches in terms of total running speed while maintaining the MOTA metric. Interested parties can access our code at https://anonymous.4open.science/r/sdof-tracker-75AE.",1
"Moving target detection plays an important role in computer vision. However, traditional algorithms such as frame difference and optical flow usually suffer from low accuracy or heavy computation. Recent algorithms such as deep learning-based convolutional neural networks have achieved high accuracy and real-time performance, but they usually need to know the classes of targets in advance, which limits the practical applications. Therefore, we proposed a model free moving target detection algorithm. This algorithm extracts the moving area through the difference of image features. Then, the color and location probability map of the moving area will be calculated through maximum a posteriori probability. And the target probability map can be obtained through the dot multiply between the two maps. Finally, the optimal moving target area can be solved by stochastic gradient descent on the target probability map. Results show that the proposed algorithm achieves the highest accuracy compared with state-of-the-art algorithms, without needing to know the classes of targets. Furthermore, as the existing datasets are not suitable for moving target detection, we proposed a method for producing evaluation dataset. Besides, we also proved the proposed algorithm can be used to assist target tracking.",0
"Computer vision heavily relies on accurate moving target detection. However, conventional algorithms, such as frame difference and optical flow, often suffer from low accuracy or heavy computation. Although recent deep learning-based convolutional neural networks have achieved high accuracy and real-time performance, they still require prior knowledge of target classes, which limits their practical applications. Therefore, we have proposed a model-free moving target detection algorithm that extracts the moving area through image feature differences. This algorithm then calculates the color and location probability map of the moving area by maximum a posteriori probability. The dot product between the two maps produces the target probability map, which helps solve the optimal moving target area through stochastic gradient descent. Our algorithm has achieved the highest accuracy compared to state-of-the-art algorithms, without needing prior knowledge of target classes. Additionally, we have proposed a method for producing an evaluation dataset since existing datasets are not fit for moving target detection. Furthermore, we have proven that our proposed algorithm can assist target tracking.",1
"Moving Object Detection (MOD) is a crucial task for the Autonomous Driving pipeline. MOD is usually handled via 2-stream convolutional architectures that incorporates both appearance and motion cues, without considering the inter-relations between the spatial or motion features. In this paper, we tackle this problem through multi-head attention mechanisms, both across the spatial and motion streams. We propose MODETR; a Moving Object DEtection TRansformer network, comprised of multi-stream transformer encoders for both spatial and motion modalities, and an object transformer decoder that produces the moving objects bounding boxes using set predictions. The whole architecture is trained end-to-end using bi-partite loss. Several methods of incorporating motion cues with the Transformer model are explored, including two-stream RGB and Optical Flow (OF) methods, and multi-stream architectures that take advantage of sequence information. To incorporate the temporal information, we propose a new Temporal Positional Encoding (TPE) approach to extend the Spatial Positional Encoding(SPE) in DETR. We explore two architectural choices for that, balancing between speed and time. To evaluate the our network, we perform the MOD task on the KITTI MOD [6] data set. Results show significant 5% mAP of the Transformer network for MOD over the state-of-the art methods. Moreover, the proposed TPE encoding provides 10% mAP improvement over the SPE baseline.",0
"The task of Moving Object Detection (MOD) is essential in the Autonomous Driving pipeline. Currently, MOD is accomplished using 2-stream convolutional architectures that incorporate appearance and motion cues but do not consider the inter-relationships between spatial or motion features. In this paper, we present a solution to this problem through multi-head attention mechanisms across spatial and motion streams. Our solution is called MODETR, a Moving Object Detection Transformer network that uses multi-stream transformer encoders for spatial and motion modalities and an object transformer decoder that produces moving object bounding boxes using set predictions. The entire architecture is trained end-to-end using bi-partite loss. We explore different ways of incorporating motion cues with the Transformer model, including two-stream RGB and Optical Flow methods, and multi-stream architectures that leverage sequence information. To incorporate temporal information, we propose a new approach called Temporal Positional Encoding (TPE), an extension of the Spatial Positional Encoding (SPE) in DETR. We also explore two architectural choices for TPE to balance speed and time. To evaluate our network, we perform the MOD task on the KITTI MOD data set and achieve significant results, with a 5% mAP improvement over state-of-the-art methods using the Transformer network for MOD and a 10% mAP improvement over the SPE baseline using the proposed TPE encoding.",1
"Transferring image-based object detectors to the domain of video remains challenging under resource constraints. Previous efforts utilised optical flow to allow unchanged features to be propagated, however, the overhead is considerable when working with very slowly changing scenes from applications such as surveillance. In this paper, we propose temporal early exits to reduce the computational complexity of per-frame video object detection. Multiple temporal early exit modules with low computational overhead are inserted at early layers of the backbone network to identify the semantic differences between consecutive frames. Full computation is only required if the frame is identified as having a semantic change to previous frames; otherwise, detection results from previous frames are reused. Experiments on CDnet show that our method significantly reduces the computational complexity and execution of per-frame video object detection up to $34 \times$ compared to existing methods with an acceptable reduction of 2.2\% in mAP.",0
"When faced with resource constraints, it remains a challenge to apply image-based object detectors to videos. Previous attempts have utilized optical flow to propagate unchanged features, but this method is impractical for slowly changing scenes, such as those in surveillance applications, due to its high overhead. Our proposed solution in this paper is to utilize temporal early exits that reduce the computational complexity of video object detection. Multiple modules with low computational overhead are inserted at early layers of the backbone network to detect semantic differences between consecutive frames. Full computation is only required when a frame is found to have a semantic change; otherwise, detection results from previous frames are reused. We conducted experiments on CDnet, which showed that our method significantly reduces computational complexity and execution time of per-frame video object detection up to $34 \times$ compared to existing methods, with an acceptable reduction of 2.2\% in mAP.",1
"Instance-level contrastive learning techniques, which rely on data augmentation and a contrastive loss function, have found great success in the domain of visual representation learning. They are not suitable for exploiting the rich dynamical structure of video however, as operations are done on many augmented instances. In this paper we propose ""Video Cross-Stream Prototypical Contrasting"", a novel method which predicts consistent prototype assignments from both RGB and optical flow views, operating on sets of samples. Specifically, we alternate the optimization process; while optimizing one of the streams, all views are mapped to one set of stream prototype vectors. Each of the assignments is predicted with all views except the one matching the prediction, pushing representations closer to their assigned prototypes. As a result, more efficient video embeddings with ingrained motion information are learned, without the explicit need for optical flow computation during inference. We obtain state-of-the-art results on nearest neighbour video retrieval and action recognition, outperforming previous best by +3.2% on UCF101 using the S3D backbone (90.5% Top-1 acc), and by +7.2% on UCF101 and +15.1% on HMDB51 using the R(2+1)D backbone.",0
"Although instance-level contrastive learning techniques have been successful in visual representation learning, they are not well-suited for utilizing the intricate dynamics of video due to the high number of augmented instances. To address this issue, we introduce a new method called ""Video Cross-Stream Prototypical Contrasting"". Our approach involves predicting consistent prototype assignments from both RGB and optical flow views, operating on sets of samples. During optimization, we alternate between optimizing one of the streams and mapping all views to a set of stream prototype vectors. We predict each assignment with all views except the one matching the prediction, pushing representations closer to their assigned prototypes. This results in more efficient video embeddings with embedded motion information, without the need for optical flow computation during inference. Our approach achieves state-of-the-art results on nearest neighbour video retrieval and action recognition, outperforming previous best by +3.2% on UCF101 using the S3D backbone (90.5% Top-1 acc), and by +7.2% on UCF101 and +15.1% on HMDB51 using the R(2+1)D backbone.",1
"Forecasting the formation and development of clouds is a central element of modern weather forecasting systems. Incorrect clouds forecasts can lead to major uncertainty in the overall accuracy of weather forecasts due to their intrinsic role in the Earth's climate system. Few studies have tackled this challenging problem from a machine learning point-of-view due to a shortage of high-resolution datasets with many historical observations globally. In this paper, we present a novel satellite-based dataset called ``CloudCast''. It consists of 70,080 images with 10 different cloud types for multiple layers of the atmosphere annotated on a pixel level. The spatial resolution of the dataset is 928 x 1530 pixels (3x3 km per pixel) with 15-min intervals between frames for the period 2017-01-01 to 2018-12-31. All frames are centered and projected over Europe. To supplement the dataset, we conduct an evaluation study with current state-of-the-art video prediction methods such as convolutional long short-term memory networks, generative adversarial networks, and optical flow-based extrapolation methods. As the evaluation of video prediction is difficult in practice, we aim for a thorough evaluation in the spatial and temporal domain. Our benchmark models show promising results but with ample room for improvement. This is the first publicly available global-scale dataset with high-resolution cloud types on a high temporal granularity to the authors' best knowledge.",0
"Modern weather forecasting systems rely heavily on accurately predicting the formation and development of clouds. The accuracy of weather forecasts can be significantly affected by incorrect cloud forecasts, as clouds play a crucial role in the Earth's climate system. However, few machine learning studies have tackled this challenging problem due to the lack of high-resolution datasets with sufficient historical observations. In this paper, we introduce a new satellite-based dataset named ""CloudCast,"" consisting of 70,080 images with annotations for 10 different cloud types in multiple atmospheric layers on a pixel level. The dataset has a spatial resolution of 928 x 1530 pixels (3x3 km per pixel) and 15-minute intervals between frames for the period of 2017-01-01 to 2018-12-31, centered and projected over Europe. To supplement the dataset, we evaluate state-of-the-art video prediction methods such as convolutional long short-term memory networks, generative adversarial networks, and optical flow-based extrapolation methods. We conduct a thorough evaluation of the models in both the spatial and temporal domains. Although our benchmark models show promising results, there is still room for improvement. This is the first publicly available global-scale dataset with high-resolution cloud types and high temporal granularity.",1
"State-of-the-art frame interpolation methods generate intermediate frames by inferring object motions in the image from consecutive key-frames. In the absence of additional information, first-order approximations, i.e. optical flow, must be used, but this choice restricts the types of motions that can be modeled, leading to errors in highly dynamic scenarios. Event cameras are novel sensors that address this limitation by providing auxiliary visual information in the blind-time between frames. They asynchronously measure per-pixel brightness changes and do this with high temporal resolution and low latency. Event-based frame interpolation methods typically adopt a synthesis-based approach, where predicted frame residuals are directly applied to the key-frames. However, while these approaches can capture non-linear motions they suffer from ghosting and perform poorly in low-texture regions with few events. Thus, synthesis-based and flow-based approaches are complementary. In this work, we introduce Time Lens, a novel indicates equal contribution method that leverages the advantages of both. We extensively evaluate our method on three synthetic and two real benchmarks where we show an up to 5.21 dB improvement in terms of PSNR over state-of-the-art frame-based and event-based methods. Finally, we release a new large-scale dataset in highly dynamic scenarios, aimed at pushing the limits of existing methods.",0
"Advanced techniques for frame interpolation create intermediate frames by predicting object movements in images based on consecutive key-frames. When additional information is not available, first-order approximations, such as optical flow, must be used, which limits the types of movements that can be modeled and leads to errors in highly dynamic scenarios. Event cameras are innovative sensors that overcome this limitation by providing auxiliary visual information during the blind-time between frames. They measure per-pixel brightness changes asynchronously, with high temporal resolution and low latency. Event-based frame interpolation methods typically use a synthesis-based approach, where predicted frame residuals are directly applied to the key-frames. However, although these approaches can capture non-linear movements, they suffer from ghosting and perform poorly in low-texture areas with few events. Therefore, synthesis-based and flow-based approaches complement each other. In this study, we introduce Time Lens, a novel method that leverages the advantages of both approaches. We extensively evaluate our method on three synthetic and two real benchmarks, where we demonstrate an improvement of up to 5.21 dB in terms of PSNR compared to state-of-the-art frame-based and event-based methods. Finally, we release a new large-scale dataset in highly dynamic scenarios to push the limits of existing techniques.",1
"Despite the continued successes of computationally efficient deep neural network architectures for video object detection, performance continually arrives at the great trilemma of speed versus accuracy versus computational resources (pick two). Current attempts to exploit temporal information in video data to overcome this trilemma are bottlenecked by the state-of-the-art in object detection models. We present, a technique which performs video object detection through the use of off-the-shelf object detectors alongside existing optical flow based motion estimation techniques in parallel. Through a set of experiments on the benchmark MOT20 dataset, we demonstrate that our approach significantly reduces the baseline latency of any given object detector without sacrificing any accuracy. Further latency reduction, up to 25x lower than the original latency, can be achieved with minimal accuracy loss. MOVEX enables low latency video object detection on common CPU based systems, thus allowing for high performance video object detection beyond the domain of GPU computing. The code is available at https://github.com/juliantrue/movex.",0
"Although computationally efficient deep neural network architectures for video object detection have been successful, they face the challenge of balancing speed, accuracy, and computational resources. Attempts to solve this issue by utilizing temporal information in video data are constrained by the current state-of-the-art in object detection models. Our approach uses off-the-shelf object detectors and optical flow based motion estimation techniques in parallel to perform video object detection. Our experiments on the MOT20 dataset demonstrate that our technique reduces latency without sacrificing accuracy. We can achieve up to 25 times lower latency with minimal accuracy loss. Our technique, MOVEX, enables low latency video object detection on CPU-based systems, extending the domain of high-performance video object detection beyond GPU computing. Our code is available at https://github.com/juliantrue/movex.",1
"Facial expressions vary from the visible to the subtle. In recent years, the analysis of micro-expressions $-$ a natural occurrence resulting from the suppression of one's true emotions, has drawn the attention of researchers with a broad range of potential applications. However, spotting microexpressions in long videos becomes increasingly challenging when intertwined with normal or macro-expressions. In this paper, we propose a shallow optical flow three-stream CNN (SOFTNet) model to predict a score that captures the likelihood of a frame being in an expression interval. By fashioning the spotting task as a regression problem, we introduce pseudo-labeling to facilitate the learning process. We demonstrate the efficacy and efficiency of the proposed approach on the recent MEGC 2020 benchmark, where state-of-the-art performance is achieved on CAS(ME)$^{2}$ with equally promising results on SAMM Long Videos.",0
"Facial expressions range from obvious to subtle, and researchers have recently focused on analyzing micro-expressions, which occur when one suppresses their true emotions. However, identifying micro-expressions within long videos becomes increasingly difficult when they are mixed with normal or macro-expressions. This paper proposes a shallow optical flow three-stream CNN (SOFTNet) model to predict the likelihood of a frame being in an expression interval. By framing the task as a regression problem, the learning process is facilitated by introducing pseudo-labeling. The proposed approach is demonstrated to be effective and efficient on the MEGC 2020 benchmark, achieving state-of-the-art performance on CAS(ME)² and promising results on SAMM Long Videos.",1
"Geometric alignment appears in a variety of applications, ranging from domain adaptation, optimal transport, and normalizing flows in machine learning; optical flow and learned augmentation in computer vision and deformable registration within biomedical imaging. A recurring challenge is the alignment of domains whose topology is not the same; a problem that is routinely ignored, potentially introducing bias in downstream analysis. As a first step towards solving such alignment problems, we propose an unsupervised topological difference detection algorithm. The model is based on a conditional variational auto-encoder and detects topological anomalies with regards to a reference alongside the registration step. We consider both a) topological changes in the image under spatial variation and b) unexpected transformations. Our approach is validated on a proxy task of unsupervised anomaly detection in images.",0
"Geometric alignment plays a crucial role in various applications, including domain adaptation, optimal transport, and normalizing flows in machine learning. It is also used in optical flow and learned augmentation in computer vision and deformable registration within biomedical imaging. However, aligning domains with different topologies remains a recurring challenge, which is often overlooked, leading to potential bias in downstream analysis. To address this issue, we propose an unsupervised algorithm that detects topological differences using a conditional variational auto-encoder during the registration step. Our approach considers both topological changes in the image under spatial variation and unexpected transformations. To validate our method, we conduct a proxy task of unsupervised anomaly detection in images.",1
"We present DistillFlow, a knowledge distillation approach to learning optical flow. DistillFlow trains multiple teacher models and a student model, where challenging transformations are applied to the input of the student model to generate hallucinated occlusions as well as less confident predictions. Then, a self-supervised learning framework is constructed: confident predictions from teacher models are served as annotations to guide the student model to learn optical flow for those less confident predictions. The self-supervised learning framework enables us to effectively learn optical flow from unlabeled data, not only for non-occluded pixels, but also for occluded pixels. DistillFlow achieves state-of-the-art unsupervised learning performance on both KITTI and Sintel datasets. Our self-supervised pre-trained model also provides an excellent initialization for supervised fine-tuning, suggesting an alternate training paradigm in contrast to current supervised learning methods that highly rely on pre-training on synthetic data. At the time of writing, our fine-tuned models ranked 1st among all monocular methods on the KITTI 2015 benchmark, and outperform all published methods on the Sintel Final benchmark. More importantly, we demonstrate the generalization capability of DistillFlow in three aspects: framework generalization, correspondence generalization and cross-dataset generalization.",0
"DistillFlow is a method for learning optical flow through knowledge distillation. The approach involves training multiple teacher models and a student model, with challenging input transformations generating occlusions and less confident predictions. A self-supervised learning framework is then constructed, with the confident predictions from teacher models serving as annotations to guide the student model in learning optical flow for less confident predictions. This approach allows for effective learning of optical flow from unlabeled data, including occluded pixels. DistillFlow achieves state-of-the-art unsupervised learning performance on KITTI and Sintel datasets, and provides an excellent initialization for supervised fine-tuning. The fine-tuned models rank 1st among all monocular methods on the KITTI 2015 benchmark and outperform all published methods on the Sintel Final benchmark. DistillFlow also demonstrates generalization capability in framework, correspondence, and cross-dataset aspects.",1
"Neuromorphic sensing and computing hold a promise for highly energy-efficient and high-bandwidth-sensor processing. A major challenge for neuromorphic computing is that learning algorithms for traditional artificial neural networks (ANNs) do not transfer directly to spiking neural networks (SNNs) due to the discrete spikes and more complex neuronal dynamics. As a consequence, SNNs have not yet been successfully applied to complex, large-scale tasks. In this article, we focus on the self-supervised learning problem of optical flow estimation from event-based camera inputs, and investigate the changes that are necessary to the state-of-the-art ANN training pipeline in order to successfully tackle it with SNNs. More specifically, we first modify the input event representation to encode a much smaller time slice with minimal explicit temporal information. Consequently, we make the network's neuronal dynamics and recurrent connections responsible for integrating information over time. Moreover, we reformulate the self-supervised loss function for event-based optical flow to improve its convexity. We perform experiments with various types of recurrent ANNs and SNNs using the proposed pipeline. Concerning SNNs, we investigate the effects of elements such as parameter initialization and optimization, surrogate gradient shape, and adaptive neuronal mechanisms. We find that initialization and surrogate gradient width play a crucial part in enabling learning with sparse inputs, while the inclusion of adaptivity and learnable neuronal parameters can improve performance. We show that the performance of the proposed ANNs and SNNs are on par with that of the current state-of-the-art ANNs trained in a self-supervised manner.",0
"The potential for highly energy-efficient and high-bandwidth-sensor processing lies in neuromorphic sensing and computing. However, transferring learning algorithms from traditional artificial neural networks (ANNs) to spiking neural networks (SNNs) presents a challenge due to the discrete spikes and complex neuronal dynamics of SNNs. Therefore, complex, large-scale tasks have not been successfully tackled using SNNs. This article focuses on the self-supervised learning problem of optical flow estimation from event-based camera inputs and explores modifications to the state-of-the-art ANN training pipeline needed to tackle this problem with SNNs successfully. The input event representation is modified to encode a smaller time slice with minimal temporal information, and the network's neuronal dynamics and recurrent connections are responsible for integrating information over time. The self-supervised loss function for event-based optical flow is reformulated to improve its convexity. Various types of recurrent ANNs and SNNs are experimented with using the proposed pipeline. Initialization and surrogate gradient width are crucial for enabling learning with sparse inputs in SNNs, while adaptivity and learnable neuronal parameters can improve performance. The proposed ANNs and SNNs achieve performance on par with the current state-of-the-art ANNs trained in a self-supervised manner.",1
"Non-Rigid Structure-from-Motion (NRSfM) reconstructs a deformable 3D object from the correspondences established between monocular 2D images. Current NRSfM methods lack statistical robustness, which is the ability to cope with correspondence errors.This prevents one to use automatically established correspondences, which are prone to errors, thereby strongly limiting the scope of NRSfM. We propose a three-step automatic pipeline to solve NRSfM robustly by exploiting isometry. Step 1 computes the optical flow from correspondences, step 2 reconstructs each 3D point's normal vector using multiple reference images and integrates them to form surfaces with the best reference and step 3 rejects the 3D points that break isometry in their local neighborhood. Importantly, each step is designed to discard or flag erroneous correspondences. Our contributions include the robustification of optical flow by warp estimation, new fast analytic solutions to local normal reconstruction and their robustification, and a new scale-independent measure of 3D local isometric coherence. Experimental results show that our robust NRSfM method consistently outperforms existing methods on both synthetic and real datasets.",0
"The process of Non-Rigid Structure-from-Motion (NRSfM) involves reconstructing a deformable 3D object based on correspondences established between monocular 2D images. However, current NRSfM methods are not statistically robust, which makes it difficult to automatically establish correspondences due to errors. This greatly limits the potential of NRSfM. Our proposed solution is a three-step automatic pipeline that utilizes isometry to solve NRSfM robustly. In step 1, we compute optical flow from correspondences, while in step 2, we reconstruct each 3D point's normal vector and integrate them to form surfaces with the best reference. Finally, in step 3, we reject 3D points that break isometry in their local neighborhood. Each step is designed to discard or flag erroneous correspondences. Our contributions include the use of warp estimation to improve the robustness of optical flow, new fast analytic solutions for local normal reconstruction, and the development of a new scale-independent measure of 3D local isometric coherence. Our experimental results demonstrate that our robust NRSfM method consistently outperforms existing methods on both synthetic and real datasets.",1
"End-to-end trained convolutional neural networks have led to a breakthrough in optical flow estimation. The most recent advances focus on improving the optical flow estimation by improving the architecture and setting a new benchmark on the publicly available MPI-Sintel dataset. Instead, in this article, we investigate how deep neural networks estimate optical flow. A better understanding of how these networks function is important for (i) assessing their generalization capabilities to unseen inputs, and (ii) suggesting changes to improve their performance. For our investigation, we focus on FlowNetS, as it is the prototype of an encoder-decoder neural network for optical flow estimation. Furthermore, we use a filter identification method that has played a major role in uncovering the motion filters present in animal brains in neuropsychological research. The method shows that the filters in the deepest layer of FlowNetS are sensitive to a variety of motion patterns. Not only do we find translation filters, as demonstrated in animal brains, but thanks to the easier measurements in artificial neural networks, we even unveil dilation, rotation, and occlusion filters. Furthermore, we find similarities in the refinement part of the network and the perceptual filling-in process which occurs in the mammal primary visual cortex.",0
"Convolutional neural networks that are trained end-to-end have resulted in a significant breakthrough in optical flow estimation. While recent advances have focused on enhancing the architecture to improve optical flow estimation and setting new benchmarks, this article delves into the workings of deep neural networks to estimate optical flow. Understanding the functioning of these networks is crucial for assessing their ability to generalize to unseen inputs and suggesting changes to enhance their performance. The investigation centers on the encoder-decoder neural network prototype, FlowNetS. A filter identification method is employed, which has been instrumental in uncovering motion filters in animal brains in neuropsychological research. The method reveals that the deepest layer of FlowNetS has filters that are sensitive to various motion patterns. Translation filters, similar to those found in animal brains, are also present, along with dilation, rotation, and occlusion filters that are more readily observable in artificial neural networks. Additionally, similarities are found between the refinement part of the network and the perceptual filling-in process in the mammal primary visual cortex.",1
"We present an unsupervised learning approach for optical flow estimation by improving the upsampling and learning of pyramid network. We design a self-guided upsample module to tackle the interpolation blur problem caused by bilinear upsampling between pyramid levels. Moreover, we propose a pyramid distillation loss to add supervision for intermediate levels via distilling the finest flow as pseudo labels. By integrating these two components together, our method achieves the best performance for unsupervised optical flow learning on multiple leading benchmarks, including MPI-SIntel, KITTI 2012 and KITTI 2015. In particular, we achieve EPE=1.4 on KITTI 2012 and F1=9.38% on KITTI 2015, which outperform the previous state-of-the-art methods by 22.2% and 15.7%, respectively.",0
"Our study introduces an unsupervised learning technique for estimating optical flow that enhances the upsampling and learning of the pyramid network. To address the issue of interpolation blur caused by bilinear upsampling between pyramid levels, we have developed a self-guided upsample module. Additionally, we have proposed a pyramid distillation loss to supervise intermediate levels by distilling the finest flow as pseudo labels. By integrating both components, our approach surpasses other unsupervised optical flow learning methods on various benchmarks, such as KITTI 2012 and KITTI 2015, with EPE=1.4 and F1=9.38%, respectively. These results outperform previous state-of-the-art methods by 22.2% and 15.7%, respectively.",1
"Estimating geometric elements such as depth, camera motion, and optical flow from images is an important part of the robot's visual perception. We use a joint self-supervised method to estimate the three geometric elements. Depth network, optical flow network and camera motion network are independent of each other but are jointly optimized during training phase. Compared with independent training, joint training can make full use of the geometric relationship between geometric elements and provide dynamic and static information of the scene. In this paper, we improve the joint self-supervision method from three aspects: network structure, dynamic object segmentation, and geometric constraints. In terms of network structure, we apply the attention mechanism to the camera motion network, which helps to take advantage of the similarity of camera movement between frames. And according to attention mechanism in Transformer, we propose a plug-and-play convolutional attention module. In terms of dynamic object, according to the different influences of dynamic objects in the optical flow self-supervised framework and the depth-pose self-supervised framework, we propose a threshold algorithm to detect dynamic regions, and mask that in the loss function respectively. In terms of geometric constraints, we use traditional methods to estimate the fundamental matrix from the corresponding points to constrain the camera motion network. We demonstrate the effectiveness of our method on the KITTI dataset. Compared with other joint self-supervised methods, our method achieves state-of-the-art performance in the estimation of pose and optical flow, and the depth estimation has also achieved competitive results. Code will be available https://github.com/jianfenglihg/Unsupervised_geometry.",0
"The robot's visual perception relies heavily on accurately estimating geometric elements such as depth, optical flow, and camera motion from images. To achieve this, we employ a joint self-supervised approach that allows for simultaneous estimation of the three geometric elements. During the training phase, the depth network, optical flow network, and camera motion network are optimized jointly, leveraging the geometric relationships between the elements. This results in the provision of both dynamic and static information of the scene. In this study, we enhance the joint self-supervision method by improving the network structure, dynamic object segmentation, and geometric constraints. To improve the network structure, we incorporate an attention mechanism into the camera motion network and propose a convolutional attention module inspired by the Transformer. To address dynamic objects, we introduce a threshold algorithm to detect dynamic regions and mask them in the loss function. Finally, we utilize traditional methods to estimate the fundamental matrix from the corresponding points to constrain the camera motion network. We demonstrate the effectiveness of our method on the KITTI dataset, where it achieves superior performance in pose and optical flow estimation, and competitive results in depth estimation. The code for our method is available on https://github.com/jianfenglihg/Unsupervised_geometry.",1
"We present a Python-based renderer built on NVIDIA's OptiX ray tracing engine and the OptiX AI denoiser, designed to generate high-quality synthetic images for research in computer vision and deep learning. Our tool enables the description and manipulation of complex dynamic 3D scenes containing object meshes, materials, textures, lighting, volumetric data (e.g., smoke), and backgrounds. Metadata, such as 2D/3D bounding boxes, segmentation masks, depth maps, normal maps, material properties, and optical flow vectors, can also be generated. In this work, we discuss design goals, architecture, and performance. We demonstrate the use of data generated by path tracing for training an object detector and pose estimator, showing improved performance in sim-to-real transfer in situations that are difficult for traditional raster-based renderers. We offer this tool as an easy-to-use, performant, high-quality renderer for advancing research in synthetic data generation and deep learning.",0
"Our Python-based renderer, which utilizes NVIDIA's OptiX ray tracing engine and OptiX AI denoiser, has been specifically created to generate superior synthetic images for computer vision and deep learning research. With our tool, users can create and manipulate intricate dynamic 3D scenes, complete with object meshes, materials, textures, lighting, volumetric data (including smoke), and backgrounds. In addition, metadata such as 2D/3D bounding boxes, segmentation masks, depth maps, normal maps, material properties, and optical flow vectors can also be produced. Throughout this paper, we detail the renderer's design objectives, architecture, and performance capabilities. We demonstrate the renderer's ability to enhance sim-to-real transfer in challenging situations that traditional raster-based renderers struggle with, by using path tracing data to train object detectors and pose estimators. We aim to provide a user-friendly, high-quality, and efficient renderer that can advance synthetic data generation and deep learning research.",1
"Semantic segmentation of aerial videos has been extensively used for decision making in monitoring environmental changes, urban planning, and disaster management. The reliability of these decision support systems is dependent on the accuracy of the video semantic segmentation algorithms. The existing CNN based video semantic segmentation methods have enhanced the image semantic segmentation methods by incorporating an additional module such as LSTM or optical flow for computing temporal dynamics of the video which is a computational overhead. The proposed research work modifies the CNN architecture by incorporating temporal information to improve the efficiency of video semantic segmentation.   In this work, an enhanced encoder-decoder based CNN architecture (UVid-Net) is proposed for UAV video semantic segmentation. The encoder of the proposed architecture embeds temporal information for temporally consistent labelling. The decoder is enhanced by introducing the feature-refiner module, which aids in accurate localization of the class labels. The proposed UVid-Net architecture for UAV video semantic segmentation is quantitatively evaluated on extended ManipalUAVid dataset. The performance metric mIoU of 0.79 has been observed which is significantly greater than the other state-of-the-art algorithms. Further, the proposed work produced promising results even for the pre-trained model of UVid-Net on urban street scene with fine tuning the final layer on UAV aerial videos.",0
"The usage of semantic segmentation in aerial videos has become widespread for decision making in various fields like environmental monitoring, urban planning, and disaster management. The accuracy of these decision support systems relies on the precision of the video semantic segmentation algorithms. CNN based video semantic segmentation methods, which are currently in use, have been improved by adding an extra module such as LSTM or optical flow to compute the video's temporal dynamics, leading to computational overhead. This research proposes modifying the CNN architecture by embedding temporal information to enhance the efficiency of video semantic segmentation. The proposed architecture, UVid-Net, is an improved encoder-decoder based CNN model designed for UAV video semantic segmentation. The encoder of the proposed architecture incorporates temporal information to achieve temporally consistent labeling, while the decoder is enhanced by introducing the feature-refiner module to improve class label localization accuracy. The proposed UVid-Net architecture is evaluated quantitatively on the extended ManipalUAVid dataset, and it achieves an mIoU performance metric of 0.79, which is significantly higher than other state-of-the-art algorithms. Furthermore, the proposed approach produces promising results even for pre-trained models on urban street scenes by fine-tuning the final layer on UAV aerial videos.",1
"Recently, DETR and Deformable DETR have been proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance as previous complex hand-crafted detectors. However, their performance on Video Object Detection (VOD) has not been well explored. In this paper, we present TransVOD, an end-to-end video object detection model based on a spatial-temporal Transformer architecture. The goal of this paper is to streamline the pipeline of VOD, effectively removing the need for many hand-crafted components for feature aggregation, e.g., optical flow, recurrent neural networks, relation networks. Besides, benefited from the object query design in DETR, our method does not need complicated post-processing methods such as Seq-NMS or Tubelet rescoring, which keeps the pipeline simple and clean. In particular, we present temporal Transformer to aggregate both the spatial object queries and the feature memories of each frame. Our temporal Transformer consists of three components: Temporal Deformable Transformer Encoder (TDTE) to encode the multiple frame spatial details, Temporal Query Encoder (TQE) to fuse object queries, and Temporal Deformable Transformer Decoder to obtain current frame detection results. These designs boost the strong baseline deformable DETR by a significant margin (3%-4% mAP) on the ImageNet VID dataset. TransVOD yields comparable results performance on the benchmark of ImageNet VID. We hope our TransVOD can provide a new perspective for video object detection. Code will be made publicly available at https://github.com/SJTU-LuHe/TransVOD.",0
"DET and Deformable DETR have been suggested as alternatives to hand-designed components for object detection, with good performance. However, their performance on Video Object Detection (VOD) has not been widely explored. In this paper, we introduce TransVOD, an end-to-end video object detection model utilizing a spatial-temporal Transformer architecture to eliminate the need for hand-crafted components such as optical flow and recurrent neural networks. Our method benefits from the object query design in DETR and does not require complicated post-processing methods. We present a temporal Transformer with three components to aggregate both spatial object queries and feature memories of each frame, boosting the performance of deformable DETR by 3%-4% mAP on the ImageNet VID dataset. TransVOD yields comparable results to existing benchmarks on ImageNet VID, providing a new perspective for video object detection. Code for TransVOD can be found at https://github.com/SJTU-LuHe/TransVOD.",1
"The technology for Visual Odometry (VO) that estimates the position and orientation of the moving object through analyzing the image sequences captured by on-board cameras, has been well investigated with the rising interest in autonomous driving. This paper studies monocular VO from the perspective of Deep Learning (DL). Unlike most current learning-based methods, our approach, called DeepAVO, is established on the intuition that features contribute discriminately to different motion patterns. Specifically, we present a novel four-branch network to learn the rotation and translation by leveraging Convolutional Neural Networks (CNNs) to focus on different quadrants of optical flow input. To enhance the ability of feature selection, we further introduce an effective channel-spatial attention mechanism to force each branch to explicitly distill related information for specific Frame to Frame (F2F) motion estimation. Experiments on various datasets involving outdoor driving and indoor walking scenarios show that the proposed DeepAVO outperforms the state-of-the-art monocular methods by a large margin, demonstrating competitive performance to the stereo VO algorithm and verifying promising potential for generalization.",0
"With the increasing interest in autonomous driving, the technology of Visual Odometry (VO) has been thoroughly researched. VO estimates the position and orientation of a moving object by analyzing the image sequences captured by on-board cameras. This paper examines monocular VO through the lens of Deep Learning (DL). Unlike current learning-based methods, our approach, called DeepAVO, is based on the understanding that features contribute differently to various motion patterns. To achieve this, we present a novel four-branch network that uses Convolutional Neural Networks (CNNs) to focus on different quadrants of optical flow input for learning rotation and translation. Additionally, we introduce an effective channel-spatial attention mechanism, which forces each branch to extract related information for specific Frame to Frame (F2F) motion estimation, enhancing feature selection. Our experiments on various datasets, including outdoor driving and indoor walking scenarios, show that DeepAVO outperforms the current state-of-the-art monocular methods by a large margin, demonstrating competitive performance to the stereo VO algorithm and indicating promising potential for generalization.",1
"With the advent of neuromorphic vision sensors such as event-based cameras, a paradigm shift is required for most computer vision algorithms. Among these algorithms, optical flow estimation is a prime candidate for this process considering that it is linked to a neuromorphic vision approach. Usage of optical flow is widespread in robotics applications due to its richness and accuracy. We present a Principal Component Analysis (PCA) approach to the problem of event-based optical flow estimation. In this approach, we examine different regularization methods which efficiently enhance the estimation of the optical flow. We show that the best variant of our proposed method, dedicated to the real-time context of visual odometry, is about two times faster compared to state-of-the-art implementations while significantly improves optical flow accuracy.",0
"The rise of neuromorphic vision sensors, such as event-based cameras, necessitates a change in the majority of computer vision algorithms. Optical flow estimation is an ideal candidate for this transition, as it aligns with a neuromorphic vision approach. Optical flow is commonly used in robotics applications due to its precision and detail. Our study proposes a Principal Component Analysis (PCA) method to address event-based optical flow estimation. We evaluate various regularization techniques that effectively improve optical flow estimation. Our research indicates that our proposed approach is the most efficient in real-time visual odometry, surpassing current leading methods by a factor of two while also significantly improving optical flow accuracy.",1
"In this work we tackle the task of video-based visual emotion recognition in the wild. Standard methodologies that rely solely on the extraction of bodily and facial features often fall short of accurate emotion prediction in cases where the aforementioned sources of affective information are inaccessible due to head/body orientation, low resolution and poor illumination. We aspire to alleviate this problem by leveraging visual context in the form of scene characteristics and attributes, as part of a broader emotion recognition framework. Temporal Segment Networks (TSN) constitute the backbone of our proposed model. Apart from the RGB input modality, we make use of dense Optical Flow, following an intuitive multi-stream approach for a more effective encoding of motion. Furthermore, we shift our attention towards skeleton-based learning and leverage action-centric data as means of pre-training a Spatial-Temporal Graph Convolutional Network (ST-GCN) for the task of emotion recognition. Our extensive experiments on the challenging Body Language Dataset (BoLD) verify the superiority of our methods over existing approaches, while by properly incorporating all of the aforementioned modules in a network ensemble, we manage to surpass the previous best published recognition scores, by a large margin.",0
"The objective of our study is to recognize visual emotions in videos captured in natural settings. Traditional methods of emotion prediction relying solely on bodily and facial features are often inadequate in cases where such information is unavailable due to various reasons. We aim to overcome this challenge by incorporating contextual information in the form of scene characteristics and attributes in our emotion recognition framework. Our proposed model uses Temporal Segment Networks (TSN) as the backbone, and we employ dense Optical Flow in addition to RGB input for better motion encoding. Additionally, we utilize skeleton-based learning and pre-train a Spatial-Temporal Graph Convolutional Network (ST-GCN) with action-centric data to enhance emotion recognition. Our experiments on the Body Language Dataset (BoLD) demonstrate the effectiveness of our approach compared to existing methods. By combining all the modules in a network ensemble, we achieve significantly better recognition scores than previous state-of-the-art methods.",1
"This paper addresses the challenging unsupervised scene flow estimation problem by jointly learning four low-level vision sub-tasks: optical flow $\textbf{F}$, stereo-depth $\textbf{D}$, camera pose $\textbf{P}$ and motion segmentation $\textbf{S}$. Our key insight is that the rigidity of the scene shares the same inherent geometrical structure with object movements and scene depth. Hence, rigidity from $\textbf{S}$ can be inferred by jointly coupling $\textbf{F}$, $\textbf{D}$ and $\textbf{P}$ to achieve more robust estimation. To this end, we propose a novel scene flow framework named EffiScene with efficient joint rigidity learning, going beyond the existing pipeline with independent auxiliary structures. In EffiScene, we first estimate optical flow and depth at the coarse level and then compute camera pose by Perspective-$n$-Points method. To jointly learn local rigidity, we design a novel Rigidity From Motion (RfM) layer with three principal components: \emph{}{(i)} correlation extraction; \emph{}{(ii)} boundary learning; and \emph{}{(iii)} outlier exclusion. Final outputs are fused based on the rigid map $M_R$ from RfM at finer levels. To efficiently train EffiScene, two new losses $\mathcal{L}_{bnd}$ and $\mathcal{L}_{unc}$ are designed to prevent trivial solutions and to regularize the flow boundary discontinuity. Extensive experiments on scene flow benchmark KITTI show that our method is effective and significantly improves the state-of-the-art approaches for all sub-tasks, i.e. optical flow ($5.19 \rightarrow 4.20$), depth estimation ($3.78 \rightarrow 3.46$), visual odometry ($0.012 \rightarrow 0.011$) and motion segmentation ($0.57 \rightarrow 0.62$).",0
"Efficiently estimating unsupervised scene flow is a challenging task that requires joint learning of four low-level vision sub-tasks: optical flow, stereo-depth, camera pose, and motion segmentation. Our approach, called EffiScene, leverages the inherent geometrical structure shared by the rigidity of the scene, object movements, and scene depth to infer rigidity from motion segmentation. EffiScene goes beyond existing pipelines by coupling optical flow, stereo-depth, and camera pose to achieve more robust estimation. We propose a Rigidity From Motion layer that extracts correlations, learns boundaries, and excludes outliers to jointly learn local rigidity. To prevent trivial solutions and regulate flow boundary discontinuity, we design two new losses, $\mathcal{L}_{bnd}$ and $\mathcal{L}_{unc}$. Extensive experiments on the KITTI scene flow benchmark demonstrate that EffiScene significantly improves the state-of-the-art approaches for optical flow, depth estimation, visual odometry, and motion segmentation.",1
"We present SMURF, a method for unsupervised learning of optical flow that improves state of the art on all benchmarks by $36\%$ to $40\%$ (over the prior best method UFlow) and even outperforms several supervised approaches such as PWC-Net and FlowNet2. Our method integrates architecture improvements from supervised optical flow, i.e. the RAFT model, with new ideas for unsupervised learning that include a sequence-aware self-supervision loss, a technique for handling out-of-frame motion, and an approach for learning effectively from multi-frame video data while still only requiring two frames for inference.",0
"The SMURF technique is introduced as a means of unsupervised learning for optical flow. It surpasses the prior best method UFlow, as well as a few supervised approaches such as PWC-Net and FlowNet2, by 36% to 40% in all benchmarks. Our approach combines the RAFT model's architecture enhancements with fresh concepts for unsupervised learning, including a self-supervision loss that is sensitive to the sequence, a mechanism for managing out-of-frame motion, and a strategy for effectively learning from multi-frame video data while still only necessitating two frames for inference.",1
"Video salient object detection (VSOD) aims to locate and segment the most attractive object by exploiting both spatial cues and temporal cues hidden in video sequences. However, spatial and temporal cues are often unreliable in real-world scenarios, such as low-contrast foreground, fast motion, and multiple moving objects. To address these problems, we propose a new framework to adaptively capture available information from spatial and temporal cues, which contains Confidence-guided Adaptive Gate (CAG) modules and Dual Differential Enhancement (DDE) modules. For both RGB features and optical flow features, CAG estimates confidence scores supervised by the IoU between predictions and the ground truths to re-calibrate the information with a gate mechanism. DDE captures the differential feature representation to enrich the spatial and temporal information and generate the fused features. Experimental results on four widely used datasets demonstrate the effectiveness of the proposed method against thirteen state-of-the-art methods.",0
"The goal of Video Salient Object Detection (VSOD) is to locate and segment the most captivating object in a video sequence using spatial and temporal cues. However, these cues are not always reliable in real-world scenarios, such as multiple moving objects, fast motion, and low-contrast foreground. To tackle these issues, we have introduced a new framework that can adaptively capture available information from spatial and temporal cues. Our framework incorporates Confidence-guided Adaptive Gate (CAG) modules and Dual Differential Enhancement (DDE) modules. The CAG modules estimate confidence scores for RGB features and optical flow features, supervised by the Intersection over Union (IoU) between predictions and the ground truths. This helps re-calibrate the information with a gate mechanism. The DDE modules capture the differential feature representation to enrich the spatial and temporal information and generate the fused features. Our experimental results on four widely used datasets demonstrate that our proposed method is more effective than thirteen state-of-the-art methods.",1
"Most Video Super-Resolution (VSR) methods enhance a video reference frame by aligning its neighboring frames and mining information on these frames. Recently, deformable alignment has drawn extensive attention in VSR community for its remarkable performance, which can adaptively align neighboring frames with the reference one. However, we experimentally find that deformable alignment methods still suffer from fast motion due to locally loss-driven offset prediction and lack explicit motion constraints. Hence, we propose a Matching-based Flow Estimation (MFE) module to conduct global semantic feature matching and estimate optical flow as coarse offset for each location. And a Flow-guided Deformable Module (FDM) is proposed to integrate optical flow into deformable convolution. The FDM uses the optical flow to warp the neighboring frames at first. And then, the warped neighboring frames and the reference one are used to predict a set of fine offsets for each coarse offset. In general, we propose an end-to-end deep network called Flow-guided Deformable Alignment Network (FDAN), which reaches the state-of-the-art performance on two benchmark datasets while is still competitive in computation and memory consumption.",0
"Many Video Super-Resolution (VSR) methods aim to improve a reference frame of a video by examining information from neighboring frames and aligning them. Deformable alignment has gained popularity in the VSR community due to its adaptive alignment of neighboring frames with the reference frame. However, we have discovered through experimentation that deformable alignment methods still suffer from fast motion due to locally loss-driven offset prediction and a lack of explicit motion constraints. To address this issue, we have introduced the Matching-based Flow Estimation (MFE) module, which conducts global semantic feature matching and estimates optical flow as a coarse offset for each location. We have also proposed the Flow-guided Deformable Module (FDM), which integrates optical flow into deformable convolution. The FDM utilizes optical flow to initially warp neighboring frames, and then predicts a set of fine offsets for each coarse offset using the warped neighboring frames and the reference frame. Our proposed end-to-end deep network, the Flow-guided Deformable Alignment Network (FDAN), yields state-of-the-art performance on two benchmark datasets while remaining competitive in terms of computation and memory consumption.",1
"Video Frame Interpolation synthesizes non-existent images between adjacent frames, with the aim of providing a smooth and consistent visual experience. Two approaches for solving this challenging task are optical flow based and kernel-based methods. In existing works, optical flow based methods can provide accurate point-to-point motion description, however, they lack constraints on object structure. On the contrary, kernel-based methods focus on structural alignment, which relies on semantic and apparent features, but tends to blur results. Based on these observations, we propose a structure-motion based iterative fusion method. The framework is an end-to-end learnable structure with two stages. First, interpolated frames are synthesized by structure-based and motion-based learning branches respectively, then, an iterative refinement module is established via spatial and temporal feature integration. Inspired by the observation that audiences have different visual preferences on foreground and background objects, we for the first time propose to use saliency masks in the evaluation processes of the task of video frame interpolation. Experimental results on three typical benchmarks show that the proposed method achieves superior performance on all evaluation metrics over the state-of-the-art methods, even when our models are trained with only one-tenth of the data other methods use.",0
"Video Frame Interpolation aims to create a seamless visual experience by generating non-existent images between adjacent frames. Optical flow based and kernel-based methods are two approaches used to tackle this challenging task. Optical flow based methods accurately describe point-to-point motion but lack object structure constraints, while kernel-based methods focus on structural alignment using semantic and apparent features but tend to blur results. To overcome these limitations, we propose a structure-motion based iterative fusion method that synthesizes interpolated frames through two learning branches and integrates spatial and temporal features via an iterative refinement module. We also introduce saliency masks in the evaluation process to account for audience preferences on foreground and background objects. Our proposed method outperforms state-of-the-art techniques on all evaluation metrics across three benchmarks, even with a smaller training dataset.",1
"Video anomaly detection is a challenging task because of diverse abnormal events. To this task, methods based on reconstruction and prediction are wildly used in recent works, which are built on the assumption that learning on normal data, anomalies cannot be reconstructed or predicated as good as normal patterns, namely the anomaly result with more errors. In this paper, we propose to discriminate anomalies from normal ones by the duality of normality-granted optical flow, which is conducive to predict normal frames but adverse to abnormal frames. The normality-granted optical flow is predicted from a single frame, to keep the motion knowledge focused on normal patterns. Meanwhile, We extend the appearance-motion correspondence scheme from frame reconstruction to prediction, which not only helps to learn the knowledge about object appearances and correlated motion, but also meets the fact that motion is the transformation between appearances. We also introduce a margin loss to enhance the learning of frame prediction. Experiments on standard benchmark datasets demonstrate the impressive performance of our approach.",0
"Detecting anomalies in videos is a difficult task due to the varied nature of abnormal events. Recent studies have relied on reconstruction and prediction methods, assuming that anomalies cannot be reconstructed or predicted as well as normal patterns. This results in more errors when detecting anomalies. In this study, we propose a method to distinguish normal frames from abnormal ones using the duality of normality-granted optical flow. This method predicts normal frames accurately but is less accurate with abnormal frames. The appearance-motion correspondence scheme is extended from frame reconstruction to prediction, allowing for learning about object appearances and motion. Additionally, a margin loss is introduced to improve frame prediction learning. Our approach is tested on benchmark datasets and shows impressive performance.",1
"Intersections where vehicles are permitted to turn and interact with vulnerable road users (VRUs) like pedestrians and cyclists are among some of the most challenging locations for automated and accurate recognition of road users' behavior. In this paper, we propose a deep conditional generative model for interaction detection at such locations. It aims to automatically analyze massive video data about the continuity of road users' behavior. This task is essential for many intelligent transportation systems such as traffic safety control and self-driving cars that depend on the understanding of road users' locomotion. A Conditional Variational Auto-Encoder based model with Gaussian latent variables is trained to encode road users' behavior and perform probabilistic and diverse predictions of interactions. The model takes as input the information of road users' type, position and motion automatically extracted by a deep learning object detector and optical flow from videos, and generates frame-wise probabilities that represent the dynamics of interactions between a turning vehicle and any VRUs involved. The model's efficacy was validated by testing on real--world datasets acquired from two different intersections. It achieved an F1-score above 0.96 at a right--turn intersection in Germany and 0.89 at a left--turn intersection in Japan, both with very busy traffic flows.",0
"Automated recognition of road users' behavior at intersections where vulnerable road users (VRUs) like pedestrians and cyclists interact with vehicles is a complex task. To address this challenge, we propose a deep conditional generative model in this paper that analyzes large amounts of video data to detect interactions. This is crucial for intelligent transportation systems like traffic safety control and self-driving cars that rely on understanding road users' movements. Our model uses a Conditional Variational Auto-Encoder with Gaussian latent variables to encode road users' behavior and generate diverse and probabilistic predictions of interactions. It takes inputs from a deep learning object detector and optical flow to automatically extract information about road users' type, position, and motion. The model generates frame-wise probabilities that represent the dynamics of interactions between a turning vehicle and any VRUs involved. We tested the model on real-world datasets from two busy intersections, achieving an F1-score above 0.96 at a right-turn intersection in Germany and 0.89 at a left-turn intersection in Japan.",1
"Remarkable progress has been made in 3D reconstruction of rigid structures from a video or a collection of images. However, it is still challenging to reconstruct nonrigid structures from RGB inputs, due to its under-constrained nature. While template-based approaches, such as parametric shape models, have achieved great success in modeling the ""closed world"" of known object categories, they cannot well handle the ""open-world"" of novel object categories or outlier shapes. In this work, we introduce a template-free approach to learn 3D shapes from a single video. It adopts an analysis-by-synthesis strategy that forward-renders object silhouette, optical flow, and pixel values to compare with video observations, which generates gradients to adjust the camera, shape and motion parameters. Without using a category-specific shape template, our method faithfully reconstructs nonrigid 3D structures from videos of human, animals, and objects of unknown classes. Code will be available at lasr-google.github.io .",0
"Significant advancements have been made in reconstructing 3D solid structures from videos or image collections. However, the reconstruction of non-rigid structures from RGB inputs remains a challenging task due to its under-constrained nature. While template-based methods like parametric shape models have been successful in modeling the ""closed world"" of known object types, they are not well-suited for handling the ""open-world"" of new object types or outlier shapes. This paper introduces a template-free method for learning 3D shapes from a single video, utilizing an analysis-by-synthesis approach that compares forward-rendered object silhouettes, optical flow, and pixel values with video observations to generate gradients for camera, shape, and motion parameter adjustments. Our approach accurately reconstructs non-rigid 3D structures from videos of human, animal, and unknown object classes without relying on a category-specific shape template. The code for this approach will be accessible at lasr-google.github.io.",1
"With the goal of predicting the future rainfall intensity in a local region over a relatively short period time, precipitation nowcasting has been a long-time scientific challenge with great social and economic impact. The radar echo extrapolation approaches for precipitation nowcasting take radar echo images as input, aiming to generate future radar echo images by learning from the historical images. To effectively handle complex and high non-stationary evolution of radar echoes, we propose to decompose the movement into optical flow field motion and morphologic deformation. Following this idea, we introduce Flow-Deformation Network (FDNet), a neural network that models flow and deformation in two parallel cross pathways. The flow encoder captures the optical flow field motion between consecutive images and the deformation encoder distinguishes the change of shape from the translational motion of radar echoes. We evaluate the proposed network architecture on two real-world radar echo datasets. Our model achieves state-of-the-art prediction results compared with recent approaches. To the best of our knowledge, this is the first network architecture with flow and deformation separation to model the evolution of radar echoes for precipitation nowcasting. We believe that the general idea of this work could not only inspire much more effective approaches but also be applied to other similar spatiotemporal prediction tasks",0
"Precipitation nowcasting has been a difficult scientific challenge for a long time, as it aims to predict future rainfall intensity in a local region over a short period. It has significant social and economic impact. To achieve this, radar echo extrapolation approaches are used to generate future radar echo images by learning from historical images. However, handling the complex and high non-stationary evolution of radar echoes is challenging. Therefore, we propose the Flow-Deformation Network (FDNet), which decomposes movement into optical flow field motion and morphologic deformation. The flow encoder captures the optical flow field motion between consecutive images, and the deformation encoder distinguishes the change of shape from the translational motion of radar echoes. We evaluate the proposed network architecture on two real-world radar echo datasets and achieve state-of-the-art prediction results. Our approach is the first network architecture with flow and deformation separation to model the evolution of radar echoes for precipitation nowcasting. We hope that this work inspires more effective approaches and can be applied to other spatiotemporal prediction tasks.",1
"In this paper, a novel video classification method is presented that aims to recognize different categories of third-person videos efficiently. Our motivation is to achieve a light model that could be trained with insufficient training data. With this intuition, the processing of the 3-dimensional video input is broken to 1D in temporal dimension on top of the 2D in spatial. The processes related to 2D spatial frames are being done by utilizing pre-trained networks with no training phase. The only step which involves training is to classify the 1D time series resulted from the description of the 2D signals. As a matter of fact, optical flow images are first calculated from consecutive frames and described by pre-trained CNN networks. Their dimension is then reduced using PCA. By stacking the description vectors beside each other, a multi-channel time series is created for each video. Each channel of the time series represents a specific feature and follows it over time. The main focus of the proposed method is to classify the obtained time series effectively. Towards this, the idea is to let the machine learn temporal features. This is done by training a multi-channel one dimensional Convolutional Neural Network (1D-CNN). The 1D-CNN learns the features along the only temporal dimension. Hence, the number of training parameters decreases significantly which would result in the trainability of the method on even smaller datasets. It is illustrated that the proposed method could reach the state-of-the-art results on two public datasets UCF11, jHMDB and competitive results on HMDB51.",0
"The paper introduces a new technique for recognizing various categories of third-person videos efficiently. The goal is to develop a lightweight model that can be trained with limited data. To achieve this, the video input is processed in a 1D temporal dimension and 2D spatial dimension. Pre-trained networks are used to handle the 2D spatial frames, with the only training required being for classifying the resulting 1D time series. The method involves calculating optical flow images from consecutive frames and reducing their dimension using PCA. The resulting time series has multiple channels, with each representing a specific feature over time. A 1D-CNN is trained to learn temporal features along the only temporal dimension, significantly reducing the number of training parameters. The proposed method shows state-of-the-art results on the UCF11 and jHMDB datasets and competitive results on the HMDB51 dataset.",1
"The goal of this paper is to self-train a 3D convolutional neural network on an unlabeled video collection for deployment on small-scale video collections. As smaller video datasets benefit more from motion than appearance, we strive to train our network using optical flow, but avoid its computation during inference. We propose the first motion-augmented self-training regime, we call MotionFit. We start with supervised training of a motion model on a small, and labeled, video collection. With the motion model we generate pseudo-labels for a large unlabeled video collection, which enables us to transfer knowledge by learning to predict these pseudo-labels with an appearance model. Moreover, we introduce a multi-clip loss as a simple yet efficient way to improve the quality of the pseudo-labeling, even without additional auxiliary tasks. We also take into consideration the temporal granularity of videos during self-training of the appearance model, which was missed in previous works. As a result we obtain a strong motion-augmented representation model suited for video downstream tasks like action recognition and clip retrieval. On small-scale video datasets, MotionFit outperforms alternatives for knowledge transfer by 5%-8%, video-only self-supervision by 1%-7% and semi-supervised learning by 9%-18% using the same amount of class labels.",0
"This paper aims to develop a 3D convolutional neural network that can be self-trained on an unlabeled video collection for use on smaller video datasets. Our focus is on using optical flow for training, but avoiding its use during inference. To achieve this, we propose MotionFit, a motion-augmented self-training regime. We begin by training a motion model on a small labeled video collection, which we then use to generate pseudo-labels for a larger unlabeled collection. We then use these labels to train an appearance model, incorporating a multi-clip loss to improve the quality of the pseudo-labeling. We also consider the temporal granularity of videos, which previous works have overlooked. Our approach yields a robust motion-augmented representation model suitable for action recognition and clip retrieval. Compared to alternative methods, MotionFit outperforms by 5%-8% for knowledge transfer, 1%-7% for video-only self-supervision, and 9%-18% for semi-supervised learning, all with the same number of class labels on small-scale video datasets.",1
"Computing optical flow is a fundamental problem in computer vision. However, deep learning-based optical flow techniques do not perform well for non-rigid movements such as those found in faces, primarily due to lack of the training data representing the fine facial motion. We hypothesize that learning optical flow on face motion data will improve the quality of predicted flow on faces. The aim of this work is threefold: (1) exploring self-supervised techniques to generate optical flow ground truth for face images; (2) computing baseline results on the effects of using face data to train Convolutional Neural Networks (CNN) for predicting optical flow; and (3) using the learned optical flow in micro-expression recognition to demonstrate its effectiveness. We generate optical flow ground truth using facial key-points in the BP4D-Spontaneous dataset. The generated optical flow is used to train the FlowNetS architecture to test its performance on the generated dataset. The performance of FlowNetS trained on face images surpassed that of other optical flow CNN architectures, demonstrating its usefulness. Our optical flow features are further compared with other methods using the STSTNet micro-expression classifier, and the results indicate that the optical flow obtained using this work has promising applications in facial expression analysis.",0
"The computation of optical flow is a crucial problem in the realm of computer vision. Despite this, optical flow techniques based on deep learning struggle when it comes to non-rigid movements, such as those present in facial expressions, mainly due to the lack of training data that can represent fine facial movements. Our hypothesis is that if we train optical flow on facial motion data, it will significantly enhance the accuracy of predicted flow on faces. Our work aims to achieve three objectives: (1) investigate self-supervised techniques that can generate optical flow ground truth for face pictures; (2) produce baseline results to measure the impact of using facial data to train Convolutional Neural Networks (CNN) for optical flow prediction; and (3) demonstrate the effectiveness of the learned optical flow in micro-expression recognition. We generated optical flow ground truth using facial key-points in the BP4D-Spontaneous dataset. This flow was then utilized to train the FlowNetS architecture to analyze its performance on the created dataset. As a result, the performance of FlowNetS exceeded that of other optical flow CNN architectures, indicating its usefulness. Furthermore, we compared our optical flow characteristics with other techniques that use the STSTNet micro-expression classifier, and the outcomes demonstrate that the optical flow obtained from our work has significant potential in facial expression analysis.",1
"Tremor is a key diagnostic feature of Parkinson's Disease (PD), Essential Tremor (ET), and other central nervous system (CNS) disorders. Clinicians or trained raters assess tremor severity with TETRAS scores by observing patients. Lacking quantitative measures, inter- or intra- observer variabilities are almost inevitable as the distinction between adjacent tremor scores is subtle. Moreover, clinician assessments also require patient visits, which limits the frequency of disease progress evaluation. Therefore it is beneficial to develop an automated assessment that can be performed remotely and repeatably at patients' convenience for continuous monitoring. In this work, we proposed to train a deep neural network (DNN) with rank-consistent ordinal regression using 276 clinical videos from 36 essential tremor patients. The videos are coupled with clinician assessed TETRAS scores, which are used as ground truth labels to train the DNN. To tackle the challenge of limited training data, optical flows are used to eliminate irrelevant background and statistic objects from RGB frames. In addition to optical flows, transfer learning is also applied to leverage pre-trained network weights from a related task of tremor frequency estimate. The approach was evaluated by splitting the clinical videos into training (67%) and testing sets (0.33%). The mean absolute error on TETRAS score of the testing results is 0.45, indicating that most of the errors were from the mismatch of adjacent labels, which is expected and acceptable. The model predications also agree well with clinical ratings. This model is further applied to smart phone videos collected from a PD patient who has an implanted device to turn ""On"" or ""Off"" tremor. The model outputs were consistent with the patient tremor states. The results demonstrate that our trained model can be used as a means to assess and track tremor severity.",0
"Tremor is a significant diagnostic characteristic of Parkinson's Disease (PD), Essential Tremor (ET), and other disorders of the central nervous system (CNS). Tremor severity is evaluated by clinicians or trained raters using TETRAS scores, but this method has limitations including inter- or intra-observer variability and a requirement for patient visits. An automated assessment that can be conducted remotely and consistently would be useful for continuous monitoring of disease progression. In this study, a deep neural network (DNN) was trained using 276 clinical videos from 36 ET patients, coupled with clinician-assessed TETRAS scores as ground truth labels. Optical flows were used to eliminate irrelevant background and objects from RGB frames, and transfer learning was applied to leverage pre-trained network weights from a related task of tremor frequency estimation. The approach was evaluated by splitting the clinical videos into training (67%) and testing sets (0.33%), with a mean absolute error on the TETRAS score of 0.45. The model was also applied to smart phone videos collected from a PD patient with an implanted device to turn ""On"" or ""Off"" tremor, and the model outputs were consistent with the patient's tremor states. These results suggest that our trained model can be used to assess and track tremor severity.",1
"Deep-learning-based video processing has yielded transformative results in recent years. However, the video analytics pipeline is energy-intensive due to high data rates and reliance on complex inference algorithms, which limits its adoption in energy-constrained applications. Motivated by the observation of high and variable spatial redundancy and temporal dynamics in video data streams, we design and evaluate an adaptive-resolution optimization framework to minimize the energy use of multi-task video analytics pipelines. Instead of heuristically tuning the input data resolution of individual tasks, our framework utilizes deep reinforcement learning to dynamically govern the input resolution and computation of the entire video analytics pipeline. By monitoring the impact of varying resolution on the quality of high-dimensional video analytics features, hence the accuracy of video analytics results, the proposed end-to-end optimization framework learns the best non-myopic policy for dynamically controlling the resolution of input video streams to globally optimize energy efficiency. Governed by reinforcement learning, optical flow is incorporated into the framework to minimize unnecessary spatio-temporal redundancy that leads to re-computation, while preserving accuracy. The proposed framework is applied to video instance segmentation which is one of the most challenging computer vision tasks, and achieves better energy efficiency than all baseline methods of similar accuracy on the YouTube-VIS dataset.",0
"In recent years, deep-learning-based video processing has achieved significant advancements. However, due to high data rates and complex inference algorithms, the energy consumption of the video analytics pipeline is substantial, which limits its practical use in energy-constrained applications. Our study focuses on reducing energy consumption by designing an adaptive-resolution optimization framework that takes into account the high and variable spatial redundancy and temporal dynamics in video data streams. Unlike conventional methods that set the input data resolution of individual tasks heuristically, our framework uses deep reinforcement learning to dynamically control the input resolution and computation of the entire video analytics pipeline. By monitoring the impact of varying resolution on the quality of high-dimensional video analytics features, the framework learns the best policy to optimize energy efficiency globally. Optical flow is incorporated into the framework to minimize spatio-temporal redundancy that leads to re-computation while maintaining accuracy. We apply the proposed framework to video instance segmentation, which is one of the most challenging computer vision tasks, and it demonstrates better energy efficiency than all baseline methods with similar accuracy on the YouTube-VIS dataset.",1
"Synthetic datasets play a critical role in pre-training CNN models for optical flow, but they are painstaking to generate and hard to adapt to new applications. To automate the process, we present AutoFlow, a simple and effective method to render training data for optical flow that optimizes the performance of a model on a target dataset. AutoFlow takes a layered approach to render synthetic data, where the motion, shape, and appearance of each layer are controlled by learnable hyperparameters. Experimental results show that AutoFlow achieves state-of-the-art accuracy in pre-training both PWC-Net and RAFT. Our code and data are available at https://autoflow-google.github.io .",0
"Generating synthetic datasets is crucial in pre-training CNN models for optical flow. However, this process is laborious and challenging to adapt to new applications. To simplify this task, we introduce AutoFlow, a straightforward and efficient method for producing training data for optical flow that enhances model performance on a target dataset. AutoFlow uses a layered approach to create synthetic data, where learnable hyperparameters control the motion, shape, and appearance of each layer. Our experiments demonstrate that AutoFlow achieves exceptional accuracy in pre-training both PWC-Net and RAFT. Access to our code and data is available at https://autoflow-google.github.io.",1
"Recently, several Space-Time Memory based networks have shown that the object cues (e.g. video frames as well as the segmented object masks) from the past frames are useful for segmenting objects in the current frame. However, these methods exploit the information from the memory by global-to-global matching between the current and past frames, which lead to mismatching to similar objects and high computational complexity. To address these problems, we propose a novel local-to-local matching solution for semi-supervised VOS, namely Regional Memory Network (RMNet). In RMNet, the precise regional memory is constructed by memorizing local regions where the target objects appear in the past frames. For the current query frame, the query regions are tracked and predicted based on the optical flow estimated from the previous frame. The proposed local-to-local matching effectively alleviates the ambiguity of similar objects in both memory and query frames, which allows the information to be passed from the regional memory to the query region efficiently and effectively. Experimental results indicate that the proposed RMNet performs favorably against state-of-the-art methods on the DAVIS and YouTube-VOS datasets.",0
"Several Space-Time Memory based networks have recently demonstrated the usefulness of object cues (such as video frames and segmented object masks) from past frames in segmenting objects in the current frame. However, these methods employ global-to-global matching between the current and past frames, leading to problems such as mismatching to similar objects and high computational complexity. To overcome these issues, we propose a new solution for semi-supervised Video Object Segmentation (VOS) called Regional Memory Network (RMNet), which uses local-to-local matching. RMNet constructs a precise regional memory by memorizing local regions where the target objects appear in past frames. For the current query frame, query regions are tracked and predicted based on optical flow estimated from the previous frame. The proposed local-to-local matching effectively reduces ambiguity between similar objects in both memory and query frames, allowing information to be passed from the regional memory to the query region efficiently and effectively. Experimental results on the DAVIS and YouTube-VOS datasets show that our proposed RMNet outperforms state-of-the-art methods.",1
"Despite the significant progress made by deep learning in natural image matting, there has been so far no representative work on deep learning for video matting due to the inherent technical challenges in reasoning temporal domain and lack of large-scale video matting datasets. In this paper, we propose a deep learning-based video matting framework which employs a novel and effective spatio-temporal feature aggregation module (ST-FAM). As optical flow estimation can be very unreliable within matting regions, ST-FAM is designed to effectively align and aggregate information across different spatial scales and temporal frames within the network decoder. To eliminate frame-by-frame trimap annotations, a lightweight interactive trimap propagation network is also introduced. The other contribution consists of a large-scale video matting dataset with groundtruth alpha mattes for quantitative evaluation and real-world high-resolution videos with trimaps for qualitative evaluation. Quantitative and qualitative experimental results show that our framework significantly outperforms conventional video matting and deep image matting methods applied to video in presence of multi-frame temporal information.",0
"Up until now, there hasn't been much research on using deep learning for video matting, despite the strides made in natural image matting. This is because it's technically difficult to reason about the temporal domain, and there is a lack of large-scale video matting datasets. However, in this paper, we introduce a deep learning-based video matting framework that uses a new and effective spatio-temporal feature aggregation module (ST-FAM). To overcome the unreliability of optical flow estimation in matting regions, ST-FAM aligns and aggregates information across various spatial scales and temporal frames within the network decoder. Additionally, we present a lightweight interactive trimap propagation network to get rid of frame-by-frame trimap annotations. Our framework also includes a large-scale video matting dataset with groundtruth alpha mattes for quantitative evaluation and real-world high-resolution videos with trimaps for qualitative evaluation. Our results show that our framework is significantly better than conventional video matting and deep image matting methods when dealing with multi-frame temporal information.",1
"We propose a self-supervised approach for training multi-frame video denoising networks. These networks predict frame t from a window of frames around t. Our self-supervised approach benefits from the video temporal consistency by penalizing a loss between the predicted frame t and a neighboring target frame, which are aligned using an optical flow. We use the proposed strategy for online internal learning, where a pre-trained network is fine-tuned to denoise a new unknown noise type from a single video. After a few frames, the proposed fine-tuning reaches and sometimes surpasses the performance of a state-of-the-art network trained with supervision. In addition, for a wide range of noise types, it can be applied blindly without knowing the noise distribution. We demonstrate this by showing results on blind denoising of different synthetic and realistic noises.",0
"Our proposition is a method of training multi-frame video denoising networks that is self-supervised. These networks predict frame t using a set of frames that surround it. The self-supervised approach we implement takes advantage of the video's temporal consistency by punishing a loss between the predicted frame t and a neighboring target frame, which are aligned using an optical flow. We use this strategy for online internal learning, where a pre-trained network is fine-tuned to denoise a new unknown noise type from a single video. Our approach reaches and sometimes exceeds the performance of a state-of-the-art network trained with supervision after a few frames. Furthermore, it can be applied blindly to a wide range of noise types without knowledge of the noise distribution. We illustrate this by demonstrating results on blind denoising of various synthetic and realistic noises.",1
"We propose an architecture and training scheme to predict video frames by explicitly modeling dis-occlusions and capturing the evolution of semantically consistent regions in the video. The scene layout (semantic map) and motion (optical flow) are decomposed into layers, which are predicted and fused with their context to generate future layouts and motions. The appearance of the scene is warped from past frames using the predicted motion in co-visible regions; dis-occluded regions are synthesized with content-aware inpainting utilizing the predicted scene layout. The result is a predictive model that explicitly represents objects and learns their class-specific motion, which we evaluate on video prediction benchmarks.",0
"Our proposal involves an architecture and training approach that takes into account dis-occlusions and the evolution of semantically consistent regions in a video to predict video frames. We achieve this by decomposing the scene layout and motion into layers, predicting them, and fusing them with their context to generate future layouts and motions. To create a complete picture, we use predicted motion to warp the appearance of the scene from past frames in co-visible regions and synthesize dis-occluded regions with content-aware inpainting based on the predicted scene layout. By explicitly representing objects and learning their class-specific motion, our predictive model performs well on video prediction benchmarks.",1
"Video segmentation for the human head and shoulders is essential in creating elegant media for videoconferencing and virtual reality applications. The main challenge is to process high-quality background subtraction in a real-time manner and address the segmentation issues under motion blurs, e.g., shaking the head or waving hands during conference video. To overcome the motion blur problem in video segmentation, we propose a novel flow-based encoder-decoder network (FUNet) that combines both traditional Horn-Schunck optical-flow estimation technique and convolutional neural networks to perform robust real-time video segmentation. We also introduce a video and image segmentation dataset: ConferenceVideoSegmentationDataset. Code and pre-trained models are available on our GitHub repository: \url{https://github.com/kuangzijian/Flow-Based-Video-Matting}.",0
"Creating sophisticated media for videoconferencing and virtual reality applications requires video segmentation of the human head and shoulders. The primary obstacle is achieving high-quality background subtraction in real-time and addressing segmentation issues caused by motion blurs such as head shaking or hand waving during conference video. To resolve the motion blur problem in video segmentation, we propose a flow-based encoder-decoder network (FUNet) that merges the Horn-Schunck optical-flow estimation technique and convolutional neural networks to perform real-time video segmentation effectively. Additionally, we present a video and image segmentation dataset known as the ConferenceVideoSegmentationDataset. Our GitHub repository contains the code and pre-trained models: \url{https://github.com/kuangzijian/Flow-Based-Video-Matting}.",1
"Today's image prediction methods struggle to change the locations of objects in a scene, producing blurry images that average over the many positions they might occupy. In this paper, we propose a simple change to existing image similarity metrics that makes them more robust to positional errors: we match the images using optical flow, then measure the visual similarity of corresponding pixels. This change leads to crisper and more perceptually accurate predictions, and can be used with any image prediction network. We apply our method to predicting future frames of a video, where it obtains strong performance with simple, off-the-shelf architectures.",0
"Current techniques for image prediction encounter difficulties in relocating objects within a scene, resulting in unclear images that blend together various possible positions. To address this issue, we suggest a basic modification to established image similarity metrics, which enhances their resilience to positional inaccuracies. Specifically, we employ optical flow to align the images and then evaluate the visual similarity of corresponding pixels. This adjustment results in sharper and more accurate predictions, and can be utilized with any image prediction network. We employ our approach to forecast future frames of a video and achieve impressive results with straightforward, readily available architectures.",1
"3D scene flow estimation is a vital tool in perceiving our environment given depth or range sensors. Unlike optical flow, the data is usually sparse and in most cases partially occluded in between two temporal samplings. Here we propose a new scene flow architecture called OGSF-Net which tightly couples the learning for both flow and occlusions between frames. Their coupled symbiosis results in a more accurate prediction of flow in space. Unlike a traditional multi-action network, our unified approach is fused throughout the network, boosting performances for both occlusion detection and flow estimation. Our architecture is the first to gauge the occlusion in 3D scene flow estimation on point clouds. In key datasets such as Flyingthings3D and KITTI, we achieve the state-of-the-art results.",0
"Perceiving our environment with depth or range sensors is reliant on 3D scene flow estimation, which is a crucial tool. Unlike optical flow, the data is often sparse and partially obscured between temporal samplings. Our proposed OGSF-Net scene flow architecture couples the learning of flow and occlusions between frames, resulting in a more precise prediction of flow in space. Our approach is distinct from traditional multi-action networks, as it is fused throughout the network, improving performances for both occlusion detection and flow estimation. Our architecture is the first of its kind to measure occlusion in 3D scene flow estimation on point clouds. We have attained state-of-the-art results in key datasets such as Flyingthings3D and KITTI.",1
"Recently, deep-learning based approaches have achieved impressive performance for autonomous driving. However, end-to-end vision-based methods typically have limited interpretability, making the behaviors of the deep networks difficult to explain. Hence, their potential applications could be limited in practice. To address this problem, we propose an interpretable end-to-end vision-based motion planning approach for autonomous driving, referred to as IVMP. Given a set of past surrounding-view images, our IVMP first predicts future egocentric semantic maps in bird's-eye-view space, which are then employed to plan trajectories for self-driving vehicles. The predicted future semantic maps not only provide useful interpretable information, but also allow our motion planning module to handle objects with low probability, thus improving the safety of autonomous driving. Moreover, we also develop an optical flow distillation paradigm, which can effectively enhance the network while still maintaining its real-time performance. Extensive experiments on the nuScenes dataset and closed-loop simulation show that our IVMP significantly outperforms the state-of-the-art approaches in imitating human drivers with a much higher success rate. Our project page is available at https://sites.google.com/view/ivmp.",0
"Autonomous driving has seen remarkable progress with the use of deep-learning based techniques. However, end-to-end vision-based methods often lack interpretability, hindering the explanation of deep networks' behaviors and limiting their real-world applications. Therefore, we propose an interpretable end-to-end vision-based motion planning approach for self-driving vehicles, known as IVMP. Our method predicts future egocentric semantic maps in bird's-eye-view space from a set of previous images of the surroundings, which are then employed to plan trajectories. The predicted semantic maps offer interpretable information and enable our motion planning module to handle low-probability objects, ultimately improving the safety of autonomous driving. Furthermore, we introduce an optical flow distillation paradigm that enhances network performance while maintaining real-time processing. Through extensive experiments on the nuScenes dataset and closed-loop simulation, our IVMP demonstrates superior performance in imitating human drivers with a significantly higher success rate. To learn more about our project, please visit https://sites.google.com/view/ivmp.",1
"The objective of this paper is to perform audio-visual sound source separation, i.e.~to separate component audios from a mixture based on the videos of sound sources. Moreover, we aim to pinpoint the source location in the input video sequence. Recent works have shown impressive audio-visual separation results when using prior knowledge of the source type (e.g. human playing instrument) and pre-trained motion detectors (e.g. keypoints or optical flows). However, at the same time, the models are limited to a certain application domain. In this paper, we address these limitations and make the following contributions: i) we propose a two-stage architecture, called Appearance and Motion network (AMnet), where the stages specialise to appearance and motion cues, respectively. The entire system is trained in a self-supervised manner; ii) we introduce an Audio-Motion Embedding (AME) framework to explicitly represent the motions that related to sound; iii) we propose an audio-motion transformer architecture for audio and motion feature fusion; iv) we demonstrate state-of-the-art performance on two challenging datasets (MUSIC-21 and AVE) despite the fact that we do not use any pre-trained keypoint detectors or optical flow estimators. Project page: https://ly-zhu.github.io/self-supervised-motion-representations",0
"The aim of this study is to separate audio components from a mixture by utilizing video footage of sound sources. Additionally, the study aims to identify the location of the sound source in the input video sequence. Previous research has demonstrated impressive results in audio-visual separation by utilizing prior knowledge of source type and pre-trained motion detectors. However, these models are limited to specific application domains. This paper addresses these limitations by proposing a two-stage architecture called AMnet, which specializes in appearance and motion cues respectively. The entire system is trained in a self-supervised manner. Furthermore, the study introduces the Audio-Motion Embedding (AME) framework to represent sound-related motions explicitly. Additionally, the study proposes an audio-motion transformer architecture for audio and motion feature fusion. Despite not using any pre-trained keypoint detectors or optical flow estimators, the proposed system demonstrates state-of-the-art performance on two challenging datasets (MUSIC-21 and AVE). Please visit the project page at https://ly-zhu.github.io/self-supervised-motion-representations.",1
"Visual sound source separation aims at identifying sound components from a given sound mixture with the presence of visual cues. Prior works have demonstrated impressive results, but with the expense of large multi-stage architectures and complex data representations (e.g. optical flow trajectories). In contrast, we study simple yet efficient models for visual sound separation using only a single video frame. Furthermore, our models are able to exploit the information of the sound source category in the separation process. To this end, we propose two models where we assume that i) the category labels are available at the training time, or ii) we know if the training sample pairs are from the same or different category. The experiments with the MUSIC dataset show that our model obtains comparable or better performance compared to several recent baseline methods. The code is available at https://github.com/ly-zhu/Leveraging-Category-Information-for-Single-Frame-Visual-Sound-Source-Separation",0
"The goal of visual sound source separation is to identify sound components from a given sound mixture while using visual cues. Previous studies have shown impressive results, but at the cost of using large, multi-stage architectures and complex data representations such as optical flow trajectories. In contrast, we have developed simple and efficient models for visual sound separation that use only a single video frame. These models are also able to take advantage of sound source category information during the separation process. We propose two models, one that assumes category labels are available during training and another that uses information about whether training sample pairs are from the same or different categories. Our experiments with the MUSIC dataset demonstrate that our models perform similarly or better compared to recent baseline methods. The code for our models is available at https://github.com/ly-zhu/Leveraging-Category-Information-for-Single-Frame-Visual-Sound-Source-Separation.",1
"Optical flow is the motion of a pixel between at least two consecutive video frames and can be estimated through an end-to-end trainable convolutional neural network. To this end, large training datasets are required to improve the accuracy of optical flow estimation. Our paper presents OmniFlow: a new synthetic omnidirectional human optical flow dataset. Based on a rendering engine we create a naturalistic 3D indoor environment with textured rooms, characters, actions, objects, illumination and motion blur where all components of the environment are shuffled during the data capturing process. The simulation has as output rendered images of household activities and the corresponding forward and backward optical flow. To verify the data for training volumetric correspondence networks for optical flow estimation we train different subsets of the data and test on OmniFlow with and without Test-Time-Augmentation. As a result we have generated 23,653 image pairs and corresponding forward and backward optical flow. Our dataset can be downloaded from: https://mytuc.org/byfs",0
"A convolutional neural network can estimate optical flow, which refers to the movement of a pixel between two video frames. However, in order to improve the accuracy of this estimation, it is necessary to have access to large training datasets. The purpose of our research is to introduce OmniFlow, a novel synthetic omnidirectional human optical flow dataset. We developed a 3D indoor environment with various elements such as characters, objects, and lighting, which were randomly shuffled during the data capturing process. Our simulation generated images of daily household activities along with the corresponding forward and backward optical flow. To validate the data for training volumetric correspondence networks for optical flow, we trained different subsets of the data and tested them on OmniFlow with and without Test-Time-Augmentation. This resulted in the creation of 23,653 image pairs and the corresponding forward and backward optical flow. Our dataset can be accessed via this link: https://mytuc.org/byfs.",1
"A majority of methods for video frame interpolation compute bidirectional optical flow between adjacent frames of a video, followed by a suitable warping algorithm to generate the output frames. However, approaches relying on optical flow often fail to model occlusions and complex non-linear motions directly from the video and introduce additional bottlenecks unsuitable for widespread deployment. We address these limitations with FLAVR, a flexible and efficient architecture that uses 3D space-time convolutions to enable end-to-end learning and inference for video frame interpolation. Our method efficiently learns to reason about non-linear motions, complex occlusions and temporal abstractions, resulting in improved performance on video interpolation, while requiring no additional inputs in the form of optical flow or depth maps. Due to its simplicity, FLAVR can deliver 3x faster inference speed compared to the current most accurate method on multi-frame interpolation without losing interpolation accuracy. In addition, we evaluate FLAVR on a wide range of challenging settings and consistently demonstrate superior qualitative and quantitative results compared with prior methods on various popular benchmarks including Vimeo-90K, UCF101, DAVIS, Adobe, and GoPro. Finally, we demonstrate that FLAVR for video frame interpolation can serve as a useful self-supervised pretext task for action recognition, optical flow estimation, and motion magnification.",0
"The majority of approaches to video frame interpolation involve calculating bidirectional optical flow between adjacent frames and applying a suitable warping algorithm to generate output frames. However, these methods often struggle to model occlusions and complex non-linear motions directly from the video, which can lead to additional bottlenecks that hinder widespread implementation. To overcome these limitations, we present FLAVR, a flexible and efficient architecture that employs 3D space-time convolutions for end-to-end learning and inference in video frame interpolation. FLAVR is capable of effectively reasoning about non-linear motions, complex occlusions, and temporal abstractions, resulting in improved performance without the need for optical flow or depth maps. FLAVR is also simpler and faster than current state-of-the-art methods, delivering 3x faster inference speeds without sacrificing accuracy. Our method outperforms prior methods on various benchmarks, including Vimeo-90K, UCF101, DAVIS, Adobe, and GoPro. Additionally, we demonstrate that FLAVR can serve as a self-supervised pretext task for action recognition, optical flow estimation, and motion magnification.",1
"A common strategy to video understanding is to incorporate spatial and motion information by fusing features derived from RGB frames and optical flow. In this work, we introduce a new way to leverage semantic segmentation as an intermediate representation for video understanding and use it in a way that requires no additional labeling.   Second, we propose a general framework which learns the intermediate representations (optical flow and semantic segmentation) jointly with the final video understanding task and allows the adaptation of the representations to the end goal. Despite the use of intermediate representations within the network, during inference, no additional data beyond RGB sequences is needed, enabling efficient recognition with a single network.   Finally, we present a way to find the optimal learning configuration by searching the best loss weighting via evolution. We obtain more powerful visual representations for videos which lead to performance gains over the state-of-the-art.",0
"An approach commonly used for video comprehension involves merging features obtained from RGB frames and optical flow to integrate spatial and motion information. Our research proposes a novel method of utilizing semantic segmentation as an intermediary for video comprehension, eliminating the need for additional labeling. Additionally, we introduce a comprehensive system that cohesively learns the intermediate representations (optical flow and semantic segmentation) alongside the final video comprehension task, allowing for customization to the end goal. Despite the use of intermediate representations within the network, our approach requires only RGB sequences during inference, resulting in efficient recognition with a single network. Finally, we utilize evolution to identify the optimal learning configuration, resulting in more robust visual representations for videos that surpass the current state-of-the-art performance.",1
"We present a dense-indirect SLAM system using external dense optical flows as input. We extend the recent probabilistic visual odometry model VOLDOR [Min et al. CVPR'20], by incorporating the use of geometric priors to 1) robustly bootstrap estimation from monocular capture, while 2) seamlessly supporting stereo and/or RGB-D input imagery. Our customized back-end tightly couples our intermediate geometric estimates with an adaptive priority scheme managing the connectivity of an incremental pose graph. We leverage recent advances in dense optical flow methods to achieve accurate and robust camera pose estimates, while constructing fine-grain globally-consistent dense environmental maps. Our open source implementation [https://github.com/htkseason/VOLDOR] operates online at around 15 FPS on a single GTX1080Ti GPU.",0
"The article introduces a SLAM system that utilizes external dense optical flows as input. The system is an extension of the probabilistic visual odometry model VOLDOR, and includes the use of geometric priors to enable robust estimation from monocular capture and seamless support for stereo and/or RGB-D input imagery. The back-end of the system features an adaptive priority scheme that manages the connectivity of an incremental pose graph and tightly couples intermediate geometric estimates. Recent advancements in dense optical flow methods are leveraged to achieve accurate and robust camera pose estimates, while constructing fine-grain globally-consistent dense environmental maps. The open source implementation of the system operates online at around 15 FPS on a single GTX1080Ti GPU.",1
"We propose a dense indirect visual odometry method taking as input externally estimated optical flow fields instead of hand-crafted feature correspondences. We define our problem as a probabilistic model and develop a generalized-EM formulation for the joint inference of camera motion, pixel depth, and motion-track confidence. Contrary to traditional methods assuming Gaussian-distributed observation errors, we supervise our inference framework under an (empirically validated) adaptive log-logistic distribution model. Moreover, the log-logistic residual model generalizes well to different state-of-the-art optical flow methods, making our approach modular and agnostic to the choice of optical flow estimators. Our method achieved top-ranking results on both TUM RGB-D and KITTI odometry benchmarks. Our open-sourced implementation is inherently GPU-friendly with only linear computational and storage growth.",0
"Our proposed method for dense indirect visual odometry utilizes externally estimated optical flow fields as input instead of hand-crafted feature correspondences. We approach the problem as a probabilistic model and use a generalized-EM formulation for joint inference of camera motion, pixel depth, and motion-track confidence. Unlike traditional methods that assume Gaussian-distributed observation errors, our inference framework is supervised using an empirically validated adaptive log-logistic distribution model. Additionally, our log-logistic residual model is versatile and works well with different state-of-the-art optical flow methods, making it modular and independent of the optical flow estimator chosen. Our method achieved top-ranking results on both TUM RGB-D and KITTI odometry benchmarks, and our open-sourced implementation is GPU-friendly with only linear computational and storage growth.",1
"The optical flow estimation has been assessed in various applications. In this paper, we propose a novel method named motion edge structure difference(MESD) to assess estimation errors of optical flow fields on edge of motion objects. We implement comparison experiments for MESD by evaluating five representative optical flow algorithms on four popular benchmarks: MPI Sintel, Middlebury, KITTI 2012 and KITTI 2015. Our experimental results demonstrate that MESD can reasonably and discriminatively assess estimation errors of optical flow fields on motion edge. The results indicate that MESD could be a supplementary metric to existing general assessment metrics for evaluating optical flow algorithms in related computer vision applications.",0
"Various applications have utilized optical flow estimation, and this paper introduces a new approach called motion edge structure difference (MESD) to evaluate the errors in optical flow field estimation specifically on the edges of moving objects. To test MESD, we conducted comparison experiments on four popular benchmarks: MPI Sintel, Middlebury, KITTI 2012, and KITTI 2015, evaluating five representative optical flow algorithms. The results of our experiments show that MESD is an effective and accurate method for assessing the errors in optical flow field estimation on motion edges. Therefore, MESD can serve as a complementary metric in evaluating optical flow algorithms in computer vision applications alongside existing general assessment metrics.",1
"Event cameras are novel vision sensors that sample, in an asynchronous fashion, brightness increments with low latency and high temporal resolution. The resulting streams of events are of high value by themselves, especially for high speed motion estimation. However, a growing body of work has also focused on the reconstruction of intensity frames from the events, as this allows bridging the gap with the existing literature on appearance- and frame-based computer vision. Recent work has mostly approached this problem using neural networks trained with synthetic, ground-truth data. In this work we approach, for the first time, the intensity reconstruction problem from a self-supervised learning perspective. Our method, which leverages the knowledge of the inner workings of event cameras, combines estimated optical flow and the event-based photometric constancy to train neural networks without the need for any ground-truth or synthetic data. Results across multiple datasets show that the performance of the proposed self-supervised approach is in line with the state-of-the-art. Additionally, we propose a novel, lightweight neural network for optical flow estimation that achieves high speed inference with only a minor drop in performance.",0
"Event cameras are innovative vision sensors that gather brightness increments with high temporal resolution and low latency in an asynchronous manner. The resulting streams of events are highly valuable, especially for quick motion estimation. However, recent research has also focused on reconstructing intensity frames from the events to connect with the pre-existing computer vision literature on appearance- and frame-based methodologies. Prior studies have mostly used synthetic, ground-truth data to train neural networks for this purpose. Our work is unique in that we tackle the intensity reconstruction problem through self-supervised learning, using our understanding of event camera mechanics to combine estimated optical flow and event-based photometric constancy. This approach does not require any synthetic or ground-truth data. Our results across various datasets show that our self-supervised method aligns with the current state-of-the-art. Moreover, we propose a new, lightweight neural network for optical flow estimation that offers fast inference with only a slight decrease in performance.",1
"This paper deals with the scarcity of data for training optical flow networks, highlighting the limitations of existing sources such as labeled synthetic datasets or unlabeled real videos. Specifically, we introduce a framework to generate accurate ground-truth optical flow annotations quickly and in large amounts from any readily available single real picture. Given an image, we use an off-the-shelf monocular depth estimation network to build a plausible point cloud for the observed scene. Then, we virtually move the camera in the reconstructed environment with known motion vectors and rotation angles, allowing us to synthesize both a novel view and the corresponding optical flow field connecting each pixel in the input image to the one in the new frame. When trained with our data, state-of-the-art optical flow networks achieve superior generalization to unseen real data compared to the same models trained either on annotated synthetic datasets or unlabeled videos, and better specialization if combined with synthetic images.",0
"The paper discusses the lack of data available for training optical flow networks and the limitations of current sources. To address this issue, the authors present a framework that can quickly generate accurate ground-truth optical flow annotations in large quantities from any single real picture. The framework uses an off-the-shelf monocular depth estimation network to create a plausible point cloud for the scene in the image. From there, the camera is virtually moved in the reconstructed environment with known motion vectors and rotation angles to synthesize a new view and its corresponding optical flow field. When optical flow networks are trained with this data, they perform better on unseen real data compared to models trained on labeled synthetic datasets or unlabeled videos. Moreover, combining synthetic images with the framework results in better specialization.",1
"We present an unsupervised optical flow estimation method by proposing an adaptive pyramid sampling in the deep pyramid network. Specifically, in the pyramid downsampling, we propose an Content Aware Pooling (CAP) module, which promotes local feature gathering by avoiding cross region pooling, so that the learned features become more representative. In the pyramid upsampling, we propose an Adaptive Flow Upsampling (AFU) module, where cross edge interpolation can be avoided, producing sharp motion boundaries. Equipped with these two modules, our method achieves the best performance for unsupervised optical flow estimation on multiple leading benchmarks, including MPI-SIntel, KITTI 2012 and KITTI 2015. Particuarlly, we achieve EPE=1.5 on KITTI 2012 and F1=9.67% KITTI 2015, which outperform the previous state-of-the-art methods by 16.7% and 13.1%, respectively.",0
"Our study introduces an innovative optical flow estimation approach that employs an adaptive pyramid sampling technique within a deep pyramid network. To enhance local feature gathering, we propose a Content Aware Pooling (CAP) module during pyramid downsampling, which avoids cross region pooling and facilitates the development of more representative learned features. Additionally, we present an Adaptive Flow Upsampling (AFU) module during pyramid upsampling, which eliminates cross edge interpolation and sharpens motion boundaries. The inclusion of these two modules enables our method to achieve optimal performance for unsupervised optical flow estimation on prominent benchmarks, such as MPI-SIntel, KITTI 2012, and KITTI 2015. Specifically, our approach achieves EPE=1.5 on KITTI 2012 and F1=9.67% on KITTI 2015, surpassing previous state-of-the-art methods by 16.7% and 13.1%, respectively.",1
"Video inpainting aims to fill spatio-temporal ""corrupted"" regions with plausible content. To achieve this goal, it is necessary to find correspondences from neighbouring frames to faithfully hallucinate the unknown content. Current methods achieve this goal through attention, flow-based warping, or 3D temporal convolution. However, flow-based warping can create artifacts when optical flow is not accurate, while temporal convolution may suffer from spatial misalignment. We propose 'Progressive Temporal Feature Alignment Network', which progressively enriches features extracted from the current frame with the feature warped from neighbouring frames using optical flow. Our approach corrects the spatial misalignment in the temporal feature propagation stage, greatly improving visual quality and temporal consistency of the inpainted videos. Using the proposed architecture, we achieve state-of-the-art performance on the DAVIS and FVI datasets compared to existing deep learning approaches. Code is available at https://github.com/MaureenZOU/TSAM.",0
"The objective of video inpainting is to fill in damaged areas in space and time with believable content. This requires identifying links between adjacent frames to accurately imagine the missing content. Existing methods rely on attention, flow-based warping, or 3D temporal convolution, but these can lead to issues such as artifacts or spatial misalignment. Our solution, the 'Progressive Temporal Feature Alignment Network', enhances features from the current frame with feature warping from nearby frames using optical flow. This fixes spatial misalignment during temporal feature propagation and significantly improves the visual quality and consistency of inpainted videos. Our approach outperforms other deep learning techniques on the DAVIS and FVI datasets, and the code is available at https://github.com/MaureenZOU/TSAM.",1
"We propose an unsupervised method for detecting and tracking moving objects in 3D, in unlabelled RGB-D videos. The method begins with classic handcrafted techniques for segmenting objects using motion cues: we estimate optical flow and camera motion, and conservatively segment regions that appear to be moving independently of the background. Treating these initial segments as pseudo-labels, we learn an ensemble of appearance-based 2D and 3D detectors, under heavy data augmentation. We use this ensemble to detect new instances of the ""moving"" type, even if they are not moving, and add these as new pseudo-labels. Our method is an expectation-maximization algorithm, where in the expectation step we fire all modules and look for agreement among them, and in the maximization step we re-train the modules to improve this agreement. The constraint of ensemble agreement helps combat contamination of the generated pseudo-labels (during the E step), and data augmentation helps the modules generalize to yet-unlabelled data (during the M step). We compare against existing unsupervised object discovery and tracking methods, using challenging videos from CATER and KITTI, and show strong improvements over the state-of-the-art.",0
"Our proposal introduces an unsupervised approach to identify and trace mobile objects in 3D without labels in RGB-D videos. Initially, we utilize conventional handcrafted methods to segment the objects using motion clues; we gauge optical flow and camera motion, and cautiously segment regions that seem to be in motion independently of the background. We then use these segmented regions as pseudo-labels to train an ensemble of appearance-based 2D and 3D detectors, with extensive data augmentation. This ensemble is employed to identify new instances of the ""moving"" type, even if they are stationary, and include them as new pseudo-labels. Our method is an expectation-maximization algorithm, where we activate all the modules to find agreement among them in the expectation step, and retrain the modules to enhance this agreement in the maximization step. The ensemble agreement constraint aids in combating the contamination of the generated pseudo-labels during the E step, while data augmentation helps to generalize the modules to unlabelled data during the M step. We compare our approach to existing unsupervised object discovery and tracking methods, using challenging videos from CATER and KITTI, and demonstrate significant improvements over the state-of-the-art.",1
"We address the problem of scene flow: given a pair of stereo or RGB-D video frames, estimate pixelwise 3D motion. We introduce RAFT-3D, a new deep architecture for scene flow. RAFT-3D is based on the RAFT model developed for optical flow but iteratively updates a dense field of pixelwise SE3 motion instead of 2D motion. A key innovation of RAFT-3D is rigid-motion embeddings, which represent a soft grouping of pixels into rigid objects. Integral to rigid-motion embeddings is Dense-SE3, a differentiable layer that enforces geometric consistency of the embeddings. Experiments show that RAFT-3D achieves state-of-the-art performance. On FlyingThings3D, under the two-view evaluation, we improved the best published accuracy (d < 0.05) from 34.3% to 83.7%. On KITTI, we achieve an error of 5.77, outperforming the best published method (6.31), despite using no object instance supervision. Code is available at https://github.com/princeton-vl/RAFT-3D.",0
"Our focus is on solving the challenge of scene flow. This involves determining the 3D motion of each pixel in a pair of video frames that are either stereo or RGB-D. Our solution is RAFT-3D, a novel deep architecture designed specifically for scene flow. RAFT-3D is built upon RAFT, a model that was originally developed for optical flow. However, instead of estimating 2D motion, RAFT-3D iteratively updates a dense field of pixelwise SE3 motion. A key feature of our model is the use of rigid-motion embeddings, which enable us to group pixels into rigid objects in a flexible manner. To enforce geometric consistency of these embeddings, we use Dense-SE3, a differentiable layer. Our experiments demonstrate that RAFT-3D delivers state-of-the-art performance, surpassing the accuracy of the best published results on both FlyingThings3D and KITTI datasets. Our code is available on GitHub at https://github.com/princeton-vl/RAFT-3D.",1
"While single-image super-resolution (SISR) has attracted substantial interest in recent years, the proposed approaches are limited to learning image priors in order to add high frequency details. In contrast, multi-frame super-resolution (MFSR) offers the possibility of reconstructing rich details by combining signal information from multiple shifted images. This key advantage, along with the increasing popularity of burst photography, have made MFSR an important problem for real-world applications.   We propose a novel architecture for the burst super-resolution task. Our network takes multiple noisy RAW images as input, and generates a denoised, super-resolved RGB image as output. This is achieved by explicitly aligning deep embeddings of the input frames using pixel-wise optical flow. The information from all frames are then adaptively merged using an attention-based fusion module. In order to enable training and evaluation on real-world data, we additionally introduce the BurstSR dataset, consisting of smartphone bursts and high-resolution DSLR ground-truth. We perform comprehensive experimental analysis, demonstrating the effectiveness of the proposed architecture.",0
"Although single-image super-resolution (SISR) has been a topic of interest in recent years, the methods proposed have been restricted to learning image priors to add high frequency details. On the other hand, multi-frame super-resolution (MFSR) allows for the reconstruction of rich details by combining signal information from multiple shifted images, making it an essential task for real-world applications due to the increasing popularity of burst photography. We propose a new structure for burst super-resolution, which takes multiple noisy RAW images as input and outputs a denoised, super-resolved RGB image by aligning deep embeddings of input frames using pixel-wise optical flow. An attention-based fusion module is then utilized to adaptively merge information from all frames. To enable training and evaluation on real-world data, we introduce the BurstSR dataset, containing smartphone bursts and high-resolution DSLR ground-truth. Through comprehensive experimental analysis, we demonstrate the effectiveness of our proposed architecture.",1
"Heart beat rhythm and heart rate (HR) are important physiological parameters of the human body. This study presents an efficient multi-hierarchical spatio-temporal convolutional network that can quickly estimate remote physiological (rPPG) signal and HR from face video clips. First, the facial color distribution characteristics are extracted using a low-level face feature Generation (LFFG) module. Then, the three-dimensional (3D) spatio-temporal stack convolution module (STSC) and multi-hierarchical feature fusion module (MHFF) are used to strengthen the spatio-temporal correlation of multi-channel features. In the MHFF, sparse optical flow is used to capture the tiny motion information of faces between frames and generate a self-adaptive region of interest (ROI) skin mask. Finally, the signal prediction module (SP) is used to extract the estimated rPPG signal. The experimental results on the three datasets show that the proposed network outperforms the state-of-the-art methods.",0
"The human body's physiological parameters, namely heart beat rhythm and heart rate (HR), hold significant importance. This research introduces a highly effective multi-hierarchical spatio-temporal convolutional network that can quickly compute remote physiological (rPPG) signal and HR from face video clips. Initially, a low-level face feature Generation (LFFG) module extracts the facial color distribution characteristics. Subsequently, the spatio-temporal stack convolution module (STSC) and multi-hierarchical feature fusion module (MHFF) are employed to enhance the spatio-temporal correlation of multi-channel features. Sparse optical flow is used in the MHFF to capture minute motion information of faces between frames and generate a self-adaptive region of interest (ROI) skin mask. Finally, the signal prediction module (SP) is utilized to extract the estimated rPPG signal. The experimental outcomes on the three datasets demonstrate that the proposed network surpasses the state-of-the-art techniques.",1
"State-of-the-art neural network models for optical flow estimation require a dense correlation volume at high resolutions for representing per-pixel displacement. Although the dense correlation volume is informative for accurate estimation, its heavy computation and memory usage hinders the efficient training and deployment of the models. In this paper, we show that the dense correlation volume representation is redundant and accurate flow estimation can be achieved with only a fraction of elements in it. Based on this observation, we propose an alternative displacement representation, named Sparse Correlation Volume, which is constructed directly by computing the k closest matches in one feature map for each feature vector in the other feature map and stored in a sparse data structure. Experiments show that our method can reduce computational cost and memory use significantly, while maintaining high accuracy compared to previous approaches with dense correlation volumes. Code is available at https://github.com/zacjiang/scv .",0
"Advanced neural network models for estimating optical flow require a comprehensive correlation volume with high resolution to represent displacement at every pixel. While this volume is informative for accurate estimation, the heavy computation and memory usage it requires pose obstacles to efficient training and deployment of the models. This study proposes an alternative displacement representation, called Sparse Correlation Volume, which is based on the observation that accurate flow estimation can be achieved with only a fraction of the elements in the dense correlation volume. The Sparse Correlation Volume is constructed by computing the k closest matches in one feature map for each feature vector in the other feature map and stored in a sparse data structure. Experiments show that this method significantly reduces computational cost and memory use while maintaining high accuracy compared to previous approaches. The code for this study is available at https://github.com/zacjiang/scv.",1
"The feature correlation layer serves as a key neural network module in numerous computer vision problems that involve dense correspondences between image pairs. It predicts a correspondence volume by evaluating dense scalar products between feature vectors extracted from pairs of locations in two images. However, this point-to-point feature comparison is insufficient when disambiguating multiple similar regions in an image, severely affecting the performance of the end task. We propose GOCor, a fully differentiable dense matching module, acting as a direct replacement to the feature correlation layer. The correspondence volume generated by our module is the result of an internal optimization procedure that explicitly accounts for similar regions in the scene. Moreover, our approach is capable of effectively learning spatial matching priors to resolve further matching ambiguities. We analyze our GOCor module in extensive ablative experiments. When integrated into state-of-the-art networks, our approach significantly outperforms the feature correlation layer for the tasks of geometric matching, optical flow, and dense semantic matching. The code and trained models will be made available at github.com/PruneTruong/GOCor.",0
"In computer vision problems that require dense correspondences between image pairs, the feature correlation layer is a crucial neural network module. This layer predicts a correspondence volume by calculating dense scalar products between feature vectors from pairs of locations in two images. However, this method falls short when it comes to distinguishing between multiple similar regions in an image, which negatively impacts the overall performance of the task. To address this issue, we propose a fully differentiable dense matching module called GOCor, which can act as a direct replacement to the feature correlation layer. Our module generates a correspondence volume through an internal optimization process that takes into account similar regions in the scene. Additionally, our approach is capable of learning spatial matching priors to further resolve any ambiguities. We conducted extensive ablative experiments to analyze our GOCor module, and we found that it outperforms the feature correlation layer when integrated into state-of-the-art networks for tasks such as geometric matching, optical flow, and dense semantic matching. To access the code and trained models, please visit github.com/PruneTruong/GOCor.",1
"Establishing dense correspondences between a pair of images is an important and general problem, covering geometric matching, optical flow and semantic correspondences. While these applications share fundamental challenges, such as large displacements, pixel-accuracy, and appearance changes, they are currently addressed with specialized network architectures, designed for only one particular task. This severely limits the generalization capabilities of such networks to new scenarios, where e.g. robustness to larger displacements or higher accuracy is required.   In this work, we propose a universal network architecture that is directly applicable to all the aforementioned dense correspondence problems. We achieve both high accuracy and robustness to large displacements by investigating the combined use of global and local correlation layers. We further propose an adaptive resolution strategy, allowing our network to operate on virtually any input image resolution. The proposed GLU-Net achieves state-of-the-art performance for geometric and semantic matching as well as optical flow, when using the same network and weights. Code and trained models are available at https://github.com/PruneTruong/GLU-Net.",0
"Establishing dense correspondences between a pair of images is a significant and broad problem that involves geometric matching, optical flow, and semantic correspondences. Despite having similar fundamental challenges, such as pixel-accuracy, large displacements, and appearance changes, specialized network architectures are currently being used for each application. As a result, these networks have limited generalization capabilities in new scenarios that require higher accuracy and robustness to larger displacements. This study proposes a universal network architecture that can be applied directly to all dense correspondence problems, achieving high accuracy and robustness to large displacements by utilizing global and local correlation layers. Moreover, the study proposes an adaptive resolution strategy that enables the network to operate on any input image resolution. The GLU-Net proposed in this study achieves state-of-the-art performance for geometric and semantic matching as well as optical flow, using the same network and weights. The code and trained models are available at https://github.com/PruneTruong/GLU-Net.",1
"Semi-supervised video object segmentation (semi-VOS) is widely used in many applications. This task is tracking class-agnostic objects from a given target mask. For doing this, various approaches have been developed based on online-learning, memory networks, and optical flow. These methods show high accuracy but are hard to be utilized in real-world applications due to slow inference time and tremendous complexity. To resolve this problem, template matching methods are devised for fast processing speed but sacrificing lots of performance in previous models. We introduce a novel semi-VOS model based on a template matching method and a temporal consistency loss to reduce the performance gap from heavy models while expediting inference time a lot. Our template matching method consists of short-term and long-term matching. The short-term matching enhances target object localization, while long-term matching improves fine details and handles object shape-changing through the newly proposed adaptive template attention module. However, the long-term matching causes error-propagation due to the inflow of the past estimated results when updating the template. To mitigate this problem, we also propose a temporal consistency loss for better temporal coherence between neighboring frames by adopting the concept of a transition matrix. Our model obtains 79.5% J&F score at the speed of 73.8 FPS on the DAVIS16 benchmark. The code is available in https://github.com/HYOJINPARK/TTVOS.",0
"Semi-supervised video object segmentation (semi-VOS) is a commonly used technique in various applications to track class-agnostic objects from a target mask. To achieve this, several approaches have been developed, such as online-learning, memory networks, and optical flow, which exhibit high accuracy but are not practical for real-world use due to their slow inference time and complexity. Template matching methods have been proposed to address this issue but are known to sacrifice performance. In this study, we present a novel semi-VOS model that utilizes a template matching method with a temporal consistency loss to reduce the performance gap while significantly speeding up inference time. Our approach incorporates short-term and long-term matching to improve object localization and handle object shape-changing through the adaptive template attention module. However, the long-term matching can cause error-propagation during template updates, which we mitigate using a temporal consistency loss based on a transition matrix. Our model achieves a J&F score of 79.5% at a speed of 73.8 FPS on the DAVIS16 benchmark. The code is available at https://github.com/HYOJINPARK/TTVOS.",1
"Video interpolation aims to generate a non-existent intermediate frame given the past and future frames. Many state-of-the-art methods achieve promising results by estimating the optical flow between the known frames and then generating the backward flows between the middle frame and the known frames. However, these methods usually suffer from the inaccuracy of estimated optical flows and require additional models or information to compensate for flow estimation errors. Following the recent development in using deformable convolution (DConv) for video interpolation, we propose a light but effective model, called Pyramid Deformable Warping Network (PDWN). PDWN uses a pyramid structure to generate DConv offsets of the unknown middle frame with respect to the known frames through coarse-to-fine successive refinements. Cost volumes between warped features are calculated at every pyramid level to help the offset inference. At the finest scale, the two warped frames are adaptively blended to generate the middle frame. Lastly, a context enhancement network further enhances the contextual detail of the final output. Ablation studies demonstrate the effectiveness of the coarse-to-fine offset refinement, cost volumes, and DConv. Our method achieves better or on-par accuracy compared to state-of-the-art models on multiple datasets while the number of model parameters and the inference time are substantially less than previous models. Moreover, we present an extension of the proposed framework to use four input frames, which can achieve significant improvement over using only two input frames, with only a slight increase in the model size and inference time.",0
"The goal of video interpolation is to create a middle frame that doesn't exist based on the past and future frames. While many advanced techniques rely on estimating optical flow to generate backward flows between the known and middle frames, this approach often suffers from inaccuracies and requires additional models to account for errors. To address these shortcomings, we introduce the Pyramid Deformable Warping Network (PDWN), a lightweight model that uses a pyramid structure to generate deformable convolution offsets for the middle frame. The network calculates cost volumes between warped features at each level of the pyramid and blends the two warped frames to produce the final output. A context enhancement network further enhances the output's detail. Our ablation studies demonstrate that the coarse-to-fine offset refinement, cost volumes, and deformable convolution are effective. The PDWN model achieves comparable or better accuracy than state-of-the-art models on multiple datasets while requiring fewer model parameters and less inference time. Additionally, we present an extension of our approach that utilizes four input frames, resulting in significant improvements with only a slight increase in model size and inference time.",1
"We present a deep neural network (DNN) that uses both sensor data (gyroscope) and image content (optical flow) to stabilize videos through unsupervised learning. The network fuses optical flow with real/virtual camera pose histories into a joint motion representation. Next, the LSTM block infers the new virtual camera pose, and this virtual pose is used to generate a warping grid that stabilizes the frame. Novel relative motion representation as well as a multi-stage training process are presented to optimize our model without any supervision. To the best of our knowledge, this is the first DNN solution that adopts both sensor data and image for stabilization. We validate the proposed framework through ablation studies and demonstrated the proposed method outperforms the state-of-art alternative solutions via quantitative evaluations and a user study.",0
"Our study introduces a new approach to stabilize videos through unsupervised learning, utilizing a deep neural network (DNN) that incorporates both gyroscope sensor data and optical flow image content. The DNN combines optical flow with camera pose histories to create a motion representation, and the LSTM block calculates a new virtual camera pose, which generates a warping grid for frame stabilization. A novel relative motion representation and multi-stage training process are utilized to optimize the model without supervision. Our study is the first to use both sensor data and image for stabilization. We validate the effectiveness of our framework through ablation studies and show that it outperforms state-of-the-art solutions through quantitative evaluations and a user study.",1
"Abnormal event detection is a challenging task that requires effectively handling intricate features of appearance and motion. In this paper, we present an approach of detecting anomalies in videos by learning a novel LSTM based self-contained network on normal dense optical flow. Due to their sigmoid implementations, standard LSTM's forget gate is susceptible to overlooking and dismissing relevant content in long sequence tasks like abnormality detection. The forget gate mitigates participation of previous hidden state for computation of cell state prioritizing current input. In addition, the hyperbolic tangent activation of standard LSTMs sacrifices performance when a network gets deeper. To tackle these two limitations, we introduce a bi-gated, light LSTM cell by discarding the forget gate and introducing sigmoid activation. Specifically, the LSTM architecture we come up with fully sustains content from previous hidden state thereby enabling the trained model to be robust and make context-independent decision during evaluation. Removing the forget gate results in a simplified and undemanding LSTM cell with improved performance effectiveness and computational efficiency. Empirical evaluations show that the proposed bi-gated LSTM based network outperforms various LSTM based models verifying its effectiveness for abnormality detection and generalization tasks on CUHK Avenue and UCSD datasets.",0
"Detecting abnormal events is a difficult task that involves effectively managing complex features of appearance and motion. Our paper proposes a method for detecting anomalies in videos by training a new LSTM-based self-contained network on normal dense optical flow. Standard LSTMs, with their sigmoid implementation, can overlook important content in long sequence tasks like abnormality detection. The forget gate prioritizes current input over previous hidden state, which can lead to missed information. Additionally, the hyperbolic tangent activation of standard LSTMs can negatively impact performance when network depth increases. To address these issues, we introduce a bi-gated, light LSTM cell that eliminates the forget gate and introduces sigmoid activation. Our LSTM architecture fully maintains content from previous hidden states, allowing the model to make context-independent decisions during evaluation. Removing the forget gate simplifies the LSTM cell and improves performance and computational efficiency. Empirical evaluations demonstrate that our proposed bi-gated LSTM-based network outperforms various LSTM-based models, proving its effectiveness for abnormality detection and generalization tasks on CUHK Avenue and UCSD datasets.",1
"The estimation of optical flow is an ambiguous task due to the lack of correspondence at occlusions, shadows, reflections, lack of texture and changes in illumination over time. Thus, unsupervised methods face major challenges as they need to tune complex cost functions with several terms designed to handle each of these sources of ambiguity. In contrast, supervised methods avoid these challenges altogether by relying on explicit ground truth optical flow obtained directly from synthetic or real data. In the case of synthetic data, the ground truth provides an exact and explicit description of what optical flow to assign to a given scene. However, the domain gap between synthetic data and real data often limits the ability of a trained network to generalize. In the case of real data, the ground truth is obtained through multiple sensors and additional data processing, which might introduce persistent errors and contaminate it. As a solution to these issues, we introduce a novel method to build a training set of pseudo-real images that can be used to train optical flow in a supervised manner. Our dataset uses two unpaired frames from real data and creates pairs of frames by simulating random warps, occlusions with super-pixels, shadows and illumination changes, and associates them to their corresponding exact optical flow. We thus obtain the benefit of directly training on real data while having access to an exact ground truth. Training with our datasets on the Sintel and KITTI benchmarks is straightforward and yields models on par or with state of the art performance compared to much more sophisticated training approaches.",0
"Optical flow estimation is a challenging task as there are many sources of ambiguity, including occlusions, shadows, reflections, lack of texture, and changes in illumination over time. Unsupervised methods must tune complex cost functions to handle each of these sources, while supervised methods rely on explicit ground truth optical flow obtained from synthetic or real data. However, synthetic data has a domain gap with real data, limiting a trained network's ability to generalize, and real data may introduce persistent errors. To address these issues, we propose a new method to build a training set of pseudo-real images that simulates random warps, occlusions, shadows, and illumination changes and associates them with their corresponding exact optical flow. Training with our dataset yields models on par with state-of-the-art performance compared to more complex approaches on the Sintel and KITTI benchmarks.",1
"Establishing dense correspondences between a pair of images is an important and general problem. However, dense flow estimation is often inaccurate in the case of large displacements or homogeneous regions. For most applications and down-stream tasks, such as pose estimation, image manipulation, or 3D reconstruction, it is crucial to know when and where to trust the estimated matches.   In this work, we aim to estimate a dense flow field relating two images, coupled with a robust pixel-wise confidence map indicating the reliability and accuracy of the prediction. We develop a flexible probabilistic approach that jointly learns the flow prediction and its uncertainty. In particular, we parametrize the predictive distribution as a constrained mixture model, ensuring better modelling of both accurate flow predictions and outliers. Moreover, we develop an architecture and training strategy tailored for robust and generalizable uncertainty prediction in the context of self-supervised training. Our approach obtains state-of-the-art results on multiple challenging geometric matching and optical flow datasets. We further validate the usefulness of our probabilistic confidence estimation for the task of pose estimation. Code and models are available at https://github.com/PruneTruong/PDCNet.",0
"The task of establishing dense correspondences between a pair of images is crucial yet challenging due to inaccurate dense flow estimation in certain scenarios, such as large displacements or homogeneous regions. It is essential to determine the reliability and accuracy of the estimated matches for downstream tasks like pose estimation, image manipulation, or 3D reconstruction. This study proposes a flexible probabilistic approach that simultaneously estimates a dense flow field and a pixel-wise confidence map to indicate the prediction's accuracy and dependability. The predictive distribution is parametrized as a constrained mixture model to model accurate flow predictions and outliers better. The researchers developed an architecture and training strategy suitable for robust and generalizable uncertainty prediction during self-supervised training. The proposed approach outperformed state-of-the-art techniques on several challenging geometric matching and optical flow datasets. The results also demonstrate the usefulness of probabilistic confidence estimation for pose estimation. The code and models are available at https://github.com/PruneTruong/PDCNet.",1
"Two-view structure-from-motion (SfM) is the cornerstone of 3D reconstruction and visual SLAM. Existing deep learning-based approaches formulate the problem by either recovering absolute pose scales from two consecutive frames or predicting a depth map from a single image, both of which are ill-posed problems. In contrast, we propose to revisit the problem of deep two-view SfM by leveraging the well-posedness of the classic pipeline. Our method consists of 1) an optical flow estimation network that predicts dense correspondences between two frames; 2) a normalized pose estimation module that computes relative camera poses from the 2D optical flow correspondences, and 3) a scale-invariant depth estimation network that leverages epipolar geometry to reduce the search space, refine the dense correspondences, and estimate relative depth maps. Extensive experiments show that our method outperforms all state-of-the-art two-view SfM methods by a clear margin on KITTI depth, KITTI VO, MVS, Scenes11, and SUN3D datasets in both relative pose and depth estimation.",0
"The foundation of 3D reconstruction and visual SLAM is the Two-view structure-from-motion (SfM). However, current deep learning-based approaches have encountered the issue of ill-posed problems in recovering absolute pose scales from two consecutive frames or predicting a depth map from a single image. To address this issue, we propose a new approach to deep two-view SfM that utilizes the well-posedness of the classic pipeline. Our method involves three steps: 1) using an optical flow estimation network to predict dense correspondences between two frames, 2) utilizing a normalized pose estimation module to compute relative camera poses from the 2D optical flow correspondences, and 3) employing a scale-invariant depth estimation network that leverages epipolar geometry to refine the dense correspondences, reduce the search space, and estimate relative depth maps. Our experiments demonstrate that our method surpasses all state-of-the-art two-view SfM methods on KITTI depth, KITTI VO, MVS, Scenes11, and SUN3D datasets in both relative pose and depth estimation.",1
"The cost volume, capturing the similarity of possible correspondences across two input images, is a key ingredient in state-of-the-art optical flow approaches. When sampling for correspondences to build the cost volume, a large neighborhood radius is required to deal with large displacements, introducing a significant computational burden. To address this, a sequential strategy is usually adopted, where correspondence sampling in a local neighborhood with a small radius suffices. However, such sequential approaches, instantiated by either a pyramid structure over a deep neural network's feature hierarchy or by a recurrent neural network, are slow due to the inherent need for sequential processing of cost volumes. In this paper, we propose dilated cost volumes to capture small and large displacements simultaneously, allowing optical flow estimation without the need for the sequential estimation strategy. To process the cost volume to get pixel-wise optical flow, existing approaches employ 2D or separable 4D convolutions, which we show either suffer from high GPU memory consumption, inferior accuracy, or large model size. Therefore, we propose using 3D convolutions for cost volume filtering to address these issues. By combining the dilated cost volumes and 3D convolutions, our proposed model DCVNet not only exhibits real-time inference (71 fps on a mid-end 1080ti GPU) but is also compact and obtains comparable accuracy to existing approaches.",0
"An essential component of modern optical flow methods is the cost volume, which captures the similarity of possible matches between two input images. However, a large neighborhood radius is needed to handle significant displacements when sampling for correspondences to construct the cost volume, resulting in a considerable computational burden. Typically, a sequential approach is employed, using correspondence sampling within a small radius to address this issue. Nevertheless, such strategies, implemented either through a pyramid structure over a deep neural network's feature hierarchy or a recurrent neural network, are slow due to the need for sequential processing of cost volumes. Our study proposes dilated cost volumes to simultaneously capture small and large displacements, allowing optical flow estimation without the requirement of sequential estimation. To obtain pixel-wise optical flow from the cost volume, current methods use either 2D or separable 4D convolutions, which we demonstrate to consume high GPU memory, produce inferior accuracy, or result in large model size. To resolve these problems, we suggest using 3D convolutions for cost volume filtering. By combining dilated cost volumes and 3D convolutions, our proposed model, DCVNet, achieves real-time inference (71 fps on a mid-end 1080ti GPU), is compact, and achieves comparable accuracy to existing approaches.",1
"Denoisers trained with synthetic data often fail to cope with the diversity of unknown noises, giving way to methods that can adapt to existing noise without knowing its ground truth. Previous image-based method leads to noise overfitting if directly applied to video denoisers, and has inadequate temporal information management especially in terms of occlusion and lighting variation, which considerably hinders its denoising performance. In this paper, we propose a general framework for video denoising networks that successfully addresses these challenges. A novel twin sampler assembles training data by decoupling inputs from targets without altering semantics, which not only effectively solves the noise overfitting problem, but also generates better occlusion masks efficiently by checking optical flow consistency. An online denoising scheme and a warping loss regularizer are employed for better temporal alignment. Lighting variation is quantified based on the local similarity of aligned frames. Our method consistently outperforms the prior art by 0.6-3.2dB PSNR on multiple noises, datasets and network architectures. State-of-the-art results on reducing model-blind video noises are achieved. Extensive ablation studies are conducted to demonstrate the significance of each technical components.",0
"Denoisers that are trained using synthetic data often struggle to handle the variety of unknown noises, prompting the need for methods that can adapt to existing noise without requiring knowledge of its ground truth. When applied directly to video denoisers, previous image-based methods lead to noise overfitting and poor temporal information management, which ultimately impede their denoising performance, especially in instances of occlusion and lighting variation. To overcome these challenges, this paper presents a general framework for video denoising networks that includes a twin sampler for assembling training data, an online denoising scheme, a warping loss regularizer, and a method for quantifying lighting variation based on local similarity of aligned frames. Our approach consistently outperforms previous methods, achieving state-of-the-art results in reducing model-blind video noises. We also conduct extensive ablation studies to highlight the significance of each technical component.",1
"Most successful self-supervised learning methods are trained to align the representations of two independent views from the data. State-of-the-art methods in video are inspired by image techniques, where these two views are similarly extracted by cropping and augmenting the resulting crop. However, these methods miss a crucial element in the video domain: time. We introduce BraVe, a self-supervised learning framework for video. In BraVe, one of the views has access to a narrow temporal window of the video while the other view has a broad access to the video content. Our models learn to generalise from the narrow view to the general content of the video. Furthermore, BraVe processes the views with different backbones, enabling the use of alternative augmentations or modalities into the broad view such as optical flow, randomly convolved RGB frames, audio or their combinations. We demonstrate that BraVe achieves state-of-the-art results in self-supervised representation learning on standard video and audio classification benchmarks including UCF101, HMDB51, Kinetics, ESC-50 and AudioSet.",0
"Self-supervised learning methods that are successful typically train to align the representations of two independent data views. In video, the current state-of-the-art methods are based on image techniques, where the two views are extracted in a similar manner by cropping and augmenting the resulting crop. However, these methods do not account for time, which is a critical element in the video domain. To address this issue, we have developed BraVe, a self-supervised learning framework specifically designed for video. In BraVe, one of the views has a narrow temporal window of the video, while the other view has broad access to the video content. Our models learn to generalize from the narrow view to the general content of the video. Additionally, BraVe processes the views with different backbones, which allows for the use of alternative augmentations or modalities, such as optical flow, randomly convolved RGB frames, audio, or their combinations, into the broad view. We show that BraVe achieves state-of-the-art results in self-supervised representation learning on standard video and audio classification benchmarks, including UCF101, HMDB51, Kinetics, ESC-50, and AudioSet.",1
"Temporal action localization (TAL) is a fundamental yet challenging task in video understanding. Existing TAL methods rely on pre-training a video encoder through action classification supervision. This results in a task discrepancy problem for the video encoder -- trained for action classification, but used for TAL. Intuitively, end-to-end model optimization is a good solution. However, this is not operable for TAL subject to the GPU memory constraints, due to the prohibitive computational cost in processing long untrimmed videos. In this paper, we resolve this challenge by introducing a novel low-fidelity end-to-end (LoFi) video encoder pre-training method. Instead of always using the full training configurations for TAL learning, we propose to reduce the mini-batch composition in terms of temporal, spatial or spatio-temporal resolution so that end-to-end optimization for the video encoder becomes operable under the memory conditions of a mid-range hardware budget. Crucially, this enables the gradient to flow backward through the video encoder from a TAL loss supervision, favourably solving the task discrepancy problem and providing more effective feature representations. Extensive experiments show that the proposed LoFi pre-training approach can significantly enhance the performance of existing TAL methods. Encouragingly, even with a lightweight ResNet18 based video encoder in a single RGB stream, our method surpasses two-stream ResNet50 based alternatives with expensive optical flow, often by a good margin.",0
"The task of temporal action localization (TAL) is difficult, but essential for video comprehension. Current TAL methods rely on pre-training a video encoder using action classification supervision, which creates a problem as the encoder is trained for one task but used for another. While an end-to-end model optimization seems like an intuitive solution, it is not feasible for TAL due to the high computational cost of processing long untrimmed videos. To address this issue, we propose a low-fidelity end-to-end (LoFi) video encoder pre-training method that reduces the mini-batch composition to make end-to-end optimization possible with mid-range hardware. This enables the gradient to flow backward through the video encoder from a TAL loss supervision, solving the task discrepancy problem and providing more effective feature representations. Our experiments show that the proposed LoFi pre-training approach can significantly improve the performance of existing TAL methods, and even outperforms two-stream ResNet50 based alternatives with expensive optical flow using a lightweight ResNet18 based video encoder in a single RGB stream.",1
"Recent work demonstrated the lack of robustness of optical flow networks to physical, patch-based adversarial attacks. The possibility to physically attack a basic component of automotive systems is a reason for serious concerns. In this paper, we analyze the cause of the problem and show that the lack of robustness is rooted in the classical aperture problem of optical flow estimation in combination with bad choices in the details of the network architecture. We show how these mistakes can be rectified in order to make optical flow networks robust to physical, patch-based attacks.",0
"Optical flow networks have been found to lack robustness against physical, patch-based adversarial attacks, which is a cause for concern, especially for automotive systems. This paper delves into the root cause of the problem and identifies the classical aperture problem in optical flow estimation and the poor network architecture choices as the culprits. We propose solutions to rectify these mistakes and enhance the robustness of optical flow networks against physical, patch-based attacks.",1
"In this paper, we focus on designing effective method for fast and accurate scene parsing. A common practice to improve the performance is to attain high resolution feature maps with strong semantic representation. Two strategies are widely used -- atrous convolutions and feature pyramid fusion, are either computation intensive or ineffective. Inspired by the Optical Flow for motion alignment between adjacent video frames, we propose a Flow Alignment Module (FAM) to learn Semantic Flow between feature maps of adjacent levels, and broadcast high-level features to high resolution features effectively and efficiently. Furthermore, integrating our module to a common feature pyramid structure exhibits superior performance over other real-time methods even on light-weight backbone networks, such as ResNet-18. Extensive experiments are conducted on several challenging datasets, including Cityscapes, PASCAL Context, ADE20K and CamVid. Especially, our network is the first to achieve 80.4\% mIoU on Cityscapes with a frame rate of 26 FPS. The code is available at \url{https://github.com/lxtGH/SFSegNets}.",0
"The objective of this study is to devise a proficient approach for swift and precise scene parsing. To enhance the overall performance, it is common practice to obtain feature maps with high resolution and robust semantic representation. However, the two widely used strategies - atrous convolutions and feature pyramid fusion - either demand high computational power or are ineffective. Thus, taking inspiration from Optical Flow for aligning motion between adjacent video frames, we have proposed the Flow Alignment Module (FAM) to learn the Semantic Flow between feature maps of adjacent levels and efficiently broadcast high-level features to high-resolution features. Furthermore, incorporating our module into a conventional feature pyramid structure yields superior performance even on lightweight backbone networks like ResNet-18. We have conducted extensive experiments on various challenging datasets, including Cityscapes, PASCAL Context, ADE20K, and CamVid. Notably, our network has achieved an mIoU of 80.4\% on Cityscapes at a frame rate of 26 FPS, making it the first to do so. The code for this project is available at \url{https://github.com/lxtGH/SFSegNets}.",1
"Despite learning-based visual odometry (VO) has shown impressive results in recent years, the pretrained networks may easily collapse in unseen environments. The large domain gap between training and testing data makes them difficult to generalize to new scenes. In this paper, we propose an online adaptation framework for deep VO with the assistance of scene-agnostic geometric computations and Bayesian inference. In contrast to learning-based pose estimation, our method solves pose from optical flow and depth while the single-view depth estimation is continuously improved with new observations by online learned uncertainties. Meanwhile, an online learned photometric uncertainty is used for further depth and pose optimization by a differentiable Gauss-Newton layer. Our method enables fast adaptation of deep VO networks to unseen environments in a self-supervised manner. Extensive experiments including Cityscapes to KITTI and outdoor KITTI to indoor TUM demonstrate that our method achieves state-of-the-art generalization ability among self-supervised VO methods.",0
"Although learning-based visual odometry (VO) has demonstrated impressive results recently, pretrained networks are vulnerable to collapsing in unfamiliar environments due to a significant domain gap between training and testing data, making it challenging to generalize to new scenes. In this study, we propose an online adaptation framework for deep VO that employs scene-agnostic geometric computations and Bayesian inference to address this issue. Unlike learning-based pose estimation, our approach utilizes optical flow and depth to solve pose while continuously enhancing single-view depth estimation with new observations through online learned uncertainties. In addition, a differentiable Gauss-Newton layer is employed for further depth and pose optimization using an online learned photometric uncertainty. Our self-supervised method allows deep VO networks to adapt rapidly to novel environments. We conducted extensive experiments, such as Cityscapes to KITTI and outdoor KITTI to indoor TUM, demonstrating that our approach achieves superior generalization performance compared to other self-supervised VO methods.",1
"We present an approach for high-resolution video frame prediction by conditioning on both past frames and past optical flows. Previous approaches rely on resampling past frames, guided by a learned future optical flow, or on direct generation of pixels. Resampling based on flow is insufficient because it cannot deal with disocclusions. Generative models currently lead to blurry results. Recent approaches synthesis a pixel by convolving input patches with a predicted kernel. However, their memory requirement increases with kernel size. Here, we spatially-displaced convolution (SDC) module for video frame prediction. We learn a motion vector and a kernel for each pixel and synthesize a pixel by applying the kernel at a displaced location in the source image, defined by the predicted motion vector. Our approach inherits the merits of both vector-based and kernel-based approaches, while ameliorating their respective disadvantages. We train our model on 428K unlabelled 1080p video game frames. Our approach produces state-of-the-art results, achieving an SSIM score of 0.904 on high-definition YouTube-8M videos, 0.918 on Caltech Pedestrian videos. Our model handles large motion effectively and synthesizes crisp frames with consistent motion.",0
"Our method for high-resolution video frame prediction involves conditioning on both past frames and past optical flows. Existing methods either resample past frames based on a learned future optical flow or directly generate pixels, but these approaches have limitations such as blurry results and increased memory requirements. Our solution, the spatially-displaced convolution (SDC) module, learns a motion vector and kernel for each pixel, and synthesizes a pixel by applying the kernel at a displaced location in the source image based on the predicted motion vector. This approach combines the benefits of both vector-based and kernel-based methods while mitigating their drawbacks. We trained our model on 428K unlabelled 1080p video game frames and achieved state-of-the-art results, with an SSIM score of 0.904 on high-definition YouTube-8M videos and 0.918 on Caltech Pedestrian videos. Our model effectively handles large motion and produces sharp, consistent frames.",1
"In this paper, we address the problem of estimating dense depth from a sequence of images using deep neural networks. Specifically, we employ a dense-optical-flow network to compute correspondences and then triangulate the point cloud to obtain an initial depth map.Parts of the point cloud, however, may be less accurate than others due to lack of common observations or small parallax. To further increase the triangulation accuracy, we introduce a depth-refinement network (DRN) that optimizes the initial depth map based on the image's contextual cues. In particular, the DRN contains an iterative refinement module (IRM) that improves the depth accuracy over iterations by refining the deep features. Lastly, the DRN also predicts the uncertainty in the refined depths, which is desirable in applications such as measurement selection for scene reconstruction. We show experimentally that our algorithm outperforms state-of-the-art approaches in terms of depth accuracy, and verify that our predicted uncertainty is highly correlated to the actual depth error.",0
"This paper discusses the challenge of accurately estimating dense depth from a sequence of images using deep neural networks. Initially, a dense-optical-flow network is employed to establish correspondences and triangulate the point cloud, resulting in an initial depth map. However, certain parts of the point cloud may be less precise due to insufficient common observations or minor parallax. To enhance the accuracy of triangulation, a depth-refinement network (DRN) is introduced, which uses contextual cues from the image to optimize the initial depth map. The DRN comprises an iterative refinement module (IRM) that fine-tunes the deep features over multiple iterations to improve depth accuracy. Additionally, the DRN predicts the uncertainty in refined depths, which is useful for applications like scene reconstruction. Experimental results demonstrate that our method outperforms existing techniques in terms of depth accuracy, and the predicted uncertainty is highly correlated with the actual depth error.",1
"Video object detection is a fundamental problem in computer vision and has a wide spectrum of applications. Based on deep networks, video object detection is actively studied for pushing the limits of detection speed and accuracy. To reduce the computation cost, we sparsely sample key frames in video and treat the rest frames are non-key frames; a large and deep network is used to extract features for key frames and a tiny network is used for non-key frames. To enhance the features of non-key frames, we propose a novel short-term feature aggregation method to propagate the rich information in key frame features to non-key frame features in a fast way. The fast feature aggregation is enabled by the freely available motion cues in compressed videos. Further, key frame features are also aggregated based on optical flow. The propagated deep features are then integrated with the directly extracted features for object detection. The feature extraction and feature integration parameters are optimized in an end-to-end manner. The proposed video object detection network is evaluated on the large-scale ImageNet VID benchmark and achieves 77.2\% mAP, which is on-par with state-of-the-art accuracy, at the speed of 30 FPS using a Titan X GPU. The source codes are available at \url{https://github.com/hustvl/LSFA}.",0
"Video object detection is a critical issue in computer vision that has a vast range of applications. With deep networks, video object detection is being actively researched to improve detection speed and accuracy. To decrease computation costs, we select key frames sparsely from the video and consider the remaining frames as non-key frames. We use a large, deep network to extract the features of key frames and a small network to extract the features of non-key frames. To enhance the features of non-key frames, we suggest a novel short-term feature aggregation method that quickly propagates the rich information in key frame features to non-key frame features. The motion cues available in compressed videos enable fast feature aggregation. Additionally, we aggregate key frame features based on optical flow. The propagated deep features are then combined with the directly extracted features for object detection. The parameters for feature extraction and integration are optimized end-to-end. The proposed video object detection network is evaluated on the ImageNet VID benchmark and achieves 77.2\% mAP, which is comparable to state-of-the-art accuracy, with a speed of 30 FPS on a Titan X GPU. The source codes can be found at \url{https://github.com/hustvl/LSFA}.",1
"Dense optical flow estimation plays a key role in many robotic vision tasks. In the past few years, with the advent of deep learning, we have witnessed great progress in optical flow estimation. However, current networks often consist of a large number of parameters and require heavy computation costs, largely hindering its application on low power-consumption devices such as mobile phones. In this paper, we tackle this challenge and design a lightweight model for fast and accurate optical flow prediction. Our proposed FastFlowNet follows the widely-used coarse-to-fine paradigm with following innovations. First, a new head enhanced pooling pyramid (HEPP) feature extractor is employed to intensify high-resolution pyramid features while reducing parameters. Second, we introduce a new center dense dilated correlation (CDDC) layer for constructing compact cost volume that can keep large search radius with reduced computation burden. Third, an efficient shuffle block decoder (SBD) is implanted into each pyramid level to accelerate flow estimation with marginal drops in accuracy. Experiments on both synthetic Sintel data and real-world KITTI datasets demonstrate the effectiveness of the proposed approach, which needs only 1/10 computation of comparable networks to achieve on par accuracy. In particular, FastFlowNet only contains 1.37M parameters; and can execute at 90 FPS (with a single GTX 1080Ti) or 5.7 FPS (embedded Jetson TX2 GPU) on a pair of Sintel images of resolution 1024x436.",0
"The estimation of dense optical flow plays a crucial role in various robotic vision tasks. Over the past few years, deep learning has led to significant advancements in optical flow estimation. However, the current networks are complicated and require a lot of computation, making it challenging to apply them to low-power devices such as mobile phones. This paper aims to address this challenge by proposing a lightweight model for quick and accurate optical flow prediction. Our proposed model, FastFlowNet, follows the conventional coarse-to-fine approach and includes several innovations. Firstly, we utilize a new head enhanced pooling pyramid (HEPP) feature extractor that strengthens high-resolution pyramid features while reducing parameters. Secondly, we introduce a new center dense dilated correlation (CDDC) layer that creates a compact cost volume with a large search radius and reduced computation burden. Thirdly, we incorporate an efficient shuffle block decoder (SBD) into each pyramid level to speed up flow estimation with minimal accuracy loss. Experiments on both synthetic Sintel data and real-world KITTI datasets demonstrate that our approach is effective, achieving comparable accuracy with only 1/10th the computation of comparable networks. FastFlowNet contains only 1.37M parameters and can perform at 90 FPS (with a single GTX 1080Ti) or 5.7 FPS (embedded Jetson TX2 GPU) on a pair of Sintel images with a resolution of 1024x436.",1
"One-sided facial paralysis causes uneven movements of facial muscles on the sides of the face. Physicians currently assess facial asymmetry in a subjective manner based on their clinical experience. This paper proposes a novel method to provide an objective and quantitative asymmetry score for frontal faces. Our metric has the potential to help physicians for diagnosis as well as monitoring the rehabilitation of patients with one-sided facial paralysis. A deep learning based landmark detection technique is used to estimate style invariant facial landmark points and dense optical flow is used to generate motion maps from a short sequence of frames. Six face regions are considered corresponding to the left and right parts of the forehead, eyes, and mouth. Motion is computed and compared between the left and the right parts of each region of interest to estimate the symmetry score. For testing, asymmetric sequences are synthetically generated from a facial expression dataset. A score equation is developed to quantify symmetry in both symmetric and asymmetric face sequences.",0
"Facial paralysis that affects only one side of the face can cause uneven movements of facial muscles, resulting in facial asymmetry. Currently, physicians rely on their clinical experience to subjectively assess facial asymmetry. However, this paper presents a new method that offers an objective and quantitative score for assessing frontal faces' asymmetry. Our metric has the potential to assist physicians in diagnosing and monitoring the rehabilitation of patients with one-sided facial paralysis. The proposed method employs a landmark detection technique based on deep learning to estimate style invariant facial landmark points. Additionally, dense optical flow is used to generate motion maps from a short sequence of frames. The proposed method considers six face regions corresponding to the left and right parts of the forehead, eyes, and mouth. By computing and comparing the motion between the left and right parts of each region of interest, the symmetry score is estimated. To test the method, asymmetric sequences are synthetically generated from a facial expression dataset. A score equation is developed to quantify symmetry in both symmetric and asymmetric face sequences.",1
"Surveillance anomaly detection searches for anomalous events, such as crimes or accidents, among normal scenes. Because it occurs rarely, most training data consists of unlabeled, normal videos, which makes the task challenging. Most existing methods use an autoencoder (AE) to learn reconstructing normal videos and detect anomalies by a failure to reconstruct the appearance of abnormal scenes. However, because anomalies are distinguished by appearance or motion, many previous approaches have explicitly separated appearance and motion information--for example, using a pre-trained optical flow model. This explicit separation restricts reciprocal representation capabilities between two information. In contrast, we propose an implicit two-path AE (ITAE), a structure in which two encoders implicitly model appearance and motion features, and a single decoder that combines them to learn normal video patterns. For the complex distribution of normal scenes, we suggest normal density estimation of ITAE features through normalizing flow (NF)-based generative models to learn the tractable likelihoods and find anomalies using out-of-distribution detection. NF models intensify ITAE performance by learning normality through implicitly learned features. Finally, we demonstrate the effectiveness of ITAE and its feature distribution modeling in three benchmarks, especially on the Shanghai Tech Campus (ST) database composed of various anomalies in real-world scenarios.",0
"Surveillance anomaly detection is the process of identifying irregular occurrences, such as accidents or crimes, among regular events. This task is challenging as there is little labeled data available, and most training data comprises unlabeled normal videos. Existing methods use autoencoders to detect anomalies by failing to reconstruct abnormal scenes. However, previous approaches have separated appearance and motion information, which limits reciprocal representation capabilities between the two. In contrast, we propose an implicit two-path autoencoder that models appearance and motion features with a single decoder. We suggest using normal density estimation of the autoencoder's features through normalizing flow-based generative models to detect anomalies. This approach improves the model's performance by implicitly learning normality through features. Finally, we demonstrate the effectiveness of our approach in three benchmarks, including the Shanghai Tech Campus database that contains various anomalies in real-world scenarios.",1
"Standard frame-based cameras that sample light intensity frames are heavily impacted by motion blur for high-speed motion and fail to perceive scene accurately when the dynamic range is high. Event-based cameras, on the other hand, overcome these limitations by asynchronously detecting the variation in individual pixel intensities. However, event cameras only provide information about pixels in motion, leading to sparse data. Hence, estimating the overall dense behavior of pixels is difficult. To address such issues associated with the sensors, we present Fusion-FlowNet, a sensor fusion framework for energy-efficient optical flow estimation using both frame- and event-based sensors, leveraging their complementary characteristics. Our proposed network architecture is also a fusion of Spiking Neural Networks (SNNs) and Analog Neural Networks (ANNs) where each network is designed to simultaneously process asynchronous event streams and regular frame-based images, respectively. Our network is end-to-end trained using unsupervised learning to avoid expensive video annotations. The method generalizes well across distinct environments (rapid motion and challenging lighting conditions) and demonstrates state-of-the-art optical flow prediction on the Multi-Vehicle Stereo Event Camera (MVSEC) dataset. Furthermore, our network offers substantial savings in terms of the number of network parameters and computational energy cost.",0
"When capturing high-speed motion or scenes with high dynamic range, standard frame-based cameras that sample light intensity frames suffer from motion blur and inaccurate perception. In contrast, event-based cameras detect variations in individual pixel intensities asynchronously, but only provide information about pixels in motion, resulting in sparse data. To overcome these limitations, we propose Fusion-FlowNet, a sensor fusion framework that combines frame- and event-based sensors to efficiently estimate optical flow. Our network architecture consists of Spiking Neural Networks and Analog Neural Networks, each designed to process event streams and frame-based images simultaneously. We train the network end-to-end using unsupervised learning and achieve state-of-the-art optical flow prediction on the MVSEC dataset. Our method also offers significant savings in terms of network parameters and computational energy cost.",1
"Predicting future frames for robotic surgical video is an interesting, important yet extremely challenging problem, given that the operative tasks may have complex dynamics. Existing approaches on future prediction of natural videos were based on either deterministic models or stochastic models, including deep recurrent neural networks, optical flow, and latent space modeling. However, the potential in predicting meaningful movements of robots with dual arms in surgical scenarios has not been tapped so far, which is typically more challenging than forecasting independent motions of one arm robots in natural scenarios. In this paper, we propose a ternary prior guided variational autoencoder (TPG-VAE) model for future frame prediction in robotic surgical video sequences. Besides content distribution, our model learns motion distribution, which is novel to handle the small movements of surgical tools. Furthermore, we add the invariant prior information from the gesture class into the generation process to constrain the latent space of our model. To our best knowledge, this is the first time that the future frames of dual arm robots are predicted considering their unique characteristics relative to general robotic videos. Experiments demonstrate that our model gains more stable and realistic future frame prediction scenes with the suturing task on the public JIGSAWS dataset.",0
"The problem of predicting future frames for robotic surgical videos is interesting, important, but also highly challenging due to the complex dynamics involved in operative tasks. Previous approaches to predicting future videos have been based on either deterministic or stochastic models, such as deep recurrent neural networks, optical flow, and latent space modeling, but these methods have not yet been applied to predicting the movements of dual-arm surgical robots. This is a particularly challenging task, as it requires predicting meaningful movements that are typically more difficult to forecast than the independent motions of one-arm robots. In this study, we introduce a novel approach called the ternary prior guided variational autoencoder (TPG-VAE) model, which can predict future frames in robotic surgical video sequences by learning both content and motion distributions. Our model incorporates invariant prior information from the gesture class into the generation process to constrain the latent space of the model, resulting in more stable and realistic predictions of future frames. To the best of our knowledge, this is the first time that future frames of dual-arm robots have been predicted, taking into account their unique characteristics in comparison to general robotic videos. The results of our experiments demonstrate that our model can predict more realistic future frames of surgical tools performing the suturing task on the public JIGSAWS dataset.",1
"Accurate fall detection for the assistance of older people is crucial to reduce incidents of deaths or injuries due to falls. Meanwhile, a vision-based fall detection system has shown some significant results to detect falls. Still, numerous challenges need to be resolved. The impact of deep learning has changed the landscape of the vision-based system, such as action recognition. The deep learning technique has not been successfully implemented in vision-based fall detection systems due to the requirement of a large amount of computation power and the requirement of a large amount of sample training data. This research aims to propose a vision-based fall detection system that improves the accuracy of fall detection in some complex environments such as the change of light condition in the room. Also, this research aims to increase the performance of the pre-processing of video images. The proposed system consists of the Enhanced Dynamic Optical Flow technique that encodes the temporal data of optical flow videos by the method of rank pooling, which thereby improves the processing time of fall detection and improves the classification accuracy in dynamic lighting conditions. The experimental results showed that the classification accuracy of the fall detection improved by around 3% and the processing time by 40 to 50ms. The proposed system concentrates on decreasing the processing time of fall detection and improving classification accuracy. Meanwhile, it provides a mechanism for summarizing a video into a single image by using a dynamic optical flow technique, which helps to increase the performance of image pre-processing steps.",0
"To reduce the incidence of falls among older people and prevent injuries and fatalities, accurate fall detection systems are essential. While vision-based fall detection systems have shown promise, there are still numerous obstacles to be overcome. Deep learning has transformed the field of vision-based systems, including action recognition. However, due to the need for substantial computational power and large amounts of training data, deep learning has not been effectively implemented in vision-based fall detection. This study aims to enhance the accuracy of fall detection in complex environments, such as those with variable lighting, by proposing a vision-based fall detection system. The system utilizes the Enhanced Dynamic Optical Flow technique, which encodes temporal data using rank pooling to improve processing time and classification accuracy. The proposed system reduces processing time and enhances classification accuracy while also summarizing video into a single image, improving image pre-processing performance. Experimental results showed a 3% improvement in classification accuracy and a 40-50ms reduction in processing time.",1
"Since the wide employment of deep learning frameworks in video salient object detection, the accuracy of the recent approaches has made stunning progress. These approaches mainly adopt the sequential modules, based on optical flow or recurrent neural network (RNN), to learn robust spatiotemporal features. These modules are effective but significantly increase the computational burden of the corresponding deep models. In this paper, to simplify the network and maintain the accuracy, we present a lightweight network tailored for video salient object detection through the spatiotemporal knowledge distillation. Specifically, in the spatial aspect, we combine a saliency guidance feature embedding structure and spatial knowledge distillation to refine the spatial features. In the temporal aspect, we propose a temporal knowledge distillation strategy, which allows the network to learn the robust temporal features through the infer-frame feature encoding and distilling information from adjacent frames. The experiments on widely used video datasets (e.g., DAVIS, DAVSOD, SegTrack-V2) prove that our approach achieves competitive performance. Furthermore, without the employment of the complex sequential modules, the proposed network can obtain high efficiency with 0.01s per frame.",0
"The accuracy of recent video salient object detection approaches has significantly improved with the widespread use of deep learning frameworks. These approaches mainly rely on sequential modules, which use either optical flow or recurrent neural network (RNN) to learn robust spatiotemporal features. However, these modules increase the computational burden of deep models. To address this issue, we propose a lightweight network for video salient object detection that simplifies the network while maintaining accuracy through spatiotemporal knowledge distillation. Our approach combines saliency guidance feature embedding structure and spatial knowledge distillation to refine spatial features and temporal knowledge distillation strategy to learn robust temporal features through infer-frame feature encoding and information distillation from adjacent frames. Our experiments on widely used video datasets (e.g., DAVIS, DAVSOD, SegTrack-V2) demonstrate that our approach achieves competitive performance. Moreover, our proposed network can achieve high efficiency with 0.01s per frame without the use of complex sequential modules.",1
"In this study, we propose a self-supervised video denoising method called ""restore-from-restored."" This method fine-tunes a pre-trained network by using a pseudo clean video during the test phase. The pseudo clean video is obtained by applying a noisy video to the baseline network. By adopting a fully convolutional neural network (FCN) as the baseline, we can improve video denoising performance without accurate optical flow estimation and registration steps, in contrast to many conventional video restoration methods, due to the translation equivariant property of the FCN. Specifically, the proposed method can take advantage of plentiful similar patches existing across multiple consecutive frames (i.e., patch-recurrence); these patches can boost the performance of the baseline network by a large margin. We analyze the restoration performance of the fine-tuned video denoising networks with the proposed self-supervision-based learning algorithm, and demonstrate that the FCN can utilize recurring patches without requiring accurate registration among adjacent frames. In our experiments, we apply the proposed method to state-of-the-art denoisers and show that our fine-tuned networks achieve a considerable improvement in denoising performance.",0
"The study introduces a video denoising technique called ""restore-from-restored,"" which is self-supervised. During the test phase, a pre-trained network is fine-tuned using a pseudo clean video obtained by applying a noisy video to the baseline network. By utilizing a fully convolutional neural network (FCN) as the baseline, accurate optical flow estimation and registration steps are not required, unlike traditional video restoration methods. The FCN's translation equivariant property allows the proposed method to benefit from numerous similar patches that exist across multiple consecutive frames (patch-recurrence), thus significantly enhancing the baseline network's performance. The study assesses the restoration performance of the fine-tuned video denoising networks using the self-supervision-based learning algorithm, demonstrating that the FCN can use recurring patches without precise registration among adjacent frames. The proposed method is then applied to state-of-the-art denoisers, and the results show that the fine-tuned networks achieve a substantial improvement in denoising performance.",1
"Abrupt motion of camera or objects in a scene result in a blurry video, and therefore recovering high quality video requires two types of enhancements: visual enhancement and temporal upsampling. A broad range of research attempted to recover clean frames from blurred image sequences or temporally upsample frames by interpolation, yet there are very limited studies handling both problems jointly. In this work, we present a novel framework for deblurring, interpolating and extrapolating sharp frames from a motion-blurred video in an end-to-end manner. We design our framework by first learning the pixel-level motion that caused the blur from the given inputs via optical flow estimation and then predict multiple clean frames by warping the decoded features with the estimated flows. To ensure temporal coherence across predicted frames and address potential temporal ambiguity, we propose a simple, yet effective flow-based rule. The effectiveness and favorability of our approach are highlighted through extensive qualitative and quantitative evaluations on motion-blurred datasets from high speed videos.",0
"To recover a high-quality video, two types of enhancements are required: visual enhancement and temporal upsampling. If the camera or objects in a scene move abruptly, the resulting video can be blurry. Although several studies have attempted to recover clean frames from blurred image sequences or temporally upsample frames by interpolation, there are limited studies that handle both problems simultaneously. In this study, we propose a novel framework that deblurs, interpolates, and extrapolates sharp frames from a motion-blurred video in an end-to-end manner. We achieve this by first learning the pixel-level motion that caused the blur from the given inputs through optical flow estimation. We then predict multiple clean frames by warping the decoded features with the estimated flows. To ensure temporal coherence across predicted frames and address potential temporal ambiguity, we propose a simple yet effective flow-based rule. Our approach is evaluated extensively on motion-blurred datasets from high-speed videos, and the results show that our approach is effective and favorable.",1
"In most of computer vision applications, motion blur is regarded as an undesirable artifact. However, it has been shown that motion blur in an image may have practical interests in fundamental computer vision problems. In this work, we propose a novel framework to estimate optical flow from a single motion-blurred image in an end-to-end manner. We design our network with transformer networks to learn globally and locally varying motions from encoded features of a motion-blurred input, and decode left and right frame features without explicit frame supervision. A flow estimator network is then used to estimate optical flow from the decoded features in a coarse-to-fine manner. We qualitatively and quantitatively evaluate our model through a large set of experiments on synthetic and real motion-blur datasets. We also provide in-depth analysis of our model in connection with related approaches to highlight the effectiveness and favorability of our approach. Furthermore, we showcase the applicability of the flow estimated by our method on deblurring and moving object segmentation tasks.",0
"Although motion blur is typically viewed as an unwanted artifact in computer vision, recent research has revealed that it can be useful in certain fundamental problems. This paper presents a new framework that uses transformer networks to estimate optical flow from a single motion-blurred image. Our network is capable of learning locally and globally varying motions from encoded features of the input image, and can decode left and right frame features without explicit supervision. Following this, a flow estimator network is used to estimate optical flow in a coarse-to-fine manner. We extensively evaluate our model on both synthetic and real motion-blur datasets, and provide a detailed analysis of our approach compared to related methods. Additionally, we demonstrate the practical applications of our method in deblurring and moving object segmentation tasks.",1
"Video deblurring models exploit consecutive frames to remove blurs from camera shakes and object motions. In order to utilize neighboring sharp patches, typical methods rely mainly on homography or optical flows to spatially align neighboring blurry frames. However, such explicit approaches are less effective in the presence of fast motions with large pixel displacements. In this work, we propose a novel implicit method to learn spatial correspondence among blurry frames in the feature space. To construct distant pixel correspondences, our model builds a correlation volume pyramid among all the pixel-pairs between neighboring frames. To enhance the features of the reference frame, we design a correlative aggregation module that maximizes the pixel-pair correlations with its neighbors based on the volume pyramid. Finally, we feed the aggregated features into a reconstruction module to obtain the restored frame. We design a generative adversarial paradigm to optimize the model progressively. Our proposed method is evaluated on the widely-adopted DVD dataset, along with a newly collected High-Frame-Rate (1000 fps) Dataset for Video Deblurring (HFR-DVD). Quantitative and qualitative experiments show that our model performs favorably on both datasets against previous state-of-the-art methods, confirming the benefit of modeling all-range spatial correspondence for video deblurring.",0
"Models for video deblurring use consecutive frames to remove blurs caused by camera shakes and object motions. Common methods use homography or optical flows to align neighboring blurry frames and utilize sharp patches. However, these explicit approaches are not effective for fast motions with large pixel displacements. This study introduces a new implicit method that learns spatial correspondence among blurry frames in the feature space. The model constructs a correlation volume pyramid among all pixel-pairs between neighboring frames to create distant pixel correspondences. A correlative aggregation module is designed to enhance the features of the reference frame by maximizing pixel-pair correlations with its neighbors based on the volume pyramid. Finally, the aggregated features are fed into a reconstruction module to obtain the restored frame. The proposed method is optimized using a generative adversarial paradigm and evaluated on the DVD dataset and the newly collected HFR-DVD dataset. Results show that the model outperforms previous state-of-the-art methods, demonstrating the benefits of modeling all-range spatial correspondence for video deblurring.",1
"Learning reliable motion representation between consecutive frames, such as optical flow, has proven to have great promotion to video understanding. However, the TV-L1 method, an effective optical flow solver, is time-consuming and expensive in storage for caching the extracted optical flow. To fill the gap, we propose UF-TSN, a novel end-to-end action recognition approach enhanced with an embedded lightweight unsupervised optical flow estimator. UF-TSN estimates motion cues from adjacent frames in a coarse-to-fine manner and focuses on small displacement for each level by extracting pyramid of feature and warping one to the other according to the estimated flow of the last level. Due to the lack of labeled motion for action datasets, we constrain the flow prediction with multi-scale photometric consistency and edge-aware smoothness. Compared with state-of-the-art unsupervised motion representation learning methods, our model achieves better accuracy while maintaining efficiency, which is competitive with some supervised or more complicated approaches.",0
"The utilization of optical flow for dependable motion representation between frames has been found to be highly beneficial for video comprehension. However, the TV-L1 method, a proficient optical flow solver, is both time-consuming and requires a significant amount of storage for caching the extracted optical flow. To address this issue, we propose UF-TSN, a new end-to-end action recognition approach that includes a lightweight unsupervised optical flow estimator. UF-TSN estimates motion cues in a coarse-to-fine manner and concentrates on small displacement for each level by extracting a pyramid of features and warping one to the other based on the last level's estimated flow. Due to the absence of labeled motion for action datasets, we constrain the flow prediction using multi-scale photometric consistency and edge-aware smoothness. Our model achieves superior accuracy while maintaining efficiency, which is comparable to some supervised or more complex approaches, when compared to state-of-the-art unsupervised motion representation learning methods.",1
"Multi-view geometry-based methods dominate the last few decades in monocular Visual Odometry for their superior performance, while they have been vulnerable to dynamic and low-texture scenes. More importantly, monocular methods suffer from scale-drift issue, i.e., errors accumulate over time. Recent studies show that deep neural networks can learn scene depths and relative camera in a self-supervised manner without acquiring ground truth labels. More surprisingly, they show that the well-trained networks enable scale-consistent predictions over long videos, while the accuracy is still inferior to traditional methods because of ignoring geometric information. Building on top of recent progress in computer vision, we design a simple yet robust VO system by integrating multi-view geometry and deep learning on Depth and optical Flow, namely DF-VO. In this work, a) we propose a method to carefully sample high-quality correspondences from deep flows and recover accurate camera poses with a geometric module; b) we address the scale-drift issue by aligning geometrically triangulated depths to the scale-consistent deep depths, where the dynamic scenes are taken into account. Comprehensive ablation studies show the effectiveness of the proposed method, and extensive evaluation results show the state-of-the-art performance of our system, e.g., Ours (1.652%) v.s. ORB-SLAM (3.247%}) in terms of translation error in KITTI Odometry benchmark. Source code is publicly available at: \href{https://github.com/Huangying-Zhan/DF-VO}{DF-VO}.",0
"In recent decades, multi-view geometry-based methods have been the preferred approach for monocular Visual Odometry due to their superior performance, although they are susceptible to dynamic and low-texture environments. Moreover, monocular methods suffer from scale-drift, where errors accumulate over time. Recent research has shown that deep neural networks can learn scene depths and relative camera in a self-supervised way without requiring ground truth labels. Surprisingly, well-trained networks can make scale-consistent predictions over long videos, but their accuracy is still inferior to traditional methods because they ignore geometric information. We propose a robust VO system called DF-VO that combines multi-view geometry and deep learning on Depth and optical Flow. Our method carefully samples high-quality correspondences from deep flows, recovers accurate camera poses with a geometric module, and addresses the scale-drift issue by aligning geometrically triangulated depths to the scale-consistent deep depths, taking dynamic scenes into account. Our comprehensive ablation studies demonstrate the effectiveness of the proposed method, and extensive evaluation results show that our system outperforms ORB-SLAM in terms of translation error in the KITTI Odometry benchmark. The source code is publicly available at: \href{https://github.com/Huangying-Zhan/DF-VO}{DF-VO}.",1
"Understanding driver activity is vital for in-vehicle systems that aim to reduce the incidence of car accidents rooted in cognitive distraction. Automating real-time behavior recognition while ensuring actions classification with high accuracy is however challenging, given the multitude of circumstances surrounding drivers, the unique traits of individuals, and the computational constraints imposed by in-vehicle embedded platforms. Prior work fails to jointly meet these runtime/accuracy requirements and mostly rely on a single sensing modality, which in turn can be a single point of failure. In this paper, we harness the exceptional feature extraction abilities of deep learning and propose a dedicated Interwoven Deep Convolutional Neural Network (InterCNN) architecture to tackle the problem of accurate classification of driver behaviors in real-time. The proposed solution exploits information from multi-stream inputs, i.e., in-vehicle cameras with different fields of view and optical flows computed based on recorded images, and merges through multiple fusion layers abstract features that it extracts. This builds a tight ensembling system, which significantly improves the robustness of the model. In addition, we introduce a temporal voting scheme based on historical inference instances, to enhance the classification accuracy. Experiments conducted with a dataset that we collect in a mock-up car environment demonstrate that the proposed InterCNN with MobileNet convolutional blocks can classify 9 different behaviors with 73.97% accuracy, and 5 'aggregated' behaviors with 81.66% accuracy. We further show that our architecture is highly computationally efficient, as it performs inferences within 15ms, which satisfies the real-time constraints of intelligent cars. Nevertheless, our InterCNN is robust to lossy input, as the classification remains accurate when two input streams are occluded.",0
"To reduce car accidents caused by cognitive distraction, it is crucial to understand driver activity. However, accurately automating real-time behavior recognition is challenging due to various driver circumstances, individual differences, and computational limitations in embedded platforms. Previous work has not met the requirements for high accuracy and runtime performance, relying on a single sensing modality that can be a single point of failure. In this study, we propose an Interwoven Deep Convolutional Neural Network (InterCNN) that utilizes deep learning's exceptional feature extraction abilities to classify driver behaviors accurately in real-time. Our solution combines multi-stream inputs, such as in-vehicle cameras with different fields of view and optical flows, through multiple fusion layers to extract abstract features and enhance model robustness. We also introduce a temporal voting scheme to improve classification accuracy. Our experiments demonstrate that the proposed InterCNN can classify nine behaviors with 73.97% accuracy and five ""aggregated"" behaviors with 81.66% accuracy, all within the real-time constraints of intelligent cars. Furthermore, our InterCNN is robust to lossy input, as classification remains accurate even with two input streams occluded.",1
"Optical flow is a regression task where convolutional neural networks (CNNs) have led to major breakthroughs. However, this comes at major computational demands due to the use of cost-volumes and pyramidal representations. This was mitigated by producing flow predictions at quarter the resolution, which are upsampled using bilinear interpolation during test time. Consequently, fine details are usually lost and post-processing is needed to restore them. We propose the Normalized Convolution UPsampler (NCUP), an efficient joint upsampling approach to produce the full-resolution flow during the training of optical flow CNNs. Our proposed approach formulates the upsampling task as a sparse problem and employs the normalized convolutional neural networks to solve it. We evaluate our upsampler against existing joint upsampling approaches when trained end-to-end with a a coarse-to-fine optical flow CNN (PWCNet) and we show that it outperforms all other approaches on the FlyingChairs dataset while having at least one order fewer parameters. Moreover, we test our upsampler with a recurrent optical flow CNN (RAFT) and we achieve state-of-the-art results on Sintel benchmark with ~6% error reduction, and on-par on the KITTI dataset, while having 7.5% fewer parameters (see Figure 1). Finally, our upsampler shows better generalization capabilities than RAFT when trained and evaluated on different datasets.",0
"Convolutional neural networks (CNNs) have made significant progress in the regression task of optical flow. However, the use of cost-volumes and pyramidal representations leads to high computational demands. This issue has been addressed by producing flow predictions at a lower resolution during testing and then upscaling them using bilinear interpolation. However, this method often results in the loss of fine details, requiring post-processing. To overcome this, we propose an efficient joint upsampling approach called the Normalized Convolution UPsampler (NCUP). Our approach formulates the upsampling task as a sparse problem and employs normalized CNNs to solve it. We evaluate our method against existing joint upsampling approaches and demonstrate that it outperforms all others on the FlyingChairs dataset while having significantly fewer parameters. We also test our upsampler with a recurrent optical flow CNN (RAFT) and achieve state-of-the-art results on the Sintel benchmark with a 6% error reduction and comparable performance on the KITTI dataset while using 7.5% fewer parameters. Our upsampler also exhibits greater generalization capabilities than RAFT when trained and evaluated on different datasets.",1
"Neural style transfer models have been used to stylize an ordinary video to specific styles. To ensure temporal inconsistency between the frames of the stylized video, a common approach is to estimate the optic flow of the pixels in the original video and make the generated pixels match the estimated optical flow. This is achieved by minimizing an optical flow-based (OFB) loss during model training. However, optical flow estimation is itself a challenging task, particularly in complex scenes. In addition, it incurs a high computational cost. We propose a much simpler temporal loss called the frame difference-based (FDB) loss to solve the temporal inconsistency problem. It is defined as the distance between the difference between the stylized frames and the difference between the original frames. The differences between the two frames are measured in both the pixel space and the feature space specified by the convolutional neural networks. A set of human behavior experiments involving 62 subjects with 25,600 votes showed that the performance of the proposed FDB loss matched that of the OFB loss. The performance was measured by subjective evaluation of stability and stylization quality of the generated videos on two typical video stylization models. The results suggest that the proposed FDB loss is a strong alternative to the commonly used OFB loss for video stylization.",0
"To stylize ordinary videos to specific styles, neural style transfer models are used. However, ensuring temporal consistency between frames in the stylized video can be a challenge. Typically, this is achieved by estimating the optic flow of the pixels in the original video and matching the generated pixels to the estimated optical flow. Unfortunately, optical flow estimation can be difficult and computationally expensive, especially in complex scenes. To address this issue, we propose a simpler temporal loss called the frame difference-based (FDB) loss. This loss calculates the distance between the difference between the stylized frames and the difference between the original frames, measured in both pixel space and feature space. We conducted human behavior experiments with 62 subjects who cast 25,600 votes to compare the performance of the FDB loss to the commonly used optical flow-based (OFB) loss. The results showed that the FDB loss performed equally well in terms of stability and stylization quality of the generated videos, making it a strong alternative to the OFB loss for video stylization.",1
"When the input to a deep neural network (DNN) is a video signal, a sequence of feature tensors is produced at the intermediate layers of the model. If neighboring frames of the input video are related through motion, a natural question is, ""what is the relationship between the corresponding feature tensors?"" By analyzing the effect of common DNN operations on optical flow, we show that the motion present in each channel of a feature tensor is approximately equal to the scaled version of the input motion. The analysis is validated through experiments utilizing common motion models. %These results will be useful in collaborative intelligence applications where sequences of feature tensors need to be compressed or further analyzed.",0
"In the case of a video signal being the input for a deep neural network (DNN), the model generates a sequence of feature tensors at the intermediate layers. If there is a connection between neighboring frames of the video due to motion, the question arises as to how the corresponding feature tensors are related. Our study involves examining the effect of standard DNN operations on optical flow and discovering that the motion in each channel of a feature tensor is similar to the input motion but scaled accordingly. The validity of this analysis is backed up by experiments utilizing common motion models. These findings can be beneficial in collaborative intelligence applications that necessitate compressing or further analyzing sequences of feature tensors.",1
"Recognizing human actions based on videos has became one of the most popular areas of research in computer vision in recent years. This area has many applications such as surveillance, robotics, health care, video search and human-computer interaction. There are many problems associated with recognizing human actions in videos such as cluttered backgrounds, obstructions, viewpoints variation, execution speed and camera movement. A large number of methods have been proposed to solve the problems. This paper focus on spatial and temporal pattern recognition for the classification of videos using Deep Neural Networks. This model takes RGB images and Optical Flow as input data and outputs an action class number. The final recognition accuracy was about 94%.",0
"In recent years, the identification of human actions from videos has emerged as a widely researched area in computer vision. Its applications span across various fields including surveillance, robotics, healthcare, video search and human-computer interaction. However, recognizing human actions in videos presents several challenges such as varied viewpoints, obstructed views, camera movement, cluttered backgrounds, and execution speed. Numerous methods have been suggested to address these issues. This research paper concentrates on the employment of Deep Neural Networks for spatial and temporal pattern recognition in video classification. The model utilizes RGB images and Optical Flow as input data and generates an action class number. The final recognition accuracy of the model was approximately 94%.",1
"We present an end-to-end joint training framework that explicitly models 6-DoF motion of multiple dynamic objects, ego-motion and depth in a monocular camera setup without supervision. Our technical contributions are three-fold. First, we highlight the fundamental difference between inverse and forward projection while modeling the individual motion of each rigid object, and propose a geometrically correct projection pipeline using a neural forward projection module. Second, we design a unified instance-aware photometric and geometric consistency loss that holistically imposes self-supervisory signals for every background and object region. Lastly, we introduce a general-purpose auto-annotation scheme using any off-the-shelf instance segmentation and optical flow models to produce video instance segmentation maps that will be utilized as input to our training pipeline. These proposed elements are validated in a detailed ablation study. Through extensive experiments conducted on the KITTI and Cityscapes dataset, our framework is shown to outperform the state-of-the-art depth and motion estimation methods. Our code, dataset, and models are available at https://github.com/SeokjuLee/Insta-DM .",0
"We have developed a comprehensive joint training framework that can explicitly model the motion of multiple dynamic objects, ego-motion, and depth in a monocular camera setup. What sets our framework apart are three major technical contributions. Firstly, we have identified the key differences between inverse and forward projection and proposed a neural forward projection module that accurately models the individual motion of each rigid object. Secondly, we have incorporated a unified instance-aware photometric and geometric consistency loss that ensures self-supervisory signals for all background and object regions. Lastly, we have introduced an auto-annotation scheme that utilizes off-the-shelf instance segmentation and optical flow models to generate video instance segmentation maps for input to our training pipeline. Our framework has been validated in a thorough ablation study, and we have demonstrated its superior performance compared to existing depth and motion estimation methods through extensive experiments on the KITTI and Cityscapes datasets. Our code, dataset, and models can be accessed at https://github.com/SeokjuLee/Insta-DM.",1
"Recent constellations of satellites, including the Skysat constellation, are able to acquire bursts of images. This new acquisition mode allows for modern image restoration techniques, including multi-frame super-resolution. As the satellite moves during the acquisition of the burst, elevation changes in the scene translate into noticeable parallax. This parallax hinders the results of the restoration. To cope with this issue, we propose a novel parallax estimation method. The method is composed of a linear Plane+Parallax decomposition of the apparent motion and a multi-frame optical flow algorithm that exploits all frames simultaneously. Using SkySat L1A images, we show that the estimated per-pixel displacements are important for applying multi-frame super-resolution on scenes containing elevation changes and that can also be used to estimate a coarse 3D surface model.",0
"Newly launched satellite constellations, such as Skysat, are capable of capturing bursts of images that enable modern image restoration techniques like multi-frame super-resolution. However, parallax caused by the satellite's movement during image acquisition can negatively affect restoration results. To address this issue, we propose a novel approach for estimating parallax. Our method involves a linear Plane+Parallax decomposition of apparent motion and a multi-frame optical flow algorithm that utilizes all frames. By using SkySat L1A images, we demonstrate that per-pixel displacements estimated by our method are crucial for applying multi-frame super-resolution to scenes with elevation changes, and can also be used to estimate a rough 3D surface model.",1
"Cycling is a promising sustainable mode for commuting and leisure in cities, however, the fear of getting hit or fall reduces its wide expansion as a commuting mode. In this paper, we introduce a novel method called CyclingNet for detecting cycling near misses from video streams generated by a mounted frontal camera on a bike regardless of the camera position, the conditions of the built, the visual conditions and without any restrictions on the riding behaviour. CyclingNet is a deep computer vision model based on convolutional structure embedded with self-attention bidirectional long-short term memory (LSTM) blocks that aim to understand near misses from both sequential images of scenes and their optical flows. The model is trained on scenes of both safe rides and near misses. After 42 hours of training on a single GPU, the model shows high accuracy on the training, testing and validation sets. The model is intended to be used for generating information that can draw significant conclusions regarding cycling behaviour in cities and elsewhere, which could help planners and policy-makers to better understand the requirement of safety measures when designing infrastructure or drawing policies. As for future work, the model can be pipelined with other state-of-the-art classifiers and object detectors simultaneously to understand the causality of near misses based on factors related to interactions of road-users, the built and the natural environments.",0
"Although cycling holds promise as a sustainable mode of transportation and leisure in cities, its expansion as a commuting mode is hindered by the potential dangers of accidents. To address this issue, we present a new approach called CyclingNet that detects cycling near misses from video streams produced by a camera mounted on a bike's front, regardless of the camera's position, the surrounding conditions, and the rider's behavior. CyclingNet employs a deep computer vision model based on a convolutional structure with self-attention bidirectional long-short term memory (LSTM) blocks, which analyze near misses from sequential images of scenes and their optical flows. The model is trained on images of both safe and near-miss rides, reaching high accuracy levels after 42 hours of training on a single GPU. Our aim is to use the model to generate valuable insights into cycling behavior in urban areas, which can inform decision-making by planners and policymakers when designing infrastructure or drafting policies. Going forward, we plan to integrate CyclingNet with other advanced classifiers and object detectors to examine the underlying causes of near misses related to the interactions of road-users, built, and natural environments.",1
The goal of this paper is propose a mathematical framework for optical flow refinement with non-quadratic regularization using variational techniques. We demonstrate how the model can be suitably adapted for both rigid and fluid motion estimation. We study the problem as an abstract IVP using an evolutionary PDE approach. We show that for a particular choice of constraint our model approximates the continuity model with non-quadratic regularization using augmented Lagrangian techniques. We subsequently show the results of our algorithm on different datasets.,0
"This paper aims to introduce a mathematical structure for enhancing optical flow through variational techniques with non-quadratic regularization. We showcase the model's adaptability for estimating both rigid and fluid motion. We approach the problem as an abstract IVP utilizing an evolutionary PDE method. By selecting a specific constraint, we demonstrate how our model approximates the continuity model with non-quadratic regularization through augmented Lagrangian techniques. Finally, we present our algorithm's outcomes on various datasets.",1
"Optical flow estimation is an essential step for many real-world computer vision tasks. Existing deep networks have achieved satisfactory results by mostly employing a pyramidal coarse-to-fine paradigm, where a key process is to adopt warped target feature based on previous flow prediction to correlate with source feature for building 3D matching cost volume. However, the warping operation can lead to troublesome ghosting problem that results in ambiguity. Moreover, occluded areas are treated equally with non occluded regions in most existing works, which may cause performance degradation. To deal with these challenges, we propose a lightweight yet efficient optical flow network, named OAS-Net (occlusion aware sampling network) for accurate optical flow. First, a new sampling based correlation layer is employed without noisy warping operation. Second, a novel occlusion aware module is presented to make raw cost volume conscious of occluded regions. Third, a shared flow and occlusion awareness decoder is adopted for structure compactness. Experiments on Sintel and KITTI datasets demonstrate the effectiveness of proposed approaches.",0
"Many computer vision tasks require optical flow estimation, which has been accomplished by deep networks using a coarse-to-fine approach that involves warping target features based on previous flow predictions. However, this warping technique has drawbacks, including ghosting and treating occluded areas equally with non-occluded regions, which can impact performance. To address these issues, we introduce a new lightweight optical flow network called OAS-Net. This network features a sampling-based correlation layer that avoids the noisy warping operation, an occlusion-aware module that considers occluded regions, and a shared flow and occlusion awareness decoder for compact structure. Our experiments on Sintel and KITTI datasets demonstrate the effectiveness of our approach.",1
"Generating non-existing frames from a consecutive video sequence has been an interesting and challenging problem in the video processing field. Typical kernel-based interpolation methods predict pixels with a single convolution process that convolves source frames with spatially adaptive local kernels, which circumvents the time-consuming, explicit motion estimation in the form of optical flow. However, when scene motion is larger than the pre-defined kernel size, these methods are prone to yield less plausible results. In addition, they cannot directly generate a frame at an arbitrary temporal position because the learned kernels are tied to the midpoint in time between the input frames. In this paper, we try to solve these problems and propose a novel non-flow kernel-based approach that we refer to as enhanced deformable separable convolution (EDSC) to estimate not only adaptive kernels, but also offsets, masks and biases to make the network obtain information from non-local neighborhood. During the learning process, different intermediate time step can be involved as a control variable by means of an extension of coord-conv trick, allowing the estimated components to vary with different input temporal information. This makes our method capable to produce multiple in-between frames. Furthermore, we investigate the relationships between our method and other typical kernel- and flow-based methods. Experimental results show that our method performs favorably against the state-of-the-art methods across a broad range of datasets. Code will be publicly available on URL: \url{https://github.com/Xianhang/EDSC-pytorch}.",0
"The generation of non-existent frames from a sequential video sequence has been a difficult and fascinating problem in the video processing field. Conventional kernel-based interpolation techniques employ a single convolution process to forecast pixels by convolving the source frames with spatially adaptive local kernels, which avoids the time-consuming explicit motion estimation in the form of optical flow. However, these methods are susceptible to producing less credible outcomes when the scene motion exceeds the pre-defined kernel size. Additionally, it is not possible to directly create a frame at any temporal position because the acquired kernels are bound to the midpoint in time between the input frames. This study presents a new non-flow kernel-based methodology, known as enhanced deformable separable convolution (EDSC), that not only estimates adaptive kernels but also offsets, masks, and biases to enable the network to extract information from a non-local neighborhood. Moreover, the proposed approach can involve different intermediate time steps as a control variable during the learning process, using an extension of the coord-conv trick, allowing the estimated components to vary with varying input temporal information. This enables our approach to create multiple in-between frames. Additionally, we investigate the connection between our approach and other conventional kernel- and flow-based methods. Our experimental outcomes demonstrate that our method outperforms state-of-the-art methods across a wide range of datasets. The code for our approach will be publicly accessible at this URL: \url{https://github.com/Xianhang/EDSC-pytorch}.",1
"We present Supervision by Registration and Triangulation (SRT), an unsupervised approach that utilizes unlabeled multi-view video to improve the accuracy and precision of landmark detectors. Being able to utilize unlabeled data enables our detectors to learn from massive amounts of unlabeled data freely available and not be limited by the quality and quantity of manual human annotations. To utilize unlabeled data, there are two key observations: (1) the detections of the same landmark in adjacent frames should be coherent with registration, i.e., optical flow. (2) the detections of the same landmark in multiple synchronized and geometrically calibrated views should correspond to a single 3D point, i.e., multi-view consistency. Registration and multi-view consistency are sources of supervision that do not require manual labeling, thus it can be leveraged to augment existing training data during detector training. End-to-end training is made possible by differentiable registration and 3D triangulation modules. Experiments with 11 datasets and a newly proposed metric to measure precision demonstrate accuracy and precision improvements in landmark detection on both images and video. Code is available at https://github.com/D-X-Y/landmark-detection.",0
"Our paper introduces a new technique called Supervision by Registration and Triangulation (SRT) that utilizes unlabeled multi-view video to enhance the accuracy and precision of landmark detectors in an unsupervised manner. By using unlabeled data, our detectors can learn from a vast amount of freely available data without being restricted by the quality and quantity of human annotations. To leverage unlabeled data, we make two key observations: (1) detections of the same landmark in adjacent frames should be coherent with registration, i.e., optical flow, and (2) detections of the same landmark in multiple synchronized and geometrically calibrated views should correspond to a single 3D point, i.e., multi-view consistency. These sources of supervision do not require manual labeling and can be used to augment existing training data during detector training. We achieve end-to-end training through differentiable registration and 3D triangulation modules. We validate our approach on 11 datasets and introduce a new metric to measure precision, demonstrating accuracy and precision improvements in landmark detection on both images and video. Our code is available at https://github.com/D-X-Y/landmark-detection.",1
"Weighted Gaussian Curvature is an important measurement for images. However, its conventional computation scheme has low performance, low accuracy and requires that the input image must be second order differentiable. To tackle these three issues, we propose a novel discrete computation scheme for the weighted Gaussian curvature. Our scheme does not require the second order differentiability. Moreover, our scheme is more accurate, has smaller support region and computationally more efficient than the conventional schemes. Therefore, our scheme holds promise for a large range of applications where the weighted Gaussian curvature is needed, for example, image smoothing, cartoon texture decomposition, optical flow estimation, etc.",0
"The measurement of Weighted Gaussian Curvature holds great significance in images. However, the traditional method of computing it has some drawbacks such as low performance, limited accuracy, and the requirement of the input image being second order differentiable. In order to address these issues, we have put forward a novel discrete computation scheme for the Weighted Gaussian Curvature. Our scheme does not demand the image to be second order differentiable and is more precise, has a smaller support region, and is computationally more efficient than conventional schemes. This makes our scheme suitable for a wide range of applications including but not limited to image smoothing, cartoon texture decomposition, and optical flow estimation.",1
"Self-driving cars and other autonomous vehicles need to detect and track objects in camera images. We present a simple online tracking algorithm that is based on a constant velocity motion model with a Kalman filter, and an assignment heuristic. The assignment heuristic relies on four metrics: An embedding vector that describes the appearance of objects and can be used to re-identify them, a displacement vector that describes the object movement between two consecutive video frames, the Mahalanobis distance between the Kalman filter states and the new detections, and a class distance. These metrics are combined with a linear SVM, and then the assignment problem is solved by the Hungarian algorithm. We also propose an efficient CNN architecture that estimates these metrics. Our multi-frame model accepts two consecutive video frames which are processed individually in the backbone, and then optical flow is estimated on the resulting feature maps. This allows the network heads to estimate the displacement vectors. We evaluate our approach on the challenging BDD100K tracking dataset. Our multi-frame model achieves a good MOTA value of 39.1% with low localization error of 0.206 in MOTP. Our fast single-frame model achieves an even lower localization error of 0.202 in MOTP, and a MOTA value of 36.8%.",0
"A tracking algorithm that relies on a Kalman filter and an assignment heuristic is presented as a means for self-driving and autonomous vehicles to identify and monitor objects in camera images. The assignment heuristic considers four metrics: an embedding vector, a displacement vector, the Mahalanobis distance, and a class distance. These metrics are combined with a linear SVM and solved using the Hungarian algorithm. An efficient CNN architecture estimates these metrics using a multi-frame model that accepts two consecutive frames and processes them individually before estimating optical flow. This approach is evaluated on the BDD100K tracking dataset, with the multi-frame model achieving a good MOTA value of 39.1% and a low localization error of 0.206 in MOTP, while the fast single-frame model achieves an even lower localization error of 0.202 in MOTP and a MOTA value of 36.8%.",1
"Recently, flow-based methods have achieved promising success in video frame interpolation. However, electron microscopic (EM) images suffer from unstable image quality, low PSNR, and disorderly deformation. Existing flow-based interpolation methods cannot precisely compute optical flow for EM images since only predicting each position's unique offset. To overcome these problems, we propose a novel interpolation framework for EM images that progressively synthesizes interpolated features in a coarse-to-fine manner. First, we extract missing intermediate features by the proposed temporal spatial-adaptive (TSA) interpolation module. The TSA interpolation module aggregates temporal contexts and then adaptively samples the spatial-related features with the proposed residual spatial adaptive block. Second, we introduce a stacked deformable refinement block (SDRB) further enhance the reconstruction quality, which is aware of the matching positions and relevant features from input frames with the feedback mechanism. Experimental results demonstrate the superior performance of our approach compared to previous works, both quantitatively and qualitatively.",0
"Video frame interpolation has seen promising success with flow-based methods. However, electron microscopic images face challenges such as unstable image quality, low PSNR, and disorderly deformation. Existing flow-based methods cannot accurately compute optical flow for EM images as they only predict unique offsets for each position. To overcome these issues, we propose a novel interpolation framework that synthesizes interpolated features progressively, from coarse to fine. Our approach involves extracting missing intermediate features using the proposed temporal spatial-adaptive (TSA) interpolation module, which aggregates temporal contexts and adaptively samples spatial-related features with the proposed residual spatial adaptive block. Additionally, we introduce a stacked deformable refinement block (SDRB) that enhances reconstruction quality, with awareness of matching positions and relevant features from input frames using a feedback mechanism. Our experimental results demonstrate that our approach outperforms previous works, both quantitatively and qualitatively.",1
