"Graph neural networks (GNNs) have been widely used to learn vector representation of graph-structured data and achieved better task performance than conventional methods. The foundation of GNNs is the message passing procedure, which propagates the information in a node to its neighbors. Since this procedure proceeds one step per layer, the range of the information propagation among nodes is small in the lower layers, and it expands toward the higher layers. Therefore, a GNN model has to be deep enough to capture global structural information in a graph. On the other hand, it is known that deep GNN models suffer from performance degradation because they lose nodes' local information, which would be essential for good model performance, through many message passing steps. In this study, we propose multi-level attention pooling (MLAP) for graph-level classification tasks, which can adapt to both local and global structural information in a graph. It has an attention pooling layer for each message passing step and computes the final graph representation by unifying the layer-wise graph representations. The MLAP architecture allows models to utilize the structural information of graphs with multiple levels of localities because it preserves layer-wise information before losing them due to oversmoothing. Results of our experiments show that the MLAP architecture improves the graph classification performance compared to the baseline architectures. In addition, analyses on the layer-wise graph representations suggest that aggregating information from multiple levels of localities indeed has the potential to improve the discriminability of learned graph representations.",0
"This paper proposes a novel method for graph neural networks (GNN) that utilizes multi-level attention pooling to capture multiple localities within each node in a graph. The proposed approach unifies different graph representations by considering each layer as a separate attention head and combining them using attention mechanisms. We demonstrate the effectiveness of our method through experiments on several benchmark datasets, showing improved performance over state-of-the-art methods in both transductive and inductive settings. Our findings suggest that multi-level attention pooling provides a powerful tool for capturing complex relationships across nodes and improving GNN predictions.",1
"Deep networks and decision forests (such as random forests and gradient boosted trees) are the leading machine learning methods for structured and tabular data, respectively. Many papers have empirically compared large numbers of classifiers on one or two different domains (e.g., on 100 different tabular data settings). However, a careful conceptual and empirical comparison of these two strategies using the most contemporary best practices has yet to be performed. Conceptually, we illustrate that both can be profitably viewed as ""partition and vote"" schemes. Specifically, the representation space that they both learn is a partitioning of feature space into a union of convex polytopes. For inference, each decides on the basis of votes from the activated nodes. This formulation allows for a unified basic understanding of the relationship between these methods. Empirically, we compare these two strategies on hundreds of tabular data settings, as well as several vision and auditory settings. Our focus is on datasets with at most 10,000 samples, which represent a large fraction of scientific and biomedical datasets. In general, we found forests to excel at tabular and structured data (vision and audition) with small sample sizes, whereas deep nets performed better on structured data with larger sample sizes. This suggests that further gains in both scenarios may be realized via further combining aspects of forests and networks. We will continue revising this technical report in the coming months with updated results.",0
"Artificial intelligence (AI) has become increasingly important over recent years due to advances in machine learning techniques such as decision forest models and deep neural networks. These methods have proven to be highly accurate in many fields, but their performance can vary greatly depending on the dataset size. In particular, decision forest models tend to perform well even with small sample sizes, while deep neural networks often require large amounts of data. The authors investigate the conceptual similarities and empirical differences between these two types of models using experimental results from simulations and real datasets. They find that both models have advantages and disadvantages, with decision forests tending to perform better on smaller datasets and deep neural networks performing better on larger ones. Their results provide insight into which model may be more suitable for different applications and shed light on how the choice of model affects accuracy.",1
"Graph convolutional networks (GCNs) are powerful tools for graph-structured data. However, they have been recently shown to be vulnerable to topological attacks. To enhance adversarial robustness, we go beyond spectral graph theory to robust graph theory. By challenging the classical graph Laplacian, we propose a new convolution operator that is provably robust in the spectral domain and is incorporated in the GCN architecture to improve expressivity and interpretability. By extending the original graph to a sequence of graphs, we also propose a robust training paradigm that encourages transferability across graphs that span a range of spatial and spectral characteristics. The proposed approaches are demonstrated in extensive experiments to simultaneously improve performance in both benign and adversarial situations.",0
"Graphs play an essential role in encoding many real-world data types such as social networks (Facebook friendships), knowledge graphs (WordNet synonym relations), biochemical interaction maps (protein– protein interactions) and others. In recent years, deep learning has become increasingly popular for mining graph structured data, thanks largely to the success of Graph Neural Networks (GNN). Since 2017, GNNs have achieved state-of-the-art performance on numerous benchmark datasets across different domains including social network analysis and link prediction, biosequence classification, drug activity prediction and more. While these models have shown powerful representation capabilities on complex node relationships within graphs, they are still susceptible to noise and outliers that can disrupt their accuracy. This study proposes a novel model architecture called Graph Powering Networks (GPNeT) which incorporates explicit control over local receptive fields by applying power operations. We show through rigorous experiments on several publicly available benchmarks, including synthetic and semi-synthetic benchmark datasets from the DeepGraphBenchmark suite and a challenging multi-graph biological dataset, that our proposed method significantly improves predictive accuracy under conditions where traditional GCNs struggle with robustness due to noisy inputs and irregular connectivity patterns. We demonstrate both qualitatively and quantitatively that GPNeT exhibits better noise tolerance, improved generalization ability over competitive baselines and most importantly maintains high levels of effectiveness throughout various regimes of hyperparameter tuning settings. Our approach enables deeper insight into handling various architectures designed for graphs while providing a clear pathway towards developing new methods fusing power operations into existing ar",1
"With the increasing popularity of Graph Neural Networks (GNNs) in several sensitive applications like healthcare and medicine, concerns have been raised over the privacy aspects of trained GNNs. More notably, GNNs are vulnerable to privacy attacks, such as membership inference attacks, even if only blackbox access to the trained model is granted. To build defenses, differential privacy has emerged as a mechanism to disguise the sensitive data in training datasets. Following the strategy of Private Aggregation of Teacher Ensembles (PATE), recent methods leverage a large ensemble of teacher models. These teachers are trained on disjoint subsets of private data and are employed to transfer knowledge to a student model, which is then released with privacy guarantees. However, splitting graph data into many disjoint training sets may destroy the structural information and adversely affect accuracy. We propose a new graph-specific scheme of releasing a student GNN, which avoids splitting private training data altogether. The student GNN is trained using public data, partly labeled privately using the teacher GNN models trained exclusively for each query node. We theoretically analyze our approach in the R\`{e}nyi differential privacy framework and provide privacy guarantees. Besides, we show the solid experimental performance of our method compared to several baselines, including the PATE baseline adapted for graph-structured data. Our anonymized code is available.",0
"Title: Enabling Private Deep Learning on Graph Data  Deep learning has revolutionized many fields and empowered applications ranging from image recognition to natural language processing. However, deep learning models often require large amounts of sensitive data, such as medical records, social network interactions, or financial transactions, which raises privacy concerns. In particular, graph data, widely used across disciplines like computer vision, natural language understanding, bioinformatics, and recommender systems, must be protected while enabling efficient training and inference. Our work introduces novel methods that guarantee differential privacy, ensuring strong protection against reidentification attacks. We propose two algorithms using graph neural networks (GNNs) trained under different levels of constraints. First, we introduce RGDN, a randomized GNN architecture equipped with local sensitivity analysis, making it feasible for real-world deployments. Next, we develop the Sensitive Degrees of Freedom algorithm incorporating knowledge distillation techniques to reduce model complexity without sacrificing accuracy. Experimental results show the efficiency and effectiveness of our approaches compared to state-of-the art private GNN techniques, demonstrating their applicability to diverse domains including drug discovery, personalized medicine, marketing predictions, and fraud detection. These groundbreaking advancements provide a key step towards democratizing machine learning by addressing critical challenges surrounding transparency, security, and accountability in artificial intelligence.",1
"Machine learning solutions for pattern classification problems are nowadays widely deployed in society and industry. However, the lack of transparency and accountability of most accurate models often hinders their safe use. Thus, there is a clear need for developing explainable artificial intelligence mechanisms. There exist model-agnostic methods that summarize feature contributions, but their interpretability is limited to predictions made by black-box models. An open challenge is to develop models that have intrinsic interpretability and produce their own explanations, even for classes of models that are traditionally considered black boxes like (recurrent) neural networks. In this paper, we propose a Long-Term Cognitive Network for interpretable pattern classification of structured data. Our method brings its own mechanism for providing explanations by quantifying the relevance of each feature in the decision process. For supporting the interpretability without affecting the performance, the model incorporates more flexibility through a quasi-nonlinear reasoning rule that allows controlling nonlinearity. Besides, we propose a recurrence-aware decision model that evades the issues posed by unique fixed points while introducing a deterministic learning method to compute the tunable parameters. The simulations show that our interpretable model obtains competitive results when compared to the state-of-the-art white and black-box models.",0
"This paper presents a novel approach to explainable pattern classification using recurrent neural networks (RNNs) that are specifically designed to incorporate time dependencies into their internal representations. We propose a new architecture called a ""long-term cognitive network"" (LTCN), which uses a combination of RNN cells and convolutional layers to model sequences of data while taking advantage of the strong expressive power of these two types of layers. LTCNs are trained on sequential tasks that require maintaining a memory state over long periods of time, allowing them to learn temporal patterns that would be difficult or impossible for traditional feedforward models to capture.  The key innovation of our method lies in its ability to explicitly represent recurrences within deep architectures without resorting to specialized techniques such as cycle consistency loss or attention mechanisms. By doing so, we achieve more efficient training and better generalization performance compared to prior methods based on attentional or generative models.  Our experiments demonstrate that LTCNs significantly outperform several baseline models on four challenging datasets: MIMIC III, HAR, PAMAP2, and Physionet/CinC Challenge. Moreover, we show that LTCNs provide interpretable explanations of their decision making process by visualizing the evolution of activation patterns throughout each layer over time. Our analysis reveals interesting insights into how different parts of the network contribute to making predictions at various stages during sequence processing.  Overall, this work represents a step forward towards developing recurrence-aware neural networks capable of solving real-world problems in areas such as healthcare informatics where understanding the underlying patterns and dynamics is crucial for improving patient outcomes.",1
"Graph neural networks (GNNs) are powerful models for many graph-structured tasks. Existing models often assume that a complete structure of a graph is available during training, however, in practice, graph-structured data is usually formed in a streaming fashion, so that learning a graph continuously is often necessary. In this paper, we aim to bridge GNN to lifelong learning by converting a graph problem to a regular learning problem, so that GNN is able to inherit the lifelong learning techniques developed for convolutional neural networks (CNNs). To this end, we propose a new graph topology based on feature cross-correlation, called the feature graph. It takes features as new nodes and turns nodes into independent graphs. This successfully converts the original problem of node classification to graph classification, in which the increasing nodes are turned into independent training samples. In the experiments, we demonstrate the efficiency and effectiveness of feature graph networks (FGN) by continuously learning a sequence of classical graph datasets. We also show that FGN achieves superior performance in human action recognition with distributed streaming signals for wearable devices.",0
"Here is a sample abstract:  Lifelong learning has been an active area of research in recent years as machine learning models face increasingly dynamic environments where they need to adapt to new tasks on their own. One approach that has gained popularity in addressing this challenge is graph neural networks (GNNs). However, current GNN methods focus mainly on single task learning and can suffer from performance degradation when faced with multiple tasks. To address this limitation, we propose a lifelong graph learning framework called GLGNN (Graph Learning and Generalization Network), which integrates graph structured knowledge distillation with regularization techniques to achieve continuous adaptation across different tasks. Our method enables better generalization by utilizing task similarities and shared structures among graphs. Experimental results demonstrate that our approach outperforms state-of-the-art lifelong learning GNN methods on benchmark datasets and real-world applications. This study highlights the potential benefits of incorporating both model transferability and capacity for handling multiple tasks during the training process.  Abstract: ""Learning to Adapt Across Tasks: Graph Neural Networks Meet Transfer Learning""  In this work, we present a comprehensive investigation into the use of graph neural networks (GNN) for continual learning under varying task conditions. We begin with a thorough analysis of the challenges presented when deploying traditional GNN architectures in these scenarios before introducing several strategies designed to mitigate against overfitting and promote generalizability. These techniques leverage insights drawn from classical machine learning literature while remaining true to the core principles of message passing and attention mechanisms at the heart of contemporary graph-based deep learning solutions. Through extensive experiments spanning multiple domains, we show that each component of our system can bring meaningful improvements to existing workflows in isolation but together lead to marked enhancements in overall performance. Ultimately, our findings suggest that the marriage of advanced neural network components alongside well-established concepts from transfer learning holds significant promise for researchers looking to build robust systems capable of thriving under uncertainty",1
"Deep learning models, such as convolutional neural networks, have long been applied to image and multi-media tasks, particularly those with structured data. More recently, there has been more attention to unstructured data that can be represented via graphs. These types of data are often found in health and medicine, social networks, and research data repositories. Graph convolutional neural networks have recently gained attention in the field of deep learning that takes advantage of graph-based data representation with automatic feature extraction via convolutions. Given the popularity of these methods in a wide range of applications, robust uncertainty quantification is vital. This remains a challenge for large models and unstructured datasets. Bayesian inference provides a principled approach to uncertainty quantification of model parameters for deep learning models. Although Bayesian inference has been used extensively elsewhere, its application to deep learning remains limited due to the computational requirements of the Markov Chain Monte Carlo (MCMC) methods. Recent advances in parallel computing and advanced proposal schemes in MCMC sampling methods has opened the path for Bayesian deep learning. In this paper, we present Bayesian graph convolutional neural networks that employ tempered MCMC sampling with Langevin-gradient proposal distribution implemented via parallel computing. Our results show that the proposed method can provide accuracy similar to advanced optimisers while providing uncertainty quantification for key benchmark problems.",0
"Title: ""Bayesian Graph Convolutional Neural Networks Using Tempered Markov Chain Monte Carlo."" Abstract: In recent years, there has been increasing interest in applying machine learning techniques to complex scientific problems. One promising approach is the use of graph convolutional neural networks (CNN), which can leverage both spatial and spectral features of data on graphs. However, training these models often requires large amounts of annotated data, which can be difficult to obtain in many applications. To address this issue, we propose using tempered Markov chain Monte Carlo methods to train Bayesian CNNs that efficiently estimate uncertainty in their predictions. Our approach allows us to effectively regularize our model by incorporating prior knowledge through probabilistic inference. We demonstrate the effectiveness of our method on several benchmark datasets, showing that it outperforms standard variational approaches in terms of accuracy and robustness. Our work paves the way for more reliable and accurate predictions in real-world scenarios where annotation data may be limited.",1
"A Graph Convolutional Network (GCN) stacks several layers and in each layer performs a PROPagation operation (PROP) and a TRANsformation operation (TRAN) for learning node representations over graph-structured data. Though powerful, GCNs tend to suffer performance drop when the model gets deep. Previous works focus on PROPs to study and mitigate this issue, but the role of TRANs is barely investigated. In this work, we study performance degradation of GCNs by experimentally examining how stacking only TRANs or PROPs works. We find that TRANs contribute significantly, or even more than PROPs, to declining performance, and moreover that they tend to amplify node-wise feature variance in GCNs, causing variance inflammation that we identify as a key factor for causing performance drop. Motivated by such observations, we propose a variance-controlling technique termed Node Normalization (NodeNorm), which scales each node's features using its own standard deviation. Experimental results validate the effectiveness of NodeNorm on addressing performance degradation of GCNs. Specifically, it enables deep GCNs to outperform shallow ones in cases where deep models are needed, and to achieve comparable results with shallow ones on 6 benchmark datasets. NodeNorm is a generic plug-in and can well generalize to other GNN architectures. Code is publicly available at https://github.com/miafei/NodeNorm.",0
"Title: Uncovering Factors Causing Performance Decline in Graph Convolutional Networks  As graph convolutional networks (GCN) gain increasing popularity due to their outstanding performance on semi-supervised tasks involving irregularly structured data such as social network graphs, understanding the factors causing performance decline becomes crucial. While existing GCN models have demonstrated impressive results, researchers still struggle to fully comprehend why certain modifications might lead to poorer outcomes, hindering progress in developing cutting-edge architectures that consistently deliver improved accuracy. This study seeks to shed light on those issues by analyzing key aspects impacting GCN performance. By scrutinizing architectural components, data characteristics, training strategies, and hyperparameter configurations, we unveil root causes underlying degraded model outputs. Our findings offer valuable insights into constructing efficient models tailored to specific use cases while helping practitioners diagnose common problems encountered during implementation. With extensive experimental validation and detailed discussion, our work provides a well-rounded examination of performance bottlenecks in GCN models, serving both academics and industry professionals alike.",1
"Transformer neural networks have achieved state-of-the-art results for unstructured data such as text and images but their adoption for graph-structured data has been limited. This is partly due to the difficulty of incorporating complex structural information in the basic transformer framework. We propose a simple yet powerful extension to the transformer - residual edge channels. The resultant framework, which we call Edge-augmented Graph Transformer (EGT), can directly accept, process and output structural information as well as node information. It allows us to use global self-attention, the key element of transformers, directly for graphs and comes with the benefit of long-range interaction among nodes. Moreover, the edge channels allow the structural information to evolve from layer to layer, and prediction tasks on edges/links can be performed directly from the output embeddings of these channels. In addition, we introduce a generalized positional encoding scheme for graphs based on Singular Value Decomposition which can improve the performance of EGT. Our framework, which relies on global node feature aggregation, achieves better performance compared to Convolutional/Message-Passing Graph Neural Networks, which rely on local feature aggregation within a neighborhood. We verify the performance of EGT in a supervised learning setting on a wide range of experiments on benchmark datasets. Our findings indicate that convolutional aggregation is not an essential inductive bias for graphs and global self-attention can serve as a flexible and adaptive alternative.",0
"This sounds like an interesting study on graph transformers! Here is a potential abstract for your paper:  ""In recent years, there has been growing interest in using deep learning techniques such as graph transformers to model complex relationships between data points. However, one common challenge faced by researchers working with graph transformers is how to handle edge information in addition to node features. Some approaches have proposed augmenting graph transformer models with additional layers specifically designed for processing edges, but these can lead to increased computational complexity and may not always outperform simpler methods. In this work, we explore whether global self-attention mechanisms alone can effectively capture important edge information, without needing specialized edge-specific layers. Our experiments show that indeed, relying solely on global attention is sufficient for achieving strong performance on benchmark datasets across a variety of tasks.""  Let me know if you would like any further revisions made. I am here to assist.",1
"Link prediction is one of the key problems for graph-structured data. With the advancement of graph neural networks, graph autoencoders (GAEs) and variational graph autoencoders (VGAEs) have been proposed to learn graph embeddings in an unsupervised way. It has been shown that these methods are effective for link prediction tasks. However, they do not work well in link predictions when a node whose degree is zero (i.g., isolated node) is involved. We have found that GAEs/VGAEs make embeddings of isolated nodes close to zero regardless of their content features. In this paper, we propose a novel Variational Graph Normalized AutoEncoder (VGNAE) that utilize L2-normalization to derive better embeddings for isolated nodes. We show that our VGNAEs outperform the existing state-of-the-art models for link prediction tasks. The code is available at https://github.com/SeongJinAhn/VGNAE.",0
"In recent years, graph normalization has emerged as an important technique in deep learning, allowing for improved performance on tasks such as image generation and semantic segmentation. However, most existing graph normalization methods focus solely on feedforward neural networks, rather than more expressive model classes like variational autoencoders (VAEs). To address this limitation, we present Variational Graph Normalized Auto-Encoders (VGNAEs), which extend standard VAEs with novel graph normalization modules that can improve their performance across multiple benchmark datasets. Our approach involves training an encoder network to learn both an estimate of data density and a mapping from input variables to lower dimensional latent code, while also leveraging graph attention mechanisms to enable flexible, adaptive normalization during training and inference. We demonstrate the effectiveness of our method through comprehensive experiments comparing against state-of-the-art baselines, showing significant improvements on key metrics across a range of challenging tasks. Overall, VGNAEs provide a powerful new tool for deep learning researchers seeking to develop high-performing generative models using modern graph normalization techniques.",1
"Data augmentation has been widely used in image data and linguistic data but remains under-explored on graph-structured data. Existing methods focus on augmenting the graph data from a global perspective and largely fall into two genres: structural manipulation and adversarial training with feature noise injection. However, the structural manipulation approach suffers information loss issues while the adversarial training approach may downgrade the feature quality by injecting noise. In this work, we introduce the local augmentation, which enhances node features by its local subgraph structures. Specifically, we model the data argumentation as a feature generation process. Given the central node's feature, our local augmentation approach learns the conditional distribution of its neighbors' features and generates the neighbors' optimal feature to boost the performance of downstream tasks. Based on the local augmentation, we further design a novel framework: LA-GNN, which can apply to any GNN models in a plug-and-play manner. Extensive experiments and analyses show that local augmentation consistently yields performance improvement for various GNN architectures across a diverse set of benchmarks. Code is available at https://github.com/Soughing0823/LAGNN.",0
"Recently there has been a surge of interest in developing models that can leverage graph structures within machine learning tasks such as node classification, edge prediction and graph generation. In particular, attention mechanisms have gained traction due to their ability to adaptively select which elements of a structured input to focus on at any given time step during training or inference. However, while these methods have shown great promise in many scenarios, they often suffer from scalability issues arising from the high computational cost of applying global attention operators over entire graphs. Here we explore local augmentations to address these limitations by adapting attention mechanisms to operate independently around small substructures of individual nodes and edges within larger graphs. These techniques significantly reduce the number of matrix multiplications required by traditional attention networks while maintaining competitive accuracy, enabling faster model evaluation and inference on smaller hardware devices. We empirically evaluate our approach on several benchmark datasets across diverse domains including social network analysis, bioinformatics and chemical compound optimization. Our results show consistent improvements both in terms of execution time and test set performance, demonstrating the effectiveness of the proposed methodology in improving the efficiency of graph neural networks without sacrificing their overall performance.",1
"Graph Neural Network (GNN) research is rapidly growing thanks to the capacity of GNNs in learning distributed representations from graph-structured data. However, centralizing a massive amount of real-world graph data for GNN training is prohibitive due to privacy concerns, regulation restrictions, and commercial competitions. Federated learning (FL), a trending distributed learning paradigm, provides possibilities to solve this challenge while preserving data privacy. Despite recent advances in vision and language domains, there is no suitable platform for the FL of GNNs. To this end, we introduce FedGraphNN, an open FL benchmark system that can facilitate research on federated GNNs. FedGraphNN is built on a unified formulation of graph FL and contains a wide range of datasets from different domains, popular GNN models, and FL algorithms, with secure and efficient system support. Particularly for the datasets, we collect, preprocess, and partition 36 datasets from 7 domains, including both publicly available ones and specifically obtained ones such as hERG and Tencent. Our empirical analysis showcases the utility of our benchmark system, while exposing significant challenges in graph FL: federated GNNs perform worse in most datasets with a non-IID split than centralized GNNs; the GNN model that attains the best result in the centralized setting may not maintain its advantage in the FL setting. These results imply that more research efforts are needed to unravel the mystery behind federated GNNs. Moreover, our system performance analysis demonstrates that the FedGraphNN system is computationally efficient and secure to large-scale graphs datasets. We maintain the source code at https://github.com/FedML-AI/FedGraphNN.",0
"The Fedgraphnn paper presents a new federated learning system and benchmark framework which allows researchers and developers to build and evaluate graph neural networks on distributed datasets. The authors discuss how current challenges related to privacy and scalability limit the development and deployment of GNNs. They then introduce their system which uses federated averaging, a decentralized optimization algorithm commonly used in federated learning, as well as two benchmark tasks (node classification and link prediction). The paper evaluates several popular GNN models using the proposed system and demonstrates that their method can achieve comparable results while preserving data privacy and reducing communication overhead compared to traditional centralized methods. This work represents a step forward towards enabling widespread use of GNNs by addressing key concerns and providing a standard evaluation platform for future research efforts in this field.",1
"Interpreting deep neural networks from the ordinary differential equations (ODEs) perspective has inspired many efficient and robust network architectures. However, existing ODE based approaches ignore the relationship among data points, which is a critical component in many problems including few-shot learning and semi-supervised learning. In this paper, inspired by the diffusive ODEs, we propose a novel diffusion residual network (Diff-ResNet) to strengthen the interactions among data points. Under the structured data assumption, it is proved that the diffusion mechanism can decrease the distance-diameter ratio that improves the separability of inter-class points and reduces the distance among local intra-class points. This property can be easily adopted by the residual networks for constructing the separable hyperplanes. The synthetic binary classification experiments demonstrate the effectiveness of the proposed diffusion mechanism. Moreover, extensive experiments of few-shot image classification and semi-supervised graph node classification in various datasets validate the advantages of the proposed Diff-ResNet over existing few-shot learning methods.",0
"Deep learning has revolutionized many application domains, but designing deep models that can handle few-shot tasks remains challenging. While recent methods based on meta learning have achieved remarkable results, they still suffer from limitations such as lack of stability and sensitivity to hyperparameters. In our work, we propose a new approach for few-shot learning using diffused residual networks (Diff-ResNets). Our method employs ODEs to model complex data relationships without relying on explicit representations like neural networks. Instead, we learn interpretable basis functions that capture task-specific information, allowing us to achieve better generalization and robustness across diverse tasks. We evaluate our method on standard benchmark datasets and demonstrate state-of-the art performance while maintaining computational efficiency and interpretability. Overall, our findings highlight the potential of ODE-based approaches for few-shot learning and open up new directions for future research.",1
"The success of machine learning stems from its structured data representation. Similar data have close representation as compressed codes for classification or emerged labels for clustering. We observe that the frequency of the internal representation follows power laws in both supervised and unsupervised learning. The scale-invariant distribution implies that machine learning largely compresses frequent typical data, and at the same time, differentiates many atypical data as outliers. In this study, we derive how the power laws can naturally arise in machine learning. In terms of information theory, the scale-invariant representation corresponds to a maximally uncertain data grouping among possible representations that guarantee pre-specified learning accuracy.",0
"Machine Learning (ML) has been used successfully across many applications over the last decade. As datasets grow larger and more complex, scaling up ML models becomes important to meet growing demand for performance. Unfortunately, as these models scale up there is often a corresponding increase in computational complexity. This makes deployment difficult as well as increasing costs. In addition, current techniques for scaling up ML require significant retraining time on existing models, even if one only wishes to change parameters of those models. Researchers at Company have developed a method that allows them to scale the size of data without incurring the same increase in model size and training cost which enables improved model performance while simultaneously reducing latency. The new technique can be applied retroactively to previously trained neural networks without having to go back through all previous layers of computation making it possible to achieve high accuracy models within minutes rather than hours. This research helps pave the road towards realtime large language processing capabilities using deep learning methods such as natural language understanding. Additionally, the research shows promise for developing explainable versions of other classes of large language models as the architecture utilized by the new approach was found highly interpretable during internal experiments allowing for more transparent systems design. Overall, our work represents a step forward towards applying deep learning techniques to problems where rapid feedback is required and achieving state of the art results quickly",1
"Sensitive medical data is often subject to strict usage constraints. In this paper, we trained a generative adversarial network (GAN) on real-world electronic health records (EHR). It was then used to create a data-set of ""fake"" patients through synthetic data generation (SDG) to circumvent usage constraints. This real-world data was tabular, binary, intensive care unit (ICU) patient diagnosis data. The entire data-set was split into separate data silos to mimic real-world scenarios where multiple ICU units across different hospitals may have similarly structured data-sets within their own organisations but do not have access to each other's data-sets. We implemented federated learning (FL) to train separate GANs locally at each organisation, using their unique data silo and then combining the GANs into a single central GAN, without any siloed data ever being exposed. This global, central GAN was then used to generate the synthetic patients data-set. We performed an evaluation of these synthetic patients with statistical measures and through a structured review by a group of medical professionals. It was shown that there was no significant reduction in the quality of the synthetic EHR when we moved between training a single central model and training on separate data silos with individual models before combining them into a central model. This was true for both the statistical evaluation (Root Mean Square Error (RMSE) of 0.0154 for single-source vs. RMSE of 0.0169 for dual-source federated) and also for the medical professionals' evaluation (no quality difference between EHR generated from a single source and EHR generated from multiple sources).",0
"Electronic health records (EHRs) contain vast amounts of data that can be used to improve patient outcomes, develop new treatments, and inform public health policy. However, accessing these records can be difficult due to privacy concerns and limited accessibility. To address these challenges, we propose using federated learning and generative adversarial networks (GANs) to generate synthetic EHRs that preserve the statistical properties of real EHRs while protecting individual patient identities.  Our approach involves training a GAN on a subset of de-identified EHRs from multiple institutions. The generator network produces synthetic EHRs, which are then evaluated by a discriminator network that determines if they are real or fake. Through iterative training, the generator learns to create more realistic synthetic EHRs that closely mimic real EHRs. These synthetic EHRs can then be shared among researchers without compromising patient privacy.  We evaluate our method through experiments involving two datasets containing over one million de-identified EHRs from different hospitals. Our results show that our model effectively generates synthetic EHRs that match the distribution of real EHRs across various clinical measures such as age, gender, admission type, length of stay, and mortality rate. We further demonstrate that these synthetic EHRs maintain patient privacy by conducting differential privacy analysis and showing low disclosure risk under simulated attacks.  Overall, our work demonstrates the potential of federated GANs in generating synthetic EHRs that could revolutionize medical research and practice while preserving patients’ privacy. Our future work includes expanding the dataset size and diversity, evaluating the performance of the generated records in downstream tasks such as disease diagnosis and treatment recommendations, and exploring additional applications such as personalized medicine and drug development.",1
"Message-Passing Neural Networks (MPNNs), the most prominent Graph Neural Network (GNN) framework, celebrate much success in the analysis of graph-structured data. Concurrently, the sparsification of Neural Network models attracts a great amount of academic and industrial interest. In this paper, we conduct a structured study of the effect of sparsification on the trainable part of MPNNs known as the Update step. To this end, we design a series of models to successively sparsify the linear transform in the Update step. Specifically, we propose the ExpanderGNN model with a tuneable sparsification rate and the Activation-Only GNN, which has no linear transform in the Update step. In agreement with a growing trend in the literature, the sparsification paradigm is changed by initialising sparse neural network architectures rather than expensively sparsifying already trained architectures. Our novel benchmark models enable a better understanding of the influence of the Update step on model performance and outperform existing simplified benchmark models such as the Simple Graph Convolution. The ExpanderGNNs, and in some cases the Activation-Only models, achieve performance on par with their vanilla counterparts on several downstream tasks while containing significantly fewer trainable parameters. In experiments with matching parameter numbers, our benchmark models outperform the state-of-the-art GNN models. Our code is publicly available at: https://github.com/ChangminWu/ExpanderGNN.",0
"Graph neural networks (GNNs) have shown promising results across numerous applications such as computer vision, natural language processing, and recommendation systems. However, the high computational complexity and memory requirements hinder their deployment on resource-constrained devices. One key component responsible for the high computation cost and large memory footprint of GNNs is the update step, which performs convolution operations over neighborhood structures, leading to redundant computations and dense representations. To address these issues, we propose sparsifying the graph structure before applying the GNN models to reduce unnecessary calculations while preserving most of the important features. Our method leverages random edge sampling techniques combined with pruning algorithms based on statistical analysis of the graphs’ edge importance. Our experiments demonstrate that our approach yields significant improvements in both accuracy and speed compared with full graph GNNs while retaining comparable performance. In summary, our study presents a novel technique to make GNNs more efficient without sacrificing their effectiveness. By introducing a simple yet powerful modification to the traditional graph update process, we enable the use of GNNs on resource-constrained environments.",1
"Graph feature extraction is a fundamental task in graphs analytics. Using feature vectors (graph descriptors) in tandem with data mining algorithms that operate on Euclidean data, one can solve problems such as classification, clustering, and anomaly detection on graph-structured data. This idea has proved fruitful in the past, with spectral-based graph descriptors providing state-of-the-art classification accuracy on benchmark datasets. However, these algorithms do not scale to large graphs since: 1) they require storing the entire graph in memory, and 2) the end-user has no control over the algorithm's runtime. In this paper, we present single-pass streaming algorithms to approximate structural features of graphs (counts of subgraphs of order $k \geq 4$). Operating on edge streams allows us to avoid keeping the entire graph in memory, and controlling the sample size enables us to control the time taken by the algorithm. We demonstrate the efficacy of our descriptors by analyzing the approximation error, classification accuracy, and scalability to massive graphs. Our experiments showcase the effect of the sample size on approximation error and predictive accuracy. The proposed descriptors are applicable on graphs with millions of edges within minutes and outperform the state-of-the-art descriptors in classification accuracy.",0
"In recent years, graph descriptors have become increasingly important in many applications such as computer vision and data mining. These descriptors aim to capture the topological properties of graphs that can effectively summarize their structure in a scalar value. However, computing these descriptors efficiently over large graphs remains challenging due to their quadratic computational complexity. This paper addresses this problem by proposing a new method for computing graph descriptors on edge streams, which are sequences of edges in a graph that arrive incrementally over time. Our approach leverages the inherent parallelism offered by stream processing engines to significantly reduce the computation cost while preserving accuracy. We demonstrate through extensive experiments on real-world datasets that our algorithm outperforms state-of-the-art baselines in terms of both scalability and efficiency. Additionally, we showcase the versatility of our framework by applying it to different types of graph descriptors and streaming settings, including one-pass and multi-pass processing. Our work paves the way towards enabling efficient graph analysis over dynamic and evolving graphs with high-throughput requirements.",1
"Networks are ubiquitous in the real world such as social networks and communication networks, and anomaly detection on networks aims at finding nodes whose structural or attributed patterns deviate significantly from the majority of reference nodes. However, most of the traditional anomaly detection methods neglect the relation structure information among data points and therefore cannot effectively generalize to the graph structure data. In this paper, we propose an end-to-end model of Deep Dual Support Vector Data description based Autoencoder (Dual-SVDAE) for anomaly detection on attributed networks, which considers both the structure and attribute for attributed networks. Specifically, Dual-SVDAE consists of a structure autoencoder and an attribute autoencoder to learn the latent representation of the node in the structure space and attribute space respectively. Then, a dual-hypersphere learning mechanism is imposed on them to learn two hyperspheres of normal nodes from the structure and attribute perspectives respectively. Moreover, to achieve joint learning between the structure and attribute of the network, we fuse the structure embedding and attribute embedding as the final input of the feature decoder to generate the node attribute. Finally, abnormal nodes can be detected by measuring the distance of nodes to the learned center of each hypersphere in the latent structure space and attribute space respectively. Extensive experiments on the real-world attributed networks show that Dual-SVDAE consistently outperforms the state-of-the-arts, which demonstrates the effectiveness of the proposed method.",0
"In recent years, data mining has become increasingly important as more businesses and organizations collect large amounts of data on their operations and customers. However, making sense of all that data can be difficult without the proper tools and methods. One approach that has gained popularity recently is anomaly detection - identifying unexpected patterns or behaviors in the data that could indicate potential problems or opportunities. This paper presents a new method called ""Deep Dual Support Vector Data Description"" (DDSVD2) for detecting anomalies in attributed networks.  Attributed networks have nodes that represent objects or entities and edges that describe relationships or interactions between them. Many real-world systems, such as social networks, transportation networks, and communication networks, can be modeled using attributed networks. Detecting anomalies in these networks is crucial for understanding system behavior and improving decision-making processes.  The proposed DDSVD2 method uses deep learning techniques to identify anomalies by modeling the structure and properties of attributed networks. Specifically, we use convolutional neural networks (CNNs) to learn a low-dimensional representation of network structures based on node attributes and graph topology. We then apply a robust multi-task optimization framework to obtain sparse representations of both normal and abnormal samples, which allows us to separate anomalous patterns from normal ones effectively. Finally, we introduce a novel regularizer to constrain the solution space during training, further enhancing performance and stability.  Experimental results demonstrate the effectiveness of our method compared with state-of-the-art baselines across several benchmark datasets representing different types of anomalies in attributed networks. Our approach achieves superior accuracy while maintaining computational efficiency and scalability, making it suitable for a wide range of applications.  Overall, this work contributes to the field of anomaly de",1
"Graph neural networks (GNNs) have shown great power in modeling graph structured data. However, similar to other machine learning models, GNNs may make biased predictions w.r.t protected sensitive attributes, e.g., skin color and gender. This is because the training data often contains historical bias towards sensitive attributes. In addition, we empirically show that the discrimination in GNNs can be magnified by graph structures and the message-passing mechanism of GNNs. As a result, the applications of GNNs in high-stake domains such as crime rate prediction would be largely limited. Though extensive studies of fair classification have been conducted on i.i.d data, methods to address the problem of discrimination on non-i.i.d data are rather limited. Generally, learning fair models require abundant sensitive attributes to regularize the model. However, for many graphs such as social networks, users are reluctant to share sensitive attributes. Thus, only limited sensitive attributes are available for fair GNN training in practice. Moreover, directly collecting and applying the sensitive attributes in fair model training may cause privacy issues, because the sensitive information can be leaked in data breach or attacks on the trained model. Therefore, we study a novel and crucial problem of learning fair GNNs with limited and private sensitive attribute information. In an attempt to address these problems, FairGNN is proposed to eliminate the bias of GNNs whilst maintaining high accuracy by leveraging graph structures and limited sensitive information. We further extend FairGNN to NT-FairGNN which can achieve both fairness and privacy on sensitive attributes by using limited and private sensitive attributes. Theoretical analysis and extensive experiments on real-world datasets demonstrate the effectiveness of FairGNN and NT-FairGNN in achieving fair and high-accurate classification.",0
"Abstract: Artificial neural networks have become increasingly important tools for analyzing data, making predictions and enabling decision-making processes. However, using these models can raise privacy concerns as they may require access to sensitive personal attributes that users would prefer to keep confidential. To address this challenge, we propose a new methodology called ""Fair Graph Neural Network"" (fGNN) which enables training machine learning algorithms on datasets containing limited and private attribute information. This approach is based on graph convolutional networks and uses node features as well as their neighborhood relationships to infer missing values. We demonstrate through experimental evaluations that our model achieves high accuracy while maintaining strict privacy requirements. The proposed solution has applications across various domains including medical research, financial analysis, social network analysis, among others. Overall, fGNN provides an effective means to balance the need for accurate prediction models against the desire for privacy and data security.",1
Learning distributions over graph-structured data is a challenging task with many applications in biology and chemistry. In this work we use an energy-based model (EBM) based on multi-channel graph neural networks (GNN) to learn permutation invariant unnormalized density functions on graphs. Unlike standard EBM training methods our approach is to learn the model via minimizing adversarial stein discrepancy. Samples from the model can be obtained via Langevin dynamics based MCMC. We find that this approach achieves competitive results on graph generation compared to benchmark models.,0
"Advances in deep learning have made generative models more versatile than ever before, enabling a wide range of applications across numerous domains. However, many existing approaches remain limited by their reliance on simplified mathematical models that fail to capture key aspects of real-world systems. In this work, we propose a novel approach based on adversarial training to overcome these limitations and develop graph energy models (GEMs) capable of generating high-fidelity data samples. Our method leverages adversarial techniques to optimize GEM parameters, producing highly accurate and efficient models suitable for complex tasks such as drug discovery and protein structure prediction. We demonstrate our model’s superior performance compared to state-of-the-art alternatives through extensive experimentation, showcasing its ability to achieve remarkable accuracy while maintaining interpretability and robustness. This research paves the way for broader adoption of generative methods in scientific inquiry, opening up new possibilities in fields where precise predictive models are crucial.",1
"UMAP is a non-parametric graph-based dimensionality reduction algorithm using applied Riemannian geometry and algebraic topology to find low-dimensional embeddings of structured data. The UMAP algorithm consists of two steps: (1) Compute a graphical representation of a dataset (fuzzy simplicial complex), and (2) Through stochastic gradient descent, optimize a low-dimensional embedding of the graph. Here, we extend the second step of UMAP to a parametric optimization over neural network weights, learning a parametric relationship between data and embedding. We first demonstrate that Parametric UMAP performs comparably to its non-parametric counterpart while conferring the benefit of a learned parametric mapping (e.g. fast online embeddings for new data). We then explore UMAP as a regularization, constraining the latent distribution of autoencoders, parametrically varying global structure preservation, and improving classifier accuracy for semi-supervised learning by capturing structure in unlabeled data. Google Colab walkthrough: https://colab.research.google.com/drive/1WkXVZ5pnMrm17m0YgmtoNjM_XHdnE5Vp?usp=sharing",0
"Abstract: In this paper, we investigate the use of Parametric Uniform Manifold Approximation and Projection (UMAP) embeddings as an alternative to traditional low-dimensional representations such as t-Distributed Stochastic Neighbor Embedding (t-SNE) and UMAP itself. We demonstrate that the additional parameters introduced by Parametric UMAP can improve both the quality of the embeddings themselves and their utility in downstream tasks using Semi Supervised Learning(SSL).",1
"Graph Neural Networks (GNNs), which generalize the deep neural networks to graph-structured data, have achieved great success in modeling graphs. However, as an extension of deep learning for graphs, GNNs lack explainability, which largely limits their adoption in scenarios that demand the transparency of models. Though many efforts are taken to improve the explainability of deep learning, they mainly focus on i.i.d data, which cannot be directly applied to explain the predictions of GNNs because GNNs utilize both node features and graph topology to make predictions. There are only very few work on the explainability of GNNs and they focus on post-hoc explanations. Since post-hoc explanations are not directly obtained from the GNNs, they can be biased and misrepresent the true explanations. Therefore, in this paper, we study a novel problem of self-explainable GNNs which can simultaneously give predictions and explanations. We propose a new framework which can find $K$-nearest labeled nodes for each unlabeled node to give explainable node classification, where nearest labeled nodes are found by interpretable similarity module in terms of both node similarity and local structure similarity. Extensive experiments on real-world and synthetic datasets demonstrate the effectiveness of the proposed framework for explainable node classification.",0
"Graph neural networks (GNNs) have shown great promise in handling graph structured data which appears pervasively across various domains. However, despite their successes, explaining why GNNs produce certain outputs remains difficult because the intrinsic nonlinearity and parallel nature of these models makes it hard to ascertain their reasoning process. In our proposed methodology, we provide a way towards self-explainability through a novel model architecture called Diffusion GCN, wherein we design explainable units that capture relevant features from different neighborhood areas within the input graphs. These mechanisms allow us to accurately interpret how each node contribution impacts the final prediction outcome and thus furnish insights into the decision-making process. Experimental results on real world benchmark datasets validate the effectiveness of Diffusion GCN against other state-of-the art methods by achieving competitive accuracy while providing meaningful explanations, making it appealing for applications demanding transparency, such as social network analysis and drug discovery.",1
"Representation learning on static graph-structured data has shown a significant impact on many real-world applications. However, less attention has been paid to the evolving nature of temporal networks, in which the edges are often changing over time. The embeddings of such temporal networks should encode both graph-structured information and the temporally evolving pattern. Existing approaches in learning temporally evolving network representations fail to capture the temporal interdependence. In this paper, we propose Toffee, a novel approach for temporal network representation learning based on tensor decomposition. Our method exploits the tensor-tensor product operator to encode the cross-time information, so that the periodic changes in the evolving networks can be captured. Experimental results demonstrate that Toffee outperforms existing methods on multiple real-world temporal networks in generating effective embeddings for the link prediction tasks.",0
"This paper presents a new framework calledTemporal Network Embedding that utilizes tensor factorization techniques to embed networks into low dimensional spaces. By capturing latent features that exist across multiple snapshots of network data over time, the approach provides meaningful representations that facilitate downstream analysis tasks such as anomaly detection, node classification, and clustering. Results on several benchmark datasets demonstrate significant improvements over state-of-the-art methods. The methodology has promising applications in domains where temporal dynamics play important roles, including social science, transportation systems, and finance. The code is made publicly available to foster future research.",1
"Graph structured data have enabled several successful applications such as recommendation systems and traffic prediction, given the rich node features and edges information. However, these high-dimensional features and high-order adjacency information are usually heterogeneous and held by different data holders in practice. Given such vertical data partition (e.g., one data holder will only own either the node features or edge information), different data holders have to develop efficient joint training protocols rather than directly transfer data to each other due to privacy concerns. In this paper, we focus on the edge privacy, and consider a training scenario where Bob with node features will first send training node features to Alice who owns the adjacency information. Alice will then train a graph neural network (GNN) with the joint information and release an inference API. During inference, Bob is able to provide test node features and query the API to obtain the predictions for test nodes. Under this setting, we first propose a privacy attack LinkTeller via influence analysis to infer the private edge information held by Alice via designing adversarial queries for Bob. We then empirically show that LinkTeller is able to recover a significant amount of private edges, outperforming existing baselines. To further evaluate the privacy leakage, we adapt an existing algorithm for differentially private graph convolutional network (DP GCN) training and propose a new DP GCN mechanism LapGraph. We show that these DP GCN mechanisms are not always resilient against LinkTeller empirically under mild privacy guarantees ($\varepsilon5$). Our studies will shed light on future research towards designing more resilient privacy-preserving GCN models; in the meantime, provide an in-depth understanding of the tradeoff between GCN model utility and robustness against potential privacy attacks.",0
This should summarize the main ideas of the paper without getting into technical details but still conveying their importance. We aim at submitting this research work to IEEE transactions on artificial intelligence which has a very competitive acceptance rate. So it should sound like something important while staying short and sweet. Thanks!,1
"Spatio-temporal forecasting is of great importance in a wide range of dynamical systems applications from atmospheric science, to recent COVID-19 spread modeling. These applications rely on accurate predictions of spatio-temporal structured data reflecting real-world phenomena. A stunning characteristic is that the dynamical system is not only driven by some physics laws but also impacted by the localized factor in spatial and temporal regions. One of the major challenges is to infer the underlying causes, which generate the perceived data stream and propagate the involved causal dynamics through the distributed observing units. Another challenge is that the success of machine learning based predictive models requires massive annotated data for model training. However, the acquisition of high-quality annotated data is objectively manual and tedious as it needs a considerable amount of human intervention, making it infeasible in fields that require high levels of expertise. To tackle these challenges, we advocate a spatio-temporal physics-coupled neural networks (ST-PCNN) model to learn the underlying physics of the dynamical system and further couple the learned physics to assist the learning of the recurring dynamics. To deal with data-acquisition constraints, an active learning mechanism with Kriging for actively acquiring the most informative data is proposed for ST-PCNN training in a partially observable environment. Our experiments on both synthetic and real-world datasets exhibit that the proposed ST-PCNN with active learning converges to near optimal accuracy with substantially fewer instances.",0
"Abstract: In this work we present a novel framework called ""Physics-Coupled Spatio-Temporal Active Learning"" (PCSAL) that enables data-efficient learning of nonlinear dynamical systems by actively sampling their spatiotemporal states under uncertainties. Unlike existing methods, which often rely on predefined uncertainty metrics and fixed sampling strategies, PCSAL adapts the spatio-temporal distribution of sampling points based on both state estimations from physical simulations and model predictions. This ensures more accurate representation of dynamics along critical regions such as phase transitions or bifurcations, while also enabling efficient exploration of large datasets using minimal resources. We demonstrate the effectiveness of our approach across various examples ranging from linear chaos circuits to nonlinear thermoacoustic systems and a turbulent flow system. Our results indicate significant improvements in accuracy compared to prior active learning techniques, highlighting the advantages of incorporating physics constraints into data acquisition and exploitation processes.",1
"Graph neural networks (GNNs) have been popularly used in analyzing graph-structured data, showing promising results in various applications such as node classification, link prediction and network recommendation. In this paper, we present a new graph attention neural network, namely GIPA, for attributed graph data learning. GIPA consists of three key components: attention, feature propagation and aggregation. Specifically, the attention component introduces a new multi-layer perceptron based multi-head to generate better non-linear feature mapping and representation than conventional implementations such as dot-product. The propagation component considers not only node features but also edge features, which differs from existing GNNs that merely consider node features. The aggregation component uses a residual connection to generate the final embedding. We evaluate the performance of GIPA using the Open Graph Benchmark proteins (ogbn-proteins for short) dataset. The experimental results reveal that GIPA can beat the state-of-the-art models in terms of prediction accuracy, e.g., GIPA achieves an average test ROC-AUC of $0.8700\pm 0.0010$ and outperforms all the previous methods listed in the ogbn-proteins leaderboard.",0
"In recent years, graph learning has emerged as a powerful tool in fields such as machine learning and data analysis. However, existing algorithms suffer from drawbacks including high computational complexity, poor scalability, and limited generalizability across different types of graphs. To address these challenges, we propose GIPA (General Information Propagation Algorithm), a novel algorithm that efficiently propagates information on complex networks while overcoming limitations of prior methods. Our approach relies on a diffusion model combined with a smart neighborhood selection strategy. Extensive experiments demonstrate the superiority of our method compared to state-of-the-art alternatives in terms of accuracy, efficiency, and robustness. This work contributes to the development of effective and efficient graph learning techniques applicable to real-world problems in diverse domains.",1
"Structured text understanding on Visually Rich Documents (VRDs) is a crucial part of Document Intelligence. Due to the complexity of content and layout in VRDs, structured text understanding has been a challenging task. Most existing studies decoupled this problem into two sub-tasks: entity labeling and entity linking, which require an entire understanding of the context of documents at both token and segment levels. However, little work has been concerned with the solutions that efficiently extract the structured data from different levels. This paper proposes a unified framework named StrucTexT, which is flexible and effective for handling both sub-tasks. Specifically, based on the transformer, we introduce a segment-token aligned encoder to deal with the entity labeling and entity linking tasks at different levels of granularity. Moreover, we design a novel pre-training strategy with three self-supervised tasks to learn a richer representation. StrucTexT uses the existing Masked Visual Language Modeling task and the new Sentence Length Prediction and Paired Boxes Direction tasks to incorporate the multi-modal information across text, image, and layout. We evaluate our method for structured text understanding at segment-level and token-level and show it outperforms the state-of-the-art counterparts with significantly superior performance on the FUNSD, SROIE, and EPHOIE datasets.",0
"This paper presents ""StrucTexT"", a new model architecture that aims to improve text understanding by allowing models to learn from multiple sources of data such as images, videos, audio etc along with the traditional text data. Our approach uses multi modal transformer architectures which have achieved state of art performance on several natural language processing tasks but lack the ability to take advantage of extra modalities available at training time. In particular we propose extending these architectures to explicitly consider structure within input data and use them for jointly representing structured knowledge across different modalities. We evaluate our proposed method on several benchmark datasets including both standard NLP benchmarks like GLUE as well as more specialized ones focused on multi modal fusion and show significant improvements compared to the previous state of art methods using only textual representation of knowledge. As one concrete application, we present results showing how our trained model can generate descriptions of object scenes directly conditioned on video inputs, improving over strong baselines.",1
"One intriguing property of deep neural networks (DNNs) is their inherent vulnerability to backdoor attacks -- a trojan model responds to trigger-embedded inputs in a highly predictable manner while functioning normally otherwise. Despite the plethora of prior work on DNNs for continuous data (e.g., images), the vulnerability of graph neural networks (GNNs) for discrete-structured data (e.g., graphs) is largely unexplored, which is highly concerning given their increasing use in security-sensitive domains. To bridge this gap, we present GTA, the first backdoor attack on GNNs. Compared with prior work, GTA departs in significant ways: graph-oriented -- it defines triggers as specific subgraphs, including both topological structures and descriptive features, entailing a large design spectrum for the adversary; input-tailored -- it dynamically adapts triggers to individual graphs, thereby optimizing both attack effectiveness and evasiveness; downstream model-agnostic -- it can be readily launched without knowledge regarding downstream models or fine-tuning strategies; and attack-extensible -- it can be instantiated for both transductive (e.g., node classification) and inductive (e.g., graph classification) tasks, constituting severe threats for a range of security-critical applications. Through extensive evaluation using benchmark datasets and state-of-the-art models, we demonstrate the effectiveness of GTA. We further provide analytical justification for its effectiveness and discuss potential countermeasures, pointing to several promising research directions.",0
"In recent years, graph neural networks (GNNs) have emerged as powerful models for learning representations from graphs, achieving state-of-the-art results on a wide range of tasks including node classification, edge prediction, and community detection. However, these models are vulnerable to backdoors which can alter their behavior without changing model parameters. This work examines the threat posed by graph backdoors to GNNs and presents new techniques that improve robustness against such attacks. We demonstrate the effectiveness of our methods through extensive experiments on several benchmark datasets, showing significant improvements over existing defenses. Our findings highlight the importance of developing effective defense mechanisms to protect machine learning models from adversarial inputs.",1
"Deep convolutional neural networks (CNNs) have shown outstanding performance in the task of semantically segmenting images. Applying the same methods on 3D data still poses challenges due to the heavy memory requirements and the lack of structured data. Here, we propose LatticeNet, a novel approach for 3D semantic segmentation, which takes raw point clouds as input. A PointNet describes the local geometry which we embed into a sparse permutohedral lattice. The lattice allows for fast convolutions while keeping a low memory footprint. Further, we introduce DeformSlice, a novel learned data-dependent interpolation for projecting lattice features back onto the point cloud. We present results of 3D segmentation on multiple datasets where our method achieves state-of-the-art performance. We also extend and evaluate our network for instance and dynamic object segmentation.",0
"In recent years, deep learning techniques have shown great promise in solving many challenges related to point cloud segmentation. Many state-of-the-art methods use either graph neural networks (GNNs) or convolutional neural networks (CNNs) for this task, but these approaches can suffer from high computational complexity or limited scalability due to their memory requirements. To address this issue, we propose LatticeNet, which utilizes permutohedral lattices as feature spaces to learn global features across point clouds while maintaining local geometry information during the training process. Our method outperforms existing state-of-the-art algorithms on several benchmark datasets while offering superior runtime performance on CPUs. In summary, our work demonstrates that using lattice representations can significantly improve point cloud segmentation accuracy without sacrificing speed. By leveraging permutohedral lattices, we provide a fast and efficient solution capable of handling large-scale datasets and enabling real-time applications. The proposed approach represents an important step toward realizing real-world applications for autonomous driving, robotics, and other fields where point cloud data is commonly used. This work has significant implications for advancing research in computer vision and offers opportunities for future exploration into lattice representations and deep learning techniques applied to spatial data structures. Overall, we believe that LatticeNet paves the way for more effective solutions in spatio-temporal point cloud segmentation, opening up new possibilities for emerging technologies relying on perceptual input like self-driving cars or industrial robots.",1
"Combinatorial optimization problems (COPs) on the graph with real-life applications are canonical challenges in Computer Science. The difficulty of finding quality labels for problem instances holds back leveraging supervised learning across combinatorial problems. Reinforcement learning (RL) algorithms have recently been adopted to solve this challenge automatically. The underlying principle of this approach is to deploy a graph neural network (GNN) for encoding both the local information of the nodes and the graph-structured data in order to capture the current state of the environment. Then, it is followed by the actor to learn the problem-specific heuristics on its own and make an informed decision at each state for finally reaching a good solution. Recent studies on this subject mainly focus on a family of combinatorial problems on the graph, such as the travel salesman problem, where the proposed model aims to find an ordering of vertices that optimizes a given objective function. We use the security-aware phone clone allocation in the cloud as a classical quadratic assignment problem (QAP) to investigate whether or not deep RL-based model is generally applicable to solve other classes of such hard problems. Extensive empirical evaluation shows that existing RL-based model may not generalize to QAP.",0
"This work examines how generalizable reinforcement learning frameworks can be applied in combinatorial optimization settings, particularly focusing on their difficulties and limitations. After discussing these challenges, we propose ways to overcome them by introducing new techniques based on algorithmic analysis and empirical studies. Our results suggest that although there may still exist problems wherein current RL methods struggle, careful consideration of the specific problem structure combined with novel algorithmic designs could lead to greater success in solving combinatorial problems.",1
"Deep learning's performance has been extensively recognized recently. Graph neural networks (GNNs) are designed to deal with graph-structural data that classical deep learning does not easily manage. Since most GNNs were created using distinct theories, direct comparisons are impossible. Prior research has primarily concentrated on categorizing existing models, with little attention paid to their intrinsic connections. The purpose of this study is to establish a unified framework that integrates GNNs based on spectral graph and approximation theory. The framework incorporates a strong integration between spatial- and spectral-based GNNs while tightly associating approaches that exist within each respective domain.",0
"This paper proposes a unified framework for graph neural networks (GNNs) that bridges the gap between spatial and spectral domains. Traditionally, GNNs have been limited by their reliance on either spatial or spectral representations alone, which can lead to suboptimal performance and limitations in model expressivity. Our proposed approach addresses these issues by combining both types of representations into a single, coherent framework. We showcase how our method achieves state-of-the-art results across several benchmark datasets, demonstrating its effectiveness at handling complex relationships within graphs. By introducing novel hybrid convolutional layers, we enable more efficient message passing among neighboring nodes, leading to improved accuracy. Additionally, through ablation studies and visualization techniques, we provide insights into the working mechanisms behind our unified framework. Ultimately, our work represents a step towards developing robust and generalizable GNN models capable of tackling a wide range of graph data challenges.",1
"Decision forests (Forests), in particular random forests and gradient boosting trees, have demonstrated state-of-the-art accuracy compared to other methods in many supervised learning scenarios. In particular, Forests dominate other methods in tabular data, that is, when the feature space is unstructured, so that the signal is invariant to a permutation of the feature indices. However, in structured data lying on a manifold (such as images, text, and speech) deep networks (Networks), specifically convolutional deep networks (ConvNets), tend to outperform Forests. We conjecture that at least part of the reason for this is that the input to Networks is not simply the feature magnitudes, but also their indices. In contrast, naive Forest implementations fail to explicitly consider feature indices. A recently proposed Forest approach demonstrates that Forests, for each node, implicitly sample a random matrix from some specific distribution. These Forests, like some classes of Networks, learn by partitioning the feature space into convex polytopes corresponding to linear functions. We build on that approach and show that one can choose distributions in a manifold-aware fashion to incorporate feature locality. We demonstrate the empirical performance on data whose features live on three different manifolds: a torus, images, and time-series. Moreover, we demonstrate its strength in multivariate simulated settings and also show superiority in predicting surgical outcome in epilepsy patients and predicting movement direction from raw stereotactic EEG data from non-motor brain regions. In all simulations and real data, Manifold Oblique Random Forest (MORF) algorithm outperforms approaches that ignore feature space structure and challenges the performance of ConvNets. Moreover, MORF runs fast and maintains interpretability and theoretical justification.",0
"Machine learning has made significant strides over the past few years, driven by advancements in algorithms, hardware, and data availability. Despite these developments, convolutional deep networks (CDNs) still remain dominant in many computer vision tasks due to their ability to capture hierarchical features from raw images. One approach that has been gaining attention recently is manifolds oblique random forest (MORF), which combines multiple random projections of input data into a low-dimensional space. This paper proposes using MORF as an alternative to CDNs for computer vision applications. We compare our proposed method to several state-of-the-art CDN architectures on four popular benchmark datasets and show competitive results while being significantly faster and less memory-intensive. Our experimental results suggest that MORF can effectively bridge the gap between simpler machine learning methods and CDNs, opening up new possibilities for efficient and accurate image recognition systems. Overall, we believe that MORF represents a promising direction for future research in computer vision and other high-dimensional data problems.",1
"Graph neural networks (GNN) have shown great success in learning from graph-structured data. They are widely used in various applications, such as recommendation, fraud detection, and search. In these domains, the graphs are typically large, containing hundreds of millions of nodes and several billions of edges. To tackle this challenge, we develop DistDGL, a system for training GNNs in a mini-batch fashion on a cluster of machines. DistDGL is based on the Deep Graph Library (DGL), a popular GNN development framework. DistDGL distributes the graph and its associated data (initial features and embeddings) across the machines and uses this distribution to derive a computational decomposition by following an owner-compute rule. DistDGL follows a synchronous training approach and allows ego-networks forming the mini-batches to include non-local nodes. To minimize the overheads associated with distributed computations, DistDGL uses a high-quality and light-weight min-cut graph partitioning algorithm along with multiple balancing constraints. This allows it to reduce communication overheads and statically balance the computations. It further reduces the communication by replicating halo nodes and by using sparse embedding updates. The combination of these design choices allows DistDGL to train high-quality models while achieving high parallel efficiency and memory scalability. We demonstrate our optimizations on both inductive and transductive GNN models. Our results show that DistDGL achieves linear speedup without compromising model accuracy and requires only 13 seconds to complete a training epoch for a graph with 100 million nodes and 3 billion edges on a cluster with 16 machines. DistDGL is now publicly available as part of DGL:https://github.com/dmlc/dgl/tree/master/python/dgl/distributed.",0
"As graphs have become increasingly prevalent in many domains, there has been growing interest in developing efficient graph neural networks (GNNs) that can handle billion-scale graphs. However, training GNNs on these large graphs presents significant challenges due to their high computational cost and memory requirements. In this work, we propose DistDGL, a distributed framework for training GNNs on massive graphs. DistDGL leverages recent advances in parallel computing and communication techniques to efficiently distribute the computation across multiple machines while minimizing data transfers. We demonstrate the effectiveness of our approach through extensive experiments on real-world datasets, showing that DistDGL significantly outperforms existing state-of-the-art methods in terms of both accuracy and scalability. Our framework paves the way for solving larger and more complex graph problems at scale.",1
"Data selection methods, such as active learning and core-set selection, are useful tools for improving the data efficiency of deep learning models on large-scale datasets. However, recent deep learning models have moved forward from independent and identically distributed data to graph-structured data, such as social networks, e-commerce user-item graphs, and knowledge graphs. This evolution has led to the emergence of Graph Neural Networks (GNNs) that go beyond the models existing data selection methods are designed for. Therefore, we present Grain, an efficient framework that opens up a new perspective through connecting data selection in GNNs with social influence maximization. By exploiting the common patterns of GNNs, Grain introduces a novel feature propagation concept, a diversified influence maximization objective with novel influence and diversity functions, and a greedy algorithm with an approximation guarantee into a unified framework. Empirical studies on public datasets demonstrate that Grain significantly improves both the performance and efficiency of data selection (including active learning and core-set selection) for GNNs. To the best of our knowledge, this is the first attempt to bridge two largely parallel threads of research, data selection, and social influence maximization, in the setting of GNNs, paving new ways for improving data efficiency.",0
"Imagine that you have just finished writing a research paper about improving data efficiency in graph neural networks using a technique called diversified influence maximization (DIM). You want to write an informative abstract that summarizes your work, but you don't know where to start. Here's a template to help guide you through the process:  ---  * **Introduction** 	+ Begin by introducing the main problem you aimed to solve. For example, ""Graph neural networks (GNN) are powerful models for learning representations on graphs, such as social networks, protein structures, and knowledge graphs. However, training these models can require large amounts of data, which may not always be available.""  * **Problem Statement** 	+ Next, clearly state the specific problem you sought to address. For instance, ""In many cases, there are only limited amounts of labeled data available, leading to suboptimal model performance.""  * **Approach** 	+ Describe how you approached solving the problem. Include any key techniques or methods used in your solution. For instance, ""To overcome this challenge, we propose a novel approach based on diversified influence maximization (DIM), which selects a small subset of nodes from the original graph, each of which is able to reach all other nodes with high probability.""  * **Results** 	+ Summarize your findings and highlight the benefits of your approach over existing solutions. For example, ""Experimental results show that our proposed method achieves significant improvements in terms of both accuracy and efficiency compared to baseline approaches.""  * **Conclusion** 	+ End by reiterating the importance of your work and potential future directions for further research. For instan",1
"The geometric structure of an optimization landscape is argued to be fundamentally important to support the success of deep neural network learning. A direct computation of the landscape beyond two layers is hard. Therefore, to capture the global view of the landscape, an interpretable model of the network-parameter (or weight) space must be established. However, the model is lacking so far. Furthermore, it remains unknown what the landscape looks like for deep networks of binary synapses, which plays a key role in robust and energy efficient neuromorphic computation. Here, we propose a statistical mechanics framework by directly building a least structured model of the high-dimensional weight space, considering realistic structured data, stochastic gradient descent training, and the computational depth of neural networks. We also consider whether the number of network parameters outnumbers the number of supplied training data, namely, over- or under-parametrization. Our least structured model reveals that the weight spaces of the under-parametrization and over-parameterization cases belong to the same class, in the sense that these weight spaces are well-connected without any hierarchical clustering structure. In contrast, the shallow-network has a broken weight space, characterized by a discontinuous phase transition, thereby clarifying the benefit of depth in deep learning from the angle of high dimensional geometry. Our effective model also reveals that inside a deep network, there exists a liquid-like central part of the architecture in the sense that the weights in this part behave as randomly as possible, providing algorithmic implications. Our data-driven model thus provides a statistical mechanics insight about why deep learning is unreasonably effective in terms of the high-dimensional weight space, and how deep networks are different from shallow ones.",0
"Developing robust models that can effectively handle complex data sets is a challenging task faced by researchers working on artificial intelligence (AI). In recent years, many studies have shown promising results using data-driven approaches such as deep learning (DL) algorithms. Despite these advancements, there are still limitations regarding their reliability, scalability, and interpretability. Addressing these concerns requires innovative techniques capable of adapting to diverse data types while maintaining high performance levels. This study presents a novel DL approach called ""liquid deep learning"" (LL), which combines key features from both traditional machine learning methods and modern DL architectures. Our experiments demonstrate that LL offers significant improvements over state-of-the-art DL models in terms of accuracy, speed, and efficiency. Additionally, our proposed methodology ensures stability under varying conditions, making it suitable for real-world applications requiring reliable predictions across multiple domains. By addressing important considerations like generalization ability and robustness, we aim to bring forward a step change in AI research, paving the way for more accurate, dependable DL systems, essential for future progress in many fields.",1
"Despite the remarkable success of deep learning, optimal convolution operation on point cloud remains indefinite due to its irregular data structure. In this paper, we present Cubic Kernel Convolution (CKConv) that learns to voxelize the features of local points by exploiting both continuous and discrete convolutions. Our continuous convolution uniquely employs a 3D cubic form of kernel weight representation that splits a feature into voxels in embedding space. By consecutively applying discrete 3D convolutions on the voxelized features in a spatial manner, preceding continuous convolution is forced to learn spatial feature mapping, i.e., feature voxelization. In this way, geometric information can be detailed by encoding with subdivided features, and our 3D convolutions on these fixed structured data do not suffer from discretization artifacts thanks to voxelization in embedding space. Furthermore, we propose a spatial attention module, Local Set Attention (LSA), to provide comprehensive structure awareness within the local point set and hence produce representative features. By learning feature voxelization with LSA, CKConv can extract enriched features for effective point cloud analysis. We show that CKConv has great applicability to point cloud processing tasks including object classification, object part segmentation, and scene semantic segmentation with state-of-the-art results.",0
"CKConv: Learning Feature Voxelization for Point Cloud Analysis presents a novel approach for converting high-resolution point cloud data into volumetric representations that can be used by computer vision algorithms. This method leverages convolutional neural networks (CNNs) to learn the optimal voxel size for each feature channel. By doing so, CKConv can generate highly detailed volumetric outputs that capture fine-grained details while minimizing noise and preserving overall shape structure. Experiments on a diverse set of benchmark datasets show that our method outperforms existing state-of-the-art approaches across multiple evaluation metrics, including accuracy and robustness against input resolution. Overall, we believe that CKConv has significant potential for advancing research in computer vision and robotics applications where high-quality point cloud analysis is crucial. -----",1
"Graph Neural Networks (GNNs) have exploded onto the machine learning scene in recent years owing to their capability to model and learn from graph-structured data. Such an ability has strong implications in a wide variety of fields whose data is inherently relational, for which conventional neural networks do not perform well. Indeed, as recent reviews can attest, research in the area of GNNs has grown rapidly and has lead to the development of a variety of GNN algorithm variants as well as to the exploration of groundbreaking applications in chemistry, neurology, electronics, or communication networks, among others. At the current stage of research, however, the efficient processing of GNNs is still an open challenge for several reasons. Besides of their novelty, GNNs are hard to compute due to their dependence on the input graph, their combination of dense and very sparse operations, or the need to scale to huge graphs in some applications. In this context, this paper aims to make two main contributions. On the one hand, a review of the field of GNNs is presented from the perspective of computing. This includes a brief tutorial on the GNN fundamentals, an overview of the evolution of the field in the last decade, and a summary of operations carried out in the multiple phases of different GNN algorithm variants. On the other hand, an in-depth analysis of current software and hardware acceleration schemes is provided, from which a hardware-software, graph-aware, and communication-centric vision for GNN accelerators is distilled.",0
"Abstract:  The rapid increase in the size of graph data sets has fueled interest in developing fast and efficient methods for computing graph neural networks (GNN). These models can effectively capture complex relationships among nodes in graphs but their training demands significant computational resources due to iterative nature of GNN computation. In addition to the high resource consumption, there have been several advancements in both hardware accelerators such as GPUs and TPUs specifically designed to perform tensor operations that power machine learning applications including those based on GNNs. In recent years, numerous efforts have focused on proposing novel algorithms for scaling up GNN inference, and further optimizing existing architectures by leveraging these hardware accelerators. This survey provides a comprehensive overview of recent progress made in designing algorithms and mapping them onto various acceleration platforms. We first present an introduction to the basics of deep learning on graphs, followed by a detailed description of popular GNN model architectures. Next we elaborate on two important components of GNN computations - feature propagation and aggregation, along with the associated scalability challenges. Finally, we discuss the most promising directions towards addressing these challenges via specialized hardware and algorithmic techniques. We hope that our review will serve as a reference guide for researchers interested in exploring opportunities within this exciting interdisciplinary field.",1
"Graph neural networks (GNNs) have achieved remarkable success as a framework for deep learning on graph-structured data. However, GNNs are fundamentally limited by their tree-structured inductive bias: the WL-subtree kernel formulation bounds the representational capacity of GNNs, and polynomial-time GNNs are provably incapable of recognizing triangles in a graph. In this work, we propose to augment the GNN message-passing operations with information defined on ego graphs (i.e., the induced subgraph surrounding each node). We term these approaches Ego-GNNs and show that Ego-GNNs are provably more powerful than standard message-passing GNNs. In particular, we show that Ego-GNNs are capable of recognizing closed triangles, which is essential given the prominence of transitivity in real-world graphs. We also motivate our approach from the perspective of graph signal processing as a form of multiplex graph convolution. Experimental results on node classification using synthetic and real data highlight the achievable performance gains using this approach.",0
"Graph Neural Network (GNN) has achieved tremendous success in handling complex graph structured data. However, these models ignore certain types of dependencies existing within graphs such as ego structures, which can significantly affect their performance on downstream tasks. To address this problem, we propose Ego-GNNs - a novel approach that explicitly considers ego structures in GNN architecture by extending each node’s neighborhood to include itself. Our proposed method introduces an additional degree of freedom in learning representations for nodes, enabling the model to capture essential first-order proximity relationships within ego structures. We evaluate our approach across diverse benchmark datasets comprising social networks, knowledge graphs, and bioinformatics domains. Results demonstrate significant improvements over state-of-the-art baselines, demonstrating the effectiveness of incorporating ego structures into Graph Neural Network architectures. Additionally, we present qualitative analyses and ablation studies to further substantiate the benefits offered by our methodology. This study advances our understanding of capturing local and global contextual information in Graph Neural Networks, paving the way for improved applications ranging from recommender systems, fraud detection, drug discovery, etc.",1
"Graph representation learning plays a vital role in processing graph-structured data. However, prior arts on graph representation learning heavily rely on labeling information. To overcome this problem, inspired by the recent success of graph contrastive learning and Siamese networks in visual representation learning, we propose a novel self-supervised approach in this paper to learn node representations by enhancing Siamese self-distillation with multi-scale contrastive learning. Specifically, we first generate two augmented views from the input graph based on local and global perspectives. Then, we employ two objectives called cross-view and cross-network contrastiveness to maximize the agreement between node representations across different views and networks. To demonstrate the effectiveness of our approach, we perform empirical experiments on five real-world datasets. Our method not only achieves new state-of-the-art results but also surpasses some semi-supervised counterparts by large margins. Code is made available at https://github.com/GRAND-Lab/MERIT",0
"Recent advancements in graph neural networks (GNN) have led to significant improvements in graph data representation learning, particularly in unsupervised domain. However, most existing methods perform well only on homogeneous graphs where each node has the same type or class label and limited attention has been given towards heterogeneous graphs which contain nodes belonging from multiple classes. In this work we introduce a new contrastive learning approach that enables multi-scale neighborhood reasoning and improves self-supervised learning performance. Our proposed method is a Siamese network architecture designed to capture high level semantic representations at different scales, enabling it to generalize better across domains. We evaluated our model on four benchmark datasets consisting of both homogenous as well as heterogeneous graphs against state-of-the art competitors, showing marked improvement in accuracy while maintaining low computational costs. Overall, our study provides evidence for the effectiveness of utilizing multi-scale contrastive learning for self-supervised graph representation learning across diverse graph domains.  This paper presents the use of multi-scale contrastive learning in self-supervised graph representation learning. Recently there has been progress in applying graph neural networks for learning graph structures but current approaches often struggle with graph classification tasks. This research proposes the use of a Siamese network which models the high-level relationship between two graphs and leverages this to build up graph embeddings. With the goal to improve upon current self-supervised learning techniques for graph representation, they demonstrate the effectiveness of their framework using four real world graph datasets. Their contributions outperform previous state-of-the-art results indicating the merit of the proposed methodology. By enhancing graph representation learning in a self-supevised manner, future applications could leverage these more accurate learned graph embeddings for improved downstream prediction tasks.",1
"Recent achievements in end-to-end deep learning have encouraged the exploration of tasks dealing with highly structured data with unified deep network models. Having such models for compressing audio signals has been challenging since it requires discrete representations that are not easy to train with end-to-end backpropagation. In this paper, we present an end-to-end deep learning approach that combines recurrent neural networks (RNNs) within the training strategy of variational autoencoders (VAEs) with a binary representation of the latent space. We apply a reparametrization trick for the Bernoulli distribution for the discrete representations, which allows smooth backpropagation. In addition, our approach allows the separation of the encoder and decoder, which is necessary for compression tasks. To our best knowledge, this is the first end-to-end learning for a single audio compression model with RNNs, and our model achieves a Signal to Distortion Ratio (SDR) of 20.54.",0
"This paper proposes using deep neural networks (DNN) for end-to-end learning to achieve high quality audio compression at low bitrates. We evaluate our approach on popular music datasets such as MIDI and WavNet and show that DNNs can outperform traditional methods like linear prediction coding and discrete cosine transform coding in terms of both objective metrics like signal fidelity and subjective human listening tests. Our results demonstrate the feasibility of using end-to-end learned models for low bitrate audio compression while maintaining high audio quality and preserving important musical characteristics. Additionally, we discuss potential future directions for improving these systems even further. Finally, we provide extensive experimental results including comparisons against state-of-the-art audio codecs like Opus and FLAC. Ultimately, our work shows great promise towards achieving highly efficient, high quality audio encoding via deep learning approaches.",1
"Graph convolutional networks are becoming indispensable for deep learning from graph-structured data. Most of the existing graph convolutional networks share two big shortcomings. First, they are essentially low-pass filters, thus the potentially useful middle and high frequency band of graph signals are ignored. Second, the bandwidth of existing graph convolutional filters is fixed. Parameters of a graph convolutional filter only transform the graph inputs without changing the curvature of a graph convolutional filter function. In reality, we are uncertain about whether we should retain or cut off the frequency at a certain point unless we have expert domain knowledge. In this paper, we propose Automatic Graph Convolutional Networks (AutoGCN) to capture the full spectrum of graph signals and automatically update the bandwidth of graph convolutional filters. While it is based on graph spectral theory, our AutoGCN is also localized in space and has a spatial form. Experimental results show that AutoGCN achieves significant improvement over baseline methods which only work as low-pass filters.",0
"New models have been developed that use graph convolution filters automatically learned from data, which can provide performance better than traditional manual design methods. We propose herein a novel approach to image processing on graphs using these automatic filters learned by a GCN architecture; we call it the 'Graph Convolutional Neural Operator'. By taking advantage of automated filtering techniques through our methodology, significant performance improvements are realized over existing low pass filter approaches, with comparisons made to current state-of-the art image processing architectures like ResNet and DenseUNet. With real world image datasets considered, results show that our proposed model outperforms competing models. As such, our work sets a new standard for future image processing research within the domain of graph signal analysis.",1
"Multi-relational graph is a ubiquitous and important data structure, allowing flexible representation of multiple types of interactions and relations between entities. Similar to other graph-structured data, link prediction is one of the most important tasks on multi-relational graphs and is often used for knowledge completion. When related graphs coexist, it is of great benefit to build a larger graph via integrating the smaller ones. The integration requires predicting hidden relational connections between entities belonged to different graphs (inter-domain link prediction). However, this poses a real challenge to existing methods that are exclusively designed for link prediction between entities of the same graph only (intra-domain link prediction). In this study, we propose a new approach to tackle the inter-domain link prediction problem by softly aligning the entity distributions between different domains with optimal transport and maximum mean discrepancy regularizers. Experiments on real-world datasets show that optimal transport regularizer is beneficial and considerably improves the performance of baseline methods.",0
"This paper focuses on inter-domain multi-relational link prediction (IMLP), which involves predicting links that exist across multiple domains and relationships within these domains. IMLP has important applications in fields such as recommendation systems, social network analysis, and natural language processing. However, existing approaches have limited performance because they rely solely on intra-domain relationships, ignore important contextual factors, or require large amounts of labeled data. To address these limitations, we propose a novel approach based on embedding learning and deep neural networks. Our method can effectively capture complex relational dependencies among different entities by utilizing inter-domain knowledge transfer through pre-training. Extensive experiments on real datasets demonstrate the effectiveness and efficiency of our approach compared to state-of-the-art methods. We discuss potential future directions and the importance of IMLP research. Overall, this work represents a significant step forward towards building more intelligent and effective information systems.",1
"Graph representation learning has attracted a surge of interest recently, whose target at learning discriminant embedding for each node in the graph. Most of these representation methods focus on supervised learning and heavily depend on label information. However, annotating graphs are expensive to obtain in the real world, especially in specialized domains (i.e. biology), as it needs the annotator to have the domain knowledge to label the graph. To approach this problem, self-supervised learning provides a feasible solution for graph representation learning. In this paper, we propose a Multi-Level Graph Contrastive Learning (MLGCL) framework for learning robust representation of graph data by contrasting space views of graphs. Specifically, we introduce a novel contrastive view - topological and feature space views. The original graph is first-order approximation structure and contains uncertainty or error, while the $k$NN graph generated by encoding features preserves high-order proximity. Thus $k$NN graph generated by encoding features not only provide a complementary view, but is more suitable to GNN encoder to extract discriminant representation. Furthermore, we develop a multi-level contrastive mode to preserve the local similarity and semantic similarity of graph-structured data simultaneously. Extensive experiments indicate MLGCL achieves promising results compared with the existing state-of-the-art graph representation learning methods on seven datasets.",0
"This study introduces a novel method for multi-level graph contrastive learning, which involves representing graphs as matrices and using these representations to learn from differences across levels within each graph. Our approach uses a combination of matrix factorization techniques and random walks on graphs to extract features that capture both local and global structure. We then train a deep neural network on pairs of positive examples (i.e., corresponding nodes at different levels) and negative examples (i.e., non-corresponding nodes), leveraging large amounts of unlabeled data to enforce consistency across multiple levels of abstraction. Experiments on several benchmark datasets demonstrate the effectiveness of our method compared to previous approaches for graph representation learning, including both linear and nonlinear models. Additionally, we showcase the utility of our learned representations by applying them to downstream node classification tasks and achieving strong results. Overall, our work advances the state of art in graph neural networks and underscores the importance of developing novel methods for multi-level contrastive learning.",1
"Various non-trivial spaces are becoming popular for embedding structured data such as graphs, texts, or images. Following spherical and hyperbolic spaces, more general product spaces have been proposed. However, searching for the best configuration of product space is a resource-intensive procedure, which reduces the practical applicability of the idea. We generalize the concept of product space and introduce an overlapping space that does not have the configuration search problem. The main idea is to allow subsets of coordinates to be shared between spaces of different types (Euclidean, hyperbolic, spherical). As a result, parameter optimization automatically learns the optimal configuration. Additionally, overlapping spaces allow for more compact representations since their geometry is more complex. Our experiments confirm that overlapping spaces outperform the competitors in graph embedding tasks. Here, we consider both distortion setup, where the aim is to preserve distances, and ranking setup, where the relative order should be preserved. The proposed method effectively solves the problem and outperforms the competitors in both settings. We also perform an empirical analysis in a realistic information retrieval task, where we compare all spaces by incorporating them into DSSM. In this case, the proposed overlapping space consistently achieves nearly optimal results without any configuration tuning. This allows for reducing training time, which can be significant in large-scale applications.",0
"This paper presents a novel approach to representing graphs compactly while still preserving important structural properties such as connectivity and clustering coefficient. Our method leverages overlapping spaces, which allows for efficient storage and retrieval of graph data. We demonstrate the effectiveness of our approach through experiments on real world datasets, showing that we can achieve significant improvements in both space utilization and query time compared to traditional methods. Furthermore, our method has applications in many areas including database systems, machine learning, and network analysis. By providing a more concise representation of complex graphs, we enable faster processing times and improved scalability for large scale data analysis tasks.",1
"Relational databases are the de facto standard for storing and querying structured data, and extracting insights from structured data requires advanced analytics. Deep neural networks (DNNs) have achieved super-human prediction performance in particular data types, e.g., images. However, existing DNNs may not produce meaningful results when applied to structured data. The reason is that there are correlations and dependencies across combinations of attribute values in a table, and these do not follow simple additive patterns that can be easily mimicked by a DNN. The number of possible such cross features is combinatorial, making them computationally prohibitive to model. Furthermore, the deployment of learning models in real-world applications has also highlighted the need for interpretability, especially for high-stakes applications, which remains another issue of concern to DNNs.   In this paper, we present ARM-Net, an adaptive relation modeling network tailored for structured data, and a lightweight framework ARMOR based on ARM-Net for relational data analytics. The key idea is to model feature interactions with cross features selectively and dynamically, by first transforming the input features into exponential space, and then determining the interaction order and interaction weights adaptively for each cross feature. We propose a novel sparse attention mechanism to dynamically generate the interaction weights given the input tuple, so that we can explicitly model cross features of arbitrary orders with noisy features filtered selectively. Then during model inference, ARM-Net can specify the cross features being used for each prediction for higher accuracy and better interpretability. Our extensive experiments on real-world datasets demonstrate that ARM-Net consistently outperforms existing models and provides more interpretable predictions for data-driven decision making.",0
"In recent years, deep learning has demonstrated promising results for data modeling tasks. However, most existing models were trained without considering relationships among structured elements (e.g., human relations from knowledge graphs) which can provide valuable insights into downstream applications. To address these challenges, we introduce a novel neural network architecture called ARM-Net that incorporates adaptive relation modeling across layers.  Our model first learns to encode rich features using a preliminary layer then processes them through multiple adaptation layers that refine representations based on relational signals. By integrating these two parts, our method balances the need for efficient feature extraction and targeted relationship mining. Experiments conducted on diverse benchmark datasets show that our approach outperforms state-of-the-art methods by significant margins while offering better interpretability due to its modular design. We hope that our work encourages future research exploring more complex interactions within structured data.",1
We propose a model for hierarchical structured data as an extension to the stochastic temporal convolutional network. The proposed model combines an autoregressive model with a hierarchical variational autoencoder and downsampling to achieve superior computational complexity. We evaluate the proposed model on two different types of sequential data: speech and handwritten text. The results are promising with the proposed model achieving state-of-the-art performance.,0
"Deep learning has revolutionized numerous fields by providing powerful tools that can automatically learn complex patterns from raw data. In particular, deep neural networks have proven highly successful at modeling sequential dependencies in time-series data. However, most datasets consist not just of individual points but rather high-dimensional hierarchies where each level exhibits some form of temporal structure. For example, climate records come as grids in both space and time. Hierarchical structures like these pose unique challenges for machine learning due to their intricate relationships. We present methods for training deep autoregressive (AR) models on hierarchically structured data through two key innovations: weight sharing and dilated convolutions. These modifications allow our network to capture both spatial and temporal dynamics while leveraging abundant supervision found in coarser levels during fine-scale prediction. Our experiments demonstrate significant improvement over strong baselines across multiple domains, including large-scale atmospheric science simulations and geospatial satellite imagery analysis tasks. By unlocking the ability to effectively model sequential and spatial hierarchy jointly via deep AR models, we open doors towards tackling many other real-world problems requiring similar representation power.",1
"Graph neural networks have recently achieved remarkable success in representing graph-structured data, with rapid progress in both the node embedding and graph pooling methods. Yet, they mostly focus on capturing information from the nodes considering their connectivity, and not much work has been done in representing the edges, which are essential components of a graph. However, for tasks such as graph reconstruction and generation, as well as graph classification tasks for which the edges are important for discrimination, accurately representing edges of a given graph is crucial to the success of the graph representation learning. To this end, we propose a novel edge representation learning framework based on Dual Hypergraph Transformation (DHT), which transforms the edges of a graph into the nodes of a hypergraph. This dual hypergraph construction allows us to apply message passing techniques for node representations to edges. After obtaining edge representations from the hypergraphs, we then cluster or drop edges to obtain holistic graph-level edge representations. We validate our edge representation learning method with hypergraphs on diverse graph datasets for graph representation and generation performance, on which our method largely outperforms existing graph representation learning methods. Moreover, our edge representation learning and pooling method also largely outperforms state-of-the-art graph pooling methods on graph classification, not only because of its accurate edge representation learning, but also due to its lossless compression of the nodes and removal of irrelevant edges for effective message passing.",0
"In recent years, representation learning has emerged as a powerful tool for solving complex problems in various fields such as computer vision, natural language processing, and machine learning. One important aspect of representation learning is how to effectively capture structural relationships among data points, which can greatly enhance the performance of downstream models. Hypergraphs offer a more expressive framework than graphs to represent these structures due to their capability of handling multiple edge types and varying degrees of connectedness. This paper presents an approach that leverages hypergraph representations to improve the quality of learned embeddings and significantly boost the performance on several benchmark datasets. Our method adopts random walks over hypergraphs to efficiently encode local neighborhood contexts while utilizing meta paths to integrate global structure information into the process. We evaluate our model against state-of-the-art methods on several tasks, including node classification, link prediction, clustering, and question answering. Experimental results demonstrate the effectiveness of our approach in producing high-quality embeddings, achieving superior performance across all evaluated benchmarks, and offering improved interpretability through hypergraph visualization techniques. Overall, our work provides insights into the benefits of using hypergraph representation learning for improving downstream task outcomes, as well as advancing knowledge within the broader field of data mining and artificial intelligence.",1
"In the world where big data reigns and there is plenty of hardware prepared to gather a huge amount of non structured data, data acquisition is no longer a problem. Surveillance cameras are ubiquitous and they capture huge numbers of people walking across different scenes. However, extracting value from this data is challenging, specially for tasks that involve human images, such as face recognition and person re-identification. Annotation of this kind of data is a challenging and expensive task. In this work we propose a domain adaptation workflow to allow CNNs that were trained in one domain to be applied to another domain without the need for new annotation of the target data. Our method uses AlignedReID++ as the baseline, trained using a Triplet loss with batch hard. Domain adaptation is done by using pseudo-labels generated using an unsupervised learning strategy. Our results show that domain adaptation techniques really improve the performance of the CNN when applied in the target domain.",0
"Title: ""Domain Adaptation for Person Re-Identification Using AlignedReID++""  Abstract: One of the key challenges facing person re-identification (Re-Id) algorithms is domain shift, which refers to differences in camera parameters, scene layouts, and lighting conditions between source and target domains. To address this challenge, we propose a novel approach that utilizes AlignedReID++, a pre-trained deep learning model designed specifically for Re-Id tasks. Our method leverages the power of transfer learning and fine-tuning to adapt existing models to new, unlabelled data from different domains. By aligning feature representations across multiple datasets through knowledge distillation, our algorithm can effectively learn more robust features while preserving identities. Experimental results demonstrate that our proposed method outperforms state-of-the-art methods on four benchmark datasets, showing significant improvements in rank-1 accuracy and mAP metrics. Overall, our work represents an important step forward in the field of person Re-Id, helping overcome domain shift issues in practice.",1
"In representation learning on the graph-structured data, under heterophily (or low homophily), many popular GNNs may fail to capture long-range dependencies, which leads to their performance degradation. To solve the above-mentioned issue, we propose a graph convolutional networks with structure learning (GCN-SL), and furthermore, the proposed approach can be applied to node classification. The proposed GCN-SL contains two improvements: corresponding to node features and edges, respectively. In the aspect of node features, we propose an efficient-spectral-clustering (ESC) and an ESC with anchors (ESC-ANCH) algorithms to efficiently aggregate feature representations from all similar nodes. In the aspect of edges, we build a re-connected adjacency matrix by using a special data preprocessing technique and similarity learning, and the re-connected adjacency matrix can be optimized directly along with GCN-SL parameters. Considering that the original adjacency matrix may provide misleading information for aggregation in GCN, especially the graphs being with a low level of homophily. The proposed GCN-SL can aggregate feature representations from nearby nodes via re-connected adjacency matrix and is applied to graphs with various levels of homophily. Experimental results on a wide range of benchmark datasets illustrate that the proposed GCN-SL outperforms the stateof-the-art GNN counterparts.",0
"Title: ""Graph Neural Networks with Structure Learning""  Abstract: This paper presents a new approach to graph convolutional networks (GCN) called GCN-SL that incorporates structure learning into the network architecture. In many real-world applications, graphs exhibit heterophily, meaning that different nodes have varying degrees of connectivity and can belong to multiple communities simultaneously. Existing GCN methods assume homophilous graphs where each node belongs to only one community, which limits their ability to capture complex relationships in heterophilous graphs.  To address this limitation, we propose a novel methodology to learn the optimal weights for both intra-layer and inter-layer connections in the GCN model by leveraging structural priors, such as degree distribution and community membership probabilities. We demonstrate that our proposed method improves performance over state-of-the-art baseline models on several benchmark datasets including Cora, Citeseer, and PubMed. Our experimental results show that the learned weights adapt well to various types of data distributions and are able to extract more accurate representations of graph structures even with limited training samples. Overall, our work represents a significant advance towards building more robust GCN architectures that can handle challenging heterophilous graphs commonly found in the wild.",1
"Self-supervised learning on graph-structured data has drawn recent interest for learning generalizable, transferable and robust representations from unlabeled graphs. Among many, graph contrastive learning (GraphCL) has emerged with promising representation learning performance. Unfortunately, unlike its counterpart on image data, the effectiveness of GraphCL hinges on ad-hoc data augmentations, which have to be manually picked per dataset, by either rules of thumb or trial-and-errors, owing to the diverse nature of graph data. That significantly limits the more general applicability of GraphCL. Aiming to fill in this crucial gap, this paper proposes a unified bi-level optimization framework to automatically, adaptively and dynamically select data augmentations when performing GraphCL on specific graph data. The general framework, dubbed JOint Augmentation Optimization (JOAO), is instantiated as min-max optimization. The selections of augmentations made by JOAO are shown to be in general aligned with previous ""best practices"" observed from handcrafted tuning: yet now being automated, more flexible and versatile. Moreover, we propose a new augmentation-aware projection head mechanism, which will route output features through different projection heads corresponding to different augmentations chosen at each training step. Extensive experiments demonstrate that JOAO performs on par with or sometimes better than the state-of-the-art competitors including GraphCL, on multiple graph datasets of various scales and types, yet without resorting to any laborious dataset-specific tuning on augmentation selection. We release the code at https://github.com/Shen-Lab/GraphCL_Automated.",0
"""Graph Contrastive Learning (GCL) has emerged as a powerful technique for unsupervised learning on graph data. In recent years, there have been several advancements made in GCL, aimed at improving its performance and expanding its application domains. This review paper provides an overview of these developments, including various forms of contrastive losses used in GCL frameworks and their applications across different types of graphs. We discuss how GCL compares with other state-of-the-art methods for representation learning on graphs, highlighting its advantages and potential drawbacks. Finally, we provide insights into future research directions that can advance our understanding of GCL further.""",1
"Data augmentation is a key element of deep learning pipelines, as it informs the network during training about transformations of the input data that keep the label unchanged. Manually finding adequate augmentation methods and parameters for a given pipeline is however rapidly cumbersome. In particular, while intuition can guide this decision for images, the design and choice of augmentation policies remains unclear for more complex types of data, such as neuroscience signals. Moreover, label independent strategies might not be suitable for such structured data and class-dependent augmentations might be necessary. This idea has been surprisingly unexplored in the literature, while it is quite intuitive: changing the color of a car image does not change the object class to be predicted, but doing the same to the picture of an orange does. This paper aims to increase the generalization power added through class-wise data augmentation. Yet, as seeking transformations depending on the class largely increases the complexity of the task, using gradient-free optimization techniques as done by most existing automatic approaches becomes intractable for real-world datasets. For this reason we propose to use differentiable data augmentation amenable to gradient-based learning. EEG signals are a perfect example of data for which good augmentation policies are mostly unknown. In this work, we demonstrate the relevance of our approach on the clinically relevant sleep staging classification task, for which we also propose differentiable transformations.",0
"Data augmentation has emerged as a popular technique for improving machine learning models by increasing their training set size. However, traditional data augmentation techniques such as rotation, flipping, and scaling may not be suitable for electroencephalogram (EEG) signals due to specific characteristics such as sensor placement and signal frequency content. This study introduces a novel class-wise automatic differentiable data augmentation (CADDA) method specifically designed for EEG signals. CADDA uses a twofold approach that includes time-domain and frequency-domain transformations. Time-domain transformations involve shuffling and permutating the order of sample points within each epoch, while frequency-domain transformations include adjusting amplitude values along different frequency bands using trigonometric functions. Both types of transformation operations were made compatible with gradient descent via auto-differentiation tools available in deep learning frameworks. Experiments showed improved generalization performance across several classification tasks compared to standard augmentations and no augmentation. CADDA also exhibited superior results over other state-of-the art methods. Therefore, our proposed approach represents a significant contribution to the field of EEG signal processing and machine learning. In conclusion, CADDA provides a new tool for researchers and practitioners working on EEG applications to enhance model accuracy without requiring large amounts of labeled data.",1
"Temporal graph signals are multivariate time series with individual components associated with nodes of a fixed graph structure. Data of this kind arises in many domains including activity of social network users, sensor network readings over time, and time course gene expression within the interaction network of a model organism. Traditional matrix decomposition methods applied to such data fall short of exploiting structural regularities encoded in the underlying graph and also in the temporal patterns of the signal. How can we take into account such structure to obtain a succinct and interpretable representation of temporal graph signals?   We propose a general, dictionary-based framework for temporal graph signal decomposition (TGSD). The key idea is to learn a low-rank, joint encoding of the data via a combination of graph and time dictionaries. We propose a highly scalable decomposition algorithm for both complete and incomplete data, and demonstrate its advantage for matrix decomposition, imputation of missing values, temporal interpolation, clustering, period estimation, and rank estimation in synthetic and real-world data ranging from traffic patterns to social media activity. Our framework achieves 28% reduction in RMSE compared to baselines for temporal interpolation when as many as 75% of the observations are missing. It scales best among baselines taking under 20 seconds on 3.5 million data points and produces the most parsimonious models. To the best of our knowledge, TGSD is the first framework to jointly model graph signals by temporal and graph dictionaries.",0
"Here is an example abstract:  A key challenge in analyzing graph signals is their intrinsic high dimensionality. In practice these signals often admit a natural decomposition into simpler components that can then be interpreted by domain experts. This article introduces a novel method called temporal graph signal decomposition (TGSD) which extends traditional graph signal processing techniques to accommodate time varying signals. TGSD builds on spectral clustering methods and is motivated by recent results from algebraic signal processing related to sparse representations of multi-dimensional signals using graphs. We describe the TGSD framework and demonstrate through numerical examples how it can reveal structure in a variety of synthetic and real data sets that cannot be easily seen otherwise. These applications span social networks, biological networks, and sensor network data where we show that our approach leads to meaningful insights and interpretations of the underlying system dynamics.",1
"We introduce the Graph Mixture Density Networks, a new family of machine learning models that can fit multimodal output distributions conditioned on graphs of arbitrary topology. By combining ideas from mixture models and graph representation learning, we address a broader class of challenging conditional density estimation problems that rely on structured data. In this respect, we evaluate our method on a new benchmark application that leverages random graphs for stochastic epidemic simulations. We show a significant improvement in the likelihood of epidemic outcomes when taking into account both multimodality and structure. The empirical analysis is complemented by two real-world regression tasks showing the effectiveness of our approach in modeling the output prediction uncertainty. Graph Mixture Density Networks open appealing research opportunities in the study of structure-dependent phenomena that exhibit non-trivial conditional output distributions.",0
"This can make it easier to create a valid section header. You may want to add a heading like ""Abstact"" above your abstract. Please note that you should never insert plain text directly into latex code as shown below! Instead, use $...$ syntax for math mode and \emph{italic} and \textbf{bold} markup for emphasis: \begin{abstract} GMDN: Graph Mixture Density Networks  We present Graph Mixture Density Networks (GMDN), a novel deep learning framework designed to tackle high-dimensional density estimation tasks such as predicting pixel intensities, audio signals, or natural language sequences. Our model achieves state-of-the art performance on several benchmark datasets by leveraging powerful neural network architectures inspired by probabilistic graphical models, specifically mixtures of linear regressions and probabilistic principle component analysis (PCA) networks, which have been pre-trained separately using variational inference techniques. We demonstrate the general applicability of our approach across multiple domains, including computer vision, speech synthesis, and generative modelling. In addition, we provide qualitative results showing the ability of GMDN to interpolate plausible intermediate points along feature manifolds and produce coherent predictions despite missing data, hallmarks of good generative modelling capabilities. \end{abstract} You could add additional sections depending on your requirements, but at minimum should have introduction (introducing problem and context), related work (discuss existing solutions), method (description of proposed solution), experiments (results for evaluation and comparison against previous approaches/benchmarks), conclusion(summary of findings). Additionally, you might consider adding other common sections like motivation, background, discussion etc if relevant. For most papers these would go before introduction.  The specific content and ordering is up to y",1
"Hypergraphs are used to model higher-order interactions amongst agents and there exist many practically relevant instances of hypergraph datasets. To enable efficient processing of hypergraph-structured data, several hypergraph neural network platforms have been proposed for learning hypergraph properties and structure, with a special focus on node classification. However, almost all existing methods use heuristic propagation rules and offer suboptimal performance on many datasets. We propose AllSet, a new hypergraph neural network paradigm that represents a highly general framework for (hyper)graph neural networks and for the first time implements hypergraph neural network layers as compositions of two multiset functions that can be efficiently learned for each task and each dataset. Furthermore, AllSet draws on new connections between hypergraph neural networks and recent advances in deep learning of multiset functions. In particular, the proposed architecture utilizes Deep Sets and Set Transformer architectures that allow for significant modeling flexibility and offer high expressive power. To evaluate the performance of AllSet, we conduct the most extensive experiments to date involving ten known benchmarking datasets and three newly curated datasets that represent significant challenges for hypergraph node classification. The results demonstrate that AllSet has the unique ability to consistently either match or outperform all other hypergraph neural networks across the tested datasets. Our implementation and dataset will be released upon acceptance.",0
"Abstract: In recent years, hypergraph neural networks (HNN) have emerged as a powerful tool for modeling complex relationships among entities, where each entity can belong to multiple classes simultaneously. However, most existing frameworks focus on handling binary attributes, which limits their expressiveness and applicability in real-world scenarios involving multi-label data. To address this limitation, we propose AllSet, a novel multiset function framework that extends HNN by allowing edges to represent any number of elements within different sets. With AllSet, our models learn more generalized features, enabling better performance across tasks while reducing the risk of overfitting. Our experiments demonstrate the effectiveness of our approach on several benchmark datasets, achieving state-of-the-art results compared to traditional binary HNNs and other multi-labeled baselines. Overall, AllSet provides a simple yet effective solution for efficient representation learning, expanding HNN capabilities towards more complex multi-set patterns.",1
"Deep neural networks, while generalize well, are known to be sensitive to small adversarial perturbations. This phenomenon poses severe security threat and calls for in-depth investigation of the robustness of deep learning models. With the emergence of neural networks for graph structured data, similar investigations are urged to understand their robustness. It has been found that adversarially perturbing the graph structure and/or node features may result in a significant degradation of the model performance. In this work, we show from a different angle that such fragility similarly occurs if the graph contains a few bad-actor nodes, which compromise a trained graph neural network through flipping the connections to any targeted victim. Worse, the bad actors found for one graph model severely compromise other models as well. We call the bad actors ``anchor nodes'' and propose an algorithm, named GUA, to identify them. Thorough empirical investigations suggest an interesting finding that the anchor nodes often belong to the same class; and they also corroborate the intuitive trade-off between the number of anchor nodes and the attack success rate. For the dataset Cora which contains 2708 nodes, as few as six anchor nodes will result in an attack success rate higher than 80\% for GCN and other three models.",0
"This paper presents a study on universal adversarial attacks (UAA) that target graph learning models such as GNNs. UAAs aim to find small perturbations that can fool any classifier regardless of model architecture and input space. Our contributions show that only a few bad actors in the graph cause significant degradation in accuracy for these classifiers. We evaluate two popular attack methods, Sparse PCA and DeepFool, which perform well even against state-of-the-art defense mechanisms. In addition, we propose a new attack algorithm based on iterative edge removal that outperforms existing methods by more effectively utilizing the limited number of adversarial edges available during inference. Lastly, we discuss potential applications of our work towards improving robustness and reliability of real-world systems that employ graph learning techniques.",1
"Graph Neural Networks (GNNs) have been extensively used for mining graph-structured data with impressive performance. However, traditional GNNs suffer from over-smoothing, non-robustness and over-fitting problems. To solve these weaknesses, we design a novel GNN solution, namely Graph Attention Network with LSTM-based Path Reweighting (PR-GAT). PR-GAT can automatically aggregate multi-hop information, highlight important paths and filter out noises. In addition, we utilize random path sampling in PR-GAT for data augmentation. The augmented data is used for predicting the distribution of corresponding labels. Finally, we demonstrate that PR-GAT can mitigate the issues of over-smoothing, non-robustness and overfitting. We achieve state-of-the-art accuracy on 5 out of 7 datasets and competitive accuracy for other 2 datasets. The average accuracy of 7 datasets have been improved by 0.5\% than the best SOTA from literature.",0
"This paper presents a novel approach for improving attention mechanisms in graph neural networks (GNN) using Long Short Term Memory (LSTM) units. GNNs have shown great success in many applications ranging from natural language processing to computer vision; however, they still suffer from limitations such as oversmoothing effects where node representations become homogeneous across layers. To address this challenge, we propose Graph Attention Networks with LSTM-based Path Reweighting (GAT+), which reweights attention paths based on their importance derived from node features and learned by a separate path selection subnetwork built with LSTMs. Our proposed method enhances attentiveness towards important nodes while preserving crucial structural relationships among them, thus achieving better performance than existing methods. We evaluate our model on several benchmark datasets and demonstrate significant improvements in accuracy. Additionally, we analyze the impact of different components of our design on results, demonstrating that GAT+ indeed captures long-term dependencies effectively. Overall, our work makes a meaningful contribution to the field of graph machine learning and paves the way for future research on enhancing attention models in GNNs.",1
"This paper presents a new approach for assembling graph neural networks based on framelet transforms. The latter provides a multi-scale representation for graph-structured data. We decompose an input graph into low-pass and high-pass frequencies coefficients for network training, which then defines a framelet-based graph convolution. The framelet decomposition naturally induces a graph pooling strategy by aggregating the graph feature into low-pass and high-pass spectra, which considers both the feature values and geometry of the graph data and conserves the total information. The graph neural networks with the proposed framelet convolution and pooling achieve state-of-the-art performance in many node and graph prediction tasks. Moreover, we propose shrinkage as a new activation for the framelet convolution, which thresholds high-frequency information at different scales. Compared to ReLU, shrinkage activation improves model performance on denoising and signal compression: noises in both node and structure can be significantly reduced by accurately cutting off the high-pass coefficients from framelet decomposition, and the signal can be compressed to less than half its original size with well-preserved prediction performance.",0
"In recent years, graph neural networks (GNNs) have emerged as powerful tools for processing structured data such as graphs, networks, and trees. However, GNNs can suffer from problems related to overfitting and lack of robustness. To address these issues, researchers have proposed using framelets, which are mathematical constructs that provide a compact representation of high-dimensional signals, as an additional tool for enhancing GNN performance. This paper presents experimental results showing how incorporating framelets into GNN architectures leads to significant improvements in accuracy and stability across multiple datasets and tasks. Additionally, we demonstrate that our methodology effectively reduces the sensitivity of the models to input perturbations while achieving better generalization on unseen test sets compared to state-of-the-art GNN approaches. Our findings suggest that framelet augmentation represents a promising new direction for advancing the field of graph learning.",1
"The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of Graphormer.",0
"This paper examines the performance of transformer models on graph representation tasks and finds that they may not perform as well as previously thought. While transformers have proven successful in many natural language processing (NLP) tasks, their ability to handle complex graph structures has been less thoroughly evaluated. To investigate this gap, we conduct experiments using several popular datasets and baseline models, comparing traditional graph neural networks (GNNs) with state-of-the-art transformer architectures like GPT-2 and T5. Our results show that while some transformer models can achieve competitive results on simpler graph problems, they tend to underperform compared to GNNs on more complex data, where knowledge about structure and topology is crucial. We conclude by discussing potential reasons behind these findings and proposing future research directions towards enhancing the capabilities of transformers for handling graphs.",1
"Graphs are versatile tools for representing structured data. As a result, a variety of machine learning methods have been studied for graph data analysis. Although many such learning methods depend on the measurement of differences between input graphs, defining an appropriate distance metric for graphs remains a controversial issue. Hence, we propose a supervised distance metric learning method for the graph classification problem. Our method, named interpretable graph metric learning (IGML), learns discriminative metrics in a subgraph-based feature space, which has a strong graph representation capability. By introducing a sparsity-inducing penalty on the weight of each subgraph, IGML can identify a small number of important subgraphs that can provide insight into the given classification task. Because our formulation has a large number of optimization variables, an efficient algorithm that uses pruning techniques based on safe screening and working set selection methods is also proposed. An important property of IGML is that solution optimality is guaranteed because the problem is formulated as a convex problem and our pruning strategies only discard unnecessary subgraphs. Furthermore, we show that IGML is also applicable to other structured data such as itemset and sequence data, and that it can incorporate vertex-label similarity by using a transportation-based subgraph feature. We empirically evaluate the computational efficiency and classification performance of IGML on several benchmark datasets and provide some illustrative examples of how IGML identifies important subgraphs from a given graph dataset.",0
"Here's an example of an abstract for ""Distance Metric Learning for Graph Structured Data"":  The problem of graph structured data arises frequently in many applications such as image processing, natural language processing, social network analysis, etc. Traditional methods for learning a distance metric on graphs rely heavily on handcrafted features which often lead to suboptimal results. This work proposes a deep learning based method that learns a neural network model capable of predicting pairwise distances directly from raw input tensors. Our proposed algorithm adopts a contrastive loss function for supervised training and can also incorporate additional unsupervised regularization terms, leading to improved performance compared to competitive baseline methods. The experimental evaluation demonstrates the efficacy of our approach across multiple application domains including visual correspondence search, node classification, and clustering tasks, validating the generality of our framework. Additionally, we analyze different components of our algorithm and provide theoretical insights into why our method works well in practice.",1
"An outlier is an observation or a data point that is far from rest of the data points in a given dataset or we can be said that an outlier is away from the center of mass of observations. Presence of outliers can skew statistical measures and data distributions which can lead to misleading representation of the underlying data and relationships. It is seen that the removal of outliers from the training dataset before modeling can give better predictions. With the advancement of machine learning, the outlier detection models are also advancing at a good pace. The goal of this work is to highlight and compare some of the existing outlier detection techniques for the data scientists to use that information for outlier algorithm selection while building a machine learning model.",0
"In recent years, outlier detection has become increasingly important due to the abundance of structured data available in various domains such as finance, healthcare, and business. Traditional unsupervised learning methods have been used to identify outliers but they often lack sufficient accuracy. This study evaluates three commonly used outlier detection techniques: local outlier factor (LOF), one-class SVMs, and isolation forest (IF). We assess their performance on benchmark datasets, comparing each method’s effectiveness at identifying both known outliers and unknown anomalies in structured data. Our results show that LOF consistently achieves high precision and recall rates, while IF performs better than traditional classifiers like support vector machines. Additionally, we found that combining multiple algorithms can improve overall outlier detection accuracy. Our research concludes by recommending appropriate usage scenarios for each technique based on specific application requirements.",1
"The spatio-temporal graph learning is becoming an increasingly important object of graph study. Many application domains involve highly dynamic graphs where temporal information is crucial, e.g. traffic networks and financial transaction graphs. Despite the constant progress made on learning structured data, there is still a lack of effective means to extract dynamic complex features from spatio-temporal structures. Particularly, conventional models such as convolutional networks or recurrent neural networks are incapable of revealing the temporal patterns in short or long terms and exploring the spatial properties in local or global scope from spatio-temporal graphs simultaneously. To tackle this problem, we design a novel multi-scale architecture, Spatio-Temporal U-Net (ST-UNet), for graph-structured time series modeling. In this U-shaped network, a paired sampling operation is proposed in spacetime domain accordingly: the pooling (ST-Pool) coarsens the input graph in spatial from its deterministic partition while abstracts multi-resolution temporal dependencies through dilated recurrent skip connections; based on previous settings in the downsampling, the unpooling (ST-Unpool) restores the original structure of spatio-temporal graphs and resumes regular intervals within graph sequences. Experiments on spatio-temporal prediction tasks demonstrate that our model effectively captures comprehensive features in multiple scales and achieves substantial improvements over mainstream methods on several real-world datasets.",0
"In order to achieve state of art results in graph structured time series modeling we propose new architecture called ST-Unet. By utilizing both spatial and temporal context our network can outperform other methods by significant margin. Unlike traditional architectures that use only one type of convolutions we have two main branches - Spatial branch that captures static features and Temporal branch that encodes temporal dependencies between consecutive input samples. This design enables us to learn fine grained spatiotemporal representation which in turn leads to better performance on benchmark datasets like M4, BAQ, SARCOS, and UTIAOD. We provide ablation study that demonstrates importance of each component as well as comparison against strong baselines such as GCN, LSTM, TCN and several others popular models. Finally we provide extensive analysis of learned representations using t-Distributed Stochastic Neighbor Embedding (t-SNE) method showing that our learned embeddings contain meaningful information and capture underlying structure of data. Overall this work presents important contribution in graph structured deep learning area and provides future research directions in this exciting domain.",1
"The Wasserstein distance provides a notion of dissimilarities between probability measures, which has recent applications in learning of structured data with varying size such as images and text documents. In this work, we analyze the $k$-nearest neighbor classifier ($k$-NN) under the Wasserstein distance and establish the universal consistency on families of distributions. Using previous known results on the consistency of the $k$-NN classifier on infinite dimensional metric spaces, it suffices to show that the families is a countable union of finite dimension sets. As a result, we show that the $k$-NN classifier is universally consistent on spaces of finitely supported measures, the space of Gaussian measures, and the space of measures with finite wavelet densities. In addition, we give a counterexample to show that the universal consistency does not hold on $\mathcal{W}_p((0,1))$.",0
"This paper investigates the theoretical properties of the Wasserstein $k$-Nearest Neighbors ($W_k$NN) algorithm, which has recently emerged as a promising approach to nonparametric classification tasks. We prove that under mild assumptions, the Bayes error rate can always be achieved by selecting a sufficiently large value of k. Moreover, we show that the choice of k that achieves universality depends only on the underlying probability distribution and the metric space structure of the data. These results provide insight into the design and selection of $W_k$NN algorithms for different applications and shed light on their relationship to other popular distance-based methods such as $\epsilon$-$k$NN and kernel density estimators. Overall, our findings indicate that $W_k$NN offers a flexible and powerful alternative for nonparametric regression and classification problems while maintaining strong statistical guarantees.",1
"Recent years have witnessed tremendous interest in deep learning on graph-structured data. Due to the high cost of collecting labeled graph-structured data, domain adaptation is important to supervised graph learning tasks with limited samples. However, current graph domain adaptation methods are generally adopted from traditional domain adaptation tasks, and the properties of graph-structured data are not well utilized. For example, the observed social networks on different platforms are controlled not only by the different crowd or communities but also by the domain-specific policies and the background noise. Based on these properties in graph-structured data, we first assume that the graph-structured data generation process is controlled by three independent types of latent variables, i.e., the semantic latent variables, the domain latent variables, and the random latent variables. Based on this assumption, we propose a disentanglement-based unsupervised domain adaptation method for the graph-structured data, which applies variational graph auto-encoders to recover these latent variables and disentangles them via three supervised learning modules. Extensive experimental results on two real-world datasets in the graph classification task reveal that our method not only significantly outperforms the traditional domain adaptation methods and the disentangled-based domain adaptation methods but also outperforms the state-of-the-art graph domain adaptation algorithms.",0
"Graph Domain Adaption is the process by which a model trained on one dataset can accurately predict outcomes for a new but related task or domain, despite differences such as changed environments or objectives. Our work focuses on developing novel techniques that leverage generative models to improve graph domain adaptation performance. We showcase three use cases demonstrating our methods achieve results comparable to those obtained from manually fine tuning the original model for each specific domain. Additionally, we provide qualitative analyses of the generated data to validate that the method creates meaningful representations that better capture underlying structure across tasks. This research has applications in industries where fast adaptation to changing conditions is crucial, including robotics, medical diagnostics, and self-driving vehicles.",1
"While the advent of Graph Neural Networks (GNNs) has greatly improved node and graph representation learning in many applications, the neighborhood aggregation scheme exposes additional vulnerabilities to adversaries seeking to extract node-level information about sensitive attributes. In this paper, we study the problem of protecting sensitive attributes by information obfuscation when learning with graph structured data. We propose a framework to locally filter out pre-determined sensitive attributes via adversarial training with the total variation and the Wasserstein distance. Our method creates a strong defense against inference attacks, while only suffering small loss in task performance. Theoretically, we analyze the effectiveness of our framework against a worst-case adversary, and characterize an inherent trade-off between maximizing predictive accuracy and minimizing information leakage. Experiments across multiple datasets from recommender systems, knowledge graphs and quantum chemistry demonstrate that the proposed approach provides a robust defense across various graph structures and tasks, while producing competitive GNN encoders for downstream tasks.",0
"In recent years, graph neural networks (GNNs) have emerged as powerful tools for processing structured data represented as graphs. However, these models rely heavily on node features to make predictions, which can lead to privacy concerns if sensitive information is present in those features. To address this issue, we propose a novel technique called ""information obfuscation"" that seeks to conceal critical information from the GNN model while minimizing impact on performance. Our approach involves adding noise to edge weights and node degrees based on their importance to downstream tasks, as well as utilizing adversarial training to further disrupt inference of sensitive attributes. Empirical evaluation shows that our method effectively reduces information leakage without significantly affecting predictive accuracy across several benchmark datasets and application domains. Overall, our work demonstrates a promising direction toward enhancing privacy preservation in GNNs while maintaining strong model performance.",1
"Link prediction is an important learning task for graph-structured data. In this paper, we propose a novel topological approach to characterize interactions between two nodes. Our topological feature, based on the extended persistent homology, encodes rich structural information regarding the multi-hop paths connecting nodes. Based on this feature, we propose a graph neural network method that outperforms state-of-the-arts on different benchmarks. As another contribution, we propose a novel algorithm to more efficiently compute the extended persistence diagrams for graphs. This algorithm can be generally applied to accelerate many other topological methods for graph learning tasks.",0
"In recent years, link prediction has become an increasingly important problem across many domains ranging from social networks to protein interaction graphs. Existing methods predominantly use features derived from graph structure such as degree centrality or clustering coefficients. However, these approaches often lack explanatory power since they cannot capture global geometric properties that may reveal inherent structures in data. Here we introduce a novel algorithm based on persistent homology that can predict links by extracting intrinsic topological features from complex network datasets. We further enhance our approach with interactive visualizations designed to enable users to interactively explore predicted links, observe their effect on persistence diagrams, and analyze how different parameters affect link predictions. Our experimental results demonstrate both the accuracy of our method as well as the usability of our interface. This work holds significance in providing new insights into data analysis beyond traditional feature extraction techniques and opening up future research directions at the intersection of algebraic topology, computer graphics, and data science.",1
"Deep neural networks have revolutionized many machine learning tasks in power systems, ranging from pattern recognition to signal processing. The data in these tasks is typically represented in Euclidean domains. Nevertheless, there is an increasing number of applications in power systems, where data are collected from non-Euclidean domains and represented as graph-structured data with high dimensional features and interdependency among nodes. The complexity of graph-structured data has brought significant challenges to the existing deep neural networks defined in Euclidean domains. Recently, many publications generalizing deep neural networks for graph-structured data in power systems have emerged. In this paper, a comprehensive overview of graph neural networks (GNNs) in power systems is proposed. Specifically, several classical paradigms of GNNs structures (e.g., graph convolutional networks) are summarized, and key applications in power systems, such as fault scenario application, time series prediction, power flow calculation, and data generation are reviewed in detail. Furthermore, main issues and some research trends about the applications of GNNs in power systems are discussed.",0
"Graph Neural Networks (GNN) have emerged as powerful deep learning models that operate directly on graph data structure, which makes them particularly suitable for power systems applications given the network nature of the grid. This review seeks to provide a comprehensive survey of GNN architectures and their recent advancements, highlighting their unique strengths and challenges relative to traditional machine learning techniques such as support vector machines (SVM), decision trees, random forest, and artificial neural networks (ANN). We then focus on various application domains within the power system where GNNs have been successfully applied, including load forecasting, state estimation, contingency analysis, voltage stability assessment, and control strategy optimization. Overall, we aim to identify promising research directions and areas requiring further attention towards realizing the full potential of GNN technology in enhancing the resilience, reliability, efficiency, and sustainability of modern electrical grids worldwide.",1
"HyperGraph Convolutional Neural Networks (HGCNNs) have demonstrated their potential in modeling high-order relations preserved in graph structured data. However, most existing convolution filters are localized and determined by the pre-defined initial hypergraph topology, neglecting to explore implicit and long-ange relations in real-world data. In this paper, we propose the first learning-based method tailored for constructing adaptive hypergraph structure, termed HypERgrAph Laplacian aDaptor (HERALD), which serves as a generic plug-in-play module for improving the representational power of HGCNNs. Specifically, HERALD adaptively optimizes the adjacency relationship between hypernodes and hyperedges in an end-to-end manner and thus the task-aware hypergraph is learned. Furthermore, HERALD employs the self-attention mechanism to capture the non-local paired-nodes relation. Extensive experiments on various popular hypergraph datasets for node classification and graph classification tasks demonstrate that our approach obtains consistent and considerable performance enhancement, proving its effectiveness and generalization ability.",0
"In recent years, hypergraph learning has emerged as a powerful tool for analyzing complex relationships within high-dimensional datasets. However, many existing methods suffer from computational complexity and scalability issues when dealing with large data sets. To address these limitations, we propose a learnable hypergraph Laplacian method that enables efficient hypergraph representation learning while also providing effective results. Our approach leverages the advantages of both linear algebraic techniques and deep neural networks, enabling accurate hypergraph embedding generation. Experimental evaluation demonstrates significant improvements over state-of-the-art methods on several benchmark datasets across different tasks including node classification, link prediction, and clustering analysis. The proposed framework shows great potential for advancing research in the field of graph mining and knowledge discovery using hypergraphs.",1
"Supervised machine learning has several drawbacks that make it difficult to use in many situations. Drawbacks include: heavy reliance on massive training data, limited generalizability and poor expressiveness of high-level semantics. Low-shot Learning attempts to address these drawbacks. Low-shot learning allows the model to obtain good predictive power with very little or no training data, where structured knowledge plays a key role as a high-level semantic representation of human. This article will review the fundamental factors of low-shot learning technologies, with a focus on the operation of structured knowledge under different low-shot conditions. We also introduce other techniques relevant to low-shot learning. Finally, we point out the limitations of low-shot learning, the prospects and gaps of industrial applications, and future research directions.",0
"This paper conducts a survey on how knowledge can enhance machine learning performance through low-shot learning techniques applied to structured data. We discuss current trends and state-of-the art methods in the field, including their advantages and disadvantages. Our focus lies specifically on approaches that leverage prior knowledge, such as pretraining tasks, intrinsic and extrinsic regularization, active data augmentation, and model selection strategies that explicitly consider domain expertise during training time. By combining these methods together, we demonstrate how transferring external knowledge into the learning process helps improve generalization abilities and enables better decision making from limited training samples. Through our analysis, we aim to provide researchers and practitioners with an overview of different methods and inspire new developments in machine learning applications where labeled examples are scarce but priors from experts are available. Ultimately, our goal is to emphasize the importance of incorporating external knowledge sources when dealing with real-world constraints in data availability, while maintaining high accuracy expectations from machine learning models.",1
"Graph Neural Networks (GNNs) have been widely applied to various fields due to their powerful representations of graph-structured data. Despite the success of GNNs, most existing GNNs are designed to learn node representations on the fixed and homogeneous graphs. The limitations especially become problematic when learning representations on a misspecified graph or a heterogeneous graph that consists of various types of nodes and edges. To address this limitations, we propose Graph Transformer Networks (GTNs) that are capable of generating new graph structures, which preclude noisy connections and include useful connections (e.g., meta-paths) for tasks, while learning effective node representations on the new graphs in an end-to-end fashion. We further propose enhanced version of GTNs, Fast Graph Transformer Networks (FastGTNs), that improve scalability of graph transformations. Compared to GTNs, FastGTNs are 230x faster and use 100x less memory while allowing the identical graph transformations as GTNs. In addition, we extend graph transformations to the semantic proximity of nodes allowing non-local operations beyond meta-paths. Extensive experiments on both homogeneous graphs and heterogeneous graphs show that GTNs and FastGTNs with non-local operations achieve the state-of-the-art performance for node classification tasks. The code is available: https://github.com/seongjunyun/Graph_Transformer_Networks",0
"Abstract ----- I am sorry i didn’t see your last message. Here is another attempt:  Graph Transformer Networks (GTN) is a new architecture that incorporates meta-path graphs into traditional graph neural networks (GNN), significantly improving their performance. In GTNs, each layer aggregates information from neighbors along specified paths and updates the node representation accordingly. By learning these paths dynamically during training, we provide an efficient alternative to predefined metapaths. Our method outperforms state-of-the-art baselines on five benchmark datasets by up to 29%. We visualize learned paths and demonstrate the utility of our approach via case studies. This research contributes novel insights for understanding how meta-paths can improve graph representations.",1
"Graph neural networks (GNNs) are one of the most popular approaches to using deep learning on graph-structured data, and they have shown state-of-the-art performances on a variety of tasks. However, according to a recent study, a careful choice of pooling functions, which are used for the aggregation or readout operation in GNNs, is crucial for enabling GNNs to extrapolate. Without the ideal combination of pooling functions, which varies across tasks, GNNs completely fail to generalize to out-of-distribution data, while the number of possible combinations grows exponentially with the number of layers. In this paper, we present GNP, a $L^p$ norm-like pooling function that is trainable end-to-end for any given task. Notably, GNP generalizes most of the widely-used pooling functions. We verify experimentally that simply replacing all pooling functions with GNP enables GNNs to extrapolate well on many node-level, graph-level, and set-related tasks; and GNP sometimes performs even better than optimal combinations of existing pooling functions.",0
"In this work we describe a new method for using graph neural networks (GNN) that can allow them to extrapolate beyond their training data. Our key insight is that GNNs usually learn to aggregate neighborhood information into a fixed size vector, but if they could learn to pool different subsets of nodes then more complex models would become feasible. We show how to train such pooled GNNs, which allows us to build models that can generalize well on many real world datasets such as MNIST and CIFAR, even though our model architecture is very simple. These results may lead to future progress on difficult problems like natural language understanding.",1
"HyperGraph Convolutional Neural Networks (HGCNNs) have demonstrated their potential in modeling high-order relations preserved in graph structured data. However, most existing convolution filters are localized and determined by the pre-defined initial hypergraph topology, neglecting to explore implicit and long-ange relations in real-world data. In this paper, we propose the first learning-based method tailored for constructing adaptive hypergraph structure, termed HypERgrAph Laplacian aDaptor (HERALD), which serves as a generic plug-in-play module for improving the representational power of HGCNNs. Specifically, HERALD adaptively optimizes the adjacency relationship between hypernodes and hyperedges in an end-to-end manner and thus the task-aware hypergraph is learned. Furthermore, HERALD employs the self-attention mechanism to capture the non-local paired-nodes relation. Extensive experiments on various popular hypergraph datasets for node classification and graph classification tasks demonstrate that our approach obtains consistent and considerable performance enhancement, proving its effectiveness and generalization ability.",0
"In order to make your job easier I have attached a draft of the abstract. --- Draft Abstract: This work introduces the Learnable Hypergraph Laplacian (LHL), which enables learning on hypergraphs via a parameterized linear algebraic framework. LHL generalizes classical graph Laplacians to allow interactions among multiple nodes, making it well suited for capturing nonlinear dependencies in data. Our method learns a local filtering operator that preserves smoothness while minimizing the rank deficiency of the Hessian matrix to improve optimization efficiency. We demonstrate LHL’s effectiveness by applying our method to several key hypergraph applications including hyperedge replacement problem, image demosaicking, sparse representation and super resolution reconstruction from compressive measurements. Extensive experimental results show that LHL significantly outperforms state-of-the-art alternatives. ​ The introduction of the Learnable Hypergraph Laplacian (LHL) represents a significant step forward in the field of hypergraph learning. This innovative approach utilizes a parameterized linear algebraic framework to enable more effective capture of nonlinear dependencies in data through generalized interactions among multiple nodes. Incorporating local filtering operations that preserve smoothness while minimizing the rank deficiency of the Hessian matrix, LHL allows for faster and more efficient optimization processes. Already proven to be highly effective in numerous applications such as hyperedge replacement problems, image demosaicking, sparse representation and super resolution reconstruction from compressive measurements; LHL holds great promise as a powerful tool within the realm of hypergraph learning. With superior performance over current state-of-the-art methods, researchers can expect improved accuracy and enhanced insights into complex datasets with the integration of LHL. Further exploration of the capabilities of the Learnable Hypergraph Laplacian is recommended for those interested i",1
"Graph is an usual representation of relational data, which are ubiquitous in manydomains such as molecules, biological and social networks. A popular approach to learningwith graph structured data is to make use of graph kernels, which measure the similaritybetween graphs and are plugged into a kernel machine such as a support vector machine.Weisfeiler-Lehman (WL) based graph kernels, which employ WL labeling scheme to extract subtree patterns and perform node embedding, are demonstrated to achieve great performance while being efficiently computable. However, one of the main drawbacks of ageneral kernel is the decoupling of kernel construction and learning process. For moleculargraphs, usual kernels such as WL subtree, based on substructures of the molecules, consider all available substructures having the same importance, which might not be suitable inpractice. In this paper, we propose a method to learn the weights of subtree patterns in the framework of WWL kernels, the state of the art method for graph classification task [14]. To overcome the computational issue on large scale data sets, we present an efficient learning algorithm and also derive a generalization gap bound to show its convergence. Finally, through experiments on synthetic and real-world data sets, we demonstrate the effectiveness of our proposed method for learning the weights of subtree patterns.",0
"Graphs have been used as data structures because they can capture both qualitative and quantitative aspects of many real world problems such as chemical compounds and social networks. Since graphs often contain repeating patterns which might not need distinct node labels, labelless graph kernels may learn better representations than their labeled counterparts. In particular, the Weisfeiler-Lehman kernel (WLK) has recently gained attention as an efficient and effective way to compute pairwise similarities on graphs using only information at a fixed depth of neighborhood. However, since there may exist redundant or irrelevant substructures that could harm WLK performance, we must consider how to determine their significance. This work presents a methodology based on mutual information to estimate the importance of different sized subtrees on graph similarity computation. Our experimental results demonstrate that pruning smaller and less relevant patterns from the feature space leads to improved accuracy across different datasets.",1
"Event forecasting is a challenging, yet important task, as humans seek to constantly plan for the future. Existing automated forecasting studies rely mostly on structured data, such as time-series or event-based knowledge graphs, to help predict future events. In this work, we aim to formulate a task, construct a dataset, and provide benchmarks for developing methods for event forecasting with large volumes of unstructured text data. To simulate the forecasting scenario on temporal news documents, we formulate the problem as a restricted-domain, multiple-choice, question-answering (QA) task. Unlike existing QA tasks, our task limits accessible information, and thus a model has to make a forecasting judgement. To showcase the usefulness of this task formulation, we introduce ForecastQA, a question-answering dataset consisting of 10,392 event forecasting questions, which have been collected and verified via crowdsourcing efforts. We present our experiments on ForecastQA using BERT-based models and find that our best model achieves 60.1% accuracy on the dataset, which still lags behind human performance by about 19%. We hope ForecastQA will support future research efforts in bridging this gap.",0
"This paper presents a new question answering challenge called ForecastQA that focuses on predictive event forecasting using temporal text data. We introduce five distinct tasks designed to evaluate systems’ ability to make accurate predictions related to future events from noisy and sparse datasets including news articles, social media posts, satellite imagery and other open source intelligence reports. These challenges span topics such as political violence, disease outbreaks, and natural disasters where timely and accurate forecasts can have significant real-world impacts. Our evaluation process involves crowdsourcing human annotations to determine system outputs ground truth labels which we make publicly available through our website https://forecastqa.com/evaluation. Through these efforts, we aim to foster community engagement in this research area by providing a comprehensive benchmark to compare different approaches and spur progress towards more advanced predictive models.",1
"Graph Neural Network (GNN) has been demonstrated its effectiveness in dealing with non-Euclidean structural data. Both spatial-based and spectral-based GNNs are relying on adjacency matrix to guide message passing among neighbors during feature aggregation. Recent works have mainly focused on powerful message passing modules, however, in this paper, we show that none of the message passing modules is necessary. Instead, we propose a pure multilayer-perceptron-based framework, Graph-MLP with the supervision signal leveraging graph structure, which is sufficient for learning discriminative node representation. In model-level, Graph-MLP only includes multi-layer perceptrons, activation function, and layer normalization. In the loss level, we design a neighboring contrastive (NContrast) loss to bridge the gap between GNNs and MLPs by utilizing the adjacency information implicitly. This design allows our model to be lighter and more robust when facing large-scale graph data and corrupted adjacency information. Extensive experiments prove that even without adjacency information in testing phase, our framework can still reach comparable and even superior performance against the state-of-the-art models in the graph node classification task.",0
"In recent years, graph neural networks (GNNs) have emerged as powerful models for analyzing complex data structures such as graphs. While GNNs can capture rich representations of input data through their message passing mechanisms, these mechanisms can suffer from high computational cost and overfitting due to the large number of parameters involved. To address these issues, we propose Graph-MLP, a novel approach that utilizes MLP layers on each node in a graph instead of performing explicit message passing operations. This allows us to learn effective representations while significantly reducing computation time and preventing overfitting. We evaluate our model on several benchmark datasets across a variety of domains including social network analysis, computer vision, and biology, and demonstrate significant improvements over state-of-the-art methods in terms of accuracy and efficiency. Our findings provide important insights into the potential of MLP-based architectures for graph classification tasks and offer new opportunities for developing more scalable and efficient approaches to deep learning on graph data. Overall, Graph-MLP represents an exciting step forward in the field of graph analytics and offers promising solutions for tackling real-world problems involving complex relational data structures.",1
"Various classes of Graph Neural Networks (GNN) have been proposed and shown to be successful in a wide range of applications with graph structured data. In this paper, we propose a theoretical framework able to compare the expressive power of these GNN architectures. The current universality theorems only apply to intractable classes of GNNs. Here, we prove the first approximation guarantees for practical GNNs, paving the way for a better understanding of their generalization. Our theoretical results are proved for invariant GNNs computing a graph embedding (permutation of the nodes of the input graph does not affect the output) and equivariant GNNs computing an embedding of the nodes (permutation of the input permutes the output). We show that Folklore Graph Neural Networks (FGNN), which are tensor based GNNs augmented with matrix multiplication are the most expressive architectures proposed so far for a given tensor order. We illustrate our results on the Quadratic Assignment Problem (a NP-Hard combinatorial problem) by showing that FGNNs are able to learn how to solve the problem, leading to much better average performances than existing algorithms (based on spectral, SDP or other GNNs architectures). On a practical side, we also implement masked tensors to handle batches of graphs of varying sizes.",0
This should clearly describe the content of your paper without leaving any information out but at the same time remain concise. Also please make sure that the abstract contains no spelling errors as they may cause me to disqualify you from the contest if I am unable to comprehend some sentences due to those errors.,1
"This paper presents Gem, a model-agnostic approach for providing interpretable explanations for any GNNs on various graph learning tasks. Specifically, we formulate the problem of providing explanations for the decisions of GNNs as a causal learning task. Then we train a causal explanation model equipped with a loss function based on Granger causality. Different from existing explainers for GNNs, Gem explains GNNs on graph-structured data from a causal perspective. It has better generalization ability as it has no requirements on the internal structure of the GNNs or prior knowledge on the graph learning tasks. In addition, Gem, once trained, can be used to explain the target GNN very quickly. Our theoretical analysis shows that several recent explainers fall into a unified framework of additive feature attribution methods. Experimental results on synthetic and real-world datasets show that Gem achieves a relative increase of the explanation accuracy by up to $30\%$ and speeds up the explanation process by up to $110\times$ as compared to its state-of-the-art alternatives.",0
"Graph neural networks have gained increasing popularity due to their ability to learn from structured data such as graphs, which are ubiquitous across many application domains. However, interpretability remains one of the primary challenges for these models, particularly when attempting to explain how they arrived at a certain decision. In our work, we propose an approach that utilizes generative causal reasoning techniques to provide interpretable explanations of graph neural network predictions by identifying key features that contributed most strongly to the predicted output. Our method can generate synthetic counterfactual examples that demonstrate how specific input factors impact model predictions, allowing users to better understand why particular decisions were made. We evaluate our proposed approach on several benchmark datasets, demonstrating its effectiveness in generating accurate and meaningful explanations of graph neural network behavior. By providing transparency into complex machine learning systems, our method has the potential to enhance trustworthiness and reliability of artificial intelligence applications, ultimately benefiting endusers.",1
"Learning the representation of data with hierarchical structures in the hyperbolic space attracts increasing attention in recent years. Due to the constant negative curvature, the hyperbolic space resembles tree metrics and captures the tree-like properties naturally, which enables the hyperbolic embeddings to improve over traditional Euclidean models. However, many real-world hierarchically structured data such as taxonomies and multitree networks have varying local structures and they are not trees, thus they do not ubiquitously match the constant curvature property of the hyperbolic space. To address this limitation of hyperbolic embeddings, we explore the complex hyperbolic space, which has the variable negative curvature, for representation learning. Specifically, we propose to learn the embeddings of hierarchically structured data in the unit ball model of the complex hyperbolic space. The unit ball model based embeddings have a more powerful representation capacity to capture a variety of hierarchical structures. Through experiments on synthetic and real-world data, we show that our approach improves over the hyperbolic embedding models significantly.",0
"This work presents a new mathematical model for embedding hierarchical structures into complex hyperbolic space. The unit ball model provides a methodology for mapping high-dimensional data onto lower-dimensional representations while preserving their structural characteristics. By exploiting the geometry of complex hyperbolic spaces, we demonstrate that our approach results in more accurate embeddings compared to traditional methods such as Euclidean and spherical maps. Our findings have important implications for fields ranging from machine learning to computational neuroscience, where understanding complex hierarchical relationships is essential. In summary, our research contributes to the development of more effective tools for exploring and visualizing high-dimensional datasets.",1
"Imbalanced classification on graphs is ubiquitous yet challenging in many real-world applications, such as fraudulent node detection. Recently, graph neural networks (GNNs) have shown promising performance on many network analysis tasks. However, most existing GNNs have almost exclusively focused on the balanced networks, and would get unappealing performance on the imbalanced networks. To bridge this gap, in this paper, we present a generative adversarial graph network model, called ImGAGN to address the imbalanced classification problem on graphs. It introduces a novel generator for graph structure data, named GraphGenerator, which can simulate both the minority class nodes' attribute distribution and network topological structure distribution by generating a set of synthetic minority nodes such that the number of nodes in different classes can be balanced. Then a graph convolutional network (GCN) discriminator is trained to discriminate between real nodes and fake (i.e., generated) nodes, and also between minority nodes and majority nodes on the synthetic balanced network. To validate the effectiveness of the proposed method, extensive experiments are conducted on four real-world imbalanced network datasets. Experimental results demonstrate that the proposed method ImGAGN outperforms state-of-the-art algorithms for semi-supervised imbalanced node classification task.",0
"In recent years, imbalanced learning has become increasingly important in machine learning due to the prevalence of class skewness in many real-world datasets. This paper presents a novel approach to addressing the problem of imbalance through the use of generative adversarial graph networks (GAGN). GAGN can effectively model complex relationships between classes by leveraging both intra-class dependencies as well as inter-class discrimination signals within graphs constructed from features extracted using deep neural networks. Our proposed method, called ""ImGAGN,"" offers several advantages over existing techniques, including improved stability during training, enhanced performance on both majority and minority classes, and applicability across different domains such as image classification, natural language processing, and medical diagnosis. Experimental results demonstrate that our approach outperforms state-of-the-art methods in terms of accuracy, F1 score, precision, recall, and visualization quality metrics. Overall, ImGAGN provides a promising new technique for tackling the challenges posed by class imbalances in modern machine learning applications.",1
"Graph Neural Networks (GNNs) are the first choice methods for graph machine learning problems thanks to their ability to learn state-of-the-art level representations from graph-structured data. However, centralizing a massive amount of real-world graph data for GNN training is prohibitive due to user-side privacy concerns, regulation restrictions, and commercial competition. Federated Learning is the de-facto standard for collaborative training of machine learning models over many distributed edge devices without the need for centralization. Nevertheless, training graph neural networks in a federated setting is vaguely defined and brings statistical and systems challenges. This work proposes SpreadGNN, a novel multi-task federated training framework capable of operating in the presence of partial labels and absence of a central server for the first time in the literature. SpreadGNN extends federated multi-task learning to realistic serverless settings for GNNs, and utilizes a novel optimization algorithm with a convergence guarantee, Decentralized Periodic Averaging SGD (DPA-SGD), to solve decentralized multi-task learning problems. We empirically demonstrate the efficacy of our framework on a variety of non-I.I.D. distributed graph-level molecular property prediction datasets with partial labels. Our results show that SpreadGNN outperforms GNN models trained over a central server-dependent federated learning system, even in constrained topologies. The source code is publicly available at https://github.com/FedML-AI/SpreadGNN",0
"Abstract: This paper presents an abstract on serverless multi-task federated learning for graph neural networks (SpreadGNN). The paper focuses on addressing the limitations associated with centralized training approaches by enabling distributed graph training without requiring any central servers. We present a decentralized approach that allows multiple edge devices to collaborate using their local model updates for jointly training GNN models while protecting sensitive information from being exposed over the network. Our method leverages novel mini-batch optimization techniques to reduce communication overheads while maintaining high convergence speeds. Our experiments demonstrate the effectiveness of our solution, outperforming state-of-the art systems under various conditions including weak links, low bandwidth, high latency, variable participation ratios, and diverse hardware configurations. Ultimately, we conclude that SpreadGNN represents a significant step forward in unlocking new possibilities at the intersection of machine learning, networking, computer architecture, security, privacy, and human behavior. By developing powerful algorithms capable of solving complex problems in these areas, we pave the way for achieving truly intelligent technology that can positively impact society as a whole.",1
"Graph Neural Networks (GNNs) are widely used deep learning models that learn meaningful representations from graph-structured data. Due to the finite nature of the underlying recurrent structure, current GNN methods may struggle to capture long-range dependencies in underlying graphs. To overcome this difficulty, we propose a graph learning framework, called Implicit Graph Neural Networks (IGNN), where predictions are based on the solution of a fixed-point equilibrium equation involving implicitly defined ""state"" vectors. We use the Perron-Frobenius theory to derive sufficient conditions that ensure well-posedness of the framework. Leveraging implicit differentiation, we derive a tractable projected gradient descent method to train the framework. Experiments on a comprehensive range of tasks show that IGNNs consistently capture long-range dependencies and outperform the state-of-the-art GNN models.",0
"Abstract: Recent advances in graph neural networks (GNNs) have greatly improved their ability to model complex relationships between data points in graphs. However, existing GNN architectures often struggle with capturing implicit relationships that cannot be directly inferred from the node attributes themselves. To address this limitation, we propose Implicit Graph Neural Networks (IGNN), which can learn both explicit and implicit relationships by leveraging techniques like attention mechanisms and meta learning. In our experiments on several benchmark datasets, IGNNs achieved state-of-the-art results and showed significant improvements over traditional GNN architectures in terms of accuracy and interpretability. Our work demonstrates the potential of using implicit relationships in graph neural networks for improving predictions across diverse domains, such as social network analysis, bioinformatics, and recommender systems. We believe that our research opens new opportunities for developing more powerful models that capture both explicit and implicit dependencies among data points, leading to even better predictive performance and deeper insights into real-world phenomena.",1
"Graph neural networks (GNNs) have been successfully employed in a myriad of applications involving graph-structured data. Theoretical findings establish that GNNs use nonlinear activation functions to create low-eigenvalue frequency content that can be processed in a stable manner by subsequent graph convolutional filters. However, the exact shape of the frequency content created by nonlinear functions is not known, and thus, it cannot be learned nor controlled. In this work, node-variant graph filters (NVGFs) are shown to be capable of creating frequency content and are thus used in lieu of nonlinear activation functions. This results in a novel GNN architecture that, although linear, is capable of creating frequency content as well. Furthermore, this new frequency content can be either designed or learned from data. In this way, the role of frequency creation is separated from the nonlinear nature of traditional GNNs. Extensive simulations are carried out to differentiate the contributions of frequency creation from those of the nonlinearity.",0
"In graph neural networks (GNN), message passing is a popular operation used to aggregate node features over neighboring nodes and edges. However, current GNN models apply uniform message functions across all neighbors, neglecting important structural differences between connected components that could lead to more expressive representations. This paper proposes a new mechanism called ""node variant filters"" that can capture localized patterns while preserving global connectivity through explicit computation of pathways from target nodes to their source communities. By leveraging variant graphs as an inductive bias and training filters on them, we show how the resulting node embeddings achieve significantly better clustering performance than state-of-the-art alternatives. Our approach enables efficient model sharing while adaptively learning per-instance task information at inference time, making our methodology appealing for large-scale machine learning tasks such as semi-supervised classification on real-world knowledge graphs where prior domain knowledge may vary widely. Experimental results demonstrate that our architecture delivers strong improvements across diverse domains ranging from biological systems to social media platforms.",1
"Networks are ubiquitous in the real world. Link prediction, as one of the key problems for network-structured data, aims to predict whether there exists a link between two nodes. The traditional approaches are based on the explicit similarity computation between the compact node representation by embedding each node into a low-dimensional space. In order to efficiently handle the intensive similarity computation in link prediction, the hashing technique has been successfully used to produce the node representation in the Hamming space. However, the hashing-based link prediction algorithms face accuracy loss from the randomized hashing techniques or inefficiency from the learning to hash techniques in the embedding process. Currently, the Graph Neural Network (GNN) framework has been widely applied to the graph-related tasks in an end-to-end manner, but it commonly requires substantial computational resources and memory costs due to massive parameter learning, which makes the GNN-based algorithms impractical without the help of a powerful workhorse. In this paper, we propose a simple and effective model called #GNN, which balances the trade-off between accuracy and efficiency. #GNN is able to efficiently acquire node representation in the Hamming space for link prediction by exploiting the randomized hashing technique to implement message passing and capture high-order proximity in the GNN framework. Furthermore, we characterize the discriminative power of #GNN in probability. The extensive experimental results demonstrate that the proposed #GNN algorithm achieves accuracy comparable to the learning-based algorithms and outperforms the randomized algorithm, while running significantly faster than the learning-based algorithms. Also, the proposed algorithm shows excellent scalability on a large-scale network with the limited resources.",0
"In recent years, graph neural networks (GNNs) have emerged as powerful tools for predicting missing links in graphs, such as social networks or protein interaction networks. However, their computational complexity has hindered their application on large graphs. To address this issue, we propose a new method that leverages hashing techniques to significantly reduce the computation time required by GNNs while maintaining high prediction accuracy. Our approach involves training GNN models using fast, hash-based neighborhood aggregation methods instead of traditional matrix multiplication. This enables us to scale up our model to handle larger graphs without sacrificing performance. Extensive experiments demonstrate that our proposed method outperforms state-of-the-art link prediction algorithms both in terms of runtime and accuracy. Our work shows the potential of combining the power of GNNs with efficient data structures like hashing, opening up exciting opportunities for applications in domains where scalability is critical.",1
"Estimating the amount of electricity that can be produced by rooftop photovoltaic systems is a time-consuming process that requires on-site measurements, a difficult task to achieve on a large scale. In this paper, we present an approach to estimate the solar potential of rooftops based on their location and architectural characteristics, as well as the amount of solar radiation they receive annually. Our technique uses computer vision to achieve semantic segmentation of roof sections and roof objects on the one hand, and a machine learning model based on structured building features to predict roof pitch on the other hand. We then compute the azimuth and maximum number of solar panels that can be installed on a rooftop with geometric approaches. Finally, we compute precise shading masks and combine them with solar irradiation data that enables us to estimate the yearly solar potential of a rooftop.",0
"This paper presents a methodology that allows local government to estimate solar energy potential from rooftop segments using digital imagery and building footprints. A convolutional neural network (CNN) was trained on aerial images labeled by human annotators. The CNN then identifies specific areas where PV panels can be installed for highest efficiency based on factors such as angle and shading from nearby buildings, trees, etc. Additionally, we provide structured data for each building including size, ownership, electricity costs, roof type and age, which all affect installation cost and return on investment. These results allow policymakers to assess their current renewable portfolio standards and target communities where they can achieve higher adoption rates through informed decision making. Ultimately, our model provides valuable insight into how municipalities can optimize their transition towards sustainable, clean energy sources. We expect this work will encourage further research along these lines in order to better predict rooftop solar viability in urban environments worldwide. In conclusion, the proposed framework enables accurate analysis of solar energy generation potential from cityscapes, promoting environmentally conscious planning practices for community leaders.",1
"Functional connectivity (FC) between regions of the brain can be assessed by the degree of temporal correlation measured with functional neuroimaging modalities. Based on the fact that these connectivities build a network, graph-based approaches for analyzing the brain connectome have provided insights into the functions of the human brain. The development of graph neural networks (GNNs) capable of learning representation from graph structured data has led to increased interest in learning the graph representation of the brain connectome. Although recent attempts to apply GNN to the FC network have shown promising results, there is still a common limitation that they usually do not incorporate the dynamic characteristics of the FC network which fluctuates over time. In addition, a few studies that have attempted to use dynamic FC as an input for the GNN reported a reduction in performance compared to static FC methods, and did not provide temporal explainability. Here, we propose STAGIN, a method for learning dynamic graph representation of the brain connectome with spatio-temporal attention. Specifically, a temporal sequence of brain graphs is input to the STAGIN to obtain the dynamic graph representation, while novel READOUT functions and the Transformer encoder provide spatial and temporal explainability with attention, respectively. Experiments on the HCP-Rest and the HCP-Task datasets demonstrate exceptional performance of our proposed method. Analysis of the spatio-temporal attention also provide concurrent interpretation with the neuroscientific knowledge, which further validates our method. Code is available at https://github.com/egyptdj/stagin",0
"This paper presents a novel deep learning approach to represent brain connectivity patterns using dynamic graphs, which can capture both static (structural) as well as time-varying aspects (functional). We introduce a graph neural network architecture that leverages spatio-temporal attention mechanisms to learn robust representations of functional brain networks from fMRI data. Our model takes advantage of the structural knowledge embedded in static graphs, while considering temporal variations by adaptively attending to different regions across distinct timesteps. Experimental results demonstrate superior performance compared to state-of-the-art methods on two publicly available datasets: Human Connectome Project (HCP), and Neuroimaging Data Archive (NDAR) multi-center study dataset. Additionally, we show that our method improves decoding accuracy in predicting cognitive test scores, suggesting that these learned dynamic graph representations provide more informative features capturing relevant neuronal activity patterns than traditional static models alone. Overall, our work paves a new path towards understanding how brain dynamics emerge from complex interactions among spatially distributed networks of interacting elements over time.",1
"Kernel methods on discrete domains have shown great promise for many challenging data types, for instance, biological sequence data and molecular structure data. Scalable kernel methods like Support Vector Machines may offer good predictive performances but do not intrinsically provide uncertainty estimates. In contrast, probabilistic kernel methods like Gaussian Processes offer uncertainty estimates in addition to good predictive performance but fall short in terms of scalability. While the scalability of Gaussian processes can be improved using sparse inducing point approximations, the selection of these inducing points remains challenging. We explore different techniques for selecting inducing points on discrete domains, including greedy selection, determinantal point processes, and simulated annealing. We find that simulated annealing, which can select inducing points that are not in the training set, can perform competitively with support vector machines and full Gaussian processes on synthetic data, as well as on challenging real-world DNA sequence data.",0
"In recent years, there has been significant interest in applying machine learning techniques to problems where data may be collected over a discrete domain rather than a continuous one. However, many existing approaches rely heavily on assumptions such as stationarity and smoothness that may not hold in these settings, leading to poor performance.  In this work, we propose a scalable method for fitting Gaussian processes (GPs) to data sampled from discrete domains, allowing us to capture complex relationships while remaining flexible enough to accommodate nonstationary and discontinuous functions. Our approach relies on adaptive quadrature rules tailored specifically to GP models defined over finite input spaces, which significantly reduces computation time compared to traditional schemes based on numerical integration or sampling strategies.  We demonstrate the effectiveness of our method using a variety of benchmark datasets drawn from diverse application areas including regression, classification, and time series modeling. Results show that our approach outperforms state-of-the-art methods across different evaluation metrics, making it well suited for tackling large-scale inference tasks and realistic scenarios involving sparsely distributed data. Additionally, we provide extensive experimental evidence emphasizing the impact of key hyperparameters on prediction accuracy and runtime efficiency under varying problem configurations.  Overall, the results highlight the potential benefits of adopting scalable methods for GP inference over discrete domains to address pressing challenges posed by big data applications in fields ranging from sensor networks and signal processing to bioinformatics and neuroscience. We hope that this research inspires further advancements toward developing robust probabilistic frameworks capable of handling increasingly intricate scientific questions motivated by real-world observations.",1
"This paper presents Sparse Tensor Classifier (STC), a supervised classification algorithm for categorical data inspired by the notion of superposition of states in quantum physics. By regarding an observation as a superposition of features, we introduce the concept of wave-particle duality in machine learning and propose a generalized framework that unifies the classical and the quantum probability. We show that STC possesses a wide range of desirable properties not available in most other machine learning methods but it is at the same time exceptionally easy to comprehend and use. Empirical evaluation of STC on structured data and text classification demonstrates that our methodology achieves state-of-the-art performances compared to both standard classifiers and deep learning, at the additional benefit of requiring minimal data pre-processing and hyper-parameter tuning. Moreover, STC provides a native explanation of its predictions both for single instances and for each target label globally.",0
"This paper presents a novel approach to building probabilistic classifiers for categorical data using principles inspired by quantum mechanics. We propose an explainable probabilistic model that can effectively capture complex dependencies between input features while still providing interpretable results. Our method uses a hybrid quantum circuit design that combines classical computing elements with quantum gates to efficiently compute Bayesian probabilities and classification confidences. By incorporating a measure of epistemic uncertainty into our decision making process, we provide a more comprehensive understanding of how different factors contribute to the final prediction. Experimental evaluations on several benchmark datasets demonstrate the effectiveness of our method compared to state-of-the-art alternatives. Overall, our work shows promising potential for applying quantum computing techniques to machine learning problems where interpretability and accuracy are important considerations.",1
"Over the last few years, we have seen increasing data generated from non-Euclidean domains, which are usually represented as graphs with complex relationships, and Graph Neural Networks (GNN) have gained a high interest because of their potential in processing graph-structured data. In particular, there is a strong interest in exploring the possibilities in performing convolution on graphs using an extension of the GNN architecture, generally referred to as Graph Convolutional Neural Networks (GCNN). Convolution on graphs has been achieved mainly in two forms: spectral and spatial convolutions. Due to the higher flexibility in exploring and exploiting the graph structure of data, recently, there is an increasing interest in investigating the possibilities that the spatial approach can offer. The idea of finding a way to adapt the network behaviour to the inputs they process to maximize the total performances has aroused much interest in the neural networks literature over the years. This paper presents a novel method to adapt the behaviour of a GCNN to the input proposing two ways to perform spatial convolution on graphs using input-based filters which are dynamically generated. Our model also investigates the problem of discovering and refining relations among nodes. The experimental assessment confirms the capabilities of the proposed approach, which achieves satisfying results using simple architectures with a low number of filters.",0
"Graph convolutional neural networks (GCNN) have become increasingly popular due to their ability to model graph structured data effectively. However, traditional GCNNs suffer from oversmoothing, where node representations lose important details as features propagate through multiple layers. This can lead to poor performance on downstream tasks that require discriminative information at fine scales. To address these issues, we propose dynamic filters, which adaptively learn high frequency channels for each layer based on local spectral analysis of the feature maps. Our approach allows the network to capture both global graph structure and local, high frequency patterns, enabling more expressive representation learning. We demonstrate state-of-the-art results across several benchmark datasets including MoleculeNet and Reddit Commentary Corpus using our proposed method. Overall, our work provides insights into how to design effective architectures for graph structured problems.",1
"Graph convolutional networks (GCNs) have achieved great success in dealing with data of non-Euclidean structures. Their success directly attributes to fitting graph structures effectively to data such as in social media and knowledge databases. For image processing applications, the use of graph structures and GCNs have not been fully explored. In this paper, we propose a novel encoder-decoder network with added graph convolutions by converting feature maps to vertexes of a pre-generated graph to synthetically construct graph-structured data. By doing this, we inexplicitly apply graph Laplacian regularization to the feature maps, making them more structured. The experiments show that it significantly boosts performance for image restoration tasks, including deblurring and super-resolution. We believe it opens up opportunities for GCN-based approaches in more applications.",0
"In recent years graph convolutional networks (GCN) have emerged as powerful tools for image restoration tasks such as deblurring and super-resolution, thanks to their ability to learn local geometric relationships among pixels in feature space. However, despite their success there remains room for improvement: current GCN models operate directly on images, leading to slow inference times and requiring large amounts of memory; furthermore, these methods may suffer from checkerboard artifacts caused by oversmoothing. To address these issues, we propose using GCNs in feature space instead. We introduce FusionGAN++, a new model that operates solely in feature space, allowing for faster inference and reduced memory usage. Our approach uses multiple generators and discriminators operating at different scales, which allows us to create high-quality outputs without suffering from checkerboard artifacts. We show that our method outperforms state-of-the-art methods on several benchmark datasets, including MNIST, CIFAR-10 and SVHN, demonstrating its effectiveness across a range of applications. Overall, we believe that this work represents an important step forward in advancing deep learning techniques for image restoration tasks.",1
"Traditional approaches for data anonymization consider relational data and textual data independently. We propose rx-anon, an anonymization approach for heterogeneous semi-structured documents composed of relational and textual attributes. We map sensitive terms extracted from the text to the structured data. This allows us to use concepts like k-anonymity to generate a joined, privacy-preserved version of the heterogeneous data input. We introduce the concept of redundant sensitive information to consistently anonymize the heterogeneous data. To control the influence of anonymization over unstructured textual data versus structured data attributes, we introduce a modified, parameterized Mondrian algorithm. The parameter $\lambda$ allows to give different weight on the relational and textual attributes during the anonymization process. We evaluate our approach with two real-world datasets using a Normalized Certainty Penalty score, adapted to the problem of jointly anonymizing relational and textual data. The results show that our approach is capable of reducing information loss by using the tuning parameter to control the Mondrian partitioning while guaranteeing k-anonymity for relational attributes as well as for sensitive terms. As rx-anon is a framework approach, it can be reused and extended by other anonymization algorithms, privacy models, and textual similarity metrics.",0
"This paper proposes a new approach to de-identify heterogenous data using a modified Mondrian algorithm. The proposed method overcomes some limitations of previous approaches by introducing additional measures of data privacy. Specifically, the authors propose modifications to ensure the protection of sensitive attributes such as age, gender, race, etc. These modifications are designed to minimize potential breaches of patient confidentiality while still preserving important medical knowledge. In addition, the authors evaluate the effectiveness of their proposed method through simulations using real world data sets, demonstrating that it effectively balances privacy concerns with utility. Overall, the findings suggest that the RX-Anon model could provide a valuable tool for researchers seeking to share healthcare data without compromising individual patient privacy.",1
"We propose an approach to learning with graph-structured data in the problem domain of graph classification. In particular, we present a novel type of readout operation to aggregate node features into a graph-level representation. To this end, we leverage persistent homology computed via a real-valued, learnable, filter function. We establish the theoretical foundation for differentiating through the persistent homology computation. Empirically, we show that this type of readout operation compares favorably to previous techniques, especially when the graph connectivity structure is informative for the learning problem.",0
"Graph filtration learning (GFL) is a new machine learning technique that utilizes graph neural networks (GNNs) along with novel filtering approaches inspired by classical image processing techniques such as bilateral and trilinear filtering. GFL allows us to capture high-resolution features from large graphs while reducing computational complexity. This method demonstrates state-of-the-art performance on several benchmark datasets across various domains, including computer vision, natural language processing, and bioinformatics. In this paper, we present the details behind this innovative approach and explore its potential applications. We provide theoretical analysis and extensive experimental evaluations, which showcase the efficacy of our proposed method compared to existing graph convolution baselines and other related methods. Overall, graph filtration learning represents a significant step forward in applying deep learning models to large scale graph data.",1
"We study the multiple manifold problem, a binary classification task modeled on applications in machine vision, in which a deep fully-connected neural network is trained to separate two low-dimensional submanifolds of the unit sphere. We provide an analysis of the one-dimensional case, proving for a simple manifold configuration that when the network depth $L$ is large relative to certain geometric and statistical properties of the data, the network width $n$ grows as a sufficiently large polynomial in $L$, and the number of i.i.d. samples from the manifolds is polynomial in $L$, randomly-initialized gradient descent rapidly learns to classify the two manifolds perfectly with high probability. Our analysis demonstrates concrete benefits of depth and width in the context of a practically-motivated model problem: the depth acts as a fitting resource, with larger depths corresponding to smoother networks that can more readily separate the class manifolds, and the width acts as a statistical resource, enabling concentration of the randomly-initialized network and its gradients. The argument centers around the neural tangent kernel and its role in the nonasymptotic analysis of training overparameterized neural networks; to this literature, we contribute essentially optimal rates of concentration for the neural tangent kernel of deep fully-connected networks, requiring width $n \gtrsim L\,\mathrm{poly}(d_0)$ to achieve uniform concentration of the initial kernel over a $d_0$-dimensional submanifold of the unit sphere $\mathbb{S}^{n_0-1}$, and a nonasymptotic framework for establishing generalization of networks trained in the NTK regime with structured data. The proof makes heavy use of martingale concentration to optimally treat statistical dependencies across layers of the initial random network. This approach should be of use in establishing similar results for other network architectures.",0
"Artificial neural networks have shown impressive results across a range of domains, from image recognition to natural language processing. However, these models often face challenges related to understanding their inner workings and ensuring safe deployment, which can limit widespread adoption. This paper introduces Deep Networks and the Multiple Manifold Problem (DNaMMP), a new model that addresses these issues by combining elements of deep learning and traditional machine learning approaches. DNaMMP enables improved transparency and interpretability through novel activation functions and attention mechanisms while retaining strong predictive performance on complex tasks. Our experimental evaluations demonstrate DNaMMP outperforms state-of-the-art methods in key metrics such as accuracy, robustness, and explainability. These findings highlight the promise of our approach for real-world applications where safety and trustworthiness are critical requirements. Overall, we believe that our work represents an important step forward in advancing the field of artificial intelligence and broadening the impact of deep learning technologies.",1
"In this paper, we propose a two-stage deep learning framework called VoxelContext-Net for both static and dynamic point cloud compression. Taking advantages of both octree based methods and voxel based schemes, our approach employs the voxel context to compress the octree structured data. Specifically, we first extract the local voxel representation that encodes the spatial neighbouring context information for each node in the constructed octree. Then, in the entropy coding stage, we propose a voxel context based deep entropy model to compress the symbols of non-leaf nodes in a lossless way. Furthermore, for dynamic point cloud compression, we additionally introduce the local voxel representations from the temporal neighbouring point clouds to exploit temporal dependency. More importantly, to alleviate the distortion from the octree construction procedure, we propose a voxel context based 3D coordinate refinement method to produce more accurate reconstructed point cloud at the decoder side, which is applicable to both static and dynamic point cloud compression. The comprehensive experiments on both static and dynamic point cloud benchmark datasets(e.g., ScanNet and Semantic KITTI) clearly demonstrate the effectiveness of our newly proposed method VoxelContext-Net for 3D point cloud geometry compression.",0
"In recent years, point cloud data has become increasingly prevalent due to advancements in remote sensing technology such as LiDAR and RGB-D cameras. However, processing and transmission of large amounts of unstructured point cloud data can pose significant challenges in terms of memory usage, computational efficiency, and storage costs. To address these issues, we propose VoxelContext-Net: an octree-based framework for efficient point cloud compression that leverages deep learning techniques.  Our approach uses 2D/3D convolutional neural networks (CNNs) embedded within a hierarchical octree structure to compress the dense point clouds while retaining their inherent geometric and semantic features. We introduce two novel variants of voxels, namely, VoxelContext and DenseVoxelContext, which further enhance the performance of our model by enabling a more effective spatial context aggregation mechanism. Our experiments demonstrate that VoxelContext-Net achieves state-of-the-art results on benchmark datasets across various metrics, including accuracy, mean squared error (MSE), structural similarity index (SSIM), peak signal-to-noise ratio (PSNR), and feature preservation measures like Chamfer distance and Hausdorff distance. Additionally, our method outperforms existing approaches by up to 6% in MSE and PSNR while reducing the file size by at least 50%.  Furthermore, VoxelContext-Net can flexibly handle large-scale real-world point cloud data from diverse domains without retraining through its robust octree hierarchy design. By balancing both lossy and lossless compression ratios, our solution enables fine-grained tradeoffs between compression rate and reconstruction quality, making it suitable for numerous applications such as autonomous driving, robotics, computer vision, virtual reality, augmented reality, and gaming industries where massive point clouds must be processed efficiently.  In summary, VoxelContext-Net is a powerful tool for effectively handling large-scale unstructured point cloud data in a variety of domai",1
"Graph neural networks (GNNs) have achieved outstanding performance in learning graph-structured data and various tasks. However, many current GNNs suffer from three common problems when facing large-size graphs or using a deeper structure: neighbors explosion, node dependence, and oversmoothing. Such problems attribute to the data structures of the graph itself or the designing of the multi-layers GNNs framework, and can lead to low training efficiency and high space complexity. To deal with these problems, in this paper, we propose a general subgraph-based training framework, namely Ripple Walk Training (RWT), for deep and large graph neural networks. RWT samples subgraphs from the full graph to constitute a mini-batch, and the full GNN is updated based on the mini-batch gradient. We analyze the high-quality subgraphs to train GNNs in a theoretical way. A novel sampling method Ripple Walk Sampler works for sampling these high-quality subgraphs to constitute the mini-batch, which considers both the randomness and connectivity of the graph-structured data. Extensive experiments on different sizes of graphs demonstrate the effectiveness and efficiency of RWT in training various GNNs (GCN & GAT).",0
"Here's a draft abstract for your paper ""Ripple Walk Training: A Subgraph-based training framework for Large and Deep Graph Neural Network"":  Training graph neural networks (GNNs) on large datasets can pose significant challenges due to their memory requirements and computational complexity. In recent years, several techniques have been proposed to address these issues, including mini-batch gradient descent, stochastic gradient descent, and subgraph sampling methods likeGraph Convolutional Network Sampling (GCNS). However, these methods suffer from limitations such as high variance, slow convergence rate, and suboptimal accuracy compared to full batch optimization.  To overcome these drawbacks, we propose a novel training framework called Ripple Walk Training (RTW), which is based on a subgraph-based method that effectively samples nodes and edges from the original graph during each iteration of the model updates. Specifically, we employ a Markov Chain Monte Carlo (MCMC)-like approach where each node moves one step at a time to neighboring nodes that satisfy certain criteria. By doing so, RTW enables efficient and parallelizable computation of GNN models while preserving the global structure of the graph.  We evaluate our proposed method on a range of tasks using four commonly used benchmark datasets for both undirected and directed graphs, demonstrating that our approach achieves significantly higher performance than state-of-the-art baseline models across all metrics. Our results showcase the effectiveness of RWT in scaling up GNNs for large, deep architectures while maintaining superior test set accuracy.  Overall, our work provides a new perspective on how to train complex GNN architectures efficiently without sacrificing model quality. We believe that our method has promising applications in areas such as social network analysis, computer vision, natural language processing, and recommendation systems.",1
"Graph neural networks (GNNs) have emerged as the standard method for numerous tasks on graph-structured data such as node classification. However, real-world graphs are often evolving over time and even new classes may arise. We model these challenges as an instance of lifelong learning, in which a learner faces a sequence of tasks and may take over knowledge acquired in past tasks. Such knowledge may be stored explicitly as historic data or implicitly within model parameters. In this work, we systematically analyze the influence of implicit and explicit knowledge. Therefore, we present an incremental training method for lifelong learning on graphs and introduce a new measure based on $k$-neighborhood time differences to address variances in the historic data. We apply our training method to five representative GNN architectures and evaluate them on three new lifelong node classification datasets. Our results show that no more than 50% of the GNN's receptive field is necessary to retain at least 95% accuracy compared to training over the complete history of the graph data. Furthermore, our experiments confirm that implicit knowledge becomes more important when fewer explicit knowledge is available.",0
"This paper presents a method for lifelong learning of graph neural networks (GNNs) for open-world node classification. GNNs have been shown to perform well on tasks involving graph data structures such as social network analysis, natural language processing, and computer vision. However, traditional GNN methods assume a static set of nodes at training time, which limits their ability to generalize to new unseen types of graphs. To address this limitation, we propose a framework that enables our model to continuously learn from novel graphs encountered during deployment, without retraining on all existing graphs. Our approach integrates meta learning techniques to adapt the base model parameters through self-supervised optimization using task scheduling. We demonstrate significant improvements over state-of-the-art baselines across multiple benchmark datasets, showing the effectiveness of our proposed method for open-world node classification problems where graph structure varies widely.",1
"\textit{Graph Neural Network} (GNN) is a promising approach for analyzing graph-structured data that tactfully captures their dependency information via node-level message passing. It has achieved state-of-the-art performances in many tasks, such as node classification, graph matching, clustering, and graph generation. As GNNs operate on non-Euclidean data, their irregular data access patterns cause considerable computational costs and overhead on conventional architectures, such as GPU and CPU. Our analysis shows that GNN adopts a hybrid computing model. The \textit{Aggregation} (or \textit{Message Passing}) phase performs vector additions where vectors are fetched with irregular strides. The \textit{Transformation} (or \textit{Node Embedding}) phase can be either dense or sparse-dense matrix multiplication. In this work, We propose \textit{VersaGNN}, an ultra-efficient, systolic-array-based versatile hardware accelerator that unifies dense and sparse matrix multiplication. By applying this single optimized systolic array to both aggregation and transformation phases, we have significantly reduced chip sizes and energy consumption. We then divide the computing engine into blocked systolic arrays to support the \textit{Strassen}'s algorithm for dense matrix multiplication, dramatically scaling down the number of multiplications and enabling high-throughput computation of GNNs. To balance the workload of sparse-dense matrix multiplication, we also introduced a greedy algorithm to combine sparse sub-matrices of compressed format into condensed ones to reduce computational cycles. Compared with current state-of-the-art GNN software frameworks, \textit{VersaGNN} achieves on average 3712$\times$ speedup with 1301.25$\times$ energy reduction on CPU, and 35.4$\times$ speedup with 17.66$\times$ energy reduction on GPU.",0
"This paper presents VersaGNN, a novel accelerator designed to efficiently execute graph neural network (GNN) workloads. GNNs have recently gained significant attention due to their success in areas such as node classification, link prediction, and community detection. However, training and inference of GNN models can be computationally expensive on traditional hardware architectures. To address these challenges, we propose a versatile accelerator that leverages both data parallelism and model parallelism to improve performance and energy efficiency. Our evaluations show that VersaGNN outperforms state-of-the-art solutions across a wide range of datasets and GNN models, while maintaining competitive energy consumption. Overall, our work highlights the potential of specialized accelerators for enabling efficient deep learning on graphs.",1
"Graph neural networks are a popular variant of neural networks that work with graph-structured data. In this work, we consider combining graph neural networks with the energy-based view of Grathwohl et al. (2019) with the aim of obtaining a more robust classifier. We successfully implement this framework by proposing a novel method to ensure generation over features as well as the adjacency matrix and evaluate our method against the standard graph convolutional network (GCN) architecture (Kipf & Welling (2016)). Our approach obtains comparable discriminative performance while improving robustness, opening promising new directions for future research for energy-based graph neural networks.",0
"Title: ""Energy-based Perspective on Graph Neural Networks""  Graph neural networks (GNNs) have emerged as powerful tools for processing graph data, allowing efficient learning from complex relationships among nodes. In recent years, GNNs have shown state-of-the-art performance across numerous domains such as social network analysis, natural language processing, computer vision, and bioinformatics. Despite their successes, understanding how GNNs work remains challenging due to their nonlinear nature and the interaction between different layers. This paper presents an energy-based view of GNNs that unifies several key perspectives of these models under one framework. By casting GNNs as systems that minimize a self-consistent energy function, we can provide insights into their behavior, improve our ability to design new architectures, and develop more effective training methods. We demonstrate the utility of our approach by analyzing several popular GNN variants and showing how they fit within our energy framework. Our findings highlight both the strengths and weaknesses of existing approaches, paving the way for future advancements in GNN research. Overall, this study offers a valuable contribution to the field of machine learning, providing new theoretical grounding for the development of cutting-edge techniques in graph data analysis.",1
"Network-structured data becomes ubiquitous in daily life and is growing at a rapid pace. It presents great challenges to feature engineering due to the high non-linearity and sparsity of the data. The local and global structure of the real-world networks can be reflected by dynamical transfer behaviors among nodes. This paper proposes a network embedding framework to capture the transfer behaviors on structured networks via deep prediction models. We first design a degree-weight biased random walk model to capture the transfer behaviors on the network. Then a deep network embedding method is introduced to preserve the transfer possibilities among the nodes. A network structure embedding layer is added into conventional deep prediction models, including Long Short-Term Memory Network and Recurrent Neural Network, to utilize the sequence prediction ability. To keep the local network neighborhood, we further perform a Laplacian supervised space optimization on the embedding feature representations. Experimental studies are conducted on various datasets including social networks, citation networks, biomedical network, collaboration network and language network. The results show that the learned representations can be effectively used as features in a variety of tasks, such as clustering, visualization, classification, reconstruction and link prediction, and achieve promising performance compared with state-of-the-arts.",0
"Here is my attempt:  Title: ""Network Embedding via Deep Prediction Model""  This paper presents a new approach to network embedding that leverages deep learning techniques to capture complex relationships among nodes in large networks. Traditional methods for network analysis have relied on shallow models such as PCA or SVD, which can struggle to accurately represent high-dimensional data or nonlinear patterns. In contrast, our proposed method uses a convolutional neural network (CNN) architecture to predict missing edges or node attributes based on embeddings learned from observed network features. Our model has been shown to achieve state-of-the-art results across multiple benchmark datasets while requiring minimal tuning or preprocessing steps. We believe this work represents a significant step forward in advancing the field of network science by providing researchers with more powerful tools for analyzing networked systems.",1
"Efficient video processing is a critical component in many IoMT applications to detect events of interest. Presently, many window optimization techniques have been proposed in event processing with an underlying assumption that the incoming stream has a structured data model. Videos are highly complex due to the lack of any underlying structured data model. Video stream sources such as CCTV cameras and smartphones are resource-constrained edge nodes. At the same time, video content extraction is expensive and requires computationally intensive Deep Neural Network (DNN) models that are primarily deployed at high-end (or cloud) nodes. This paper presents VID-WIN, an adaptive 2-stage allied windowing approach to accelerate video event analytics in an edge-cloud paradigm. VID-WIN runs parallelly across edge and cloud nodes and performs the query and resource-aware optimization for state-based complex event matching. VID-WIN exploits the video content and DNN input knobs to accelerate the video inference process across nodes. The paper proposes a novel content-driven micro-batch resizing, queryaware caching and micro-batch based utility filtering strategy of video frames under resource-constrained edge nodes to improve the overall system throughput, latency, and network usage. Extensive evaluations are performed over five real-world datasets. The experimental results show that VID-WIN video event matching achieves ~2.3X higher throughput with minimal latency and ~99% bandwidth reduction compared to other baselines while maintaining query-level accuracy and resource bounds.",0
"In this work, we present VID-WIN, a method for fast video event matching that utilizes query-aware windowing at the edge in order to facilitate efficient processing in the internet of multimedia things (IoMT). Our approach leverages advances in hardware acceleration and neural network pruning techniques to achieve real-time performance while minimizing computational overhead. By tailoring our algorithm to specific application domains and user queries, we demonstrate significant improvements over state-of-the-art methods in terms of both accuracy and speed. Furthermore, we showcase the versatility of our framework by evaluating its effectiveness on two distinct use cases, namely smart surveillance systems and health monitoring scenarios. Ultimately, VID-WIN provides a scalable solution for enabling robust event detection and retrieval capabilities across diverse IoMT settings, paving the way towards next-generation multimedia services. Keywords: video event match",1
"Graph neural networks (GNNs), an emerging deep learning model class, can extract meaningful representations from highly expressive graph-structured data and are therefore gaining popularity for wider ranges of applications. However, current GNNs suffer from the poor performance of their sparse-dense matrix multiplication (SpMM) operator, even when using powerful GPUs. Our analysis shows that 95% of the inference time could be spent on SpMM when running popular GNN models on NVIDIA's advanced V100 GPU. Such SpMM performance bottleneck hinders GNNs' applicability to large-scale problems or the development of more sophisticated GNN models. To address this inference time bottleneck, we introduce ES-SpMM, a cache-first edge sampling mechanism and codesigned SpMM kernel. ES-SpMM uses edge sampling to downsize the graph to fit into GPU's shared memory. It thus reduces the computation cost and improves SpMM's cache locality. To evaluate ES-SpMM's performance, we integrated it with a popular GNN framework, DGL, and tested it using representative GNN models and datasets. Our results show that ES-SpMM outperforms the highly optimized cuSPARSE SpMM kernel by up to 4.35x with no accuracy loss and by 45.3x with less than a 1% accuracy loss.",0
"In recent years, graph neural networks (GNNs) have emerged as powerful models for analyzing large-scale graphs, particularly those containing heterogeneous data types such as social network data. One crucial component of many GNN algorithms is the Sparse Matrix Vector Multiplication (SpMV) kernel, which calculates the dot product between a sparse matrix and a dense vector. However, due to the high computational cost of this operation, it becomes difficult to scale these methods to larger datasets. To address this issue, we propose a novel approach based on cache-first edge sampling that accelerates the SpMV kernel computation in GNN training while maintaining accuracy. Our method samples edges from the original graph using a simple probability distribution, constructing a subgraph whose size can effectively control runtime complexity and memory usage. We demonstrate through extensive experiments that our approach significantly reduces the time required for GNN training without sacrificing model performance. This work contributes valuable insights into efficient algorithm design for scaling up complex computations over large datasets, enabling more effective processing of richly structured data across diverse domains.",1
"Tensor networks are a powerful modeling framework developed for computational many-body physics, which have only recently been applied within machine learning. In this work we utilize a uniform matrix product state (u-MPS) model for probabilistic modeling of sequence data. We first show that u-MPS enable sequence-level parallelism, with length-n sequences able to be evaluated in depth O(log n). We then introduce a novel generative algorithm giving trained u-MPS the ability to efficiently sample from a wide variety of conditional distributions, each one defined by a regular expression. Special cases of this algorithm correspond to autoregressive and fill-in-the-blank sampling, but more complex regular expressions permit the generation of richly structured data in a manner that has no direct analogue in neural generative models. Experiments on sequence modeling with synthetic and real text data show u-MPS outperforming a variety of baselines and effectively generalizing their predictions in the presence of limited data.",0
"Tensor networks have recently gained popularity as a powerful tool for modeling complex probability distributions over high-dimensional spaces. In particular, tensor network methods such as tensor trains and hierarchical matrices have been shown to provide highly efficient representations of multivariate densities, enabling fast likelihood evaluations and posterior inference in latent variable models. These developments have significant potential impact across many domains, including natural language processing, computer vision, and robotics, where probabilistic sequence models play an important role. This work reviews some recent advances in using tensor networks for probabilistic sequence modeling, highlighting both their promise and challenges. We discuss several approaches based on different types of tensor network decompositions, comparing them against traditional Markov Chain Monte Carlo (MCMC) techniques. Our experiments show that tensor network methods can often yield more accurate approximations of target distributions while requiring fewer samples or computational resources than standard MCMC algorithms. Overall, we conclude that tensor networks offer a promising direction for building flexible, scalable, and performant probabilistic models for real-world applications.",1
"Bayesian optimization (BO) is a popular paradigm for global optimization of expensive black-box functions, but there are many domains where the function is not completely black-box. The data may have some known structure, e.g. symmetries, and the data generation process can yield useful intermediate or auxiliary information in addition to the value of the optimization objective. However, surrogate models traditionally employed in BO, such as Gaussian Processes (GPs), scale poorly with dataset size and struggle to incorporate known structure or auxiliary information. Instead, we propose performing BO on complex, structured problems by using Bayesian Neural Networks (BNNs), a class of scalable surrogate models that have the representation power and flexibility to handle structured data and exploit auxiliary information. We demonstrate BO on a number of realistic problems in physics and chemistry, including topology optimization of photonic crystal materials using convolutional neural networks, and chemical property optimization of molecules using graph neural networks. On these complex tasks, we show that BNNs often outperform GPs as surrogate models for BO in terms of both sampling efficiency and computational cost.",0
"Title: ""Scalable and Flexible Deep Bayesian Optimization with Auxiliary Information for Scientific Problems""  Abstract: This paper presents a novel approach to deep Bayesian optimization that leverages auxiliary information to improve scalability and flexibility in scientific problem solving. The proposed method combines efficient global sampling techniques with a state-of-the-art local search algorithm to balance exploration and exploitation effectively. By integrating prior knowledge into the modeling process through flexible auxiliary variables, our framework can learn more quickly and adapt to changing environments, leading to better solutions across diverse domains. Our experiments demonstrate significant improvements over existing methods on both high-dimensional benchmark problems and real-world applications in computational physics and engineering.  Keywords: bayesian optimization, deep learning, auxiliary information, scientific computing  ------------------------------  Introduction: This paper addresses the challenges faced by traditional Bayesian optimization (BO) algorithms when dealing with complex and computationally expensive functions. To address these limitations, we propose a new approach called deep BO with auxiliary variables (AuxBO). We introduce two main contributions. Firstly, we adopt an effective integration of Gaussian processes (GPs) with machine learning models, which allows us to capture different levels of complexity and scale smoothly from low to high dimensions while maintaining accuracy. Secondly, we incorporate auxiliary information directly into the GP model as additional latent variables, which enables flexible learning from prior knowledge without sacrificing generalizability or robustness. These advancements make AuxBO particularly suited to tackling large-scale optimization problems involving black box objective functions that require many function evaluations. Our approach outperforms standard BO methods and other related state-ofthe- art alternatives on several test cases, demonstrating clear benefits in terms o",1
"The mean shift (MS) algorithm is a nonparametric method used to cluster sample points and find the local modes of kernel density estimates, using an idea based on iterative gradient ascent. In this paper we develop a mean-shift-inspired algorithm to estimate the modes of regression functions and partition the sample points in the input space. We prove convergence of the sequences generated by the algorithm and derive the non-asymptotic rates of convergence of the estimated local modes for the underlying regression model. We also demonstrate the utility of the algorithm for data-enabled discovery through an application on biomolecular structure data. An extension to subspace constrained mean shift (SCMS) algorithm used to extract ridges of regression functions is briefly discussed.",0
"This algorithm uses mean shift analysis (MSA) of feature vectors in a kD tree data structure; it allows us to efficiently find cluster centers which can then be used as model coefficients through MSA regression. We use a modification of the KML-Tree search method from Fukui et al.'s ""KML Tree: An Efficient Nearest Neighbor Searching Method Based on Hierarchical Clustering"" (2009), which we call a hierarchal partitioned nearest neighbor tree (HPNNT). By using these clusters as starting points for our linear least squares problem we achieve faster convergence rates than previous approaches and better generalization performance in terms of R^2 and root mean squared error (RMSE) values on both synthetic and real datasets. Overall, our approach improves upon existing methods by providing an efficient means of locating appropriate initial conditions for iteratively reweighted least squares that allows for improved model selection over other space partitioning techniques like random subspace sampling.",1
"Recently, there has been great success in applying deep neural networks on graph structured data. Most work, however, focuses on either node- or graph-level supervised learning, such as node, link or graph classification or node-level unsupervised learning (e.g. node clustering). Despite its wide range of possible applications, graph-level unsupervised learning has not received much attention yet. This might be mainly attributed to the high representation complexity of graphs, which can be represented by n! equivalent adjacency matrices, where n is the number of nodes. In this work we address this issue by proposing a permutation-invariant variational autoencoder for graph structured data. Our proposed model indirectly learns to match the node ordering of input and output graph, without imposing a particular node ordering or performing expensive graph matching. We demonstrate the effectiveness of our proposed model on various graph reconstruction and generation tasks and evaluate the expressive power of extracted representations for downstream graph-level classification and regression.",0
"In this paper we propose a new model architecture that can learn graph level representations while preserving permutation equivariance. Our method leverages recent advances in graph neural networks (GNNs) and variational autoencoders (VAEs). We modify VAEs to incorporate GNN architectures as message passing layers which enables us to capture structured information of graphs such as node connections, edge weights etc., while training on data that may have different topological properties. This approach results in learning latent spaces where graphs can be meaningfully reconstructed from latent codes while permitting permutations to map one another, i.e. our approach learns graph level representation of nodes that capture their positions within clusters. To evaluate our proposed framework we use standard benchmark datasets, namely, MNIST digit dataset, RedDigits, and a real world citation network dataset DBLP, using two common metrics: clustering coefficient and normalized mutual information. Results show that our proposed model achieves competitive performance on all three benchmarks across these evaluation criteria indicating that it produces good quality representations that enable downstream applications. Finally, we conclude by discussing some limitations of our study, possible extensions, and future directions for research in this area.",1
"Graph neural networks (GNNs) have been successfully applied in many structured data domains, with applications ranging from molecular property prediction to the analysis of social networks. Motivated by the broad applicability of GNNs, we propose the family of so-called RankGNNs, a combination of neural Learning to Rank (LtR) methods and GNNs. RankGNNs are trained with a set of pair-wise preferences between graphs, suggesting that one of them is preferred over the other. One practical application of this problem is drug screening, where an expert wants to find the most promising molecules in a large collection of drug candidates. We empirically demonstrate that our proposed pair-wise RankGNN approach either significantly outperforms or at least matches the ranking performance of the naive point-wise baseline approach, in which the LtR problem is solved via GNN-based graph regression.",0
"This research paper presents a novel approach using graph neural networks (GNN) to rank structured objects based on their properties. Inspired by recent advancements in deep learning techniques that exploit nonlinear relationships, we design a GNN architecture specifically tailored towards ranking tasks. Our model leverages graph convolutional layers to propagate node features and capture complex interactions among neighboring nodes within each object. Moreover, we introduce two new mechanisms: edge attention and differentiable pooling, which dynamically weight feature messages from different edges and aggregate high-level representations across all network components respectively. We evaluate our method on multiple benchmark datasets spanning diverse domains such as protein structure prediction, molecular interaction prediction, image and text classification, and quantum chemistry simulation. Experimental results demonstrate that our approach outperforms traditional methods that utilize handcrafted features and other state-of-the-art GNN models. Furthermore, ablation studies confirm that both proposed mechanisms contribute significantly towards improving performance. Therefore, our work offers promising insights into the development of advanced graph reasoning frameworks suited for realworld applications.",1
"Graph neural networks (GNN) have been proven to be mature enough for handling graph-structured data on node-level graph representation learning tasks. However, the graph pooling technique for learning expressive graph-level representation is critical yet still challenging. Existing pooling methods either struggle to capture the local substructure or fail to effectively utilize high-order dependency, thus diminishing the expression capability. In this paper we propose HAP, a hierarchical graph-level representation learning framework, which is adaptively sensitive to graph structures, i.e., HAP clusters local substructures incorporating with high-order dependencies. HAP utilizes a novel cross-level attention mechanism MOA to naturally focus more on close neighborhood while effectively capture higher-order dependency that may contain crucial information. It also learns a global graph content GCont that extracts the graph pattern properties to make the pre- and post-coarsening graph content maintain stable, thus providing global guidance in graph coarsening. This novel innovation also facilitates generalization across graphs with the same form of features. Extensive experiments on fourteen datasets show that HAP significantly outperforms twelve popular graph pooling methods on graph classification task with an maximum accuracy improvement of 22.79%, and exceeds the performance of state-of-the-art graph matching and graph similarity learning algorithms by over 3.5% and 16.7%.",0
"In recent years, graph representation learning has become increasingly important in many fields, including computer vision, natural language processing, and social network analysis. One common challenge faced in these areas is the difficulty of capturing high-order dependencies within graphs, which can lead to limited performance in downstream tasks. To address this issue, we propose a new method called Hierarchical Adaptive Pooling by Capturing High-order Dependency (HAPCHD). Our approach leverages a hierarchical architecture that adaptively pools node features at different levels of granularity while preserving high-order dependency structures. We demonstrate through extensive experiments on several benchmark datasets that HAPCHD significantly outperforms state-of-the-art methods across a variety of evaluation metrics. Furthermore, our ablation studies confirm the effectiveness of each component in the proposed framework. Overall, our work represents an important contribution to the field of graph representation learning and highlights the potential of HAPCHD as a powerful tool for addressing complex dependency structure challenges.",1
"In recent years, graph neural networks (GNNs) have been widely adopted in the representation learning of graph-structured data and provided state-of-the-art performance in various applications such as link prediction, node classification, and recommendation. Motivated by recent advances of self-supervision for representation learning in natural language processing and computer vision, self-supervised learning has been recently studied to leverage unlabeled graph-structured data. However, employing self-supervision tasks as auxiliary tasks to assist a primary task has been less explored in the literature on graphs. In this paper, we propose a novel self-supervised auxiliary learning framework to effectively learn graph neural networks. Moreover, this work is the first study showing that a meta-path prediction is beneficial as a self-supervised auxiliary task for heterogeneous graphs. Our method is learning to learn a primary task with various auxiliary tasks to improve generalization performance. The proposed method identifies an effective combination of auxiliary tasks and automatically balances them to improve the primary task. Our methods can be applied to any graph neural network in a plug-in manner without manual labeling or additional data. Also, it can be extended to any other auxiliary tasks. Our experiments demonstrate that the proposed method consistently improves the performance of node classification and link prediction.",0
"In recent years, graph neural networks (GNNs) have emerged as powerful models for learning on graphs. However, they often require large amounts of labeled data, which can be difficult or expensive to obtain. To address this problem, we propose a self-supervised auxiliary learning approach that uses meta-learning to train GNNs using unlabeled data. Our method involves training two subnetworks simultaneously: a main network that learns the desired task using supervision, and an auxiliary network that learns a contrastive objective using unlabeled data. We show through extensive experiments on several benchmark datasets that our method significantly outperforms baseline approaches that use only supervised or semi-supervised learning. Additionally, we demonstrate the effectiveness of our approach on real-world social and biological applications where label scarcity is a major challenge. Overall, our work represents an important step towards making GNNs more scalable and accessible for real-world tasks.",1
"Graph Neural Networks (GNNs) have proved to be an effective representation learning framework for graph-structured data, and have achieved state-of-the-art performance on many practical predictive tasks, such as node classification, link prediction and graph classification. Among the variants of GNNs, Graph Attention Networks (GATs) learn to assign dense attention coefficients over all neighbors of a node for feature aggregation, and improve the performance of many graph learning tasks. However, real-world graphs are often very large and noisy, and GATs are prone to overfitting if not regularized properly. Even worse, the local aggregation mechanism of GATs may fail on disassortative graphs, where nodes within local neighborhood provide more noise than useful information for feature aggregation. In this paper, we propose Sparse Graph Attention Networks (SGATs) that learn sparse attention coefficients under an $L_0$-norm regularization, and the learned sparse attentions are then used for all GNN layers, resulting in an edge-sparsified graph. By doing so, we can identify noisy/task-irrelevant edges, and thus perform feature aggregation on most informative neighbors. Extensive experiments on synthetic and real-world graph learning benchmarks demonstrate the superior performance of SGATs. In particular, SGATs can remove about 50\%-80\% edges from large assortative graphs, while retaining similar classification accuracies. On disassortative graphs, SGATs prune majority of noisy edges and outperform GATs in classification accuracies by significant margins. Furthermore, the removed edges can be interpreted intuitively and quantitatively. To the best of our knowledge, this is the first graph learning algorithm that shows significant redundancies in graphs and edge-sparsified graphs can achieve similar or sometimes higher predictive performances than original graphs.",0
"Deep learning has revolutionized computer vision by enabling powerful models that can learn from large amounts of data without explicit programming. Recent research has shown promising results using convolutional neural networks (CNN) pretrained on ImageNet for fine-grained tasks such as image classification, object detection, and semantic segmentation. However, these models have two main limitations: they suffer from the ""black box"" problem because their internal workings are difficult to interpret, and they require vast computational resources during both training and deployment. In this paper, we present sparse graph attention networks (SGN), a novel deep learning architecture designed to address these issues while maintaining high accuracy. Our approach uses classical techniques from graph theory and linear algebra to build interpretable connections among features extracted at different stages of processing. We employ a spatial transformer network (STN) to generate affinity maps based on feature similarity, which serve as soft weights for bilinear pooling. This ensures local connectivity and allows our model to capture global context in a computationally efficient manner. Experiments conducted on benchmark datasets show that SGNs achieve state-of-the-art performance on several challenging vision problems while requiring fewer parameters and less memory than competing methods. Overall, our proposed architecture represents a significant step towards transparent and scalable artificial intelligence, opening up new opportunities for scientific inquiry into how biological systems process sensory input.",1
"Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.",0
"Abstract Graph neural networks have gained significant attention due to their ability to model complex nonlinear patterns underlying graphs, which arise naturally from diverse real-world applications such as social media analysis, biological network analysis, recommendation systems, image classification, natural language processing, and more. This review article provides a comprehensive overview of state-of-the-art methods developed under the umbrella term graph neural networks (GNN). We focus on summarizing key achievements and developments made by researchers within the last few years. Our aim is to provide readers with a concise yet complete overview of popular GNN architectures, algorithms, and applications across different domains. Specifically, we cover message passing, attention mechanism, pooling strategies, node embedding techniques, and edge-conditioned learning. We further discuss critical factors that enable or hinder performance improvement using GNN models while emphasizing potential future directions worth exploring in this rapidly evolving area. Overall, our literature survey reveals current trends, methodologies, challenges faced during deployment, limitations associated with existing approaches, as well as promising solutions applicable to numerous problem types encompassed in graph data mining. Keywords: Graph neural networks, GNN architecture, Message Passing, Attention Mechanism, Pooling Strategies, Node Embedding Techniques, Edge-Conditioned Learning. Note: If you want me to write without these instructions I can, but please clarify your requirements first before asking questions later regarding them so there won't be any confusion.",1
"Deep learning has recently demonstrated its promising performance for vision-based parking-slot detection. However, very few existing methods explicitly take into account learning the link information of the marking-points, resulting in complex post-processing and erroneous detection. In this paper, we propose an attentional graph neural network based parking-slot detection method, which refers the marking-points in an around-view image as graph-structured data and utilize graph neural network to aggregate the neighboring information between marking-points. Without any manually designed post-processing, the proposed method is end-to-end trainable. Extensive experiments have been conducted on public benchmark dataset, where the proposed method achieves state-of-the-art accuracy. Code is publicly available at \url{https://github.com/Jiaolong/gcn-parking-slot}.",0
"This research paper presents a novel approach to parking slot detection using attentional graph neural networks (AGNN). Parking slot detection is a crucial task for autonomous driving systems as it helps vehicles locate suitable parking spots quickly and accurately. Existing methods rely heavily on manually defined features which can lead to low accuracy and high computational cost. AGNN addresses these issues by leveraging attention mechanisms within graph neural networks, allowing the model to learn effective representations directly from raw sensor data without handcrafted features. Our experimental results demonstrate that AGNN significantly outperforms state-of-the-art parking spot detectors while requiring fewer computations. Moreover, we show that our method generalizes well across diverse weather conditions, lighting environments, and parking settings, making it highly applicable in real-world scenarios. Overall, this study offers important contributions towards enabling safer and more efficient transportation systems through advanced computer vision technologies.",1
"Generalizable, transferrable, and robust representation learning on graph-structured data remains a challenge for current graph neural networks (GNNs). Unlike what has been developed for convolutional neural networks (CNNs) for image data, self-supervised learning and pre-training are less explored for GNNs. In this paper, we propose a graph contrastive learning (GraphCL) framework for learning unsupervised representations of graph data. We first design four types of graph augmentations to incorporate various priors. We then systematically study the impact of various combinations of graph augmentations on multiple datasets, in four different settings: semi-supervised, unsupervised, and transfer learning as well as adversarial attacks. The results show that, even without tuning augmentation extents nor using sophisticated GNN architectures, our GraphCL framework can produce graph representations of similar or better generalizability, transferrability, and robustness compared to state-of-the-art methods. We also investigate the impact of parameterized graph augmentation extents and patterns, and observe further performance gains in preliminary experiments. Our codes are available at https://github.com/Shen-Lab/GraphCL.",0
"Recent advances in graph contrastive learning have achieved impressive performance on node classification tasks across diverse domains such as biology, chemistry, and computer science. However, most existing methods assume that input graphs are deterministic and noise-free, which may limit their effectiveness on real-world datasets where data can exhibit high variability due to measurement errors, missing values, or other sources of uncertainty. In this work, we propose a novel augmentation framework that generates multiple stochastic variants of each input graph by applying random transformations based on statistical properties of the underlying dataset. We show that these augmented versions significantly boost the performance of state-of-the-art GCL algorithms while reducing overfitting risks due to memorization effects associated with batch normalization layers. Our experiments demonstrate the competitive empirical results of our approach across several benchmarks from different fields, confirming its generalizability and suitability for large-scale applications in various scientific disciplines. Overall, this research provides insights into how advanced machine learning techniques could benefit from incorporating more systematic strategies designed to account for the presence of noisy inputs, potentially leading to new breakthroughs in knowledge discovery using heterogeneous big data collections beyond traditional image recognition scenarios.",1
"Generative modeling of set-structured data, such as point clouds, requires reasoning over local and global structures at various scales. However, adopting multi-scale frameworks for ordinary sequential data to a set-structured data is nontrivial as it should be invariant to the permutation of its elements. In this paper, we propose SetVAE, a hierarchical variational autoencoder for sets. Motivated by recent progress in set encoding, we build SetVAE upon attentive modules that first partition the set and project the partition back to the original cardinality. Exploiting this module, our hierarchical VAE learns latent variables at multiple scales, capturing coarse-to-fine dependency of the set elements while achieving permutation invariance. We evaluate our model on point cloud generation task and achieve competitive performance to the prior arts with substantially smaller model capacity. We qualitatively demonstrate that our model generalizes to unseen set sizes and learns interesting subset relations without supervision. Our implementation is available at https://github.com/jw9730/setvae.",0
"This paper presents a novel architecture called Set Variational Autoencoder (SetVAE) which can learn hierarchical composition of sets for generative modeling of set structured data. Set-structured data has recently gained significant attention due to its prevalence in numerous real world applications such as image collections, social media datasets, and biological systems. Existing models used for generating synthetic examples from these type of data suffer from limitations including their lack of capacity to generate high quality results and limited ability to preserve crucial attributes during generation processes. We propose SetVAE as a solution that addresses these issues by modeling sets using latent variables that capture hierarchical relationships between objects within sets and allowing more efficient inference through variational methods. Our experiments on several benchmark dataset demonstrate that our proposed framework outperforms state-of-the art algorithms both quantitatively and qualitatively. Overall, we believe our work significantly advances research in generative modeling of complex set-structured data.",1
"Graph Neural Networks (GNNs) draw their strength from explicitly modeling the topological information of structured data. However, existing GNNs suffer from limited capability in capturing the hierarchical graph representation which plays an important role in graph classification. In this paper, we innovatively propose hierarchical graph capsule network (HGCN) that can jointly learn node embeddings and extract graph hierarchies. Specifically, disentangled graph capsules are established by identifying heterogeneous factors underlying each node, such that their instantiation parameters represent different properties of the same entity. To learn the hierarchical representation, HGCN characterizes the part-whole relationship between lower-level capsules (part) and higher-level capsules (whole) by explicitly considering the structure information among the parts. Experimental studies demonstrate the effectiveness of HGCN and the contribution of each component.",0
"Here is an example:  ---  Hierarchical graph capsules (HGCs) are a new type of capsule that can learn and represent hierarchies of interrelated concepts in images. HGCs capture both spatial and relational information and can encode structured knowledge into the network parameters. They have been shown to be effective at solving tasks such as image classification, object detection, and segmentation. In this paper we present results on using HGCs to learn representations from raw pixel data directly, without the need for convolutional preprocessing layers. We show that our approach outperforms state-of-the-art models on several benchmark datasets. Our implementation is open source and we provide detailed explanations and examples so others can use and build upon our work.  ---",1
"Graph Neural Networks (GNNs) have attracted increasing attention due to its successful applications on various graph-structure data. However, recent studies have shown that adversarial attacks are threatening the functionality of GNNs. Although numerous works have been proposed to defend adversarial attacks from various perspectives, most of them can be robust against the attacks only on specific scenarios. To address this shortage of robust generalization, we propose to defend the adversarial attacks on GNN through applying the Spatio-Temporal sparsification (called ST-Sparse) on the GNN hidden node representation. ST-Sparse is similar to the Dropout regularization in spirit. Through intensive experiment evaluation with GCN as the target GNN model, we identify the benefits of ST-Sparse as follows: (1) ST-Sparse shows the defense performance improvement in most cases, as it can effectively increase the robust accuracy by up to 6\% improvement; (2) ST-Sparse illustrates its robust generalization capability by integrating with the existing defense methods, similar to the integration of Dropout into various deep learning models as a standard regularization technique; (3) ST-Sparse also shows its ordinary generalization capability on clean datasets, in that ST-SparseGCN (the integration of ST-Sparse and the original GCN) even outperform the original GCN, while the other three representative defense methods are inferior to the original GCN.",0
"This paper introduces spatiotemporal sparsification for general robust graph convolution networks (GRGCN), which allows efficient training of deep GRGCN models on large datasets while maintaining high accuracy. We first investigate the properties of spatial sparsification and temporal sparsification individually, then combine them into a single algorithm that can effectively prune both spatial edges and temporal dimensions simultaneously. Our approach uses an adaptive edge importance ranking mechanism to filter out unimportant edges at runtime, resulting in significant performance improvements over baseline methods without compromising model accuracy. Experimental results show that our method achieves up to 4x speedups on GPU and CPU platforms with negligible loss in prediction accuracy.",1
"Heart failure (HF) is a major cause of mortality. Accurately monitoring HF progress and adjust therapies are critical for improving patient outcomes. An experienced cardiologist can make accurate HF stage diagnoses based on combination of symptoms, signs, and lab results from the electronic health records (EHR) of a patient, without directly measuring heart function. We examined whether machine learning models, more specifically the XGBoost model, can accurately predict patient stage based on EHR, and we further applied the SHapley Additive exPlanations (SHAP) framework to identify informative features and their interpretations. Our results indicate that based on structured data from EHR, our models could predict patients' ejection fraction (EF) scores with moderate accuracy. SHAP analyses identified informative features and revealed potential clinical subtypes of HF. Our findings provide insights on how to design computing systems to accurately monitor disease progression of HF patients through continuously mining patients' EHR data.",0
"In recent years, Electronic Health Records (EHRs) have become increasingly important tools in medical research. By analyzing large amounts of patient data stored in these records, researchers can identify patterns that could lead to more effective treatments and better outcomes for patients. One challenge faced by researchers working with EHRs is understanding how different features contribute to clinical predictions made by machine learning models. This study aims to address this issue by using Shapley Additive Explanations (SHAP), which provides feature attributions to explain black box model predictions.  The specific focus of this study is heart failure, a chronic condition affecting millions of individuals worldwide. To investigate the relationship between clinical features and prediction accuracy, we trained a tree-based model on a dataset containing EHR data from heart failure patients. We then used SHAP to interpret the model's predictions and determine which clinical features had the greatest impact on predictive performance. Our results show that certain clinical features such as age, sex, race, and co-morbidities were consistently significant across all tree nodes and contributed to improved prediction accuracy. However, there were some unexpected findings, including a negative association between serum creatinine levels and hospital readmission prediction. Overall, our work demonstrates the importance of interpreting machine learning models and underscores the need for further investigation into potential causes behind counterintuitive results. Ultimately, our analysis may inform future interventions aimed at improving outcomes for heart failure patients.",1
"The emergence of Graph Convolutional Network (GCN) has greatly boosted the progress of graph learning. However, two disturbing factors, noise and redundancy in graph data, and lack of interpretation for prediction results, impede further development of GCN. One solution is to recognize a predictive yet compressed subgraph to get rid of the noise and redundancy and obtain the interpretable part of the graph. This setting of subgraph is similar to the information bottleneck (IB) principle, which is less studied on graph-structured data and GCN. Inspired by the IB principle, we propose a novel subgraph information bottleneck (SIB) framework to recognize such subgraphs, named IB-subgraph. However, the intractability of mutual information and the discrete nature of graph data makes the objective of SIB notoriously hard to optimize. To this end, we introduce a bilevel optimization scheme coupled with a mutual information estimator for irregular graphs. Moreover, we propose a continuous relaxation for subgraph selection with a connectivity loss for stabilization. We further theoretically prove the error bound of our estimation scheme for mutual information and the noise-invariant nature of IB-subgraph. Extensive experiments on graph learning and large-scale point cloud tasks demonstrate the superior property of IB-subgraph.",0
"This is an example of a well thought out informative and scientific sounding title: ""A Machine Learning Approach To Recognition And Classification Of Complex Data Sets Using Graph Based Feature Extraction"" The Abstract should be written in language that can be easily understood by someone who has some knowledge of the field but may not have read all the background material associated with your paper. You want them to understand what you did and why they should care. Start with something interesting/informative related to your research question. End with three to five sentences discussing conclusions made based on results obtained from applying this methodology to data sets then list references used. Recent advances in machine learning approaches have revolutionized the ability to recognize complex patterns in large datasets, allowing for more accurate predictions than ever before. One particularly effective technique involves using graph-based feature extraction (GBFE) to identify substructures within predictive models which exhibit bottlenecks in their flow of information. These information bottlenecks act as crucial points for classification accuracy, enabling more precise prediction of future events through recognition of past trends. By leveraging GBFE methods alongside classical statistical modeling techniques, we demonstrate improved performance in recognizing important substructure elements compared to traditional modeling frameworks alone. This study provides insights into how advanced machine learning algorithms can enhance our understanding of complex system behaviors across many domains, including biological networks, social media analysis, and cybersecurity threat detection.",1
"Random forests on the one hand, and neural networks on the other hand, have met great success in the machine learning community for their predictive performance. Combinations of both have been proposed in the literature, notably leading to the so-called deep forests (DF) (Zhou \& Feng,2019). In this paper, our aim is not to benchmark DF performances but to investigate instead their underlying mechanisms. Additionally, DF architecture can be generally simplified into more simple and computationally efficient shallow forest networks. Despite some instability, the latter may outperform standard predictive tree-based methods. We exhibit a theoretical framework in which a shallow tree network is shown to enhance the performance of classical decision trees. In such a setting, we provide tight theoretical lower and upper bounds on its excess risk. These theoretical results show the interest of tree-network architectures for well-structured data provided that the first layer, acting as a data encoder, is rich enough.",0
"In recent years, there has been increasing interest in understanding deep neural networks (DNNs) due to their impressive performance on complex tasks such as image classification, speech recognition, and language translation. However, the lack of interpretability of DNNs remains one of the major obstacles to their widespread adoption. In particular, the internal representations learned by the hidden layers of these models have proven difficult to comprehend, which makes it challenging to evaluate their robustness and reliability. This study focuses on analyzing the tree-layer structure present in deep forest models, a variant of randomized decision trees that operates through averaging over multiple independent predictions from weak learner ensembles. We explore the relationship between the tree depth and different metrics commonly used to evaluate classifiers, such as accuracy, precision, recall, F1 score, AUC ROC curve, and confusion matrix. Our results indicate that deeper trees can lead to better performance across all evaluation measures, although there exists an optimal depth beyond which further growth leads to diminishing returns. Additionally, we analyze the stability of the tree structures under perturbations in input features and show how the use of randomization during training improves the resilience of the model against noise. Overall, our work provides insight into the behavior of deep forest models and contributes towards enhancing the transparency of modern machine learning algorithms. Keywords: Random Forest, Decision Trees, Classification Evaluation Metrics, Interpretability, Perturbations, Robustness, Resilience",1
"Graph-structured data are ubiquitous. However, graphs encode diverse types of information and thus play different roles in data representation. In this paper, we distinguish the \textit{representational} and the \textit{correlational} roles played by the graphs in node-level prediction tasks, and we investigate how Graph Neural Network (GNN) models can effectively leverage both types of information. Conceptually, the representational information provides guidance for the model to construct better node features; while the correlational information indicates the correlation between node outcomes conditional on node features. Through a simulation study, we find that many popular GNN models are incapable of effectively utilizing the correlational information. By leveraging the idea of the copula, a principled way to describe the dependence among multivariate random variables, we offer a general solution. The proposed Copula Graph Neural Network (CopulaGNN) can take a wide range of GNN models as base models and utilize both representational and correlational information stored in the graphs. Experimental results on two types of regression tasks verify the effectiveness of the proposed method.",0
"In recent years, graph neural networks (GNNs) have emerged as powerful tools for modeling complex data structures such as social networks, knowledge graphs, and biological networks. However, existing GNN models often overlook the inherent correlation structure present in these graphs, which can lead to suboptimal results. To address this issue, we propose a new framework called ""CopulaGNN"" that integrates both representational and correlational roles of graphs into the learning process. Our approach leverages copula functions to capture the underlying dependency relationships among nodes while preserving their individual features using conventional GNN layers. This enables our model to better learn and reason about interdependent relationships within dense graphs, resulting in improved performance on downstream tasks. We evaluate our method on multiple benchmark datasets across different domains and show consistent improvements over state-of-the-art alternatives. Overall, CopulaGNN offers a promising direction towards developing more effective GNN architectures for real-world applications.",1
"Graph convolutional neural networks (GCNNs) have received much attention recently, owing to their capability in handling graph-structured data. Among the existing GCNNs, many methods can be viewed as instances of a neural message passing motif; features of nodes are passed around their neighbors, aggregated and transformed to produce better nodes' representations. Nevertheless, these methods seldom use node transition probabilities, a measure that has been found useful in exploring graphs. Furthermore, when the transition probabilities are used, their transition direction is often improperly considered in the feature aggregation step, resulting in an inefficient weighting scheme. In addition, although a great number of GCNN models with increasing level of complexity have been introduced, the GCNNs often suffer from over-fitting when being trained on small graphs. Another issue of the GCNNs is over-smoothing, which tends to make nodes' representations indistinguishable. This work presents a new method to improve the message passing process based on node transition probabilities by properly considering the transition direction, leading to a better weighting scheme in nodes' features aggregation compared to the existing counterpart. Moreover, we propose a novel regularization method termed DropNode to address the over-fitting and over-smoothing issues simultaneously. DropNode randomly discards part of a graph, thus it creates multiple deformed versions of the graph, leading to data augmentation regularization effect. Additionally, DropNode lessens the connectivity of the graph, mitigating the effect of over-smoothing in deep GCNNs. Extensive experiments on eight benchmark datasets for node and graph classification tasks demonstrate the effectiveness of the proposed methods in comparison with the state of the art.",0
"In recent years, graph convolutional neural networks (GCNNs) have emerged as powerful tools for processing graph data such as social network analysis, bioinformatics, computer vision, etc. However, existing approaches face challenges due to oversmoothing issues caused by propagating messages across all neighbor nodes indiscriminately and failure to capture localized features from each node’s neighborhood. To address these issues, we propose a novel message passing algorithm that considers the transition probability of edges in graphs. This approach significantly reduces the impact of distant neighbors on node representations while retaining the importance of immediate neighbors. Furthermore, dropout regularization techniques can mitigate overfitting issues during training. Our extensive experimental results show promising performance improvements compared to state-of-the-art methods on various benchmark datasets. By incorporating transition probabilities into message passing and introducing dropout regularization, our method enhances GCNN performance in capturing localized features and reducing oversmoothing effects, making it suitable for numerous real-world applications involving complex graphs.",1
"Graph neural networks (GNNs) have demonstrated strong performance on a wide variety of tasks due to their ability to model non-uniform structured data. Despite their promise, there exists little research exploring methods to make them more efficient at inference time. In this work, we explore the viability of training quantized GNNs, enabling the usage of low precision integer arithmetic during inference. We identify the sources of error that uniquely arise when attempting to quantize GNNs, and propose an architecturally-agnostic method, Degree-Quant, to improve performance over existing quantization-aware training baselines commonly used on other architectures, such as CNNs. We validate our method on six datasets and show, unlike previous attempts, that models generalize to unseen graphs. Models trained with Degree-Quant for INT8 quantization perform as well as FP32 models in most cases; for INT4 models, we obtain up to 26% gains over the baselines. Our work enables up to 4.7x speedups on CPU when using INT8 arithmetic.",0
"Here is a possible abstract for the ""Degree-Quant"" paper on quantization-aware training for graph neural networks:  Graph neural networks (GNNs) have shown impressive results on numerous tasks, but their computational demands often make them challenging to deploy in resource-constrained environments such as mobile devices. One approach to addressing this issue is through the use of quantization techniques that reduce the precision of floating-point numbers used by GNN models, allowing for faster inference times without sacrificing accuracy. However, traditional approaches to quantizing GNNs can lead to significant losses in performance due to the complex nature of these models.  In this work, we propose a novel method called ""degree-quant"" for improving quantization-aware training of GNNs. Our approach uses degree normalization to control the magnitude of weights in the model, which allows us to tune the bitwidth during training to maximize efficiency while maintaining high levels of accuracy. We demonstrate the effectiveness of our method using several benchmark datasets and show that our proposed technique outperforms existing state-of-the-art methods for quantizing GNNs. By incorporating degree-quant into training pipelines, developers can create more efficient and powerful GNN models that can operate effectively even on limited hardware resources. Overall, our research offers valuable insights into improving the tradeoff between computation efficiency and quality of predictions in GNNs under tight constraints.",1
"In this thesis, we develop various techniques for working with sets in machine learning. Each input or output is not an image or a sequence, but a set: an unordered collection of multiple objects, each object described by a feature vector. Their unordered nature makes them suitable for modeling a wide variety of data, ranging from objects in images to point clouds to graphs. Deep learning has recently shown great success on other types of structured data, so we aim to build the necessary structures for sets into deep neural networks.   The first focus of this thesis is the learning of better set representations (sets as input). Existing approaches have bottlenecks that prevent them from properly modeling relations between objects within the set. To address this issue, we develop a variety of techniques for different scenarios and show that alleviating the bottleneck leads to consistent improvements across many experiments.   The second focus of this thesis is the prediction of sets (sets as output). Current approaches do not take the unordered nature of sets into account properly. We determine that this results in a problem that causes discontinuity issues with many set prediction tasks and prevents them from learning some extremely simple datasets. To avoid this problem, we develop two models that properly take the structure of sets into account. Various experiments show that our set prediction techniques can significantly benefit over existing approaches.",0
"This work presents a new approach for representing sets using deep neural networks, allowing them to make predictions on set variables that have no explicit training examples. We show how sets can be encoded as graphs where each element is treated as a node connected by edges, which represent relationships between elements. Our approach first learns the underlying structure of the graph representation through an unsupervised pre-training stage. Then, we use reinforcement learning from human feedback to fine-tune the model for predicting missing set variables. Experimental results demonstrate significant improvements over previous methods in terms of both prediction accuracy and interpretability of the learned representations. Our method has potential applications in areas such as natural language processing, computer vision, and recommendation systems.",1
"An increasing number of machine learning tasks deal with learning representations from set-structured data. Solutions to these problems involve the composition of permutation-equivariant modules (e.g., self-attention, or individual processing via feed-forward neural networks) and permutation-invariant modules (e.g., global average pooling, or pooling by multi-head attention). In this paper, we propose a geometrically-interpretable framework for learning representations from set-structured data, which is rooted in the optimal mass transportation problem. In particular, we treat elements of a set as samples from a probability measure and propose an exact Euclidean embedding for Generalized Sliced Wasserstein (GSW) distances to learn from set-structured data effectively. We evaluate our proposed framework on multiple supervised and unsupervised set learning tasks and demonstrate its superiority over state-of-the-art set representation learning approaches.",0
"This paper presents a novel approach to set representation learning using generalized sliced-Wasserstein embeddings (GSWE). The proposed method extends classical sliced-Wasserstein distance to better handle high-dimensional data by incorporating additional structure from tensorial representations. GSWE further enables computationally efficient embedding learning through regularization techniques inspired by deep generative models, such as variational autoencoders. An extensive experimental evaluation demonstrates that our approach outperforms prior state-of-the-art methods on several benchmark datasets across diverse applications including image retrieval, outlier detection, and clustering. Overall, we believe this work represents a significant advancement in the field of set geometry and machine learning, opening up new possibilities for addressing challenges related to scalability and interpretability in complex data analysis tasks.",1
"Graph neural networks (GNNs) have achieved state-of-the-art performance for node classification on graphs. The vast majority of existing works assume that genuine node labels are always provided for training. However, there has been very little research effort on how to improve the robustness of GNNs in the presence of label noise. Learning with label noise has been primarily studied in the context of image classification, but these techniques cannot be directly applied to graph-structured data, due to two major challenges -- label sparsity and label dependency -- faced by learning on graphs. In this paper, we propose a new framework, UnionNET, for learning with noisy labels on graphs under a semi-supervised setting. Our approach provides a unified solution for robustly training GNNs and performing label correction simultaneously. The key idea is to perform label aggregation to estimate node-level class probability distributions, which are used to guide sample reweighting and label correction. Compared with existing works, UnionNET has two appealing advantages. First, it requires no extra clean supervision, or explicit estimation of the noise transition matrix. Second, a unified learning framework is proposed to robustly train GNNs in an end-to-end manner. Experimental results show that our proposed approach: (1) is effective in improving model robustness against different types and levels of label noise; (2) yields significant improvements over state-of-the-art baselines.",0
"Graph neural networks (GNNs) have achieved state-of-the-art performance on tasks involving graphs such as social networks, chemical compounds, and knowledge graphs. However, these models can be sensitive to label noise, which occurs when training labels contain errors. Existing methods for tackling label noise typically rely either on assumptions that are unrealistic or require strong prior knowledge about data distributions. In contrast, our work proposes a new approach called Unified Robust Training (URT), which addresses both node classification and edge prediction problems under noisy labels without any prior assumption. We demonstrate the effectiveness of URT by applying it to several benchmark datasets across different domains, including citation networks, co-purchase networks, and bioinformatics data. Our results show that URT significantly outperforms other robust learning algorithms even when the label noise ratio reaches up to 50%. Overall, our research advances the field of graph neural network training by providing a simple yet effective solution for handling label noise in real-world applications.",1
"While most neural generative models generate outputs in a single pass, the human creative process is usually one of iterative building and refinement. Recent work has proposed models of editing processes, but these mostly focus on editing sequential data and/or only model a single editing pass. In this paper, we present a generic model for incremental editing of structured data (i.e., ""structural edits""). Particularly, we focus on tree-structured data, taking abstract syntax trees of computer programs as our canonical example. Our editor learns to iteratively generate tree edits (e.g., deleting or adding a subtree) and applies them to the partially edited data, thereby the entire editing process can be formulated as consecutive, incremental tree transformations. To show the unique benefits of modeling tree edits directly, we further propose a novel edit encoder for learning to represent edits, as well as an imitation learning method that allows the editor to be more robust. We evaluate our proposed editor on two source code edit datasets, where results show that, with the proposed edit encoder, our editor significantly improves accuracy over previous approaches that generate the edited program directly in one pass. Finally, we demonstrate that training our editor to imitate experts and correct its mistakes dynamically can further improve its performance.",0
"This is not how you write an academic abstract. You need to provide some context first: Explain the importance of structural editing, why incremental tree transformations can play a role, summarize your key ideas (e.g., that it allows editors to work on parts of trees instead of having to rebuild them from scratch), explain your methods and results (and their significance). Don’t forget to close by stating the takeaways of your study (or at least one possible) and end with brief acknowledgements if any contributors or funders were involved. A well written abstract should draw in readers who would benefit from reading your full paper. Start with something like “Structural editing refers to…” as most search committees won’t know what it means off the bat (though some might argue they shouldn’t have to read past page one in such a case ; )  . I hope these tips assist! Let me know if there’s more I can do.",1
"Graph Neural Networks (GNNs) are widely used for analyzing graph-structured data. Most GNN methods are highly sensitive to the quality of graph structures and usually require a perfect graph structure for learning informative embeddings. However, the pervasiveness of noise in graphs necessitates learning robust representations for real-world problems. To improve the robustness of GNN models, many studies have been proposed around the central concept of Graph Structure Learning (GSL), which aims to jointly learn an optimized graph structure and corresponding representations. Towards this end, in the presented survey, we broadly review recent progress of GSL methods for learning robust representations. Specifically, we first formulate a general paradigm of GSL, and then review state-of-the-art methods classified by how they model graph structures, followed by applications that incorporate the idea of GSL in other graph tasks. Finally, we point out some issues in current studies and discuss future directions.",0
"In recent years, deep graph structure learning has emerged as an important research topic within machine learning. As we enter a new era where artificial intelligence (AI) systems become more prevalent in our everyday lives, developing methods that can effectively represent complex data structures becomes increasingly crucial. With advancements in computing power and parallel processing architectures, there have been significant efforts towards understanding the importance of graph representation techniques such as Deep Belief Networks (DBN), Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Generative Adversarial Networks (GAN), and Attention Models. These models take into account both local and global connections across graphs which allows them to capture hierarchical knowledge at multiple scales, making them robust against noise and perturbation. This survey focuses on reviewing these state-of-the-art deep graph structure learning algorithms and their applications, examining their strengths, weaknesses, and limitations. By exploring existing approaches, this work seeks to provide insights into areas requiring further investigation and potential opportunities for future developments.",1
