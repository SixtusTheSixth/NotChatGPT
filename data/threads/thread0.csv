"Deep Reinforcement Learning (RL) demonstrates excellent performance on tasks that can be solved by trained policy. It plays a dominant role among cutting-edge machine learning approaches using multi-layer Neural networks (NNs). At the same time, Deep RL suffers from high sensitivity to noisy, incomplete, and misleading input data. Following biological intuition, we involve Spiking Neural Networks (SNNs) to address some deficiencies of deep RL solutions. Previous studies in image classification domain demonstrated that standard NNs (with ReLU nonlinearity) trained using supervised learning can be converted to SNNs with negligible deterioration in performance. In this paper, we extend those conversion results to the domain of Q-Learning NNs trained using RL. We provide a proof of principle of the conversion of standard NN to SNN. In addition, we show that the SNN has improved robustness to occlusion in the input image. Finally, we introduce results with converting full-scale Deep Q-network to SNN, paving the way for future research to robust Deep RL applications.",0
"Cutting-edge machine learning approaches, particularly Deep Reinforcement Learning (RL), have shown remarkable performance in solving tasks that can be accomplished by trained policy. However, Deep RL is vulnerable to noisy, incomplete, and misleading input data. To address this issue, we incorporate Spiking Neural Networks (SNNs) based on biological intuition. Previous studies have shown that standard Neural Networks (NNs) trained using supervised learning can be converted to SNNs with minimal degradation in performance. In this study, we extend this approach to Q-Learning NNs trained using RL, demonstrating the potential for converting standard NN to SNN. Furthermore, our results show that SNNs have improved resistance to occlusion in input images. Finally, we present the conversion of full-scale Deep Q-network to SNN, opening up possibilities for robust applications of Deep RL in the future.",1
"Most e-commerce product feeds provide blended results of advertised products and recommended products to consumers. The underlying advertising and recommendation platforms share similar if not exactly the same set of candidate products. Consumers' behaviors on the advertised results constitute part of the recommendation model's training data and therefore can influence the recommended results. We refer to this process as Leverage. Considering this mechanism, we propose a novel perspective that advertisers can strategically bid through the advertising platform to optimize their recommended organic traffic. By analyzing the real-world data, we first explain the principles of Leverage mechanism, i.e., the dynamic models of Leverage. Then we introduce a novel Leverage optimization problem and formulate it with a Markov Decision Process. To deal with the sample complexity challenge in model-free reinforcement learning, we propose a novel Hybrid Training Leverage Bidding (HTLB) algorithm which combines the real-world samples and the emulator-generated samples to boost the learning speed and stability. Our offline experiments as well as the results from the online deployment demonstrate the superior performance of our approach.",0
"E-commerce product feeds typically display a mix of advertised and recommended products to consumers, with the advertising and recommendation platforms sharing a common set of products. The behavior of consumers when using the advertised products can influence the recommendations they receive, a process known as Leverage. With this in mind, we propose a new approach whereby advertisers can strategically bid on advertising platforms to optimize their organic traffic recommendations. Our research analyzes real-world data to explain the Leverage mechanism and introduce a novel Leverage optimization problem, formulated using a Markov Decision Process. To overcome the challenges of sample complexity in model-free reinforcement learning, we have developed a Hybrid Training Leverage Bidding (HTLB) algorithm that combines real-world and emulator-generated samples to enhance learning speed and stability. Our offline experiments and online deployment demonstrate the superior performance of our approach.",1
"We start with a brief introduction to reinforcement learning (RL), about its successful stories, basics, an example, issues, the ICML 2019 Workshop on RL for Real Life, how to use it, study material and an outlook. Then we discuss a selection of RL applications, including recommender systems, computer systems, energy, finance, healthcare, robotics, and transportation.",0
"The initial part of our discussion covers a concise overview of reinforcement learning (RL), encompassing its triumphs, fundamentals, a case study, concerns, the ICML 2019 Workshop on RL for Real Life, utilization, educational resources, and prospects. Afterward, we delve into various RL implementations such as recommender systems, computer systems, energy, finance, healthcare, robotics, and transportation.",1
"This paper proposes a cascading failure mitigation strategy based on Reinforcement Learning (RL) method. Firstly, the principles of RL are introduced. Then, the Multi-Stage Cascading Failure (MSCF) problem is presented and its challenges are investigated. The problem is then tackled by the RL based on DC-OPF (Optimal Power Flow). Designs of the key elements of the RL framework (rewards, states, etc.) are also discussed in detail. Experiments on the IEEE 118-bus system by both shallow and deep neural networks demonstrate promising results in terms of reduced system collapse rates.",0
"The objective of this paper is to propose a strategy for mitigating cascading failures through the utilization of Reinforcement Learning (RL) approach. The initial section introduces the principles of RL, followed by an overview of the Multi-Stage Cascading Failure (MSCF) issue, along with an exploration of its challenges. The RL based on DC-OPF (Optimal Power Flow) is implemented to address the problem. The framework's crucial components, including rewards and states, are also thoroughly discussed. Experiments conducted on the IEEE 118-bus network using shallow and deep neural networks exhibit positive outcomes, particularly in terms of diminishing system collapse rates.",1
"In this paper, we show how novel transfer reinforcement learning techniques can be applied to the complex task of target driven navigation using the photorealistic AI2THOR simulator. Specifically, we build on the concept of Universal Successor Features with an A3C agent. We introduce the novel architectural contribution of a Successor Feature Dependant Policy (SFDP) and adopt the concept of Variational Information Bottlenecks to achieve state of the art performance. VUSFA, our final architecture, is a straightforward approach that can be implemented using our open source repository. Our approach is generalizable, showed greater stability in training, and outperformed recent approaches in terms of transfer learning ability.",0
"This article demonstrates how innovative transfer reinforcement learning techniques can be utilized to accomplish the challenging task of target-driven navigation in the photorealistic AI2THOR simulator. The study builds on the Universal Successor Features principle, with the addition of an A3C agent. A Successor Feature Dependent Policy (SFDP) is introduced as a new architectural component, and Variational Information Bottlenecks are employed to achieve state-of-the-art performance. The final architecture, VUSFA, is a simple technique that can be implemented using the authors' open-source repository. The approach is generalizable, demonstrated greater training stability, and surpassed earlier techniques in terms of transfer learning capabilities.",1
"This paper studies a recent proposal to use randomized value functions to drive exploration in reinforcement learning. These randomized value functions are generated by injecting random noise into the training data, making the approach compatible with many popular methods for estimating parameterized value functions. By providing a worst-case regret bound for tabular finite-horizon Markov decision processes, we show that planning with respect to these randomized value functions can induce provably efficient exploration.",0
"The aim of this study is to examine the proposal of utilizing randomized value functions for reinforcement learning exploration. The method involves injecting random noise into the training data to generate the value functions, which can be applied with various popular techniques for parameterized value function estimation. Our research establishes a worst-case regret bound for tabular finite-horizon Markov decision processes, demonstrating that adopting this approach can lead to efficient exploration by planning based on the randomized value functions.",1
"Recent advances in both machine learning and Internet-of-Things have attracted attention to automatic Activity Recognition, where users wear a device with sensors and their outputs are mapped to a predefined set of activities. However, few studies have considered the balance between wearable power consumption and activity recognition accuracy. This is particularly important when part of the computational load happens on the wearable device. In this paper, we present a new methodology to perform feature selection on the device based on Reinforcement Learning (RL) to find the optimum balance between power consumption and accuracy. To accelerate the learning speed, we extend the RL algorithm to address multiple sources of feedback, and use them to tailor the policy in conjunction with estimating the feedback accuracy. We evaluated our system on the SPHERE challenge dataset, a publicly available research dataset. The results show that our proposed method achieves a good trade-off between wearable power consumption and activity recognition accuracy.",0
"The use of machine learning and Internet-of-Things has recently gained attention in automatic Activity Recognition. This involves wearing a device with sensors that map outputs to specific activities. However, not many studies focus on balancing wearable power consumption and activity recognition accuracy, especially when computing is done on the wearable device. In this paper, we introduce a new approach to feature selection on the device using Reinforcement Learning (RL). Our method aims to find the best balance between accuracy and power consumption by incorporating multiple sources of feedback into the RL algorithm. We also estimate the feedback accuracy to better tailor the policy. We tested our system on the publicly available SPHERE challenge dataset and found that our approach achieves a favorable balance between wearable power consumption and activity recognition accuracy.",1
"This paper studies reinforcement learning (RL) under malicious falsification on cost signals and introduces a quantitative framework of attack models to understand the vulnerabilities of RL. Focusing on $Q$-learning, we show that $Q$-learning algorithms converge under stealthy attacks and bounded falsifications on cost signals. We characterize the relation between the falsified cost and the $Q$-factors as well as the policy learned by the learning agent which provides fundamental limits for feasible offensive and defensive moves. We propose a robust region in terms of the cost within which the adversary can never achieve the targeted policy. We provide conditions on the falsified cost which can mislead the agent to learn an adversary's favored policy. A numerical case study of water reservoir control is provided to show the potential hazards of RL in learning-based control systems and corroborate the results.",0
"The goal of this study is to examine the impact of malicious falsification on cost signals in reinforcement learning (RL) and develop a quantitative framework of attack models. Specifically, we focus on $Q$-learning and demonstrate that even when cost signals are subject to stealthy attacks and bounded falsifications, $Q$-learning algorithms still converge. We analyze the relationship between the falsified cost and the $Q$-factors, as well as the policy learned by the agent, to understand the limitations of both offensive and defensive strategies. We propose a robust region of cost values that the adversary cannot manipulate to achieve their desired policy. Finally, we identify conditions under which the falsified cost can mislead the learning agent into adopting the adversary's preferred policy. A case study of water reservoir control serves as an example of the potential dangers of RL in learning-based control systems and supports our findings.",1
"Inverse reinforcement learning (IRL) infers a reward function from demonstrations, allowing for policy improvement and generalization. However, despite much recent interest in IRL, little work has been done to understand the minimum set of demonstrations needed to teach a specific sequential decision-making task. We formalize the problem of finding maximally informative demonstrations for IRL as a machine teaching problem where the goal is to find the minimum number of demonstrations needed to specify the reward equivalence class of the demonstrator. We extend previous work on algorithmic teaching for sequential decision-making tasks by showing a reduction to the set cover problem which enables an efficient approximation algorithm for determining the set of maximally-informative demonstrations. We apply our proposed machine teaching algorithm to two novel applications: providing a lower bound on the number of queries needed to learn a policy using active IRL and developing a novel IRL algorithm that can learn more efficiently from informative demonstrations than a standard IRL approach.",0
"The process of inverse reinforcement learning (IRL) involves learning a reward function from demonstrations, which can lead to policy improvement and generalization. Although IRL has gained significant attention, there is a lack of research on identifying the minimum number of demonstrations necessary to teach a specific sequential decision-making task. We introduce a solution to this problem by framing it as a machine teaching challenge, aimed at discovering the fewest demonstrations required to define the reward equivalence class of the demonstrator. Our approach builds on prior work in algorithmic teaching for sequential decision-making tasks and involves a reduction to the set cover problem, which facilitates the development of an efficient approximation algorithm for identifying maximally informative demonstrations. We apply our machine teaching algorithm to two novel applications: estimating the minimal number of queries required for learning a policy using active IRL, and creating an innovative IRL algorithm capable of more efficient learning from informative demonstrations than traditional IRL methods.",1
"Counterfactual thinking describes a psychological phenomenon that people re-infer the possible results with different solutions about things that have already happened. It helps people to gain more experience from mistakes and thus to perform better in similar future tasks. This paper investigates the counterfactual thinking for agents to find optimal decision-making strategies in multi-agent reinforcement learning environments. In particular, we propose a multi-agent deep reinforcement learning model with a structure which mimics the human-psychological counterfactual thinking process to improve the competitive abilities for agents. To this end, our model generates several possible actions (intent actions) with a parallel policy structure and estimates the rewards and regrets for these intent actions based on its current understanding of the environment. Our model incorporates a scenario-based framework to link the estimated regrets with its inner policies. During the iterations, our model updates the parallel policies and the corresponding scenario-based regrets for agents simultaneously. To verify the effectiveness of our proposed model, we conduct extensive experiments on two different environments with real-world applications. Experimental results show that counterfactual thinking can actually benefit the agents to obtain more accumulative rewards from the environments with fair information by comparing to their opponents while keeping high performing efficiency.",0
"The psychological phenomenon of counterfactual thinking involves people imagining different outcomes for past events with alternative solutions. This type of thinking allows individuals to learn from their mistakes and perform better in future tasks. This study aims to explore the use of counterfactual thinking in improving decision-making strategies for agents in multi-agent reinforcement learning environments. A multi-agent deep reinforcement learning model is proposed, which mirrors the human counterfactual thinking process. The model generates multiple possible actions and estimates the rewards and regrets for these actions based on its understanding of the environment. A scenario-based framework is used to link the estimated regrets with the model's policies. The model updates its policies and corresponding regrets simultaneously during iterations. The effectiveness of the proposed model is tested through extensive experiments on two real-world environments. Results show that counterfactual thinking can enhance an agent's ability to accumulate rewards and perform efficiently while maintaining fair competition with opponents.",1
"We show how to teach machines to paint like human painters, who can use a small number of strokes to create fantastic paintings. By employing a neural renderer in model-based Deep Reinforcement Learning (DRL), our agents learn to determine the position and color of each stroke and make long-term plans to decompose texture-rich images into strokes. Experiments demonstrate that excellent visual effects can be achieved using hundreds of strokes. The training process does not require the experience of human painters or stroke tracking data. The code is available at https://github.com/hzwer/ICCV2019-LearningToPaint.",0
"Our approach demonstrates how machines can be trained to paint like human artists, who have the ability to create incredible paintings using just a few strokes. Utilizing a neural renderer in model-based Deep Reinforcement Learning (DRL), our agents acquire the skill to identify stroke position and color, and devise long-term plans to break down texture-heavy images into strokes. Our experiments reveal that exceptional visual outcomes can be attained with a stroke count in the hundreds, and that the training process does not necessitate human painter expertise or stroke tracking data. The code is readily available at https://github.com/hzwer/ICCV2019-LearningToPaint.",1
"Multi-agent systems have a wide range of applications in cooperative and competitive tasks. As the number of agents increases, nonstationarity gets more serious in multi-agent reinforcement learning (MARL), which brings great difficulties to the learning process. Besides, current mainstream algorithms configure each agent an independent network,so that the memory usage increases linearly with the number of agents which greatly slows down the interaction with the environment. Inspired by Generative Adversarial Networks (GAN), this paper proposes an iterative update method (IU) to stabilize the nonstationary environment. Further, we add first-person perspective and represent all agents by only one network which can change agents' policies from sequential compute to batch compute. Similar to continual lifelong learning, we realize the iterative update method in this unified representative network (IUUR). In this method, iterative update can greatly alleviate the nonstationarity of the environment, unified representation can speed up the interaction with environment and avoid the linear growth of memory usage. Besides, this method does not bother decentralized execution and distributed deployment. Experiments show that compared with MADDPG, our algorithm achieves state-of-the-art performance and saves wall-clock time by a large margin especially with more agents.",0
"The use of multi-agent systems is widespread in cooperative and competitive tasks. However, as the number of agents increases, nonstationarity becomes a more significant issue in multi-agent reinforcement learning (MARL), which makes learning more challenging. Additionally, current algorithms utilize individual networks for each agent, causing memory usage to increase linearly with the number of agents, thereby slowing down the interaction with the environment. To address these issues, this study proposes an iterative update method (IU) based on Generative Adversarial Networks (GAN) to stabilize the nonstationary environment. Furthermore, the study introduces a first-person perspective and employs a single network to represent all agents, allowing agents' policies to move from sequential to batch computation. This unified representative network (IUUR) uses the iterative update method, which effectively alleviates nonstationarity, speeds up interaction with the environment, and avoids the linear growth of memory usage. Additionally, this method does not require decentralized execution or distributed deployment. Experimental results demonstrate that our algorithm outperforms MADDPG and saves a significant amount of wall-clock time, particularly with a higher number of agents.",1
"Model-based Reinforcement Learning (MBRL) allows data-efficient learning which is required in real world applications such as robotics. However, despite the impressive data-efficiency, MBRL does not achieve the final performance of state-of-the-art Model-free Reinforcement Learning (MFRL) methods. We leverage the strengths of both realms and propose an approach that obtains high performance with a small amount of data. In particular, we combine MFRL and Model Predictive Control (MPC). While MFRL's strength in exploration allows us to train a better forward dynamics model for MPC, MPC improves the performance of the MFRL policy by sampling-based planning. The experimental results in standard continuous control benchmarks show that our approach can achieve MFRL`s level of performance while being as data-efficient as MBRL.",0
"In real world applications like robotics, data-efficient learning is necessary and Model-based Reinforcement Learning (MBRL) provides this. However, MBRL falls short of state-of-the-art Model-free Reinforcement Learning (MFRL) methods in terms of final performance. To overcome this, we propose a novel approach that combines the strengths of both MBRL and MFRL, resulting in high performance with minimal data. Our approach involves using MFRL for exploration and training a better forward dynamics model for Model Predictive Control (MPC), which in turn improves the performance of MFRL policy through sampling-based planning. Our experimental results in continuous control benchmarks demonstrate that our approach achieves the same level of performance as MFRL while being as data-efficient as MBRL.",1
"Generating image descriptions in different languages is essential to satisfy users worldwide. However, it is prohibitively expensive to collect large-scale paired image-caption dataset for every target language which is critical for training descent image captioning models. Previous works tackle the unpaired cross-lingual image captioning problem through a pivot language, which is with the help of paired image-caption data in the pivot language and pivot-to-target machine translation models. However, such language-pivoted approach suffers from inaccuracy brought by the pivot-to-target translation, including disfluency and visual irrelevancy errors. In this paper, we propose to generate cross-lingual image captions with self-supervised rewards in the reinforcement learning framework to alleviate these two types of errors. We employ self-supervision from mono-lingual corpus in the target language to provide fluency reward, and propose a multi-level visual semantic matching model to provide both sentence-level and concept-level visual relevancy rewards. We conduct extensive experiments for unpaired cross-lingual image captioning in both English and Chinese respectively on two widely used image caption corpora. The proposed approach achieves significant performance improvement over state-of-the-art methods.",0
"To cater to users worldwide, it is crucial to create image descriptions in multiple languages. Nonetheless, it is impractical to gather vast paired image-caption datasets for each language, which is necessary for training effective image captioning models. Prior research has addressed the issue of cross-lingual image captioning without paired data by using a pivot language as a reference. However, this approach is not without flaws, as it can result in inaccuracies due to errors in pivot-to-target machine translations. This paper introduces a new method for generating cross-lingual image captions that employs self-supervised rewards in the reinforcement learning framework. This approach helps reduce disfluency and visual irrelevancy errors by using fluency reward provided by self-supervision from mono-lingual corpus in the target language and a multi-level visual semantic matching model that provides both sentence-level and concept-level visual relevancy rewards. We evaluate our approach on two widely used image caption corpora in English and Chinese and find that it outperforms existing state-of-the-art methods.",1
"Data efficiency and robustness to task-irrelevant perturbations are long-standing challenges for deep reinforcement learning algorithms. Here we introduce a modular approach to addressing these challenges in a continuous control environment, without using hand-crafted or supervised information. Our Curious Object-Based seaRch Agent (COBRA) uses task-free intrinsically motivated exploration and unsupervised learning to build object-based models of its environment and action space. Subsequently, it can learn a variety of tasks through model-based search in very few steps and excel on structured hold-out tests of policy robustness.",0
"For deep reinforcement learning algorithms, achieving data efficiency and resilience against task-irrelevant disturbances has always been a daunting task. To tackle these issues in a continuous control environment, we have developed a modular approach that does not rely on hand-crafted or supervised information. Our approach, called Curious Object-Based seaRch Agent (COBRA), employs task-free intrinsically motivated exploration and unsupervised learning to construct object-based models of the environment and action space. Using this approach, COBRA can quickly learn a variety of tasks through model-based search and perform exceptionally well on structured hold-out tests that assess policy robustness.",1
"Embodied Question Answering (EQA) is a recently proposed task, where an agent is placed in a rich 3D environment and must act based solely on its egocentric input to answer a given question. The desired outcome is that the agent learns to combine capabilities such as scene understanding, navigation and language understanding in order to perform complex reasoning in the visual world. However, initial advancements combining standard vision and language methods with imitation and reinforcement learning algorithms have shown EQA might be too complex and challenging for these techniques. In order to investigate the feasibility of EQA-type tasks, we build the VideoNavQA dataset that contains pairs of questions and videos generated in the House3D environment. The goal of this dataset is to assess question-answering performance from nearly-ideal navigation paths, while considering a much more complete variety of questions than current instantiations of the EQA task. We investigate several models, adapted from popular VQA methods, on this new benchmark. This establishes an initial understanding of how well VQA-style methods can perform within this novel EQA paradigm.",0
"A new task called Embodied Question Answering (EQA) has been proposed, which involves an agent navigating a complex 3D environment and answering questions based solely on its egocentric input. The aim is to enable the agent to combine scene understanding, navigation, and language understanding in order to perform complex reasoning in the visual world. However, early attempts using standard vision and language methods with imitation and reinforcement learning algorithms have proven to be too challenging for EQA. To explore the feasibility of EQA-type tasks, we have created the VideoNavQA dataset, which includes pairs of questions and videos generated in the House3D environment. The dataset evaluates question-answering performance while considering a wide range of questions and nearly-ideal navigation paths. We have tested several models adapted from popular VQA methods on this benchmark, providing an initial understanding of how VQA-style methods perform within the EQA paradigm.",1
"Although significant progress has been made in the field of automatic image captioning, it is still a challenging task. Previous works normally pay much attention to improving the quality of the generated captions but ignore the diversity of captions. In this paper, we combine determinantal point process (DPP) and reinforcement learning (RL) and propose a novel reinforcing DPP (R-DPP) approach to generate a set of captions with high quality and diversity for an image. We show that R-DPP performs better on accuracy and diversity than using noise as a control signal (GANs, VAEs). Moreover, R-DPP is able to preserve the modes of the learned distribution. Hence, beam search algorithm can be applied to generate a single accurate caption, which performs better than other RL-based models.",0
"Despite the advancements in automatic image captioning, generating diverse captions remains a difficult task. Previous studies have primarily focused on improving the quality of captions, neglecting diversity. This study presents a new approach, the Reinforcing Determinantal Point Process (R-DPP), that combines determinantal point process and reinforcement learning to generate a set of captions with both high quality and diversity. Our results show that R-DPP outperforms other methods, such as GANs and VAEs, in terms of accuracy and diversity. Moreover, R-DPP preserves the modes of the learned distribution, making it possible to use the beam search algorithm to generate a single accurate caption that outperforms other RL-based models.",1
"Recent developments in machine-learning algorithms have led to impressive performance increases in many traditional application scenarios of artificial intelligence research. In the area of deep reinforcement learning, deep learning functional architectures are combined with incremental learning schemes for sequential tasks that include interaction-based, but often delayed feedback. Despite their impressive successes, modern machine-learning approaches, including deep reinforcement learning, still perform weakly when compared to flexibly adaptive biological systems in certain naturally occurring scenarios. Such scenarios include transfers to environments different than the ones in which the training took place or environments that dynamically change, both of which are often mastered by biological systems through a capability that we here term ""fluid adaptivity"" to contrast it from the much slower adaptivity (""crystallized adaptivity"") of the prior learning from which the behavior emerged. In this article, we derive and discuss research strategies, based on analyzes of fluid adaptivity in biological systems and its neuronal modeling, that might aid in equipping future artificially intelligent systems with capabilities of fluid adaptivity more similar to those seen in some biologically intelligent systems. A key component of this research strategy is the dynamization of the problem space itself and the implementation of this dynamization by suitably designed flexibly interacting modules.",0
"The advances in machine-learning algorithms have resulted in remarkable performance improvements in many traditional applications of artificial intelligence research. Deep reinforcement learning employs deep learning functional architectures along with incremental learning schemes for sequential tasks that involve interaction-based but often delayed feedback. Despite their impressive achievements, modern machine-learning approaches, including deep reinforcement learning, are still inferior to biologically adaptive systems in certain natural scenarios. These scenarios include transfers to different environments or environments that undergo dynamic changes, which are readily mastered by biological systems through ""fluid adaptivity,"" in contrast to the slower ""crystallized adaptivity"" of prior learning. In this article, we propose research strategies based on the analysis of fluid adaptivity in biological systems and its neuronal modeling to equip future artificially intelligent systems with the fluid adaptivity capabilities seen in some biologically intelligent systems. A critical component of this research strategy involves dynamizing the problem space and implementing it through appropriately designed flexibly interacting modules.",1
"Deep reinforcement learning has learned to play many games well, but failed on others. To better characterize the modes and reasons of failure of deep reinforcement learners, we test the widely used Asynchronous Actor-Critic (A2C) algorithm on four deceptive games, which are specially designed to provide challenges to game-playing agents. These games are implemented in the General Video Game AI framework, which allows us to compare the behavior of reinforcement learning-based agents with planning agents based on tree search. We find that several of these games reliably deceive deep reinforcement learners, and that the resulting behavior highlights the shortcomings of the learning algorithm. The particular ways in which agents fail differ from how planning-based agents fail, further illuminating the character of these algorithms. We propose an initial typology of deceptions which could help us better understand pitfalls and failure modes of (deep) reinforcement learning.",0
"Although deep reinforcement learning has demonstrated proficiency in playing numerous games, it has been unable to succeed in certain cases. In order to analyze the modes and reasons for these failures, we conducted a study using the widely utilized Asynchronous Actor-Critic (A2C) algorithm on four deliberately challenging games that were designed to test the abilities of game-playing agents. These games were built into the General Video Game AI framework, which allowed us to compare the decision-making of reinforcement learning-based agents with planning agents based on tree search. We discovered that deep reinforcement learners were easily deceived by several of these games, highlighting the weaknesses of the learning algorithm. The manner in which agents fail differs from that of planning-based agents, providing further insight into the nature of these algorithms. We have developed an initial typology of deceptions that may assist in comprehending the pitfalls and failure modes of (deep) reinforcement learning.",1
"Robotic systems are ever more capable of automation and fulfilment of complex tasks, particularly with reliance on recent advances in intelligent systems, deep learning and artificial intelligence. However, as robots and humans come closer in their interactions, the matter of interpretability, or explainability of robot decision-making processes for the human grows in importance. A successful interaction and collaboration will only take place through mutual understanding of underlying representations of the environment and the task at hand. This is currently a challenge in deep learning systems. We present a hierarchical deep reinforcement learning system, consisting of a low-level agent handling the large actions/states space of a robotic system efficiently, by following the directives of a high-level agent which is learning the high-level dynamics of the environment and task. This high-level agent forms a representation of the world and task at hand that is interpretable for a human operator. The method, which we call Dot-to-Dot, is tested on a MuJoCo-based model of the Fetch Robotics Manipulator, as well as a Shadow Hand, to test its performance. Results show efficient learning of complex actions/states spaces by the low-level agent, and an interpretable representation of the task and decision-making process learned by the high-level agent.",0
"With recent advancements in intelligent systems, deep learning, and artificial intelligence, robots are becoming more capable of automating and performing complex tasks. However, as robots and humans interact more closely, the issue of interpretability becomes more important. It is crucial for successful collaboration and interaction between robots and humans to have a mutual understanding of the environment and task at hand. However, this poses a challenge in deep learning systems. Our solution to this is a hierarchical deep reinforcement learning system called Dot-to-Dot. It consists of a low-level agent that efficiently handles the large actions/states space of a robotic system by following the directives of a high-level agent. The high-level agent forms an interpretable representation of the task and decision-making process for human operators. We tested this method on a MuJoCo-based model of the Fetch Robotics Manipulator and a Shadow Hand, and the results showed efficient learning of complex actions/states spaces by the low-level agent and an interpretable representation of the task and decision-making process learned by the high-level agent.",1
"In this paper, we propose a deep reinforcement learning (DRL) based mobility load balancing (MLB) algorithm along with a two-layer architecture to solve the large-scale load balancing problem for ultra-dense networks (UDNs). Our contribution is three-fold. First, this work proposes a two-layer architecture to solve the large-scale load balancing problem in a self-organized manner. The proposed architecture can alleviate the global traffic variations by dynamically grouping small cells into self-organized clusters according to their historical loads, and further adapt to local traffic variations through intra-cluster load balancing afterwards. Second, for the intra-cluster load balancing, this paper proposes an off-policy DRL-based MLB algorithm to autonomously learn the optimal MLB policy under an asynchronous parallel learning framework, without any prior knowledge assumed over the underlying UDN environments. Moreover, the algorithm enables joint exploration with multiple behavior policies, such that the traditional MLB methods can be used to guide the learning process thereby improving the learning efficiency and stability. Third, this work proposes an offline-evaluation based safeguard mechanism to ensure that the online system can always operate with the optimal and well-trained MLB policy, which not only stabilizes the online performance but also enables the exploration beyond current policies to make full use of machine learning in a safe way. Empirical results verify that the proposed framework outperforms the existing MLB methods in general UDN environments featured with irregular network topologies, coupled interferences, and random user movements, in terms of the load balancing performance.",0
"Our paper presents a two-layer architecture and a deep reinforcement learning (DRL) based mobility load balancing (MLB) algorithm to address the challenge of large-scale load balancing in ultra-dense networks (UDNs). Our contribution is three-fold. Firstly, we propose a self-organized clustering approach that groups small cells according to their historical loads to mitigate global traffic variations. Secondly, we introduce an off-policy DRL-based MLB algorithm to autonomously learn the optimal MLB policy for intra-cluster load balancing, which incorporates multiple behavior policies to enhance learning efficiency and stability. Thirdly, we propose an offline-evaluation based safeguard mechanism to ensure the online system always operates with the optimal and well-trained MLB policy, enabling safe exploration beyond current policies. Empirical results demonstrate that our proposed framework outperforms existing MLB methods in UDN environments with irregular network topologies, coupled interferences, and random user movements, in terms of load balancing performance.",1
"Many practical applications of reinforcement learning constrain agents to learn from a fixed batch of data which has already been gathered, without offering further possibility for data collection. In this paper, we demonstrate that due to errors introduced by extrapolation, standard off-policy deep reinforcement learning algorithms, such as DQN and DDPG, are incapable of learning with data uncorrelated to the distribution under the current policy, making them ineffective for this fixed batch setting. We introduce a novel class of off-policy algorithms, batch-constrained reinforcement learning, which restricts the action space in order to force the agent towards behaving close to on-policy with respect to a subset of the given data. We present the first continuous control deep reinforcement learning algorithm which can learn effectively from arbitrary, fixed batch data, and empirically demonstrate the quality of its behavior in several tasks.",0
"The use of reinforcement learning in practical applications often involves agents learning from a set of pre-collected data with no opportunity for further data collection. This paper highlights the limitations of standard off-policy deep reinforcement learning algorithms, such as DQN and DDPG, due to errors introduced by extrapolation. These algorithms are ineffective in learning from data that is not correlated with the current policy distribution. To address this issue, a new class of off-policy algorithms called batch-constrained reinforcement learning is introduced. This algorithm restricts the action space to encourage the agent to behave similarly to an on-policy subset of the given data. The paper presents the first continuous control deep reinforcement learning algorithm that can effectively learn from any fixed batch data. The results show the effectiveness of this approach in several tasks.",1
"Policy gradient methods have demonstrated success in reinforcement learning tasks that have high-dimensional continuous state and action spaces. However, policy gradient methods are also notoriously sample inefficient. This can be attributed, at least in part, to the high variance in estimating the gradient of the task objective with Monte Carlo methods. Previous research has endeavored to contend with this problem by studying control variates (CVs) that can reduce the variance of estimates without introducing bias, including the early use of baselines, state dependent CVs, and the more recent state-action dependent CVs. In this work, we analyze the properties and drawbacks of previous CV techniques and, surprisingly, we find that these works have overlooked an important fact that Monte Carlo gradient estimates are generated by trajectories of states and actions. We show that ignoring the correlation across the trajectories can result in suboptimal variance reduction, and we propose a simple fix: a class of ""trajectory-wise"" CVs, that can further drive down the variance. We show that constructing trajectory-wise CVs can be done recursively and requires only learning state-action value functions like the previous CVs for policy gradient. We further prove that the proposed trajectory-wise CVs are optimal for variance reduction under reasonable assumptions.",0
"Reinforcement learning tasks with high-dimensional continuous state and action spaces have been successful using policy gradient methods. However, these methods are known for their inefficient use of samples, which is partly due to the high variance in estimating the task objective gradient with Monte Carlo methods. Previous research has tackled this issue by studying control variates (CVs) that can reduce variance without introducing bias. This includes the use of baselines, state-dependent CVs, and state-action dependent CVs. In this study, we analyze the drawbacks and properties of previous CV techniques. We found that previous works have overlooked an important aspect that Monte Carlo gradient estimates are generated by trajectories of states and actions. We propose a simple fix: a class of ""trajectory-wise"" CVs that can further reduce the variance. Constructing trajectory-wise CVs can be done recursively and requires only learning state-action value functions like the previous CVs for policy gradient. We prove that the proposed trajectory-wise CVs are optimal for variance reduction under reasonable assumptions.",1
"In artificial intelligence, we often specify tasks through a reward function. While this works well in some settings, many tasks are hard to specify this way. In deep reinforcement learning, for example, directly specifying a reward as a function of a high-dimensional observation is challenging. Instead, we present an interface for specifying tasks interactively using demonstrations. Our approach defines a set of increasingly complex policies. The interface allows the user to switch between these policies at fixed intervals to generate demonstrations of novel, more complex, tasks. We train new policies based on these demonstrations and repeat the process. We present a case study of our approach in the Lunar Lander domain, and show that this simple approach can quickly learn a successful landing policy and outperforms an existing comparison-based deep RL method.",0
"Tasks in artificial intelligence are often defined using a reward function, but this can be difficult for complex tasks like those in deep reinforcement learning. To address this, we developed an interface that allows users to specify tasks through interactive demonstrations. Our approach involves creating a series of progressively more complex policies that users can switch between to generate demonstrations of new tasks. We then train new policies based on these demonstrations and repeat the process. We tested our approach in the Lunar Lander domain and found that it quickly learned a successful landing policy, outperforming existing deep RL methods that rely on comparisons.",1
"We consider the problem of fine-grained classification on an edge camera device that has limited power. The edge device must sparingly interact with the cloud to minimize communication bits to conserve power, and the cloud upon receiving the edge inputs returns a classification label. To deal with fine-grained classification, we adopt the perspective of sequential fixation with a foveated field-of-view to model cloud-edge interactions. We propose a novel deep reinforcement learning-based foveation model, DRIFT, that sequentially generates and recognizes mixed-acuity images.Training of DRIFT requires only image-level category labels and encourages fixations to contain task-relevant information, while maintaining data efficiency. Specifically, wetrain a foveation actor network with a novel Deep Deterministic Policy Gradient by Conditioned Critic and Coaching (DDPGC3) algorithm. In addition, we propose to shape the reward to provide informative feedback after each fixation to better guide RL training. We demonstrate the effectiveness of DRIFT on this task by evaluating on five fine-grained classification benchmark datasets, and show that the proposed approach achieves state-of-the-art performance with over 3X reduction in transmitted pixels.",0
"The problem of fine-grained classification on an edge camera device with limited power is considered in this study. To conserve power and minimize communication bits, the edge device must interact with the cloud sparingly, with the cloud returning a classification label upon receiving the edge inputs. To address fine-grained classification, the researchers propose a foveated field-of-view with sequential fixation to model cloud-edge interactions. A novel deep reinforcement learning-based foveation model called DRIFT is introduced, which generates and recognizes mixed-acuity images. DRIFT's training only requires image-level category labels and encourages fixations to contain task-relevant information while maintaining data efficiency. The researchers train a foveation actor network using a novel Deep Deterministic Policy Gradient by Conditioned Critic and Coaching (DDPGC3) algorithm and shape the reward to provide informative feedback after each fixation to guide RL training better. DRIFT's effectiveness is demonstrated by evaluating its performance on five fine-grained classification benchmark datasets, showing that it achieves state-of-the-art performance with over 3X reduction in transmitted pixels.",1
"Automatic data abstraction is an important capability for both benchmarking machine intelligence and supporting summarization applications. In the former one asks whether a machine can `understand' enough about the meaning of input data to produce a meaningful but more compact abstraction. In the latter this capability is exploited for saving space or human time by summarizing the essence of input data. In this paper we study a general reinforcement learning based framework for learning to abstract sequential data in a goal-driven way. The ability to define different abstraction goals uniquely allows different aspects of the input data to be preserved according to the ultimate purpose of the abstraction. Our reinforcement learning objective does not require human-defined examples of ideal abstraction. Importantly our model processes the input sequence holistically without being constrained by the original input order. Our framework is also domain agnostic -- we demonstrate applications to sketch, video and text data and achieve promising results in all domains.",0
"The capacity for automatic data abstraction is crucial for both evaluating machine intelligence and facilitating summarization software. In the former, the question is whether a machine can comprehend enough about the input data to create a concise and meaningful abstraction. In the latter, this ability is utilized to save space or time by summarizing the core of the input data. This study investigates a general framework for learning to abstract sequential data using reinforcement learning in a goal-oriented manner. The unique ability to establish different abstraction goals enables the preservation of various aspects of the input data according to the intended purpose of the abstraction. Our reinforcement learning objective does not rely on human-defined examples of ideal abstraction. Crucially, our model processes the input sequence comprehensively without being restricted by the original input order. Our framework is also versatile, as we demonstrate its effectiveness in sketch, video, and text data domains with promising outcomes.",1
"Continuous reinforcement learning such as DDPG and A3C are widely used in robot control and autonomous driving. However, both methods have theoretical weaknesses. While DDPG cannot control noises in the control process, A3C does not satisfy the continuity conditions under the Gaussian policy. To address these concerns, we propose a new continues reinforcement learning method based on stochastic differential equations and we call it Incremental Reinforcement Learning (IRL). This method not only guarantees the continuity of actions within any time interval, but controls the variance of actions in the training process. In addition, our method does not assume Markov control in agents' action control and allows agents to predict scene changes for action selection. With our method, agents no longer passively adapt to the environment. Instead, they positively interact with the environment for maximum rewards.",0
"Robot control and autonomous driving commonly rely on continuous reinforcement learning, specifically DDPG and A3C. However, both approaches have theoretical limitations. DDPG struggles to manage noise during control, while A3C fails to meet continuity requirements with the Gaussian policy. To overcome these drawbacks, we present our new incremental reinforcement learning method, known as IRL. This approach employs stochastic differential equations to ensure action continuity and control action variance in training. Notably, IRL doesn't assume Markov control and enables agents to anticipate changes in the environment for optimal action selection. Rather than passively adapting, agents actively engage with the environment to maximize rewards.",1
"Modern Reinforcement Learning (RL) is commonly applied to practical problems with an enormous number of states, where function approximation must be deployed to approximate either the value function or the policy. The introduction of function approximation raises a fundamental set of challenges involving computational and statistical efficiency, especially given the need to manage the exploration/exploitation tradeoff. As a result, a core RL question remains open: how can we design provably efficient RL algorithms that incorporate function approximation? This question persists even in a basic setting with linear dynamics and linear rewards, for which only linear function approximation is needed.   This paper presents the first provable RL algorithm with both polynomial runtime and polynomial sample complexity in this linear setting, without requiring a ""simulator"" or additional assumptions. Concretely, we prove that an optimistic modification of Least-Squares Value Iteration (LSVI)---a classical algorithm frequently studied in the linear setting---achieves $\tilde{\mathcal{O}}(\sqrt{d^3H^3T})$ regret, where $d$ is the ambient dimension of feature space, $H$ is the length of each episode, and $T$ is the total number of steps. Importantly, such regret is independent of the number of states and actions.",0
"The application of Modern Reinforcement Learning (RL) to practical problems with a large number of states often requires function approximation to approximate the value function or policy. However, this introduces challenges in computational and statistical efficiency, particularly in balancing exploration/exploitation. A key question remains about how to design efficient RL algorithms that incorporate function approximation. This paper presents a provable RL algorithm that achieves polynomial runtime and sample complexity in a linear setting without additional assumptions or a simulator. Specifically, an optimistic modification of Least-Squares Value Iteration (LSVI) achieves regret of $\tilde{\mathcal{O}}(\sqrt{d^3H^3T})$, which is not dependent on the number of states or actions.",1
"Sequences play an important role in many applications and systems. Discovering sequences with desired properties has long been an interesting intellectual pursuit. This paper puts forth a new paradigm, AlphaSeq, to discover desired sequences algorithmically using deep reinforcement learning (DRL) techniques. AlphaSeq treats the sequence discovery problem as an episodic symbol-filling game, in which a player fills symbols in the vacant positions of a sequence set sequentially during an episode of the game. Each episode ends with a completely-filled sequence set, upon which a reward is given based on the desirability of the sequence set. AlphaSeq models the game as a Markov Decision Process (MDP), and adapts the DRL framework of AlphaGo to solve the MDP. Sequences discovered improve progressively as AlphaSeq, starting as a novice, learns to become an expert game player through many episodes of game playing. Compared with traditional sequence construction by mathematical tools, AlphaSeq is particularly suitable for problems with complex objectives intractable to mathematical analysis. We demonstrate the searching capabilities of AlphaSeq in two applications: 1) AlphaSeq successfully rediscovers a set of ideal complementary codes that can zero-force all potential interferences in multi-carrier CDMA systems. 2) AlphaSeq discovers new sequences that triple the signal-to-interference ratio -- benchmarked against the well-known Legendre sequence -- of a mismatched filter estimator in pulse compression radar systems.",0
"Sequences have a significant role in various applications and systems, and finding sequences with specific properties has been an intriguing pursuit. This research introduces a novel approach, AlphaSeq, to algorithmically discover desired sequences using deep reinforcement learning techniques. AlphaSeq treats the sequence discovery task as a symbol-filling game where the player sequentially fills vacant positions in a sequence set during an episode. At the end of each episode, a reward is given based on the sequence set's desirability. AlphaSeq models the game as a Markov Decision Process (MDP) and employs the DRL framework of AlphaGo to solve the MDP. As AlphaSeq progresses from a novice to an expert through many game-playing episodes, the discovered sequences improve progressively. AlphaSeq is particularly useful for problems with complex objectives that are not amenable to mathematical analysis. The capabilities of AlphaSeq are demonstrated in two applications: 1) AlphaSeq successfully rediscovered a set of ideal complementary codes that can zero-force all potential interferences in multi-carrier CDMA systems. 2) AlphaSeq discovered new sequences that triple the signal-to-interference ratio of a mismatched filter estimator in pulse compression radar systems, benchmarked against the well-known Legendre sequence.",1
"Rewards are sparse in the real world and most of today's reinforcement learning algorithms struggle with such sparsity. One solution to this problem is to allow the agent to create rewards for itself - thus making rewards dense and more suitable for learning. In particular, inspired by curious behaviour in animals, observing something novel could be rewarded with a bonus. Such bonus is summed up with the real task reward - making it possible for RL algorithms to learn from the combined reward. We propose a new curiosity method which uses episodic memory to form the novelty bonus. To determine the bonus, the current observation is compared with the observations in memory. Crucially, the comparison is done based on how many environment steps it takes to reach the current observation from those in memory - which incorporates rich information about environment dynamics. This allows us to overcome the known ""couch-potato"" issues of prior work - when the agent finds a way to instantly gratify itself by exploiting actions which lead to hardly predictable consequences. We test our approach in visually rich 3D environments in ViZDoom, DMLab and MuJoCo. In navigational tasks from ViZDoom and DMLab, our agent outperforms the state-of-the-art curiosity method ICM. In MuJoCo, an ant equipped with our curiosity module learns locomotion out of the first-person-view curiosity only.",0
"Sparse rewards in the real world pose a challenge for most reinforcement learning algorithms. One solution is to allow the agent to generate its own rewards, resulting in more frequent and effective learning. Inspired by animals' curious behavior, our proposed method rewards novel observations with a bonus, which is added to the real task reward for combined learning. Our approach uses episodic memory to determine the novelty bonus by comparing current observations to those in memory based on the number of environment steps required to reach them. This overcomes prior issues of instant gratification and exploitation. We tested our method in 3D environments and found it outperformed the state-of-the-art curiosity method ICM in navigational tasks and allowed an ant to learn locomotion solely through first-person-view curiosity.",1
"We investigate a classification problem using multiple mobile agents capable of collecting (partial) pose-dependent observations of an unknown environment. The objective is to classify an image over a finite time horizon. We propose a network architecture on how agents should form a local belief, take local actions, and extract relevant features from their raw partial observations. Agents are allowed to exchange information with their neighboring agents to update their own beliefs. It is shown how reinforcement learning techniques can be utilized to achieve decentralized implementation of the classification problem by running a decentralized consensus protocol. Our experimental results on the MNIST handwritten digit dataset demonstrates the effectiveness of our proposed framework.",0
"Our study focuses on a classification challenge that involves using multiple mobile agents that can gather (partial) pose-dependent data about an unfamiliar environment. The goal is to classify an image within a limited timeframe. We present a network design that outlines how agents should create a local belief, carry out local actions, and extract pertinent features from their raw, incomplete observations. Agents can share information with nearby agents to update their own beliefs. We demonstrate how reinforcement learning approaches can be leveraged to achieve decentralized execution of the classification task using a decentralized consensus protocol. Our test results using the MNIST handwritten digit dataset validate the efficacy of our proposed framework.",1
"This paper considers the problem of image set-based face verification and identification. Unlike traditional single sample (an image or a video) setting, this situation assumes the availability of a set of heterogeneous collection of orderless images and videos. The samples can be taken at different check points, different identity documents $etc$. The importance of each image is usually considered either equal or based on a quality assessment of that image independent of other images and/or videos in that image set. How to model the relationship of orderless images within a set remains a challenge. We address this problem by formulating it as a Markov Decision Process (MDP) in a latent space. Specifically, we first propose a dependency-aware attention control (DAC) network, which uses actor-critic reinforcement learning for attention decision of each image to exploit the correlations among the unordered images. An off-policy experience replay is introduced to speed up the learning process. Moreover, the DAC is combined with a temporal model for videos using divide and conquer strategies. We also introduce a pose-guided representation (PGR) scheme that can further boost the performance at extreme poses. We propose a parameter-free PGR without the need for training as well as a novel metric learning-based PGR for pose alignment without the need for pose detection in testing stage. Extensive evaluations on IJB-A/B/C, YTF, Celebrity-1000 datasets demonstrate that our method outperforms many state-of-art approaches on the set-based as well as video-based face recognition databases.",0
"In this paper, we examine the challenge of verifying and identifying faces based on sets of images. This differs from the traditional approach of using a single image or video, as it involves a collection of unordered images and videos that may have been taken at different times and places. Each image is typically given equal importance or evaluated independently of the others. The difficulty lies in modeling the relationship between these images. To address this issue, we propose using a Markov Decision Process in a latent space. Our approach involves a dependency-aware attention control network that uses reinforcement learning to determine the relevance of each image in the set. We also introduce a pose-guided representation scheme that improves performance in extreme poses. Our method outperforms many state-of-the-art approaches on various face recognition databases.",1
"We propose the use of a proportional-derivative (PD) control based policy learned via reinforcement learning (RL) to estimate and forecast 3D human pose from egocentric videos. The method learns directly from unsegmented egocentric videos and motion capture data consisting of various complex human motions (e.g., crouching, hopping, bending, and motion transitions). We propose a video-conditioned recurrent control technique to forecast physically-valid and stable future motions of arbitrary length. We also introduce a value function based fail-safe mechanism which enables our method to run as a single pass algorithm over the video data. Experiments with both controlled and in-the-wild data show that our approach outperforms previous art in both quantitative metrics and visual quality of the motions, and is also robust enough to transfer directly to real-world scenarios. Additionally, our time analysis shows that the combined use of our pose estimation and forecasting can run at 30 FPS, making it suitable for real-time applications.",0
"Our proposal suggests using reinforcement learning (RL) to teach a proportional-derivative (PD) control based policy for the purpose of estimating and predicting 3D human pose from egocentric videos. This method can learn directly from unsegmented egocentric videos and motion capture data, which includes a variety of complex human motions, such as bending, hopping, crouching, and motion transitions. We also introduce a video-conditioned recurrent control technique to predict future motions of arbitrary length that are stable and physically valid. Moreover, to ensure our method runs as a single pass algorithm over the video data, we include a fail-safe mechanism based on the value function. Our experiments on controlled and in-the-wild data demonstrate that our approach surpasses previous methods in quantitative metrics and visual quality of the motions. Additionally, our method can transfer directly to real-world scenarios and run at 30 FPS, making it suitable for real-time applications.",1
"Training deep reinforcement learning agents complex behaviors in 3D virtual environments requires significant computational resources. This is especially true in environments with high degrees of aliasing, where many states share nearly identical visual features. Minecraft is an exemplar of such an environment. We hypothesize that interactive machine learning IML, wherein human teachers play a direct role in training through demonstrations, critique, or action advice, may alleviate agent susceptibility to aliasing. However, interactive machine learning is only practical when the number of human interactions is limited, requiring a balance between human teacher effort and agent performance. We conduct experiments with two reinforcement learning algorithms which enable human teachers to give action advice, Feedback Arbitration and Newtonian Action Advice, under visual aliasing conditions. To assess potential cognitive load per advice type, we vary the accuracy and frequency of various human action advice techniques. Training efficiency, robustness against infrequent and inaccurate advisor input, and sensitivity to aliasing are examined.",0
"To train deep reinforcement learning agents on complex behaviors in 3D virtual environments that have high degrees of aliasing, significant computational resources are needed. Minecraft is an example of such an environment. It is believed that interactive machine learning (IML) can help reduce the agent's susceptibility to aliasing by involving human teachers in the training process through demonstrations, critique, or action advice. However, the practicality of IML is limited by the number of human interactions required, and a balance must be struck between human teacher effort and agent performance. This study investigates the effectiveness of two reinforcement learning algorithms, Feedback Arbitration and Newtonian Action Advice, in reducing aliasing under visual aliasing conditions. The study also examines the potential cognitive load of different types of human action advice by varying their accuracy and frequency, and assesses the training efficiency, robustness against infrequent and inaccurate advisor input, and sensitivity to aliasing.",1
"Video Recognition has drawn great research interest and great progress has been made. A suitable frame sampling strategy can improve the accuracy and efficiency of recognition. However, mainstream solutions generally adopt hand-crafted frame sampling strategies for recognition. It could degrade the performance, especially in untrimmed videos, due to the variation of frame-level saliency. To this end, we concentrate on improving untrimmed video classification via developing a learning-based frame sampling strategy. We intuitively formulate the frame sampling procedure as multiple parallel Markov decision processes, each of which aims at picking out a frame/clip by gradually adjusting an initial sampling. Then we propose to solve the problems with multi-agent reinforcement learning (MARL). Our MARL framework is composed of a novel RNN-based context-aware observation network which jointly models context information among nearby agents and historical states of a specific agent, a policy network which generates the probability distribution over a predefined action space at each step and a classification network for reward calculation as well as final recognition. Extensive experimental results show that our MARL-based scheme remarkably outperforms hand-crafted strategies with various 2D and 3D baseline methods. Our single RGB model achieves a comparable performance of ActivityNet v1.3 champion submission with multi-modal multi-model fusion and new state-of-the-art results on YouTube Birds and YouTube Cars.",0
"The study of Video Recognition has garnered significant attention and progress has been made in this field. Enhancing the accuracy and effectiveness of recognition can be achieved by implementing an appropriate frame sampling strategy. However, conventional solutions typically rely on pre-designed frame sampling strategies for recognition, which may lead to suboptimal performance, particularly in untrimmed videos due to the varying level of frame saliency. Our focus is on enhancing untrimmed video classification by implementing a learning-based frame sampling approach. We have formulated the frame sampling process as multiple parallel Markov decision processes, with the aim of gradually adjusting an initial sampling to select a frame/clip. We have developed a multi-agent reinforcement learning (MARL) framework to address these issues. Our MARL framework comprises a novel RNN-based context-aware observation network that models context information among nearby agents and historical states of a specific agent, a policy network that generates the probability distribution over a predefined action space at each step, and a classification network for reward calculation and final recognition. Our extensive experimental results demonstrate that our MARL-based approach outperforms conventional strategies using various 2D and 3D baseline methods. Our single RGB model achieves comparable performance to ActivityNet v1.3 champion submission with multi-modal multi-model fusion and new state-of-the-art results on YouTube Birds and YouTube Cars datasets.",1
"Future robots should follow human social norms in order to be useful and accepted in human society. In this paper, we leverage already existing social knowledge in human societies by capturing it in our framework through the notion of social norms. We show how norms can be used to guide a reinforcement learning agent towards achieving normative behavior and apply the same set of norms over different domains. Thus, we are able to: (1) provide a way to intuitively encode social knowledge (through norms); (2) guide learning towards normative behaviors (through an automatic norm reward system); and (3) achieve a transfer of learning by abstracting policies; Finally, (4) the method is not dependent on a particular RL algorithm. We show how our approach can be seen as a means to achieve abstract representation and learn procedural knowledge based on the declarative semantics of norms and discuss possible implications of this in some areas of cognitive science.",0
"To ensure usefulness and acceptance in human society, future robots ought to conform to human social norms. Our framework captures this social knowledge by utilizing the concept of social norms, which can guide a reinforcement learning agent towards normative behavior and can be applied across different domains. Our approach allows for intuitive encoding of social knowledge, automatic norm reward systems to guide learning, and transfer of learning through policy abstraction. Additionally, our method is not reliant on a specific RL algorithm. We propose that our approach can enable abstract representation and procedural knowledge acquisition based on declarative semantics of norms, which could have implications in cognitive science.",1
"Language systems have been of great interest to the research community and have recently reached the mass market through various assistant platforms on the web. Reinforcement Learning methods that optimize dialogue policies have seen successes in past years and have recently been extended into methods that personalize the dialogue, e.g. take the personal context of users into account. These works, however, are limited to personalization to a single user with whom they require multiple interactions and do not generalize the usage of context across users. This work introduces a problem where a generalized usage of context is relevant and proposes two Reinforcement Learning (RL)-based approaches to this problem. The first approach uses a single learner and extends the traditional POMDP formulation of dialogue state with features that describe the user context. The second approach segments users by context and then employs a learner per context. We compare these approaches in a benchmark of existing non-RL and RL-based methods in three established and one novel application domain of financial product recommendation. We compare the influence of context and training experiences on performance and find that learning approaches generally outperform a handcrafted gold standard.",0
"The research community has shown much interest in language systems, which have become widely available through various web-based assistant platforms. Reinforcement Learning methods have been successful in optimizing dialogue policies, and more recently, in personalizing dialogue based on user context. However, current methods are limited to personalizing for a single user and do not generalize context usage across multiple users. This study proposes two Reinforcement Learning-based approaches to address this issue, including a single learner with user context features and multiple learners segmented by context. The study compares these approaches with existing non-RL and RL-based methods in financial product recommendation. The results show that learning approaches generally outperform handcrafted methods, and context and training experiences significantly influence performance.",1
"Control policies, trained using the Deep Reinforcement Learning, have been recently shown to be vulnerable to adversarial attacks introducing even very small perturbations to the policy input. The attacks proposed so far have been designed using heuristics, and build on existing adversarial example crafting techniques used to dupe classifiers in supervised learning. In contrast, this paper investigates the problem of devising optimal attacks, depending on a well-defined attacker's objective, e.g., to minimize the main agent average reward. When the policy and the system dynamics, as well as rewards, are known to the attacker, a scenario referred to as a white-box attack, designing optimal attacks amounts to solving a Markov Decision Process. For what we call black-box attacks, where neither the policy nor the system is known, optimal attacks can be trained using Reinforcement Learning techniques. Through numerical experiments, we demonstrate the efficiency of our attacks compared to existing attacks (usually based on Gradient methods). We further quantify the potential impact of attacks and establish its connection to the smoothness of the policy under attack. Smooth policies are naturally less prone to attacks (this explains why Lipschitz policies, with respect to the state, are more resilient). Finally, we show that from the main agent perspective, the system uncertainties and the attacker can be modeled as a Partially Observable Markov Decision Process. We actually demonstrate that using Reinforcement Learning techniques tailored to POMDP (e.g. using Recurrent Neural Networks) leads to more resilient policies.",0
"Recently, it has been discovered that control policies developed through Deep Reinforcement Learning are susceptible to adversarial attacks. Even the slightest modifications to the policy input can result in these attacks. Current adversarial attack techniques rely on heuristics and are based on existing methods used to deceive classifiers in supervised learning. In contrast, this study focuses on developing optimal attacks based on a well-defined objective, such as minimizing the average reward of the main agent. If the attacker has access to the policy, system dynamics, and rewards, referred to as a white-box attack, optimal attacks can be devised by solving a Markov Decision Process. However, for black-box attacks, where neither the policy nor the system is known, Reinforcement Learning techniques can be used to train optimal attacks. Through numerical experiments, it is shown that these attacks are more efficient than existing attacks, which are typically based on Gradient methods. The potential impact of attacks is also quantified, and it is found that the smoothness of the policy under attack is a key factor in its vulnerability. Additionally, it is shown that the system uncertainties and the attacker can be modeled as a Partially Observable Markov Decision Process from the main agent's perspective. Finally, it is demonstrated that using Reinforcement Learning techniques tailored to POMDP, such as Recurrent Neural Networks, leads to more resilient policies.",1
"We consider the problem of learning to behave optimally in a Markov Decision Process when a reward function is not specified, but instead we have access to a set of demonstrators of varying performance. We assume the demonstrators are classified into one of k ranks, and use ideas from ordinal regression to find a reward function that maximizes the margin between the different ranks. This approach is based on the idea that agents should not only learn how to behave from experts, but also how not to behave from non-experts. We show there are MDPs where important differences in the reward function would be hidden from existing algorithms by the behaviour of the expert. Our method is particularly useful for problems where we have access to a large set of agent behaviours with varying degrees of expertise (such as through GPS or cellphones). We highlight the differences between our approach and existing methods using a simple grid domain and demonstrate its efficacy on determining passenger-finding strategies for taxi drivers, using a large dataset of GPS trajectories.",0
"The problem we address is how to learn optimal behavior in a Markov Decision Process without a specified reward function, but with access to a group of demonstrators who exhibit different levels of performance. We assume that these demonstrators can be classified into k ranks and we use ideas from ordinal regression to determine a reward function that maximizes the margin between these ranks. Our approach is based on the concept that agents must learn not only from experts but also from non-experts how not to behave. We prove that some MDPs have important differences in the reward function which are hidden by expert behavior and that our method is especially useful for problems where there exists a large dataset of agent behaviors with varying levels of expertise, such as through GPS or cellphones. We compare our approach to existing methods using a simple grid domain and demonstrate its effectiveness in determining passenger-finding strategies for taxi drivers, using a large dataset of GPS trajectories.",1
"We describe an application of Wasserstein distance to Reinforcement Learning. The Wasserstein distance in question is between the distribution of mappings of trajectories of a policy into some metric space, and some other fixed distribution (which may, for example, come from another policy). Different policies induce different distributions, so given an underlying metric, the Wasserstein distance quantifies how different policies are. This can be used to learn multiple polices which are different in terms of such Wasserstein distances by using a Wasserstein regulariser. Changing the sign of the regularisation parameter, one can learn a policy for which its trajectory mapping distribution is attracted to a given fixed distribution.",0
"In this article, we explore the application of the Wasserstein distance to Reinforcement Learning. Specifically, we examine the distance between the distribution of trajectory mappings of a policy into a metric space and another fixed distribution (which may be derived from another policy). Since different policies create distinct distributions, the Wasserstein distance provides a measure of policy dissimilarity based on a given metric. To learn multiple policies with varying Wasserstein distances, we employ a Wasserstein regularizer. By adjusting the sign of the regularizer, it's possible to train a policy that attracts its trajectory mapping distribution to a specific fixed distribution.",1
"Reinforcement learning agents are prone to undesired behaviors due to reward mis-specification. Finding a set of reward functions to properly guide agent behaviors is particularly challenging in multi-agent scenarios. Inverse reinforcement learning provides a framework to automatically acquire suitable reward functions from expert demonstrations. Its extension to multi-agent settings, however, is difficult due to the more complex notions of rational behaviors. In this paper, we propose MA-AIRL, a new framework for multi-agent inverse reinforcement learning, which is effective and scalable for Markov games with high-dimensional state-action space and unknown dynamics. We derive our algorithm based on a new solution concept and maximum pseudolikelihood estimation within an adversarial reward learning framework. In the experiments, we demonstrate that MA-AIRL can recover reward functions that are highly correlated with ground truth ones, and significantly outperforms prior methods in terms of policy imitation.",0
"Reward mis-specification can lead to undesired behaviors in reinforcement learning agents, which is particularly challenging in multi-agent scenarios. Inverse reinforcement learning can automatically acquire suitable reward functions from expert demonstrations, but its extension to multi-agent settings is difficult due to the more complex notions of rational behaviors. To address this, we propose MA-AIRL, a new framework for multi-agent inverse reinforcement learning that is effective and scalable for Markov games with high-dimensional state-action space and unknown dynamics. Our algorithm is based on a new solution concept and maximum pseudolikelihood estimation within an adversarial reward learning framework. In experiments, we show that MA-AIRL can recover highly correlated reward functions with ground truth ones and outperforms prior methods in terms of policy imitation.",1
"In several reinforcement learning (RL) scenarios, mainly in security settings, there may be adversaries trying to interfere with the reward generating process. In this paper, we introduce Threatened Markov Decision Processes (TMDPs), which provide a framework to support a decision maker against a potential adversary in RL. Furthermore, we propose a level-$k$ thinking scheme resulting in a new learning framework to deal with TMDPs. After introducing our framework and deriving theoretical results, relevant empirical evidence is given via extensive experiments, showing the benefits of accounting for adversaries while the agent learns.",0
"The reward generating process in various reinforcement learning (RL) situations, particularly in security contexts, may face interference from adversaries. This paper presents Threatened Markov Decision Processes (TMDPs) as a framework for assisting decision makers in RL against potential adversaries. Additionally, a level-$k$ thinking scheme is suggested, which leads to a novel learning framework for handling TMDPs. Following the introduction of this framework and the derivation of theoretical outcomes, extensive experiments are conducted to provide relevant empirical evidence, demonstrating the advantages of considering adversaries during the agent's learning process.",1
"In many optimization problems in wireless communications, the expressions of objective function or constraints are hard or even impossible to derive, which makes the solutions difficult to find. In this paper, we propose a model-free learning framework to solve constrained optimization problems without the supervision of the optimal solution. Neural networks are used respectively for parameterizing the function to be optimized, parameterizing the Lagrange multiplier associated with instantaneous constraints, and approximating the unknown objective function or constraints. We provide learning algorithms to train all the neural networks simultaneously, and reveal the connections of the proposed framework with reinforcement learning. Numerical and simulation results validate the proposed framework and demonstrate the efficiency of model-free learning by taking power control problem as an example.",0
"The task of finding solutions to optimization problems in wireless communications can be challenging due to the difficulty or impossibility of deriving expressions for objective functions or constraints. This paper introduces a learning framework that does not require supervision of the optimal solution to solve constrained optimization problems. Neural networks are used to parameterize the function to be optimized, the Lagrange multiplier associated with instantaneous constraints, and to approximate the unknown objective function or constraints. The paper includes learning algorithms to train all the neural networks simultaneously and demonstrates the framework's connection to reinforcement learning. The proposed framework is validated through numerical and simulation results that showcase the effectiveness of model-free learning, with the power control problem serving as an example.",1
"Modern control theories such as systems engineering approaches try to solve nonlinear system problems by revelation of causal relationship or co-relationship among the components; most of those approaches focus on control of sophisticatedly modeled white-boxed systems. We suggest an application of actor-critic reinforcement learning approach to control a nonlinear, complex and black-boxed system. We demonstrated this approach on artificial green-house environment simulator all of whose control inputs have several side effects so human cannot figure out how to control this system easily. Our approach succeeded to maintain the circumstance at least 20 times longer than PID and Deep Q Learning.",0
"Contemporary control theories, like systems engineering techniques, aim to resolve nonlinear system issues by revealing causal or co-relationships among components. However, these methods primarily focus on regulating sophisticatedly modeled white-boxed systems. In contrast, we propose implementing an actor-critic reinforcement learning approach to manage a nonlinear, highly intricate, and black-boxed system. To demonstrate the efficacy of this approach, we employed it on an artificial greenhouse environment simulator with control inputs that have multiple side effects, making it challenging for humans to control. Our technique proved successful in sustaining the environment for a minimum of 20 times longer than both PID and Deep Q Learning methods.",1
"The sample inefficiency of standard deep reinforcement learning methods precludes their application to many real-world problems. Methods which leverage human demonstrations require fewer samples but have been researched less. As demonstrated in the computer vision and natural language processing communities, large-scale datasets have the capacity to facilitate research by serving as an experimental and benchmarking platform for new methods. However, existing datasets compatible with reinforcement learning simulators do not have sufficient scale, structure, and quality to enable the further development and evaluation of methods focused on using human examples. Therefore, we introduce a comprehensive, large-scale, simulator-paired dataset of human demonstrations: MineRL. The dataset consists of over 60 million automatically annotated state-action pairs across a variety of related tasks in Minecraft, a dynamic, 3D, open-world environment. We present a novel data collection scheme which allows for the ongoing introduction of new tasks and the gathering of complete state information suitable for a variety of methods. We demonstrate the hierarchality, diversity, and scale of the MineRL dataset. Further, we show the difficulty of the Minecraft domain along with the potential of MineRL in developing techniques to solve key research challenges within it.",0
"Standard deep reinforcement learning methods are not suitable for many real-world problems due to their sample inefficiency. Although methods that utilize human demonstrations require fewer samples, they have not been extensively studied. To facilitate research and serve as a benchmarking platform for new methods, large-scale datasets have been used in computer vision and natural language processing communities. However, existing datasets compatible with reinforcement learning simulators lack sufficient scale, structure, and quality to develop and evaluate methods that focus on human examples. Therefore, we introduce MineRL, a comprehensive, large-scale dataset of human demonstrations in Minecraft. With over 60 million automatically annotated state-action pairs across various tasks, MineRL offers a novel data collection scheme that allows for ongoing task introduction and complete state information gathering. We demonstrate the hierarchical, diverse, and scalable nature of MineRL and showcase its potential in developing techniques to solve key research challenges in the challenging Minecraft domain.",1
"The automatic generation of radiology reports given medical radiographs has significant potential to operationally and improve clinical patient care. A number of prior works have focused on this problem, employing advanced methods from computer vision and natural language generation to produce readable reports. However, these works often fail to account for the particular nuances of the radiology domain, and, in particular, the critical importance of clinical accuracy in the resulting generated reports. In this work, we present a domain-aware automatic chest X-ray radiology report generation system which first predicts what topics will be discussed in the report, then conditionally generates sentences corresponding to these topics. The resulting system is fine-tuned using reinforcement learning, considering both readability and clinical accuracy, as assessed by the proposed Clinically Coherent Reward. We verify this system on two datasets, Open-I and MIMIC-CXR, and demonstrate that our model offers marked improvements on both language generation metrics and CheXpert assessed accuracy over a variety of competitive baselines.",0
"The automatic creation of radiology reports from medical radiographs has the potential to significantly improve patient care and operational efficiency. While previous research has utilized advanced techniques from computer vision and natural language generation to generate readable reports, many fail to consider the unique nuances of the radiology field, particularly the importance of clinical accuracy in the final product. In this study, we present a domain-specific system for generating chest X-ray radiology reports that predicts report topics and generates corresponding sentences. Our system is fine-tuned using reinforcement learning, taking into account both readability and clinical accuracy as evaluated by the Clinically Coherent Reward. We evaluated our model on two datasets, Open-I and MIMIC-CXR, and found that it outperformed other models in both language generation and CheXpert accuracy.",1
"How to best explore in domains with sparse, delayed, and deceptive rewards is an important open problem for reinforcement learning (RL). This paper considers one such domain, the recently-proposed multi-agent benchmark of Pommerman. This domain is very challenging for RL --- past work has shown that model-free RL algorithms fail to achieve significant learning without artificially reducing the environment's complexity. In this paper, we illuminate reasons behind this failure by providing a thorough analysis on the hardness of random exploration in Pommerman. While model-free random exploration is typically futile, we develop a model-based automatic reasoning module that can be used for safer exploration by pruning actions that will surely lead the agent to death. We empirically demonstrate that this module can significantly improve learning.",0
"Reinforcement learning (RL) faces a significant challenge when exploring domains that offer sparse, delayed, and deceptive rewards. To address this issue, this paper focuses on the Pommerman multi-agent benchmark, which has proven to be particularly difficult for RL, with previous attempts at model-free RL failing to achieve significant learning without simplifying the environment. This paper explores why this failure occurs and identifies the challenges of random exploration in Pommerman. The authors propose a model-based automatic reasoning module that can improve the safety and effectiveness of exploration by pruning actions that are likely to lead to the agent's death. Empirical results demonstrate the effectiveness of this module in improving learning.",1
"In recent years, \emph{search story}, a combined display with other organic channels, has become a major source of user traffic on platforms such as e-commerce search platforms, news feed platforms and web and image search platforms. The recommended search story guides a user to identify her own preference and personal intent, which subsequently influences the user's real-time and long-term search behavior. %With such an increased importance of search stories, As search stories become increasingly important, in this work, we study the problem of personalized search story recommendation within a search engine, which aims to suggest a search story relevant to both a search keyword and an individual user's interest. To address the challenge of modeling both immediate and future values of recommended search stories (i.e., cross-channel effect), for which conventional supervised learning framework is not applicable, we resort to a Markov decision process and propose a deep reinforcement learning architecture trained by both imitation learning and reinforcement learning. We empirically demonstrate the effectiveness of our proposed approach through extensive experiments on real-world data sets from JD.com.",0
"Over the past few years, search story has emerged as a significant source of user traffic on various platforms such as e-commerce, news feeds, and web and image search platforms. The recommended search story helps users identify their preferences and intentions, thereby influencing their real-time and long-term search behavior. Given the growing importance of search stories, this study focuses on personalized search story recommendation within a search engine. The aim is to suggest a search story that aligns with both a user's interest and their search keyword. To overcome the challenge of modeling immediate and future values of recommended search stories, a Markov decision process and deep reinforcement learning architecture trained by imitation and reinforcement learning are proposed. Extensive experiments on real-world data sets from JD.com demonstrate the effectiveness of this approach.",1
"Deep reinforcement learning is prone to overfitting, and traditional benchmarks such as Atari 2600 benchmark can exacerbate this problem. The Obstacle Tower Challenge addresses this by using randomized environments and separate seeds for training, validation, and test runs. This paper examines various improvements and best practices to the PPO algorithm using the Obstacle Tower Challenge to empirically study their impact with regards to generalization. Our experiments show that the combination provides state-of-the-art performance on the Obstacle Tower Challenge.",0
"Overfitting is a common issue in deep reinforcement learning, which can be worsened by conventional benchmarks like the Atari 2600 benchmark. To tackle this problem, the Obstacle Tower Challenge utilizes randomized environments and distinct seeds for training, validation, and testing. This study explores different enhancements and optimal techniques for the PPO algorithm by analyzing their effectiveness in generalization through the Obstacle Tower Challenge. Our findings indicate that this approach yields exceptional performance on the Obstacle Tower Challenge.",1
"Deep reinforcement learning has achieved great successes in recent years, however, one main challenge is the sample inefficiency. In this paper, we focus on how to use action guidance by means of a non-expert demonstrator to improve sample efficiency in a domain with sparse, delayed, and possibly deceptive rewards: the recently-proposed multi-agent benchmark of Pommerman. We propose a new framework where even a non-expert simulated demonstrator, e.g., planning algorithms such as Monte Carlo tree search with a small number rollouts, can be integrated within asynchronous distributed deep reinforcement learning methods. Compared to a vanilla deep RL algorithm, our proposed methods both learn faster and converge to better policies on a two-player mini version of the Pommerman game.",0
"While deep reinforcement learning has seen remarkable achievements lately, it still faces the issue of sample inefficiency. This paper focuses on enhancing sample efficiency in a domain that has sparse, delayed, and potentially misleading rewards - the Pommerman multi-agent benchmark - by utilizing action guidance from a non-expert demonstrator. We present a novel framework that incorporates a non-expert simulated demonstrator, such as Monte Carlo tree search with a limited number of rollouts, into asynchronous distributed deep reinforcement learning techniques. Our proposed approaches outperform vanilla deep RL algorithms in terms of faster learning and better policy convergence on a two-player mini version of the Pommerman game.",1
"In many real-world decision making problems, reaching an optimal decision requires taking into account a variable number of objects around the agent. Autonomous driving is a domain in which this is especially relevant, since the number of cars surrounding the agent varies considerably over time and affects the optimal action to be taken. Classical methods that process object lists can deal with this requirement. However, to take advantage of recent high-performing methods based on deep reinforcement learning in modular pipelines, special architectures are necessary. For these, a number of options exist, but a thorough comparison of the different possibilities is missing. In this paper, we elaborate limitations of fully-connected neural networks and other established approaches like convolutional and recurrent neural networks in the context of reinforcement learning problems that have to deal with variable sized inputs. We employ the structure of Deep Sets in off-policy reinforcement learning for high-level decision making, highlight their capabilities to alleviate these limitations, and show that Deep Sets not only yield the best overall performance but also offer better generalization to unseen situations than the other approaches.",0
"When making decisions in the real world, it's often important to consider multiple objects around the agent. This is especially true in autonomous driving, where the number of surrounding cars can change and impact the best course of action. While traditional methods can handle object lists, newer deep reinforcement learning methods require special architectures. There are several options available, but there hasn't been a comprehensive comparison. This paper examines the limitations of established neural networks in reinforcement learning problems with variable inputs. We use Deep Sets in off-policy reinforcement learning for high-level decision making, highlighting their ability to overcome these limitations. Our results show that Deep Sets perform the best overall and are better at generalizing to new situations than other approaches.",1
"Deep reinforcement learning has achieved great successes in recent years, but there are still open challenges, such as convergence to locally optimal policies and sample inefficiency. In this paper, we contribute a novel self-supervised auxiliary task, i.e., Terminal Prediction (TP), estimating temporal closeness to terminal states for episodic tasks. The intuition is to help representation learning by letting the agent predict how close it is to a terminal state, while learning its control policy. Although TP could be integrated with multiple algorithms, this paper focuses on Asynchronous Advantage Actor-Critic (A3C) and demonstrating the advantages of A3C-TP. Our extensive evaluation includes: a set of Atari games, the BipedalWalker domain, and a mini version of the recently proposed multi-agent Pommerman game. Our results on Atari games and the BipedalWalker domain suggest that A3C-TP outperforms standard A3C in most of the tested domains and in others it has similar performance. In Pommerman, our proposed method provides significant improvement both in learning efficiency and converging to better policies against different opponents.",0
"Despite the recent successes of deep reinforcement learning, there are still challenges to overcome, including issues with policies converging to locally optimal solutions and sample inefficiency. This paper introduces a new self-supervised auxiliary task called Terminal Prediction (TP), which estimates the proximity to terminal states for episodic tasks. The aim is to improve representation learning by allowing the agent to predict how close it is to a terminal state while learning its control policy. Although TP can be used with multiple algorithms, this paper focuses on demonstrating the advantages of A3C-TP, which is an integration of TP with Asynchronous Advantage Actor-Critic (A3C). An extensive evaluation was carried out on a variety of domains, including Atari games, the BipedalWalker domain, and a mini version of the multi-agent Pommerman game. The results show that A3C-TP outperforms standard A3C in most of the tested domains, and in others, it has similar performance. In Pommerman, the proposed method significantly improves learning efficiency and helps to converge to better policies against different opponents.",1
"Decision support systems (e.g., for ecological conservation) and autonomous systems (e.g., adaptive controllers in smart cities) start to be deployed in real applications. Although their operations often impact many users or stakeholders, no fairness consideration is generally taken into account in their design, which could lead to completely unfair outcomes for some users or stakeholders. To tackle this issue, we advocate for the use of social welfare functions that encode fairness and present this general novel problem in the context of (deep) reinforcement learning, although it could possibly be extended to other machine learning tasks.",0
"Real-world applications are beginning to utilize decision support systems, such as those employed in ecological conservation, and autonomous systems, such as adaptive controllers in smart cities. Unfortunately, these systems often affect numerous users or stakeholders, yet fairness is not typically factored into their design. This oversight could result in unjust outcomes for certain individuals or groups. To address this concern, we propose implementing social welfare functions that incorporate fairness considerations. Specifically, we explore this issue in the realm of (deep) reinforcement learning, though it may have potential applications in other machine learning tasks.",1
"Deep reinforcement learning (DRL) has achieved great success in various applications. However, recent studies show that machine learning models are vulnerable to adversarial attacks. DRL models have been attacked by adding perturbations to observations. While such observation based attack is only one aspect of potential attacks on DRL, other forms of attacks which are more practical require further analysis, such as manipulating environment dynamics. Therefore, we propose to understand the vulnerabilities of DRL from various perspectives and provide a thorough taxonomy of potential attacks. We conduct the first set of experiments on the unexplored parts within the taxonomy. In addition to current observation based attacks against DRL, we propose the first targeted attacks based on action space and environment dynamics. We also introduce the online sequential attacks based on temporal consistency information among frames. To better estimate gradient in black-box setting, we propose a sampling strategy and theoretically prove its efficiency and estimation error bound. We conduct extensive experiments to compare the effectiveness of different attacks with several baselines in various environments, including game playing, robotics control, and autonomous driving.",0
"DRL has been successful in many applications, but recent studies have revealed that adversarial attacks can harm machine learning models. DRL models have been attacked through observation-based perturbations, but other forms of attacks that are more practical need further analysis, such as altering environment dynamics. We propose to thoroughly understand DRL's vulnerabilities and provide a comprehensive taxonomy of potential attacks. Our experiments cover unexplored areas within the taxonomy, including targeted attacks based on action space and environment dynamics, as well as online sequential attacks based on temporal consistency. We also present a sampling strategy that can better estimate gradient in a black-box setting. Our extensive experiments compare different attacks with baselines in various environments, such as game playing, robotics control, and autonomous driving, to assess their effectiveness.",1
"Robust Markov Decision Processes (RMDPs) intend to ensure robustness with respect to changing or adversarial system behavior. In this framework, transitions are modeled as arbitrary elements of a known and properly structured uncertainty set and a robust optimal policy can be derived under the worst-case scenario. In this study, we address the issue of learning in RMDPs using a Bayesian approach. We introduce the Uncertainty Robust Bellman Equation (URBE) which encourages safe exploration for adapting the uncertainty set to new observations while preserving robustness. We propose a URBE-based algorithm, DQN-URBE, that scales this method to higher dimensional domains. Our experiments show that the derived URBE-based strategy leads to a better trade-off between less conservative solutions and robustness in the presence of model misspecification. In addition, we show that the DQN-URBE algorithm can adapt significantly faster to changing dynamics online compared to existing robust techniques with fixed uncertainty sets.",0
"The aim of Robust Markov Decision Processes (RMDPs) is to maintain stability when faced with changes or hostile behavior in the system. The model represents transitions as arbitrary elements of an established and structured uncertainty set, allowing for the derivation of a robust optimal policy for worst-case scenarios. This research explores the concept of learning in RMDPs, utilizing a Bayesian methodology. The Uncertainty Robust Bellman Equation (URBE) is introduced, encouraging safe exploration for adapting the uncertainty set to new observations while maintaining robustness. A URBE-based algorithm, DQN-URBE, is proposed, which can be scaled to higher dimensional domains. The experimental results suggest that the derived URBE-based strategy offers a better balance between less conservative solutions and robustness in the presence of model misspecification. Furthermore, the DQN-URBE algorithm has shown to adapt much faster to changing dynamics online, in comparison to existing robust techniques with fixed uncertainty sets.",1
"The detection of anatomical landmarks is a vital step for medical image analysis and applications for diagnosis, interpretation and guidance. Manual annotation of landmarks is a tedious process that requires domain-specific expertise and introduces inter-observer variability. This paper proposes a new detection approach for multiple landmarks based on multi-agent reinforcement learning. Our hypothesis is that the position of all anatomical landmarks is interdependent and non-random within the human anatomy, thus finding one landmark can help to deduce the location of others. Using a Deep Q-Network (DQN) architecture we construct an environment and agent with implicit inter-communication such that we can accommodate K agents acting and learning simultaneously, while they attempt to detect K different landmarks. During training the agents collaborate by sharing their accumulated knowledge for a collective gain. We compare our approach with state-of-the-art architectures and achieve significantly better accuracy by reducing the detection error by 50%, while requiring fewer computational resources and time to train compared to the naive approach of training K agents separately.",0
"The identification of anatomical landmarks plays a crucial role in medical image analysis and applications such as diagnosis, interpretation, and guidance. However, manually tagging landmarks is a laborious task that demands specialized knowledge and introduces discrepancies between different observers. This study introduces a new approach for detecting multiple landmarks by utilizing multi-agent reinforcement learning. The hypothesis is that the position of all anatomical landmarks is interdependent and non-random within the human body, meaning that locating one landmark can assist in identifying the location of others. To achieve this, we employ a Deep Q-Network (DQN) architecture to create an environment and agent capable of accommodating K agents working simultaneously to detect K different landmarks. During training, the agents collaborate and share their knowledge for collective gain. Compared to the current state-of-the-art approaches, our method yields significantly better accuracy by reducing the detection error by 50% and requires fewer computational resources and training time than the conventional approach of training K agents separately.",1
"Owe to the recent advancements in Artificial Intelligence especially deep learning, many data-driven decision support systems have been implemented to facilitate medical doctors in delivering personalized care. We focus on the deep reinforcement learning (DRL) models in this paper. DRL models have demonstrated human-level or even superior performance in the tasks of computer vision and game playings, such as Go and Atari game. However, the adoption of deep reinforcement learning techniques in clinical decision optimization is still rare. We present the first survey that summarizes reinforcement learning algorithms with Deep Neural Networks (DNN) on clinical decision support. We also discuss some case studies, where different DRL algorithms were applied to address various clinical challenges. We further compare and contrast the advantages and limitations of various DRL algorithms and present a preliminary guide on how to choose the appropriate DRL algorithm for particular clinical applications.",0
"Recent developments in Artificial Intelligence, particularly in deep learning, have led to the implementation of data-driven decision support systems that aid medical doctors in providing personalized care. This paper specifically examines deep reinforcement learning (DRL) models, which have shown human-level or superior performance in computer vision and game playing tasks like Atari games and Go. However, the use of DRL techniques in clinical decision optimization is still uncommon. This paper presents the first survey summarizing reinforcement learning algorithms that use Deep Neural Networks (DNN) for clinical decision support. Additionally, it discusses several case studies that apply different DRL algorithms to tackle various clinical challenges. The paper also compares and contrasts the advantages and limitations of different DRL algorithms and offers a preliminary guide on selecting an appropriate DRL algorithm for specific clinical applications.",1
"Reinforcement learning (RL) constitutes a promising solution for alleviating the problem of traffic congestion. In particular, deep RL algorithms have been shown to produce adaptive traffic signal controllers that outperform conventional systems. However, in order to be reliable in highly dynamic urban areas, such controllers need to be robust with the respect to a series of exogenous sources of uncertainty. In this paper, we develop an open-source callback-based framework for promoting the flexible evaluation of different deep RL configurations under a traffic simulation environment. With this framework, we investigate how deep RL-based adaptive traffic controllers perform under different scenarios, namely under demand surges caused by special events, capacity reductions from incidents and sensor failures. We extract several key insights for the development of robust deep RL algorithms for traffic control and propose concrete designs to mitigate the impact of the considered exogenous uncertainties.",0
"The use of reinforcement learning (RL) is a promising solution to address traffic congestion. Specifically, deep RL algorithms have demonstrated superior performance in developing adaptive traffic signal controllers compared to traditional systems. Nonetheless, to ensure reliability in highly dynamic urban areas, such controllers must be resistant to various external sources of uncertainty. This article presents an open-source callback-based framework to facilitate the flexible evaluation of different deep RL configurations under a traffic simulation environment. Using this framework, we examine the performance of deep RL-based adaptive traffic controllers in various scenarios, such as demand surges from special events, capacity reductions from incidents, and sensor failures. We derive several valuable insights for building robust deep RL algorithms for traffic control and suggest specific designs to minimize the impact of the aforementioned external uncertainties.",1
"Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a machine learning problem. It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience. In this paper, we review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperparameter learning (AutoMHL), and automated deep learning (AutoDL). State-of-the-art techniques adopted in the three categories are presented, including Bayesian optimization, reinforcement learning, evolutionary algorithm, and gradient-based approaches. We summarize popular AutoML frameworks and conclude with current open challenges of AutoML.",0
"The objective of Automated Machine Learning (AutoML) is to discover the most efficient machine learning solutions automatically based on the machine learning problem. This technology can alleviate the tedious manual tuning process for data scientists and permit domain experts to utilize ready-made machine learning solutions without extensive expertise. This article evaluates the current advancements in AutoML in three categories: Automated Feature Engineering (AutoFE), Automated Model and Hyperparameter Learning (AutoMHL), and Automated Deep Learning (AutoDL). The paper highlights the cutting-edge techniques employed in these categories, such as Bayesian optimization, reinforcement learning, evolutionary algorithm, and gradient-based approaches. Additionally, it outlines commonly used AutoML frameworks and concludes with the existing challenges in AutoML.",1
"This paper augments the reward received by a reinforcement learning agent with potential functions in order to help the agent learn (possibly stochastic) optimal policies. We show that a potential-based reward shaping scheme is able to preserve optimality of stochastic policies, and demonstrate that the ability of an agent to learn an optimal policy is not affected when this scheme is augmented to soft Q-learning. We propose a method to impart potential based advice schemes to policy gradient algorithms. An algorithm that considers an advantage actor-critic architecture augmented with this scheme is proposed, and we give guarantees on its convergence. Finally, we evaluate our approach on a puddle-jump grid world with indistinguishable states, and the continuous state and action mountain car environment from classical control. Our results indicate that these schemes allow the agent to learn a stochastic optimal policy faster and obtain a higher average reward.",0
"In order to assist a reinforcement learning agent in learning optimal policies, this study incorporates potential functions to enhance the agent's reward. Through the use of a potential-based reward shaping scheme, we establish that the optimality of stochastic policies is maintained. Furthermore, we demonstrate that the addition of this scheme to soft Q-learning does not affect the agent's ability to learn an optimal policy. We also propose a technique for introducing potential-based advice schemes to policy gradient algorithms, and present an algorithm featuring an advantage actor-critic architecture augmented with this scheme, which we guarantee will converge. Our approach is evaluated in a puddle-jump grid world with indistinguishable states, as well as the continuous state and action mountain car environment from classical control. Our findings indicate that these schemes facilitate the agent in quickly learning a stochastic optimal policy and achieving a higher average reward.",1
"We introduce Arena, a toolkit for multi-agent reinforcement learning (MARL) research. In MARL, it usually requires customizing observations, rewards and actions for each agent, changing cooperative-competitive agent-interaction, and playing with/against a third-party agent, etc. We provide a novel modular design, called Interface, for manipulating such routines in essentially two ways: 1) Different interfaces can be concatenated and combined, which extends the OpenAI Gym Wrappers concept to MARL scenarios. 2) During MARL training or testing, interfaces can be embedded in either wrapped OpenAI Gym compatible Environments or raw environment compatible Agents. We offer off-the-shelf interfaces for several popular MARL platforms, including StarCraft II, Pommerman, ViZDoom, Soccer, etc. The interfaces effectively support self-play RL and cooperative-competitive hybrid MARL. Also, Arena can be conveniently extended to your own favorite MARL platform.",0
"Our toolkit, Arena, is designed for researching multi-agent reinforcement learning (MARL). In MARL, it's common to modify observations, rewards, and actions for each agent, adjust cooperative-competitive interactions, and incorporate a third-party agent. To simplify these processes, we've developed Interface, a modular design that can be used in two ways: 1) Different interfaces can be combined to expand the OpenAI Gym Wrappers concept to MARL scenarios. 2) Interfaces can be integrated into either wrapped OpenAI Gym compatible Environments or raw environment compatible Agents during MARL training or testing. We've also included ready-to-use interfaces for several popular MARL platforms, like StarCraft II, Pommerman, ViZDoom, and Soccer. These interfaces enable self-play RL and hybrid cooperative-competitive MARL. Additionally, Arena can be easily customized to suit your preferred MARL platform.",1
"In reinforcement learning algorithms, leveraging multiple views of the environment can improve the learning of complicated policies. In multi-view environments, due to the fact that the views may frequently suffer from partial observability, their level of importance are often different. In this paper, we propose a deep reinforcement learning method and an attention mechanism in a multi-view environment. Each view can provide various representative information about the environment. Through our attention mechanism, our method generates a single feature representation of environment given its multiple views. It learns a policy to dynamically attend to each view based on its importance in the decision-making process. Through experiments, we show that our method outperforms its state-of-the-art baselines on TORCS racing car simulator and three other complex 3D environments with obstacles. We also provide experimental results to evaluate the performance of our method on noisy conditions and partial observation settings.",0
"The learning of complex policies in reinforcement learning algorithms can be improved by utilizing multiple perspectives of the environment. However, the importance of each view in multi-view environments may differ due to partial observability. To address this, we introduce a novel approach that combines deep reinforcement learning and an attention mechanism in a multi-view environment. Our method generates a single feature representation by dynamically attending to each view based on its significance in decision-making. We demonstrate through experiments that our approach surpasses current state-of-the-art techniques on challenging 3D environments with obstacles, including the TORCS racing car simulator. We also evaluate the performance of our method in noisy and partially observable settings.",1
"Most known regret bounds for reinforcement learning are either episodic or assume an environment without traps. We derive a regret bound without making either assumption, by allowing the algorithm to occasionally delegate an action to an external advisor. We thus arrive at a setting of active one-shot model-based reinforcement learning that we call DRL (delegative reinforcement learning.) The algorithm we construct in order to demonstrate the regret bound is a variant of Posterior Sampling Reinforcement Learning supplemented by a subroutine that decides which actions should be delegated. The algorithm is not anytime, since the parameters must be adjusted according to the target time discount. Currently, our analysis is limited to Markov decision processes with finite numbers of hypotheses, states and actions.",0
"Episodic or trap-free environments are often assumed in known regret bounds for reinforcement learning. However, by occasionally allowing the algorithm to delegate an action to an external advisor, we have developed a regret bound without making either of these assumptions. This approach is known as DRL (delegative reinforcement learning) in the context of active one-shot model-based reinforcement learning. Our algorithm is a variant of Posterior Sampling Reinforcement Learning, with a subroutine that determines which actions should be delegated. It is important to note that the algorithm is not anytime, as the parameters must be adjusted based on the target time discount. Currently, our analysis is limited to Markov decision processes with finite numbers of hypotheses, states, and actions.",1
"Off-policy learning is more unstable compared to on-policy learning in reinforcement learning (RL). One reason for the instability of off-policy learning is a discrepancy between the target ($\pi$) and behavior (b) policy distributions. The discrepancy between $\pi$ and b distributions can be alleviated by employing a smooth variant of the importance sampling (IS), such as the relative importance sampling (RIS). RIS has parameter $\beta\in[0, 1]$ which controls smoothness. To cope with instability, we present the first relative importance sampling-off-policy actor-critic (RIS-Off-PAC) model-free algorithms in RL. In our method, the network yields a target policy (the actor), a value function (the critic) assessing the current policy ($\pi$) using samples drawn from behavior policy. We use action value generated from the behavior policy in reward function to train our algorithm rather than from the target policy. We also use deep neural networks to train both actor and critic. We evaluated our algorithm on a number of Open AI Gym benchmark problems and demonstrate better or comparable performance to several state-of-the-art RL baselines.",0
"Compared to on-policy learning in reinforcement learning (RL), off-policy learning is known to be more unstable. This instability is caused by a discrepancy between the target ($\pi$) and behavior (b) policy distributions. However, one solution to this problem is to use a smooth variant of importance sampling (IS), such as relative importance sampling (RIS), which has a parameter $\beta\in[0, 1]$ that controls smoothness. To address the instability issue, we propose the first model-free algorithm in RL called relative importance sampling-off-policy actor-critic (RIS-Off-PAC). Our approach utilizes a deep neural network to generate a target policy (the actor) and a value function (the critic) that assesses the current policy ($\pi$) using samples obtained from the behavior policy. Unlike other algorithms, we use action value generated from the behavior policy in the reward function to train our model. We evaluated our algorithm on several Open AI Gym benchmark problems and demonstrated better or comparable performance to other state-of-the-art RL baselines.",1
"Recently, reinforcement learning models have achieved great success, completing complex tasks such as mastering Go and other games with higher scores than human players. Many of these models collect considerable data on the tasks and improve accuracy by extracting visual and time-series features using convolutional neural networks (CNNs) and recurrent neural networks, respectively. However, these networks have very high computational costs because they need to be trained by repeatedly using a large volume of past playing data. In this study, we propose a novel practical approach called reinforcement learning with convolutional reservoir computing (RCRC) model. The RCRC model has several desirable features: 1. it can extract visual and time-series features very fast because it uses random fixed-weight CNN and the reservoir computing model; 2. it does not require the training data to be stored because it extracts features without training and decides action with evolution strategy. Furthermore, the model achieves state of the art score in the popular reinforcement learning task. Incredibly, we find the random weight-fixed simple networks like only one dense layer network can also reach high score in the RL task.",0
"In recent times, reinforcement learning models have accomplished remarkable feats in completing complex tasks such as excelling in Go and other games with scores surpassing those of human players. These models accumulate substantial data on the tasks and enhance accuracy by utilizing convolutional neural networks (CNNs) and recurrent neural networks to extract visual and time-series features, respectively. However, these networks come with a high computational cost due to the requirement of extensive training via a vast amount of historical playing data. This study proposes a practical and groundbreaking approach called reinforcement learning with convolutional reservoir computing (RCRC) model. This model has various desirable features, such as fast extraction of visual and time-series features through the use of random fixed-weight CNN and the reservoir computing model. Moreover, it does not demand the training data to be stored as it extracts features without training and adopts an evolution strategy to decide the action. Additionally, the model achieves state of the art scores in the popular reinforcement learning task. Interestingly, the study reveals that random weight-fixed simple networks, such as a single dense layer network, can also attain high scores in the RL task.",1
"An optimal feedback controller for a given Markov decision process (MDP) can in principle be synthesized by value or policy iteration. However, if the system dynamics and the reward function are unknown, a learning agent must discover an optimal controller via direct interaction with the environment. Such interactive data gathering commonly leads to divergence towards dangerous or uninformative regions of the state space unless additional regularization measures are taken. Prior works proposed bounding the information loss measured by the Kullback-Leibler (KL) divergence at every policy improvement step to eliminate instability in the learning dynamics. In this paper, we consider a broader family of $f$-divergences, and more concretely $\alpha$-divergences, which inherit the beneficial property of providing the policy improvement step in closed form at the same time yielding a corresponding dual objective for policy evaluation. Such entropic proximal policy optimization view gives a unified perspective on compatible actor-critic architectures. In particular, common least-squares value function estimation coupled with advantage-weighted maximum likelihood policy improvement is shown to correspond to the Pearson $\chi^2$-divergence penalty. Other actor-critic pairs arise for various choices of the penalty-generating function $f$. On a concrete instantiation of our framework with the $\alpha$-divergence, we carry out asymptotic analysis of the solutions for different values of $\alpha$ and demonstrate the effects of the divergence function choice on common standard reinforcement learning problems.",0
"Value or policy iteration can be used to synthesize an optimal feedback controller for a given Markov decision process (MDP). However, when the system dynamics and reward function are unknown, a learning agent must interact with the environment to discover an optimal controller. This can lead to divergence towards dangerous or uninformative regions of the state space without regularization measures. Previous works have proposed bounding the information loss with the Kullback-Leibler (KL) divergence at every policy improvement step to eliminate instability. This paper expands on this by considering a broader family of $f$-divergences, specifically the $\alpha$-divergences, which provide a closed form policy improvement step and a corresponding dual objective for policy evaluation. This results in a unified perspective on compatible actor-critic architectures, with least-squares value function estimation and advantage-weighted maximum likelihood policy improvement corresponding to the Pearson $\chi^2$-divergence penalty. Different choices of the penalty-generating function $f$ generate other actor-critic pairs. The paper carries out asymptotic analysis of the solutions for different values of $\alpha$ and demonstrates the effects of divergence function choice on standard reinforcement learning problems.",1
"Despite the tremendous advances that have been made in the last decade on developing useful machine-learning applications, their wider adoption has been hindered by the lack of strong assurance guarantees that can be made about their behavior. In this paper, we consider how formal verification techniques developed for traditional software systems can be repurposed for verification of reinforcement learning-enabled ones, a particularly important class of machine learning systems. Rather than enforcing safety by examining and altering the structure of a complex neural network implementation, our technique uses blackbox methods to synthesizes deterministic programs, simpler, more interpretable, approximations of the network that can nonetheless guarantee desired safety properties are preserved, even when the network is deployed in unanticipated or previously unobserved environments. Our methodology frames the problem of neural network verification in terms of a counterexample and syntax-guided inductive synthesis procedure over these programs. The synthesis procedure searches for both a deterministic program and an inductive invariant over an infinite state transition system that represents a specification of an application's control logic. Additional specifications defining environment-based constraints can also be provided to further refine the search space. Synthesized programs deployed in conjunction with a neural network implementation dynamically enforce safety conditions by monitoring and preventing potentially unsafe actions proposed by neural policies. Experimental results over a wide range of cyber-physical applications demonstrate that software-inspired formal verification techniques can be used to realize trustworthy reinforcement learning systems with low overhead.",0
"The lack of reliable guarantees about machine-learning applications has slowed their adoption, despite significant progress in their development over the last decade. In this paper, we explore the repurposing of formal verification techniques from traditional software systems to verify reinforcement learning-enabled systems. Rather than modifying the complex neural network structure, our technique synthesizes simpler and more interpretable deterministic programs that can ensure desired safety properties are maintained, even in unpredictable environments. Our methodology uses a counterexample and syntax-guided inductive synthesis process to search for a deterministic program and inductive invariant over an infinite state transition system, with further specifications available to refine the search. The synthesized programs work with a neural network implementation to monitor and prevent unsafe actions, resulting in trustworthy reinforcement learning systems with minimal overhead. Our experimental results demonstrate the effectiveness of software-inspired formal verification techniques in a range of cyber-physical applications.",1
"We present a training pipeline for the autonomous driving task given the current camera image and vehicle speed as the input to produce the throttle, brake, and steering control output. The simulator Airsim's convenient weather and lighting API provides a sufficient diversity during training which can be very helpful to increase the trained policy's robustness. In order to not limit the possible policy's performance, we use a continuous and deterministic control policy setting. We utilize ResNet-34 as our actor and critic networks with some slight changes in the fully connected layers. Considering human's mastery of this task and the high-complexity nature of this task, we first use imitation learning to mimic the given human policy and leverage the trained policy and its weights to the reinforcement learning phase for which we use DDPG. This combination shows a considerable performance boost comparing to both pure imitation learning and pure DDPG for the autonomous driving task.",0
"Our study introduces a training pipeline that utilizes current camera image and vehicle speed to generate throttle, brake, and steering control output for autonomous driving. The inclusion of Airsim's weather and lighting API provides ample diversity during training, which enhances the trained policy's robustness. To ensure optimal performance, we employ a continuous and deterministic control policy setting and implement ResNet-34 as our actor and critic networks with minor modifications in the fully connected layers. Due to the intricate nature of this task and humans' expertise in it, we begin with imitation learning to replicate the human policy and then integrate the trained policy and its weights into the reinforcement learning phase using DDPG. This combination yields a significant performance improvement compared to pure imitation learning and pure DDPG for autonomous driving.",1
"During the development of autonomous systems such as driverless cars, it is important to characterize the scenarios that are most likely to result in failure. Adaptive Stress Testing (AST) provides a way to search for the most-likely failure scenario as a Markov decision process (MDP). Our previous work used a deep reinforcement learning (DRL) solver to identify likely failure scenarios. However, the solver's use of a feed-forward neural network with a discretized space of possible initial conditions poses two major problems. First, the system is not treated as a black box, in that it requires analyzing the internal state of the system, which leads to considerable implementation complexities. Second, in order to simulate realistic settings, a new instance of the solver needs to be run for each initial condition. Running a new solver for each initial condition not only significantly increases the computational complexity, but also disregards the underlying relationship between similar initial conditions. We provide a solution to both problems by employing a recurrent neural network that takes a set of initial conditions from a continuous space as input. This approach enables robust and efficient detection of failures because the solution generalizes across the entire space of initial conditions. By simulating an instance where an autonomous car drives while a pedestrian is crossing a road, we demonstrate the solver is now capable of finding solutions for problems that would have previously been intractable.",0
"In the development of self-driving vehicles, it is crucial to identify scenarios that are most prone to failure. One way to achieve this is by using Adaptive Stress Testing (AST), which utilizes a Markov decision process (MDP) to search for the most probable failure scenario. In our prior work, we employed a deep reinforcement learning (DRL) solver to identify potential failure scenarios. However, this approach had two significant drawbacks. First, the solver relied on a feed-forward neural network with a limited set of initial conditions, which complicated its implementation. Second, to simulate real-world settings, we had to run a new solver for each initial condition, which increased computational complexity and disregarded the connection between similar initial conditions. To address these issues, we introduced a recurrent neural network that can handle a continuous set of initial conditions as input. This solution offers a more efficient and robust method of identifying failures since the solution generalizes across the entire range of initial conditions. Our experiments, which included a pedestrian crossing a road while a self-driving car was moving, demonstrate the improved solver's ability to solve previously unsolvable problems.",1
On-policy reinforcement learning (RL) algorithms have high sample complexity while off-policy algorithms are difficult to tune. Merging the two holds the promise to develop efficient algorithms that generalize across diverse environments. It is however challenging in practice to find suitable hyper-parameters that govern this trade off. This paper develops a simple algorithm named P3O that interleaves off-policy updates with on-policy updates. P3O uses the effective sample size between the behavior policy and the target policy to control how far they can be from each other and does not introduce any additional hyper-parameters. Extensive experiments on the Atari-2600 and MuJoCo benchmark suites show that this simple technique is effective in reducing the sample complexity of state-of-the-art algorithms. Code to reproduce experiments in this paper is at https://github.com/rasoolfa/P3O.,0
"While on-policy reinforcement learning (RL) algorithms require a large number of samples, off-policy algorithms are challenging to adjust. However, combining the two could lead to the development of efficient algorithms that can perform well in various environments. Nevertheless, it is difficult to determine the appropriate hyper-parameters that balance these approaches. This study introduces a straightforward algorithm called P3O that alternates between off-policy and on-policy updates. P3O regulates the distance between the behavior policy and the target policy by using the effective sample size and does not require additional hyper-parameters. The results of extensive experiments on the Atari-2600 and MuJoCo benchmark suites indicate that this simple approach reduces the sample complexity of leading algorithms. The code to replicate the experiments is available at https://github.com/rasoolfa/P3O.",1
"In this paper, we propose a dual memory structure for reinforcement learning algorithms with replay memory. The dual memory consists of a main memory that stores various data and a cache memory that manages the data and trains the reinforcement learning agent efficiently. Experimental results show that the dual memory structure achieves higher training and test scores than the conventional single memory structure in three selected environments of OpenAI Gym. This implies that the dual memory structure enables better and more efficient training than the single memory structure.",0
"Our paper presents a proposal for a dual memory structure aimed at enhancing the performance of reinforcement learning algorithms featuring replay memory. This structure comprises a primary memory that stores diverse data and a cache memory that oversees the data and trains the reinforcement learning agent in an optimal manner. Based on our experiments, the dual memory structure outperforms the conventional single memory structure in three OpenAI Gym environments, as evidenced by the higher training and test scores attained. These findings signify that the dual memory structure facilitates superior and more effective training than the single memory structure.",1
"We study two time-scale linear stochastic approximation algorithms, which can be used to model well-known reinforcement learning algorithms such as GTD, GTD2, and TDC. We present finite-time performance bounds for the case where the learning rate is fixed. The key idea in obtaining these bounds is to use a Lyapunov function motivated by singular perturbation theory for linear differential equations. We use the bound to design an adaptive learning rate scheme which significantly improves the convergence rate over the known optimal polynomial decay rule in our experiments, and can be used to potentially improve the performance of any other schedule where the learning rate is changed at pre-determined time instants.",0
"The focus of our research is on two linear stochastic approximation algorithms that have the ability to model popular reinforcement learning algorithms like GTD, GTD2, and TDC. Our objective is to provide finite-time performance bounds in situations where the learning rate remains fixed. To achieve this, we employ a Lyapunov function based on the singular perturbation theory for linear differential equations. Our results enable us to create an adaptive learning rate scheme that surpasses the known optimal polynomial decay rule in terms of convergence rate. This scheme can also be used to enhance the performance of other schedules that change the learning rate at predetermined time instants.",1
"In this paper, we investigate the problem of overfitting in deep reinforcement learning. Among the most common benchmarks in RL, it is customary to use the same environments for both training and testing. This practice offers relatively little insight into an agent's ability to generalize. We address this issue by using procedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called CoinRun, designed as a benchmark for generalization in RL. Using CoinRun, we find that agents overfit to surprisingly large training sets. We then show that deeper convolutional architectures improve generalization, as do methods traditionally found in supervised learning, including L2 regularization, dropout, data augmentation and batch normalization.",0
"The aim of this study is to examine the issue of overfitting in deep reinforcement learning. The conventional method of using the same environments for both training and testing in RL benchmarks provides limited information about an agent's capacity to generalize. To overcome this limitation, we have used procedurally generated environments to create distinct training and test sets. Furthermore, we have introduced an innovative environment named CoinRun, which serves as a benchmark for evaluating generalization in RL. Our findings demonstrate that agents tend to overfit even to large training sets. However, we have observed that deeper convolutional architectures and techniques commonly employed in supervised learning, such as L2 regularization, dropout, data augmentation, and batch normalization, enhance generalization.",1
"Despite the empirical success of the actor-critic algorithm, its theoretical understanding lags behind. In a broader context, actor-critic can be viewed as an online alternating update algorithm for bilevel optimization, whose convergence is known to be fragile. To understand the instability of actor-critic, we focus on its application to linear quadratic regulators, a simple yet fundamental setting of reinforcement learning. We establish a nonasymptotic convergence analysis of actor-critic in this setting. In particular, we prove that actor-critic finds a globally optimal pair of actor (policy) and critic (action-value function) at a linear rate of convergence. Our analysis may serve as a preliminary step towards a complete theoretical understanding of bilevel optimization with nonconvex subproblems, which is NP-hard in the worst case and is often solved using heuristics.",0
"Theoretical understanding of the actor-critic algorithm is lacking despite its empirical success. This algorithm can be seen as an online alternating update for bilevel optimization and is known to have fragile convergence. To investigate this instability, we examine its application to linear quadratic regulators in basic reinforcement learning. Our nonasymptotic convergence analysis of actor-critic in this context shows that it can find a globally optimal actor and critic pair at a linear convergence rate. This analysis is a preliminary step towards a complete theoretical understanding of nonconvex subproblems in bilevel optimization, which is an NP-hard problem often solved with heuristics.",1
"Reinforcement learning aims at searching the best policy model for decision making, and has been shown powerful for sequential recommendations. The training of the policy by reinforcement learning, however, is placed in an environment. In many real-world applications, however, the policy training in the real environment can cause an unbearable cost, due to the exploration in the environment. Environment reconstruction from the past data is thus an appealing way to release the power of reinforcement learning in these applications. The reconstruction of the environment is, basically, to extract the casual effect model from the data. However, real-world applications are often too complex to offer fully observable environment information. Therefore, quite possibly there are unobserved confounding variables lying behind the data. The hidden confounder can obstruct an effective reconstruction of the environment. In this paper, by treating the hidden confounder as a hidden policy, we propose a deconfounded multi-agent environment reconstruction (DEMER) approach in order to learn the environment together with the hidden confounder. DEMER adopts a multi-agent generative adversarial imitation learning framework. It proposes to introduce the confounder embedded policy, and use the compatible discriminator for training the policies. We then apply DEMER in an application of driver program recommendation. We firstly use an artificial driver program recommendation environment, abstracted from the real application, to verify and analyze the effectiveness of DEMER. We then test DEMER in the real application of Didi Chuxing. Experiment results show that DEMER can effectively reconstruct the hidden confounder, and thus can build the environment better. DEMER also derives a recommendation policy with a significantly improved performance in the test phase of the real application.",0
"Reinforcement learning is utilized to find the optimal policy model for decision-making, particularly in sequential recommendations. However, training the policy in the real environment can be costly due to the exploration required. One solution is to reconstruct the environment from past data, which involves extracting the causal effect model. However, real-world applications are often too complex to provide fully observable environment information, and unobserved confounding variables may exist. These hidden confounders can hinder effective environment reconstruction. To address this issue, we propose the Deconfounded Multi-Agent Environment Reconstruction (DEMER) approach, which treats the hidden confounder as a hidden policy. DEMER utilizes a multi-agent generative adversarial imitation learning framework, introducing the confounder-embedded policy and a compatible discriminator to train the policies. We apply DEMER to an application of driver program recommendation and test it in an artificial and real-world environment. The results show that DEMER effectively reconstructs the hidden confounder, leading to better environment building and significantly improved recommendation policy performance in the test phase of the real-world application.",1
"We aim to conduct a systematic mapping in the area of testing ML programs. We identify, analyze and classify the existing literature to provide an overview of the area. We followed well-established guidelines of systematic mapping to develop a systematic protocol to identify and review the existing literature. We formulate three sets of research questions, define inclusion and exclusion criteria and systematically identify themes for the classification of existing techniques. We also report the quality of the published works using established assessment criteria. we finally selected 37 papers out of 1654 based on our selection criteria up to January 2019. We analyze trends such as contribution facet, research facet, test approach, type of ML and the kind of testing with several other attributes. We also discuss the empirical evidence and reporting quality of selected papers. The data from the study is made publicly available for other researchers and practitioners. We present an overview of the area by answering several research questions. The area is growing rapidly, however, there is lack of enough empirical evidence to compare and assess the effectiveness of the techniques. More publicly available tools are required for use of practitioners and researchers. Further attention is needed on non-functional testing and testing of ML programs using reinforcement learning. We believe that this study can help researchers and practitioners to obtain an overview of the area and identify several sub-areas where more research is required",0
"Our objective is to systematically map the field of ML program testing by examining, categorizing, and analyzing existing literature. We adhered to established guidelines for systematic mapping and established a protocol to identify and evaluate relevant literature. We formulated three sets of research questions, defined criteria for inclusion and exclusion, and identified themes for classifying existing techniques. We evaluated the quality of published works using established assessment criteria and ultimately selected 37 papers out of 1654 based on our selection criteria up to January 2019. We analyzed trends such as contribution facet, research facet, test approach, type of ML, and testing methods, among other attributes. We also discussed the empirical evidence and reporting quality of the selected papers, and made the data from the study publicly available for other researchers and practitioners. Our study provides an overview of the area and answers several research questions. Although the field is growing rapidly, there is a lack of sufficient empirical evidence to compare and assess the effectiveness of techniques. More publicly available tools are needed for practitioners and researchers, and further attention is required on non-functional testing and testing using reinforcement learning. This study can assist researchers and practitioners in gaining an understanding of the area and identifying sub-areas that require further research.",1
"While model-based deep reinforcement learning (RL) holds great promise for sample efficiency and generalization, learning an accurate dynamics model is often challenging and requires substantial interaction with the environment. A wide variety of domains have dynamics that share common foundations like the laws of classical mechanics, which are rarely exploited by existing algorithms. In fact, humans continuously acquire and use such dynamics priors to easily adapt to operating in new environments. In this work, we propose an approach to learn task-agnostic dynamics priors from videos and incorporate them into an RL agent. Our method involves pre-training a frame predictor on task-agnostic physics videos to initialize dynamics models (and fine-tune them) for unseen target environments. Our frame prediction architecture, SpatialNet, is designed specifically to capture localized physical phenomena and interactions. Our approach allows for both faster policy learning and convergence to better policies, outperforming competitive approaches on several different environments. We also demonstrate that incorporating this prior allows for more effective transfer between environments.",0
"Although model-based deep reinforcement learning (RL) shows potential for efficiency and generalization, the process of accurately learning dynamics models is often difficult and requires extensive interaction with the environment. Despite the existence of common principles such as classical mechanics that underlie a wide range of domains, current algorithms rarely utilize them. Conversely, humans are adept at acquiring and applying such dynamics priors to adapt easily to new environments. This study proposes an approach to extract task-agnostic dynamics priors from videos and integrate them into an RL agent. The method involves pre-training a frame predictor on task-agnostic physics videos to establish dynamics models for unseen target environments. The frame prediction design, SpatialNet, is tailored to capture localized physical phenomena and interactions. The approach enables faster policy learning and better convergence to policies, outperforming other methods across several environments. Additionally, the study shows that the incorporation of this prior facilitates more effective transfer between environments.",1
"Batch Reinforcement Learning (Batch RL) consists in training a policy using trajectories collected with another policy, called the behavioural policy. Safe policy improvement (SPI) provides guarantees with high probability that the trained policy performs better than the behavioural policy, also called baseline in this setting. Previous work shows that the SPI objective improves mean performance as compared to using the basic RL objective, which boils down to solving the MDP with maximum likelihood. Here, we build on that work and improve more precisely the SPI with Baseline Bootstrapping algorithm (SPIBB) by allowing the policy search over a wider set of policies. Instead of binarily classifying the state-action pairs into two sets (the \textit{uncertain} and the \textit{safe-to-train-on} ones), we adopt a softer strategy that controls the error in the value estimates by constraining the policy change according to the local model uncertainty. The method can take more risks on uncertain actions all the while remaining provably-safe, and is therefore less conservative than the state-of-the-art methods. We propose two algorithms (one optimal and one approximate) to solve this constrained optimization problem and empirically show a significant improvement over existing SPI algorithms both on finite MDPs and on infinite MDPs with a neural network function approximation.",0
"The concept of Batch Reinforcement Learning involves using trajectories acquired by a behavioural policy to train another policy. Safe policy improvement (SPI) ensures that the trained policy performs better than the behavioural policy, which is considered the baseline. Previous research indicates that utilizing the SPI objective leads to superior mean performance compared to the basic RL objective. However, our work builds upon this by introducing the Baseline Bootstrapping algorithm (SPIBB), which expands the search for policies that can be used. Rather than classifying state-action pairs into two distinct categories, our approach employs a more flexible strategy that considers the estimation error by restricting policy changes based on the local model uncertainty. This technique allows for more risk-taking on uncertain actions while still maintaining safety. We offer two algorithms, one optimal and one approximate, to address this constrained optimization issue and demonstrate significant improvements over existing SPI approaches in both finite and infinite MDPs that use neural network function approximation.",1
"Reinforcement learning usually makes use of numerical rewards, which have nice properties but also come with drawbacks and difficulties. Using rewards on an ordinal scale (ordinal rewards) is an alternative to numerical rewards that has received more attention in recent years. In this paper, a general approach to adapting reinforcement learning problems to the use of ordinal rewards is presented and motivated. We show how to convert common reinforcement learning algorithms to an ordinal variation by the example of Q-learning and introduce Ordinal Deep Q-Networks, which adapt deep reinforcement learning to ordinal rewards. Additionally, we run evaluations on problems provided by the OpenAI Gym framework, showing that our ordinal variants exhibit a performance that is comparable to the numerical variations for a number of problems. We also give first evidence that our ordinal variant is able to produce better results for problems with less engineered and simpler-to-design reward signals.",0
"Reinforcement learning often utilizes numerical rewards, which have advantages but also pose challenges. Recently, there has been a growing interest in using ordinal rewards as an alternative to numerical rewards. In this research, we propose a comprehensive strategy for adapting reinforcement learning tasks to the use of ordinal rewards. We demonstrate how to modify common reinforcement learning algorithms, such as Q-learning, to accommodate ordinal rewards and introduce Ordinal Deep Q-Networks, which incorporate deep reinforcement learning with ordinal rewards. Moreover, we evaluate our ordinal variants on problems from the OpenAI Gym framework and find that they perform similarly to the numerical variations in many cases. Furthermore, we provide initial evidence that our ordinal variant may achieve better results for problems with simpler and less engineered reward signals.",1
"In multi-task reinforcement learning there are two main challenges: at training time, the ability to learn different policies with a single model; at test time, inferring which of those policies applying without an external signal. In the case of continual reinforcement learning a third challenge arises: learning tasks sequentially without forgetting the previous ones. In this paper, we tackle these challenges by proposing DisCoRL, an approach combining state representation learning and policy distillation. We experiment on a sequence of three simulated 2D navigation tasks with a 3 wheel omni-directional robot. Moreover, we tested our approach's robustness by transferring the final policy into a real life setting. The policy can solve all tasks and automatically infer which one to run.",0
"Multi-task reinforcement learning presents two major hurdles: first, the need to acquire various policies using a single model during training, and second, the capability to determine which policy to execute without external cues during testing. In the case of continuous reinforcement learning, a third challenge emerges: the capacity to learn tasks in sequence while not forgetting previous ones. This article proposes DisCoRL, an approach that combines state representation learning and policy distillation to tackle these challenges. The technique is tested on three simulated 2D navigation tasks utilizing a 3-wheel omni-directional robot, and the final policy is transferred to a real-life environment to test its robustness. The policy can handle all tasks and make autonomous inferences about which task to execute.",1
"We propose a policy improvement algorithm for Reinforcement Learning (RL) which is called Rerouted Behavior Improvement (RBI). RBI is designed to take into account the evaluation errors of the Q-function. Such errors are common in RL when learning the $Q$-value from finite past experience data. Greedy policies or even constrained policy optimization algorithms which ignore these errors may suffer from an improvement penalty (i.e. a negative policy improvement). To minimize the improvement penalty, the RBI idea is to attenuate rapid policy changes of low probability actions which were less frequently sampled. This approach is shown to avoid catastrophic performance degradation and reduce regret when learning from a batch of past experience. Through a two-armed bandit with Gaussian distributed rewards example, we show that it also increases data efficiency when the optimal action has a high variance. We evaluate RBI in two tasks in the Atari Learning Environment: (1) learning from observations of multiple behavior policies and (2) iterative RL. Our results demonstrate the advantage of RBI over greedy policies and other constrained policy optimization algorithms as a safe learning approach and as a general data efficient learning algorithm. An anonymous Github repository of our RBI implementation is found at https://github.com/eladsar/rbi.",0
"The Rerouted Behavior Improvement (RBI) algorithm is proposed as a policy improvement method for Reinforcement Learning (RL) that takes into account the evaluation errors of the Q-function. These errors are common in RL when learning the Q-value from finite past experience data, and ignoring them can result in negative policy improvement. To minimize this penalty, RBI attenuates rapid policy changes of low probability actions that were less frequently sampled. This approach prevents catastrophic performance degradation and reduces regret when learning from a batch of past experience. Additionally, it increases data efficiency when the optimal action has high variance. RBI is evaluated in two tasks in the Atari Learning Environment, namely learning from observations of multiple behavior policies and iterative RL. Our results show that RBI is a safe learning approach and a general data-efficient learning algorithm that outperforms greedy policies and other constrained policy optimization algorithms. An anonymous Github repository of our RBI implementation can be found at https://github.com/eladsar/rbi.",1
"In this paper, we present a Bayesian view on model-based reinforcement learning. We use expert knowledge to impose structure on the transition model and present an efficient learning scheme based on variational inference. This scheme is applied to a heteroskedastic and bimodal benchmark problem on which we compare our results to NFQ and show how our approach yields human-interpretable insight about the underlying dynamics while also increasing data-efficiency.",0
"Our paper adopts a Bayesian perspective in examining model-based reinforcement learning. We leverage expert knowledge to structure the transition model and propose a learning scheme based on variational inference that yields significant efficiency gains. We evaluate our approach on a challenging benchmark problem with heteroskedastic and bimodal characteristics, and compare our results to those of NFQ. Our approach not only enhances data efficiency but also provides human-readable insights on the dynamics underlying the problem.",1
"Deep reinforcement learning (DRL) has achieved significant breakthroughs in various tasks. However, most DRL algorithms suffer a problem of generalizing the learned policy which makes the learning performance largely affected even by minor modifications of the training environment. Except that, the use of deep neural networks makes the learned policies hard to be interpretable. To address these two challenges, we propose a novel algorithm named Neural Logic Reinforcement Learning (NLRL) to represent the policies in reinforcement learning by first-order logic. NLRL is based on policy gradient methods and differentiable inductive logic programming that have demonstrated significant advantages in terms of interpretability and generalisability in supervised tasks. Extensive experiments conducted on cliff-walking and blocks manipulation tasks demonstrate that NLRL can induce interpretable policies achieving near-optimal performance while demonstrating good generalisability to environments of different initial states and problem sizes.",0
"Significant progress has been made in various tasks through the use of deep reinforcement learning (DRL). However, a major impediment to DRL algorithms is their inability to generalize learned policy, which negatively impacts learning performance even with minor adjustments to the training environment. Additionally, deep neural networks used in DRL result in learned policies that are difficult to interpret. To overcome these challenges, we propose a new approach called Neural Logic Reinforcement Learning (NLRL), which represents reinforcement learning policies using first-order logic. NLRL is based on policy gradient methods and differentiable inductive logic programming, which have demonstrated superior interpretability and generalizability in supervised tasks. Our extensive experiments on cliff-walking and blocks manipulation tasks show that NLRL can induce interpretable policies that achieve near-optimal performance while also demonstrating good generalizability to different problem sizes and initial states.",1
"Recommendation problems with large numbers of discrete items, such as products, webpages, or videos, are ubiquitous in the technology industry. Deep neural networks are being increasingly used for these recommendation problems. These models use embeddings to represent discrete items as continuous vectors, and the vocabulary sizes and embedding dimensions, although heavily influence the model's accuracy, are often manually selected in a heuristical manner. We present Neural Input Search (NIS), a technique for learning the optimal vocabulary sizes and embedding dimensions for categorical features. The goal is to maximize prediction accuracy subject to a constraint on the total memory used by all embeddings. Moreover, we argue that the traditional Single-size Embedding (SE), which uses the same embedding dimension for all values of a feature, suffers from inefficient usage of model capacity and training data. We propose a novel type of embedding, namely Multi-size Embedding (ME), which allows the embedding dimension to vary for different values of the feature. During training we use reinforcement learning to find the optimal vocabulary size for each feature and embedding dimension for each value of the feature. In experiments on two common types of large scale recommendation problems, i.e. retrieval and ranking problems, NIS automatically found better vocabulary and embedding sizes that result in $6.8\%$ and $1.8\%$ relative improvements on Recall@1 and ROC-AUC over manually optimized ones.",0
"The technology industry frequently encounters recommendation problems involving a vast number of discrete items like products, webpages, and videos. These issues are now being addressed with deep neural networks that use embeddings to create continuous vector representations of the discrete items. However, the vocabulary sizes and embedding dimensions, which significantly impact the model's accuracy, are typically chosen through manual heuristics. We introduce Neural Input Search (NIS) as a method for determining the optimal vocabulary sizes and embedding dimensions for categorical features. Our objective is to obtain the highest prediction accuracy while limiting the total memory used by all embeddings. Furthermore, we argue that the conventional Single-size Embedding (SE) that employs the same embedding dimension for all feature values is ineffective in utilizing model capacity and training data. Instead, we propose Multi-size Embedding (ME), a new type of embedding that allows the embedding dimension to vary for different feature values. We use reinforcement learning during training to obtain the ideal vocabulary size for each feature and embedding dimension for each feature value. In experiments on two common types of large-scale recommendation problems, namely retrieval and ranking problems, NIS automatically identifies better vocabulary and embedding sizes, resulting in 6.8% and 1.8% relative improvements in Recall@1 and ROC-AUC, respectively, compared to manually optimized alternatives.",1
"Variance plays a crucial role in risk-sensitive reinforcement learning, and most risk measures can be analyzed via variance. In this paper, we consider two law-invariant risks as examples: mean-variance risk and exponential utility risk. With the aid of the state-augmentation transformation (SAT), we show that, the two risks can be estimated in Markov decision processes (MDPs) with a stochastic transition-based reward and a randomized policy. To relieve the enlarged state space, a novel definition of isotopic states is proposed for state lumping, considering the special structure of the transformed transition probability. In the numerical experiment, we illustrate state lumping in the SAT, errors from a naive reward simplification, and the validity of the SAT for the two risk estimations.",0
"The role of variance is vital in risk-sensitive reinforcement learning, and most risk measures can be analyzed by it. This article focuses on two examples of law-invariant risks: mean-variance risk and exponential utility risk. The authors utilize the state-augmentation transformation (SAT) to demonstrate that these risks can be estimated in Markov decision processes (MDPs) that involve stochastic transition-based rewards and randomized policies. To reduce the state space, a new definition of isotopic states is suggested for state lumping, taking into account the unique structure of the transformed transition probability. The authors conduct a numerical experiment to demonstrate state lumping in the SAT, errors arising from a naive reward simplification, and the SAT's applicability for estimating the two risks.",1
"We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent's policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and dueling agents (entropy reward and $\epsilon$-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.",0
"The article presents NoisyNet, a deep reinforcement learning agent that incorporates parametric noise into its weights. By doing so, the agent's policy becomes stochastic, which helps facilitate efficient exploration. The noise parameters are learned using gradient descent, alongside the other network weights. Implementing NoisyNet is simple and does not require significant computational resources. The study demonstrates that using NoisyNet, rather than conventional exploration heuristics like entropy reward and $\epsilon$-greedy, leads to significantly higher scores in various Atari games. In some cases, the agent's performance improves from sub-human to super-human levels.",1
"A critical flaw of existing inverse reinforcement learning (IRL) methods is their inability to significantly outperform the demonstrator. This is because IRL typically seeks a reward function that makes the demonstrator appear near-optimal, rather than inferring the underlying intentions of the demonstrator that may have been poorly executed in practice. In this paper, we introduce a novel reward-learning-from-observation algorithm, Trajectory-ranked Reward EXtrapolation (T-REX), that extrapolates beyond a set of (approximately) ranked demonstrations in order to infer high-quality reward functions from a set of potentially poor demonstrations. When combined with deep reinforcement learning, T-REX outperforms state-of-the-art imitation learning and IRL methods on multiple Atari and MuJoCo benchmark tasks and achieves performance that is often more than twice the performance of the best demonstration. We also demonstrate that T-REX is robust to ranking noise and can accurately extrapolate intention by simply watching a learner noisily improve at a task over time.",0
"One major drawback of current methods in inverse reinforcement learning (IRL) is their limited capability to surpass the performance of the demonstrator. This is because IRL mainly focuses on finding a reward function that makes the demonstrator seem nearly optimal instead of deducing the underlying intentions of the demonstrator that may have been executed poorly in practice. This research introduces a new approach to reward learning from observation, known as Trajectory-ranked Reward EXtrapolation (T-REX), which goes beyond a set of ranked demonstrations to deduce high-quality reward functions from a potentially substandard set of demonstrations. When coupled with deep reinforcement learning, T-REX outperforms the most advanced imitation learning and IRL techniques on various Atari and MuJoCo benchmark tasks, achieving performance that is frequently more than double the best demonstration's performance. Additionally, the research shows that T-REX is resistant to ranking noise and can accurately extrapolate intention by simply observing a learner gradually improve at a task despite noise.",1
"Active Inference is a theory of action arising from neuroscience which casts action and planning as a bayesian inference problem to be solved by minimizing a single quantity - the variational free energy. Active Inference promises a unifying account of action and perception coupled with a biologically plausible process theory. Despite these potential advantages, current implementations of Active Inference can only handle small, discrete policy and state-spaces and typically require the environmental dynamics to be known. In this paper we propose a novel deep Active Inference algorithm which approximates key densities using deep neural networks as flexible function approximators, which enables Active Inference to scale to significantly larger and more complex tasks. We demonstrate our approach on a suite of OpenAIGym benchmark tasks and obtain performance comparable with common reinforcement learning baselines. Moreover, our algorithm shows similarities with maximum entropy reinforcement learning and the policy gradients algorithm, which reveals interesting connections between the Active Inference framework and reinforcement learning.",0
"The theory of Active Inference is rooted in neuroscience and views action and planning as a problem of bayesian inference. This theory aims to provide a comprehensive explanation of action and perception, while adhering to a biologically plausible process. However, the current implementation of Active Inference is limited to small, well-defined policies and states, and requires pre-knowledge of environmental dynamics. To address these limitations, we introduce a new deep Active Inference algorithm that uses deep neural networks to approximate crucial densities, making it possible to tackle larger and more complex problems. Our algorithm was tested on a set of OpenAIGym benchmarks and performed comparably with standard reinforcement learning techniques. Furthermore, our approach exhibits similarities with maximum entropy reinforcement learning and policy gradients, uncovering intriguing links between the Active Inference framework and reinforcement learning.",1
"Most deep reinforcement learning (RL) systems are not able to learn effectively from off-policy data, especially if they cannot explore online in the environment. These are critical shortcomings for applying RL to real-world problems where collecting data is expensive, and models must be tested offline before being deployed to interact with the environment -- e.g. systems that learn from human interaction. Thus, we develop a novel class of off-policy batch RL algorithms, which are able to effectively learn offline, without exploring, from a fixed batch of human interaction data. We leverage models pre-trained on data as a strong prior, and use KL-control to penalize divergence from this prior during RL training. We also use dropout-based uncertainty estimates to lower bound the target Q-values as a more efficient alternative to Double Q-Learning. The algorithms are tested on the problem of open-domain dialog generation -- a challenging reinforcement learning problem with a 20,000-dimensional action space. Using our Way Off-Policy algorithm, we can extract multiple different reward functions post-hoc from collected human interaction data, and learn effectively from all of these. We test the real-world generalization of these systems by deploying them live to converse with humans in an open-domain setting, and demonstrate that our algorithm achieves significant improvements over prior methods in off-policy batch RL.",0
"Many deep reinforcement learning (RL) systems struggle to effectively learn from off-policy data, particularly if they are unable to explore the environment online. These limitations are especially problematic when using RL for real-world applications, where data collection can be costly, and models need to be tested offline before being deployed to interact with the environment (e.g., systems that learn from human interaction). To address these challenges, we introduce a novel class of off-policy batch RL algorithms that can learn offline from a fixed batch of human interaction data without exploring. Leveraging models pre-trained on data as a strong prior, we use KL-control to penalize divergence from the prior during RL training, and employ dropout-based uncertainty estimates to efficiently lower bound target Q-values as an alternative to Double Q-Learning. Our algorithms are tested on the difficult task of open-domain dialog generation, which features a 20,000-dimensional action space. We demonstrate the effectiveness of our Way Off-Policy algorithm by extracting multiple reward functions post-hoc from collected human interaction data and effectively learning from all of them. We also evaluate the real-world generalization of our systems by deploying them live to converse with humans in an open-domain setting and show significant improvements over prior methods in off-policy batch RL.",1
"Interest in derivative-free optimization (DFO) and ""evolutionary strategies"" (ES) has recently surged in the Reinforcement Learning (RL) community, with growing evidence that they can match state of the art methods for policy optimization problems in Robotics. However, it is well known that DFO methods suffer from prohibitively high sampling complexity. They can also be very sensitive to noisy rewards and stochastic dynamics. In this paper, we propose a new class of algorithms, called Robust Blackbox Optimization (RBO). Remarkably, even if up to $23\%$ of all the measurements are arbitrarily corrupted, RBO can provably recover gradients to high accuracy. RBO relies on learning gradient flows using robust regression methods to enable off-policy updates. On several MuJoCo robot control tasks, when all other RL approaches collapse in the presence of adversarial noise, RBO is able to train policies effectively. We also show that RBO can be applied to legged locomotion tasks including path tracking for quadruped robots.",0
"The Reinforcement Learning (RL) community has recently shown a growing interest in derivative-free optimization (DFO) and ""evolutionary strategies"" (ES), as they have been found to be equally effective as state-of-the-art methods for policy optimization problems in Robotics. However, the high sampling complexity associated with DFO methods and their sensitivity to noisy rewards and stochastic dynamics are well known. This paper introduces a new algorithmic class, Robust Blackbox Optimization (RBO), which is capable of recovering gradients to high accuracy, even when up to 23% of all measurements are arbitrarily corrupted. RBO uses robust regression methods to learn gradient flows and enables off-policy updates. In contrast to other RL approaches that collapse in the presence of adversarial noise, RBO can effectively train policies on several MuJoCo robot control tasks. Additionally, RBO can be applied to legged locomotion tasks, including path tracking for quadruped robots.",1
"Recent advances in Reinforcement Learning, grounded on combining classical theoretical results with Deep Learning paradigm, led to breakthroughs in many artificial intelligence tasks and gave birth to Deep Reinforcement Learning (DRL) as a field of research. In this work latest DRL algorithms are reviewed with a focus on their theoretical justification, practical limitations and observed empirical properties.",0
"By integrating classical theoretical findings with the Deep Learning paradigm, Reinforcement Learning has made significant progress in various artificial intelligence tasks and spawned the field of Deep Reinforcement Learning (DRL). This article examines the most recent DRL algorithms, highlighting their theoretical backing, practical constraints, and observed empirical characteristics.",1
"At an early age, human infants are able to learn and build a model of the world very quickly by constantly observing and interacting with objects around them. One of the most fundamental intuitions human infants acquire is intuitive physics. Human infants learn and develop these models, which later serve as prior knowledge for further learning. Inspired by such behaviors exhibited by human infants, we introduce a graphical physics network integrated with deep reinforcement learning. Specifically, we introduce an intrinsic reward normalization method that allows our agent to efficiently choose actions that can improve its intuitive physics model the most.   Using a 3D physics engine, we show that our graphical physics network is able to infer object's positions and velocities very effectively, and our deep reinforcement learning network encourages an agent to improve its model by making it continuously interact with objects only using intrinsic motivation. We experiment our model in both stationary and non-stationary state problems and show benefits of our approach in terms of the number of different actions the agent performs and the accuracy of agent's intuition model.   Videos are at https://www.youtube.com/watch?v=pDbByp91r3M&t=2s",0
"At an early stage of development, human infants quickly learn about the world by observing and interacting with objects around them. Intuitive physics is one of the fundamental concepts that infants acquire and develop. These models serve as prior knowledge for future learning. Inspired by the learning behavior of human infants, we propose a graphical physics network with deep reinforcement learning. We introduce an intrinsic reward normalization method, allowing our agent to choose actions that can improve its intuitive physics model efficiently. Our graphical physics network, utilizing a 3D physics engine, effectively infers object positions and velocities. Our deep reinforcement learning network encourages the agent to improve its model by interacting with objects intrinsically. We conduct experiments on stationary and non-stationary state problems and demonstrate the benefits of our approach in terms of the accuracy of the agent's intuitive model and the number of actions performed. Videos of our model can be found at https://www.youtube.com/watch?v=pDbByp91r3M&t=2s.",1
"Flappy Bird, which has a very high popularity, has been trained in many algorithms. Some of these studies were trained from raw pixel values of game and some from specific attributes. In this study, the model was trained with raw game images, which had not been seen before. The trained model has learned as reinforcement when to make which decision. As an input to the model, the reward or penalty at the end of each step was returned and the training was completed. Flappy Bird game was trained with the Reinforcement Learning algorithm Deep Q-Network and Asynchronous Advantage Actor Critic (A3C) algorithms.",0
"Numerous algorithms have been utilized to train the extremely popular Flappy Bird game, including studies that utilize raw pixel values and specific attributes. This particular study trained the model using never-before-seen raw game images, allowing the model to learn through reinforcement when to make certain decisions. The training process involved inputting rewards or penalties at the end of each step, ultimately resulting in a fully trained model. The Reinforcement Learning algorithm Deep Q-Network and the Asynchronous Advantage Actor Critic (A3C) algorithms were both used to train Flappy Bird.",1
This paper considers a distributed reinforcement learning problem in which a network of multiple agents aim to cooperatively maximize the globally averaged return through communication with only local neighbors. A randomized communication-efficient multi-agent actor-critic algorithm is proposed for possibly unidirectional communication relationships depicted by a directed graph. It is shown that the algorithm can solve the problem for strongly connected graphs by allowing each agent to transmit only two scalar-valued variables at one time.,0
"The focus of this article is on a distributed reinforcement learning issue where numerous agents within a network work together to maximize the overall average return by communicating exclusively with local peers. The proposed solution to this potentially one-way communication issue, portrayed by a directed graph, is a randomized communication-efficient multi-agent actor-critic algorithm. This algorithm has the capability of resolving the problem for strongly connected graphs by enabling each agent to send only two scalar-valued variables at once.",1
"Imitation Learning (IL) is a machine learning approach to learn a policy from a dataset of demonstrations. IL can be useful to kick-start learning before applying reinforcement learning (RL) but it can also be useful on its own, e.g. to learn to imitate human players in video games. However, a major limitation of current IL approaches is that they learn only a single ""average"" policy based on a dataset that possibly contains demonstrations of numerous different types of behaviors. In this paper, we propose a new approach called Behavioral Repertoire Imitation Learning (BRIL) that instead learns a repertoire of behaviors from a set of demonstrations by augmenting the state-action pairs with behavioral descriptions. The outcome of this approach is a single neural network policy conditioned on a behavior description that can be precisely modulated. We apply this approach to train a policy on 7,777 human replays to perform build-order planning in StarCraft II. Principal Component Analysis (PCA) is applied to construct a low-dimensional behavioral space from the high-dimensional army unit composition of each demonstration. The results demonstrate that the learned policy can be effectively manipulated to express distinct behaviors. Additionally, by applying the UCB1 algorithm, we are able to adapt the behavior of the policy - in-between games - to reach a performance beyond that of the traditional IL baseline approach.",0
"Imitation Learning (IL) is a technique in machine learning that involves learning a policy from a dataset of demonstrations. It can be used to initiate learning before applying reinforcement learning (RL), or on its own to replicate human behavior in video games. However, current IL approaches only learn a single ""average"" policy from a dataset that may contain various types of behaviors. To address this limitation, we introduce a new approach called Behavioral Repertoire Imitation Learning (BRIL). BRIL learns a range of behaviors from a set of demonstrations by augmenting state-action pairs with behavioral descriptions. The result is a single neural network policy that can be fine-tuned to express different behaviors. We apply BRIL to train a policy on 7,777 human replays for build-order planning in StarCraft II. By using Principal Component Analysis (PCA) to construct a low-dimensional behavioral space, we demonstrate that the policy can be effectively modulated to express distinct behaviors. We also show that by using the UCB1 algorithm, we can adapt the policy's behavior between games and achieve better performance than the traditional IL approach.",1
"Temporal difference methods enable efficient estimation of value functions in reinforcement learning in an incremental fashion, and are of broader interest because they correspond learning as observed in biological systems. Standard value functions correspond to the expected value of a sum of discounted returns. While this formulation is often sufficient for many purposes, it would often be useful to be able to represent functions of the return as well. Unfortunately, most such functions cannot be estimated directly using TD methods. We propose a means of estimating functions of the return using its moments, which can be learned online using a modified TD algorithm. The moments of the return are then used as part of a Taylor expansion to approximate analytic functions of the return.",0
"Temporal difference methods are effective in estimating value functions incrementally in reinforcement learning and have broader applications due to their similarity to learning in biological systems. The standard value functions represent the expected value of discounted returns, which is generally adequate, but it would be beneficial to represent functions of the return as well. However, TD methods cannot directly estimate most of these functions. To solve this problem, we suggest estimating the moments of the return using a modified TD algorithm and using them in a Taylor expansion to approximate analytic functions of the return. This approach can be learned online.",1
"This paper targets the problem of image set-based face verification and identification. Unlike traditional single media (an image or video) setting, we encounter a set of heterogeneous contents containing orderless images and videos. The importance of each image is usually considered either equal or based on their independent quality assessment. How to model the relationship of orderless images within a set remains a challenge. We address this problem by formulating it as a Markov Decision Process (MDP) in the latent space. Specifically, we first present a dependency-aware attention control (DAC) network, which resorts to actor-critic reinforcement learning for sequential attention decision of each image embedding to fully exploit the rich correlation cues among the unordered images. Moreover, we introduce its sample-efficient variant with off-policy experience replay to speed up the learning process. The pose-guided representation scheme can further boost the performance at the extremes of the pose variation.",0
"The objective of this paper is to tackle the issue of face verification and identification utilizing sets of images. This differs from the traditional approach of using single media, as the sets are composed of a variety of images and videos without a specific order. The significance of each image is considered equal or based on their individual quality assessment, making it challenging to model the relationship between them. To address this challenge, we propose a Markov Decision Process (MDP) in the latent space. Our solution involves a dependency-aware attention control (DAC) network that uses actor-critic reinforcement learning to make sequential attention decisions for each image embedding, exploiting the rich correlation cues among the unordered images. We also introduce a sample-efficient variant with off-policy experience replay to speed up the learning process. Additionally, our pose-guided representation scheme can enhance performance for extreme variations in pose.",1
"Composing previously mastered skills to solve novel tasks promises dramatic improvements in the data efficiency of reinforcement learning. Here, we analyze two recent works composing behaviors represented in the form of action-value functions and show that they perform poorly in some situations. As part of this analysis, we extend an important generalization of policy improvement to the maximum entropy framework and introduce an algorithm for the practical implementation of successor features in continuous action spaces. Then we propose a novel approach which addresses the failure cases of prior work and, in principle, recovers the optimal policy during transfer. This method works by explicitly learning the (discounted, future) divergence between base policies. We study this approach in the tabular case and on non-trivial continuous control problems with compositional structure and show that it outperforms or matches existing methods across all tasks considered.",0
"To increase the data efficiency of reinforcement learning, it is beneficial to apply previously learned skills to new tasks. However, recent studies have shown that combining action-value functions to create new behaviors can result in poor performance in certain situations. To address this issue, we introduced an algorithm that implements successor features in continuous action spaces and extended policy improvement to the maximum entropy framework. Additionally, we proposed a new approach that explicitly learns the divergence between base policies to recover optimal policies during transfer. Our method outperforms existing techniques in tabular cases and complex continuous control problems with compositional structures.",1
"Many deep reinforcement learning algorithms contain inductive biases that sculpt the agent's objective and its interface to the environment. These inductive biases can take many forms, including domain knowledge and pretuned hyper-parameters. In general, there is a trade-off between generality and performance when algorithms use such biases. Stronger biases can lead to faster learning, but weaker biases can potentially lead to more general algorithms. This trade-off is important because inductive biases are not free; substantial effort may be required to obtain relevant domain knowledge or to tune hyper-parameters effectively. In this paper, we re-examine several domain-specific components that bias the objective and the environmental interface of common deep reinforcement learning agents. We investigated whether the performance deteriorates when these components are replaced with adaptive solutions from the literature. In our experiments, performance sometimes decreased with the adaptive components, as one might expect when comparing to components crafted for the domain, but sometimes the adaptive components performed better. We investigated the main benefit of having fewer domain-specific components, by comparing the learning performance of the two systems on a different set of continuous control problems, without additional tuning of either system. As hypothesized, the system with adaptive components performed better on many of the new tasks.",0
"Numerous deep reinforcement learning algorithms have inherent biases that influence the agent's objective and its interaction with the environment. These biases may include domain expertise and pre-set hyper-parameters. The use of such biases can affect the performance and generality of the algorithms, with stronger biases resulting in faster learning but weaker biases potentially leading to more general algorithms. However, inductive biases require substantial effort to acquire relevant domain knowledge or tune hyper-parameters effectively. This study revisits the domain-specific components that bias the objective and environmental interface of common deep reinforcement learning agents, examining whether performance declines when replaced with adaptive solutions from the literature. The results demonstrate that the adaptive components sometimes outperformed the original components, while the main benefit of having fewer domain-specific components was observed in the improved learning performance of the adaptive system on a different set of continuous control problems.",1
"Sharing knowledge between tasks is vital for efficient learning in a multi-task setting. However, most research so far has focused on the easier case where knowledge transfer is not harmful, i.e., where knowledge from one task cannot negatively impact the performance on another task. In contrast, we present an approach to multi-task deep reinforcement learning based on attention that does not require any a-priori assumptions about the relationships between tasks. Our attention network automatically groups task knowledge into sub-networks on a state level granularity. It thereby achieves positive knowledge transfer if possible, and avoids negative transfer in cases where tasks interfere. We test our algorithm against two state-of-the-art multi-task/transfer learning approaches and show comparable or superior performance while requiring fewer network parameters.",0
"Efficient learning in a multi-task environment depends on the exchange of knowledge between tasks. However, previous research has mainly focused on cases where knowledge transfer does not have a detrimental effect, meaning that knowledge from one task does not negatively affect the performance of another task. Conversely, we propose an attention-based approach to multi-task deep reinforcement learning that does not rely on any prior assumptions about the relationships between tasks. Our attention network automatically categorizes task knowledge into sub-networks at the state level, promoting positive knowledge transfer while avoiding negative transfer in instances where tasks interfere. We evaluate our algorithm against two state-of-the-art multi-task/transfer learning approaches and demonstrate comparable or superior performance while utilizing fewer network parameters.",1
"We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.",0
"Our approach involves applying the principles that have led to the success of Deep Q-Learning in order to address continuous action scenarios. To this end, we have developed an actor-critic, model-free algorithm that utilizes the deterministic policy gradient to operate in continuous action spaces. By employing the same learning algorithm, network architecture, and hyper-parameters, our method has been shown to effectively solve more than 20 simulated physics problems, including traditionally challenging tasks such as cartpole swing-up, dexterous manipulation, legged locomotion, and car driving. Moreover, our approach is capable of generating policies that are competitive with those produced by planning algorithms that have complete access to the dynamics of the domain and its derivatives. Additionally, we have demonstrated that our algorithm has the ability to learn policies end-to-end, directly from raw pixel inputs, for many of the tasks.",1
"Multiagent reinforcement learning (MARL) is commonly considered to suffer from non-stationary environments and exponentially increasing policy space. It would be even more challenging when rewards are sparse and delayed over long trajectories. In this paper, we study hierarchical deep MARL in cooperative multiagent problems with sparse and delayed reward. With temporal abstraction, we decompose the problem into a hierarchy of different time scales and investigate how agents can learn high-level coordination based on the independent skills learned at the low level. Three hierarchical deep MARL architectures are proposed to learn hierarchical policies under different MARL paradigms. Besides, we propose a new experience replay mechanism to alleviate the issue of the sparse transitions at the high level of abstraction and the non-stationarity of multiagent learning. We empirically demonstrate the effectiveness of our approaches in two domains with extremely sparse feedback: (1) a variety of Multiagent Trash Collection tasks, and (2) a challenging online mobile game, i.e., Fever Basketball Defense.",0
"MARL is known to face challenges such as non-stationary environments and exponentially increasing policy space. These difficulties become even more pronounced in situations where rewards are scarce and delayed over long periods. In this study, we explore hierarchical deep MARL in cooperative multiagent scenarios that involve sparse and delayed reward. Our approach involves decomposing the problem into a hierarchy of different time scales to enable high-level coordination based on the independent skills learned at the low level. We propose three hierarchical deep MARL architectures to learn hierarchical policies under different MARL paradigms. Moreover, we propose a new experience replay mechanism to mitigate the impact of sparse transitions at the high level of abstraction and the non-stationarity of multiagent learning. Our empirical results demonstrate the efficacy of our approaches in two domains with extremely sparse feedback: (1) various Multiagent Trash Collection tasks, and (2) a challenging online mobile game, Fever Basketball Defense.",1
"Dyna is an architecture for model-based reinforcement learning (RL), where simulated experience from a model is used to update policies or value functions. A key component of Dyna is search-control, the mechanism to generate the state and action from which the agent queries the model, which remains largely unexplored. In this work, we propose to generate such states by using the trajectory obtained from Hill Climbing (HC) the current estimate of the value function. This has the effect of propagating value from high-value regions and of preemptively updating value estimates of the regions that the agent is likely to visit next. We derive a noisy projected natural gradient algorithm for hill climbing, and highlight a connection to Langevin dynamics. We provide an empirical demonstration on four classical domains that our algorithm, HC-Dyna, can obtain significant sample efficiency improvements. We study the properties of different sampling distributions for search-control, and find that there appears to be a benefit specifically from using the samples generated by climbing on current value estimates from low-value to high-value region.",0
"Dyna is a model-based reinforcement learning architecture that updates policies or value functions using simulated experience from a model. However, the mechanism for generating the state and action from which the agent queries the model, known as search-control, has not been thoroughly explored. To address this, we propose using the trajectory obtained from Hill Climbing (HC) the current estimate of the value function to generate these states. This approach propagates value from high-value regions and preemptively updates value estimates of regions likely to be visited next. We introduce a noisy projected natural gradient algorithm for hill climbing, which has a connection to Langevin dynamics. Empirical results on four classical domains show that our HC-Dyna algorithm significantly improves sample efficiency. Additionally, we investigate different sampling distributions for search-control and find that using samples generated by climbing on current value estimates from low-value to high-value regions provides a benefit.",1
"Despite the remarkable success of Deep RL in learning control policies from raw pixels, the resulting models do not generalize. We demonstrate that a trained agent fails completely when facing small visual changes, and that fine-tuning---the common transfer learning paradigm---fails to adapt to these changes, to the extent that it is faster to re-train the model from scratch. We show that by separating the visual transfer task from the control policy we achieve substantially better sample efficiency and transfer behavior, allowing an agent trained on the source task to transfer well to the target tasks. The visual mapping from the target to the source domain is performed using unaligned GANs, resulting in a control policy that can be further improved using imitation learning from imperfect demonstrations. We demonstrate the approach on synthetic visual variants of the Breakout game, as well as on transfer between subsequent levels of Road Fighter, a Nintendo car-driving game. A visualization of our approach can be seen in https://youtu.be/4mnkzYyXMn4 and https://youtu.be/KCGTrQi6Ogo .",0
"Although Deep RL has achieved remarkable success in learning control policies from raw pixels, these models lack generalization abilities. Our research proves that trained agents are unable to handle slight visual changes, and that fine-tuning, the typical transfer learning method, fails to adapt to those modifications. It is more effective to retrain the model from scratch than to attempt fine-tuning. By separating the visual transfer task from the control policy, we have improved sample efficiency and transfer behavior. This allows the agent trained on the source task to transfer well to target tasks. We utilize unaligned GANs to perform the visual mapping from the target to the source domain, resulting in a better control policy that can be further enhanced using imitation learning from imperfect demonstrations. Our approach has been successfully tested on synthetic visual variants of the Breakout game and transfer between subsequent levels of Road Fighter, a Nintendo car-driving game. For a demonstration of our approach, please visit https://youtu.be/4mnkzYyXMn4 and https://youtu.be/KCGTrQi6Ogo.",1
"Model-based reinforcement learning (MBRL) is widely seen as having the potential to be significantly more sample efficient than model-free RL. However, research in model-based RL has not been very standardized. It is fairly common for authors to experiment with self-designed environments, and there are several separate lines of research, which are sometimes closed-sourced or not reproducible. Accordingly, it is an open question how these various existing MBRL algorithms perform relative to each other. To facilitate research in MBRL, in this paper we gather a wide collection of MBRL algorithms and propose over 18 benchmarking environments specially designed for MBRL. We benchmark these algorithms with unified problem settings, including noisy environments. Beyond cataloguing performance, we explore and unify the underlying algorithmic differences across MBRL algorithms. We characterize three key research challenges for future MBRL research: the dynamics bottleneck, the planning horizon dilemma, and the early-termination dilemma. Finally, to maximally facilitate future research on MBRL, we open-source our benchmark in http://www.cs.toronto.edu/~tingwuwang/mbrl.html.",0
"Model-based reinforcement learning (MBRL) has the potential to be much more efficient than model-free RL, but its research lacks standardization. Authors often create their own environments, and there are multiple closed-source or non-reproducible lines of research. It is unclear how existing MBRL algorithms compare to each other. To address this, we have gathered a collection of MBRL algorithms and designed over 18 benchmarking environments specifically for MBRL. We have tested these algorithms with consistent problem settings, including noisy environments. Our study not only evaluates performance but also identifies and unifies underlying algorithmic differences among MBRL algorithms. We outline three critical research challenges for future MBRL research: the dynamics bottleneck, the planning horizon dilemma, and the early-termination dilemma. To promote future MBRL research, we have made our benchmark open-source at http://www.cs.toronto.edu/~tingwuwang/mbrl.html.",1
"We study the problem of learning sequential decision-making policies in settings with multiple state-action representations. Such settings naturally arise in many domains, such as planning (e.g., multiple integer programming formulations) and various combinatorial optimization problems (e.g., those with both integer programming and graph-based formulations). Inspired by the classical co-training framework for classification, we study the problem of co-training for policy learning. We present sufficient conditions under which learning from two views can improve upon learning from a single view alone. Motivated by these theoretical insights, we present a meta-algorithm for co-training for sequential decision making. Our framework is compatible with both reinforcement learning and imitation learning. We validate the effectiveness of our approach across a wide range of tasks, including discrete/continuous control and combinatorial optimization.",0
"Our focus is on acquiring sequential decision-making policies in situations that have multiple state-action representations. These circumstances are common in various domains, such as planning, which may have several integer programming formulations, and combinatorial optimization problems that involve both graph-based and integer programming formulations. Our approach is inspired by the co-training framework used in classification. We aim to establish the optimal conditions for learning from two perspectives to improve the performance of learning from a single perspective. Our theoretical insights motivate the development of a co-training meta-algorithm for sequential decision-making that is compatible with both reinforcement and imitation learning. We demonstrate the efficacy of our method in a broad range of tasks, including discrete/continuous control and combinatorial optimization.",1
"Before deploying autonomous agents in the real world, we need to be confident they will perform safely in novel situations. Ideally, we would expose agents to a very wide range of situations during training, allowing them to learn about every possible danger, but this is often impractical. This paper investigates safety and generalization from a limited number of training environments in deep reinforcement learning (RL). We find RL algorithms can fail dangerously on unseen test environments even when performing perfectly on training environments. Firstly, in a gridworld setting, we show that catastrophes can be significantly reduced with simple modifications, including ensemble model averaging and the use of a blocking classifier. In the more challenging CoinRun environment we find similar methods do not significantly reduce catastrophes. However, we do find that the uncertainty information from the ensemble is useful for predicting whether a catastrophe will occur within a few steps and hence whether human intervention should be requested.",0
"Prior to implementing autonomous agents in real-world situations, it is essential to ensure their safe performance in unfamiliar scenarios. While it would be ideal to expose agents to a wide range of situations during training to acquaint them with all possible risks, this is often impractical. This study examines safety and generalization in deep reinforcement learning (RL) with a limited number of training environments. Even when performing perfectly in training environments, RL algorithms can pose dangerous risks in unseen test environments. Our investigation indicates that by implementing simple modifications such as ensemble model averaging and the use of a blocking classifier, catastrophes can be significantly reduced in a gridworld setting. However, in the more challenging CoinRun environment, such techniques do not make a significant difference. Nevertheless, the ensemble's uncertainty information is beneficial in predicting the likelihood of a catastrophe within a few steps, and therefore, whether human intervention is necessary.",1
"Training a robotic arm to accomplish real-world tasks has been attracting increasing attention in both academia and industry. This work discusses the role of computer vision algorithms in this field. We focus on low-cost arms on which no sensors are equipped and thus all decisions are made upon visual recognition, e.g., real-time 3D pose estimation. This requires annotating a lot of training data, which is not only time-consuming but also laborious.   In this paper, we present an alternative solution, which uses a 3D model to create a large number of synthetic data, trains a vision model in this virtual domain, and applies it to real-world images after domain adaptation. To this end, we design a semi-supervised approach, which fully leverages the geometric constraints among keypoints. We apply an iterative algorithm for optimization. Without any annotations on real images, our algorithm generalizes well and produces satisfying results on 3D pose estimation, which is evaluated on two real-world datasets. We also construct a vision-based control system for task accomplishment, for which we train a reinforcement learning agent in a virtual environment and apply it to the real-world. Moreover, our approach, with merely a 3D model being required, has the potential to generalize to other types of multi-rigid-body dynamic systems.",0
"Recently, there has been an increasing interest in training robotic arms to perform real-world tasks, both in academia and industry. This article discusses the importance of computer vision algorithms in this field, particularly for low-cost arms without sensors that rely solely on visual recognition, such as real-time 3D pose estimation. However, annotating training data for this is not only time-consuming but also labor-intensive. To address this, the authors propose an alternative approach that uses a 3D model to generate synthetic data for training a vision model in a virtual environment. They then apply this model to real-world images using domain adaptation and a semi-supervised approach that takes into account geometric constraints among keypoints. The authors demonstrate the effectiveness of their method on 3D pose estimation and use it to construct a vision-based control system for task completion. Their approach, which only requires a 3D model, has the potential to generalize to other types of multi-rigid-body dynamic systems.",1
"There is a growing desire in the field of reinforcement learning (and machine learning in general) to move from black-box models toward more ""interpretable AI."" We improve interpretability of reinforcement learning by increasing the utility of decision tree policies learned via reinforcement learning. These policies consist of a decision tree over the state space, which requires fewer parameters to express than traditional policy representations. Existing methods for creating decision tree policies via reinforcement learning focus on accurately representing an action-value function during training, but this leads to much larger trees than would otherwise be required. To address this shortcoming, we propose a novel algorithm which only increases tree size when the estimated discounted future reward of the overall policy would increase by a sufficient amount. Through evaluation in a simulated environment, we show that its performance is comparable or superior to traditional tree-based approaches and that it yields a more succinct policy. Additionally, we discuss tuning parameters to control the tradeoff between optimizing for smaller tree size or for overall reward.",0
"The field of reinforcement learning (and machine learning in general) is increasingly interested in creating AI models that are more interpretable than black-box models. One way to achieve this is by enhancing the interpretability of decision tree policies learned through reinforcement learning. Decision tree policies are simpler than traditional policy representations, as they consist of a decision tree over the state space, and require fewer parameters to express. However, existing methods for creating these policies result in larger trees than necessary due to their focus on accurately representing an action-value function during training. To overcome this problem, we introduce a novel algorithm that enlarges the tree size only when the estimated discounted future reward of the overall policy is expected to increase significantly. We demonstrate through a simulated environment that our approach performs as well as or better than traditional tree-based techniques while yielding a more concise policy. Moreover, we explore tuning parameters to balance the tradeoff between optimizing for smaller tree size versus overall reward.",1
"In many real-world scenarios, an autonomous agent often encounters various tasks within a single complex environment. We propose to build a graph abstraction over the environment structure to accelerate the learning of these tasks. Here, nodes are important points of interest (pivotal states) and edges represent feasible traversals between them. Our approach has two stages. First, we jointly train a latent pivotal state model and a curiosity-driven goal-conditioned policy in a task-agnostic manner. Second, provided with the information from the world graph, a high-level Manager quickly finds solution to new tasks and expresses subgoals in reference to pivotal states to a low-level Worker. The Worker can then also leverage the graph to easily traverse to the pivotal states of interest, even across long distance, and explore non-locally. We perform a thorough ablation study to evaluate our approach on a suite of challenging maze tasks, demonstrating significant advantages from the proposed framework over baselines that lack world graph knowledge in terms of performance and efficiency.",0
"When an autonomous agent faces multiple tasks in a complex environment, it can be helpful to create a graph abstraction of the environment's structure to speed up task learning. This graph has nodes representing pivotal states and edges that show possible connections between them. Our two-stage approach starts by training a latent pivotal state model and a curiosity-driven goal-conditioned policy in a task-agnostic way. Then, using the world graph information, a high-level Manager can quickly find solutions to new tasks and communicate subgoals to a low-level Worker based on pivotal states. With the graph, the Worker can easily traverse to pivotal states of interest, even far away, and explore non-locally. We tested our approach on challenging maze tasks and found it significantly outperformed baselines lacking world graph knowledge in terms of efficiency and performance.",1
"We consider the exploration-exploitation trade-off in reinforcement learning and we show that an agent imbued with an epistemic-risk-seeking utility function is able to explore efficiently, as measured by regret. The parameter that controls how risk-seeking the agent is can be optimized to minimize regret, or annealed according to a schedule. We call the resulting algorithm K-learning and we show that the K-values that the agent maintains are optimistic for the expected optimal Q-values at each state-action pair. The utility function approach induces a natural Boltzmann exploration policy for which the 'temperature' parameter is equal to the risk-seeking parameter. This policy achieves a Bayesian regret bound of $\tilde O(L^{3/2} \sqrt{SAT})$, where L is the time horizon, S is the number of states, A is the number of actions, and T is the total number of elapsed time-steps. K-learning can be interpreted as mirror descent in the policy space, and it is similar to other well-known methods in the literature, including Q-learning, soft-Q-learning, and maximum entropy policy gradient. K-learning is simple to implement, as it only requires adding a bonus to the reward at each state-action and then solving a Bellman equation. We conclude with a numerical example demonstrating that K-learning is competitive with other state-of-the-art algorithms in practice.",0
"The exploration-exploitation trade-off in reinforcement learning is examined in this study, and it is demonstrated that an agent equipped with a utility function that seeks epistemic risk can effectively explore, as indicated by regret. The degree of risk-seeking behavior exhibited by the agent can be optimized to minimize regret or modified according to a predetermined schedule. The resultant algorithm, K-learning, maintains optimistic K-values for the anticipated optimal Q-values at every state-action pair. The utility function approach leads to a natural Boltzmann exploration policy, in which the temperature parameter is identical to the risk-seeking parameter. This policy achieves a Bayesian regret bound of approximately O(L^{3/2} sqrt{SAT}), where L represents the time horizon, S represents the number of states, A represents the number of actions, and T represents the total number of elapsed time-steps. K-learning can be interpreted as mirror descent in policy space and is comparable to other commonly used techniques in the literature such as Q-learning, soft-Q-learning, and maximum entropy policy gradient. K-learning is straightforward to implement by adding a bonus to the reward at each state-action and then solving a Bellman equation. Finally, a numerical example is presented, demonstrating that K-learning performs well in practice and is competitive with other advanced algorithms.",1
"Traditionally, machine learning algorithms rely on the assumption that all features of a given dataset are available for free. However, there are many concerns such as monetary data collection costs, patient discomfort in medical procedures, and privacy impacts of data collection that require careful consideration in any real-world health analytics system. An efficient solution would only acquire a subset of features based on the value it provides while considering acquisition costs. Moreover, datasets that provide feature costs are very limited, especially in healthcare. In this paper, we provide a health dataset as well as a method for assigning feature costs based on the total level of inconvenience asking for each feature entails. Furthermore, based on the suggested dataset, we provide a comparison of recent and state-of-the-art approaches to cost-sensitive feature acquisition and learning. Specifically, we analyze the performance of major sensitivity-based and reinforcement learning based methods in the literature on three different problems in the health domain, including diabetes, heart disease, and hypertension classification.",0
"In the conventional sense, machine learning algorithms assume that all features of a given dataset are readily available. However, there are numerous concerns, including the high cost of data collection, patient discomfort during medical procedures, and privacy implications of data collection, that require careful consideration in any health analytics system. To address this issue, an efficient solution would be to acquire only a subset of features based on their value, while considering acquisition costs. Nevertheless, datasets that provide feature costs are scarce, particularly in healthcare. This paper presents a health dataset and a method for assigning feature costs based on the total level of inconvenience associated with requesting each feature. Moreover, using the suggested dataset, a comparison of recent and state-of-the-art approaches to cost-sensitive feature acquisition and learning is provided. Specifically, the performance of major sensitivity-based and reinforcement learning-based methods in the literature is analyzed on three different health problems, including diabetes, heart disease, and hypertension classification.",1
"Current reinforcement learning methods fail if the reward function is imperfect, i.e. if the agent observes reward different from what it actually receives. We study this problem within the formalism of Corrupt Reward Markov Decision Processes (CRMDPs). We show that if the reward corruption in a CRMDP is sufficiently ""spiky"", the environment is solvable. We fully characterize the regret bound of a Spiky CRMDP, and introduce an algorithm that is able to detect its corrupt states. We show that this algorithm can be used to learn the optimal policy with any common reinforcement learning algorithm. Finally, we investigate our algorithm in a pair of simple gridworld environments, finding that our algorithm can detect the corrupt states and learn the optimal policy despite the corruption.",0
"Reinforcement learning methods are insufficient when there are flaws in the reward function, meaning the agent perceives a different reward than what it actually receives. Our research focuses on Corrupt Reward Markov Decision Processes (CRMDPs) to address this issue. We discovered that if the reward corruption in a CRMDP is ""spiky"" enough, the environment can be solved. We established the regret bound of a Spiky CRMDP and created an algorithm that can identify corrupt states. This algorithm can be utilized with any common reinforcement learning algorithm to learn the optimal policy. We tested our algorithm in basic gridworld environments and found that it can successfully identify corrupt states and learn the optimal policy despite the corruption.",1
"Sequence prediction models can be learned from example sequences with a variety of training algorithms. Maximum likelihood learning is simple and efficient, yet can suffer from compounding error at test time. Reinforcement learning such as policy gradient addresses the issue but can have prohibitively poor exploration efficiency. A rich set of other algorithms such as RAML, SPG, and data noising, have also been developed from different perspectives. This paper establishes a formal connection between these algorithms. We present a generalized entropy regularized policy optimization formulation, and show that the apparently distinct algorithms can all be reformulated as special instances of the framework, with the only difference being the configurations of a reward function and a couple of hyperparameters. The unified interpretation offers a systematic view of the varying properties of exploration and learning efficiency. Besides, inspired from the framework, we present a new algorithm that dynamically interpolates among the family of algorithms for scheduled sequence model learning. Experiments on machine translation, text summarization, and game imitation learning demonstrate the superiority of the proposed algorithm.",0
"Various training algorithms can be utilized to learn sequence prediction models from example sequences. While maximum likelihood learning is uncomplicated and effective, it may encounter compounding errors during testing. Reinforcement learning, such as policy gradient, addresses the problem but may not explore efficiently. Other algorithms, including RAML, SPG, and data noising, have been developed from different perspectives. This paper establishes a formal connection between these algorithms by presenting a generalized entropy regularized policy optimization formulation. The study shows that all the apparently distinct algorithms can be reformed as special instances of the framework, with the only difference being the reward function configurations and a couple of hyperparameters. This unified interpretation provides a systematic view of the varying exploration and learning efficiency properties. Additionally, a new algorithm that dynamically interpolates among the family of algorithms for scheduled sequence model learning is presented, inspired by the framework. Experiments on machine translation, text summarization, and game imitation learning demonstrate the superiority of the proposed algorithm.",1
"In complex tasks, such as those with large combinatorial action spaces, random exploration may be too inefficient to achieve meaningful learning progress. In this work, we use a curriculum of progressively growing action spaces to accelerate learning. We assume the environment is out of our control, but that the agent may set an internal curriculum by initially restricting its action space. Our approach uses off-policy reinforcement learning to estimate optimal value functions for multiple action spaces simultaneously and efficiently transfers data, value estimates, and state representations from restricted action spaces to the full task. We show the efficacy of our approach in proof-of-concept control tasks and on challenging large-scale StarCraft micromanagement tasks with large, multi-agent action spaces.",0
"Random exploration may not be effective in achieving significant progress in learning for complex tasks, particularly those with extensive combinatorial action spaces. To expedite learning, we implement a curriculum that involves action spaces that gradually increase in complexity. Although we cannot control the environment, the agent can set an internal curriculum by initially limiting its action space. Our off-policy reinforcement learning approach allows for the estimation of optimal value functions for multiple action spaces simultaneously, with efficient transfer of data, value estimates, and state representations from restricted to full action spaces. We demonstrate the effectiveness of our approach in proof-of-concept control tasks and challenging StarCraft micromanagement tasks with large, multi-agent action spaces.",1
"Temporal point process is an expressive tool for modeling event sequences over time. In this paper, we take a reinforcement learning view whereby the observed sequences are assumed to be generated from a mixture of latent policies. The purpose is to cluster the sequences with different temporal patterns into the underlying policies while learning each of the policy model. The flexibility of our model lies in: i) all the components are networks including the policy network for modeling the intensity function of temporal point process; ii) to handle varying-length event sequences, we resort to inverse reinforcement learning by decomposing the observed sequence into states (RNN hidden embedding of history) and actions (time interval to next event) in order to learn the reward function, thus achieving better performance or increasing efficiency compared to existing methods using rewards over the entire sequence such as log-likelihood or Wasserstein distance. We adopt an expectation-maximization framework with the E-step estimating the cluster labels for each sequence, and the M-step aiming to learn the respective policy. Extensive experiments show the efficacy of our method against state-of-the-arts.",0
"In this paper, we utilize temporal point process to model event sequences over time. Our approach is based on reinforcement learning, where we assume that observed sequences are generated from a mixture of latent policies. Our goal is to cluster sequences with diverse temporal patterns into underlying policies while simultaneously learning each policy model. The key feature of our model is that all components, including the policy network for modeling the intensity function of temporal point process, are networks. To accommodate varying-length event sequences, we utilize inverse reinforcement learning by breaking down the observed sequence into states (RNN hidden embedding of history) and actions (time interval to next event) in order to learn the reward function. This approach leads to better performance and increased efficiency compared to existing methods that use rewards over the entire sequence, such as log-likelihood or Wasserstein distance. We employ an expectation-maximization framework with the E-step to estimate the cluster labels for each sequence, and the M-step to learn the respective policy. Our extensive experiments demonstrate the effectiveness of our method compared to state-of-the-art techniques.",1
"A simple, flexible approach to creating expressive priors in Gaussian process (GP) models makes new kernels from a combination of basic kernels, e.g. summing a periodic and linear kernel can capture seasonal variation with a long term trend. Despite a well-studied link between GPs and Bayesian neural networks (BNNs), the BNN analogue of this has not yet been explored. This paper derives BNN architectures mirroring such kernel combinations. Furthermore, it shows how BNNs can produce periodic kernels, which are often useful in this context. These ideas provide a principled approach to designing BNNs that incorporate prior knowledge about a function. We showcase the practical value of these ideas with illustrative experiments in supervised and reinforcement learning settings.",0
"A Gaussian process (GP) model can utilize a straightforward and adaptable technique for generating expressive priors by forming novel kernels from basic kernels, such as combining a linear kernel with a periodic kernel to capture long-term trends and seasonal variations. Although there exists a well-established connection between GPs and Bayesian neural networks (BNNs), the BNN equivalent of this approach has yet to be explored. This study presents BNN architectures that mirror such kernel combinations and demonstrates how BNNs can produce periodic kernels, which are frequently advantageous in this situation. These concepts establish a systematic method for designing BNNs that integrate previous knowledge of a function. To demonstrate the practical significance of these concepts, we conduct illustrative experiments in supervised and reinforcement learning settings.",1
"The security of Deep Reinforcement Learning (Deep RL) algorithms deployed in real life applications are of a primary concern. In particular, the robustness of RL agents in cyber-physical systems against adversarial attacks are especially vital since the cost of a malevolent intrusions can be extremely high. Studies have shown Deep Neural Networks (DNN), which forms the core decision-making unit in most modern RL algorithms, are easily subjected to adversarial attacks. Hence, it is imperative that RL agents deployed in real-life applications have the capability to detect and mitigate adversarial attacks in an online fashion. An example of such a framework is the Meta-Learned Advantage Hierarchy (MLAH) agent that utilizes a meta-learning framework to learn policies robustly online. Since the mechanism of this framework are still not fully explored, we conducted multiple experiments to better understand the framework's capabilities and limitations. Our results shows that the MLAH agent exhibits interesting coping behaviors when subjected to different adversarial attacks to maintain a nominal reward. Additionally, the framework exhibits a hierarchical coping capability, based on the adaptability of the Master policy and sub-policies themselves. From empirical results, we also observed that as the interval of adversarial attacks increase, the MLAH agent can maintain a higher distribution of rewards, though at the cost of higher instabilities.",0
"Ensuring the security of Deep Reinforcement Learning (Deep RL) algorithms used in real-life applications is crucial, particularly in cyber-physical systems where adversarial attacks can be very costly. Studies reveal that Deep Neural Networks (DNNs), which are the core decision-making units in modern RL algorithms, are highly vulnerable to such attacks. Therefore, it is essential for RL agents in real-life applications to be able to detect and mitigate adversarial attacks online. The Meta-Learned Advantage Hierarchy (MLAH) agent is an example of such a framework that uses a meta-learning approach to learn robust policies online. We conducted several experiments to explore the capabilities and limitations of the MLAH agent, and our findings show that it exhibits interesting coping behaviors when faced with various adversarial attacks to maintain nominal rewards. Furthermore, the framework has a hierarchical coping ability, which depends on both the adaptability of the Master policy and sub-policies. Our empirical results also demonstrate that as the interval of adversarial attacks increases, the MLAH agent can maintain a higher distribution of rewards, but at the cost of greater instability.",1
"Advances in renewable energy generation and introduction of the government targets to improve energy efficiency gave rise to a concept of a Zero Energy Building (ZEB). A ZEB is a building whose net energy usage over a year is zero, i.e., its energy use is not larger than its overall renewables generation. A collection of ZEBs forms a Zero Energy Community (ZEC). This paper addresses the problem of energy sharing in such a community. This is different from previously addressed energy sharing between buildings as our focus is on the improvement of community energy status, while traditionally research focused on reducing losses due to transmission and storage, or achieving economic gains. We model this problem in a multi-agent environment and propose a Deep Reinforcement Learning (DRL) based solution. Each building is represented by an intelligent agent that learns over time the appropriate behaviour to share energy. We have evaluated the proposed solution in a multi-agent simulation built using osBrain. Results indicate that with time agents learn to collaborate and learn a policy comparable to the optimal policy, which in turn improves the ZEC's energy status. Buildings with no renewables preferred to request energy from their neighbours rather than from the supply grid.",0
"The emergence of the Zero Energy Building (ZEB) concept was driven by advancements in renewable energy generation and the government's push to enhance energy efficiency. ZEBs are buildings that consume only as much energy as they generate through renewables over a year, and a group of such structures constitutes a Zero Energy Community (ZEC). In this paper, we tackle the issue of energy sharing within a ZEC. Unlike previous research that focused on minimizing transmission and storage losses or achieving economic gains, our focus is on improving the community's energy status. We propose a Deep Reinforcement Learning (DRL) based solution, where each building is represented by an intelligent agent that learns how to share energy with other agents in a multi-agent environment. Our evaluation, conducted using osBrain, reveals that the agents gradually learn to collaborate and adopt a policy that enhances the ZEC's energy status, with non-renewable buildings preferring to obtain energy from their neighbors rather than the supply grid.",1
"Hyperparameter tuning is an omnipresent problem in machine learning as it is an integral aspect of obtaining the state-of-the-art performance for any model. Most often, hyperparameters are optimized just by training a model on a grid of possible hyperparameter values and taking the one that performs best on a validation sample (grid search). More recently, methods have been introduced that build a so-called surrogate model that predicts the validation loss for a specific hyperparameter setting, model and dataset and then sequentially select the next hyperparameter to test, based on a heuristic function of the expected value and the uncertainty of the surrogate model called acquisition function (sequential model-based Bayesian optimization, SMBO).   In this paper we model the hyperparameter optimization problem as a sequential decision problem, which hyperparameter to test next, and address it with reinforcement learning. This way our model does not have to rely on a heuristic acquisition function like SMBO, but can learn which hyperparameters to test next based on the subsequent reduction in validation loss they will eventually lead to, either because they yield good models themselves or because they allow the hyperparameter selection policy to build a better surrogate model that is able to choose better hyperparameters later on. Experiments on a large battery of 50 data sets demonstrate that our method outperforms the state-of-the-art approaches for hyperparameter learning.",0
"The process of hyperparameter tuning is an essential component of achieving optimal performance in machine learning models. Traditionally, hyperparameters are optimized by training models using a grid of potential values and selecting the option that performs best on a validation sample (known as grid search). More recently, a surrogate model approach has been introduced, which predicts the validation loss for specific hyperparameters, models, and datasets. This method sequentially selects the next hyperparameter to test using an acquisition function based on the expected value and uncertainty of the surrogate model (known as sequential model-based Bayesian optimization, SMBO). In this study, we propose a reinforcement learning approach to model hyperparameter optimization as a sequential decision problem. This method eliminates the need for a heuristic acquisition function like SMBO, as the model learns which hyperparameters to test based on their potential for reducing validation loss. Our experiments using 50 datasets demonstrate that our method outperforms existing approaches to hyperparameter learning.",1
"Recommender systems play a crucial role in mitigating the problem of information overload by suggesting users' personalized items or services. The vast majority of traditional recommender systems consider the recommendation procedure as a static process and make recommendations following a fixed strategy. In this paper, we propose a novel recommender system with the capability of continuously improving its strategies during the interactions with users. We model the sequential interactions between users and a recommender system as a Markov Decision Process (MDP) and leverage Reinforcement Learning (RL) to automatically learn the optimal strategies via recommending trial-and-error items and receiving reinforcements of these items from users' feedbacks. In particular, we introduce an online user-agent interacting environment simulator, which can pre-train and evaluate model parameters offline before applying the model online. Moreover, we validate the importance of list-wise recommendations during the interactions between users and agent, and develop a novel approach to incorporate them into the proposed framework LIRD for list-wide recommendations. The experimental results based on a real-world e-commerce dataset demonstrate the effectiveness of the proposed framework.",0
"The problem of information overload can be addressed by recommenders that suggest personalized items or services to users. However, most traditional systems have fixed strategies for making recommendations. This paper proposes a recommender system that continuously improves its strategies through interactions with users. The system models these interactions as a Markov Decision Process and uses Reinforcement Learning to learn optimal strategies by recommending trial-and-error items and receiving feedback from users. The proposed framework includes an online user-agent simulator for offline pre-training and evaluation of model parameters. The system also incorporates list-wise recommendations and introduces a novel approach for list-wide recommendations. The proposed framework is validated on a real-world e-commerce dataset, demonstrating its effectiveness.",1
"Standard computer vision systems assume access to intelligently captured inputs (e.g., photos from a human photographer), yet autonomously capturing good observations is a major challenge in itself. We address the problem of learning to look around: how can an agent learn to acquire informative visual observations? We propose a reinforcement learning solution, where the agent is rewarded for reducing its uncertainty about the unobserved portions of its environment. Specifically, the agent is trained to select a short sequence of glimpses after which it must infer the appearance of its full environment. To address the challenge of sparse rewards, we further introduce sidekick policy learning, which exploits the asymmetry in observability between training and test time. The proposed methods learn observation policies that not only perform the completion task for which they are trained, but also generalize to exhibit useful ""look-around"" behavior for a range of active perception tasks.",0
"Computer vision systems typically rely on well-captured inputs, such as photographs taken by a skilled photographer, but capturing good observations independently is a significant challenge. Our focus is on solving the problem of learning how to observe effectively. We suggest a reinforcement learning approach where the agent is incentivized to minimize its uncertainty about unobserved areas in its environment. The agent is trained to choose a brief sequence of glimpses that will allow it to deduce the appearance of its surroundings. We address the difficulty of insufficient rewards by introducing sidekick policy learning, which utilizes the asymmetry in observability between training and testing. Our methods teach observation policies that not only meet the completion task but also demonstrate valuable ""look-around"" behavior for a variety of active perception tasks.",1
"We consider the finite horizon continuous reinforcement learning problem. Our contribution is three-fold. First,we give a tractable algorithm based on optimistic value iteration for the problem. Next,we give a lower bound on regret of order $\Omega(T^{2/3})$ for any algorithm discretizes the state space, improving the previous regret bound of $\Omega(T^{1/2})$ of Ortner and Ryabko \cite{contrl} for the same problem. Next,under the assumption that the rewards and transitions are H\""{o}lder Continuous we show that the upper bound on the discretization error is $const.Ln^{-\alpha}T$. Finally,we give some simple experiments to validate our propositions.",0
"The problem we are addressing is the finite horizon continuous reinforcement learning problem. Our contribution consists of three parts. Firstly, we provide a feasible algorithm utilizing optimistic value iteration to solve the problem. Secondly, we improve the regret bound for any algorithm that discretizes the state space from Ortner and Ryabko's previous bound of $\Omega(T^{1/2})$ to a new, lower bound of $\Omega(T^{2/3})$. Thirdly, assuming that the rewards and transitions are H\""{o}lder Continuous, we demonstrate that the discretization error's upper bound is $const.Ln^{-\alpha}T$. Lastly, we conduct some straightforward experiments to support our arguments.",1
"As reinforcement learning (RL) scales to solve increasingly complex tasks, interest continues to grow in the fields of AI safety and machine ethics. As a contribution to these fields, this paper introduces an extension to Deep Q-Networks (DQNs), called Empathic DQN, that is loosely inspired both by empathy and the golden rule (""Do unto others as you would have them do unto you""). Empathic DQN aims to help mitigate negative side effects to other agents resulting from myopic goal-directed behavior. We assume a setting where a learning agent coexists with other independent agents (who receive unknown rewards), where some types of reward (e.g. negative rewards from physical harm) may generalize across agents. Empathic DQN combines the typical (self-centered) value with the estimated value of other agents, by imagining (by its own standards) the value of it being in the other's situation (by considering constructed states where both agents are swapped). Proof-of-concept results in two gridworld environments highlight the approach's potential to decrease collateral harms. While extending Empathic DQN to complex environments is non-trivial, we believe that this first step highlights the potential of bridge-work between machine ethics and RL to contribute useful priors for norm-abiding RL agents.",0
"As reinforcement learning (RL) continues to tackle more complex tasks, the interest in AI safety and machine ethics also grows. This paper proposes an extension to Deep Q-Networks (DQNs), called Empathic DQN, as a contribution to these fields. Empathic DQN is inspired by empathy and the golden rule, aiming to prevent negative side effects resulting from goal-directed behavior of a learning agent in a setting where it coexists with other independent agents. Empathic DQN combines the self-centered value with the estimated value of other agents by imagining the value of it being in the other's situation. Two gridworld environments show the approach's potential to decrease collateral harms, and although extending it to complex environments is challenging, this first step showcases the potential of bridging machine ethics and RL to provide useful priors for norm-abiding RL agents.",1
"We propose a new perspective on representation learning in reinforcement learning based on geometric properties of the space of value functions. We leverage this perspective to provide formal evidence regarding the usefulness of value functions as auxiliary tasks. Our formulation considers adapting the representation to minimize the (linear) approximation of the value function of all stationary policies for a given environment. We show that this optimization reduces to making accurate predictions regarding a special class of value functions which we call adversarial value functions (AVFs). We demonstrate that using value functions as auxiliary tasks corresponds to an expected-error relaxation of our formulation, with AVFs a natural candidate, and identify a close relationship with proto-value functions (Mahadevan, 2005). We highlight characteristics of AVFs and their usefulness as auxiliary tasks in a series of experiments on the four-room domain.",0
"A new viewpoint is proposed for representation learning in reinforcement learning by analyzing the geometric characteristics of the value function space. This perspective is utilized to offer formal proof of the value functions' effectiveness as auxiliary tasks. Our approach involves modifying the representation to minimize the linear approximation of the value function for all stationary policies in a given environment. We prove that this optimization can be achieved by accurately predicting a specific type of value function called adversarial value functions (AVFs). Additionally, we demonstrate that using value functions as auxiliary tasks is a relaxation of our formulation that reduces expected error, with AVFs being a natural option. We also identify a correlation between AVFs and proto-value functions (Mahadevan, 2005). Finally, using the four-room domain, we conduct a series of experiments that highlight the features of AVFs and their importance as auxiliary tasks.",1
"Hierarchy in reinforcement learning agents allows for control at multiple time scales yielding improved sample efficiency, the ability to deal with long time horizons and transferability of sub-policies to tasks outside the training distribution. It is often implemented as a master policy providing goals to a sub-policy. Ideally, we would like the goal-spaces to be learned, however, properties of optimal goal spaces still remain unknown and consequently there is no method yet to learn optimal goal spaces. Motivated by this, we systematically analyze how various modifications to the ground-truth goal-space affect learning in hierarchical models with the aim of identifying important properties of optimal goal spaces. Our results show that, while rotation of ground-truth goal spaces and noise had no effect, having additional unnecessary factors significantly impaired learning in hierarchical models.",0
"Reinforcement learning agents that implement hierarchy allow for control at multiple time scales, resulting in improved sample efficiency, the ability to handle long-term goals, and the transferability of sub-policies to tasks outside of the training distribution. Typically, a master policy is utilized to provide goals to a sub-policy. Ideally, we would like to learn the goal-spaces, but since the properties of optimal goal spaces are still unknown, there is no current method to do so. To address this issue, we conducted a systematic analysis of how modifications to the ground-truth goal-space affect learning in hierarchical models. Our findings indicate that rotating the ground-truth goal spaces and adding noise had no effect, but including unnecessary factors significantly impaired learning in hierarchical models.",1
"Model-based reinforcement learning has the potential to be more sample efficient than model-free approaches. However, existing model-based methods are vulnerable to model bias, which leads to poor generalization and asymptotic performance compared to model-free counterparts. In addition, they are typically based on the model predictive control (MPC) framework, which not only is computationally inefficient at decision time but also does not enable policy transfer due to the lack of an explicit policy representation. In this paper, we propose a novel uncertainty-aware model-based policy optimization framework which solves those issues. In this framework, the agent simultaneously learns an uncertainty-aware dynamics model and optimizes the policy according to these learned models. In the optimization step, the policy gradient is computed by automatic differentiation through the models. With respect to sample efficiency alone, our approach shows promising results on challenging continuous control benchmarks with competitive asymptotic performance and significantly lower sample complexity than state-of-the-art baselines.",0
"The potential for greater sample efficiency exists with model-based reinforcement learning in comparison to model-free approaches. However, current model-based methods are susceptible to model bias, leading to inferior generalization and asymptotic performance when compared to model-free counterparts. Moreover, these methods typically rely on the computationally inefficient model predictive control (MPC) framework and lack an explicit policy representation, which hinders policy transfer. In this study, we introduce a new uncertainty-aware model-based policy optimization framework that addresses these limitations. Our framework allows the agent to learn an uncertainty-aware dynamics model while simultaneously optimizing the policy based on these models. We compute the policy gradient using automatic differentiation through the models during the optimization step. Our approach demonstrates promising results on challenging continuous control benchmarks in terms of sample efficiency alone, exhibiting competitive asymptotic performance and significantly lower sample complexity than current state-of-the-art baselines.",1
"Reinforcement learning agents that operate in diverse and complex environments can benefit from the structured decomposition of their behavior. Often, this is addressed in the context of hierarchical reinforcement learning, where the aim is to decompose a policy into lower-level primitives or options, and a higher-level meta-policy that triggers the appropriate behaviors for a given situation. However, the meta-policy must still produce appropriate decisions in all states. In this work, we propose a policy design that decomposes into primitives, similarly to hierarchical reinforcement learning, but without a high-level meta-policy. Instead, each primitive can decide for themselves whether they wish to act in the current state. We use an information-theoretic mechanism for enabling this decentralized decision: each primitive chooses how much information it needs about the current state to make a decision and the primitive that requests the most information about the current state acts in the world. The primitives are regularized to use as little information as possible, which leads to natural competition and specialization. We experimentally demonstrate that this policy architecture improves over both flat and hierarchical policies in terms of generalization.",0
"Agents using reinforcement learning in complex and varied environments can benefit from structured behavior decomposition. This is typically accomplished through hierarchical reinforcement learning, which breaks down policies into lower-level primitives or options and a higher-level meta-policy that chooses appropriate behaviors for specific situations. However, the meta-policy must still make appropriate decisions in all states. This study proposes a policy design that breaks down into primitives like hierarchical reinforcement learning, but without a high-level meta-policy. Instead, each primitive can decide whether to act in the current state independently. An information-theoretic mechanism is used to enable this decentralized decision-making, with each primitive determining the amount of information needed to make a decision. The primitive requiring the most information acts in the world. The primitives are incentivized to use as little information as possible, leading to natural competition and specialization. This policy architecture is experimentally shown to improve generalization compared to both flat and hierarchical policies.",1
"Reinforcement Learning, a machine learning framework for training an autonomous agent based on rewards, has shown outstanding results in various domains. However, it is known that learning a good policy is difficult in a domain where rewards are rare. We propose a method, optimistic proximal policy optimization (OPPO) to alleviate this difficulty. OPPO considers the uncertainty of the estimated total return and optimistically evaluates the policy based on that amount. We show that OPPO outperforms the existing methods in a tabular task.",0
"Various domains have witnessed the exceptional performance of Reinforcement Learning, which is a machine learning framework that trains an independent agent on rewards. Nonetheless, acquiring a good policy is challenging in a domain that lacks rewards. Therefore, we suggest a solution called optimistic proximal policy optimization (OPPO) that can ease this hurdle. OPPO assesses the policy based on the estimated total return's uncertainty and optimistically evaluates it. Our study indicates that OPPO surpasses the current methods in a tabular task.",1
"Motivated by the study of $Q$-learning algorithms in reinforcement learning, we study a class of stochastic approximation procedures based on operators that satisfy monotonicity and quasi-contractivity conditions with respect to an underlying cone. We prove a general sandwich relation on the iterate error at each time, and use it to derive non-asymptotic bounds on the error in terms of a cone-induced gauge norm. These results are derived within a deterministic framework, requiring no assumptions on the noise. We illustrate these general bounds in application to synchronous $Q$-learning for discounted Markov decision processes with discrete state-action spaces, in particular by deriving non-asymptotic bounds on the $\ell_\infty$-norm for a range of stepsizes. These results are the sharpest known to date, and we show via simulation that the dependence of our bounds cannot be improved in a worst-case sense. These results show that relative to a model-based $Q$-iteration, the $\ell_\infty$-based sample complexity of $Q$-learning is suboptimal in terms of the discount factor $\gamma$.",0
"Our focus is on a category of stochastic approximation methods that build on operators satisfying monotonicity and quasi-contractivity conditions related to a base cone. Our interest in investigating these methods stems from their association with $Q$-learning algorithms utilized in reinforcement learning. We offer a general sandwich relation on the error at each iteration, which we use to deduce non-asymptotic error bounds in relation to a gauge norm generated by the underlying cone. Our approach is deterministic, without any assumptions about the noise. We apply the derived results to synchronous $Q$-learning for discounted Markov decision processes with discrete state-action spaces. Specifically, we establish non-asymptotic bounds on the $\ell_\infty$-norm for diverse step sizes, which are the most precise to date. Through simulations, we demonstrate that our bounds cannot be surpassed in the worst-case scenario. Our findings reveal that, compared to model-based $Q$-iteration, the $\ell_\infty$-based sample complexity of $Q$-learning is not optimal regarding the discount factor $\gamma$.",1
"When agents interact with a complex environment, they must form and maintain beliefs about the relevant aspects of that environment. We propose a way to efficiently train expressive generative models in complex environments. We show that a predictive algorithm with an expressive generative model can form stable belief-states in visually rich and dynamic 3D environments. More precisely, we show that the learned representation captures the layout of the environment as well as the position and orientation of the agent. Our experiments show that the model substantially improves data-efficiency on a number of reinforcement learning (RL) tasks compared with strong model-free baseline agents. We find that predicting multiple steps into the future (overshooting), in combination with an expressive generative model, is critical for stable representations to emerge. In practice, using expressive generative models in RL is computationally expensive and we propose a scheme to reduce this computational burden, allowing us to build agents that are competitive with model-free baselines.",0
"In order for agents to effectively navigate a complex environment, they must establish and maintain beliefs about the relevant factors within that environment. Our proposal outlines an efficient method for training expressive generative models within these complex environments. Through our research, we have demonstrated that a predictive algorithm utilizing an expressive generative model can establish stable belief-states in dynamic 3D environments with rich visual content. Our experiments indicate that the learned representation captures the environment's layout and the agent's position and orientation. In comparison to strong model-free baseline agents, our model substantially enhances data-efficiency on various reinforcement learning (RL) tasks. We have discovered that predicting multiple future steps (overshooting) with an expressive generative model is essential for stable representations to emerge. Employing expressive generative models in RL can be computationally intensive, but we have devised a method to reduce this computational burden, enabling the creation of agents that are just as competitive as model-free baselines.",1
"In Reinforcement Learning we look for meaning in the flow of input/output information. If we do not find meaning, the information flow is not more than noise to us. Before we are able to find meaning, we should first learn how to discover and identify objects. What is an object? In this article we will demonstrate that an object is an event-driven model. These models are a generalization of action-driven models. In Markov Decision Process we have an action-driven model which changes its state at each step. The advantage of event-driven models is their greater sustainability as they change their states only upon the occurrence of particular events. These events may occur very rarely, therefore the state of the event-driven model is much more predictable.",0
"The focus of Reinforcement Learning is on extracting significance from the input/output data flow. Without any significance, the flow of data is insignificant noise. In order to derive meaning, it is essential to acquire the skill of object recognition. But what exactly constitutes an object? This article proposes that an object is an event-driven model, which is a broader concept than an action-driven model. Typically, in a Markov Decision Process, an action-driven model changes its state at every step. However, the advantage of event-driven models is that they only change their state upon the occurrence of specific events, which may be rare. Therefore, the state of an event-driven model is much more predictable.",1
"Although reinforcement learning has made great strides recently, a continuing limitation is that it requires an extremely high number of interactions with the environment. In this paper, we explore the effectiveness of reusing experience from the experience replay buffer in the Deep Q-Learning algorithm. We test the effectiveness of applying learning update steps multiple times per environmental step in the VizDoom environment and show first, this requires a change in the learning rate, and second that it does not improve the performance of the agent. Furthermore, we show that updating less frequently is effective up to a ratio of 4:1, after which performance degrades significantly. These results quantitatively confirm the widespread practice of performing learning updates every 4th environmental step.",0
"While reinforcement learning has progressed significantly, a major drawback remains: it necessitates a great many interactions with the environment. This study examines the usefulness of reusing experience from the experience replay buffer in the Deep Q-Learning algorithm. We evaluate the effectiveness of executing learning updates multiple times per environmental step in the VizDoom environment and demonstrate that this necessitates altering the learning rate and does not enhance the agent's performance. We also demonstrate that updating less often is beneficial up to a 4:1 ratio, beyond which performance deteriorates considerably. These findings support the common approach of performing learning updates every fourth environmental step.",1
"Our goal is for agents to optimize the right reward function, despite how difficult it is for us to specify what that is. Inverse Reinforcement Learning (IRL) enables us to infer reward functions from demonstrations, but it usually assumes that the expert is noisily optimal. Real people, on the other hand, often have systematic biases: risk-aversion, myopia, etc. One option is to try to characterize these biases and account for them explicitly during learning. But in the era of deep learning, a natural suggestion researchers make is to avoid mathematical models of human behavior that are fraught with specific assumptions, and instead use a purely data-driven approach. We decided to put this to the test -- rather than relying on assumptions about which specific bias the demonstrator has when planning, we instead learn the demonstrator's planning algorithm that they use to generate demonstrations, as a differentiable planner. Our exploration yielded mixed findings: on the one hand, learning the planner can lead to better reward inference than relying on the wrong assumption; on the other hand, this benefit is dwarfed by the loss we incur by going from an exact to a differentiable planner. This suggest that at least for the foreseeable future, agents need a middle ground between the flexibility of data-driven methods and the useful bias of known human biases. Code is available at https://tinyurl.com/learningbiases.",0
"Our objective is to have agents optimize the appropriate reward function, even though it is challenging to specify. Inverse Reinforcement Learning (IRL) is useful for inferring reward functions from demonstrations, but it assumes that the expert is imperfectly optimal. Conversely, actual individuals often have systematic biases, such as risk-aversion and myopia. One option is to identify these biases and take them into account during learning. However, in the era of deep learning, researchers suggest a data-driven approach instead of mathematical models of human behavior with specific assumptions. We tested this approach by learning the demonstrator's planning algorithm that they utilized to generate demonstrations, as a differentiable planner, instead of relying on assumptions about the specific bias the demonstrator has when planning. Our exploration showed mixed results. While learning the planner can lead to better reward inference than relying on the wrong assumption, the benefit is outweighed by the loss incurred by going from an exact to a differentiable planner. This indicates that agents require a middle ground between the flexibility of data-driven methods and the useful bias of known human biases. The code is available at https://tinyurl.com/learningbiases.",1
"Scaling end-to-end reinforcement learning to control real robots from vision presents a series of challenges, in particular in terms of sample efficiency. Against end-to-end learning, state representation learning can help learn a compact, efficient and relevant representation of states that speeds up policy learning, reducing the number of samples needed, and that is easier to interpret. We evaluate several state representation learning methods on goal based robotics tasks and propose a new unsupervised model that stacks representations and combines strengths of several of these approaches. This method encodes all the relevant features, performs on par or better than end-to-end learning with better sample efficiency, and is robust to hyper-parameters change.",0
"The process of using end-to-end reinforcement learning for controlling actual robots using sight poses various difficulties, particularly regarding the effectiveness of sample collection. However, state representation learning can aid in the acquisition of a concise, effective, and applicable representation of states, which hastens policy learning, decreases the quantity of necessary samples, and simplifies interpretation. We assess multiple state representation learning techniques in goal-oriented robotics tasks and introduce a novel unsupervised model that combines the strengths of several approaches by stacking representations. This method records all relevant characteristics, performs as well as or better than end-to-end learning with superior sample efficiency, and is resilient to changes in hyper-parameters.",1
"Model-based reinforcement learning (RL) has proven to be a data efficient approach for learning control tasks but is difficult to utilize in domains with complex observations such as images. In this paper, we present a method for learning representations that are suitable for iterative model-based policy improvement, even when the underlying dynamical system has complex dynamics and image observations, in that these representations are optimized for inferring simple dynamics and cost models given data from the current policy. This enables a model-based RL method based on the linear-quadratic regulator (LQR) to be used for systems with image observations. We evaluate our approach on a range of robotics tasks, including manipulation with a real-world robotic arm directly from images. We find that our method produces substantially better final performance than other model-based RL methods while being significantly more efficient than model-free RL.",0
"Learning control tasks through model-based reinforcement learning (RL) has been proven to be data efficient. However, it is challenging to implement this approach in domains with complex observations such as images. Our paper proposes a method for learning representations that are suitable for iterative model-based policy improvement, even when dealing with complex dynamics and image observations. These representations are optimized to infer simple dynamics and cost models from data generated by the current policy. This enables the use of a model-based RL method based on the linear-quadratic regulator (LQR) for systems with image observations. Our approach is evaluated on various robotics tasks, including manipulation with a real-world robotic arm from images. Our results show that our method significantly outperforms other model-based RL methods while being more efficient than model-free RL.",1
"We consider the problem of imitation learning from expert demonstrations in partially observable Markov decision processes (POMDPs). Belief representations, which characterize the distribution over the latent states in a POMDP, have been modeled using recurrent neural networks and probabilistic latent variable models, and shown to be effective for reinforcement learning in POMDPs. In this work, we investigate the belief representation learning problem for generative adversarial imitation learning in POMDPs. Instead of training the belief module and the policy separately as suggested in prior work, we learn the belief module jointly with the policy, using a task-aware imitation loss to ensure that the representation is more aligned with the policy's objective. To improve robustness of representation, we introduce several informative belief regularization techniques, including multi-step prediction of dynamics and action-sequences. Evaluated on various partially observable continuous-control locomotion tasks, our belief-module imitation learning approach (BMIL) substantially outperforms several baselines, including the original GAIL algorithm and the task-agnostic belief learning algorithm. Extensive ablation analysis indicates the effectiveness of task-aware belief learning and belief regularization.",0
"The focus of our study is on how to learn from expert demonstrations in partially observable Markov decision processes (POMDPs) using imitation learning. Recurrent neural networks and probabilistic latent variable models have been utilized to model belief representations, which describe the distribution over latent states in POMDPs, and have proven effective for reinforcement learning in such systems. We explore the problem of belief representation learning for generative adversarial imitation learning in POMDPs. Instead of training the belief module and policy separately, as suggested in previous research, we jointly learn both modules using a task-specific imitation loss to ensure alignment with the policy's objective. Additionally, we introduce informative belief regularization techniques, such as multi-step prediction of dynamics and action-sequences, to enhance the robustness of the representation. Our approach, called the belief-module imitation learning approach (BMIL), performs significantly better than several baselines including the original GAIL algorithm and the task-agnostic belief learning algorithm when evaluated on various partially observable continuous-control locomotion tasks. Our extensive ablation analysis demonstrates the effectiveness of task-aware belief learning and belief regularization.",1
"Animals need to devise strategies to maximize returns while interacting with their environment based on incoming noisy sensory observations. Task-relevant states, such as the agent's location within an environment or the presence of a predator, are often not directly observable but must be inferred using available sensory information. Successor representations (SR) have been proposed as a middle-ground between model-based and model-free reinforcement learning strategies, allowing for fast value computation and rapid adaptation to changes in the reward function or goal locations. Indeed, recent studies suggest that features of neural responses are consistent with the SR framework. However, it is not clear how such representations might be learned and computed in partially observed, noisy environments. Here, we introduce a neurally plausible model using distributional successor features, which builds on the distributed distributional code for the representation and computation of uncertainty, and which allows for efficient value function computation in partially observed environments via the successor representation. We show that distributional successor features can support reinforcement learning in noisy environments in which direct learning of successful policies is infeasible.",0
"To optimize their interactions with the environment, animals must develop strategies based on incomplete and unreliable sensory information. Relevant information, such as location or the presence of a predator, is often inferred from available sensory data. Successor representations (SR) offer a balance between model-based and model-free reinforcement learning, allowing for quick adaptation to changes in reward or goals. Recent studies suggest that SR may align with neural responses, but it is unclear how they are learned in noisy environments. In this study, we propose a neural model utilizing distributional successor features that can efficiently compute value functions in partially observed environments, enabling reinforcement learning in noisy environments where direct policy learning is difficult. The model builds on the distributed distributional code for uncertainty representation and computation.",1
"Rewards and punishments in different forms are pervasive and present in a wide variety of decision-making scenarios. By observing the outcome of a sufficient number of repeated trials, one would gradually learn the value and usefulness of a particular policy or strategy. However, in a given environment, the outcomes resulting from different trials are subject to chance influence and variations. In learning about the usefulness of a given policy, significant costs are involved in systematically undertaking the sequential trials; therefore, in most learning episodes, one would wish to keep the cost within bounds by adopting learning stopping rules. In this paper, we examine the deployment of different stopping strategies in given learning environments which vary from highly stringent for mission critical operations to highly tolerant for non-mission critical operations, and emphasis is placed on the former with particular application to aviation safety. In policy evaluation, two sequential phases of learning are identified, and we describe the outcomes variations using a probabilistic model, with closedform expressions obtained for the key measures of performance. Decision rules that map the trial observations to policy choices are also formulated. In addition, simulation experiments are performed, which corroborate the validity of the theoretical results.",0
"Different forms of rewards and punishments are present in many decision-making scenarios. One can gradually learn the value and usefulness of a policy or strategy by observing the outcome of repeated trials. However, chance influence and variations can affect the outcomes in a given environment. Undertaking sequential trials to learn about a policy's usefulness incurs significant costs, so it is necessary to adopt learning stopping rules to keep the cost within bounds. This paper examines the deployment of different stopping strategies in various learning environments, with emphasis on mission-critical operations such as aviation safety. We identify two sequential phases of learning in policy evaluation and describe outcome variations using a probabilistic model. We also formulate decision rules that map trial observations to policy choices and perform simulation experiments to validate our theoretical results.",1
"End-to-end reinforcement learning agents learn a state representation and a policy at the same time. Recurrent neural networks (RNNs) have been trained successfully as reinforcement learning agents in settings like dialogue that require structured prediction. In this paper, we investigate the representations learned by RNN-based agents when trained with both policy gradient and value-based methods. We show through extensive experiments and analysis that, when trained with policy gradient, recurrent neural networks often fail to learn a state representation that leads to an optimal policy in settings where the same action should be taken at different states. To explain this failure, we highlight the problem of state aliasing, which entails conflating two or more distinct states in the representation space. We demonstrate that state aliasing occurs when several states share the same optimal action and the agent is trained via policy gradient. We characterize this phenomenon through experiments on a simple maze setting and a more complex text-based game, and make recommendations for training RNNs with reinforcement learning.",0
"This paper examines the effectiveness of recurrent neural networks (RNNs) as reinforcement learning agents in structured prediction scenarios, such as dialogue. These agents learn both a state representation and a policy simultaneously. The study investigates the quality of the representations learned by RNN-based agents trained with both policy gradient and value-based methods. The results of the experiments reveal that RNNs trained with policy gradient often struggle to learn a state representation that leads to an optimal policy in situations where the same action should be taken at different states. The reason for this issue is state aliasing, where multiple distinct states are combined in the representation space, resulting in the same optimal action. The paper provides insights into this phenomenon through experiments on a simple maze setting and a more complex text-based game, and recommends best practices for training RNNs with reinforcement learning.",1
"We propose a novel framework for multi-task reinforcement learning (MTRL). Using a variational inference formulation, we learn policies that generalize across both changing dynamics and goals. The resulting policies are parametrized by shared parameters that allow for transfer between different dynamics and goal conditions, and by task-specific latent-space embeddings that allow for specialization to particular tasks. We show how the latent-spaces enable generalization to unseen dynamics and goals conditions. Additionally, policies equipped with such embeddings serve as a space of skills (or options) for hierarchical reinforcement learning. Since we can change task dynamics and goals independently, we name our framework Disentangled Skill Embeddings (DSE).",0
"Our proposed framework for multi-task reinforcement learning (MTRL) involves a unique approach. By utilizing a variational inference formulation, we can develop policies that can adapt to varying dynamics and goals. These policies rely on shared parameters for transferability between different dynamics and goals, as well as task-specific latent-space embeddings for specialization in specific tasks. The latent-spaces allow for generalization to unknown dynamics and goals, and the policies equipped with such embeddings can function as a skill or option space for hierarchical reinforcement learning. Our framework is named Disentangled Skill Embeddings (DSE), as we can alter task dynamics and goals independently.",1
"Deep reinforcement learning has made significant progress in the field of continuous control, such as physical control and autonomous driving. However, it is challenging for a reinforcement model to learn a policy for each task sequentially due to catastrophic forgetting. Specifically, the model would forget knowledge it learned in the past when trained on a new task. We consider this challenge from two perspectives: i) acquiring task-specific skills is difficult since task information and rewards are not highly related; ii) learning knowledge from previous experience is difficult in continuous control domains. In this paper, we introduce an end-to-end framework namely Continual Diversity Adversarial Network (CDAN). We first develop an unsupervised diversity exploration method to learn task-specific skills using an unsupervised objective. Then, we propose an adversarial self-correction mechanism to learn knowledge by exploiting past experience. The two learning procedures are presumably reciprocal. To evaluate the proposed method, we propose a new continuous reinforcement learning environment named Continual Ant Maze (CAM) and a new metric termed Normalized Shorten Distance (NSD). The experimental results confirm the effectiveness of diversity exploration and self-correction. It is worthwhile noting that our final result outperforms baseline by 18.35% in terms of NSD, and 0.61 according to the average reward.",0
"The field of continuous control, such as physical control and autonomous driving, has seen significant progress with the use of deep reinforcement learning. However, catastrophic forgetting presents a challenge for reinforcement models when learning policies for tasks sequentially. This occurs when the model forgets past knowledge when trained on a new task. Two perspectives are considered for this challenge: the difficulty of acquiring task-specific skills due to the lack of relation between task information and rewards, and the difficulty of learning from previous experience in continuous control domains. To address this, we propose the Continual Diversity Adversarial Network (CDAN) framework, which includes an unsupervised diversity exploration method to learn task-specific skills and an adversarial self-correction mechanism to learn from past experience. We evaluate our approach using a new continuous reinforcement learning environment called Continual Ant Maze (CAM) and a new metric called Normalized Shorten Distance (NSD). Our experimental results demonstrate the effectiveness of diversity exploration and self-correction, with our final result outperforming the baseline by 18.35% in terms of NSD and 0.61 in average reward.",1
"Exploration in sparse reward reinforcement learning remains an open challenge. Many state-of-the-art methods use intrinsic motivation to complement the sparse extrinsic reward signal, giving the agent more opportunities to receive feedback during exploration. Commonly these signals are added as bonus rewards, which results in a mixture policy that neither conducts exploration nor task fulfillment resolutely. In this paper, we instead learn separate intrinsic and extrinsic task policies and schedule between these different drives to accelerate exploration and stabilize learning. Moreover, we introduce a new type of intrinsic reward denoted as successor feature control (SFC), which is general and not task-specific. It takes into account statistics over complete trajectories and thus differs from previous methods that only use local information to evaluate intrinsic motivation. We evaluate our proposed scheduled intrinsic drive (SID) agent using three different environments with pure visual inputs: VizDoom, DeepMind Lab and DeepMind Control Suite. The results show a substantially improved exploration efficiency with SFC and the hierarchical usage of the intrinsic drives. A video of our experimental results can be found at https://youtu.be/b0MbY3lUlEI.",0
"Sparse reward reinforcement learning exploration remains a challenge that has yet to be fully addressed. To complement the limited external reward, many advanced techniques use intrinsic motivation to provide additional feedback during exploration. However, adding these signals as bonus rewards results in a mixed policy that neither explores nor fulfills tasks effectively. Instead, our approach focuses on learning separate intrinsic and extrinsic task policies and scheduling between them to expedite exploration and enhance learning stability. Additionally, we introduce a new type of intrinsic reward called successor feature control (SFC) that is not task-specific but general. SFC considers complete trajectory statistics and differs from prior methods that only use local information for intrinsic motivation assessment. We evaluate our proposed scheduled intrinsic drive (SID) agent using pure visual input in three different environments: VizDoom, DeepMind Lab, and DeepMind Control Suite. The results demonstrate a significant increase in exploration efficiency with SFC and the hierarchical use of intrinsic drives. An experimental video of our findings is available at https://youtu.be/b0MbY3lUlEI.",1
"Real Time Strategy (RTS) games require macro strategies as well as micro strategies to obtain satisfactory performance since it has large state space, action space, and hidden information. This paper presents a novel hierarchical reinforcement learning model for mastering Multiplayer Online Battle Arena (MOBA) games, a sub-genre of RTS games. The novelty of this work are: (1) proposing a hierarchical framework, where agents execute macro strategies by imitation learning and carry out micromanipulations through reinforcement learning, (2) developing a simple self-learning method to get better sample efficiency for training, and (3) designing a dense reward function for multi-agent cooperation in the absence of game engine or Application Programming Interface (API). Finally, various experiments have been performed to validate the superior performance of the proposed method over other state-of-the-art reinforcement learning algorithms. Agent successfully learns to combat and defeat bronze-level built-in AI with 100% win rate, and experiments show that our method can create a competitive multi-agent for a kind of mobile MOBA game {\it King of Glory} in 5v5 mode.",0
"In order to achieve satisfactory results in Real Time Strategy (RTS) games, both macro and micro strategies are required due to the vast state space, action space, and hidden information involved. This study introduces a new hierarchical reinforcement learning model for mastering Multiplayer Online Battle Arena (MOBA) games, a subset of RTS games. The model proposes a hierarchical framework where agents execute macro strategies through imitation learning and micromanipulations through reinforcement learning. Additionally, a simple self-learning method is presented to improve sample efficiency during training, and a dense reward function for multi-agent cooperation is designed in the absence of a game engine or Application Programming Interface (API). The effectiveness of the proposed method is validated through various experiments, highlighting its superiority over other reinforcement learning algorithms. The agent successfully learns to defeat the bronze-level built-in AI with a 100% win rate, and the experiments demonstrate that the method can create a competitive multi-agent for the mobile MOBA game ""King of Glory"" in 5v5 mode.",1
"We present Placeto, a reinforcement learning (RL) approach to efficiently find device placements for distributed neural network training. Unlike prior approaches that only find a device placement for a specific computation graph, Placeto can learn generalizable device placement policies that can be applied to any graph. We propose two key ideas in our approach: (1) we represent the policy as performing iterative placement improvements, rather than outputting a placement in one shot; (2) we use graph embeddings to capture relevant information about the structure of the computation graph, without relying on node labels for indexing. These ideas allow Placeto to train efficiently and generalize to unseen graphs. Our experiments show that Placeto requires up to 6.1x fewer training steps to find placements that are on par with or better than the best placements found by prior approaches. Moreover, Placeto is able to learn a generalizable placement policy for any given family of graphs, which can then be used without any retraining to predict optimized placements for unseen graphs from the same family. This eliminates the large overhead incurred by prior RL approaches whose lack of generalizability necessitates re-training from scratch every time a new graph is to be placed.",0
"Introducing Placeto, an RL-based technique for efficiently locating device placements for distributed neural network training. Unlike previous methods that determine a device placement for a specific computation graph, Placeto can learn placement policies that are generalizable and applicable to any graph. Our approach involves two main concepts: (1) representing the policy as a series of iterative placement enhancements rather than a one-shot output and (2) using graph embeddings to capture relevant information regarding the computation graph's structure without relying on node labels for indexing. These ideas allow Placeto to train effectively and generalize to new graphs. Our experiments reveal that Placeto requires up to 6.1x fewer training steps to find placements that match or exceed the best placements found by previous techniques. Furthermore, Placeto can learn a generalizable placement policy for any given graph family, which can be used without retraining to predict optimized placements for new graphs from the same family. This eliminates the significant overhead associated with prior RL approaches that require retraining from scratch each time a new graph is placed because of their lack of generalizability.",1
"Global routing has been a historically challenging problem in electronic circuit design, where the challenge is to connect a large and arbitrary number of circuit components with wires without violating the design rules for the printed circuit boards or integrated circuits. Similar routing problems also exist in the design of complex hydraulic systems, pipe systems and logistic networks. Existing solutions typically consist of greedy algorithms and hard-coded heuristics. As such, existing approaches suffer from a lack of model flexibility and non-optimum solutions. As an alternative approach, this work presents a deep reinforcement learning method for solving the global routing problem in a simulated environment. At the heart of the proposed method is deep reinforcement learning that enables an agent to produce an optimal policy for routing based on the variety of problems it is presented with leveraging the conjoint optimization mechanism of deep reinforcement learning. Conjoint optimization mechanism is explained and demonstrated in details; the best network structure and the parameters of the learned model are explored. Based on the fine-tuned model, routing solutions and rewards are presented and analyzed. The results indicate that the approach can outperform the benchmark method of a sequential A* method, suggesting a promising potential for deep reinforcement learning for global routing and other routing or path planning problems in general. Another major contribution of this work is the development of a global routing problem sets generator with the ability to generate parameterized global routing problem sets with different size and constraints, enabling evaluation of different routing algorithms and the generation of training datasets for future data-driven routing approaches.",0
"The task of connecting a large number of electronic circuit components with wires while adhering to design rules has always been a difficult problem in circuit design. The same problem exists in the design of complex hydraulic and pipe systems, as well as logistics networks. Existing solutions to this problem usually employ greedy algorithms and fixed heuristics, which limit their flexibility and can lead to suboptimal solutions. In this paper, we propose a new approach using deep reinforcement learning to solve the global routing problem in a simulated environment. Our method utilizes the conjoint optimization mechanism of deep reinforcement learning to create an optimal routing policy, allowing the agent to handle a variety of routing problems. We explain and demonstrate the conjoint optimization mechanism and explore the best network structure and parameters of the learned model. Using the fine-tuned model, we present and analyze routing solutions and rewards, showing that our approach outperforms the benchmark sequential A* method, indicating potential for deep reinforcement learning in routing and path planning problems in general. We also develop a global routing problem sets generator, which can generate parameterized problem sets of different sizes and constraints for evaluation of different routing algorithms and the generation of training datasets for future data-driven routing approaches.",1
"Interference among concurrent transmissions in a wireless network is a key factor limiting the system performance. One way to alleviate this problem is to manage the radio resources in order to maximize either the average or the worst-case performance. However, joint consideration of both metrics is often neglected as they are competing in nature. In this article, a mechanism for radio resource management using multi-agent deep reinforcement learning (RL) is proposed, which strikes the right trade-off between maximizing the average and the $5^{th}$ percentile user throughput. Each transmitter in the network is equipped with a deep RL agent, receiving partial observations from the network (e.g., channel quality, interference level, etc.) and deciding whether to be active or inactive at each scheduling interval for given radio resources, a process referred to as link scheduling. Based on the actions of all agents, the network emits a reward to the agents, indicating how good their joint decisions were. The proposed framework enables the agents to make decisions in a distributed manner, and the reward is designed in such a way that the agents strive to guarantee a minimum performance, leading to a fair resource allocation among all users across the network. Simulation results demonstrate the superiority of our approach compared to decentralized baselines in terms of average and $5^{th}$ percentile user throughput, while achieving performance close to that of a centralized exhaustive search approach. Moreover, the proposed framework is robust to mismatches between training and testing scenarios. In particular, it is shown that an agent trained on a network with low transmitter density maintains its performance and outperforms the baselines when deployed in a network with a higher transmitter density.",0
"The performance of a wireless network is often limited by interference caused by concurrent transmissions. To address this issue, radio resources can be managed to maximize either the average or worst-case performance. However, balancing these metrics is challenging as they are in competition with one another. This article proposes a new approach for radio resource management using multi-agent deep reinforcement learning (RL) to strike a balance between maximizing average and $5^{th}$ percentile user throughput. Each transmitter in the network is equipped with a deep RL agent that receives partial observations and decides whether to be active or inactive at each scheduling interval. The network rewards the agents based on their joint decisions, encouraging them to work together to guarantee a minimum performance and ensure fair resource allocation. The proposed framework enables distributed decision-making, and simulation results show that it outperforms decentralized baselines in terms of average and $5^{th}$ percentile user throughput while remaining robust to changes in network density.",1
"Model-based reinforcement learning (MBRL) with model-predictive control or online planning has shown great potential for locomotion control tasks in terms of both sample efficiency and asymptotic performance. Despite their initial successes, the existing planning methods search from candidate sequences randomly generated in the action space, which is inefficient in complex high-dimensional environments. In this paper, we propose a novel MBRL algorithm, model-based policy planning (POPLIN), that combines policy networks with online planning. More specifically, we formulate action planning at each time-step as an optimization problem using neural networks. We experiment with both optimization w.r.t. the action sequences initialized from the policy network, and also online optimization directly w.r.t. the parameters of the policy network. We show that POPLIN obtains state-of-the-art performance in the MuJoCo benchmarking environments, being about 3x more sample efficient than the state-of-the-art algorithms, such as PETS, TD3 and SAC. To explain the effectiveness of our algorithm, we show that the optimization surface in parameter space is smoother than in action space. Further more, we found the distilled policy network can be effectively applied without the expansive model predictive control during test time for some environments such as Cheetah. Code is released in https://github.com/WilsonWangTHU/POPLIN.",0
"The efficiency and performance of model-based reinforcement learning (MBRL) with model-predictive control or online planning has been established for locomotion control tasks. However, the current planning methods employ a random generation of candidate sequences in the action space, limiting their efficacy in complex high-dimensional environments. In this paper, we introduce a new MBRL algorithm, namely model-based policy planning (POPLIN), which combines policy networks with online planning to solve for action planning at each time-step using neural networks. We conduct experiments with optimization with respect to both the action sequences and the parameters of the policy network and demonstrate that POPLIN outperforms other state-of-the-art algorithms such as PETS, TD3 and SAC in the MuJoCo benchmarking environments, with a 3x increase in sample efficiency. We attribute this success to the smoother optimization surface in parameter space and the effective application of the distilled policy network during test time for certain environments such as Cheetah. Our code is publicly available at https://github.com/WilsonWangTHU/POPLIN.",1
"Policy gradient methods are widely used for control in reinforcement learning, particularly for the continuous action setting. There have been a host of theoretically sound algorithms proposed for the on-policy setting, due to the existence of the policy gradient theorem which provides a simplified form for the gradient. In off-policy learning, however, where the behaviour policy is not necessarily attempting to learn and follow the optimal policy for the given task, the existence of such a theorem has been elusive. In this work, we solve this open problem by providing the first off-policy policy gradient theorem. The key to the derivation is the use of $emphatic$ $weightings$. We develop a new actor-critic algorithm$\unicode{x2014}$called Actor Critic with Emphatic weightings (ACE)$\unicode{x2014}$that approximates the simplified gradients provided by the theorem. We demonstrate in a simple counterexample that previous off-policy policy gradient methods$\unicode{x2014}$particularly OffPAC and DPG$\unicode{x2014}$converge to the wrong solution whereas ACE finds the optimal solution.",0
"In reinforcement learning, Policy gradient methods are widely used for control, especially in the continuous action setting. For the on-policy setting, numerous algorithms have been proposed that are theoretically sound because of the policy gradient theorem, which provides a simplified form for the gradient. However, in off-policy learning, where the behavior policy may not be trying to learn and follow the optimal policy for the given task, the existence of such a theorem has been hard to come by. In this study, we have solved this open problem by presenting the first off-policy policy gradient theorem, which utilizes emphatic weightings. We have developed a new actor-critic algorithm, named Actor Critic with Emphatic weightings (ACE), that approximates the simplified gradients provided by the theorem. We have demonstrated, through a simple counterexample, that previous off-policy policy gradient methods, specifically OffPAC and DPG, converge to the incorrect solution, whereas ACE finds the optimal solution.",1
"Experience replay enables reinforcement learning agents to memorize and reuse past experiences, just as humans replay memories for the situation at hand. Contemporary off-policy algorithms either replay past experiences uniformly or utilize a rule-based replay strategy, which may be sub-optimal. In this work, we consider learning a replay policy to optimize the cumulative reward. Replay learning is challenging because the replay memory is noisy and large, and the cumulative reward is unstable. To address these issues, we propose a novel experience replay optimization (ERO) framework which alternately updates two policies: the agent policy, and the replay policy. The agent is updated to maximize the cumulative reward based on the replayed data, while the replay policy is updated to provide the agent with the most useful experiences. The conducted experiments on various continuous control tasks demonstrate the effectiveness of ERO, empirically showing promise in experience replay learning to improve the performance of off-policy reinforcement learning algorithms.",0
"The technique of experience replay allows reinforcement learning agents to store and reuse past experiences, similar to how humans recall memories to handle current situations. However, current off-policy algorithms either replay experiences uniformly or use a rule-based approach, which may not be optimal. This study proposes a novel approach called experience replay optimization (ERO) to learn a replay policy that maximizes the cumulative reward. The challenge lies in dealing with a noisy and large replay memory, as well as unstable cumulative rewards. The ERO framework addresses these issues by updating two policies alternately: the agent policy and the replay policy. The agent policy maximizes the cumulative reward based on the replayed data, while the replay policy provides the agent with the most useful experiences. Experimental results on various continuous control tasks show that ERO is effective in improving the performance of off-policy reinforcement learning algorithms through experience replay learning.",1
"Estimates of predictive uncertainty are important for accurate model-based planning and reinforcement learning. However, predictive uncertainties---especially ones derived from modern deep learning systems---can be inaccurate and impose a bottleneck on performance. This paper explores which uncertainties are needed for model-based reinforcement learning and argues that good uncertainties must be calibrated, i.e. their probabilities should match empirical frequencies of predicted events. We describe a simple way to augment any model-based reinforcement learning agent with a calibrated model and show that doing so consistently improves planning, sample complexity, and exploration. On the \textsc{HalfCheetah} MuJoCo task, our system achieves state-of-the-art performance using 50\% fewer samples than the current leading approach. Our findings suggest that calibration can improve the performance of model-based reinforcement learning with minimal computational and implementation overhead.",0
"Accurate model-based planning and reinforcement learning require reliable estimates of predictive uncertainty. However, modern deep learning systems may produce inaccurate predictive uncertainties, which hinder performance. This study examines the necessary uncertainties for model-based reinforcement learning and argues that good uncertainties must be calibrated to match empirical frequencies of predicted events. We propose a simple method to enhance any model-based reinforcement learning agent with a calibrated model, and we demonstrate that doing so consistently enhances planning, sample complexity, and exploration. Our system achieves state-of-the-art performance on the \textsc{HalfCheetah} MuJoCo task with 50\% fewer samples than the current leading approach. Our results suggest that calibration can improve the performance of model-based reinforcement learning with minimal computational and implementation overhead.",1
"Deep reinforcement learning (deep RL) holds the promise of automating the acquisition of complex controllers that can map sensory inputs directly to low-level actions. In the domain of robotic locomotion, deep RL could enable learning locomotion skills with minimal engineering and without an explicit model of the robot dynamics. Unfortunately, applying deep RL to real-world robotic tasks is exceptionally difficult, primarily due to poor sample complexity and sensitivity to hyperparameters. While hyperparameters can be easily tuned in simulated domains, tuning may be prohibitively expensive on physical systems, such as legged robots, that can be damaged through extensive trial-and-error learning. In this paper, we propose a sample-efficient deep RL algorithm based on maximum entropy RL that requires minimal per-task tuning and only a modest number of trials to learn neural network policies. We apply this method to learning walking gaits on a real-world Minitaur robot. Our method can acquire a stable gait from scratch directly in the real world in about two hours, without relying on any model or simulation, and the resulting policy is robust to moderate variations in the environment. We further show that our algorithm achieves state-of-the-art performance on simulated benchmarks with a single set of hyperparameters. Videos of training and the learned policy can be found on the project website.",0
"The potential of deep reinforcement learning (deep RL) lies in its ability to automate the acquisition of intricate controllers that can directly link sensory inputs to low-level actions. For robotic locomotion, deep RL could facilitate the learning of locomotion skills with minimal engineering and no explicit model of the robot dynamics. However, applying deep RL to real-world robotic tasks is exceptionally challenging due to the poor sample complexity and sensitivity to hyperparameters. While tuning hyperparameters in simulated domains is easy, it can be costly on physical systems, such as legged robots, which can be damaged during trial-and-error learning. In this study, we present a sample-efficient deep RL algorithm based on maximum entropy RL that requires minimal per-task tuning and only a modest number of trials to learn neural network policies. We demonstrate the effectiveness of our approach by teaching a real-world Minitaur robot to walk stably from scratch in about two hours without relying on any model or simulation. Furthermore, our algorithm achieves state-of-the-art performance on simulated benchmarks with a single set of hyperparameters. The project website contains videos of training and the learned policy.",1
"Imitation Learning describes the problem of recovering an expert policy from demonstrations. While inverse reinforcement learning approaches are known to be very sample-efficient in terms of expert demonstrations, they usually require problem-dependent reward functions or a (task-)specific reward-function regularization. In this paper, we show a natural connection between inverse reinforcement learning approaches and Optimal Transport, that enables more general reward functions with desirable properties (e.g., smoothness). Based on our observation, we propose a novel approach called Wasserstein Adversarial Imitation Learning. Our approach considers the Kantorovich potentials as a reward function and further leverages regularized optimal transport to enable large-scale applications. In several robotic experiments, our approach outperforms the baselines in terms of average cumulative rewards and shows a significant improvement in sample-efficiency, by requiring just one expert demonstration.",0
"The problem of recovering an expert policy from demonstrations is referred to as Imitation Learning. Although inverse reinforcement learning methods are known for their efficiency in using expert demonstrations, they typically require a task-specific reward function or regularization. This paper establishes a connection between inverse reinforcement learning methods and Optimal Transport, allowing for reward functions with desirable properties, such as smoothness. Our proposed method, Wasserstein Adversarial Imitation Learning, utilizes Kantorovich potentials as a reward function and applies regularized optimal transport for use in large-scale applications. Through several experiments involving robots, our approach outperforms the baselines in terms of average cumulative rewards and demonstrates significant improvements in sample-efficiency, requiring only one expert demonstration.",1
