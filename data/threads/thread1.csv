"Optical flow estimation with occlusion or large displacement is a problematic challenge due to the lost of corresponding pixels between consecutive frames. In this paper, we discover that the lost information is related to a large quantity of motion features (more than 40%) computed from the popular discriminative cost-volume feature would completely vanish due to invalid sampling, leading to the low efficiency of optical flow learning. We call this phenomenon the Vanishing Cost Volume Problem. Inspired by the fact that local motion tends to be highly consistent within a short temporal window, we propose a novel iterative Motion Feature Recovery (MFR) method to address the vanishing cost volume via modeling motion consistency across multiple frames. In each MFR iteration, invalid entries from original motion features are first determined based on the current flow. Then, an efficient network is designed to adaptively learn the motion correlation to recover invalid features for lost-information restoration. The final optical flow is then decoded from the recovered motion features. Experimental results on Sintel and KITTI show that our method achieves state-of-the-art performances. In fact, MFR currently ranks second on Sintel public website.",0
"The problem of estimating optical flow in situations involving occlusion or large displacement is challenging due to the loss of corresponding pixels between frames. This paper presents findings that reveal the loss of information is associated with motion features computed from the discriminative cost-volume feature, with over 40% of these features vanishing as a result of invalid sampling. This phenomenon is referred to as the Vanishing Cost Volume Problem, which hampers optical flow learning efficiency. To address this issue, the paper proposes an iterative Motion Feature Recovery (MFR) method that models motion consistency across multiple frames. The MFR method identifies invalid entries in the original motion features based on current flow, and an efficient network is used to learn motion correlation and recover invalid features for lost-information restoration. The final optical flow is decoded from the recovered motion features. Experimental results on Sintel and KITTI demonstrate that the MFR method achieves state-of-the-art performances, ranking second on the Sintel public website.",1
"Micro-Expression Recognition has become challenging, as it is extremely difficult to extract the subtle facial changes of micro-expressions. Recently, several approaches proposed several expression-shared features algorithms for micro-expression recognition. However, they do not reveal the specific discriminative characteristics, which lead to sub-optimal performance. This paper proposes a novel Feature Refinement ({FR}) with expression-specific feature learning and fusion for micro-expression recognition. It aims to obtain salient and discriminative features for specific expressions and also predict expression by fusing the expression-specific features. FR consists of an expression proposal module with attention mechanism and a classification branch. First, an inception module is designed based on optical flow to obtain expression-shared features. Second, in order to extract salient and discriminative features for specific expression, expression-shared features are fed into an expression proposal module with attention factors and proposal loss. Last, in the classification branch, labels of categories are predicted by a fusion of the expression-specific features. Experiments on three publicly available databases validate the effectiveness of FR under different protocol. Results on public benchmarks demonstrate that our FR provides salient and discriminative information for micro-expression recognition. The results also show our FR achieves better or competitive performance with the existing state-of-the-art methods on micro-expression recognition.",0
"Recognizing micro-expressions is a difficult task due to the subtle changes in facial expressions that are hard to detect. Previous approaches have proposed algorithms that use expression-shared features, but these methods do not identify the specific characteristics that lead to optimal performance. To address this issue, this paper introduces the Feature Refinement ({FR}) method, which uses expression-specific feature learning and fusion to obtain salient and discriminative features for each expression. FR consists of an inception module that extracts expression-shared features, an expression proposal module with attention factors and proposal loss to extract specific features, and a classification branch that predicts labels by fusing the expression-specific features. Experiments on three publicly available databases demonstrate that FR is effective in micro-expression recognition and achieves better or competitive performance compared to existing state-of-the-art methods.",1
"The objective of this paper is visual-only self-supervised video representation learning. We make the following contributions: (i) we investigate the benefit of adding semantic-class positives to instance-based Info Noise Contrastive Estimation (InfoNCE) training, showing that this form of supervised contrastive learning leads to a clear improvement in performance; (ii) we propose a novel self-supervised co-training scheme to improve the popular infoNCE loss, exploiting the complementary information from different views, RGB streams and optical flow, of the same data source by using one view to obtain positive class samples for the other; (iii) we thoroughly evaluate the quality of the learnt representation on two different downstream tasks: action recognition and video retrieval. In both cases, the proposed approach demonstrates state-of-the-art or comparable performance with other self-supervised approaches, whilst being significantly more efficient to train, i.e. requiring far less training data to achieve similar performance.",0
"The goal of this research is to learn video representations through self-supervised visual-only methods. Our paper makes three major contributions: firstly, we examine the advantages of incorporating semantic-class positives into instance-based Info Noise Contrastive Estimation (InfoNCE) training, which leads to improved performance in supervised contrastive learning. Secondly, we propose a new self-supervised co-training technique that utilizes the information from different views, such as RGB streams and optical flow, to enhance the popular infoNCE loss. This method allows us to obtain positive class samples from one view to improve the other view. Finally, we extensively evaluate the learned representation's quality on two downstream tasks: action recognition and video retrieval. Our approach achieves state-of-the-art or comparable performance with other self-supervised methods, while requiring less training data and being more efficient to train.",1
"Temporal information is essential to learning effective policies with Reinforcement Learning (RL). However, current state-of-the-art RL algorithms either assume that such information is given as part of the state space or, when learning from pixels, use the simple heuristic of frame-stacking to implicitly capture temporal information present in the image observations. This heuristic is in contrast to the current paradigm in video classification architectures, which utilize explicit encodings of temporal information through methods such as optical flow and two-stream architectures to achieve state-of-the-art performance. Inspired by leading video classification architectures, we introduce the Flow of Latents for Reinforcement Learning (Flare), a network architecture for RL that explicitly encodes temporal information through latent vector differences. We show that Flare (i) recovers optimal performance in state-based RL without explicit access to the state velocity, solely with positional state information, (ii) achieves state-of-the-art performance on pixel-based challenging continuous control tasks within the DeepMind control benchmark suite, namely quadruped walk, hopper hop, finger turn hard, pendulum swing, and walker run, and is the most sample efficient model-free pixel-based RL algorithm, outperforming the prior model-free state-of-the-art by 1.9X and 1.5X on the 500k and 1M step benchmarks, respectively, and (iv), when augmented over rainbow DQN, outperforms this state-of-the-art level baseline on 5 of 8 challenging Atari games at 100M time step benchmark.",0
"Temporal information is significant in developing effective policies through Reinforcement Learning (RL). However, current state-of-the-art RL algorithms either assume that this information is a part of the state space or use the simple heuristic of frame-stacking to capture temporal information in image observations. This heuristic is different from the current video classification paradigm, where explicit encodings of temporal information are utilized to achieve optimal performance. Drawing inspiration from leading video classification architectures, Flare is introduced as a network architecture for RL that explicitly encodes temporal information through latent vector differences. Results show that Flare recovers optimal performance in state-based RL without requiring access to state velocity and achieves state-of-the-art performance on challenging continuous control tasks within the DeepMind control benchmark suite. Additionally, Flare is found to be the most sample efficient model-free pixel-based RL algorithm, outperforming prior model-free state-of-the-art by 1.9X and 1.5X on the 500k and 1M step benchmarks, respectively. When augmented over rainbow DQN, Flare outperforms the state-of-the-art level baseline on 5 of 8 challenging Atari games at the 100M time step benchmark.",1
"In this paper, we propose a \textbf{Tr}ansformer-based RGB-D \textbf{e}gocentric \textbf{a}ction \textbf{r}ecognition framework, called Trear. It consists of two modules, inter-frame attention encoder and mutual-attentional fusion block. Instead of using optical flow or recurrent units, we adopt self-attention mechanism to model the temporal structure of the data from different modalities. Input frames are cropped randomly to mitigate the effect of the data redundancy. Features from each modality are interacted through the proposed fusion block and combined through a simple yet effective fusion operation to produce a joint RGB-D representation. Empirical experiments on two large egocentric RGB-D datasets, THU-READ and FPHA, and one small dataset, WCVS, have shown that the proposed method outperforms the state-of-the-art results by a large margin.",0
"We introduce a novel framework for recognizing egocentric actions, named Trear, which employs a Transformer-based approach using RGB-D data. The framework is comprised of two modules, namely the inter-frame attention encoder and mutual-attentional fusion block. Unlike traditional methods that utilize optical flow or recurrent units, we leverage a self-attention mechanism to effectively model the temporal structure of data from various modalities. To mitigate data redundancy, we randomly crop input frames. Our proposed fusion block facilitates the interaction between features from each modality, which are then combined through a simple yet effective fusion operation to obtain a joint RGB-D representation. Empirical evaluations conducted on three datasets, including THU-READ, FPHA, and WCVS, demonstrate that our proposed method significantly outperforms the state-of-the-art.",1
"Optical flow techniques are becoming increasingly performant and robust when estimating motion in a scene, but their performance has yet to be proven in the area of facial expression recognition. In this work, a variety of optical flow approaches are evaluated across multiple facial expression datasets, so as to provide a consistent performance evaluation. The aim of this work is not to propose a new expression recognition technique, but to understand better the adequacy of existing state-of-the art optical flow for encoding facial motion in the context of facial expression recognition. Our evaluations highlight the fact that motion approximation methods used to overcome motion discontinuities have a significant impact when optical flows are used to characterize facial expressions.",0
"The estimation of motion in a scene has seen improvements in the performance and reliability of optical flow techniques. However, these techniques have yet to be tested for facial expression recognition. The purpose of this study is to evaluate various optical flow approaches across multiple facial expression datasets to provide a consistent performance assessment. The goal is not to introduce a new technique for expression recognition, but rather to gain a deeper understanding of the suitability of current state-of-the-art optical flow for capturing facial motion in the context of expression recognition. Our analysis highlights the importance of motion approximation methods in addressing motion discontinuities when using optical flows to represent facial expressions.",1
"Video facial expression recognition is useful for many applications and received much interest lately. Although some solutions give really good results in a controlled environment (no occlusion), recognition in the presence of partial facial occlusion remains a challenging task. To handle occlusions, solutions based on the reconstruction of the occluded part of the face have been proposed. These solutions are mainly based on the texture or the geometry of the face. However, the similarity of the face movement between different persons doing the same expression seems to be a real asset for the reconstruction. In this paper we exploit this asset and propose a new solution based on an auto-encoder with skip connections to reconstruct the occluded part of the face in the optical flow domain. To the best of our knowledge, this is the first proposition to directly reconstruct the movement for facial expression recognition. We validated our approach in the controlled dataset CK+ on which different occlusions were generated. Our experiments show that the proposed method reduce significantly the gap, in terms of recognition accuracy, between occluded and non-occluded situations. We also compare our approach with existing state-of-the-art solutions. In order to lay the basis of a reproducible and fair comparison in the future, we also propose a new experimental protocol that includes occlusion generation and reconstruction evaluation.",0
"Facial expression recognition through video has gained significant attention and practical uses lately. Although some solutions have shown promising results in a controlled environment without obstructions, recognizing expressions in the presence of partial facial occlusion poses a challenging task. To address this issue, methods have been proposed that involve reconstructing the occluded portion of the face based on texture or geometry. However, it has been observed that the similarity of facial movements across individuals performing the same expression can aid in reconstruction. In this study, we propose a new solution that leverages facial movement for reconstruction by using an auto-encoder with skip connections in the optical flow domain. This approach is the first to directly reconstruct facial movement for expression recognition. We tested our method on the controlled CK+ dataset, generating various occlusions. Our experiments demonstrate that our approach significantly reduces the gap in recognition accuracy between occluded and non-occluded situations. We also compared our method with existing state-of-the-art solutions and proposed a new experimental protocol that includes occlusion generation and reconstruction evaluation to facilitate a fair and reproducible comparison in the future.",1
"We study the problem of animating images by transferring spatio-temporal visual effects (such as melting) from a collection of videos. We tackle two primary challenges in visual effect transfer: 1) how to capture the effect we wish to distill; and 2) how to ensure that only the effect, rather than content or artistic style, is transferred from the source videos to the input image. To address the first challenge, we evaluate five loss functions; the most promising one encourages the generated animations to have similar optical flow and texture motions as the source videos. To address the second challenge, we only allow our model to move existing image pixels from the previous frame, rather than predicting unconstrained pixel values. This forces any visual effects to occur using the input image's pixels, preventing unwanted artistic style or content from the source video from appearing in the output. We evaluate our method in objective and subjective settings, and show interesting qualitative results which demonstrate objects undergoing atypical transformations, such as making a face melt or a deer bloom.",0
"Our focus is on animating images through the transfer of spatio-temporal visual effects, like melting, from a set of videos. The primary challenges we faced were determining how to capture the desired effect and ensuring that only the effect itself, rather than the artistic style or content, was transferred from the source videos to the input image. To tackle these challenges, we experimented with five loss functions and found that the most promising one resulted in generated animations having similar optical flow and texture motions as the source videos. Additionally, we limited our model to only move existing image pixels from the previous frame, rather than predicting unconstrained pixel values, to prevent unwanted artistic style or content from appearing in the output. We tested our method in both objective and subjective settings and observed interesting qualitative results, such as faces melting or deer blooming.",1
"Speech-driven facial video generation has been a complex problem due to its multi-modal aspects namely audio and video domain. The audio comprises lots of underlying features such as expression, pitch, loudness, prosody(speaking style) and facial video has lots of variability in terms of head movement, eye blinks, lip synchronization and movements of various facial action units along with temporal smoothness. Synthesizing highly expressive facial videos from the audio input and static image is still a challenging task for generative adversarial networks. In this paper, we propose a multi-modal adaptive normalization(MAN) based architecture to synthesize a talking person video of arbitrary length using as input: an audio signal and a single image of a person. The architecture uses the multi-modal adaptive normalization, keypoint heatmap predictor, optical flow predictor and class activation map[58] based layers to learn movements of expressive facial components and hence generates a highly expressive talking-head video of the given person. The multi-modal adaptive normalization uses the various features of audio and video such as Mel spectrogram, pitch, energy from audio signals and predicted keypoint heatmap/optical flow and a single image to learn the respective affine parameters to generate highly expressive video. Experimental evaluation demonstrates superior performance of the proposed method as compared to Realistic Speech-Driven Facial Animation with GANs(RSDGAN) [53], Speech2Vid [10], and other approaches, on multiple quantitative metrics including: SSIM (structural similarity index), PSNR (peak signal to noise ratio), CPBD (image sharpness), WER(word error rate), blinks/sec and LMD(landmark distance). Further, qualitative evaluation and Online Turing tests demonstrate the efficacy of our approach.",0
"Generating facial videos through speech has been a challenging task due to the multi-modal nature of the problem, involving both audio and video domains. Audio features, including expression, pitch, loudness, and speaking style, as well as video features, such as head movement, eye blinks, lip synchronization, and movements of facial action units with temporal smoothness, makes it difficult to synthesize highly expressive facial videos. To address this issue, we propose a multi-modal adaptive normalization-based architecture that utilizes various features of audio and video, such as Mel spectrogram, pitch, energy, predicted keypoint heatmap/optical flow, and a single image for generating highly expressive videos. Our approach outperforms other methods, including RSDGAN and Speech2Vid, on multiple quantitative metrics such as SSIM, PSNR, CPBD, WER, blinks/sec, and LMD, as well as qualitative evaluation and online Turing tests.",1
"In this paper, we consider the compressed video background subtraction problem that separates the background and foreground of a video from its compressed measurements. The background of a video usually lies in a low dimensional space and the foreground is usually sparse. More importantly, each video frame is a natural image that has textural patterns. By exploiting these properties, we develop a message passing algorithm termed offline denoising-based turbo message passing (DTMP). We show that these structural properties can be efficiently handled by the existing denoising techniques under the turbo message passing framework. We further extend the DTMP algorithm to the online scenario where the video data is collected in an online manner. The extension is based on the similarity/continuity between adjacent video frames. We adopt the optical flow method to refine the estimation of the foreground. We also adopt the sliding window based background estimation to reduce complexity. By exploiting the Gaussianity of messages, we develop the state evolution to characterize the per-iteration performance of offline and online DTMP. Comparing to the existing algorithms, DTMP can work at much lower compression rates, and can subtract the background successfully with a lower mean squared error and better visual quality for both offline and online compressed video background subtraction.",0
"The compressed video background subtraction problem is examined in this paper, which involves separating the background and foreground of a video from its compressed measurements. Typically, the video background is located in a low dimensional space while the foreground is sparse, and each video frame features textural patterns. To take advantage of these characteristics, an offline denoising-based turbo message passing (DTMP) algorithm has been developed, which effectively utilizes existing denoising techniques in the turbo message passing framework. Additionally, the DTMP algorithm has been extended to the online scenario, where video data is collected in real-time, by exploiting the similarity and continuity between adjacent video frames and adopting the optical flow method to refine foreground estimation. The sliding window based background estimation is also implemented to reduce complexity. State evolution is employed to characterize the per-iteration performance of offline and online DTMP by leveraging the Gaussianity of messages. Compared to existing algorithms, DTMP can operate at much lower compression rates and achieve successful background subtraction with a lower mean squared error and better visual quality for both offline and online compressed video background subtraction.",1
"Wearable cameras are becoming more and more popular in several applications, increasing the interest of the research community in developing approaches for recognizing actions from the first-person point of view. An open challenge in egocentric action recognition is that videos lack detailed information about the main actor's pose and thus tend to record only parts of the movement when focusing on manipulation tasks. Thus, the amount of information about the action itself is limited, making crucial the understanding of the manipulated objects and their context. Many previous works addressed this issue with two-stream architectures, where one stream is dedicated to modeling the appearance of objects involved in the action, and another to extracting motion features from optical flow. In this paper, we argue that learning features jointly from these two information channels is beneficial to capture the spatio-temporal correlations between the two better. To this end, we propose a single stream architecture able to do so, thanks to the addition of a self-supervised block that uses a pretext motion prediction task to intertwine motion and appearance knowledge. Experiments on several publicly available databases show the power of our approach.",0
"The use of wearable cameras is on the rise in various fields, which has piqued the interest of researchers in developing methods for recognizing actions from a first-person perspective. However, recognizing actions from egocentric videos is challenging due to the limited information about the actor's pose, which often only records parts of the movement during manipulation tasks. Therefore, understanding the context of the manipulated objects is crucial. Previous studies have addressed this issue by using two-stream architectures, but this paper argues that jointly learning features from appearance and motion channels is more effective in capturing spatio-temporal correlations. Thus, a single stream architecture incorporating a self-supervised block that uses a pretext motion prediction task is proposed. This approach is shown to be powerful through experiments on publicly available databases.",1
"We propose an unsupervised vision-based system to estimate the joint configurations of the robot arm from a sequence of RGB or RGB-D images without knowing the model a priori, and then adapt it to the task of category-independent articulated object pose estimation. We combine a classical geometric formulation with deep learning and extend the use of epipolar constraint to multi-rigid-body systems to solve this task. Given a video sequence, the optical flow is estimated to get the pixel-wise dense correspondences. After that, the 6D pose is computed by a modified PnP algorithm. The key idea is to leverage the geometric constraints and the constraint between multiple frames. Furthermore, we build a synthetic dataset with different kinds of robots and multi-joint articulated objects for the research of vision-based robot control and robotic vision. We demonstrate the effectiveness of our method on three benchmark datasets and show that our method achieves higher accuracy than the state-of-the-art supervised methods in estimating joint angles of robot arms and articulated objects.",0
"Our proposal presents an innovative vision-based system that can accurately estimate the joint configurations of a robot arm using RGB or RGB-D images, without prior knowledge of the model. We have extended the use of the epipolar constraint to multi-rigid-body systems and combined deep learning with a classical geometric formulation to achieve category-independent articulated object pose estimation. Our system works by estimating the optical flow of a video sequence to obtain pixel-wise dense correspondences, followed by the computation of the 6D pose using a modified PnP algorithm. We leverage the geometric constraints and the constraint between multiple frames to achieve accurate results. To support research into vision-based robot control and robotic vision, we have built a synthetic dataset containing different types of robots and multi-joint articulated objects. Our method outperforms state-of-the-art supervised methods in estimating joint angles of robot arms and articulated objects, as demonstrated by our results on three benchmark datasets.",1
"Analyzing motion between two consecutive images is one of the fundamental tasks in computer vision. In the lack of labeled data, the loss functions are split into consistency and smoothness, allowing for self-supervised training. This paper focuses on the cost function derivation and presents an unrolling iterative approach, transferring the hard L1 smoothness constraint into a softer multi-layer iterative scheme. More accurate gradients, especially near non-differential positions, improve the network's convergence, providing superior results on tested scenarios. We report state-of-the-art results on both MPI Sintel and KITTI 2015 unsupervised optical flow benchmarks. The provided approach can be used to enhance various architectures and not limited just to the presented pipeline.",0
"One of the primary tasks in computer vision is to analyze the motion between consecutive images. When there is no labeled data available, the loss functions are divided into consistency and smoothness, which enables self-supervised training. This study concentrates on the derivation of the cost function and introduces an iterative unrolling technique that transforms the tough L1 smoothness constraint into a more flexible multi-layer iterative scheme. The network's convergence is enhanced by more precise gradients, particularly in non-differential locations, resulting in superior outcomes in tested situations. We achieved state-of-the-art results in both MPI Sintel and KITTI 2015 unsupervised optical flow benchmarks. The proposed method can be adapted to various architectures and is not restricted to the presented pipeline.",1
"Low-quality modalities contain not only a lot of noisy information but also some discriminative features in RGBT tracking. However, the potentials of low-quality modalities are not well explored in existing RGBT tracking algorithms. In this work, we propose a novel duality-gated mutual condition network to fully exploit the discriminative information of all modalities while suppressing the effects of data noise. In specific, we design a mutual condition module, which takes the discriminative information of a modality as the condition to guide feature learning of target appearance in another modality. Such module can effectively enhance target representations of all modalities even in the presence of low-quality modalities. To improve the quality of conditions and further reduce data noise, we propose a duality-gated mechanism and integrate it into the mutual condition module. To deal with the tracking failure caused by sudden camera motion, which often occurs in RGBT tracking, we design a resampling strategy based on optical flow algorithms. It does not increase much computational cost since we perform optical flow calculation only when the model prediction is unreliable and then execute resampling when the sudden camera motion is detected. Extensive experiments on four RGBT tracking benchmark datasets show that our method performs favorably against the state-of-the-art tracking algorithms",0
"The presence of noisy information and valuable features in low-quality modalities is often overlooked in current RGBT tracking algorithms. To address this, we introduce a new approach called the duality-gated mutual condition network, which utilizes all modalities to extract discriminative information while reducing data noise. Our method employs a mutual condition module that leverages the target appearance in one modality to guide feature learning in another modality, resulting in enhanced target representations. To further enhance the quality of conditions and minimize data noise, we integrate a duality-gated mechanism into the mutual condition module. Additionally, we implement a resampling strategy based on optical flow algorithms to tackle tracking failure caused by sudden camera motion. Our experiments on four RGBT tracking benchmark datasets demonstrate that our method outperforms existing state-of-the-art tracking algorithms.",1
"Unsupervised learning of optical flow, which leverages the supervision from view synthesis, has emerged as a promising alternative to supervised methods. However, the objective of unsupervised learning is likely to be unreliable in challenging scenes. In this work, we present a framework to use more reliable supervision from transformations. It simply twists the general unsupervised learning pipeline by running another forward pass with transformed data from augmentation, along with using transformed predictions of original data as the self-supervision signal. Besides, we further introduce a lightweight network with multiple frames by a highly-shared flow decoder. Our method consistently gets a leap of performance on several benchmarks with the best accuracy among deep unsupervised methods. Also, our method achieves competitive results to recent fully supervised methods while with much fewer parameters.",0
"The utilization of view synthesis for unsupervised learning of optical flow has shown promise as an alternative to supervised methods. However, the reliability of the unsupervised learning objective is questionable in challenging scenarios. To address this, we propose a framework that incorporates more dependable supervision from transformations. Our approach modifies the unsupervised learning pipeline by performing another forward pass with transformed data from augmentation and using the transformed predictions of the original data as the self-supervision signal. Additionally, we introduce a lightweight network with multiple frames that utilizes a highly-shared flow decoder. Our method consistently outperforms other deep unsupervised methods on several benchmarks and achieves competitive results with recent fully supervised methods, all while using significantly fewer parameters.",1
"Optical flow estimation is an important yet challenging problem in the field of video analytics. The features of different semantics levels/layers of a convolutional neural network can provide information of different granularity. To exploit such flexible and comprehensive information, we propose a semi-supervised Feature Pyramidal Correlation and Residual Reconstruction Network (FPCR-Net) for optical flow estimation from frame pairs. It consists of two main modules: pyramid correlation mapping and residual reconstruction. The pyramid correlation mapping module takes advantage of the multi-scale correlations of global/local patches by aggregating features of different scales to form a multi-level cost volume. The residual reconstruction module aims to reconstruct the sub-band high-frequency residuals of finer optical flow in each stage. Based on the pyramid correlation mapping, we further propose a correlation-warping-normalization (CWN) module to efficiently exploit the correlation dependency. Experiment results show that the proposed scheme achieves the state-of-the-art performance, with improvement by 0.80, 1.15 and 0.10 in terms of average end-point error (AEE) against competing baseline methods - FlowNet2, LiteFlowNet and PWC-Net on the Final pass of Sintel dataset, respectively.",0
"The process of estimating optical flow is challenging, but crucial to video analytics. By utilizing the features of various semantic layers in a convolutional neural network, we can obtain information at different levels of detail. In order to make the most of this flexible and comprehensive data, we have developed a semi-supervised network called the Feature Pyramidal Correlation and Residual Reconstruction Network (FPCR-Net). This network consists of two primary modules: pyramid correlation mapping and residual reconstruction. The former combines features of different scales to create a multi-level cost volume that takes advantage of the correlations between global and local patches. The latter reconstructs the sub-band high-frequency residuals of finer optical flow at each stage. Using the pyramid correlation mapping, we have also introduced a correlation-warping-normalization (CWN) module to efficiently exploit correlation dependency. Our experimental results demonstrate that our proposed method outperforms FlowNet2, LiteFlowNet, and PWC-Net on the Final pass of Sintel dataset, with improvements of 0.80, 1.15, and 0.10 in terms of average end-point error (AEE), respectively.",1
"Independent Sign Language Recognition is a complex visual recognition problem that combines several challenging tasks of Computer Vision due to the necessity to exploit and fuse information from hand gestures, body features and facial expressions. While many state-of-the-art works have managed to deeply elaborate on these features independently, to the best of our knowledge, no work has adequately combined all three information channels to efficiently recognize Sign Language. In this work, we employ SMPL-X, a contemporary parametric model that enables joint extraction of 3D body shape, face and hands information from a single image. We use this holistic 3D reconstruction for SLR, demonstrating that it leads to higher accuracy than recognition from raw RGB images and their optical flow fed into the state-of-the-art I3D-type network for 3D action recognition and from 2D Openpose skeletons fed into a Recurrent Neural Network. Finally, a set of experiments on the body, face and hand features showed that neglecting any of these, significantly reduces the classification accuracy, proving the importance of jointly modeling body shape, facial expression and hand pose for Sign Language Recognition.",0
"The recognition of Independent Sign Language involves a complicated visual recognition issue, which requires the fusion of information from hand gestures, body features, and facial expressions. Although many researchers have focused on these features separately, no work has effectively combined all three channels to recognize Sign Language. In this study, we utilize SMPL-X, a modern parametric model, to extract body shape, face, and hand information from a single image. Our use of this holistic 3D reconstruction for SLR leads to higher accuracy than recognition from raw RGB images and their optical flow. We also demonstrate that neglecting any of these features significantly reduces the classification accuracy, revealing the significance of jointly modeling body shape, facial expression, and hand pose for Sign Language Recognition.",1
"The goal of this paper is to formulate a general framework for a constraint-based refinement of the optical flow using variational methods. We demonstrate that for a particular choice of the constraint, formulated as a minimization problem with the quadratic regularization, our results are close to the continuity equation based fluid flow. This closeness to the continuity model is theoretically justified through a modified augmented Lagrangian method and validated numerically. Further, along with the continuity constraint, our model can include geometric constraints as well. The correctness of our process is studied in the Hilbert space setting. Moreover, a special feature of our system is the possibility of a diagonalization by the Cauchy-Riemann operator and transforming it to a diffusion process on the curl and the divergence of the flow. Using the theory of semigroups on the decoupled system, we show that our process preserves the spatial characteristics of the divergence and the vorticities. We perform several numerical experiments and show the results on different datasets.",0
"The aim of this study is to create a comprehensive structure for refining optical flow using constraint-based approaches and variational methods. Our findings reveal that, by selecting a specific constraint in the form of a minimization problem with quadratic regularization, we can achieve outcomes that resemble fluid flow based on the continuity equation. This similarity to the continuity model is theoretically supported by a modified augmented Lagrangian technique and verified numerically. Additionally, our model can incorporate geometric constraints in conjunction with the continuity constraint. We validate our process in the Hilbert space environment. Another unique aspect of our system is its ability to diagonalize the Cauchy-Riemann operator and transform it into a diffusion process on the curl and divergence of the flow. We demonstrate that our process preserves the spatial features of the divergence and vorticities by applying the theory of semigroups on the uncoupled system. Finally, we present several numerical experiments and showcase the results on various datasets.",1
"The paper addresses the problem of recognition of actions in video with low inter-class variability such as Table Tennis strokes. Two stream, ""twin"" convolutional neural networks are used with 3D convolutions both on RGB data and optical flow. Actions are recognized by classification of temporal windows. We introduce 3D attention modules and examine their impact on classification efficiency. In the context of the study of sportsmen performances, a corpus of the particular actions of table tennis strokes is considered. The use of attention blocks in the network speeds up the training step and improves the classification scores up to 5% with our twin model. We visualize the impact on the obtained features and notice correlation between attention and player movements and position. Score comparison of state-of-the-art action classification method and proposed approach with attentional blocks is performed on the corpus. Proposed model with attention blocks outperforms previous model without them and our baseline.",0
"The article discusses the challenge of identifying actions in videos that have minimal differences between classes, such as Table Tennis strokes. To tackle this problem, the authors employ twin convolutional neural networks that use 3D convolutions on both RGB and optical flow data. The recognition of actions is achieved by classifying temporal windows. The authors introduce 3D attention modules to enhance the efficiency of classification and evaluate their impact. The study examines a corpus of Table Tennis stroke actions to analyze the performance of athletes. The use of attention blocks in the network improves classification accuracy by up to 5% with the twin model, and the authors observe a correlation between attention and player movements and position. The proposed model with attention blocks outperforms previous models without them, as well as the baseline, which is demonstrated through score comparisons on the corpus.",1
"Learning the necessary high-level reasoning for video stabilization without the help of optical flow has proved to be one of the most challenging tasks in the field of computer vision. In this work, we present an iterative frame interpolation strategy to generate a novel dataset that is diverse enough to formulate video stabilization as a supervised learning problem unassisted by optical flow. A major benefit of treating video stabilization as a pure RGB based generative task over the conventional optical flow assisted approaches is the preservation of content and resolution, which is usually obstructed in the latter approaches. To do so, we provide a new video stabilization dataset and train an efficient network that can produce competitive stabilization results in a fraction of the time taken to do the same with the recent iterative frame interpolation schema. Our method provides qualitatively and quantitatively better results than those generated through state-of-the-art video stabilization methods. To the best of our knowledge, this is the only work that demonstrates the importance of perspective in formulating video stabilization as a deep learning problem instead of replacing it with an inter-frame motion measure",0
"The field of computer vision faces a significant challenge in acquiring the necessary high-level reasoning for video stabilization without relying on optical flow. Therefore, this paper introduces an iterative frame interpolation technique to create a diverse dataset for establishing video stabilization as a supervised learning task without the need for optical flow. By treating video stabilization as an RGB-based generative task, our approach preserves content and resolution, which is often obstructed by conventional optical flow-assisted methods. We provide a new dataset and train an efficient network that produces better results than state-of-the-art video stabilization techniques. Our method highlights the significance of perspective in formulating video stabilization as a deep learning problem, rather than replacing it with an inter-frame motion measure. Furthermore, our approach requires less time than other recent iterative frame interpolation schemes.",1
"In optical flow estimation task, coarse-to-fine (C2F) warping strategy is widely used to deal with the large displacement problem and provides efficiency and speed. However, limited by the small search range between the first images and warped second images, current coarse-to-fine optical flow networks fail to capture small and fast-moving objects which disappear at coarse resolution levels. To address this problem, we introduce a lightweight but effective Global Matching Component (GMC) to grab global matching features. We propose a new Hybrid Matching Optical Flow Network (HMFlow) by integrating GMC into existing coarse-to-fine networks seamlessly. Besides keeping in high accuracy and small model size, our proposed HMFlow can apply global matching features to guide the network to discover the small and fast-moving objects mismatched by local matching features. We also build a new dataset, named Small and Fast-Moving Chairs (SFChairs), for evaluation. The experimental results show that our proposed network achieves considerable performance, especially at regions with small and fast-moving objects.",0
"The coarse-to-fine (C2F) warping strategy is commonly used in optical flow estimation to address the issue of large displacement and improve efficiency. However, existing C2F optical flow networks have limitations in capturing small and fast-moving objects that vanish at coarse resolution levels due to a small search range between the initial and warped second images. To overcome this challenge, we have introduced a lightweight but effective Global Matching Component (GMC) that captures global matching features. Our novel Hybrid Matching Optical Flow Network (HMFlow) combines GMC with existing C2F networks seamlessly, maintaining high accuracy and small model size. By leveraging global matching features, HMFlow can detect small and fast-moving objects that are missed by local matching features. We have also created a new dataset called Small and Fast-Moving Chairs (SFChairs) for evaluation. Experimental results demonstrate that our proposed network performs significantly better, especially in regions with small and fast-moving objects.",1
"Abnormal event detection (AED) in urban surveillance videos has multiple challenges. Unlike other computer vision problems, the AED is not solely dependent on the content of frames. It also depends on the appearance of the objects and their movements in the scene. Various methods have been proposed to address the AED problem. Among those, deep learning based methods show the best results. This paper is based on deep learning methods and provides an effective way to detect and locate abnormal events in videos by handling spatio temporal data. This paper uses generative adversarial networks (GANs) and performs transfer learning algorithms on pre trained convolutional neural network (CNN) which result in an accurate and efficient model. The efficiency of the model is further improved by processing the optical flow information of the video. This paper runs experiments on two benchmark datasets for AED problem (UCSD Peds1 and UCSD Peds2) and compares the results with other previous methods. The comparisons are based on various criteria such as area under curve (AUC) and true positive rate (TPR). Experimental results show that the proposed method can effectively detect and locate abnormal events in crowd scenes.",0
"Detecting abnormal events in urban surveillance videos poses several challenges. Unlike other computer vision problems, AED depends not only on the content of frames but also on the appearance of objects and their movements in the scene. Several methods have been proposed to tackle this problem, with deep learning techniques showing the best results. This paper presents an effective approach to detecting and locating abnormal events in video using spatio-temporal data. Specifically, the paper leverages generative adversarial networks (GANs) and transfer learning algorithms on pre-trained convolutional neural networks (CNNs) to create an accurate and efficient model. The model's efficiency is further improved by processing optical flow information. The proposed method is evaluated on two benchmark datasets for AED (UCSD Peds1 and UCSD Peds2) and compared to previous methods based on various criteria, including AUC and TPR. Experimental results demonstrate that the proposed method effectively detects and locates abnormal events in crowded scenes.",1
"Visual odometry networks commonly use pretrained optical flow networks in order to derive the ego-motion between consecutive frames. The features extracted by these networks represent the motion of all the pixels between frames. However, due to the existence of dynamic objects and texture-less surfaces in the scene, the motion information for every image region might not be reliable for inferring odometry due to the ineffectiveness of dynamic objects in derivation of the incremental changes in position. Recent works in this area lack attention mechanisms in their structures to facilitate dynamic reweighing of the feature maps for extracting more refined egomotion information. In this paper, we explore the effectiveness of self-attention in visual odometry. We report qualitative and quantitative results against the SOTA methods. Furthermore, saliency-based studies alongside specially designed experiments are utilized to investigate the effect of self-attention on VO. Our experiments show that using self-attention allows for the extraction of better features while achieving a better odometry performance compared to networks that lack such structures.",0
"Pretrained optical flow networks are commonly used in visual odometry networks to determine the ego-motion between consecutive frames. These networks extract features that represent the motion of all the pixels between frames. However, the motion information for every image region may not be reliable due to the presence of dynamic objects and texture-less surfaces in the scene. This is because dynamic objects are ineffective in deriving the incremental changes in position. Recent studies lack attention mechanisms to facilitate dynamic reweighing of the feature maps for extracting more refined egomotion information. In this paper, we investigate the effectiveness of self-attention in visual odometry and compare our results with state-of-the-art methods. We also conduct saliency-based studies and specially designed experiments to examine the effect of self-attention on VO. Our experiments demonstrate that self-attention allows for the extraction of better features, resulting in better odometry performance compared to networks that lack such structures.",1
"Skeleton-based action recognition has attracted research attentions in recent years. One common drawback in currently popular skeleton-based human action recognition methods is that the sparse skeleton information alone is not sufficient to fully characterize human motion. This limitation makes several existing methods incapable of correctly classifying action categories which exhibit only subtle motion differences. In this paper, we propose a novel framework for employing human pose skeleton and joint-centered light-weight information jointly in a two-stream graph convolutional network, namely, JOLO-GCN. Specifically, we use Joint-aligned optical Flow Patches (JFP) to capture the local subtle motion around each joint as the pivotal joint-centered visual information. Compared to the pure skeleton-based baseline, this hybrid scheme effectively boosts performance, while keeping the computational and memory overheads low. Experiments on the NTU RGB+D, NTU RGB+D 120, and the Kinetics-Skeleton dataset demonstrate clear accuracy improvements attained by the proposed method over the state-of-the-art skeleton-based methods.",0
"In recent years, there has been an increase in research attention towards skeleton-based action recognition. However, current widely-used methods have a common drawback of only relying on sparse skeleton information, which is insufficient in completely characterizing human motion. This deficiency leads to the inability to accurately classify action categories that have only subtle differences in motion. To address this limitation, this paper introduces JOLO-GCN, a novel framework that combines joint-centered light-weight information with human pose skeleton in a two-stream graph convolutional network. Joint-aligned optical Flow Patches (JFP) are utilized to capture local subtle motion around each joint, serving as a crucial joint-centered visual information. This hybrid approach not only effectively enhances performance compared to pure skeleton-based methods but also maintains low computational and memory overheads. Experiments conducted on the NTU RGB+D, NTU RGB+D 120, and Kinetics-Skeleton dataset demonstrate the proposed method's clear accuracy improvements over state-of-the-art skeleton-based methods.",1
"Complex blur such as the mixup of space-variant and space-invariant blur, which is hard to model mathematically, widely exists in real images. In this paper, we propose a novel image deblurring method that does not need to estimate blur kernels. We utilize a pair of images that can be easily acquired in low-light situations: (1) a blurred image taken with low shutter speed and low ISO noise; and (2) a noisy image captured with high shutter speed and high ISO noise. Slicing the blurred image into patches, we extend the Gaussian mixture model (GMM) to model the underlying intensity distribution of each patch using the corresponding patches in the noisy image. We compute patch correspondences by analyzing the optical flow between the two images. The Expectation Maximization (EM) algorithm is utilized to estimate the parameters of GMM. To preserve sharp features, we add an additional bilateral term to the objective function in the M-step. We eventually add a detail layer to the deblurred image for refinement. Extensive experiments on both synthetic and real-world data demonstrate that our method outperforms state-of-the-art techniques, in terms of robustness, visual quality, and quantitative metrics.",0
"Real-world images often contain complex blur, which is a mixture of space-variant and space-invariant blur that is difficult to model mathematically. In this study, we present a new image deblurring method that does not require the estimation of blur kernels. Our approach involves using a pair of images obtained in low-light environments: a blurred image captured with low shutter speed and low ISO noise, and a noisy image captured with high shutter speed and high ISO noise. By dividing the blurred image into patches and extending the Gaussian mixture model (GMM), we can model the underlying intensity distribution of each patch using the corresponding patches in the noisy image. We analyze the optical flow between the two images to determine patch correspondences, and we use the Expectation Maximization (EM) algorithm to estimate the GMM parameters. To maintain sharp features, we add a bilateral term to the objective function in the M-step. Finally, we add a detail layer to the deblurred image for refinement. Our method outperforms state-of-the-art techniques in terms of robustness, visual quality, and quantitative metrics, as demonstrated by extensive experiments on both synthetic and real-world data.",1
"Facial features deformed according to the intended facial expression. Specific facial features are associated with specific facial expression, i.e. happy means the deformation of mouth. This paper presents the study of facial feature deformation for each facial expression by using an optical flow algorithm and segmented into three different regions of interest. The deformation of facial features shows the relation between facial the and facial expression. Based on the experiments, the deformations of eye and mouth are significant in all expressions except happy. For happy expression, cheeks and mouths are the significant regions. This work also suggests that different facial features' intensity varies in the way that they contribute to the recognition of the different facial expression intensity. The maximum magnitude across all expressions is shown by the mouth for surprise expression which is 9x10-4. While the minimum magnitude is shown by the mouth for angry expression which is 0.4x10-4.",0
"This paper examines how facial features change based on the intended facial expression. Each expression is associated with specific facial features, such as a happy expression involving the mouth. Using an optical flow algorithm, the study analyzes facial feature deformation in three regions of interest to understand the relationship between facial expression and features. Results show that the eye and mouth deformations are significant in all expressions except happy, where the cheeks and mouth are more significant. The study also found that the intensity of different facial features varies in their contribution to recognizing facial expressions. The mouth has the highest magnitude for surprise expression at 9x10-4 and the lowest for angry expression at 0.4x10-4.",1
"The importance of inference in Machine Learning (ML) has led to an explosive number of different proposals in ML, and particularly in Deep Learning. In an attempt to reduce the complexity of Convolutional Neural Networks, we propose a Volterra filter-inspired Network architecture. This architecture introduces controlled non-linearities in the form of interactions between the delayed input samples of data. We propose a cascaded implementation of Volterra Filtering so as to significantly reduce the number of parameters required to carry out the same classification task as that of a conventional Neural Network. We demonstrate an efficient parallel implementation of this Volterra Neural Network (VNN), along with its remarkable performance while retaining a relatively simpler and potentially more tractable structure. Furthermore, we show a rather sophisticated adaptation of this network to nonlinearly fuse the RGB (spatial) information and the Optical Flow (temporal) information of a video sequence for action recognition. The proposed approach is evaluated on UCF-101 and HMDB-51 datasets for action recognition, and is shown to outperform state of the art CNN approaches.",0
"Machine Learning (ML) has seen a surge in proposals, particularly in Deep Learning, due to the significance of inference. To simplify Convolutional Neural Networks, we suggest a Network architecture inspired by Volterra filters. This architecture uses controlled non-linearities in the form of interactions between delayed input data samples. We propose a Volterra Filtering implementation in a cascade to significantly reduce the required parameters for the same classification task as a conventional Neural Network. Our Volterra Neural Network (VNN) has an efficient parallel implementation with exceptional performance while maintaining a potentially more manageable structure. Additionally, we demonstrate a sophisticated adaptation of our network to fuse the RGB (spatial) and Optical Flow (temporal) information of a video sequence for action recognition. We evaluate our approach on UCF-101 and HMDB-51 datasets and show that it outperforms state-of-the-art CNN approaches.",1
"Contrary to the ongoing trend in automotive applications towards usage of more diverse and more sensors, this work tries to solve the complex scene flow problem under a monocular camera setup, i.e. using a single sensor. Towards this end, we exploit the latest achievements in single image depth estimation, optical flow, and sparse-to-dense interpolation and propose a monocular combination approach (MonoComb) to compute dense scene flow. MonoComb uses optical flow to relate reconstructed 3D positions over time and interpolates occluded areas. This way, existing monocular methods are outperformed in dynamic foreground regions which leads to the second best result among the competitors on the challenging KITTI 2015 scene flow benchmark.",0
"This work goes against the trend in automotive applications, which is to use more diverse and numerous sensors. Instead, the goal is to solve the complex scene flow problem using only one sensor, specifically a monocular camera setup. To accomplish this, the latest advancements in single image depth estimation, optical flow, and sparse-to-dense interpolation are utilized. A new approach, called MonoComb, is proposed which combines these techniques to compute dense scene flow. This method uses optical flow to relate reconstructed 3D positions over time and interpolates occluded regions. As a result, MonoComb outperforms existing monocular methods in dynamic foreground regions, achieving the second best result among competitors in the challenging KITTI 2015 scene flow benchmark.",1
"Video semantic segmentation is active in recent years benefited from the great progress of image semantic segmentation. For such a task, the per-frame image segmentation is generally unacceptable in practice due to high computation cost. To tackle this issue, many works use the flow-based feature propagation to reuse the features of previous frames. However, the optical flow estimation inevitably suffers inaccuracy and then causes the propagated features distorted. In this paper, we propose distortion-aware feature correction to alleviate the issue, which improves video segmentation performance by correcting distorted propagated features. To be specific, we firstly propose to transfer distortion patterns from feature into image space and conduct effective distortion map prediction. Benefited from the guidance of distortion maps, we proposed Feature Correction Module (FCM) to rectify propagated features in the distorted areas. Our proposed method can significantly boost the accuracy of video semantic segmentation at a low price. The extensive experimental results on Cityscapes and CamVid show that our method outperforms the recent state-of-the-art methods.",0
"In recent years, video semantic segmentation has made significant progress due to advancements in image semantic segmentation. However, per-frame image segmentation is impractical due to high computational costs. To address this issue, many researchers have used flow-based feature propagation to reuse features from previous frames. However, optical flow estimation is often inaccurate, leading to distorted propagated features. To improve video segmentation performance, we propose distortion-aware feature correction, which corrects distorted features. Specifically, we transfer distortion patterns from features to image space and predict distortion maps. With the guidance of these maps, we introduce the Feature Correction Module (FCM) to rectify features in distorted areas. Our approach significantly increases accuracy at a low cost. Experimental results on Cityscapes and CamVid show that our method outperforms recent state-of-the-art methods.",1
"Multimodal large-scale datasets for outdoor scenes are mostly designed for urban driving problems. The scenes are highly structured and semantically different from scenarios seen in nature-centered scenes such as gardens or parks. To promote machine learning methods for nature-oriented applications, such as agriculture and gardening, we propose the multimodal synthetic dataset for Enclosed garDEN scenes (EDEN). The dataset features more than 300K images captured from more than 100 garden models. Each image is annotated with various low/high-level vision modalities, including semantic segmentation, depth, surface normals, intrinsic colors, and optical flow. Experimental results on the state-of-the-art methods for semantic segmentation and monocular depth prediction, two important tasks in computer vision, show positive impact of pre-training deep networks on our dataset for unstructured natural scenes. The dataset and related materials will be available at https://lhoangan.github.io/eden.",0
"Most large-scale datasets that focus on outdoor scenes are geared towards urban driving issues, featuring highly organized and semantically distinct environments not commonly found in natural settings such as parks or gardens. In order to encourage machine learning techniques for nature-oriented applications, such as agriculture and gardening, we have proposed a new dataset called EDEN (Enclosed garDEN scenes). This multimodal synthetic dataset contains over 300,000 images captured from more than 100 garden models, each annotated with various low/high-level vision modalities such as semantic segmentation, depth, surface normals, intrinsic colors, and optical flow. Our experiments on state-of-the-art methods for semantic segmentation and monocular depth prediction - two critical tasks in computer vision - demonstrate the positive impact of pre-training deep networks on our dataset for unstructured natural scenes. The dataset and accompanying resources will be available at https://lhoangan.github.io/eden.",1
"Micro-expression (ME) recognition plays a crucial role in a wide range of applications, particularly in public security and psychotherapy. Recently, traditional methods rely excessively on machine learning design and the recognition rate is not high enough for its practical application because of its short duration and low intensity. On the other hand, some methods based on deep learning also cannot get high accuracy due to problems such as the imbalance of databases. To address these problems, we design a multi-stream convolutional neural network (MSCNN) for ME recognition in this paper. Specifically, we employ EVM and optical flow to magnify and visualize subtle movement changes in MEs and extract the masks from the optical flow images. And then, we add the masks, optical flow images, and grayscale images into the MSCNN. After that, in order to overcome the imbalance of databases, we added a random over-sampler after the Dense Layer of the neural network. Finally, extensive experiments are conducted on two public ME databases: CASME II and SAMM. Compared with many recent state-of-the-art approaches, our method achieves more promising recognition results.",0
"ME recognition is essential in various fields such as public security and psychotherapy. However, traditional methods relying heavily on machine learning design have limited practical application due to the short duration and low intensity of MEs. Even deep learning methods suffer from accuracy issues, such as database imbalance. To overcome these problems, we propose a multi-stream convolutional neural network (MSCNN) for ME recognition. Our approach includes using EVM and optical flow to magnify and visualize subtle movement changes in MEs, extracting masks from optical flow images, and adding them to grayscale images in the MSCNN. Additionally, we added a random over-sampler after the Dense Layer to address database imbalance. Our method achieves promising recognition results compared to recent state-of-the-art approaches after extensive experiments on two public ME databases: CASME II and SAMM.",1
"Capsule networks (CapsNets) have recently shown promise to excel in most computer vision tasks, especially pertaining to scene understanding. In this paper, we explore CapsNet's capabilities in optical flow estimation, a task at which convolutional neural networks (CNNs) have already outperformed other approaches. We propose a CapsNet-based architecture, termed FlowCaps, which attempts to a) achieve better correspondence matching via finer-grained, motion-specific, and more-interpretable encoding crucial for optical flow estimation, b) perform better-generalizable optical flow estimation, c) utilize lesser ground truth data, and d) significantly reduce the computational complexity in achieving good performance, in comparison to its CNN-counterparts.",0
"Recently, Capsule networks (CapsNets) have demonstrated potential for excelling in computer vision tasks, particularly in the realm of scene understanding. The focus of this paper is to investigate the effectiveness of CapsNets in optical flow estimation, a task that has already been surpassed by convolutional neural networks (CNNs) compared to other methods. To achieve this, we propose a CapsNet-based architecture called FlowCaps, which aims to a) enhance correspondence matching through more detailed, motion-specific, and easily interpretable encoding for optical flow estimation, b) increase generalization performance, c) reduce the need for extensive ground truth data, and d) reduce computational complexity while still achieving optimal performance compared to CNN models.",1
"Accurate object segmentation is a crucial task in the context of robotic manipulation. However, creating sufficient annotated training data for neural networks is particularly time consuming and often requires manual labeling. To this end, we propose a simple, yet robust solution for learning to segment unknown objects grasped by a robot. Specifically, we exploit motion and temporal cues in RGB video sequences. Using optical flow estimation we first learn to predict segmentation masks of our given manipulator. Then, these annotations are used in combination with motion cues to automatically distinguish between background, manipulator and unknown, grasped object. In contrast to existing systems our approach is fully self-supervised and independent of precise camera calibration, 3D models or potentially imperfect depth data. We perform a thorough comparison with alternative baselines and approaches from literature. The object masks and views are shown to be suitable training data for segmentation networks that generalize to novel environments and also allow for watertight 3D reconstruction.",0
"The task of accurately segmenting objects is critical for robotic manipulation, but it can be time-consuming to create annotated training data for neural networks, often requiring manual labeling. In response, we propose a simple yet robust solution for learning to segment unfamiliar objects held by a robot. Specifically, we use motion and temporal cues in RGB video sequences to predict segmentation masks of the manipulator. These annotations, combined with motion cues, automatically distinguish between the background, manipulator, and unknown grasped object. Our approach is fully self-supervised, independent of camera calibration, 3D models, or potential imperfections in depth data. We compare our method with existing systems and demonstrate that our object masks and views are suitable training data for segmentation networks that can generalize to new environments and enable watertight 3D reconstruction.",1
"The construction of models for video action classification progresses rapidly. However, the performance of those models can still be easily improved by ensembling with the same models trained on different modalities (e.g. Optical flow). Unfortunately, it is computationally expensive to use several modalities during inference. Recent works examine the ways to integrate advantages of multi-modality into a single RGB-model. Yet, there is still a room for improvement. In this paper, we explore the various methods to embed the ensemble power into a single model. We show that proper initialization, as well as mutual modality learning, enhances single-modality models. As a result, we achieve state-of-the-art results in the Something-Something-v2 benchmark.",0
"Advancements in building models for video action classification are happening rapidly. Nonetheless, there is room to easily enhance the performance of these models by ensembling them with the same models trained on different modalities like Optical flow. However, using multiple modalities during inference is computationally expensive. Recent studies aim to integrate the benefits of multi-modality into a single RGB-model, but there is still scope for improvement. This article explores various ways to embed the ensemble power into a single model. We demonstrate that appropriate initialization and mutual modality learning improve single-modality models. Consequently, we achieve state-of-the-art results in the Something-Something-v2 benchmark.",1
"Interpolation of sparse pixel information towards a dense target resolution finds its application across multiple disciplines in computer vision. State-of-the-art interpolation of motion fields applies model-based interpolation that makes use of edge information extracted from the target image. For depth completion, data-driven learning approaches are widespread. Our work is inspired by latest trends in depth completion that tackle the problem of dense guidance for sparse information. We extend these ideas and create a generic cross-domain architecture that can be applied for a multitude of interpolation problems like optical flow, scene flow, or depth completion. In our experiments, we show that our proposed concept of Sparse Spatial Guided Propagation (SSGP) achieves improvements to robustness, accuracy, or speed compared to specialized algorithms.",0
"The use of interpolation to increase the density of pixel information has a broad range of applications in computer vision. Modern methods for interpolating motion fields typically rely on models that incorporate edge information from the target image. Meanwhile, data-driven learning techniques are prevalent for depth completion. Our research builds on recent developments in depth completion that address the challenge of providing dense guidance for sparse data. We have developed a versatile cross-domain architecture that can be used for various interpolation problems, including optical flow, scene flow, and depth completion. Through our experiments, we demonstrate that our Sparse Spatial Guided Propagation (SSGP) approach outperforms specialized algorithms in terms of accuracy, robustness, and speed.",1
"The interpretation of ego motion and scene change is a fundamental task for mobile robots. Optical flow information can be employed to estimate motion in the surroundings. Recently, unsupervised optical flow estimation has become a research hotspot. However, unsupervised approaches are often easy to be unreliable on partially occluded or texture-less regions. To deal with this problem, we propose CoT-AMFlow in this paper, an unsupervised optical flow estimation approach. In terms of the network architecture, we develop an adaptive modulation network that employs two novel module types, flow modulation modules (FMMs) and cost volume modulation modules (CMMs), to remove outliers in challenging regions. As for the training paradigm, we adopt a co-teaching strategy, where two networks simultaneously teach each other about challenging regions to further improve accuracy. Experimental results on the MPI Sintel, KITTI Flow and Middlebury Flow benchmarks demonstrate that our CoT-AMFlow outperforms all other state-of-the-art unsupervised approaches, while still running in real time. Our project page is available at https://sites.google.com/view/cot-amflow.",0
"Mobile robots require accurate interpretation of ego motion and scene change, making optical flow information a valuable tool for motion estimation. Unsupervised optical flow estimation has gained attention in recent years, but such approaches can be unreliable in regions with partial occlusion or lack of texture. To address this issue, we introduce CoT-AMFlow, an unsupervised optical flow estimation approach. Our method employs an adaptive modulation network that uses flow modulation modules (FMMs) and cost volume modulation modules (CMMs) to eliminate outliers in challenging areas. We also utilize a co-teaching strategy, where two networks teach each other about difficult regions to enhance accuracy. Our experimental results on the MPI Sintel, KITTI Flow, and Middlebury Flow benchmarks demonstrate that CoT-AMFlow outperforms other unsupervised approaches while maintaining real-time performance. More information about CoT-AMFlow can be found on our project page at https://sites.google.com/view/cot-amflow.",1
"In this paper, we propose a spatio-temporal contextual network, STC-Flow, for optical flow estimation. Unlike previous optical flow estimation approaches with local pyramid feature extraction and multi-level correlation, we propose a contextual relation exploration architecture by capturing rich long-range dependencies in spatial and temporal dimensions. Specifically, STC-Flow contains three key context modules - pyramidal spatial context module, temporal context correlation module and recurrent residual contextual upsampling module, to build the relationship in each stage of feature extraction, correlation, and flow reconstruction, respectively. Experimental results indicate that the proposed scheme achieves the state-of-the-art performance of two-frame based methods on the Sintel dataset and the KITTI 2012/2015 datasets.",0
"The aim of this paper is to introduce STC-Flow, a novel spatio-temporal contextual network for optical flow estimation. In contrast to previous optical flow estimation techniques, which employed local pyramid feature extraction and multi-level correlation, we propose a contextual relation exploration architecture that captures rich long-range dependencies in spatial and temporal dimensions. STC-Flow comprises three context modules - pyramidal spatial context module, temporal context correlation module, and recurrent residual contextual upsampling module - to establish the relationship in each stage of feature extraction, correlation, and flow reconstruction. The experimental results demonstrate that the proposed approach achieves the state-of-the-art performance of two-frame based methods on the Sintel dataset and the KITTI 2012/2015 datasets.",1
"Learning matching costs has been shown to be critical to the success of the state-of-the-art deep stereo matching methods, in which 3D convolutions are applied on a 4D feature volume to learn a 3D cost volume. However, this mechanism has never been employed for the optical flow task. This is mainly due to the significantly increased search dimension in the case of optical flow computation, ie, a straightforward extension would require dense 4D convolutions in order to process a 5D feature volume, which is computationally prohibitive. This paper proposes a novel solution that is able to bypass the requirement of building a 5D feature volume while still allowing the network to learn suitable matching costs from data. Our key innovation is to decouple the connection between 2D displacements and learn the matching costs at each 2D displacement hypothesis independently, ie, displacement-invariant cost learning. Specifically, we apply the same 2D convolution-based matching net independently on each 2D displacement hypothesis to learn a 4D cost volume. Moreover, we propose a displacement-aware projection layer to scale the learned cost volume, which reconsiders the correlation between different displacement candidates and mitigates the multi-modal problem in the learned cost volume. The cost volume is then projected to optical flow estimation through a 2D soft-argmin layer. Extensive experiments show that our approach achieves state-of-the-art accuracy on various datasets, and outperforms all published optical flow methods on the Sintel benchmark.",0
"Current deep stereo matching methods have demonstrated the importance of learning matching costs in achieving success. These methods employ 3D convolutions on a 4D feature volume to produce a 3D cost volume. However, this approach has not been applied to optical flow tasks due to the increased search dimension. The required dense 4D convolutions to process a 5D feature volume would be computationally prohibitive. This paper presents a new approach that bypasses the need for a 5D feature volume while still allowing the network to learn suitable matching costs. The novel solution involves decoupling the connection between 2D displacements and learning the matching costs at each 2D displacement hypothesis independently using displacement-invariant cost learning. The same 2D convolution-based matching net is applied independently on each 2D displacement hypothesis to learn a 4D cost volume. A displacement-aware projection layer scales the learned cost volume, which considers the correlation between different displacement candidates and mitigates the multi-modal problem in the learned cost volume. The cost volume is then projected to optical flow estimation through a 2D soft-argmin layer. The experimental results show that this approach achieves state-of-the-art accuracy on various datasets and outperforms all published optical flow methods on the Sintel benchmark.",1
"Automatic fall detection is a vital technology for ensuring the health and safety of people. Home-based camera systems for fall detection often put people's privacy at risk. Thermal cameras can partially or fully obfuscate facial features, thus preserving the privacy of a person. Another challenge is the less occurrence of falls in comparison to the normal activities of daily living. As fall occurs rarely, it is non-trivial to learn algorithms due to class imbalance. To handle these problems, we formulate fall detection as an anomaly detection within an adversarial framework using thermal imaging. We present a novel adversarial network that comprises of two-channel 3D convolutional autoencoders which reconstructs the thermal data and the optical flow input sequences respectively. We introduce a technique to track the region of interest, a region-based difference constraint, and a joint discriminator to compute the reconstruction error. A larger reconstruction error indicates the occurrence of a fall. The experiments on a publicly available thermal fall dataset show the superior results obtained compared to the standard baseline.",0
"Ensuring the health and safety of individuals is crucial, which is why automatic fall detection technology is important. However, fall detection systems using home-based cameras can potentially invade people's privacy. To address this issue, thermal cameras can be used to obscure facial features and preserve privacy. Additionally, fall detection algorithms can be challenging to develop due to the infrequent occurrence of falls compared to normal daily activities. To overcome these obstacles, we propose an anomaly detection approach within an adversarial framework using thermal imaging. Our novel adversarial network includes two-channel 3D convolutional autoencoders for reconstructing thermal data and optical flow input sequences, respectively. We introduce region-based difference constraints, a technique for tracking the region of interest, and a joint discriminator to compute the reconstruction error. A higher reconstruction error indicates a fall occurrence. Our experiments on a publicly available thermal fall dataset show superior results compared to the standard baseline.",1
"Applying image processing algorithms independently to each video frame often leads to temporal inconsistency in the resulting video. To address this issue, we present a novel and general approach for blind video temporal consistency. Our method is only trained on a pair of original and processed videos directly instead of a large dataset. Unlike most previous methods that enforce temporal consistency with optical flow, we show that temporal consistency can be achieved by training a convolutional network on a video with the Deep Video Prior. Moreover, a carefully designed iteratively reweighted training strategy is proposed to address the challenging multimodal inconsistency problem. We demonstrate the effectiveness of our approach on 7 computer vision tasks on videos. Extensive quantitative and perceptual experiments show that our approach obtains superior performance than state-of-the-art methods on blind video temporal consistency. Our source codes are publicly available at github.com/ChenyangLEI/deep-video-prior.",0
"The use of image processing algorithms on individual video frames can result in inconsistencies over time in the final video. To overcome this issue, we introduce a new and comprehensive method for achieving blind video temporal consistency. Our approach involves training on a pair of original and processed videos, rather than a large dataset. We prove that temporal consistency can be obtained by training a convolutional network on a video using the Deep Video Prior, unlike most previous methods that rely on optical flow. We also propose an iteratively reweighted training strategy to address the challenging multimodal inconsistency problem. Our approach has been tested on 7 computer vision tasks and has shown superior performance compared to existing methods for blind video temporal consistency. Our source codes are publicly available on github.com/ChenyangLEI/deep-video-prior.",1
"Video-based human pose estimation in crowded scenes is a challenging problem due to occlusion, motion blur, scale variation and viewpoint change, etc. Prior approaches always fail to deal with this problem because of (1) lacking of usage of temporal information; (2) lacking of training data in crowded scenes. In this paper, we focus on improving human pose estimation in videos of crowded scenes from the perspectives of exploiting temporal context and collecting new data. In particular, we first follow the top-down strategy to detect persons and perform single-person pose estimation for each frame. Then, we refine the frame-based pose estimation with temporal contexts deriving from the optical-flow. Specifically, for one frame, we forward the historical poses from the previous frames and backward the future poses from the subsequent frames to current frame, leading to stable and accurate human pose estimation in videos. In addition, we mine new data of similar scenes to HIE dataset from the Internet for improving the diversity of training set. In this way, our model achieves best performance on 7 out of 13 videos and 56.33 average w\_AP on test dataset of HIE challenge.",0
"The task of estimating human poses from videos in crowded settings is a difficult one due to various factors, such as occlusion, motion blur, scale variation, and viewpoint changes. Previous methods have not been successful in addressing this issue, primarily due to a lack of temporal information and training data in such environments. This research aims to enhance human pose estimation in crowded video scenes by leveraging temporal context and gathering new data. The approach involves detecting individuals and estimating their poses in each frame using a top-down strategy, followed by refining the pose estimation with temporal contexts from optical flow. This involves forwarding historical poses from previous frames and backwarding future poses from subsequent frames to the current frame, resulting in stable and accurate human pose estimation. Additionally, new data from similar scenes to the HIE dataset were obtained from the internet to diversify the training set and improve performance. The proposed model achieved the best results on 7 out of 13 videos and an average w\_AP of 56.33 on the HIE challenge test dataset.",1
"This paper presents our solution to ACM MM challenge: Large-scale Human-centric Video Analysis in Complex Events\cite{lin2020human}; specifically, here we focus on Track3: Crowd Pose Tracking in Complex Events. Remarkable progress has been made in multi-pose training in recent years. However, how to track the human pose in crowded and complex environments has not been well addressed. We formulate the problem as several subproblems to be solved. First, we use a multi-object tracking method to assign human ID to each bounding box generated by the detection model. After that, a pose is generated to each bounding box with ID. At last, optical flow is used to take advantage of the temporal information in the videos and generate the final pose tracking result.",0
"In this paper, we offer our solution to the ACM MM challenge of analyzing human-centric videos in complex events. More specifically, we concentrate on Track3, which involves tracking crowd poses in such events. Although there has been progress in multi-pose training, the issue of tracking human poses in crowded and complex settings remains unaddressed. To tackle this challenge, we break down the problem into several subproblems. Firstly, we apply a multi-object tracking approach to assign human IDs to the bounding boxes produced by the detection model. Next, we generate a pose for each bounding box with an ID. Finally, we incorporate optical flow to leverage the temporal information present in the videos and produce the ultimate pose tracking output.",1
"In this paper, we introduce a new benchmark dataset named IPN Hand with sufficient size, variety, and real-world elements able to train and evaluate deep neural networks. This dataset contains more than 4,000 gesture samples and 800,000 RGB frames from 50 distinct subjects. We design 13 different static and dynamic gestures focused on interaction with touchless screens. We especially consider the scenario when continuous gestures are performed without transition states, and when subjects perform natural movements with their hands as non-gesture actions. Gestures were collected from about 30 diverse scenes, with real-world variation in background and illumination. With our dataset, the performance of three 3D-CNN models is evaluated on the tasks of isolated and continuous real-time HGR. Furthermore, we analyze the possibility of increasing the recognition accuracy by adding multiple modalities derived from RGB frames, i.e., optical flow and semantic segmentation, while keeping the real-time performance of the 3D-CNN model. Our empirical study also provides a comparison with the publicly available nvGesture (NVIDIA) dataset. The experimental results show that the state-of-the-art ResNext-101 model decreases about 30% accuracy when using our real-world dataset, demonstrating that the IPN Hand dataset can be used as a benchmark, and may help the community to step forward in the continuous HGR. Our dataset and pre-trained models used in the evaluation are publicly available at https://github.com/GibranBenitez/IPN-hand.",0
"A new benchmark dataset called IPN Hand is presented in this paper, which is large enough to train and evaluate deep neural networks, and includes real-world elements and diverse scenarios. The dataset comprises over 4,000 gesture samples and 800,000 RGB frames from 50 subjects performing 13 different static and dynamic gestures for touchless screen interaction. The focus is on continuous gestures without transition states, and natural hand movements as non-gesture actions. The dataset was collected from about 30 scenes with real-world variation in background and illumination. The performance of three 3D-CNN models is evaluated for isolated and continuous real-time hand gesture recognition (HGR) tasks, and the possibility of improving recognition accuracy by adding optical flow and semantic segmentation derived from RGB frames is analyzed. The IPN Hand dataset is compared to the publicly available nvGesture (NVIDIA) dataset, and the state-of-the-art ResNext-101 model is found to decrease about 30% in accuracy when using the IPN Hand dataset. The results suggest that the IPN Hand dataset can be used as a benchmark for continuous HGR and may advance the field. The dataset and pre-trained models used in the evaluation are publicly available.",1
"In recent years, surveillance cameras are widely deployed in public places, and the general crime rate has been reduced significantly due to these ubiquitous devices. Usually, these cameras provide cues and evidence after crimes are conducted, while they are rarely used to prevent or stop criminal activities in time. It is both time and labor consuming to manually monitor a large amount of video data from surveillance cameras. Therefore, automatically recognizing violent behaviors from video signals becomes essential. This paper summarizes several existing video datasets for violence detection and proposes the RWF-2000 database with 2,000 videos captured by surveillance cameras in real-world scenes. Also, we present a new method that utilizes both the merits of 3D-CNNs and optical flow, namely Flow Gated Network. The proposed approach obtains an accuracy of 87.25% on the test set of our proposed database. The database and source codes are currently open to access.",0
"Surveillance cameras have become increasingly prevalent in public areas, resulting in a significant reduction in the overall crime rate. Despite their widespread use, these cameras are typically only utilized for evidence gathering after a crime has occurred, rather than preventing criminal activity in real-time. Manually monitoring large amounts of video data from surveillance cameras is a time-consuming and labor-intensive process, making the automatic recognition of violent behavior from video signals crucial. This paper outlines various existing video datasets for violence detection and introduces the RWF-2000 database, which contains 2,000 videos captured by surveillance cameras in real-world scenarios. Additionally, a new method that combines 3D-CNNs and optical flow, known as the Flow Gated Network, is proposed. This approach achieves an accuracy of 87.25% on the test set of the proposed database. The database and source codes are currently available for access.",1
"Visual voice activity detection (V-VAD) uses visual features to predict whether a person is speaking or not. V-VAD is useful whenever audio VAD (A-VAD) is inefficient either because the acoustic signal is difficult to analyze or because it is simply missing. We propose two deep architectures for V-VAD, one based on facial landmarks and one based on optical flow. Moreover, available datasets, used for learning and for testing V-VAD, lack content variability. We introduce a novel methodology to automatically create and annotate very large datasets in-the-wild -- WildVVAD -- based on combining A-VAD with face detection and tracking. A thorough empirical evaluation shows the advantage of training the proposed deep V-VAD models with this dataset.",0
"The process of visual voice activity detection (V-VAD) involves utilizing visual cues to determine if an individual is speaking. This method proves beneficial in cases where audio VAD (A-VAD) is not effective due to difficulties in analyzing the acoustic signal or its absence. Our study proposes two deep architectures for V-VAD, with one relying on facial landmarks and the other on optical flow. However, existing datasets used to train and test V-VAD lack diversity in their content. To address this issue, we introduce a new approach called WildVVAD, which involves generating and annotating large datasets in-the-wild. This methodology combines A-VAD, face detection, and tracking to create a more varied dataset. Our empirical evaluation showcases the effectiveness of training the proposed deep V-VAD models with this diverse dataset.",1
"Nowadays 360 video analysis has become a significant research topic in the field since the appearance of high-quality and low-cost 360 wearable devices. In this paper, we propose a novel LiteFlowNet360 architecture for 360 videos optical flow estimation. We design LiteFlowNet360 as a domain adaptation framework from perspective video domain to 360 video domain. We adapt it from simple kernel transformation techniques inspired by Kernel Transformer Network (KTN) to cope with inherent distortion in 360 videos caused by the sphere-to-plane projection. First, we apply an incremental transformation of convolution layers in feature pyramid network and show that further transformation in inference and regularization layers are not important, hence reducing the network growth in terms of size and computation cost. Second, we refine the network by training with augmented data in a supervised manner. We perform data augmentation by projecting the images in a sphere and re-projecting to a plane. Third, we train LiteFlowNet360 in a self-supervised manner using target domain 360 videos. Experimental results show the promising results of 360 video optical flow estimation using the proposed novel architecture.",0
"Due to the emergence of inexpensive and high-quality 360 wearable devices, the analysis of 360 videos has become an important research area. This paper presents a new architecture, LiteFlowNet360, for optical flow estimation in 360 videos. We have designed LiteFlowNet360 as a domain adaptation framework that transforms from the perspective video domain to the 360 video domain. To address the inherent distortion in 360 videos caused by the sphere-to-plane projection, we have applied incremental convolution layer transformations in the feature pyramid network. We have also trained the network with augmented data in a supervised manner and in a self-supervised manner using target domain 360 videos. Our results demonstrate the potential of the proposed architecture for 360 video optical flow estimation. Furthermore, we have reduced the network's growth in terms of size and computation cost by demonstrating that further transformation in inference and regularization layers is unnecessary.",1
"Moving objects in scenes are still a severe challenge for the SLAM system. Many efforts have tried to remove the motion regions in the images by detecting moving objects. In this way, the keypoints belonging to motion regions will be ignored in the later calculations. In this paper, we proposed a novel motion removal method, leveraging semantic information and optical flow to extract motion regions. Different from previous works, we don't predict moving objects or motion regions directly from image sequences. We computed rigid optical flow, synthesized by the depth and pose, and compared it against the estimated optical flow to obtain initial motion regions. Then, we utilized K-means to finetune the motion region masks with instance segmentation masks. The ORB-SLAM2 integrated with the proposed motion removal method achieved the best performance in both indoor and outdoor dynamic environments.",0
"The SLAM system still faces a major obstacle with moving objects in scenes. Previous attempts have been made to tackle this issue by detecting and eliminating motion regions in images. Our paper presents a new motion removal technique that employs semantic information and optical flow to identify motion regions. Unlike previous methods, we do not directly predict moving objects or motion regions from image sequences. Instead, we use the depth and pose to compute rigid optical flow and compare it with estimated optical flow to identify initial motion regions. We then refine these motion regions using K-means and instance segmentation masks. Our proposed method, when integrated with ORB-SLAM2, yielded the best results in both indoor and outdoor dynamic environments.",1
"Drowsiness driving is a major cause of traffic accidents and thus numerous previous researches have focused on driver drowsiness detection. Many drive relevant factors have been taken into consideration for fatigue detection and can lead to high precision, but there are still several serious constraints, such as most existing models are environmentally susceptible. In this paper, fatigue detection is considered as temporal action detection problem instead of image classification. The proposed detection system can be divided into four parts: (1) Localize the key patches of the detected driver picture which are critical for fatigue detection and calculate the corresponding optical flow. (2) Contrast Limited Adaptive Histogram Equalization (CLAHE) is used in our system to reduce the impact of different light conditions. (3) Three individual two-stream networks combined with attention mechanism are designed for each feature to extract temporal information. (4) The outputs of the three sub-networks will be concatenated and sent to the fully-connected network, which judges the status of the driver. The drowsiness detection system is trained and evaluated on the famous Nation Tsing Hua University Driver Drowsiness Detection (NTHU-DDD) dataset and we obtain an accuracy of 94.46%, which outperforms most existing fatigue detection models.",0
"Numerous studies have focused on detecting drowsiness in drivers, as it is a leading cause of traffic accidents. Although many factors have been considered for fatigue detection, there are still significant limitations, such as the susceptibility of existing models to environmental conditions. In this paper, we propose a novel approach that views fatigue detection as a temporal action detection problem, instead of an image classification problem. Our system consists of four parts: (1) localizing critical patches in the driver's picture and calculating corresponding optical flow, (2) using Contrast Limited Adaptive Histogram Equalization (CLAHE) to address variable lighting conditions, (3) designing three two-stream networks with attention mechanisms to extract temporal information, and (4) concatenating the outputs of the sub-networks and sending them to a fully-connected network for status judgement. Our drowsiness detection system was trained and evaluated on the Nation Tsing Hua University Driver Drowsiness Detection (NTHU-DDD) dataset, achieving an accuracy of 94.46%, surpassing most existing fatigue detection models.",1
"Scene flow represents the 3D motion of every point in the dynamic environments. Like the optical flow that represents the motion of pixels in 2D images, 3D motion representation of scene flow benefits many applications, such as autonomous driving and service robot. This paper studies the problem of scene flow estimation from two consecutive 3D point clouds. In this paper, a novel hierarchical neural network with double attention is proposed for learning the correlation of point features in adjacent frames and refining scene flow from coarse to fine layer by layer. The proposed network has a new more-for-less hierarchical architecture. The more-for-less means that the number of input points is greater than the number of output points for scene flow estimation, which brings more input information and balances the precision and resource consumption. In this hierarchical architecture, scene flow of different levels is generated and supervised respectively. A novel attentive embedding module is introduced to aggregate the features of adjacent points using a double attention method in a patch-to-patch manner. The proper layers for flow embedding and flow supervision are carefully considered in our network designment. Experiments show that the proposed network outperforms the state-of-the-art performance of 3D scene flow estimation on the FlyingThings3D and KITTI Scene Flow 2015 datasets. We also apply the proposed network to realistic LiDAR odometry task, which is an key problem in autonomous driving. The experiment results demonstrate that our proposed network can outperform the ICP-based method and shows the good practical application ability.",0
"The motion of every point in dynamic environments is represented by scene flow in 3D. This representation is similar to optical flow, which represents the motion of pixels in 2D images and has numerous applications such as autonomous driving and service robots. This paper focuses on estimating scene flow from two consecutive 3D point clouds and proposes a novel hierarchical neural network with double attention to learn the correlation of point features. The proposed network has a more-for-less hierarchical architecture, which means that the number of input points is greater than the number of output points for scene flow estimation. This architecture generates and supervises scene flow of different levels separately, and an attentive embedding module is introduced to aggregate the features of adjacent points using a double attention method. The network design is carefully considered for flow embedding and supervision, and experimental results show that it outperforms state-of-the-art performance on the FlyingThings3D and KITTI Scene Flow 2015 datasets. The proposed network is also applied to realistic LiDAR odometry tasks and outperforms the ICP-based method, demonstrating its practical application ability.",1
"Current state-of-the-art trackers often fail due to distractorsand large object appearance changes. In this work, we explore the use ofdense optical flow to improve tracking robustness. Our main insight is that, because flow estimation can also have errors, we need to incorporate an estimate of flow uncertainty for robust tracking. We present a novel tracking framework which combines appearance and flow uncertainty information to track objects in challenging scenarios. We experimentally verify that our framework improves tracking robustness, leading to new state-of-the-art results. Further, our experimental ablations shows the importance of flow uncertainty for robust tracking.",0
"The current advanced trackers frequently encounter problems caused by distractions and substantial changes in the appearance of objects. This study investigates the potential of dense optical flow to enhance tracking resilience. The primary realization is that incorporating an assessment of flow uncertainty is necessary because flow estimation errors can also occur. We introduce a new tracking framework that combines both appearance and flow uncertainty information to track objects in difficult situations. Our experimentation confirms that our framework enhances tracking robustness, resulting in new top-performing outcomes. Additionally, our experimental ablations demonstrate the significance of flow uncertainty for resilient tracking.",1
"Person Re-Identification (ReID) is a challenging problem in many video analytics and surveillance applications, where a person's identity must be associated across a distributed non-overlapping network of cameras. Video-based person ReID has recently gained much interest because it allows capturing discriminant spatio-temporal information from video clips that is unavailable for image-based ReID. Despite recent advances, deep learning (DL) models for video ReID often fail to leverage this information to improve the robustness of feature representations. In this paper, the motion pattern of a person is explored as an additional cue for ReID. In particular, a flow-guided Mutual Attention network is proposed for fusion of image and optical flow sequences using any 2D-CNN backbone, allowing to encode temporal information along with spatial appearance information. Our Mutual Attention network relies on the joint spatial attention between image and optical flow features maps to activate a common set of salient features across them. In addition to flow-guided attention, we introduce a method to aggregate features from longer input streams for better video sequence-level representation. Our extensive experiments on three challenging video ReID datasets indicate that using the proposed Mutual Attention network allows to improve recognition accuracy considerably with respect to conventional gated-attention networks, and state-of-the-art methods for video-based person ReID.",0
"The identification of individuals across a network of cameras is a complex task known as Person Re-Identification (ReID). Although image-based ReID has been the traditional approach, video-based ReID has been gaining attention as it can capture more informative spatio-temporal data. However, DL models for video ReID often fail to utilize this data for improved feature representation. This paper proposes a flow-guided Mutual Attention network that utilizes the motion pattern of a person as an additional cue for ReID. The network fuses image and optical flow sequences using a 2D-CNN backbone and encodes both spatial and temporal information. The Mutual Attention network employs joint spatial attention between image and optical flow feature maps to activate a common set of salient features. Additionally, a method for aggregating features from longer input streams is introduced for better representation of video sequences. Results from experiments on three challenging video ReID datasets demonstrate that the proposed Mutual Attention network significantly improves recognition accuracy compared to conventional gated-attention networks and state-of-the-art methods for video-based person ReID.",1
"Drones shooting can be applied in dynamic traffic monitoring, object detecting and tracking, and other vision tasks. The variability of the shooting location adds some intractable challenges to these missions, such as varying scale, unstable exposure, and scene migration. In this paper, we strive to tackle the above challenges and automatically understand the crowd from the visual data collected from drones. First, to alleviate the background noise generated in cross-scene testing, a double-stream crowd counting model is proposed, which extracts optical flow and frame difference information as an additional branch. Besides, to improve the model's generalization ability at different scales and time, we randomly combine a variety of data transformation methods to simulate some unseen environments. To tackle the crowd density estimation problem under extreme dark environments, we introduce synthetic data generated by game Grand Theft Auto V(GTAV). Experiment results show the effectiveness of the virtual data. Our method wins the challenge with a mean absolute error (MAE) of 12.70. Moreover, a comprehensive ablation study is conducted to explore each component's contribution.",0
"The use of drones for shooting can aid in various dynamic traffic monitoring and object detection and tracking tasks. However, the variable shooting locations pose challenges such as unstable exposure, varying scale, and scene migration. This study aims to address these challenges and automatically understand crowd behavior from drone captured visual data. A double-stream crowd counting model is proposed to reduce background noise in cross-scene testing by extracting optical flow and frame difference information. To improve the model's generalization ability, a variety of data transformation methods are randomly combined to simulate unseen environments. Synthetic data generated by Grand Theft Auto V (GTAV) is introduced to tackle crowd density estimation under extreme dark environments. The effectiveness of the virtual data is demonstrated by winning the challenge with a mean absolute error (MAE) of 12.70. Additionally, a comprehensive ablation study is conducted to explore each component's contribution.",1
"Optical flow, which expresses pixel displacement, is widely used in many computer vision tasks to provide pixel-level motion information. However, with the remarkable progress of the convolutional neural network, recent state-of-the-art approaches are proposed to solve problems directly on feature-level. Since the displacement of feature vector is not consistent to the pixel displacement, a common approach is to:forward optical flow to a neural network and fine-tune this network on the task dataset. With this method,they expect the fine-tuned network to produce tensors encoding feature-level motion information. In this paper, we rethink this de facto paradigm and analyze its drawbacks in the video object detection task. To mitigate these issues, we propose a novel network (IFF-Net) with an \textbf{I}n-network \textbf{F}eature \textbf{F}low estimation module (IFF module) for video object detection. Without resorting pre-training on any additional dataset, our IFF module is able to directly produce \textbf{feature flow} which indicates the feature displacement. Our IFF module consists of a shallow module, which shares the features with the detection branches. This compact design enables our IFF-Net to accurately detect objects, while maintaining a fast inference speed. Furthermore, we propose a transformation residual loss (TRL) based on \textit{self-supervision}, which further improves the performance of our IFF-Net. Our IFF-Net outperforms existing methods and sets a state-of-the-art performance on ImageNet VID.",0
"The use of optical flow to provide pixel-level motion information has been commonplace in many computer vision tasks. However, recent advancements in convolutional neural networks have led to the development of state-of-the-art approaches that solve problems directly on the feature-level. Despite the fact that the displacement of feature vectors is inconsistent with that of pixel displacement, a popular method involves forwarding optical flow to a neural network and fine-tuning it on the task dataset to produce tensors encoding feature-level motion information. In this paper, the authors critique this approach and highlight its shortcomings in video object detection. To address these issues, they propose a new network called IFF-Net that incorporates an in-network feature flow estimation module (IFF module). This module is capable of directly producing feature flow without requiring pre-training on additional datasets. The IFF-Net is designed to accurately detect objects while maintaining a fast inference speed. Additionally, the authors propose a transformation residual loss (TRL) based on self-supervision to further enhance the performance of their IFF-Net. Their approach outperforms existing methods and achieves state-of-the-art performance on ImageNet VID.",1
"Video object detection is a tough task due to the deteriorated quality of video sequences captured under complex environments. Currently, this area is dominated by a series of feature enhancement based methods, which distill beneficial semantic information from multiple frames and generate enhanced features through fusing the distilled information. However, the distillation and fusion operations are usually performed at either frame level or instance level with external guidance using additional information, such as optical flow and feature memory. In this work, we propose a dual semantic fusion network (abbreviated as DSFNet) to fully exploit both frame-level and instance-level semantics in a unified fusion framework without external guidance. Moreover, we introduce a geometric similarity measure into the fusion process to alleviate the influence of information distortion caused by noise. As a result, the proposed DSFNet can generate more robust features through the multi-granularity fusion and avoid being affected by the instability of external guidance. To evaluate the proposed DSFNet, we conduct extensive experiments on the ImageNet VID dataset. Notably, the proposed dual semantic fusion network achieves, to the best of our knowledge, the best performance of 84.1\% mAP among the current state-of-the-art video object detectors with ResNet-101 and 85.4\% mAP with ResNeXt-101 without using any post-processing steps.",0
"Detecting objects in videos is a challenging task due to the poor quality of video footage captured in complex environments. Existing methods rely on feature enhancement techniques to extract relevant semantic information from multiple frames and generate enhanced features by combining this information. However, these methods often require external guidance and operate at either the frame or instance level. This study proposes a dual semantic fusion network (DSFNet) that integrates both frame-level and instance-level semantics into a single fusion framework without external guidance. Additionally, a geometric similarity measure is introduced to reduce the impact of information distortion caused by noise. The proposed DSFNet generates more robust features through multi-granularity fusion and is not affected by the instability of external guidance. Experiments on the ImageNet VID dataset show that the proposed method achieves the best performance among current state-of-the-art video object detectors with ResNet-101 and ResNeXt-101, without any post-processing steps, with 84.1% mAP and 85.4% mAP, respectively.",1
"Deformable convolution, originally proposed for the adaptation to geometric variations of objects, has recently shown compelling performance in aligning multiple frames and is increasingly adopted for video super-resolution. Despite its remarkable performance, its underlying mechanism for alignment remains unclear. In this study, we carefully investigate the relation between deformable alignment and the classic flow-based alignment. We show that deformable convolution can be decomposed into a combination of spatial warping and convolution. This decomposition reveals the commonality of deformable alignment and flow-based alignment in formulation, but with a key difference in their offset diversity. We further demonstrate through experiments that the increased diversity in deformable alignment yields better-aligned features, and hence significantly improves the quality of video super-resolution output. Based on our observations, we propose an offset-fidelity loss that guides the offset learning with optical flow. Experiments show that our loss successfully avoids the overflow of offsets and alleviates the instability problem of deformable alignment. Aside from the contributions to deformable alignment, our formulation inspires a more flexible approach to introduce offset diversity to flow-based alignment, improving its performance.",0
"Deformable convolution was originally designed to adapt to the geometric variations of objects and has recently gained popularity in video super-resolution for aligning multiple frames. Despite its impressive performance, the mechanism behind its alignment process is unclear. This study aims to investigate the relationship between deformable alignment and flow-based alignment. The findings reveal that deformable convolution is a combination of spatial warping and convolution, which shares a similar formulation with flow-based alignment but with a key difference in their offset diversity. Experiments show that the increased diversity in deformable alignment leads to better-aligned features and significantly improves the quality of video super-resolution output. To address the offset overflow and instability problem of deformable alignment, an offset-fidelity loss is proposed to guide offset learning with optical flow. Furthermore, the formulation inspires a flexible approach to introduce offset diversity to flow-based alignment, leading to improved performance.",1
"Optical flow estimation is an important computer vision task, which aims at estimating the dense correspondences between two frames. RAFT (Recurrent All Pairs Field Transforms) currently represents the state-of-the-art in optical flow estimation. It has excellent generalization ability and has obtained outstanding results across several benchmarks. To further improve the robustness and achieve accurate optical flow estimation, we present PRAFlow (Pyramid Recurrent All-Pairs Flow), which builds upon the pyramid network structure. Due to computational limitation, our proposed network structure only uses two pyramid layers. At each layer, the RAFT unit is used to estimate the optical flow at the current resolution. Our model was trained on several simulate and real-image datasets, submitted to multiple leaderboards using the same model and parameters, and won the 2nd place in the optical flow task of ECCV 2020 workshop: Robust Vision Challenge.",0
"The task of estimating optical flow, which involves determining the dense correspondences between two frames, is crucial in computer vision. RAFT is currently the most advanced method for this task, with exceptional generalization capabilities and impressive results across various benchmarks. However, to enhance accuracy and robustness, we introduce PRAFlow (Pyramid Recurrent All-Pairs Flow), which utilizes a pyramid network structure. Due to computational constraints, our model only employs two pyramid layers, with the RAFT unit used at each layer to estimate optical flow at the current resolution. We trained our model on multiple simulated and real-image datasets, and it achieved the second position on the optical flow task of ECCV 2020 workshop: Robust Vision Challenge, using the same parameters and model for multiple leaderboards.",1
"In the learning based video compression approaches, it is an essential issue to compress pixel-level optical flow maps by developing new motion vector (MV) encoders. In this work, we propose a new framework called Resolution-adaptive Flow Coding (RaFC) to effectively compress the flow maps globally and locally, in which we use multi-resolution representations instead of single-resolution representations for both the input flow maps and the output motion features of the MV encoder. To handle complex or simple motion patterns globally, our frame-level scheme RaFC-frame automatically decides the optimal flow map resolution for each video frame. To cope different types of motion patterns locally, our block-level scheme called RaFC-block can also select the optimal resolution for each local block of motion features. In addition, the rate-distortion criterion is applied to both RaFC-frame and RaFC-block and select the optimal motion coding mode for effective flow coding. Comprehensive experiments on four benchmark datasets HEVC, VTL, UVG and MCL-JCV clearly demonstrate the effectiveness of our overall RaFC framework after combing RaFC-frame and RaFC-block for video compression.",0
"Developing new motion vector encoders to compress pixel-level optical flow maps is crucial in learning-based video compression approaches. Our proposed Resolution-adaptive Flow Coding (RaFC) framework effectively compresses flow maps globally and locally by utilizing multi-resolution representations for input flow maps and output motion features of the MV encoder. RaFC-frame automatically determines the optimal flow map resolution for each video frame to handle complex or simple motion patterns globally. RaFC-block selects the optimal resolution for each local block of motion features to cope with different types of motion patterns locally. The rate-distortion criterion is applied to both RaFC-frame and RaFC-block to select the optimal motion coding mode for effective flow coding. Our comprehensive experiments on four benchmark datasets demonstrate the effectiveness of our overall RaFC framework, which combines RaFC-frame and RaFC-block for video compression.",1
"We propose a lightweight real-time sign language detection model, as we identify the need for such a case in videoconferencing. We extract optical flow features based on human pose estimation and, using a linear classifier, show these features are meaningful with an accuracy of 80%, evaluated on the DGS Corpus. Using a recurrent model directly on the input, we see improvements of up to 91% accuracy, while still working under 4ms. We describe a demo application to sign language detection in the browser in order to demonstrate its usage possibility in videoconferencing applications.",0
"Our proposition is a model for detecting sign language in real-time, which is lightweight and essential for videoconferencing purposes. We utilize optical flow features extracted from human pose estimation and demonstrate their significance with an accuracy rate of 80%, evaluated on the DGS Corpus, through a linear classifier. Furthermore, by applying a recurrent model directly on the input, we observe an accuracy improvement of up to 91% while still operating within 4ms. To showcase its potential in videoconferencing applications, we present a demo application for sign language detection in the browser.",1
"Matching and partitioning problems are fundamentals of computer vision applications with examples in multilabel segmentation, stereo estimation and optical-flow computation. These tasks can be posed as non-convex energy minimization problems and solved near-globally optimal by recent convex lifting approaches. Yet, applying these techniques comes with a significant computational effort, reducing their feasibility in practical applications. We discuss spatial discretization of continuous partitioning problems into a graph structure, generalizing discretization onto a Cartesian grid. This setup allows us to faithfully work on super-pixel graphs constructed by SLIC or Cut-Pursuit, massively decreasing the computational effort for lifted partitioning problems compared to a Cartesian grid, while optimal energy values remain similar: The global matching is still solved near-globally optimal. We discuss this methodology in detail and show examples in multi-label segmentation by minimal partitions and stereo estimation, where we demonstrate that the proposed graph discretization can reduce runtime as well as memory consumption of convex relaxations of matching problems by up to a factor of 10.",0
"The basics of computer vision applications include matching and partitioning problems. These are utilized in various tasks such as multilabel segmentation, stereo estimation, and optical-flow computation. These problems can be presented as non-convex energy minimization issues and can be solved nearly optimally through recent convex lifting methods. However, using these techniques require significant computational effort, making them less feasible for practical applications. In this paper, we explore how to discretize continuous partitioning problems spatially into a graph structure. This setup allows us to work on super-pixel graphs created by SLIC or Cut-Pursuit, which greatly decreases the computational effort for partitioning problems compared to a Cartesian grid. Despite this, optimal energy values remain similar, and the global matching is still solved near-globally optimal. We provide a detailed discussion of this methodology and present examples of its use in multi-label segmentation and stereo estimation. Our results show that using graph discretization can reduce runtime and memory consumption of matching problems' convex relaxations by up to 10 times.",1
"Real-time tool segmentation is an essential component in computer-assisted surgical systems. We propose a novel real-time automatic method based on Fully Convolutional Networks (FCN) and optical flow tracking. Our method exploits the ability of deep neural networks to produce accurate segmentations of highly deformable parts along with the high speed of optical flow. Furthermore, the pre-trained FCN can be fine-tuned on a small amount of medical images without the need to hand-craft features. We validated our method using existing and new benchmark datasets, covering both ex vivo and in vivo real clinical cases where different surgical instruments are employed. Two versions of the method are presented, non-real-time and real-time. The former, using only deep learning, achieves a balanced accuracy of 89.6% on a real clinical dataset, outperforming the (non-real-time) state of the art by 3.8% points. The latter, a combination of deep learning with optical flow tracking, yields an average balanced accuracy of 78.2% across all the validated datasets.",0
"In computer-assisted surgical systems, real-time tool segmentation plays a crucial role. Our innovative approach, which is based on Fully Convolutional Networks (FCN) and optical flow tracking, provides an automatic and real-time solution for this task. By utilizing deep neural networks and optical flow, our method is able to accurately segment highly deformable parts quickly. Additionally, our pre-trained FCN can be fine-tuned with a small amount of medical images without the need for feature engineering. We tested our method on various benchmark datasets comprising ex vivo and in vivo clinical cases involving different surgical instruments. We present two versions of our method, one of which is non-real-time and solely relies on deep learning and achieves a balanced accuracy of 89.6%, outperforming the current state-of-the-art by 3.8%. The other version combines deep learning with optical flow tracking and yields an average balanced accuracy of 78.2% across all the validated datasets.",1
"Visual odometry (VO) is a prevalent way to deal with the relative localization problem, which is becoming increasingly mature and accurate, but it tends to be fragile under challenging environments. Comparing with classical geometry-based methods, deep learning-based methods can automatically learn effective and robust representations, such as depth, optical flow, feature, ego-motion, etc., from data without explicit computation. Nevertheless, there still lacks a thorough review of the recent advances of deep learning-based VO (Deep VO). Therefore, this paper aims to gain a deep insight on how deep learning can profit and optimize the VO systems. We first screen out a number of qualifications including accuracy, efficiency, scalability, dynamicity, practicability, and extensibility, and employ them as the criteria. Then, using the offered criteria as the uniform measurements, we detailedly evaluate and discuss how deep learning improves the performance of VO from the aspects of depth estimation, feature extraction and matching, pose estimation. We also summarize the complicated and emerging areas of Deep VO, such as mobile robots, medical robots, augmented reality and virtual reality, etc. Through the literature decomposition, analysis, and comparison, we finally put forward a number of open issues and raise some future research directions in this field.",0
"Visual odometry (VO) is an established method for dealing with relative localization, but it can be fragile in challenging environments. Deep learning-based methods have the advantage of automatically learning robust representations from data without explicit computation. However, there is a lack of comprehensive reviews of recent advances in deep learning-based VO (Deep VO). This paper aims to provide insights into how deep learning can optimize VO systems. The paper identifies accuracy, efficiency, scalability, dynamicity, practicability, and extensibility as criteria for evaluation. Using these criteria, the paper evaluates and discusses how deep learning improves the performance of VO in areas such as depth estimation, feature extraction and matching, and pose estimation. The paper also summarizes emerging areas such as mobile robots, medical robots, augmented reality, and virtual reality. Through literature analysis and comparison, the paper raises open issues and suggests future research directions in the field.",1
"Objective Semi-supervised video object segmentation refers to segmenting the object in subsequent frames given the object label in the first frame. Existing algorithms are mostly based on the objectives of matching and propagation strategies, which often make use of the previous frame with masking or optical flow. This paper explores a new propagation method, uses short-term matching modules to extract the information of the previous frame and apply it in propagation, and proposes the network of Long-Short-Term similarity matching for video object segmentation (LSMOVS) Method: By conducting pixel-level matching and correlation between long-term matching module and short-term matching module with the first frame and previous frame, global similarity map and local similarity map are obtained, as well as feature pattern of current frame and masking of previous frame. After two refine networks, final results are obtained through segmentation network. Results: According to the experiments on the two data sets DAVIS 2016 and 2017, the method of this paper achieves favorable average of region similarity and contour accuracy without online fine tuning, which achieves 86.5% and 77.4% in terms of single target and multiple targets. Besides, the count of segmented frames per second reached 21. Conclusion: The short-term matching module proposed in this paper is more conducive to extracting the information of the previous frame than only the mask. By combining the long-term matching module with the short-term matching module, the whole network can achieve efficient video object segmentation without online fine tuning",0
"Semi-supervised video object segmentation involves identifying the object in consecutive frames using the label from the first frame. Most current algorithms rely on matching and propagation strategies, often utilizing masking or optical flow from previous frames. This paper introduces a new propagation method, which employs short-term matching modules to extract information from the previous frame and implement it in propagation. The proposed network, Long-Short-Term Similarity Matching for Video Object Segmentation (LSMOVS), uses pixel-level matching and correlation between long-term and short-term matching modules, producing global and local similarity maps and feature patterns for the current frame and previous frame masking. Two refinement networks are applied, and the segmentation network produces the final result. Experimental results on DAVIS 2016 and 2017 datasets show that the proposed method achieves favorable region similarity and contour accuracy without online fine-tuning, with single target and multiple targets reaching 86.5% and 77.4%, respectively. Additionally, the segmented frames per second reach 21. The short-term matching module in this paper extracts previous frame information more efficiently than masking alone, and the combination of long-term and short-term matching modules achieves efficient video object segmentation without online fine-tuning.",1
"In this paper, we propose a panorama stitching algorithm based on asymmetric bidirectional optical flow. This algorithm expects multiple photos captured by fisheye lens cameras as input, and then, through the proposed algorithm, these photos can be merged into a high-quality 360-degree spherical panoramic image. For photos taken from a distant perspective, the parallax among them is relatively small, and the obtained panoramic image can be nearly seamless and undistorted. For photos taken from a close perspective or with a relatively large parallax, a seamless though partially distorted panoramic image can also be obtained. Besides, with the help of Graphics Processing Unit (GPU), this algorithm can complete the whole stitching process at a very fast speed: typically, it only takes less than 30s to obtain a panoramic image of 9000-by-4000 pixels, which means our panorama stitching algorithm is of high value in many real-time applications. Our code is available at https://github.com/MungoMeng/Panorama-OpticalFlow.",0
"This paper proposes an algorithm for stitching panoramic images using asymmetric bidirectional optical flow. The algorithm takes multiple photos captured by fisheye lens cameras as input and merges them into a high-quality 360-degree spherical panoramic image. The resulting image is nearly seamless and undistorted for photos taken from a distant perspective with relatively small parallax. However, for photos taken from a close perspective or with a larger parallax, a partially distorted seamless panoramic image can still be obtained. The algorithm utilizes Graphics Processing Unit (GPU) to complete the stitching process quickly, taking less than 30 seconds to obtain a panoramic image of 9000-by-4000 pixels. This makes it valuable for real-time applications. The code is available at https://github.com/MungoMeng/Panorama-OpticalFlow.",1
"As a vital topic in media content interpretation, video anomaly detection (VAD) has made fruitful progress via deep neural network (DNN). However, existing methods usually follow a reconstruction or frame prediction routine. They suffer from two gaps: (1) They cannot localize video activities in a both precise and comprehensive manner. (2) They lack sufficient abilities to utilize high-level semantics and temporal context information. Inspired by frequently-used cloze test in language study, we propose a brand-new VAD solution named Video Event Completion (VEC) to bridge gaps above: First, we propose a novel pipeline to achieve both precise and comprehensive enclosure of video activities. Appearance and motion are exploited as mutually complimentary cues to localize regions of interest (RoIs). A normalized spatio-temporal cube (STC) is built from each RoI as a video event, which lays the foundation of VEC and serves as a basic processing unit. Second, we encourage DNN to capture high-level semantics by solving a visual cloze test. To build such a visual cloze test, a certain patch of STC is erased to yield an incomplete event (IE). The DNN learns to restore the original video event from the IE by inferring the missing patch. Third, to incorporate richer motion dynamics, another DNN is trained to infer erased patches' optical flow. Finally, two ensemble strategies using different types of IE and modalities are proposed to boost VAD performance, so as to fully exploit the temporal context and modality information for VAD. VEC can consistently outperform state-of-the-art methods by a notable margin (typically 1.5%-5% AUROC) on commonly-used VAD benchmarks. Our codes and results can be verified at github.com/yuguangnudt/VEC_VAD.",0
"Video anomaly detection (VAD) has been an important area of focus in media content interpretation and has shown significant progress through the use of deep neural network (DNN). However, current methods often rely on reconstruction or frame prediction routines and are plagued by two issues: (1) They lack the ability to precisely and comprehensively localize video activities, and (2) They fail to effectively utilize high-level semantics and temporal context information. To address these gaps, we propose a novel VAD approach called Video Event Completion (VEC). Our solution includes a unique pipeline that employs appearance and motion as complementary cues to localize regions of interest (RoIs). We construct a normalized spatio-temporal cube (STC) from each RoI to serve as the foundation of VEC and as a basic processing unit. To capture high-level semantics, we propose a visual cloze test where a certain patch of STC is erased to yield an incomplete event (IE). The DNN is trained to restore the original video event from the IE by inferring the missing patch. In addition, we also train another DNN to infer erased patches' optical flow to incorporate richer motion dynamics. We propose two ensemble strategies using different types of IE and modalities to boost VAD performance, fully exploiting the temporal context and modality information. Our VEC solution consistently outperforms state-of-the-art methods with a notable margin (typically 1.5%-5% AUROC) on commonly-used VAD benchmarks. Our codes and results can be verified on github.com/yuguangnudt/VEC_VAD.",1
"Our previous work classified a taxonomy of suturing gestures during a vesicourethral anastomosis of robotic radical prostatectomy in association with tissue tears and patient outcomes. Herein, we train deep-learning based computer vision (CV) to automate the identification and classification of suturing gestures for needle driving attempts. Using two independent raters, we manually annotated live suturing video clips to label timepoints and gestures. Identification (2395 videos) and classification (511 videos) datasets were compiled to train CV models to produce two- and five-class label predictions, respectively. Networks were trained on inputs of raw RGB pixels as well as optical flow for each frame. Each model was trained on 80/20 train/test splits. In this study, all models were able to reliably predict either the presence of a gesture (identification, AUC: 0.88) as well as the type of gesture (classification, AUC: 0.87) at significantly above chance levels. For both gesture identification and classification datasets, we observed no effect of recurrent classification model choice (LSTM vs. convLSTM) on performance. Our results demonstrate CV's ability to recognize features that not only can identify the action of suturing but also distinguish between different classifications of suturing gestures. This demonstrates the potential to utilize deep learning CV towards future automation of surgical skill assessment.",0
"In our previous work, we established a taxonomy of suturing gestures involved in a vesicourethral anastomosis of robotic radical prostatectomy, which was linked to tissue tears and patient outcomes. In this study, we aimed to automate the identification and classification of these suturing gestures using deep-learning based computer vision (CV). To train the CV models, we manually annotated live suturing video clips using two independent raters and compiled identification and classification datasets. The models were trained on raw RGB pixels and optical flow for each frame and were able to predict the presence and type of the gesture with high accuracy. We found that the choice of recurrent classification model did not affect performance. Our results suggest that deep learning CV has the potential to automate surgical skill assessment by recognizing and distinguishing between different suturing gestures.",1
"We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network architecture for optical flow. RAFT extracts per-pixel features, builds multi-scale 4D correlation volumes for all pairs of pixels, and iteratively updates a flow field through a recurrent unit that performs lookups on the correlation volumes. RAFT achieves state-of-the-art performance. On KITTI, RAFT achieves an F1-all error of 5.10%, a 16% error reduction from the best published result (6.10%). On Sintel (final pass), RAFT obtains an end-point-error of 2.855 pixels, a 30% error reduction from the best published result (4.098 pixels). In addition, RAFT has strong cross-dataset generalization as well as high efficiency in inference time, training speed, and parameter count. Code is available at https://github.com/princeton-vl/RAFT.",0
"RAFT is a new deep network architecture for optical flow that has achieved state-of-the-art performance. It involves extracting per-pixel features and creating multi-scale 4D correlation volumes for every pair of pixels. A flow field is updated iteratively through a recurrent unit that performs lookups on the correlation volumes. RAFT has proven to be highly effective, with an F1-all error of 5.10% on KITTI, which is a 16% error reduction from the best published result, and an end-point-error of 2.855 pixels on Sintel (final pass), which is a 30% error reduction from the best published result. RAFT also has strong cross-dataset generalization and is highly efficient in terms of inference time, training speed, and parameter count. The code for RAFT is available at https://github.com/princeton-vl/RAFT.",1
"Event cameras are paradigm-shifting novel sensors that report asynchronous, per-pixel brightness changes called 'events' with unparalleled low latency. This makes them ideal for high speed, high dynamic range scenes where conventional cameras would fail. Recent work has demonstrated impressive results using Convolutional Neural Networks (CNNs) for video reconstruction and optic flow with events. We present strategies for improving training data for event based CNNs that result in 20-40% boost in performance of existing state-of-the-art (SOTA) video reconstruction networks retrained with our method, and up to 15% for optic flow networks. A challenge in evaluating event based video reconstruction is lack of quality ground truth images in existing datasets. To address this, we present a new High Quality Frames (HQF) dataset, containing events and ground truth frames from a DAVIS240C that are well-exposed and minimally motion-blurred. We evaluate our method on HQF + several existing major event camera datasets.",0
"The sensors known as event cameras are revolutionary in that they report brightness changes on a per-pixel basis, known as ""events"", with exceptionally low latency. This feature makes them well-suited for scenes with high speed and high dynamic range, where traditional cameras would fall short. Recently, researchers have successfully employed Convolutional Neural Networks (CNNs) to reconstruct videos and capture optic flow using event cameras. In this study, we propose techniques to enhance the training data for event-based CNNs, resulting in a 20-40% improvement in the performance of existing state-of-the-art video reconstruction networks and up to 15% for optic flow networks. However, evaluating event-based video reconstruction poses a challenge due to a lack of high-quality ground truth images in existing datasets. To address this issue, we introduce the High Quality Frames (HQF) dataset, featuring well-exposed and minimally motion-blurred events and ground truth frames captured by a DAVIS240C camera. We evaluate our approach using the HQF dataset and several other major event camera datasets.",1
"Voice Activity Detection (VAD) refers to the task of identification of regions of human speech in digital signals such as audio and video. While VAD is a necessary first step in many speech processing systems, it poses challenges when there are high levels of ambient noise during the audio recording. To improve the performance of VAD in such conditions, several methods utilizing the visual information extracted from the region surrounding the mouth/lip region of the speakers' video recording have been proposed. Even though these provide advantages over audio-only methods, they depend on faithful extraction of lip/mouth regions. Motivated by these, a new paradigm for VAD based on the fact that respiration forms the primary source of energy for speech production is proposed. Specifically, an audio-independent VAD technique using the respiration pattern extracted from the speakers' video is developed. The Respiration Pattern is first extracted from the video focusing on the abdominal-thoracic region of a speaker using an optical flow based method. Subsequently, voice activity is detected from the respiration pattern signal using neural sequence-to-sequence prediction models. The efficacy of the proposed method is demonstrated through experiments on a challenging dataset recorded in real acoustic environments and compared with four previous methods based on audio and visual cues.",0
"The task of identifying human speech in digital signals like audio and video is known as Voice Activity Detection (VAD). However, VAD can be difficult when there is a lot of background noise during audio recording. To improve VAD performance in such situations, methods that use visual information from the mouth/lip region of a speaker's video recording have been proposed, although they rely on accurate extraction of lip/mouth regions. To address this issue, a new VAD paradigm based on the fact that respiration is the primary energy source for speech production is suggested. This audio-independent VAD technique uses the respiration pattern extracted from the speaker's video, focusing on the abdominal-thoracic region, and neural sequence-to-sequence prediction models to detect voice activity. Experiments on a challenging dataset recorded in real acoustic environments show that this method performs better than four previous methods based on audio and visual cues.",1
"In this paper, we introduce a novel suspect-and-investigate framework, which can be easily embedded in a drone for automated parking violation detection (PVD). Our proposed framework consists of: 1) SwiftFlow, an efficient and accurate convolutional neural network (CNN) for unsupervised optical flow estimation; 2) Flow-RCNN, a flow-guided CNN for car detection and classification; and 3) an illegally parked car (IPC) candidate investigation module developed based on visual SLAM. The proposed framework was successfully embedded in a drone from ATG Robotics. The experimental results demonstrate that, firstly, our proposed SwiftFlow outperforms all other state-of-the-art unsupervised optical flow estimation approaches in terms of both speed and accuracy; secondly, IPC candidates can be effectively and efficiently detected by our proposed Flow-RCNN, with a better performance than our baseline network, Faster-RCNN; finally, the actual IPCs can be successfully verified by our investigation module after drone re-localization.",0
"In this paper, we present a new approach for detecting parking violations using a drone. Our approach involves a suspect-and-investigate framework that can be easily integrated into a drone. The framework includes three components: SwiftFlow, a convolutional neural network that estimates optical flow with high accuracy and efficiency; Flow-RCNN, a flow-guided CNN that detects and classifies cars; and a module for investigating illegally parked cars based on visual SLAM. We successfully implemented this framework in a drone from ATG Robotics and conducted experiments to evaluate its effectiveness. Our results showed that SwiftFlow outperforms other unsupervised optical flow estimation methods in terms of both speed and accuracy. Flow-RCNN effectively detected IPC candidates with better performance than Faster-RCNN. Finally, our investigation module successfully verified actual IPCs after drone re-localization.",1
"In autonomous driving, monocular sequences contain lots of information. Monocular depth estimation, camera ego-motion estimation and optical flow estimation in consecutive frames are high-profile concerns recently. By analyzing tasks above, pixels in the middle frame are modeled into three parts: the rigid region, the non-rigid region, and the occluded region. In joint unsupervised training of depth and pose, we can segment the occluded region explicitly. The occlusion information is used in unsupervised learning of depth, pose and optical flow, as the image reconstructed by depth-pose and optical flow will be invalid in occluded regions. A less-than-mean mask is designed to further exclude the mismatched pixels interfered with by motion or illumination change in the training of depth and pose networks. This method is also used to exclude some trivial mismatched pixels in the training of the optical flow network. Maximum normalization is proposed for depth smoothness term to restrain depth degradation in textureless regions. In the occluded region, as depth and camera motion can provide more reliable motion estimation, they can be used to instruct unsupervised learning of optical flow. Our experiments in KITTI dataset demonstrate that the model based on three regions, full and explicit segmentation of the occlusion region, the rigid region, and the non-rigid region with corresponding unsupervised losses can improve performance on three tasks significantly. The source code is available at: https://github.com/guangmingw/DOPlearning.",0
"Monocular sequences in autonomous driving contain a wealth of information, with recent high-profile concerns being monocular depth estimation, camera ego-motion estimation, and optical flow estimation in consecutive frames. An analysis of these tasks has led to the modeling of pixels in the middle frame into three distinct regions: the rigid region, the non-rigid region, and the occluded region. By employing joint unsupervised training of depth and pose, the occluded region can be explicitly segmented and used in the unsupervised learning of depth, pose, and optical flow. A less-than-mean mask is designed to further exclude mismatched pixels from the training of the depth and pose networks, while maximum normalization is utilized to prevent depth degradation in textureless regions. In the occluded region, depth and camera motion are used to instruct unsupervised learning of optical flow, resulting in significant improvements in performance on all three tasks. The source code for this method is available at https://github.com/guangmingw/DOPlearning.",1
"We propose a simply method to generate high quality synthetic dataset based on open-source game Minecraft includes rendered image, Depth map, surface normal map, and 6-dof camera trajectory. This dataset has a perfect ground-truth generated by plug-in program, and thanks for the large game's community, there is an extremely large number of 3D open-world environment, users can find suitable scenes for shooting and build data sets through it and they can also build scenes in-game. as such, We don't need to worry about manual over fitting caused by too small datasets. what's more, there is also a shader community which We can use to minimize data bias between rendered images and real-images as little as possible. Last but not least, we now provide three tools to generate the data for depth prediction ,surface normal prediction and visual odometry, user can also develop the plug-in module for other vision task like segmentation or optical flow prediction.",0
"Our proposal offers a straightforward approach for generating synthetic datasets of superior quality using the open-source game Minecraft. This includes rendered images, depth maps, surface normal maps, and 6-dof camera trajectories. The dataset is accompanied by a precise ground-truth, generated by a plug-in program. With a vast community of players and an extensive range of 3D open-world environments, users can easily locate suitable scenes for data generation, or create new ones within the game. Such a feature eliminates the concern of manual overfitting caused by insufficient data. Additionally, we can utilize the shader community to minimize data bias between rendered images and real images. We provide three tools for generating data for depth prediction, surface normal prediction, and visual odometry. Users can also create plug-in modules for other vision tasks, such as segmentation or optical flow prediction.",1
"Activity detection from first-person videos (FPV) captured using a wearable camera is an active research field with potential applications in many sectors, including healthcare, law enforcement, and rehabilitation. State-of-the-art methods use optical flow-based hybrid techniques that rely on features derived from the motion of objects from consecutive frames. In this work, we developed a two-stream network, the \emph{SegCodeNet}, that uses a network branch containing video-streams with color-coded semantic segmentation masks of relevant objects in addition to the original RGB video-stream. We also include a stream-wise attention gating that prioritizes between the two streams and a frame-wise attention module that prioritizes the video frames that contain relevant features. Experiments are conducted on an FPV dataset containing $18$ activity classes in office environments. In comparison to a single-stream network, the proposed two-stream method achieves an absolute improvement of $14.366\%$ and $10.324\%$ for averaged F1 score and accuracy, respectively, when average results are compared for three different frame sizes $224\times224$, $112\times112$, and $64\times64$. The proposed method provides significant performance gains for lower-resolution images with absolute improvements of $17\%$ and $26\%$ in F1 score for input dimensions of $112\times112$ and $64\times64$, respectively. The best performance is achieved for a frame size of $224\times224$ yielding an F1 score and accuracy of $90.176\%$ and $90.799\%$ which outperforms the state-of-the-art Inflated 3D ConvNet (I3D) \cite{carreira2017quo} method by an absolute margin of $4.529\%$ and $2.419\%$, respectively.",0
"The detection of activities in first-person videos captured by wearable cameras is an area of active research with potential applications in various sectors, such as healthcare, law enforcement, and rehabilitation. Current methods rely on optical flow-based hybrid techniques that use features from consecutive frames' object movements. In this study, we introduce the SegCodeNet, a two-stream network that uses a branch with video streams consisting of color-coded semantic segmentation masks of relevant objects and the original RGB video stream. We also incorporate a stream-wise attention gating and a frame-wise attention module to prioritize relevant features and frames. We evaluate the SegCodeNet on an FPV dataset, including 18 activity classes in office environments. Our proposed method outperforms the single-stream network with absolute improvements of 14.366% and 10.324% for averaged F1 score and accuracy, respectively. We also obtain significant performance gains for lower-resolution images with absolute improvements of 17% and 26% in F1 score for input dimensions of 112x112 and 64x64, respectively. The SegCodeNet's optimal performance is achieved with a frame size of 224x224, yielding an F1 score and accuracy of 90.176% and 90.799%, respectively, surpassing the state-of-the-art Inflated 3D ConvNet (I3D) method by an absolute margin of 4.529% and 2.419%, respectively.",1
"Crowd flow describes the elementary group behavior of crowds. Understanding the dynamics behind these movements can help to identify various abnormalities in crowds. However, developing a crowd model describing these flows is a challenging task. In this paper, a physics-based model is proposed to describe the movements in dense crowds. The crowd model is based on active Langevin equation where the motion points are assumed to be similar to active colloidal particles in fluids. The model is further augmented with computer-vision techniques to segment both linear and non-linear motion flows in a dense crowd. The evaluation of the active Langevin equation-based crowd segmentation has been done on publicly available crowd videos and on our own videos. The proposed method is able to segment the flow with lesser optical flow error and better accuracy in comparison to existing state-of-the-art methods.",0
"The behavior of crowds, referred to as crowd flow, can be analyzed to detect abnormalities within a group. However, developing a model to describe this behavior is difficult. This paper suggests a physics-based model, utilizing an active Langevin equation, which assumes crowd movements to be similar to active colloidal particles in fluids. Computer-vision techniques are also used to segment both linear and non-linear motion flows in dense crowds. The proposed method was evaluated on both publicly available and personal crowd videos, and it proved to be more effective than existing methods, with improved accuracy and less optical flow error.",1
"Personal robots and driverless cars need to be able to operate in novel environments and thus quickly and efficiently learn to recognise new object classes. We address this problem by considering the task of video object segmentation. Previous accurate methods for this task finetune a model using the first annotated frame, and/or use additional inputs such as optical flow and complex post-processing. In contrast, we develop a fast, causal algorithm that requires no finetuning, auxiliary inputs or post-processing, and segments a variable number of objects in a single forward-pass. We represent an object with clusters, or ""visual words"", in the embedding space, which correspond to object parts in the image space. This allows us to robustly match to the reference objects throughout the video, because although the global appearance of an object changes as it undergoes occlusions and deformations, the appearance of more local parts may stay consistent. We learn these visual words in an unsupervised manner, using meta-learning to ensure that our training objective matches our inference procedure. We achieve comparable accuracy to finetuning based methods (whilst being 1 to 2 orders of magnitude faster), and state-of-the-art in terms of speed/accuracy trade-offs on four video segmentation datasets. Code is available at https://github.com/harkiratbehl/MetaVOS.",0
"The ability for personal robots and driverless cars to function effectively in new surroundings and identify unfamiliar objects is crucial. Our solution to this issue involves video object segmentation, a task that previous methods have achieved accuracy through finetuning with the first annotated frame and utilizing additional inputs such as optical flow and complex post-processing. Our approach, however, is different in that we have developed a fast, causal algorithm that requires no finetuning, auxiliary inputs, or post-processing. Our algorithm can segment a variable number of objects in a single forward-pass. We represent objects with clusters or ""visual words"" in the embedding space, which correspond to object parts in the image space. This allows us to accurately match reference objects throughout the video, as the appearance of more local parts may remain consistent despite global changes in appearance due to occlusions and deformations. We use meta-learning to learn these visual words in an unsupervised manner, ensuring that our training objective aligns with our inference procedure. Our method achieves comparable accuracy to finetuning-based methods but is 1 to 2 orders of magnitude faster. Additionally, we have achieved state-of-the-art speed/accuracy trade-offs on four video segmentation datasets. Our code is available at https://github.com/harkiratbehl/MetaVOS.",1
"Nowadays, digital facial content manipulation has become ubiquitous and realistic with the success of generative adversarial networks (GANs), making face recognition (FR) systems suffer from unprecedented security concerns. In this paper, we investigate and introduce a new type of adversarial attack to evade FR systems by manipulating facial content, called \textbf{\underline{a}dversarial \underline{mor}phing \underline{a}ttack} (a.k.a. Amora). In contrast to adversarial noise attack that perturbs pixel intensity values by adding human-imperceptible noise, our proposed adversarial morphing attack works at the semantic level that perturbs pixels spatially in a coherent manner. To tackle the black-box attack problem, we devise a simple yet effective joint dictionary learning pipeline to obtain a proprietary optical flow field for each attack. Our extensive evaluation on two popular FR systems demonstrates the effectiveness of our adversarial morphing attack at various levels of morphing intensity with smiling facial expression manipulations. Both open-set and closed-set experimental results indicate that a novel black-box adversarial attack based on local deformation is possible, and is vastly different from additive noise attacks. The findings of this work potentially pave a new research direction towards a more thorough understanding and investigation of image-based adversarial attacks and defenses.",0
"With the advancement of generative adversarial networks (GANs), digital facial content manipulation has become more realistic and prevalent, leading to significant security concerns for face recognition (FR) systems. This study explores a new type of adversarial attack called adversarial morphing attack (Amora) that manipulates facial content to evade FR systems. Unlike traditional adversarial noise attacks that add human-imperceptible noise to disturb pixel intensity values, our proposed attack perturbs pixels spatially at the semantic level in a coherent manner. We also devise a joint dictionary learning pipeline to tackle the black-box attack problem by obtaining a proprietary optical flow field for each attack. Our experiments demonstrate the effectiveness of Amora at different levels of morphing intensity with smiling facial expression manipulations on two popular FR systems. Our results suggest the possibility of a novel black-box adversarial attack based on local deformation, which differs significantly from additive noise attacks. These findings pave the way for further research to enhance our understanding and defenses against image-based adversarial attacks.",1
"We propose a light-weight variational framework for online tracking of object segmentations in videos based on optical flow and image boundaries. While high-end computer vision methods on this task rely on sequence specific training of dedicated CNN architectures, we show the potential of a variational model, based on generic video information from motion and color. Such cues are usually required for tasks such as robot navigation or grasp estimation. We leverage them directly for video object segmentation and thus provide accurate segmentations at potentially very low extra cost. Our simple method can provide competitive results compared to the costly CNN-based methods with parameter tuning. Furthermore, we show that our approach can be combined with state-of-the-art CNN-based segmentations in order to improve over their respective results. We evaluate our method on the datasets DAVIS 16,17 and SegTrack v2.",0
"A new method for tracking object segmentations in videos is proposed using a lightweight variational framework that utilizes optical flow and image boundaries. Unlike other computer vision methods that require specific training of CNN architectures, this method employs generic video information from motion and color, which are commonly used in robot navigation or grasp estimation. By directly leveraging these cues, accurate segmentations can be achieved at a lower cost compared to CNN-based methods. The proposed approach can also be combined with CNN-based segmentations to further improve accuracy. The method is evaluated on DAVIS 16, 17, and SegTrack v2 datasets and shows competitive results compared to costly CNN-based methods.",1
"We systematically compare and analyze a set of key components in unsupervised optical flow to identify which photometric loss, occlusion handling, and smoothness regularization is most effective. Alongside this investigation we construct a number of novel improvements to unsupervised flow models, such as cost volume normalization, stopping the gradient at the occlusion mask, encouraging smoothness before upsampling the flow field, and continual self-supervision with image resizing. By combining the results of our investigation with our improved model components, we are able to present a new unsupervised flow technique that significantly outperforms the previous unsupervised state-of-the-art and performs on par with supervised FlowNet2 on the KITTI 2015 dataset, while also being significantly simpler than related approaches.",0
"Our study involves a methodical comparison and evaluation of various components in unsupervised optical flow. Our aim is to determine the most effective photometric loss, occlusion handling, and smoothness regularization techniques. Additionally, we have developed novel enhancements to unsupervised flow models, including cost volume normalization, gradient termination at the occlusion mask, prioritizing smoothness before flow field upsampling, and continuous self-supervision through image resizing. Through the integration of our research findings and improved model components, we have introduced a new unsupervised flow technique that surpasses the previous unsupervised state-of-the-art and operates at the same level as supervised FlowNet2 on the KITTI 2015 dataset. Our approach is significantly simpler than other related methods.",1
"This paper presents a novel end-to-end dynamic time-lapse video generation framework, named DTVNet, to generate diversified time-lapse videos from a single landscape image, which are conditioned on normalized motion vectors. The proposed DTVNet consists of two submodules: \emph{Optical Flow Encoder} (OFE) and \emph{Dynamic Video Generator} (DVG). The OFE maps a sequence of optical flow maps to a \emph{normalized motion vector} that encodes the motion information inside the generated video. The DVG contains motion and content streams that learn from the motion vector and the single image respectively, as well as an encoder and a decoder to learn shared content features and construct video frames with corresponding motion respectively. Specifically, the \emph{motion stream} introduces multiple \emph{adaptive instance normalization} (AdaIN) layers to integrate multi-level motion information that are processed by linear layers. In the testing stage, videos with the same content but various motion information can be generated by different \emph{normalized motion vectors} based on only one input image. We further conduct experiments on Sky Time-lapse dataset, and the results demonstrate the superiority of our approach over the state-of-the-art methods for generating high-quality and dynamic videos, as well as the variety for generating videos with various motion information.",0
"The article introduces a new framework, named DTVNet, which generates a variety of time-lapse videos from a single landscape image. The videos are produced based on normalized motion vectors. The framework comprises two submodules, the Optical Flow Encoder (OFE) and the Dynamic Video Generator (DVG). The OFE maps a sequence of optical flow maps to a normalized motion vector, and the DVG employs motion and content streams to learn from the motion vector and single image, respectively. The DVG also includes an encoder and a decoder to learn shared content features and generate video frames with corresponding motion. The motion stream introduces multiple adaptive instance normalization (AdaIN) layers to integrate multi-level motion information processed by linear layers. In testing, the framework generates videos with various motion information by employing different normalized motion vectors based on a single input image. The study further investigates the performance of the proposed approach on the Sky Time-lapse dataset, and the results demonstrate its excellence over existing methods in generating high-quality and dynamic videos with diverse motion information.",1
"Depth map estimation is a crucial task in computer vision, and new approaches have recently emerged taking advantage of light fields, as this new imaging modality captures much more information about the angular direction of light rays compared to common approaches based on stereoscopic images or multi-view. In this paper, we propose a novel depth estimation method from light fields based on existing optical flow estimation methods. The optical flow estimator is applied on a sequence of images taken along an angular dimension of the light field, which produces several disparity map estimates. Considering both accuracy and efficiency, we choose the feature flow method as our optical flow estimator. Thanks to its spatio-temporal edge-aware filtering properties, the different disparity map estimates that we obtain are very consistent, which allows a fast and simple aggregation step to create a single disparity map, which can then converted into a depth map. Since the disparity map estimates are consistent, we can also create a depth map from each disparity estimate, and then aggregate the different depth maps in the 3D space to create a single dense depth map.",0
"Recently, new techniques for depth map estimation in computer vision have emerged that utilize light fields. This imaging modality captures more information about the angular direction of light rays than traditional methods, such as stereoscopic images or multi-view. In this study, we propose a novel depth estimation method using existing optical flow estimation techniques. Our method applies an optical flow estimator to a sequence of images taken along an angular dimension of the light field, producing several disparity map estimates. We use the feature flow method as our optical flow estimator due to its spatio-temporal edge-aware filtering properties. The resulting disparity map estimates are highly consistent, facilitating a fast and simple aggregation step to create a single disparity map that can be converted into a depth map. Since the disparity map estimates are consistent, we can also create a depth map from each disparity estimate and aggregate the different depth maps in 3D space, resulting in a single dense depth map.",1
"Our objective is to transform a video into a set of discrete audio-visual objects using self-supervised learning. To this end, we introduce a model that uses attention to localize and group sound sources, and optical flow to aggregate information over time. We demonstrate the effectiveness of the audio-visual object embeddings that our model learns by using them for four downstream speech-oriented tasks: (a) multi-speaker sound source separation, (b) localizing and tracking speakers, (c) correcting misaligned audio-visual data, and (d) active speaker detection. Using our representation, these tasks can be solved entirely by training on unlabeled video, without the aid of object detectors. We also demonstrate the generality of our method by applying it to non-human speakers, including cartoons and puppets.Our model significantly outperforms other self-supervised approaches, and obtains performance competitive with methods that use supervised face detection.",0
"Our goal is to utilize self-supervised learning to convert video footage into distinct audio-visual objects. To accomplish this, we have developed a model that incorporates attention to pinpoint and group sound sources, as well as optical flow to gather information over time. We have confirmed the efficacy of our model's audio-visual object embeddings by utilizing them in four speech-oriented tasks: (a) separating sound from multiple speakers, (b) locating and tracking speakers, (c) rectifying misaligned audio-visual data, and (d) detecting active speakers. The use of unlabeled video alone, without object detectors, can solve these tasks by utilizing our representation. Additionally, we have demonstrated the versatility of our approach by applying it to non-human speakers, including puppets and cartoons. Our model outperforms other self-supervised techniques and delivers competitive results when compared to supervised face detection methods.",1
"To address the problem of training on small datasets for action recognition tasks, most prior works are either based on a large number of training samples or require pre-trained models transferred from other large datasets to tackle overfitting problems. However, it limits the research within organizations that have strong computational abilities. In this work, we try to propose a data-efficient framework that can train the model from scratch on small datasets while achieving promising results. Specifically, by introducing a 3D central difference convolution operation, we proposed a novel C3D neural network-based two-stream (Rank Pooling RGB and Optical Flow) framework for the task. The method is validated on the action recognition track of the ECCV 2020 VIPriors challenges and got the 2nd place (88.31%). It is proved that our method can achieve a promising result even without a pre-trained model on large scale datasets. The code will be released soon.",0
"Previous methods for addressing the issue of training on small datasets for action recognition tasks have either used a large number of training samples or relied on pre-trained models transferred from other large datasets to avoid overfitting. However, this approach limits the scope of research to organizations with strong computational capabilities. In this study, we propose a data-efficient framework that trains a model from scratch on small datasets, while still achieving promising results. Our approach involves introducing a 3D central difference convolution operation and implementing a novel C3D neural network-based two-stream framework (Rank Pooling RGB and Optical Flow) for the task. We validate our method on the action recognition track of the ECCV 2020 VIPriors challenges and achieve 2nd place (88.31%), demonstrating that our method can produce satisfactory results without relying on pre-trained models from large scale datasets. The code for our approach will be made available shortly.",1
"Currently, the safety of people has become a very important problem in different places including subway station, universities, colleges, airport, shopping mall and square, city squares. Therefore, considering intelligence event detection systems is more and urgently required. The event detection method is developed to identify abnormal behavior intelligently, so public can take action as soon as possible to prevent unwanted activities. The problem is very challenging due to high crowd density in different areas. One of these issues is occlusion due to which individual tracking and analysis becomes impossible as shown in Fig. 1. Secondly, more challenging is the proper representation of individual behavior in the crowd. We consider a novel method to deal with these challenges. Considering the challenge of tracking, we partition complete frame into smaller patches, and extract motion pattern to demonstrate the motion in each individual patch. For this purpose, our work takes into account KLT corners as consolidated features to describe moving regions and track these features by considering optical flow method. To embed motion patterns, we develop and consider the distribution of all motion information in a patch as Gaussian distribution, and formulate parameters of Gaussian model as our motion pattern descriptor.",0
"The safety of individuals has become a pressing concern in various locations such as subway stations, universities, airports, shopping malls, squares, and city centers. As a result, the implementation of intelligent event detection systems is increasingly necessary and urgent. This method detects abnormal behavior in order to enable prompt action and prevent undesirable activities, which is particularly challenging due to high crowd density. One such challenge is occlusion, as shown in Figure 1, which hinders individual tracking and analysis. Additionally, properly representing individual behavior in a crowd is also difficult. To address these issues, we propose a novel method. To overcome the tracking challenge, we divide the entire frame into smaller patches and extract motion patterns to demonstrate the motion in each patch. We utilize KLT corners as consolidated features to describe moving regions and track them using the optical flow method. To incorporate motion patterns, we develop a Gaussian distribution representing the distribution of all motion information in a patch, and use the parameters of this Gaussian model as our motion pattern descriptor.",1
"Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of microseconds), very high dynamic range (140 dB vs. 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.",0
"Event cameras are sensors that have been inspired by biology and are different from traditional frame cameras. They measure brightness changes asynchronously on a per-pixel basis and output events that encode the location, time, and sign of these changes instead of capturing images at a fixed rate. Compared to traditional cameras, event cameras offer several benefits, including high temporal resolution, very high dynamic range, low power consumption, and high pixel bandwidth, which reduces motion blur. As a result, they have great potential for robotics and computer vision in scenarios that are challenging for traditional cameras, such as low-latency, high speed, and high dynamic range. However, unconventional methods are necessary to process the output of these sensors and unlock their potential. This paper provides an extensive overview of the field of event-based vision, focusing on applications and algorithms developed to take advantage of the outstanding properties of event cameras. The paper covers the working principle of event cameras, available sensors, and tasks they have been used for, from low-level vision to high-level vision. It also discusses the techniques developed to process events, including learning-based techniques and specialized processors for these sensors. Additionally, the paper highlights the challenges that remain to be addressed and the opportunities that lie ahead as we seek to develop a more efficient, bio-inspired way for machines to perceive and interact with the world.",1
"Efficiently modeling dynamic motion information in videos is crucial for action recognition task. Most state-of-the-art methods heavily rely on dense optical flow as motion representation. Although combining optical flow with RGB frames as input can achieve excellent recognition performance, the optical flow extraction is very time-consuming. This undoubtably will count against real-time action recognition. In this paper, we shed light on fast action recognition by lifting the reliance on optical flow. Our motivation lies in the observation that small displacements of motion boundaries are the most critical ingredients for distinguishing actions, so we design a novel motion cue called Persistence of Appearance (PA). In contrast to optical flow, our PA focuses more on distilling the motion information at boundaries. Also, it is more efficient by only accumulating pixel-wise differences in feature space, instead of using exhaustive patch-wise search of all the possible motion vectors. Our PA is over 1000x faster (8196fps vs. 8fps) than conventional optical flow in terms of motion modeling speed. To further aggregate the short-term dynamics in PA to long-term dynamics, we also devise a global temporal fusion strategy called Various-timescale Aggregation Pooling (VAP) that can adaptively model long-range temporal relationships across various timescales. We finally incorporate the proposed PA and VAP to form a unified framework called Persistent Appearance Network (PAN) with strong temporal modeling ability. Extensive experiments on six challenging action recognition benchmarks verify that our PAN outperforms recent state-of-the-art methods at low FLOPs. Codes and models are available at: https://github.com/zhang-can/PAN-PyTorch.",0
"For the task of action recognition, it is essential to effectively model dynamic motion information within videos. While many current approaches rely heavily on dense optical flow as a means of motion representation, the process of extracting optical flow is time-consuming and may hinder real-time recognition. In this paper, we propose an alternative approach to fast action recognition by introducing a novel motion cue, Persistence of Appearance (PA), which focuses on distilling motion information at boundaries and is much more efficient than optical flow. We also introduce a global temporal fusion strategy, Various-timescale Aggregation Pooling (VAP), which can adaptively model long-range temporal relationships. By combining PA and VAP, we present the Persistent Appearance Network (PAN), a unified framework with strong temporal modeling ability. Our experiments show that PAN outperforms recent state-of-the-art methods at low FLOPs. Code and models can be found at: https://github.com/zhang-can/PAN-PyTorch.",1
"Situational awareness and Indoor location tracking for firefighters is one of the tasks with paramount importance in search and rescue operations. For Indoor Positioning systems (IPS), GPS is not the best possible solution. There are few other techniques like dead reckoning, Wifi and bluetooth based triangulation, Structure from Motion (SFM) based scene reconstruction for Indoor positioning system. However due to high temperatures, the rapidly changing environment of fires, and low parallax in the thermal images, these techniques are not suitable for relaying the necessary information in a fire fighting environment needed to increase situational awareness in real time. In fire fighting environments, thermal imaging cameras are used due to smoke and low visibility hence obtaining relative orientation from the vanishing point estimation is very difficult. The following technique that is the content of this research implements a novel optical flow based video compass for orientation estimation and fused IMU data based activity recognition for IPS. This technique helps first responders to go into unprepared, unknown environments and still maintain situational awareness like the orientation and, position of the victim fire fighters.",0
"In search and rescue operations, situational awareness and indoor location tracking are crucial tasks for firefighters. Although GPS is not an ideal solution for Indoor Positioning Systems (IPS), there are alternative techniques such as dead reckoning, triangulation using WiFi and Bluetooth, and Structure from Motion (SFM) based scene reconstruction. However, these methods are not suitable for real-time information relay in fire-fighting environments due to high temperatures, rapidly changing conditions, and low parallax in thermal images. Thermal imaging cameras are commonly used in such environments, but obtaining relative orientation from vanishing point estimation is challenging. To address this, a novel optical flow-based video compass for orientation estimation and fused IMU data-based activity recognition for IPS have been developed. This technique enables first responders to maintain situational awareness, including the orientation and position of firefighters and victims, even in unprepared and unknown environments.",1
"The objective of this paper is self-supervised learning from video, in particular for representations for action recognition. We make the following contributions: (i) We propose a new architecture and learning framework Memory-augmented Dense Predictive Coding (MemDPC) for the task. It is trained with a predictive attention mechanism over the set of compressed memories, such that any future states can always be constructed by a convex combination of the condense representations, allowing to make multiple hypotheses efficiently. (ii) We investigate visual-only self-supervised video representation learning from RGB frames, or from unsupervised optical flow, or both. (iii) We thoroughly evaluate the quality of learnt representation on four different downstream tasks: action recognition, video retrieval, learning with scarce annotations, and unintentional action classification. In all cases, we demonstrate state-of-the-art or comparable performance over other approaches with orders of magnitude fewer training data.",0
"The main purpose of this article is to focus on self-supervised learning from video, specifically concentrating on developing representations for action recognition. Our contributions to this field are as follows: firstly, we introduce a novel architecture and learning framework called Memory-augmented Dense Predictive Coding (MemDPC) that is trained with a predictive attention mechanism over compressed memories. By doing so, we are able to construct future states using a convex combination of condensed representations, thus allowing for multiple hypotheses to be made efficiently. Secondly, we explore the use of visual-only self-supervised video representation learning from RGB frames, unsupervised optical flow, or both. Lastly, we conduct an extensive evaluation of the quality of the learnt representation in four different downstream tasks, including action recognition, video retrieval, learning with scarce annotations, and unintentional action classification. In all cases, we demonstrate state-of-the-art or comparable performance over other approaches, despite requiring orders of magnitude fewer training data.",1
"Human action recognition is regarded as a key cornerstone in domains such as surveillance or video understanding. Despite recent progress in the development of end-to-end solutions for video-based action recognition, achieving state-of-the-art performance still requires using auxiliary hand-crafted motion representations, e.g., optical flow, which are usually computationally demanding. In this work, we propose to use residual frames (i.e., differences between adjacent RGB frames) as an alternative ""lightweight"" motion representation, which carries salient motion information and is computationally efficient. In addition, we develop a new pseudo-3D convolution module which decouples 3D convolution into 2D and 1D convolution. The proposed module exploits residual information in the feature space to better structure motions, and is equipped with a self-attention mechanism that assists to recalibrate the appearance and motion features. Empirical results confirm the efficiency and effectiveness of residual frames as well as the proposed pseudo-3D convolution module.",0
"The recognition of human actions plays a crucial role in various fields such as video comprehension and surveillance. Although recent advancements in creating end-to-end solutions for action recognition using videos have been made, the use of auxiliary motion representations like optical flow, which can be computationally intensive, is still necessary to achieve top-notch performance. This paper, however, suggests an alternative ""lightweight"" motion representation using residual frames (i.e., the differences between adjacent RGB frames) that are computationally efficient while still carrying important motion information. Additionally, the paper introduces a new pseudo-3D convolution module that decomposes 3D convolution into 2D and 1D convolution. This new module utilizes residual information in the feature space to better organize motions and features a self-attention mechanism that recalibrates appearance and motion features. Experimental results validate the effectiveness and efficiency of residual frames and the proposed pseudo-3D convolution module.",1
"Dynamic Vision Sensors (DVSs) asynchronously stream events in correspondence of pixels subject to brightness changes. Differently from classic vision devices, they produce a sparse representation of the scene. Therefore, to apply standard computer vision algorithms, events need to be integrated into a frame or event-surface. This is usually attained through hand-crafted grids that reconstruct the frame using ad-hoc heuristics. In this paper, we propose Matrix-LSTM, a grid of Long Short-Term Memory (LSTM) cells that efficiently process events and learn end-to-end task-dependent event-surfaces. Compared to existing reconstruction approaches, our learned event-surface shows good flexibility and expressiveness on optical flow estimation on the MVSEC benchmark and it improves the state-of-the-art of event-based object classification on the N-Cars dataset.",0
"Dynamic Vision Sensors (DVSs) capture events asynchronously when there are changes in the brightness of pixels. Unlike traditional vision devices, DVSs produce a sparse representation of the scene. To use common computer vision algorithms, events must be integrated into a frame or event-surface. This is typically accomplished using customized grids that reconstruct the frame using specific rules. In this study, we suggest Matrix-LSTM, a grid of Long Short-Term Memory (LSTM) cells that effectively handles events and can learn event-surfaces that are tailored to specific tasks. Our learned event-surface shows greater flexibility and expressiveness compared to current reconstruction methods. It is demonstrated through optical flow estimation on the MVSEC benchmark, and it improves the state-of-the-art of event-based object classification on the N-Cars dataset.",1
"Particle Image Velocimetry (PIV) is a classical flow estimation problem which is widely considered and utilised, especially as a diagnostic tool in experimental fluid dynamics and the remote sensing of environmental flows. Recently, the development of deep learning based methods has inspired new approaches to tackle the PIV problem. These supervised learning based methods are driven by large volumes of data with ground truth training information. However, it is difficult to collect reliable ground truth data in large-scale, real-world scenarios. Although synthetic datasets can be used as alternatives, the gap between the training set-ups and real-world scenarios limits applicability. We present here what we believe to be the first work which takes an unsupervised learning based approach to tackle PIV problems. The proposed approach is inspired by classic optical flow methods. Instead of using ground truth data, we make use of photometric loss between two consecutive image frames, consistency loss in bidirectional flow estimates and spatial smoothness loss to construct the total unsupervised loss function. The approach shows significant potential and advantages for fluid flow estimation. Results presented here demonstrate that our method outputs competitive results compared with classical PIV methods as well as supervised learning based methods for a broad PIV dataset, and even outperforms these existing approaches in some difficult flow cases. Codes and trained models are available at https://github.com/erizmr/UnLiteFlowNet-PIV.",0
"The Particle Image Velocimetry (PIV) is a widely used flow estimation method for experimental fluid dynamics and environmental flow remote sensing. Recently, deep learning methods have been developed to tackle PIV problems using supervised learning based on large volumes of data with ground truth training information. However, reliable ground truth data collection in real-world scenarios is difficult, and synthetic datasets have limitations in applicability. Therefore, this study proposes an unsupervised learning approach to solve PIV problems. The proposed approach is based on classic optical flow methods and utilizes photometric loss, consistency loss, and spatial smoothness loss to construct the total unsupervised loss function. The approach shows significant potential and advantages for fluid flow estimation, with competitive results compared to classical PIV methods and supervised learning-based methods for a broad PIV dataset. The proposed method even outperforms existing approaches in some difficult flow cases. The codes and trained models are available at https://github.com/erizmr/UnLiteFlowNet-PIV.",1
"Many researches have been carried out for change detection using temporal SAR images. In this paper an algorithm for change detection using SAR videos has been proposed. There are various challenges related to SAR videos such as high level of speckle noise, rotation of SAR image frames of the video around a particular axis due to the circular movement of airborne vehicle, non-uniform back scattering of SAR pulses. Hence conventional change detection algorithms used for optical videos and SAR temporal images cannot be directly utilized for SAR videos. We propose an algorithm which is a combination of optical flow calculation using Lucas Kanade (LK) method and blob detection. The developed method follows a four steps approach: image filtering and enhancement, applying LK method, blob analysis and combining LK method with blob analysis. The performance of the developed approach was tested on SAR videos available on Sandia National Laboratories website and SAR videos generated by a SAR simulator.",0
"Numerous studies have explored change detection using temporal SAR images, but this paper presents a novel algorithm for change detection using SAR videos. SAR videos present unique difficulties, including high speckle noise, circular movement of airborne vehicles causing rotation of image frames, and non-uniform backscattering of SAR pulses. As a result, traditional change detection algorithms for optical videos and SAR temporal images are unsuitable for SAR videos. To overcome this, we propose an algorithm that combines optical flow calculation using the Lucas Kanade (LK) method with blob detection. The method follows a four-step process: image filtering and enhancement, LK method application, blob analysis, and LK method and blob analysis combination. We tested the performance of our approach on SAR videos from the Sandia National Laboratories website and SAR videos generated by a SAR simulator.",1
"Video super-resolution (VSR) aims to utilize multiple low-resolution frames to generate a high-resolution prediction for each frame. In this process, inter- and intra-frames are the key sources for exploiting temporal and spatial information. However, there are a couple of limitations for existing VSR methods. First, optical flow is often used to establish temporal correspondence. But flow estimation itself is error-prone and affects recovery results. Second, similar patterns existing in natural images are rarely exploited for the VSR task. Motivated by these findings, we propose a temporal multi-correspondence aggregation strategy to leverage similar patches across frames, and a cross-scale nonlocal-correspondence aggregation scheme to explore self-similarity of images across scales. Based on these two new modules, we build an effective multi-correspondence aggregation network (MuCAN) for VSR. Our method achieves state-of-the-art results on multiple benchmark datasets. Extensive experiments justify the effectiveness of our method.",0
"The objective of Video super-resolution (VSR) is to generate a high-resolution forecast for each frame by utilizing multiple low-resolution frames. During this process, inter- and intra-frames play a vital role in extracting temporal and spatial information. However, current VSR methods have certain limitations. Firstly, the use of optical flow to establish temporal correspondence is prone to errors and can impact recovery outcomes. Secondly, natural image patterns that could be useful for VSR are not often exploited. To address these issues, we suggest a temporal multi-correspondence aggregation strategy to leverage similar patches across frames, and a cross-scale nonlocal-correspondence aggregation scheme to explore the self-similarity of images across scales. Based on these modules, we created an effective multi-correspondence aggregation network (MuCAN) for VSR which achieved exceptional results on various benchmark datasets. Our method was extensively tested, and the results demonstrate its effectiveness.",1
"Video style transfer techniques inspire many exciting applications on mobile devices. However, their efficiency and stability are still far from satisfactory. To boost the transfer stability across frames, optical flow is widely adopted, despite its high computational complexity, e.g. occupying over 97% inference time. This paper proposes to learn a lightweight video style transfer network via knowledge distillation paradigm. We adopt two teacher networks, one of which takes optical flow during inference while the other does not. The output difference between these two teacher networks highlights the improvements made by optical flow, which is then adopted to distill the target student network. Furthermore, a low-rank distillation loss is employed to stabilize the output of student network by mimicking the rank of input videos. Extensive experiments demonstrate that our student network without an optical flow module is still able to generate stable video and runs much faster than the teacher network.",0
"The use of video style transfer techniques has sparked interest in various mobile applications; however, their efficiency and stability still require improvement. To enhance transfer stability, optical flow has been commonly used despite its high computational complexity and significant inference time. This paper introduces a lightweight video style transfer network developed using knowledge distillation methods. Two teacher networks are employed, one with optical flow and the other without, and the output difference between them highlights the enhancements made by optical flow. This difference is then used to distill the target student network. Additionally, a low-rank distillation loss is utilized to stabilize the output of the student network by imitating the rank of input videos. Our experiments demonstrate that the student network, without an optical flow module, is capable of generating stable video and operates much faster than the teacher network.",1
"Novel view synthesis often needs the paired data from both the source and target views. This paper proposes a view translation model under cVAE-GAN framework without requiring the paired data. We design a conditional deformable module (CDM) which uses the view condition vectors as the filters to convolve the feature maps of the main branch in VAE. It generates several pairs of displacement maps to deform the features, like the 2D optical flows. The results are fed into the deformed feature based normalization module (DFNM), which scales and offsets the main branch feature, given its deformed one as the input from the side branch. Taking the advantage of the CDM and DFNM, the encoder outputs a view-irrelevant posterior, while the decoder takes the code drawn from it to synthesize the reconstructed and the viewtranslated images. To further ensure the disentanglement between the views and other factors, we add adversarial training on the code. The results and ablation studies on MultiPIE and 3D chair datasets validate the effectiveness of the framework in cVAE and the designed module.",0
"The traditional method of novel view synthesis requires data from both the source and target views. However, this paper introduces a view translation model that operates within a cVAE-GAN framework and does not rely on paired data. The model incorporates a conditional deformable module (CDM) which utilizes view condition vectors as filters to convolve feature maps from the main branch in VAE. By generating displacement maps, similar to 2D optical flows, the CDM deforms features and feeds them into the deformed feature-based normalization module (DFNM) which scales and offsets the main branch feature based on its deformed input from the side branch. This approach allows for the encoder to output a view-irrelevant posterior, while the decoder uses the code to synthesize both reconstructed and view-translated images. Additionally, adversarial training is added to the code to ensure disentanglement between the views and other factors. The effectiveness of the framework, including the cVAE and designed module, is validated through results and ablation studies on MultiPIE and 3D chair datasets.",1
"Cost volume is an essential component of recent deep models for optical flow estimation and is usually constructed by calculating the inner product between two feature vectors. However, the standard inner product in the commonly-used cost volume may limit the representation capacity of flow models because it neglects the correlation among different channel dimensions and weighs each dimension equally. To address this issue, we propose a learnable cost volume (LCV) using an elliptical inner product, which generalizes the standard inner product by a positive definite kernel matrix. To guarantee its positive definiteness, we perform spectral decomposition on the kernel matrix and re-parameterize it via the Cayley representation. The proposed LCV is a lightweight module and can be easily plugged into existing models to replace the vanilla cost volume. Experimental results show that the LCV module not only improves the accuracy of state-of-the-art models on standard benchmarks, but also promotes their robustness against illumination change, noises, and adversarial perturbations of the input signals.",0
"Recent deep models for optical flow estimation rely heavily on cost volume, which is typically constructed by taking the inner product of two feature vectors. However, this approach may not fully capture the correlation among different channel dimensions and assigns equal weight to each dimension, potentially limiting the representation capacity of flow models. To overcome this limitation, we propose a learnable cost volume (LCV) that utilizes an elliptical inner product. This method involves a positive definite kernel matrix, which is obtained through spectral decomposition and re-parameterization via the Cayley representation. The LCV is a lightweight module that can be easily integrated into existing models to replace the standard cost volume. Our experiments show that the LCV not only enhances the accuracy of state-of-the-art models on standard benchmarks but also improves their robustness against changes in illumination, noise, and adversarial perturbations.",1
"Recently, researchers in Machine Learning algorithms, Computer Vision scientists, engineers and others, showed a growing interest in 3D simulators as a mean to artificially create experimental settings that are very close to those in the real world. However, most of the existing platforms to interface algorithms with 3D environments are often designed to setup navigation-related experiments, to study physical interactions, or to handle ad-hoc cases that are not thought to be customized, sometimes lacking a strong photorealistic appearance and an easy-to-use software interface. In this paper, we present a novel platform, SAILenv, that is specifically designed to be simple and customizable, and that allows researchers to experiment visual recognition in virtual 3D scenes. A few lines of code are needed to interface every algorithm with the virtual world, and non-3D-graphics experts can easily customize the 3D environment itself, exploiting a collection of photorealistic objects. Our framework yields pixel-level semantic and instance labeling, depth, and, to the best of our knowledge, it is the only one that provides motion-related information directly inherited from the 3D engine. The client-server communication operates at a low level, avoiding the overhead of HTTP-based data exchanges. We perform experiments using a state-of-the-art object detector trained on real-world images, showing that it is able to recognize the photorealistic 3D objects of our environment. The computational burden of the optical flow compares favourably with the estimation performed using modern GPU-based convolutional networks or more classic implementations. We believe that the scientific community will benefit from the easiness and high-quality of our framework to evaluate newly proposed algorithms in their own customized realistic conditions.",0
"In recent times, there has been an increasing interest among Machine Learning algorithm researchers, Computer Vision scientists, engineers and others to utilize 3D simulators as a tool to artificially create experimental settings that closely resemble real-world scenarios. However, most of the existing platforms that enable the interface of algorithms with 3D environments are primarily designed to carry out navigation-related experiments, study physical interactions or handle ad-hoc cases, and lack a strong photorealistic appearance and an easy-to-use software interface. In this paper, we introduce a new platform called SAILenv, which is specifically designed to be simple and customizable, allowing researchers to experiment with visual recognition in virtual 3D scenes. With just a few lines of code, algorithms can be easily interfaced with the virtual world, and non-experts can customize the 3D environment using a range of photorealistic objects. Our framework provides pixel-level semantic and instance labeling, depth, and motion-related information inherited directly from the 3D engine. The client-server communication operates at a low level, eliminating the overhead of HTTP-based data exchanges. We conducted experiments using a state-of-the-art object detector trained on real-world images, demonstrating its ability to recognize photorealistic 3D objects in our environment. The computational burden of the optical flow is competitive with modern GPU-based convolutional networks or more traditional implementations. We believe that the scientific community will benefit greatly from the simplicity and high-quality of our framework, allowing them to evaluate newly proposed algorithms in their own customized realistic conditions.",1
"Motion plays a crucial role in understanding videos and most state-of-the-art neural models for video classification incorporate motion information typically using optical flows extracted by a separate off-the-shelf method. As the frame-by-frame optical flows require heavy computation, incorporating motion information has remained a major computational bottleneck for video understanding. In this work, we replace external and heavy computation of optical flows with internal and light-weight learning of motion features. We propose a trainable neural module, dubbed MotionSqueeze, for effective motion feature extraction. Inserted in the middle of any neural network, it learns to establish correspondences across frames and convert them into motion features, which are readily fed to the next downstream layer for better prediction. We demonstrate that the proposed method provides a significant gain on four standard benchmarks for action recognition with only a small amount of additional cost, outperforming the state of the art on Something-Something-V1&V2 datasets.",0
"Understanding videos requires motion to be taken into account, and current neural models for video classification typically rely on optical flows obtained from a separate method. However, this approach is computationally intensive and has therefore been a challenge in video comprehension. To address this issue, we introduce a new method called MotionSqueeze, which is a trainable neural module that extracts motion features in a more efficient and lightweight manner. By incorporating MotionSqueeze into a neural network, it can learn to identify motion features and improve the accuracy of predictions without requiring heavy computation. Our experiments demonstrate that MotionSqueeze outperforms the state-of-the-art on Something-Something-V1&V2 datasets while only incurring a small additional cost.",1
"Supervised learning in large discriminative models is a mainstay for modern computer vision. Such an approach necessitates investing in large-scale human-annotated datasets for achieving state-of-the-art results. In turn, the efficacy of supervised learning may be limited by the size of the human annotated dataset. This limitation is particularly notable for image segmentation tasks, where the expense of human annotation is especially large, yet large amounts of unlabeled data may exist. In this work, we ask if we may leverage semi-supervised learning in unlabeled video sequences and extra images to improve the performance on urban scene segmentation, simultaneously tackling semantic, instance, and panoptic segmentation. The goal of this work is to avoid the construction of sophisticated, learned architectures specific to label propagation (e.g., patch matching and optical flow). Instead, we simply predict pseudo-labels for the unlabeled data and train subsequent models with both human-annotated and pseudo-labeled data. The procedure is iterated for several times. As a result, our Naive-Student model, trained with such simple yet effective iterative semi-supervised learning, attains state-of-the-art results at all three Cityscapes benchmarks, reaching the performance of 67.8% PQ, 42.6% AP, and 85.2% mIOU on the test set. We view this work as a notable step towards building a simple procedure to harness unlabeled video sequences and extra images to surpass state-of-the-art performance on core computer vision tasks.",0
"Modern computer vision relies heavily on supervised learning in large discriminative models, which requires significant investment in large-scale human-annotated datasets to achieve the best results. However, the effectiveness of supervised learning may be limited by the size of the human annotated dataset, especially in image segmentation tasks where human annotation is expensive. This is where semi-supervised learning can be leveraged to improve performance on urban scene segmentation, addressing semantic, instance, and panoptic segmentation simultaneously. The goal of this work is to avoid the need for complex learned architectures specifically designed for label propagation, but instead to predict pseudo-labels for unlabeled data and train subsequent models using both human-annotated and pseudo-labeled data. This process is repeated several times, resulting in our Naive-Student model achieving state-of-the-art results in all three Cityscapes benchmarks, with a performance of 67.8% PQ, 42.6% AP, and 85.2% mIOU on the test set. This work represents a significant step towards developing a simple procedure for utilizing unlabeled video sequences and extra images to surpass state-of-the-art performance in core computer vision tasks.",1
"Single encoder-decoder methodologies for semantic segmentation are reaching their peak in terms of segmentation quality and efficiency per number of layers. To address these limitations, we propose a new architecture based on a decoder which uses a set of shallow networks for capturing more information content. The new decoder has a new topology of skip connections, namely backward and stacked residual connections. In order to further improve the architecture we introduce a weight function which aims to re-balance classes to increase the attention of the networks to under-represented objects. We carried out an extensive set of experiments that yielded state-of-the-art results for the CamVid, Gatech and Freiburg Forest datasets. Moreover, to further prove the effectiveness of our decoder, we conducted a set of experiments studying the impact of our decoder to state-of-the-art segmentation techniques. Additionally, we present a set of experiments augmenting semantic segmentation with optical flow information, showing that motion clues can boost pure image based semantic segmentation approaches.",0
"Single encoder-decoder techniques for semantic segmentation have reached their maximum potential in terms of segmentation quality and efficiency for each layer. To overcome these limitations, we propose a novel architecture that utilizes a decoder with a set of shallow networks to capture more information content. The new decoder features a unique topology of skip connections, including backward and stacked residual connections. To further enhance the architecture, we introduce a weight function that rebalances classes to increase network attention towards under-represented objects. Our extensive experiments resulted in state-of-the-art outcomes for the CamVid, Gatech, and Freiburg Forest datasets. We also conducted experiments to demonstrate the effectiveness of our decoder on current segmentation techniques. Additionally, we conducted experiments that increased semantic segmentation with optical flow information, demonstrating that motion clues can improve image-based semantic segmentation.",1
"In this work we review the coarse-to-fine spatial feature pyramid concept, which is used in state-of-the-art optical flow estimation networks to make exploration of the pixel flow search space computationally tractable and efficient. Within an individual pyramid level, we improve the cost volume construction process by departing from a warping- to a sampling-based strategy, which avoids ghosting and hence enables us to better preserve fine flow details. We further amplify the positive effects through a level-specific, loss max-pooling strategy that adaptively shifts the focus of the learning process on under-performing predictions. Our second contribution revises the gradient flow across pyramid levels. The typical operations performed at each pyramid level can lead to noisy, or even contradicting gradients across levels. We show and discuss how properly blocking some of these gradient components leads to improved convergence and ultimately better performance. Finally, we introduce a distillation concept to counteract the issue of catastrophic forgetting and thus preserving knowledge over models sequentially trained on multiple datasets. Our findings are conceptually simple and easy to implement, yet result in compelling improvements on relevant error measures that we demonstrate via exhaustive ablations on datasets like Flying Chairs2, Flying Things, Sintel and KITTI. We establish new state-of-the-art results on the challenging Sintel and KITTI 2012 test datasets, and even show the portability of our findings to different optical flow and depth from stereo approaches.",0
"This work explores the concept of coarse-to-fine spatial feature pyramids, which are utilized in modern optical flow estimation networks to improve computational efficiency and make the pixel flow search space more manageable. We propose a sampling-based strategy for constructing cost volumes within individual pyramid levels, which avoids ghosting and better preserves fine flow details. Additionally, we introduce a level-specific, loss max-pooling strategy to adaptively shift the focus of the learning process on predictions that under-perform. Our second contribution is a revision of the gradient flow across pyramid levels. We demonstrate how blocking some gradient components can lead to improved convergence and ultimately better performance. Lastly, we introduce a distillation concept to preserve knowledge over models trained on multiple datasets. Our findings are straightforward to implement and result in significant improvements on error measures. We establish new state-of-the-art results on the Sintel and KITTI 2012 test datasets and demonstrate that our findings can be applied to different optical flow and depth from stereo approaches.",1
"Deep learning approaches have achieved great success in addressing the problem of optical flow estimation. The keys to success lie in the use of cost volume and coarse-to-fine flow inference. However, the matching problem becomes ill-posed when partially occluded or homogeneous regions exist in images. This causes a cost volume to contain outliers and affects the flow decoding from it. Besides, the coarse-to-fine flow inference demands an accurate flow initialization. Ambiguous correspondence yields erroneous flow fields and affects the flow inferences in subsequent levels. In this paper, we introduce LiteFlowNet3, a deep network consisting of two specialized modules, to address the above challenges. (1) We ameliorate the issue of outliers in the cost volume by amending each cost vector through an adaptive modulation prior to the flow decoding. (2) We further improve the flow accuracy by exploring local flow consistency. To this end, each inaccurate optical flow is replaced with an accurate one from a nearby position through a novel warping of the flow field. LiteFlowNet3 not only achieves promising results on public benchmarks but also has a small model size and a fast runtime.",0
"The problem of optical flow estimation has been effectively tackled by deep learning methods, primarily due to the use of cost volume and coarse-to-fine flow inference techniques. However, the presence of partially occluded or homogeneous regions in images makes the matching problem ill-posed, leading to outliers in the cost volume and inaccurate flow decoding. Additionally, accurate flow initialization is crucial for coarse-to-fine flow inference, and erroneous correspondence can lead to erroneous flow fields in subsequent levels. In this paper, we introduce LiteFlowNet3, a specialized deep network comprising two modules that address these challenges. Firstly, we address the issue of outliers by implementing adaptive modulation prior to flow decoding. Secondly, we enhance flow accuracy by exploring local flow consistency through a novel warping of the flow field. LiteFlowNet3 delivers promising results on public benchmarks while maintaining a small model size and fast runtime.",1
"This paper considers the generic problem of dense alignment between two images, whether they be two frames of a video, two widely different views of a scene, two paintings depicting similar content, etc. Whereas each such task is typically addressed with a domain-specific solution, we show that a simple unsupervised approach performs surprisingly well across a range of tasks. Our main insight is that parametric and non-parametric alignment methods have complementary strengths. We propose a two-stage process: first, a feature-based parametric coarse alignment using one or more homographies, followed by non-parametric fine pixel-wise alignment. Coarse alignment is performed using RANSAC on off-the-shelf deep features. Fine alignment is learned in an unsupervised way by a deep network which optimizes a standard structural similarity metric (SSIM) between the two images, plus cycle-consistency. Despite its simplicity, our method shows competitive results on a range of tasks and datasets, including unsupervised optical flow on KITTI, dense correspondences on Hpatches, two-view geometry estimation on YFCC100M, localization on Aachen Day-Night, and, for the first time, fine alignment of artworks on the Brughel dataset. Our code and data are available at http://imagine.enpc.fr/~shenx/RANSAC-Flow/",0
"In this paper, the problem of dense alignment between two images is explored, regardless of whether they are frames from a video, vastly different views of a scene, or paintings with similar content. Typically, each of these tasks is tackled with a specialized solution, but the authors demonstrate that a simple unsupervised approach works surprisingly well across a variety of tasks. The authors propose a two-stage process that combines the strengths of parametric and non-parametric alignment methods. The first stage involves feature-based parametric coarse alignment using homographies, while the second stage involves non-parametric fine pixel-wise alignment. Coarse alignment is performed with RANSAC on pre-existing deep features, while fine alignment is learned in an unsupervised manner using a deep network that optimizes a standard structural similarity metric and cycle-consistency. Despite its simplicity, the method performs competitively on a range of tasks and datasets, including optical flow, dense correspondences, two-view geometry estimation, localization, and fine alignment of artworks. Code and data are available at http://imagine.enpc.fr/~shenx/RANSAC-Flow/.",1
"For semantic segmentation, most existing real-time deep models trained with each frame independently may produce inconsistent results for a video sequence. Advanced methods take into considerations the correlations in the video sequence, e.g., by propagating the results to the neighboring frames using optical flow, or extracting the frame representations with other frames, which may lead to inaccurate results or unbalanced latency. In this work, we process efficient semantic video segmentation in a per-frame fashion during the inference process. Different from previous per-frame models, we explicitly consider the temporal consistency among frames as extra constraints during the training process and embed the temporal consistency into the segmentation network. Therefore, in the inference process, we can process each frame independently with no latency, and improve the temporal consistency with no extra computational cost and post-processing. We employ compact models for real-time execution. To narrow the performance gap between compact models and large models, new knowledge distillation methods are designed. Our results outperform previous keyframe based methods with a better trade-off between the accuracy and the inference speed on popular benchmarks, including the Cityscapes and Camvid. The temporal consistency is also improved compared with corresponding baselines which are trained with each frame independently. Code is available at: https://tinyurl.com/segment-video",0
"When it comes to semantic segmentation, real-time deep models that are trained independently for each frame can often generate inconsistent results for a video sequence. While some advanced methods try to account for these inconsistencies by considering correlations in the video sequence, such as propagating results to neighboring frames using optical flow or extracting frame representations with other frames, these methods can lead to inaccurate results or imbalanced latency. In this study, we propose a per-frame approach for efficient semantic video segmentation during the inference process. Unlike previous per-frame models, we incorporate temporal consistency among frames as additional constraints during the training process and embed it into the segmentation network. This allows us to process each frame independently with no latency during inference, improving the temporal consistency without any extra computational cost or post-processing. We use compact models for real-time execution and design new knowledge distillation methods to narrow the performance gap between compact models and larger ones. Our results outperform previous keyframe-based methods, achieving a better trade-off between accuracy and inference speed on popular benchmarks such as Cityscapes and Camvid. Additionally, compared to corresponding baselines trained independently for each frame, our proposed method improves temporal consistency. Code for our approach is available at: https://tinyurl.com/segment-video.",1
"We present a novel deep learning architecture for probabilistic future prediction from video. We predict the future semantics, geometry and motion of complex real-world urban scenes and use this representation to control an autonomous vehicle. This work is the first to jointly predict ego-motion, static scene, and the motion of dynamic agents in a probabilistic manner, which allows sampling consistent, highly probable futures from a compact latent space. Our model learns a representation from RGB video with a spatio-temporal convolutional module. The learned representation can be explicitly decoded to future semantic segmentation, depth, and optical flow, in addition to being an input to a learnt driving policy. To model the stochasticity of the future, we introduce a conditional variational approach which minimises the divergence between the present distribution (what could happen given what we have seen) and the future distribution (what we observe actually happens). During inference, diverse futures are generated by sampling from the present distribution.",0
"Our team has developed a new type of deep learning architecture that can predict the future of real-world urban scenes using videos. We can anticipate the future semantics, geometry, and motion within these scenes, which can then be used to control autonomous vehicles. What sets our work apart is that we are the first to use a probabilistic approach to jointly predict ego-motion, static scenes, and the movements of dynamic agents. This allows us to sample highly probable futures from a compact latent space. Our model uses a spatio-temporal convolutional module to learn a representation from RGB video. This representation can be decoded to future semantic segmentation, depth, and optical flow, and can be used as an input for a driving policy. To account for the stochastic nature of the future, we have introduced a conditional variational approach that minimizes the divergence between the present and future distributions. During inference, we generate diverse futures by sampling from the present distribution.",1
"The use of hand gestures can be a useful tool for many applications in the human-computer interaction community. In a broad range of areas hand gesture techniques can be applied specifically in sign language recognition, robotic surgery, etc. In the process of hand gesture recognition, proper detection, and tracking of the moving hand become challenging due to the varied shape and size of the hand. Here the objective is to track the movement of the hand irrespective of the shape, size, and color of the hand. And, for this, a motion template guided by optical flow (OFMT) is proposed. OFMT is a compact representation of the motion information of a gesture encoded into a single image. In the experimentation, different datasets using bare hand with an open palm, and folded palm wearing green-glove are used, and in both cases, we could generate the OFMT images with equal precision. Recently, deep network-based techniques have shown impressive improvements as compared to conventional hand-crafted feature-based techniques. Moreover, in the literature, it is seen that the use of different streams with informative input data helps to increase the performance in the recognition accuracy. This work basically proposes a two-stream fusion model for hand gesture recognition and a compact yet efficient motion template based on optical flow. Specifically, the two-stream network consists of two layers: a 3D convolutional neural network (C3D) that takes gesture videos as input and a 2D-CNN that takes OFMT images as input. C3D has shown its efficiency in capturing spatio-temporal information of a video. Whereas OFMT helps to eliminate irrelevant gestures providing additional motion information. Though each stream can work independently, they are combined with a fusion scheme to boost the recognition results. We have shown the efficiency of the proposed two-stream network on two databases.",0
"Hand gestures can be a valuable tool in various areas of human-computer interaction, such as sign language recognition and robotic surgery. However, accurately detecting and tracking the movement of the hand can be challenging due to variations in shape and size. To address this, a motion template guided by optical flow (OFMT) is proposed, which encodes gesture motion into a single image. The OFMT technique is tested on datasets using bare hand and a green-gloved hand with equal precision. While deep network-based techniques have shown better results than traditional feature-based methods, this work proposes a two-stream network that combines a 3D convolutional neural network (C3D) for gesture videos with a 2D-CNN for OFMT images. The fusion of these two streams improves recognition accuracy, and the efficiency of the proposed network is demonstrated on two databases.",1
"Optical flow is a crucial component of the feature space for early visual processing of dynamic scenes especially in new applications such as self-driving vehicles, drones and autonomous robots. The dynamic vision sensors are well suited for such applications because of their asynchronous, sparse and temporally precise representation of the visual dynamics. Many algorithms proposed for computing visual flow for these sensors suffer from the aperture problem as the direction of the estimated flow is governed by the curvature of the object rather than the true motion direction. Some methods that do overcome this problem by temporal windowing under-utilize the true precise temporal nature of the dynamic sensors. In this paper, we propose a novel multi-scale plane fitting based visual flow algorithm that is robust to the aperture problem and also computationally fast and efficient. Our algorithm performs well in many scenarios ranging from fixed camera recording simple geometric shapes to real world scenarios such as camera mounted on a moving car and can successfully perform event-by-event motion estimation of objects in the scene to allow for predictions of upto 500 ms i.e. equivalent to 10 to 25 frames with traditional cameras.",0
"In the context of emerging technologies like self-driving vehicles, drones, and autonomous robots, optical flow is a vital aspect of the feature space used for early visual processing of dynamic scenes. Dynamic vision sensors are an ideal choice for these applications due to their asynchronous, sparse, and temporally precise representation of visual dynamics. However, most algorithms designed for computing visual flow for these sensors encounter the aperture problem, where the direction of estimated flow is determined by object curvature instead of true motion direction. Some methods that do solve this problem through temporal windowing under-utilize the precise temporal nature of dynamic sensors. This paper introduces a new multi-scale plane fitting based visual flow algorithm that is both robust to the aperture problem and computationally fast and efficient. The algorithm performs well in various scenarios, including fixed camera recording simple geometric shapes and real-world situations like a camera attached to a moving car. It can achieve event-by-event motion estimation of objects in the scene and make predictions up to 500 ms, equivalent to 10 to 25 frames with traditional cameras.",1
"Transferring existing image-based detectors to the video is non-trivial since the quality of frames is always deteriorated by part occlusion, rare pose, and motion blur. Previous approaches exploit to propagate and aggregate features across video frames by using optical flow-warping. However, directly applying image-level optical flow onto the high-level features might not establish accurate spatial correspondences. Therefore, a novel module called Learnable Spatio-Temporal Sampling (LSTS) has been proposed to learn semantic-level correspondences among adjacent frame features accurately. The sampled locations are first randomly initialized, then updated iteratively to find better spatial correspondences guided by detection supervision progressively. Besides, Sparsely Recursive Feature Updating (SRFU) module and Dense Feature Aggregation (DFA) module are also introduced to model temporal relations and enhance per-frame features, respectively. Without bells and whistles, the proposed method achieves state-of-the-art performance on the ImageNet VID dataset with less computational complexity and real-time speed. Code will be made available at https://github.com/jiangzhengkai/LSTS.",0
"It is difficult to transfer image-based detectors to video due to the deterioration of frame quality caused by factors such as part occlusion, rare pose, and motion blur. Previous methods have used optical flow-warping to propagate and aggregate features across frames, but applying image-level optical flow directly to high-level features may not establish accurate spatial correspondences. A new module called Learnable Spatio-Temporal Sampling (LSTS) has been proposed to accurately learn semantic-level correspondences among adjacent frame features. The sampled locations are initialized randomly and updated iteratively for better spatial correspondences guided by detection supervision. Additionally, Sparsely Recursive Feature Updating (SRFU) and Dense Feature Aggregation (DFA) modules are introduced to model temporal relations and enhance per-frame features. The proposed method achieves state-of-the-art performance on the ImageNet VID dataset with less computational complexity and real-time speed. Code will be available at https://github.com/jiangzhengkai/LSTS.",1
"Modern methods for counting people in crowded scenes rely on deep networks to estimate people densities in individual images. As such, only very few take advantage of temporal consistency in video sequences, and those that do only impose weak smoothness constraints across consecutive frames.   In this paper, we advocate estimating people flows across image locations between consecutive images and inferring the people densities from these flows instead of directly regressing. This enables us to impose much stronger constraints encoding the conservation of the number of people. As a result, it significantly boosts performance without requiring a more complex architecture. Furthermore, it also enables us to exploit the correlation between people flow and optical flow to further improve the results.   We will demonstrate that we consistently outperform state-of-the-art methods on five benchmark datasets.",0
"Counting people in crowded scenes nowadays involves utilizing deep networks to assess the density of individuals in individual images. However, only a few of these methods make use of temporal consistency in video sequences, and those that do only implement weak smoothness restrictions across consecutive frames. In this article, we propose a novel approach to estimating people densities by estimating people flows between consecutive images and inferring the densities from these flows, rather than directly regressing. This practice enables us to impose stronger constraints that encode the conservation of the number of people, significantly enhancing performance without necessitating a more complex architecture. Additionally, it enables us to exploit the relationship between people flow and optical flow to further enhance the results. We will demonstrate that our approach consistently outperforms state-of-the-art methods across five benchmark datasets.",1
"In this paper, we focus on a prediction-based novelty estimation strategy upon the deep reinforcement learning (DRL) framework, and present a flow-based intrinsic curiosity module (FICM) to exploit the prediction errors from optical flow estimation as exploration bonuses. We propose the concept of leveraging motion features captured between consecutive observations to evaluate the novelty of observations in an environment. FICM encourages a DRL agent to explore observations with unfamiliar motion features, and requires only two consecutive frames to obtain sufficient information when estimating the novelty. We evaluate our method and compare it with a number of existing methods on multiple benchmark environments, including Atari games, Super Mario Bros., and ViZDoom. We demonstrate that FICM is favorable to tasks or environments featuring moving objects, which allow FICM to utilize the motion features between consecutive observations. We further ablatively analyze the encoding efficiency of FICM, and discuss its applicable domains comprehensively.",0
"The focus of this paper is on a strategy that estimates novelty based on predictions using deep reinforcement learning (DRL). To achieve this, we introduce a flow-based intrinsic curiosity module (FICM) that utilizes prediction errors from optical flow estimation as exploration bonuses. Our approach involves evaluating the novelty of observations in an environment by leveraging motion features captured between consecutive observations. FICM encourages a DRL agent to explore observations with unfamiliar motion features, and only requires two consecutive frames to estimate the novelty adequately. We evaluate our method on various benchmark environments such as Atari games, Super Mario Bros., and ViZDoom, and compare it with existing methods. Our findings indicate that FICM works best in tasks or environments featuring moving objects, allowing it to leverage the motion features between consecutive observations. We also analyze the encoding efficiency of FICM and discuss its applicable domains in detail.",1
"The objective of this paper is to recover the original component signals from a mixture audio with the aid of visual cues of the sound sources. Such task is usually referred as visually guided sound source separation. The proposed Cascaded Opponent Filter (COF) framework consists of multiple stages, which recursively refine the source separation. A key element in COF is a novel opponent filter module that identifies and relocates residual components between sources. The system is guided by the appearance and motion of the source, and, for this purpose, we study different representations based on video frames, optical flows, dynamic images, and their combinations. Finally, we propose a Sound Source Location Masking (SSLM) technique, which, together with COF, produces a pixel level mask of the source location. The entire system is trained end-to-end using a large set of unlabelled videos. We compare COF with recent baselines and obtain the state-of-the-art performance in three challenging datasets (MUSIC, A-MUSIC, and A-NATURAL). Project page: https://ly-zhu.github.io/cof-net.",0
"The aim of this article is to utilize visual cues of sound sources to separate mixed audio signals and recover the original component signals. This process is commonly known as visually guided sound source separation and is achieved through the Cascaded Opponent Filter (COF) framework, which involves multiple stages that refine the source separation recursively. The COF framework includes a unique opponent filter module that identifies and relocates residual components between sources. The system is guided by the appearance and motion of the source, and different representations based on video frames, optical flows, dynamic images, and their combinations are explored. Additionally, a Sound Source Location Masking (SSLM) technique is proposed, which, in combination with COF, produces a pixel level mask of the source location. The entire system is trained end-to-end using a large set of unlabelled videos. COF is compared to recent baselines, and its state-of-the-art performance is demonstrated in three challenging datasets (MUSIC, A-MUSIC, and A-NATURAL). More information about the project can be found at https://ly-zhu.github.io/cof-net.",1
"Many semantic events in team sport activities e.g. basketball often involve both group activities and the outcome (score or not). Motion patterns can be an effective means to identify different activities. Global and local motions have their respective emphasis on different activities, which are difficult to capture from the optical flow due to the mixture of global and local motions. Hence it calls for a more effective way to separate the global and local motions. When it comes to the specific case for basketball game analysis, the successful score for each round can be reliably detected by the appearance variation around the basket. Based on the observations, we propose a scheme to fuse global and local motion patterns (MPs) and key visual information (KVI) for semantic event recognition in basketball videos. Firstly, an algorithm is proposed to estimate the global motions from the mixed motions based on the intrinsic property of camera adjustments. And the local motions could be obtained from the mixed and global motions. Secondly, a two-stream 3D CNN framework is utilized for group activity recognition over the separated global and local motion patterns. Thirdly, the basket is detected and its appearance features are extracted through a CNN structure. The features are utilized to predict the success or failure. Finally, the group activity recognition and success/failure prediction results are integrated using the kronecker product for event recognition. Experiments on NCAA dataset demonstrate that the proposed method obtains state-of-the-art performance.",0
"In team sports like basketball, semantic events often involve group activities and the outcome of the game. Motion patterns can be useful in identifying different activities, but global and local motions have different emphases that are difficult to separate using optical flow. Detecting successful scores in basketball can be reliably done through appearance variation around the basket. To recognize semantic events in basketball videos, we propose a scheme that fuses global and local motion patterns and key visual information. Our algorithm estimates global motions from mixed motions and separates local motions. We then use a two-stream 3D CNN framework to recognize group activities and a CNN structure to extract appearance features of the basket for success/failure prediction. Finally, we integrate the group activity recognition and success/failure prediction using the kronecker product. Our experiments show that this method achieves state-of-the-art performance on the NCAA dataset.",1
"We present a new lightweight CNN-based algorithm for multi-frame optical flow estimation. Our solution introduces a double recurrence over spatial scale and time through repeated use of a generic ""STaR"" (SpatioTemporal Recurrent) cell. It includes (i) a temporal recurrence based on conveying learned features rather than optical flow estimates; (ii) an occlusion detection process which is coupled with optical flow estimation and therefore uses a very limited number of extra parameters. The resulting STaRFlow algorithm gives state-of-the-art performances on MPI Sintel and Kitti2015 and involves significantly less parameters than all other methods with comparable results.",0
"A novel CNN-based algorithm is introduced for multi-frame optical flow estimation which is lightweight. The proposed technique utilizes a ""STaR"" cell (SpatioTemporal Recurrent) to enable double recurrence over spatial scale and time. Specifically, the algorithm incorporates a temporal recurrence that relies on learned features instead of optical flow estimates and an occlusion detection process that is integrated with optical flow estimation and requires minimal additional parameters. The resulting STaRFlow approach achieves outstanding results on MPI Sintel and Kitti2015 datasets while employing significantly fewer parameters compared to other methods with similar outcomes.",1
"Shopping behaviour analysis through counting and tracking of people in shop-like environments offers valuable information for store operators and provides key insights in the stores layout (e.g. frequently visited spots). Instead of using extra staff for this, automated on-premise solutions are preferred. These automated systems should be cost-effective, preferably on lightweight embedded hardware, work in very challenging situations (e.g. handling occlusions) and preferably work real-time. We solve this challenge by implementing a real-time TensorRT optimized YOLOv3-based pedestrian detector, on a Jetson TX2 hardware platform. By combining the detector with a sparse optical flow tracker we assign a unique ID to each customer and tackle the problem of loosing partially occluded customers. Our detector-tracker based solution achieves an average precision of 81.59% at a processing speed of 10 FPS. Besides valuable statistics, heat maps of frequently visited spots are extracted and used as an overlay on the video stream.",0
"Counting and tracking people in shop-like environments can provide store operators with useful information about shopping behavior and insights into frequently visited spots in the store layout. Rather than relying on additional staff, automated on-premise solutions are preferred. These systems should be cost-effective, lightweight, able to handle challenging situations, and work in real-time. To meet this challenge, we have implemented a real-time TensorRT optimized YOLOv3-based pedestrian detector on a Jetson TX2 hardware platform. By combining the detector with a sparse optical flow tracker, each customer is assigned a unique ID, and the problem of losing partially occluded customers is addressed. Our detector-tracker based solution achieves an average precision of 81.59% at a processing speed of 10 FPS. In addition to providing valuable statistics, heat maps of frequently visited spots are extracted and overlaid onto the video stream.",1
"Fast motion feedback is crucial in computer-aided surgery (CAS) on moving tissue. Image-assistance in safety-critical vision applications requires a dense tracking of tissue motion. This can be done using optical flow (OF). Accurate motion predictions at high processing rates lead to higher patient safety. Current deep learning OF models show the common speed vs. accuracy trade-off. To achieve high accuracy at high processing rates, we propose patient-specific fine-tuning of a fast model. This minimizes the domain gap between training and application data, while reducing the target domain to the capability of the lower complex, fast model. We propose to obtain training sequences pre-operatively in the operation room. We handle missing ground truth, by employing teacher-student learning. Using flow estimations from teacher model FlowNet2 we specialize a fast student model FlowNet2S on the patient-specific domain. Evaluation is performed on sequences from the Hamlyn dataset. Our student model shows very good performance after fine-tuning. Tracking accuracy is comparable to the teacher model at a speed up of factor six. Fine-tuning can be performed within minutes, making it feasible for the operation room. Our method allows to use a real-time capable model that was previously not suited for this task. This method is laying the path for improved patient-specific motion estimation in CAS.",0
"In computer-aided surgery (CAS) on moving tissue, quick motion feedback is essential. For safety-critical vision applications that require image-assistance, precise tracking of tissue motion is necessary. Optical flow (OF) can accomplish this task. However, current deep learning OF models face a speed vs. accuracy trade-off. To achieve high accuracy at high processing rates, we suggest fine-tuning a fast model for each patient. This reduces the gap between training and application data and limits the target domain to the capabilities of the fast model. We propose obtaining training sequences before surgery in the operating room and using teacher-student learning to handle missing ground truth. By specializing a fast student model (FlowNet2S) on the patient-specific domain using flow estimations from the teacher model (FlowNet2), our model shows excellent performance after fine-tuning. Our method enables the use of a real-time capable model, previously unsuitable for this task, and can be performed within minutes in the operation room. This approach paves the way for improved patient-specific motion estimation in CAS.",1
"We design and implement an end-to-end system for real-time crime detection in low-light environments. Unlike Closed-Circuit Television, which performs reactively, the Low-Light Environment Neural Surveillance provides real time crime alerts. The system uses a low-light video feed processed in real-time by an optical-flow network, spatial and temporal networks, and a Support Vector Machine to identify shootings, assaults, and thefts. We create a low-light action-recognition dataset, LENS-4, which will be publicly available. An IoT infrastructure set up via Amazon Web Services interprets messages from the local board hosting the camera for action recognition and parses the results in the cloud to relay messages. The system achieves 71.5% accuracy at 20 FPS. The user interface is a mobile app which allows local authorities to receive notifications and to view a video of the crime scene. Citizens have a public app which enables law enforcement to push crime alerts based on user proximity.",0
"Our team has developed and executed a comprehensive system that can detect crimes in real-time in low-light environments. In contrast to Closed-Circuit Television, our Low-Light Environment Neural Surveillance is proactive and can provide immediate alerts. The system utilizes a low-light video feed that is processed through an optical-flow network, spatial and temporal networks, and a Support Vector Machine to identify shootings, assaults, and thefts. We have also generated a publicly-accessible low-light action-recognition dataset, LENS-4. Through Amazon Web Services, an IoT infrastructure has been established to receive messages from the camera's local board, interpret them for action recognition, and transmit the results to the cloud for analysis. At 20 FPS, our system has achieved an accuracy of 71.5%. Local authorities can receive notifications and view crime scene videos through a mobile app, while citizens have access to a public app that enables law enforcement to push crime alerts based on user proximity.",1
"An object's geocentric pose, defined as the height above ground and orientation with respect to gravity, is a powerful representation of real-world structure for object detection, segmentation, and localization tasks using RGBD images. For close-range vision tasks, height and orientation have been derived directly from stereo-computed depth and more recently from monocular depth predicted by deep networks. For long-range vision tasks such as Earth observation, depth cannot be reliably estimated with monocular images. Inspired by recent work in monocular height above ground prediction and optical flow prediction from static images, we develop an encoding of geocentric pose to address this challenge and train a deep network to compute the representation densely, supervised by publicly available airborne lidar. We exploit these attributes to rectify oblique images and remove observed object parallax to dramatically improve the accuracy of localization and to enable accurate alignment of multiple images taken from very different oblique viewpoints. We demonstrate the value of our approach by extending two large-scale public datasets for semantic segmentation in oblique satellite images. All of our data and code are publicly available.",0
"The geocentric pose of an object, which includes its height above ground and orientation with respect to gravity, is a useful representation for detecting, segmenting, and localizing objects in RGBD images. While stereo-computed depth and monocular depth predicted by deep networks have been used to derive height and orientation for close-range vision tasks, it is not reliable for long-range vision tasks such as Earth observation. To overcome this challenge, we have developed an encoding of geocentric pose and trained a deep network to compute it densely using publicly available airborne lidar for supervision. This approach enables us to rectify oblique images, remove observed object parallax, and significantly improve localization accuracy. We have successfully applied our methodology to extend two large-scale public datasets for semantic segmentation in oblique satellite images, and all of our data and code are publicly available.",1
"Occlusion is an inevitable and critical problem in unsupervised optical flow learning. Existing methods either treat occlusions equally as non-occluded regions or simply remove them to avoid incorrectness. However, the occlusion regions can provide effective information for optical flow learning. In this paper, we present OccInpFlow, an occlusion-inpainting framework to make full use of occlusion regions. Specifically, a new appearance-flow network is proposed to inpaint occluded flows based on the image content. Moreover, a boundary warp is proposed to deal with occlusions caused by displacement beyond image border. We conduct experiments on multiple leading flow benchmark data sets such as Flying Chairs, KITTI and MPI-Sintel, which demonstrate that the performance is significantly improved by our proposed occlusion handling framework.",0
"Unsupervised optical flow learning is faced with a critical and inevitable issue known as occlusion. Current methods treat occluded areas similarly to non-occluded regions or eliminate them to prevent inaccuracies. However, occluded regions can provide valuable information for optical flow learning. This article introduces OccInpFlow, an occlusion-inpainting approach that utilizes occlusion regions to their full potential. The method involves a novel appearance-flow network that inpaints occluded flows based on image content, as well as a boundary warp that addresses occlusions caused by displacement beyond the image border. We conducted experiments on several leading flow benchmark datasets like Flying Chairs, KITTI and MPI-Sintel, which demonstrated that the proposed occlusion handling framework significantly improved performance.",1
"Special cameras that provide useful features for face anti-spoofing are desirable, but not always an option. In this work we propose a method to utilize the difference in dynamic appearance between bona fide and spoof samples by creating artificial modalities from RGB videos. We introduce two types of artificial transforms: rank pooling and optical flow, combined in end-to-end pipeline for spoof detection. We demonstrate that using intermediate representations that contain less identity and fine-grained features increase model robustness to unseen attacks as well as to unseen ethnicities. The proposed method achieves state-of-the-art on the largest cross-ethnicity face anti-spoofing dataset CASIA-SURF CeFA (RGB).",0
"While it is desirable to have specialized cameras equipped with features that help prevent facial spoofing, these are not always available. In this study, we present an approach that leverages the varied dynamic appearance of genuine and spoofed samples through the creation of artificial modalities from RGB videos. We have introduced two types of artificial transforms, namely rank pooling and optical flow, that are integrated in an end-to-end pipeline for spoof detection. Our findings indicate that intermediate representations containing less identity and fine-grained features lead to greater model resilience towards both unfamiliar ethnicities and novel attacks. As supported by our results, our proposed method sets a new benchmark for the largest cross-ethnicity face anti-spoofing dataset, CASIA-SURF CeFA (RGB).",1
"One of the most relevant tasks in an intelligent vehicle navigation system is the detection of obstacles. It is important that a visual perception system for navigation purposes identifies obstacles, and it is also important that this system can extract essential information that may influence the vehicle's behavior, whether it will be generating an alert for a human driver or guide an autonomous vehicle in order to be able to make its driving decisions. In this paper we present an approach for the identification of obstacles and extraction of class, position, depth and motion information from these objects that employs data gained exclusively from passive vision. We performed our experiments on two different data-sets and the results obtained shown a good efficacy from the use of depth and motion patterns to assess the obstacles' potential threat status.",0
"Detecting obstacles is a crucial task in intelligent vehicle navigation systems. A visual perception system must identify obstacles and extract essential information that can impact the vehicle's behavior, whether through alerting a human driver or guiding an autonomous vehicle's driving decisions. This paper outlines an approach that uses passive vision data to identify obstacles, extract class, position, depth, and motion information, and assess their potential threat status. Our experiments on two data-sets show that depth and motion patterns are effective for this purpose.",1
"Robust and accurate six degree-of-freedom tracking on portable devices remains a challenging problem, especially on small hand-held devices such as smartphones. For improved robustness and accuracy, complementary movement information from an IMU and a camera is often fused. Conventional visual-inertial methods fuse information from IMUs with a sparse cloud of feature points tracked by the device camera. We consider a visually dense approach, where the IMU data is fused with the dense optical flow field estimated from the camera data. Learning-based methods applied to the full image frames can leverage visual cues and global consistency of the flow field to improve the flow estimates. We show how a learning-based optical flow model can be combined with conventional inertial navigation, and how ideas from probabilistic deep learning can aid the robustness of the measurement updates. The practical applicability is demonstrated on real-world data acquired by an iPad in a challenging low-texture environment.",0
"Tracking movement accurately in six degrees of freedom on portable devices remains a difficult task, especially on small hand-held gadgets like smartphones. To enhance accuracy and robustness, complementary movement information from an IMU and a camera are often combined. Traditional visual-inertial methods merge the information from IMUs with a sparse cloud of feature points that are tracked by the device camera. However, we propose a visually dense approach where the dense optical flow field estimated from the camera data is fused with the IMU data. By using learning-based methods applied to the entire image frames, visual cues and global consistency of the flow field can be leveraged to enhance flow estimates. We exhibit how a learning-based optical flow model can be integrated with conventional inertial navigation, and how probabilistic deep learning concepts can enhance the robustness of the measurement updates. We demonstrate the practical applicability of this approach using real-world data acquired by an iPad in a low-texture environment.",1
"Visual Odometry (VO) accumulates a positional drift in long-term robot navigation tasks. Although Convolutional Neural Networks (CNNs) improve VO in various aspects, VO still suffers from moving obstacles, discontinuous observation of features, and poor textures or visual information. While recent approaches estimate a 6DoF pose either directly from (a series of) images or by merging depth maps with optical flow (OF), research that combines absolute pose regression with OF is limited. We propose ViPR, a novel modular architecture for long-term 6DoF VO that leverages temporal information and synergies between absolute pose estimates (from PoseNet-like modules) and relative pose estimates (from FlowNet-based modules) by combining both through recurrent layers. Experiments on known datasets and on our own Industry dataset show that our modular design outperforms state of the art in long-term navigation tasks.",0
"Long-term robot navigation tasks may cause positional drift in Visual Odometry (VO), despite the various improvements made by Convolutional Neural Networks (CNNs). VO can still be hindered by moving obstacles, inconsistent feature observation, and insufficient visual information. While some approaches estimate 6DoF pose from images or merge depth maps with optical flow (OF), there is limited research on combining absolute pose regression with OF. Our proposed solution, ViPR, is a modular architecture that utilizes temporal information and synergies between absolute and relative pose estimates through recurrent layers. Results from experiments on established and in-house datasets demonstrate that our design surpasses current state-of-the-art methods in long-term navigation tasks.",1
"Significant progress has been made for estimating optical flow using deep neural networks. Advanced deep models achieve accurate flow estimation often with a considerable computation complexity and time-consuming training processes. In this work, we present a lightweight yet effective model for real-time optical flow estimation, termed FDFlowNet (fast deep flownet). We achieve better or similar accuracy on the challenging KITTI and Sintel benchmarks while being about 2 times faster than PWC-Net. This is achieved by a carefully-designed structure and newly proposed components. We first introduce an U-shape network for constructing multi-scale feature which benefits upper levels with global receptive field compared with pyramid network. In each scale, a partial fully connected structure with dilated convolution is proposed for flow estimation that obtains a good balance among speed, accuracy and number of parameters compared with sequential connected and dense connected structures. Experiments demonstrate that our model achieves state-of-the-art performance while being fast and lightweight.",0
"Deep neural networks have made significant strides in estimating optical flow, with advanced models achieving high accuracy. However, these models often require a considerable amount of computation and time-consuming training. To address this, we introduce FDFlowNet, a lightweight model for real-time optical flow estimation. Our model achieves comparable or better accuracy on challenging benchmarks like KITTI and Sintel while being about twice as fast as PWC-Net. This is due to our carefully designed structure and newly proposed components, such as a U-shape network for constructing multi-scale features and a partial fully connected structure with dilated convolution for flow estimation. Our experiments show that FDFlowNet is state-of-the-art in terms of performance, speed, and weight.",1
"Dense pixel matching is required for many computer vision algorithms such as disparity, optical flow or scene flow estimation. Feature Pyramid Networks (FPN) have proven to be a suitable feature extractor for CNN-based dense matching tasks. FPN generates well localized and semantically strong features at multiple scales. However, the generic FPN is not utilizing its full potential, due to its reasonable but limited localization accuracy. Thus, we present ResFPN -- a multi-resolution feature pyramid network with multiple residual skip connections, where at any scale, we leverage the information from higher resolution maps for stronger and better localized features. In our ablation study, we demonstrate the effectiveness of our novel architecture with clearly higher accuracy than FPN. In addition, we verify the superior accuracy of ResFPN in many different pixel matching applications on established datasets like KITTI, Sintel, and FlyingThings3D.",0
"Many computer vision algorithms, such as disparity, optical flow, and scene flow estimation, require dense pixel matching. Feature Pyramid Networks (FPN) have shown to be a suitable feature extractor for CNN-based dense matching tasks as they generate well-localized and semantically strong features at multiple scales. However, FPN's generic version is not utilizing its full potential due to limited localization accuracy. Therefore, we propose ResFPN, a multi-resolution feature pyramid network with multiple residual skip connections that leverages information from higher resolution maps for stronger and better localized features at any scale. Through our ablation study, we demonstrate the effectiveness of ResFPN, which yields higher accuracy compared to FPN. Furthermore, we verify ResFPN's superior accuracy in various pixel matching applications using established datasets such as KITTI, Sintel, and FlyingThings3D.",1
"Recently, 3D convolutional networks (3D ConvNets) yield good performance in action recognition. However, optical flow stream is still needed to ensure better performance, the cost of which is very high. In this paper, we propose a fast but effective way to extract motion features from videos utilizing residual frames as the input data in 3D ConvNets. By replacing traditional stacked RGB frames with residual ones, 35.6% and 26.6% points improvements over top-1 accuracy can be obtained on the UCF101 and HMDB51 datasets when ResNet-18 models are trained from scratch. And we achieved the state-of-the-art results in this training mode. Analysis shows that better motion features can be extracted using residual frames compared to RGB counterpart. By combining with a simple appearance path, our proposal can be even better than some methods using optical flow streams.",0
"Action recognition performance has been improved by 3D convolutional networks (3D ConvNets) but optical flow stream is still necessary for better performance despite its high cost. The paper proposes a quick yet effective way of extracting motion features from videos, which utilizes residual frames as the input data in 3D ConvNets. By replacing traditional stacked RGB frames with residual ones, the results showed a 35.6% and 26.6% boost in top-1 accuracy on the UCF101 and HMDB51 datasets when ResNet-18 models were trained from scratch, resulting in state-of-the-art performance. Analysis revealed that residual frames were better in extracting motion features compared to RGB frames. Combining with a simple appearance path, the proposed method even outperforms some methods utilizing optical flow streams.",1
"Pseudo-LiDAR point cloud interpolation is a novel and challenging task in the field of autonomous driving, which aims to address the frequency mismatching problem between camera and LiDAR. Previous works represent the 3D spatial motion relationship induced by a coarse 2D optical flow, and the quality of interpolated point clouds only depends on the supervision of depth maps. As a result, the generated point clouds suffer from inferior global distributions and local appearances. To solve the above problems, we propose a Pseudo-LiDAR point cloud interpolation network to generates temporally and spatially high-quality point cloud sequences. By exploiting the scene flow between point clouds, the proposed network is able to learn a more accurate representation of the 3D spatial motion relationship. For the more comprehensive perception of the distribution of point cloud, we design a novel reconstruction loss function that implements the chamfer distance to supervise the generation of Pseudo-LiDAR point clouds in 3D space. In addition, we introduce a multi-modal deep aggregation module to facilitate the efficient fusion of texture and depth features. As the benefits of the improved motion representation, training loss function, and model structure, our approach gains significant improvements on the Pseudo-LiDAR point cloud interpolation task. The experimental results evaluated on KITTI dataset demonstrate the state-of-the-art performance of the proposed network, quantitatively and qualitatively.",0
"The task of Pseudo-LiDAR point cloud interpolation is a challenging and innovative technique utilized in the field of autonomous driving. Its main objective is to address the issue of frequency mismatching between camera and LiDAR. Previous works have used a coarse 2D optical flow to represent the 3D spatial motion relationship, resulting in inferior global distributions and local appearances of the generated point clouds. To overcome these issues, we propose a Pseudo-LiDAR point cloud interpolation network that can generate high-quality point cloud sequences using a more accurate representation of the 3D spatial motion relationship. We also introduce a novel reconstruction loss function that employs the chamfer distance to supervise the generation of Pseudo-LiDAR point clouds in 3D space. Additionally, we utilize a multi-modal deep aggregation module to efficiently fuse texture and depth features. Our approach achieves significant improvements on the Pseudo-LiDAR point cloud interpolation task due to the improved motion representation, training loss function, and model structure. We demonstrate the state-of-the-art performance of our proposed network on the KITTI dataset through quantitative and qualitative evaluation.",1
"Visual attention serves as a means of feature selection mechanism in the perceptual system. Motivated by Broadbent's leaky filter model of selective attention, we evaluate how such mechanism could be implemented and affect the learning process of deep reinforcement learning. We visualize and analyze the feature maps of DQN on a toy problem Catch, and propose an approach to combine visual selective attention with deep reinforcement learning. We experiment with optical flow-based attention and A2C on Atari games. Experiment results show that visual selective attention could lead to improvements in terms of sample efficiency on tested games. An intriguing relation between attention and batch normalization is also discovered.",0
"In the perceptual system, visual attention functions as a mechanism for selecting features. Inspired by Broadbent's model of selective attention, we investigate how this mechanism can impact the learning process of deep reinforcement learning. By examining the feature maps of DQN on a simple Catch problem, we suggest a way to merge visual selective attention with deep reinforcement learning. Our experiment involves using optical flow-based attention and A2C on Atari games, and we discover that visual selective attention can enhance sample efficiency in the tested games. Additionally, we uncover an interesting correlation between attention and batch normalization.",1
"Self-supervised learning allows for better utilization of unlabelled data. The feature representation obtained by self-supervision can be used in downstream tasks such as classification, object detection, segmentation, and anomaly detection. While classification, object detection, and segmentation have been investigated with self-supervised learning, anomaly detection needs more attention. We consider the problem of anomaly detection in images and videos, and present a new visual anomaly detection technique for videos. Numerous seminal and state-of-the-art self-supervised methods are evaluated for anomaly detection on a variety of image datasets. The best performing image-based self-supervised representation learning method is then used for video anomaly detection to see the importance of spatial features in visual anomaly detection in videos. We also propose a simple self-supervision approach for learning temporal coherence across video frames without the use of any optical flow information. At its core, our method identifies the frame indices of a jumbled video sequence allowing it to learn the spatiotemporal features of the video. This intuitive approach shows superior performance of visual anomaly detection compared to numerous methods for images and videos on UCF101 and ILSVRC2015 video datasets.",0
"Utilizing unlabelled data more effectively is made possible by self-supervised learning. Downstream tasks such as classification, object detection, segmentation, and anomaly detection can benefit from the feature representation achieved through self-supervision. Although self-supervised learning has been explored in classification, object detection, and segmentation, anomaly detection requires further attention. In this study, we address anomaly detection in images and videos and present a novel visual anomaly detection approach for videos. Various self-supervised techniques, both seminal and state-of-the-art, are evaluated for anomaly detection on diverse image datasets. The optimal image-based self-supervised representation learning approach is subsequently employed for video anomaly detection to assess the significance of spatial features in visual anomaly detection in videos. Furthermore, we introduce a simple self-supervision method for learning temporal coherence across video frames without the use of optical flow information. Our approach identifies the frame indices in a jumbled video sequence to learn the spatiotemporal features of the video, resulting in superior visual anomaly detection performance compared to numerous methods for images and videos on UCF101 and ILSVRC2015 video datasets.",1
"Recently, several studies proposed methods to utilize some classes of optimization problems in designing deep neural networks to encode constraints that conventional layers cannot capture. However, these methods are still in their infancy and require special treatments, such as analyzing the KKT condition, for deriving the backpropagation formula. In this paper, we propose a new layer formulation called the fixed-point iteration (FPI) layer that facilitates the use of more complicated operations in deep networks. The backward FPI layer is also proposed for backpropagation, which is motivated by the recurrent back-propagation (RBP) algorithm. But in contrast to RBP, the backward FPI layer yields the gradient by a small network module without an explicit calculation of the Jacobian. In actual applications, both the forward and backward FPI layers can be treated as nodes in the computational graphs. All components in the proposed method are implemented at a high level of abstraction, which allows efficient higher-order differentiations on the nodes. In addition, we present two practical methods of the FPI layer, FPI_NN and FPI_GD, where the update operations of FPI are a small neural network module and a single gradient descent step based on a learnable cost function, respectively. FPI\_NN is intuitive, simple, and fast to train, while FPI_GD can be used for efficient training of energy networks that have been recently studied. While RBP and its related studies have not been applied to practical examples, our experiments show the FPI layer can be successfully applied to real-world problems such as image denoising, optical flow, and multi-label classification.",0
"In recent studies, methods have been proposed to incorporate certain classes of optimization problems into the design of deep neural networks. These methods aim to encode constraints that traditional layers are unable to capture. However, they are still in their early stages and require special treatments such as analyzing the KKT condition to derive the backpropagation formula. In this paper, we introduce a new layer formulation called the fixed-point iteration (FPI) layer that enables more complex operations in deep networks. We also propose the backward FPI layer for backpropagation, which is inspired by the recurrent back-propagation (RBP) algorithm. Unlike RBP, the backward FPI layer utilizes a small network module to obtain the gradient, without explicitly calculating the Jacobian. Both the forward and backward FPI layers can be considered as nodes in computational graphs. Our proposed method is implemented at a high level of abstraction, enabling efficient higher-order differentiations on the nodes. Additionally, we present two practical FPI layer methods: FPI_NN and FPI_GD. FPI_NN is straightforward and quick to train, while FPI_GD can efficiently train energy networks. Our experiments demonstrate that the FPI layer can solve real-world problems like image denoising, optical flow, and multi-label classification, unlike RBP and its related studies.",1
"Humans are very good at directing their visual attention toward relevant areas when they search for different types of objects. For instance, when we search for cars, we will look at the streets, not at the top of buildings. The motivation of this paper is to train a network to do the same via a multi-task learning approach. To train visual attention, we produce foreground/background segmentation labels in a semi-supervised way, using background subtraction or optical flow. Using these labels, we train an object detection model to produce foreground/background segmentation maps as well as bounding boxes while sharing most model parameters. We use those segmentation maps inside the network as a self-attention mechanism to weight the feature map used to produce the bounding boxes, decreasing the signal of non-relevant areas. We show that by using this method, we obtain a significant mAP improvement on two traffic surveillance datasets, with state-of-the-art results on both UA-DETRAC and UAVDT.",0
"The ability of humans to focus their visual attention on relevant areas while searching for objects is impressive. For example, when looking for cars, we typically scan the streets rather than the tops of buildings. This paper aims to teach a network to do the same using a multi-task learning approach. To achieve this, we generate foreground/background segmentation labels in a semi-supervised manner, utilizing background subtraction or optical flow. We then train an object detection model to create both foreground/background segmentation maps and bounding boxes, sharing most model parameters. Inside the network, we utilize these segmentation maps as a self-attention mechanism to weigh the feature map responsible for producing the bounding boxes, reducing the impact of non-relevant areas. Our results show that this method significantly improves mAP on two traffic surveillance datasets, achieving state-of-the-art performance on both UA-DETRAC and UAVDT.",1
"Fall detection in specialized homes for the elderly is challenging. Vision-based fall detection solutions have a significant advantage over sensor-based ones as they do not instrument the resident who can suffer from mental diseases. This work is part of a project intended to deploy fall detection solutions in nursing homes. The proposed solution, based on Deep Learning, is built on a Convolutional Neural Network (CNN) trained to maximize a sensitivity-based metric. This work presents the requirements from the medical side and how it impacts the tuning of a CNN. Results highlight the importance of the temporal aspect of a fall. Therefore, a custom metric adapted to this use case and an implementation of a decision-making process are proposed in order to best meet the medical teams requirements. Clinical relevance This work presents a fall detection solution enabled to detect 86.2% of falls while producing only 11.6% of false alarms in average on the considered databases.",0
"Detecting falls in specialized homes for the elderly is a difficult task. Sensor-based solutions can be problematic as residents may suffer from mental illnesses and may not tolerate being instrumented. Vision-based solutions have an advantage in this regard. This project aims to implement fall detection solutions in nursing homes using Deep Learning with a Convolutional Neural Network (CNN) that maximizes a sensitivity-based metric. The medical requirements are discussed, and the importance of the temporal aspect of a fall is highlighted. A custom metric and decision-making process are proposed to meet the medical team's needs. The results show that the proposed solution detects 86.2% of falls while producing only 11.6% of false alarms, making it clinically relevant.",1
"Inter-vehicle distance and relative velocity estimations are two basic functions for any ADAS (Advanced driver-assistance systems). In this paper, we propose a monocular camera-based inter-vehicle distance and relative velocity estimation method based on end-to-end training of a deep neural network. The key novelty of our method is the integration of multiple visual clues provided by any two time-consecutive monocular frames, which include deep feature clue, scene geometry clue, as well as temporal optical flow clue. We also propose a vehicle-centric sampling mechanism to alleviate the effect of perspective distortion in the motion field (i.e. optical flow). We implement the method by a light-weight deep neural network. Extensive experiments are conducted which confirm the superior performance of our method over other state-of-the-art methods, in terms of estimation accuracy, computational speed, and memory footprint.",0
"The fundamental functions of any advanced driver-assistance system are the estimation of inter-vehicle distance and relative velocity. This research proposes a method that utilizes monocular camera-based end-to-end training of a deep neural network to estimate these factors. Our approach integrates various visual clues from two consecutive monocular frames, such as deep feature, scene geometry, and temporal optical flow clues. Additionally, we suggest a vehicle-centric sampling mechanism to address the issue of perspective distortion in motion fields. The method is implemented with a lightweight deep neural network and is validated through extensive experiments demonstrating superior performance compared to other state-of-the-art methods in terms of estimation accuracy, computational speed, and memory footprint.",1
"In this work, we demonstrate that receptive fields in 3D pose estimation can be effectively specified using optical flow. We introduce adaptive receptive fields, a simple and effective method to aid receptive field selection in pose estimation models based on optical flow inference. We contrast the performance of a benchmark state-of-the-art model running on fixed receptive fields with their adaptive field counterparts. By using a reduced receptive field, our model can process slow-motion sequences (10x longer) 23% faster than the benchmark model running at regular speed. The reduction in computational cost is achieved while producing a pose prediction accuracy to within 0.36% of the benchmark model.",0
"The study presented here showcases the efficacy of using optical flow to determine receptive fields in 3D pose estimation. We introduce a straightforward and efficient approach called adaptive receptive fields to assist with selecting receptive fields in pose estimation models that rely on optical flow inference. Our research compares the performance of a cutting-edge model that operates on fixed receptive fields with its adaptive field equivalents. Our findings reveal that by using a smaller receptive field, our model can process slow-motion video sequences that are 10 times longer than those of the benchmark model at a rate that is 23% faster. Despite the decrease in computational cost, our model produces pose prediction results that are off by only 0.36% compared to the benchmark model.",1
"Learning to represent videos is a very challenging task both algorithmically and computationally. Standard video CNN architectures have been designed by directly extending architectures devised for image understanding to include the time dimension, using modules such as 3D convolutions, or by using two-stream design to capture both appearance and motion in videos. We interpret a video CNN as a collection of multi-stream convolutional blocks connected to each other, and propose the approach of automatically finding neural architectures with better connectivity and spatio-temporal interactions for video understanding. This is done by evolving a population of overly-connected architectures guided by connection weight learning. Architectures combining representations that abstract different input types (i.e., RGB and optical flow) at multiple temporal resolutions are searched for, allowing different types or sources of information to interact with each other. Our method, referred to as AssembleNet, outperforms prior approaches on public video datasets, in some cases by a great margin. We obtain 58.6% mAP on Charades and 34.27% accuracy on Moments-in-Time.",0
"Representing videos is a complex task that poses challenges both algorithmically and computationally. Conventional video CNN architectures have been developed by modifying image understanding architectures to incorporate the time dimension, using techniques such as 3D convolutions or two-stream design to capture appearance and motion in videos. We consider a video CNN as a set of multi-stream convolutional blocks connected to one another and propose an approach to automatically discover neural architectures with improved connectivity and spatio-temporal interactions for video comprehension. This is achieved by evolving a population of overly-connected architectures guided by connection weight learning. We search for architectures that combine representations that abstract different input types (e.g. RGB and optical flow) at various temporal resolutions, allowing different information types or sources to interact. Our approach, named AssembleNet, outperforms previous methods on public video datasets, surpassing them by a significant margin in some cases. We achieve 58.6% mAP on Charades and 34.27% accuracy on Moments-in-Time.",1
"Motion is a salient cue to recognize actions in video. Modern action recognition models leverage motion information either explicitly by using optical flow as input or implicitly by means of 3D convolutional filters that simultaneously capture appearance and motion information. This paper proposes an alternative approach based on a learnable correlation operator that can be used to establish frame-toframe matches over convolutional feature maps in the different layers of the network. The proposed architecture enables the fusion of this explicit temporal matching information with traditional appearance cues captured by 2D convolution. Our correlation network compares favorably with widely-used 3D CNNs for video modeling, and achieves competitive results over the prominent two-stream network while being much faster to train. We empirically demonstrate that correlation networks produce strong results on a variety of video datasets, and outperform the state of the art on four popular benchmarks for action recognition: Kinetics, Something-Something, Diving48 and Sports1M.",0
"Recognizing actions in video heavily relies on detecting motion, which can be achieved through either explicit usage of optical flow or implicit utilization of 3D convolutional filters. This study, however, proposes an alternative approach that employs a learnable correlation operator to establish matches between frames across convolutional feature maps in various network layers. This architecture combines temporal matching information with traditional appearance cues, resulting in a correlation network that outperforms widely-used 3D CNNs for video modeling and the prominent two-stream network while being faster to train. Empirical evidence shows that correlation networks yield impressive results across multiple video datasets and surpass state-of-the-art benchmarks for action recognition, namely Kinetics, Something-Something, Diving48, and Sports1M.",1
"Learning based approaches have not yet achieved their full potential in optical flow estimation, where their performance still trails heuristic approaches. In this paper, we present a CNN based patch matching approach for optical flow estimation. An important contribution of our approach is a novel thresholded loss for Siamese networks. We demonstrate that our loss performs clearly better than existing losses. It also allows to speed up training by a factor of 2 in our tests. Furthermore, we present a novel way for calculating CNN based features for different image scales, which performs better than existing methods. We also discuss new ways of evaluating the robustness of trained features for the application of patch matching for optical flow. An interesting discovery in our paper is that low-pass filtering of feature maps can increase the robustness of features created by CNNs. We proved the competitive performance of our approach by submitting it to the KITTI 2012, KITTI 2015 and MPI-Sintel evaluation portals where we obtained state-of-the-art results on all three datasets.",0
"While heuristic approaches currently outperform learning based approaches in optical flow estimation, we introduce a CNN based patch matching method in this paper. One of the key features of our approach is a novel thresholded loss for Siamese networks, which outperforms existing losses and speeds up training by 2x in our tests. We also propose a new method for calculating CNN based features for various image scales that performs better than current techniques, and explore new ways to evaluate the robustness of trained features for patch matching in optical flow. Interestingly, we found that low-pass filtering of feature maps can increase the robustness of CNN-created features. By submitting our approach to the KITTI 2012, KITTI 2015, and MPI-Sintel evaluation portals, we demonstrate its competitive performance with state-of-the-art results on all three datasets.",1
"Face spoofing causes severe security threats in face recognition systems. Previous anti-spoofing works focused on supervised techniques, typically with either binary or auxiliary supervision. Most of them suffer from limited robustness and generalization, especially in the cross-dataset setting. In this paper, we propose a semi-supervised adversarial learning framework for spoof face detection, which largely relaxes the supervision condition. To capture the underlying structure of live faces data in latent representation space, we propose to train the live face data only, with a convolutional Encoder-Decoder network acting as a Generator. Meanwhile, we add a second convolutional network serving as a Discriminator. The generator and discriminator are trained by competing with each other while collaborating to understand the underlying concept in the normal class(live faces). Since the spoof face detection is video based (i.e., temporal information), we intuitively take the optical flow maps converted from consecutive video frames as input. Our approach is free of the spoof faces, thus being robust and general to different types of spoof, even unknown spoof. Extensive experiments on intra- and cross-dataset tests show that our semi-supervised method achieves better or comparable results to state-of-the-art supervised techniques.",0
"The use of fake faces poses a significant risk to the security of facial recognition systems. Previous attempts to combat this issue have relied on supervised techniques, often with limited effectiveness and applicability across datasets. This paper proposes a semi-supervised adversarial learning framework to detect fake faces, which does not require as much supervision. To accomplish this, the live face data is trained with a convolutional Encoder-Decoder network acting as a Generator, while a second convolutional network serves as a Discriminator. The Generator and Discriminator are trained by competing against each other while working together to understand the underlying concept of the normal class (live faces). As the detection of fake faces relies on temporal information, the optical flow maps converted from consecutive video frames are used as input. This approach is robust and effective in detecting different types of fake faces, even those that are unknown. The experiments conducted on both intra- and cross-dataset tests demonstrate that our semi-supervised method achieves results that are better or comparable to existing supervised techniques.",1
"Instance segmentation of unknown objects from images is regarded as relevant for several robot skills including grasping, tracking and object sorting. Recent results in computer vision have shown that large hand-labeled datasets enable high segmentation performance. To overcome the time-consuming process of manually labeling data for new environments, we present a transfer learning approach for robots that learn to segment objects by interacting with their environment in a self-supervised manner. Our robot pushes unknown objects on a table and uses information from optical flow to create training labels in the form of object masks. To achieve this, we fine-tune an existing DeepMask network for instance segmentation on the self-labeled training data acquired by the robot. We evaluate our trained network (SelfDeepMask) on a set of real images showing challenging and cluttered scenes with novel objects. Here, SelfDeepMask outperforms the DeepMask network trained on the COCO dataset by 9.5% in average precision. Furthermore, we combine our approach with recent approaches for training with noisy labels in order to better cope with induced label noise.",0
"Several robot skills such as grasping, tracking, and object sorting require instance segmentation of unknown objects from images. Although large hand-labeled datasets have shown high segmentation performance, the process of manually labeling new environments is time-consuming. To address this, we propose a transfer learning approach for robots to learn to segment objects in a self-supervised manner by interacting with their environment. Our robot pushes unknown objects on a table and uses information from optical flow to create training labels. We fine-tune an existing DeepMask network for instance segmentation on the self-labeled training data. Our trained network (SelfDeepMask) outperforms the DeepMask network trained on the COCO dataset by 9.5% in average precision on real images with challenging and cluttered scenes. We also combine our approach with recent techniques for training with noisy labels to better cope with induced label noise.",1
"We reveal that the Analytic Signal phase, and its gradient have a hitherto unstudied discontinuity in $2-D $ and higher dimensions. The shortcoming can result in severe artifacts whereas the problem does not exist in $1-D $ signals. Direct use of Gabor phase, or its gradient, in computer vision and biometric recognition e.g., as done in influential studies \cite{fleet90,wiskott1997face}, may produce undesired results that will go unnoticed unless special images similar to ours reveal them. Instead of the Analytic Signal phase, we suggest the use of Linear Symmetry phase, relying on more than one set of Gabor filters, but with a negligible computational add-on, as a remedy. Gradient magnitudes of this phase are continuous in contrast to that of the analytic signal whereas continuity of the gradient direction of the phase is guaranteed if Linear Symmetry Tensor replaces gradient vector. The suggested phase has also a built-in automatic scale estimator, useful for robust detection of patterns by multi-scale processing. We show crucial concepts on synthesized fingerprint images, where ground truth regarding instantaneous frequency, (scale \& direction), and phase are known with favorable results. A comparison to a baseline alternative is also reported. To that end, a novel multi-scale minutia model where location, direction, and scale of minutia parameters are steerable, without the creation of uncontrollable minutia is also presented. This is a useful tool, to reduce development times of minutia detection methods with explainable behavior. A revealed consequence is that minutia directions are not determined by the linear phase alone, but also by each other and the influence must be corrected to obtain steerability and accurate ground truths. Essential conclusions are readily transferable to $N-D $, and unrelated applications, e.g. optical flow or disparity estimation in stereo.",0
"In this study, we found that the Analytic Signal phase and its gradient exhibit an unexplored discontinuity in dimensions higher than 1-D, leading to undesirable artifacts in computer vision and biometric recognition. However, this problem is not present in 1-D signals. To address this issue, we propose the use of the Linear Symmetry phase, which involves multiple sets of Gabor filters and has a negligible computational overhead. The gradient magnitudes of the Linear Symmetry phase are continuous, and the gradient direction can be ensured to be continuous by using the Linear Symmetry Tensor. Additionally, the Linear Symmetry phase has an automatic scale estimator that makes it useful for multi-scale processing. We demonstrate the effectiveness of this approach on synthesized fingerprint images, where we know the ground truth regarding scale, direction, and phase. We also introduce a multi-scale minutia model that enables steerable minutia parameters without uncontrollable minutia creation. However, we found that minutia directions are influenced by each other and must be corrected for accurate ground truths. Our findings have implications for N-D applications, such as optical flow or disparity estimation in stereo.",1
"In this paper, we address the open research problem of surgical gesture recognition using motion cues from video data only. We adapt Optical flow ConvNets initially proposed by Simonyan et al.. While Simonyan uses both RGB frames and dense optical flow, we use only dense optical flow representations as input to emphasize the role of motion in surgical gesture recognition, and present it as a robust alternative to kinematic data. We also overcome one of the limitations of Optical flow ConvNets by initializing our model with cross modality pre-training. A large number of promising studies that address surgical gesture recognition highly rely on kinematic data which requires additional recording devices. To our knowledge, this is the first paper that addresses surgical gesture recognition using dense optical flow information only. We achieve competitive results on JIGSAWS dataset, moreover, our model achieves more robust results with less standard deviation, which suggests optical flow information can be used as an alternative to kinematic data for the recognition of surgical gestures.",0
"This paper aims to tackle the unresolved research issue of recognizing surgical gestures using solely motion cues from video data. To do so, we modify the Optical flow ConvNets method, which was first suggested by Simonyan et al. Instead of using both RGB frames and dense optical flow, we solely utilize dense optical flow representations as input to emphasize the significance of motion in surgical gesture recognition. We propose this as a more reliable alternative to kinematic data and overcome one of the limitations of Optical flow ConvNets by initializing our model with cross modality pre-training. Many studies on surgical gesture recognition rely heavily on kinematic data, which requires additional recording devices. This is the first study that specifically addresses surgical gesture recognition using only dense optical flow information. Our findings on the JIGSAWS dataset are quite promising, as our model achieves competitive results and proves to be more robust with less standard deviation. This suggests that optical flow information can be utilized as a substitute for kinematic data in recognizing surgical gestures.",1
"We present a self-supervised learning framework to estimate the individual object motion and monocular depth from video. We model the object motion as a 6 degree-of-freedom rigid-body transformation. The instance segmentation mask is leveraged to introduce the information of object. Compared with methods which predict dense optical flow map to model the motion, our approach significantly reduces the number of values to be estimated. Our system eliminates the scale ambiguity of motion prediction through imposing a novel geometric constraint loss term. Experiments on KITTI driving dataset demonstrate our system is capable to capture the object motion without external annotation. Our system outperforms previous self-supervised approaches in terms of 3D scene flow prediction, and contribute to the disparity prediction in dynamic area.",0
"We have developed a self-supervised learning framework that can determine an object's motion and monocular depth from video footage. Our framework represents the object's motion as a rigid-body transformation with six degrees of freedom. We incorporate the instance segmentation mask to provide object information. Compared to methods that use dense optical flow maps for motion modeling, our approach requires fewer estimations. Our system resolves the scale ambiguity of motion prediction by implementing a novel geometric constraint loss term. Our experiments on the KITTI driving dataset demonstrate that our system can capture object motion without external annotation. Our system outperforms previous self-supervised approaches in terms of 3D scene flow prediction and contributes to disparity prediction in dynamic areas.",1
"Micro-expressions are brief and subtle facial expressions that go on and off the face in a fraction of a second. This kind of facial expressions usually occurs in high stake situations and is considered to reflect a human's real intent. There has been some interest in micro-expression analysis, however, a great majority of the methods are based on classically established computer vision methods such as local binary patterns, histogram of gradients and optical flow. A novel methodology for micro-expression recognition using the Riesz pyramid, a multi-scale steerable Hilbert transform is presented. In fact, an image sequence is transformed with this tool, then the image phase variations are extracted and filtered as proxies for motion. Furthermore, the dominant orientation constancy from the Riesz transform is exploited to average the micro-expression sequence into an image pair. Based on that, the Mean Oriented Riesz Feature description is introduced. Finally the performance of our methods are tested in two spontaneous micro-expressions databases and compared to state-of-the-art methods.",0
"Micro-expressions are fleeting and subtle facial movements that occur rapidly in high-stress situations, and are believed to reveal a person's true intentions. Despite interest in analyzing micro-expressions, most existing methods rely on traditional computer vision techniques like local binary patterns, histogram of gradients, and optical flow. To address this, a new approach using the Riesz pyramid, a multi-scale steerable Hilbert transform, has been developed. This technique transforms image sequences, extracting and filtering phase variations as motion proxies. The Riesz transform's dominant orientation constancy is exploited to average micro-expressions into an image pair, and the Mean Oriented Riesz Feature description is introduced. Testing on two spontaneous micro-expression databases showed superior performance compared to state-of-the-art methods.",1
"Understanding on-road vehicle behaviour from a temporal sequence of sensor data is gaining in popularity. In this paper, we propose a pipeline for understanding vehicle behaviour from a monocular image sequence or video. A monocular sequence along with scene semantics, optical flow and object labels are used to get spatial information about the object (vehicle) of interest and other objects (semantically contiguous set of locations) in the scene. This spatial information is encoded by a Multi-Relational Graph Convolutional Network (MR-GCN), and a temporal sequence of such encodings is fed to a recurrent network to label vehicle behaviours. The proposed framework can classify a variety of vehicle behaviours to high fidelity on datasets that are diverse and include European, Chinese and Indian on-road scenes. The framework also provides for seamless transfer of models across datasets without entailing re-annotation, retraining and even fine-tuning. We show comparative performance gain over baseline Spatio-temporal classifiers and detail a variety of ablations to showcase the efficacy of the framework.",0
"The popularity of understanding on-road vehicle behaviour through a temporal sequence of sensor data is increasing. This paper introduces a pipeline for comprehending vehicle behaviour through a monocular image sequence or video. Utilizing a monocular sequence, scene semantics, optical flow, and object labels, spatial information about the object of interest (vehicle) and other objects in the scene (semantically contiguous set of locations) is obtained. This spatial information is then encoded using a Multi-Relational Graph Convolutional Network (MR-GCN), and a recurrent network is used to label vehicle behaviours based on a temporal sequence of such encodings. The proposed framework can accurately classify a range of vehicle behaviours on diverse datasets that include European, Chinese, and Indian on-road scenes. The framework allows for models to be seamlessly transferred across datasets without requiring re-annotation, retraining, or fine-tuning. Comparative performance gains over baseline Spatio-temporal classifiers are demonstrated, and various ablations are presented to showcase the effectiveness of the framework.",1
"In this paper, we tackle the problem of egocentric action anticipation, i.e., predicting what actions the camera wearer will perform in the near future and which objects they will interact with. Specifically, we contribute Rolling-Unrolling LSTM, a learning architecture to anticipate actions from egocentric videos. The method is based on three components: 1) an architecture comprised of two LSTMs to model the sub-tasks of summarizing the past and inferring the future, 2) a Sequence Completion Pre-Training technique which encourages the LSTMs to focus on the different sub-tasks, and 3) a Modality ATTention (MATT) mechanism to efficiently fuse multi-modal predictions performed by processing RGB frames, optical flow fields and object-based features. The proposed approach is validated on EPIC-Kitchens, EGTEA Gaze+ and ActivityNet. The experiments show that the proposed architecture is state-of-the-art in the domain of egocentric videos, achieving top performances in the 2019 EPIC-Kitchens egocentric action anticipation challenge. The approach also achieves competitive performance on ActivityNet with respect to methods not based on unsupervised pre-training and generalizes to the tasks of early action recognition and action recognition. To encourage research on this challenging topic, we made our code, trained models, and pre-extracted features available at our web page: http://iplab.dmi.unict.it/rulstm.",0
"The focus of this paper is on the prediction of actions and object interactions that a camera wearer will perform in the near future, which is known as egocentric action anticipation. The proposed method, Rolling-Unrolling LSTM, is an architecture that uses two LSTMs to model the sub-tasks of summarizing the past and inferring the future. Additionally, a Sequence Completion Pre-Training technique is employed to encourage the LSTMs to focus on the different sub-tasks, and a Modality ATTention (MATT) mechanism is used to fuse multi-modal predictions efficiently. The approach is tested on three datasets and achieves state-of-the-art performance in egocentric action anticipation, including achieving top performance in the 2019 EPIC-Kitchens challenge. The approach also performs well in early action recognition and action recognition tasks. The authors have made their code, trained models, and pre-extracted features available for others to use and build upon.",1
"Temporal feature extraction is an important issue in video-based action recognition. Optical flow is a popular method to extract temporal feature, which produces excellent performance thanks to its capacity of capturing pixel-level correlation information between consecutive frames. However, such a pixel-level correlation is extracted at the cost of high computational complexity and large storage resource. In this paper, we propose a novel temporal feature extraction method, named Attentive Correlated Temporal Feature (ACTF), by exploring inter-frame correlation within a certain region. The proposed ACTF exploits both bilinear and linear correlation between successive frames on the regional level. Our method has the advantage of achieving performance comparable to or better than optical flow-based methods while avoiding the introduction of optical flow. Experimental results demonstrate our proposed method achieves the state-of-the-art performances of 96.3% on UCF101 and 76.3% on HMDB51 benchmark datasets.",0
"Video-based action recognition requires effective temporal feature extraction. Optical flow is a widely used method for this purpose, as it can capture pixel-level correlation between consecutive frames, resulting in high performance. However, this method is computationally demanding and requires significant storage resources. In this study, we propose a new approach called Attentive Correlated Temporal Feature (ACTF), which focuses on inter-frame correlation within a specific region. Our method uses both bilinear and linear correlation between successive frames on the regional level, achieving comparable or superior performance to optical flow-based methods while avoiding the need for optical flow. Our experiments on UCF101 and HMDB51 benchmark datasets show that our proposed method achieves state-of-the-art performances of 96.3% and 76.3%, respectively.",1
"Understanding ego-motion and surrounding vehicle state is essential to enable automated driving and advanced driving assistance technologies. Typical approaches to solve this problem use fusion of multiple sensors such as LiDAR, camera, and radar to recognize surrounding vehicle state, including position, velocity, and orientation. Such sensing modalities are overly complex and costly for production of personal use vehicles. In this paper, we propose a novel machine learning method to estimate ego-motion and surrounding vehicle state using a single monocular camera. Our approach is based on a combination of three deep neural networks to estimate the 3D vehicle bounding box, depth, and optical flow from a sequence of images. The main contribution of this paper is a new framework and algorithm that integrates these three networks in order to estimate the ego-motion and surrounding vehicle state. To realize more accurate 3D position estimation, we address ground plane correction in real-time. The efficacy of the proposed method is demonstrated through experimental evaluations that compare our results to ground truth data available from other sensors including Can-Bus and LiDAR.",0
"To enable automated driving and advanced driving assistance technologies, it is crucial to comprehend ego-motion and surrounding vehicle state. The conventional approach to solving this problem involves using a fusion of various sensors such as LiDAR, radar, and camera to identify the surrounding vehicle state, including position, velocity, and orientation. However, these sensing modalities are too complicated and expensive for personal use vehicles. In this article, we present a new machine learning approach that employs a single monocular camera to estimate ego-motion and surrounding vehicle state. Our method involves combining three deep neural networks to determine the 3D vehicle bounding box, depth, and optical flow from a sequence of images. The primary contribution of this paper is a new framework and algorithm that integrates these three networks to estimate the ego-motion and surrounding vehicle state. To achieve more accurate 3D position estimation, we address ground plane correction in real-time. We demonstrate the effectiveness of the proposed method through experimental evaluations that compare our results to ground truth data obtained from other sensors, including Can-Bus and LiDAR.",1
"We propose to modify the common training protocols of optical flow, leading to sizable accuracy improvements without adding to the computational complexity of the training process. The improvement is based on observing the bias in sampling challenging data that exists in the current training protocol, and improving the sampling process. In addition, we find that both regularization and augmentation should decrease during the training protocol.   Using an existing low parameters architecture, the method is ranked first on the MPI Sintel benchmark among all other methods, improving the best two frames method accuracy by more than 10%. The method also surpasses all similar architecture variants by more than 12% and 19.7% on the KITTI benchmarks, achieving the lowest Average End-Point Error on KITTI2012 among two-frame methods, without using extra datasets.",0
"Our proposal involves modifying the standard optical flow training protocols to achieve significant accuracy improvements while maintaining the same level of computational complexity. We achieved this by identifying the bias in the current training protocol's sampling process and improving it. Additionally, we discovered that both regularization and augmentation should decrease during the training process. Using an existing low parameter architecture, our method outperformed all other methods on the MPI Sintel benchmark, improving the accuracy of the best two frames method by over 10%. Our method also surpassed all similar architecture variants by more than 12% and 19.7% on the KITTI benchmarks, achieving the lowest Average End-Point Error on KITTI2012 among two-frame methods without requiring additional datasets.",1
"Egocentric activity recognition in first-person videos has an increasing importance with a variety of applications such as lifelogging, summarization, assisted-living and activity tracking. Existing methods for this task are based on interpretation of various sensor information using pre-determined weights for each feature. In this work, we propose a new framework for egocentric activity recognition problem based on combining audio-visual features with multi-kernel learning (MKL) and multi-kernel boosting (MKBoost). For that purpose, firstly grid optical-flow, virtual-inertia feature, log-covariance, cuboid are extracted from the video. The audio signal is characterized using a ""supervector"", obtained based on Gaussian mixture modelling of frame-level features, followed by a maximum a-posteriori adaptation. Then, the extracted multi-modal features are adaptively fused by MKL classifiers in which both the feature and kernel selection/weighing and recognition tasks are performed together. The proposed framework was evaluated on a number of egocentric datasets. The results showed that using multi-modal features with MKL outperforms the existing methods.",0
"The significance of egocentric activity recognition in first-person videos is growing due to its various applications, including lifelogging, summarization, assisted-living, and activity tracking. Current techniques for this task rely on predetermined weights for each sensor feature to interpret sensor information. This study presents a novel approach to the egocentric activity recognition problem by combining audio-visual features with multi-kernel learning (MKL) and multi-kernel boosting (MKBoost). The proposed framework extracts grid optical-flow, virtual-inertia feature, log-covariance, and cuboid from the video and characterizes the audio signal using a ""supervector"" obtained through Gaussian mixture modelling of frame-level features. MKL classifiers are used to adaptively fuse the extracted multi-modal features, combining the feature and kernel selection/weighing and recognition tasks. The proposed framework is evaluated on various egocentric datasets, and the results show that using multi-modal features with MKL outperforms existing methods.",1
"Current benchmarks for optical flow algorithms evaluate the estimation either directly by comparing the predicted flow fields with the ground truth or indirectly by using the predicted flow fields for frame interpolation and then comparing the interpolated frames with the actual frames. In the latter case, objective quality measures such as the mean squared error are typically employed. However, it is well known that for image quality assessment, the actual quality experienced by the user cannot be fully deduced from such simple measures. Hence, we conducted a subjective quality assessment crowdscouring study for the interpolated frames provided by one of the optical flow benchmarks, the Middlebury benchmark. We collected forced-choice paired comparisons between interpolated images and corresponding ground truth. To increase the sensitivity of observers when judging minute difference in paired comparisons we introduced a new method to the field of full-reference quality assessment, called artefact amplification. From the crowdsourcing data, we reconstructed absolute quality scale values according to Thurstone's model. As a result, we obtained a re-ranking of the 155 participating algorithms w.r.t. the visual quality of the interpolated frames. This re-ranking not only shows the necessity of visual quality assessment as another evaluation metric for optical flow and frame interpolation benchmarks, the results also provide the ground truth for designing novel image quality assessment (IQA) methods dedicated to perceptual quality of interpolated images. As a first step, we proposed such a new full-reference method, called WAE-IQA. By weighing the local differences between an interpolated image and its ground truth WAE-IQA performed slightly better than the currently best FR-IQA approach from the literature.",0
"Optical flow algorithm benchmarks are currently evaluated either by direct comparison to ground truth or indirectly by interpolating frames and comparing them to actual frames using objective quality measures such as mean squared error. However, these simple measures do not fully capture the quality experienced by users. To address this, we conducted a subjective quality assessment study using crowdsourcing and forced-choice paired comparisons between interpolated images and corresponding ground truth. To increase sensitivity to minute differences, we introduced a new method called artefact amplification to the field of full-reference quality assessment. Using data from the study, we reconstructed absolute quality scale values and re-ranked the 155 participating algorithms based on visual quality. This highlights the importance of visual quality assessment as an evaluation metric for optical flow and frame interpolation benchmarks and provides ground truth for developing novel image quality assessment methods. As a first step, we proposed a new full-reference method called WAE-IQA, which outperformed the current best approach from the literature by weighing local differences between interpolated images and their ground truth.",1
"Modeling hand-object manipulations is essential for understanding how humans interact with their environment. While of practical importance, estimating the pose of hands and objects during interactions is challenging due to the large mutual occlusions that occur during manipulation. Recent efforts have been directed towards fully-supervised methods that require large amounts of labeled training samples. Collecting 3D ground-truth data for hand-object interactions, however, is costly, tedious, and error-prone. To overcome this challenge we present a method to leverage photometric consistency across time when annotations are only available for a sparse subset of frames in a video. Our model is trained end-to-end on color images to jointly reconstruct hands and objects in 3D by inferring their poses. Given our estimated reconstructions, we differentiably render the optical flow between pairs of adjacent images and use it within the network to warp one frame to another. We then apply a self-supervised photometric loss that relies on the visual consistency between nearby images. We achieve state-of-the-art results on 3D hand-object reconstruction benchmarks and demonstrate that our approach allows us to improve the pose estimation accuracy by leveraging information from neighboring frames in low-data regimes.",0
"Understanding human interaction with their environment requires modeling hand-object manipulations. However, estimating the pose of hands and objects during interactions is challenging due to mutual occlusions. Existing methods require large amounts of labeled training samples, but collecting 3D ground-truth data is costly, tedious, and error-prone. To address this, we propose a method that leverages photometric consistency across time when annotations are only available for a sparse subset of frames. Our end-to-end model jointly reconstructs hands and objects in 3D by inferring their poses from color images. We then use differentiable rendering of optical flow between adjacent images to warp frames and apply a self-supervised photometric loss that relies on visual consistency. Our approach achieves state-of-the-art results on 3D hand-object reconstruction benchmarks and improves pose estimation accuracy by leveraging information from neighboring frames in low-data regimes.",1
"Recent deep learning approaches have achieved impressive performance on visual sound separation tasks. However, these approaches are mostly built on appearance and optical flow like motion feature representations, which exhibit limited abilities to find the correlations between audio signals and visual points, especially when separating multiple instruments of the same types, such as multiple violins in a scene. To address this, we propose ""Music Gesture,"" a keypoint-based structured representation to explicitly model the body and finger movements of musicians when they perform music. We first adopt a context-aware graph network to integrate visual semantic context with body dynamics, and then apply an audio-visual fusion model to associate body movements with the corresponding audio signals. Experimental results on three music performance datasets show: 1) strong improvements upon benchmark metrics for hetero-musical separation tasks (i.e. different instruments); 2) new ability for effective homo-musical separation for piano, flute, and trumpet duets, which to our best knowledge has never been achieved with alternative methods. Project page: http://music-gesture.csail.mit.edu.",0
"Remarkable advancements have been made in deep learning methods for separating sound from visual stimuli. However, these methods primarily rely on optical flow and appearance-based features, which are limited in their capacity to link audio signals with visual cues - particularly when separating multiple instruments of the same type, like multiple violins in a single scene. To overcome this limitation, we propose a new method called ""Music Gesture,"" which is based on structured representations of the body and finger movements of musicians as they perform music. Our approach uses a context-aware graph network to combine visual semantic context with body dynamics, and then applies an audio-visual fusion model to correlate body movements with corresponding audio signals. We evaluated our method on three different music performance datasets and found that it significantly outperformed existing methods in separating different instruments. Moreover, we achieved successful separation of piano, flute, and trumpet duets - an accomplishment not previously attained by other methods. Further details about our project can be found at http://music-gesture.csail.mit.edu.",1
"We study the energy minimization problem in low-level vision tasks from a novel perspective. We replace the heuristic regularization term with a learnable subspace constraint, and preserve the data term to exploit domain knowledge derived from the first principle of a task. This learning subspace minimization (LSM) framework unifies the network structures and the parameters for many low-level vision tasks, which allows us to train a single network for multiple tasks simultaneously with completely shared parameters, and even generalizes the trained network to an unseen task as long as its data term can be formulated. We demonstrate our LSM framework on four low-level tasks including interactive image segmentation, video segmentation, stereo matching, and optical flow, and validate the network on various datasets. The experiments show that the proposed LSM generates state-of-the-art results with smaller model size, faster training convergence, and real-time inference.",0
"Our approach to studying the energy minimization problem in low-level vision tasks is innovative. Instead of using a heuristic regularization term, we employ a learnable subspace constraint and retain the data term to leverage domain knowledge obtained from a task's first principle. By utilizing this learning subspace minimization (LSM) framework, we can unify the network structures and parameters for multiple low-level vision tasks, enabling us to train a single network for several tasks simultaneously with fully shared parameters. Furthermore, the trained network can generalize to an unseen task, provided its data term can be formulated. We tested our LSM framework on four low-level tasks, namely interactive image segmentation, video segmentation, stereo matching, and optical flow, and assessed its efficacy on various datasets. The results demonstrate that our LSM produces state-of-the-art outcomes with a smaller model size, faster training convergence, and real-time inference.",1
"Object tracking and 3D reconstruction are often performed together, with tracking used as input for reconstruction. However, the obtained reconstructions also provide useful information for improving tracking. We propose a novel method that closes this loop, first tracking to reconstruct, and then reconstructing to track. Our approach, MOTSFusion (Multi-Object Tracking, Segmentation and dynamic object Fusion), exploits the 3D motion extracted from dynamic object reconstructions to track objects through long periods of complete occlusion and to recover missing detections. Our approach first builds up short tracklets using 2D optical flow, and then fuses these into dynamic 3D object reconstructions. The precise 3D object motion of these reconstructions is used to merge tracklets through occlusion into long-term tracks, and to locate objects when detections are missing. On KITTI, our reconstruction-based tracking reduces the number of ID switches of the initial tracklets by more than 50%, and outperforms all previous approaches for both bounding box and segmentation tracking.",0
"The combination of object tracking and 3D reconstruction is common, where tracking is utilized as input for reconstruction. However, the obtained reconstructions can also enhance tracking accuracy. Our proposed method, MOTSFusion (Multi-Object Tracking, Segmentation and dynamic object Fusion), presents a novel approach that closes this loop by first reconstructing to track and then tracking to reconstruct. Our method utilizes the 3D motion obtained from dynamic object reconstructions to track objects during instances of complete occlusion and to recover missing detections. Initially, our approach builds short tracklets using 2D optical flow, which are then fused into dynamic 3D object reconstructions. The precise 3D object motion is then used to merge tracklets through occlusion, forming long-term tracks, and to locate objects when detections are absent. Our reconstruction-based tracking approach reduces the number of ID switches of initial tracklets by over 50% on KITTI, outperforming all previous methods for both bounding box and segmentation tracking.",1
"Semi-supervised video object segmentation aims to separate a target object from a video sequence, given the mask in the first frame. Most of current prevailing methods utilize information from additional modules trained in other domains like optical flow and instance segmentation, and as a result they do not compete with other methods on common ground. To address this issue, we propose a simple yet strong transductive method, in which additional modules, datasets, and dedicated architectural designs are not needed. Our method takes a label propagation approach where pixel labels are passed forward based on feature similarity in an embedding space. Different from other propagation methods, ours diffuses temporal information in a holistic manner which take accounts of long-term object appearance. In addition, our method requires few additional computational overhead, and runs at a fast $\sim$37 fps speed. Our single model with a vanilla ResNet50 backbone achieves an overall score of 72.3 on the DAVIS 2017 validation set and 63.1 on the test set. This simple yet high performing and efficient method can serve as a solid baseline that facilitates future research. Code and models are available at \url{https://github.com/microsoft/transductive-vos.pytorch}.",0
"The objective of semi-supervised video object segmentation is to isolate a target object from a video sequence using the mask from the first frame. Many of the existing approaches rely on information from supplementary modules trained in unrelated domains, such as optical flow and instance segmentation, making it difficult to compare them using the same criteria. To address this issue, we have developed a straightforward yet powerful transductive technique that does not require additional modules, datasets, or specialized architectural designs. Our method adopts a label propagation technique that forwards pixel labels based on feature similarity in an embedding space. Unlike other propagation methods, ours integrates long-term object appearance by diffusing temporal information in a holistic way. Furthermore, our method requires minimal computational overhead and operates at a fast speed of about 37 fps. Our single model, which employs a plain ResNet50 backbone, achieves an overall score of 72.3 on the DAVIS 2017 validation set and 63.1 on the test set. This efficient and high-performing method can serve as a strong benchmark for future studies. You can access the code and models at \url{https://github.com/microsoft/transductive-vos.pytorch}.",1
"Scene flow estimation has been receiving increasing attention for 3D environment perception. Monocular scene flow estimation -- obtaining 3D structure and 3D motion from two temporally consecutive images -- is a highly ill-posed problem, and practical solutions are lacking to date. We propose a novel monocular scene flow method that yields competitive accuracy and real-time performance. By taking an inverse problem view, we design a single convolutional neural network (CNN) that successfully estimates depth and 3D motion simultaneously from a classical optical flow cost volume. We adopt self-supervised learning with 3D loss functions and occlusion reasoning to leverage unlabeled data. We validate our design choices, including the proxy loss and augmentation setup. Our model achieves state-of-the-art accuracy among unsupervised/self-supervised learning approaches to monocular scene flow, and yields competitive results for the optical flow and monocular depth estimation sub-tasks. Semi-supervised fine-tuning further improves the accuracy and yields promising results in real-time.",0
"The estimation of scene flow has become increasingly important for understanding 3D environments. However, the process of monocular scene flow estimation, which involves determining 3D structure and motion from two consecutive images, is a challenging task that lacks practical solutions. In order to address this issue, we present a new approach to monocular scene flow estimation that delivers both high accuracy and real-time performance. Our method is based on a single convolutional neural network that uses an optical flow cost volume to estimate depth and 3D motion together. To improve our model's performance, we incorporate self-supervised learning and 3D loss functions, as well as occlusion reasoning to make use of unlabeled data. Our model achieves state-of-the-art accuracy for unsupervised/self-supervised approaches to monocular scene flow, and also performs competitively for optical flow and monocular depth estimation. By fine-tuning our model with semi-supervised learning, we are able to further improve its accuracy and achieve promising results in real-time.",1
"Correspondence estimation is one of the most widely researched and yet only partially solved area of computer vision with many applications in tracking, mapping, recognition of objects and environment. In this paper, we propose a novel way to estimate dense correspondence on an RGB image where visual descriptors are learned from video examples by training a fully convolutional network. Most deep learning methods solve this by training the network with a large set of expensive labeled data or perform labeling through strong 3D generative models using RGB-D videos. Our method learns from RGB videos using contrastive loss, where relative labeling is estimated from optical flow. We demonstrate the functionality in a quantitative analysis on rendered videos, where ground truth information is available. Not only does the method perform well on test data with the same background, it also generalizes to situations with a new background. The descriptors learned are unique and the representations determined by the network are global. We further show the applicability of the method to real-world videos.",0
"The field of computer vision has extensively studied correspondence estimation, which has various applications such as tracking, mapping, object recognition, and environment identification. This research proposes a new approach to estimating dense correspondence on RGB images, utilizing visual descriptors learned from video examples through a fully convolutional network. Unlike other deep learning methods that require expensive labeled data or strong 3D generative models using RGB-D videos, our method employs contrastive loss to learn from RGB videos and estimate relative labeling from optical flow. Our approach is evaluated quantitatively on rendered videos, where ground truth information is available. The method performs well on test data with the same background and can generalize to new backgrounds. The descriptors learned are unique, and the network's representations are global. The method's applicability is further demonstrated through real-world videos.",1
"In classic video action recognition, labels may not contain enough information about the diverse video appearance and dynamics, thus, existing models that are trained under the standard supervised learning paradigm may extract less generalizable features. We evaluate these models under a cross-dataset experiment setting, as the above label bias problem in video analysis is even more prominent across different data sources. We find that using the optical flows as model inputs harms the generalization ability of most video recognition models.   Based on these findings, we present a multi-task learning paradigm for video classification. Our key idea is to avoid label bias and improve the generalization ability by taking data as its own supervision or supervising constraints on the data. First, we take the optical flows and the RGB frames by taking them as auxiliary supervisions, and thus naming our model as Reversed Two-Stream Networks (Rev2Net). Further, we collaborate the auxiliary flow prediction task and the frame reconstruction task by introducing a new training objective to Rev2Net, named Decoding Discrepancy Penalty (DDP), which constraints the discrepancy of the multi-task features in a self-supervised manner. Rev2Net is shown to be effective on the classic action recognition task. It specifically shows a strong generalization ability in the cross-dataset experiments.",0
"The standard supervised learning paradigm in classic video action recognition may result in less generalizable features due to insufficient information in labels about the diverse appearance and dynamics of videos. This issue is further exacerbated when models are evaluated across different data sources. We discovered that using optical flows as model inputs can harm the generalization ability of most video recognition models. To address this problem, we propose a multi-task learning approach that avoids label bias and enhances generalization by using data as its own supervision and supervising constraints on the data. Our model, called Reversed Two-Stream Networks (Rev2Net), takes both optical flows and RGB frames as auxiliary supervisions. We introduce a new training objective, named Decoding Discrepancy Penalty (DDP), to constrain the discrepancy of multi-task features in a self-supervised manner by collaborating the auxiliary flow prediction task and the frame reconstruction task. Our experiments show that Rev2Net is effective in classic action recognition tasks and exhibits strong generalization ability across datasets.",1
"Feature warping is a core technique in optical flow estimation; however, the ambiguity caused by occluded areas during warping is a major problem that remains unsolved. In this paper, we propose an asymmetric occlusion-aware feature matching module, which can learn a rough occlusion mask that filters useless (occluded) areas immediately after feature warping without any explicit supervision. The proposed module can be easily integrated into end-to-end network architectures and enjoys performance gains while introducing negligible computational cost. The learned occlusion mask can be further fed into a subsequent network cascade with dual feature pyramids with which we achieve state-of-the-art performance. At the time of submission, our method, called MaskFlownet, surpasses all published optical flow methods on the MPI Sintel, KITTI 2012 and 2015 benchmarks. Code is available at https://github.com/microsoft/MaskFlownet.",0
"Optical flow estimation employs feature warping as a fundamental technique, but the challenge of ambiguity caused by occluded regions during warping is a significant issue that remains unresolved. This study presents an asymmetric occlusion-aware feature matching module that can learn an approximate occlusion mask to instantly filter out useless (occluded) areas after feature warping without explicit supervision. This module can be conveniently integrated into end-to-end network architectures and yields improved performance with minimal computational cost. Furthermore, the learned occlusion mask can be fed into a subsequent network cascade with dual feature pyramids, resulting in state-of-the-art performance. Our approach, MaskFlownet, outperforms all other optical flow methods on the MPI Sintel, KITTI 2012, and 2015 benchmarks at the time of submission. The code for MaskFlownet is available at https://github.com/microsoft/MaskFlownet.",1
"Akin to many subareas of computer vision, the recent advances in deep learning have also significantly influenced the literature on optical flow. Previously, the literature had been dominated by classical energy-based models, which formulate optical flow estimation as an energy minimization problem. However, as the practical benefits of Convolutional Neural Networks (CNNs) over conventional methods have become apparent in numerous areas of computer vision and beyond, they have also seen increased adoption in the context of motion estimation to the point where the current state of the art in terms of accuracy is set by CNN approaches. We first review this transition as well as the developments from early work to the current state of CNNs for optical flow estimation. Alongside, we discuss some of their technical details and compare them to recapitulate which technical contribution led to the most significant accuracy improvements. Then we provide an overview of the various optical flow approaches introduced in the deep learning age, including those based on alternative learning paradigms (e.g., unsupervised and semi-supervised methods) as well as the extension to the multi-frame case, which is able to yield further accuracy improvements.",0
"The progress in deep learning has had a significant impact on the study of optical flow, much like other facets of computer vision. Until recently, most literature on optical flow estimation focused on energy-based models that minimized energy for estimation. However, Convolutional Neural Networks (CNNs) have proved to be more practical than traditional methods, leading to their increased adoption in motion estimation and the current state-of-the-art in terms of accuracy. We will review this transition, from early work to current CNN approaches, and compare the technical contributions that led to the most significant accuracy improvements. We will also provide an overview of optical flow approaches in the deep learning era, including unsupervised and semi-supervised learning methods, as well as multi-frame approaches that can enhance accuracy even further.",1
"We present a simple and effective deep convolutional neural network (CNN) model for video deblurring. The proposed algorithm mainly consists of optical flow estimation from intermediate latent frames and latent frame restoration steps. It first develops a deep CNN model to estimate optical flow from intermediate latent frames and then restores the latent frames based on the estimated optical flow. To better explore the temporal information from videos, we develop a temporal sharpness prior to constrain the deep CNN model to help the latent frame restoration. We develop an effective cascaded training approach and jointly train the proposed CNN model in an end-to-end manner. We show that exploring the domain knowledge of video deblurring is able to make the deep CNN model more compact and efficient. Extensive experimental results show that the proposed algorithm performs favorably against state-of-the-art methods on the benchmark datasets as well as real-world videos.",0
"Our study introduces a deep convolutional neural network (CNN) model that is both straightforward and efficient in video deblurring. The algorithm consists primarily of two steps: optical flow estimation from intermediate latent frames and restoration of the latent frames. We begin by establishing a deep CNN model to estimate optical flow from intermediate latent frames and subsequently restore latent frames based on the optical flow calculations. To capitalize on the temporal information present in videos, we develop a temporal sharpness prior to constrain the deep CNN model and improve the restoration of the latent frames. We employ an effective cascaded training approach to jointly train the CNN model in an end-to-end manner. Our study demonstrates that incorporating domain knowledge of video deblurring results in a more streamlined and efficient deep CNN model. Our proposed algorithm yields favorable results when compared to state-of-the-art methods on both benchmark datasets and real-world videos.",1
"In this paper, we propose a unified method to jointly learn optical flow and stereo matching. Our first intuition is stereo matching can be modeled as a special case of optical flow, and we can leverage 3D geometry behind stereoscopic videos to guide the learning of these two forms of correspondences. We then enroll this knowledge into the state-of-the-art self-supervised learning framework, and train one single network to estimate both flow and stereo. Second, we unveil the bottlenecks in prior self-supervised learning approaches, and propose to create a new set of challenging proxy tasks to boost performance. These two insights yield a single model that achieves the highest accuracy among all existing unsupervised flow and stereo methods on KITTI 2012 and 2015 benchmarks. More remarkably, our self-supervised method even outperforms several state-of-the-art fully supervised methods, including PWC-Net and FlowNet2 on KITTI 2012.",0
"The aim of this paper is to suggest a comprehensive technique for learning optical flow and stereo matching together. Initially, we believe that stereo matching can be viewed as a specific case of optical flow, and we can use the 3D geometry of stereoscopic videos to aid the learning of these two corresponding forms. This knowledge is then incorporated into the self-supervised learning framework, where we train one network to estimate both flow and stereo. Additionally, we identify the limitations of prior self-supervised learning methods and suggest the creation of a new set of difficult proxy tasks to enhance performance. These two ideas lead to a single model that achieves the highest accuracy among all existing unsupervised flow and stereo methods on KITTI 2012 and 2015 benchmarks. Notably, our self-supervised approach even surpasses several state-of-the-art fully supervised methods, including PWC-Net and FlowNet2 on KITTI 2012.",1
"In dense foggy scenes, existing optical flow methods are erroneous. This is due to the degradation caused by dense fog particles that break the optical flow basic assumptions such as brightness and gradient constancy. To address the problem, we introduce a semi-supervised deep learning technique that employs real fog images without optical flow ground-truths in the training process. Our network integrates the domain transformation and optical flow networks in one framework. Initially, given a pair of synthetic fog images, its corresponding clean images and optical flow ground-truths, in one training batch we train our network in a supervised manner. Subsequently, given a pair of real fog images and a pair of clean images that are not corresponding to each other (unpaired), in the next training batch, we train our network in an unsupervised manner. We then alternate the training of synthetic and real data iteratively. We use real data without ground-truths, since to have ground-truths in such conditions is intractable, and also to avoid the overfitting problem of synthetic data training, where the knowledge learned on synthetic data cannot be generalized to real data testing. Together with the network architecture design, we propose a new training strategy that combines supervised synthetic-data training and unsupervised real-data training. Experimental results show that our method is effective and outperforms the state-of-the-art methods in estimating optical flow in dense foggy scenes.",0
"Optical flow methods are inaccurate in dense foggy settings due to the interference caused by fog particles, which disrupt the basic assumptions of optical flow, such as brightness and gradient constancy. To resolve this issue, we have developed a semi-supervised deep learning approach that incorporates real fog images into the training process, without requiring optical flow ground-truths. Our network integrates domain transformation and optical flow networks into a single framework. We first utilize a supervised method to train our network using synthetic fog images, corresponding clean images, and optical flow ground-truths. Next, we use an unsupervised method to train our network with real fog images and clean images that are not paired. We alternate between training with synthetic and real data to avoid overfitting and to account for the absence of ground-truths in real foggy conditions. Our training strategy, in combination with our network design, produces superior results in estimating optical flow in dense foggy scenes compared to existing state-of-the-art methods.",1
"The widespread adoption of deep learning models places demands on their robustness. In this paper, we consider the robustness of deep neural networks on videos, which comprise both the spatial features of individual frames extracted by a convolutional neural network and the temporal dynamics between adjacent frames captured by a recurrent neural network. To measure robustness, we study the maximum safe radius problem, which computes the minimum distance from the optical flow sequence obtained from a given input to that of an adversarial example in the neighbourhood of the input. We demonstrate that, under the assumption of Lipschitz continuity, the problem can be approximated using finite optimisation via discretising the optical flow space, and the approximation has provable guarantees. We then show that the finite optimisation problem can be solved by utilising a two-player turn-based game in a cooperative setting, where the first player selects the optical flows and the second player determines the dimensions to be manipulated in the chosen flow. We employ an anytime approach to solve the game, in the sense of approximating the value of the game by monotonically improving its upper and lower bounds. We exploit a gradient-based search algorithm to compute the upper bounds, and the admissible A* algorithm to update the lower bounds. Finally, we evaluate our framework on the UCF101 video dataset.",0
"This paper examines the robustness of deep neural networks applied to videos, which includes both spatial features of individual frames and temporal dynamics between adjacent frames. The widespread adoption of deep learning models has placed a demand on their robustness. The study measures robustness by analyzing the maximum safe radius problem, determining the minimum distance between the optical flow sequence of a given input and that of an adversarial example in the input's neighborhood. The problem can be approximated using finite optimization by discretizing the optical flow space and has proven guarantees under Lipschitz continuity assumptions. A two-player turn-based game in a cooperative setting can solve the finite optimization problem, where the first player selects optical flows and the second player decides the dimensions to be manipulated in the chosen flow. An anytime approach solves the game, approximating the value of the game by monotonically improving its upper and lower bounds. The upper bounds are computed using a gradient-based search algorithm, and the lower bounds are updated using the admissible A* algorithm. The framework is evaluated on the UCF101 video dataset.",1
"In this paper, we propose Two-Stream AMTnet, which leverages recent advances in video-based action representation[1] and incremental action tube generation[2]. Majority of the present action detectors follow a frame-based representation, a late-fusion followed by an offline action tube building steps. These are sub-optimal as: frame-based features barely encode the temporal relations; late-fusion restricts the network to learn robust spatiotemporal features; and finally, an offline action tube generation is not suitable for many real-world problems such as autonomous driving, human-robot interaction to name a few. The key contributions of this work are: (1) combining AMTnet's 3D proposal architecture with an online action tube generation technique which allows the model to learn stronger temporal features needed for accurate action detection and facilitates running inference online; (2) an efficient fusion technique allowing the deep network to learn strong spatiotemporal action representations. This is achieved by augmenting the previous Action Micro-Tube (AMTnet) action detection framework in three distinct ways: by adding a parallel motion stIn this paper, we propose a new deep neural network architecture for online action detection, termed ream to the original appearance one in AMTnet; (2) in opposition to state-of-the-art action detectors which train appearance and motion streams separately, and use a test time late fusion scheme to fuse RGB and flow cues, by jointly training both streams in an end-to-end fashion and merging RGB and optical flow features at training time; (3) by introducing an online action tube generation algorithm which works at video-level, and in real-time (when exploiting only appearance features). Two-Stream AMTnet exhibits superior action detection performance over state-of-the-art approaches on the standard action detection benchmarks.",0
"This article proposes a novel approach called Two-Stream AMTnet that builds on recent advancements in action representation and action tube generation. Most current action detectors rely on frame-based representation, offline action tube building, and late-fusion, which have limitations such as poor temporal encoding, restricted learning of spatiotemporal features, and unsuitability for real-world problems. The proposed approach addresses these limitations by combining AMTnet's 3D proposal architecture with an online action tube generation technique, an efficient fusion technique, and a joint training of appearance and motion streams. It also introduces an online action tube generation algorithm that works in real-time. Two-Stream AMTnet outperforms state-of-the-art approaches on standard action detection benchmarks.",1
"We present a learning-based approach for removing unwanted obstructions, such as window reflections, fence occlusions or raindrops, from a short sequence of images captured by a moving camera. Our method leverages the motion differences between the background and the obstructing elements to recover both layers. Specifically, we alternate between estimating dense optical flow fields of the two layers and reconstructing each layer from the flow-warped images via a deep convolutional neural network. The learning-based layer reconstruction allows us to accommodate potential errors in the flow estimation and brittle assumptions such as brightness consistency. We show that training on synthetically generated data transfers well to real images. Our results on numerous challenging scenarios of reflection and fence removal demonstrate the effectiveness of the proposed method.",0
"A technique utilizing learning has been introduced to eliminate unwanted obstructions, such as raindrops, fence occlusions, and window reflections, from a brief sequence of images taken by a moving camera. Our approach exploits the motion disparities between the obstructing elements and the background to recover both layers. We use a deep convolutional neural network to reconstruct each layer from the flow-warped images by alternating between computing dense optical flow fields of the two layers. The learning-based layer reconstruction allows for accommodating errors in flow estimation and brittle assumptions, such as brightness consistency. We demonstrate that our method's training on artificially generated data transfers effectively to real images. The proposed method's effectiveness is demonstrated in solving numerous challenging scenarios of fence and reflection removal.",1
"Event cameras are bio-inspired sensors that asynchronously report intensity changes in microsecond resolution. DAVIS can capture high dynamics of a scene and simultaneously output high temporal resolution events and low frame-rate intensity images. In this paper, we propose a single image (potentially blurred) and events based optical flow estimation approach. First, we demonstrate how events can be used to improve flow estimates. To this end, we encode the relation between flow and events effectively by presenting an event-based photometric consistency formulation. Then, we consider the special case of image blur caused by high dynamics in the visual environments and show that including the blur formation in our model further constrains flow estimation. This is in sharp contrast to existing works that ignore the blurred images while our formulation can naturally handle either blurred or sharp images to achieve accurate flow estimation. Finally, we reduce flow estimation, as well as image deblurring, to an alternative optimization problem of an objective function using the primal-dual algorithm. Experimental results on both synthetic and real data (with blurred and non-blurred images) show the superiority of our model in comparison to state-of-the-art approaches.",0
"Event cameras are sensors that report changes in intensity with microsecond resolution in a bio-inspired manner. DAVIS can capture a scene's high dynamics and output events with high temporal resolution and low frame-rate intensity images simultaneously. This paper proposes an optical flow estimation approach that is based on a single image that may be blurred and events. First, the paper demonstrates how events can improve flow estimates by presenting an event-based photometric consistency formulation that encodes the relation between flow and events effectively. Then, the paper considers image blur caused by high dynamics in visual environments and shows that including the blur formation in the model further constrains flow estimation. This differs from previous works that ignore blurred images, and our formulation can handle both blurred or sharp images for accurate flow estimation. Finally, the paper reduces flow estimation and image deblurring to an alternative optimization problem of an objective function using the primal-dual algorithm. Experimental results on synthetic and real data with blurred and non-blurred images show the superiority of the model compared to state-of-the-art approaches.",1
"Detecting and segmenting individual objects, regardless of their category, is crucial for many applications such as action detection or robotic interaction. While this problem has been well-studied under the classic formulation of spatio-temporal grouping, state-of-the-art approaches do not make use of learning-based methods. To bridge this gap, we propose a simple learning-based approach for spatio-temporal grouping. Our approach leverages motion cues from optical flow as a bottom-up signal for separating objects from each other. Motion cues are then combined with appearance cues that provide a generic objectness prior for capturing the full extent of objects. We show that our approach outperforms all prior work on the benchmark FBMS dataset. One potential worry with learning-based methods is that they might overfit to the particular type of objects that they have been trained on. To address this concern, we propose two new benchmarks for generic, moving object detection, and show that our model matches top-down methods on common categories, while significantly out-performing both top-down and bottom-up methods on never-before-seen categories.",0
"The identification and division of individual objects, regardless of their category, is a critical task for various applications, including action detection and robotic interaction. Although the problem has been extensively explored using the traditional spatio-temporal grouping, current state-of-the-art techniques do not utilize learning-based approaches. To address this gap, we present a straightforward learning-based method for spatio-temporal grouping, which incorporates motion cues from optical flow as a bottom-up signal to distinguish objects from one another. In addition, appearance cues are combined to provide a generic objectness prior that captures the full range of objects. We demonstrate that our approach surpasses all previous work on the FBMS dataset. One potential concern regarding learning-based methods is that they may overfit to specific types of objects on which they have been trained. To address this concern, we suggest two new benchmarks for identifying generic, moving objects and demonstrate that our model performs similarly to top-down methods for common categories, while outperforming both top-down and bottom-up methods for previously unseen categories.",1
"Encoder-decoder networks have found widespread use in various dense prediction tasks. However, the strong reduction of spatial resolution in the encoder leads to a loss of location information as well as boundary artifacts. To address this, image-adaptive post-processing methods have shown beneficial by leveraging the high-resolution input image(s) as guidance data. We extend such approaches by considering an important orthogonal source of information: the network's confidence in its own predictions. We introduce probabilistic pixel-adaptive convolutions (PPACs), which not only depend on image guidance data for filtering, but also respect the reliability of per-pixel predictions. As such, PPACs allow for image-adaptive smoothing and simultaneously propagating pixels of high confidence into less reliable regions, while respecting object boundaries. We demonstrate their utility in refinement networks for optical flow and semantic segmentation, where PPACs lead to a clear reduction in boundary artifacts. Moreover, our proposed refinement step is able to substantially improve the accuracy on various widely used benchmarks.",0
"Various dense prediction tasks have widely utilized encoder-decoder networks. However, the encoder's strong reduction of spatial resolution causes a loss of location information and boundary artifacts. To rectify this issue, image-adaptive post-processing methods have proven beneficial by utilizing high-resolution input image(s) as guidance data. We have enhanced these approaches by incorporating an orthogonal source of information: the network's confidence in its own predictions. We have introduced probabilistic pixel-adaptive convolutions (PPACs), which rely on both image guidance data and the reliability of per-pixel predictions for filtering. PPACs allow for image-adaptive smoothing and propagation of high-confidence pixels into less reliable regions while respecting object boundaries. We have demonstrated the efficacy of PPACs in refinement networks for optical flow and semantic segmentation, where they significantly reduce boundary artifacts and improve accuracy on widely used benchmarks.",1
"Whole understanding of the surroundings is paramount to autonomous systems. Recent works have shown that deep neural networks can learn geometry (depth) and motion (optical flow) from a monocular video without any explicit supervision from ground truth annotations, particularly hard to source for these two tasks. In this paper, we take an additional step toward holistic scene understanding with monocular cameras by learning depth and motion alongside with semantics, with supervision for the latter provided by a pre-trained network distilling proxy ground truth images. We address the three tasks jointly by a) a novel training protocol based on knowledge distillation and self-supervision and b) a compact network architecture which enables efficient scene understanding on both power hungry GPUs and low-power embedded platforms. We thoroughly assess the performance of our framework and show that it yields state-of-the-art results for monocular depth estimation, optical flow and motion segmentation.",0
"Autonomous systems require a complete grasp of their surroundings. Recent studies indicate that deep neural networks can learn geometry and motion from monocular videos without ground truth annotations, which are challenging to obtain. This paper takes a step further by incorporating semantics into the learning of depth and motion, using a pre-trained network to provide supervision. The training protocol combines knowledge distillation and self-supervision, and the network architecture is designed for efficient processing on both high-powered GPUs and low-power embedded platforms. The framework is evaluated extensively and found to produce superior results for monocular depth estimation, optical flow, and motion segmentation.",1
"In this work we contribute a novel pipeline to automatically generate training data, and to improve over state-of-the-art multi-object tracking and segmentation (MOTS) methods. Our proposed track mining algorithm turns raw street-level videos into high-fidelity MOTS training data, is scalable and overcomes the need of expensive and time-consuming manual annotation approaches. We leverage state-of-the-art instance segmentation results in combination with optical flow predictions, also trained on automatically harvested training data. Our second major contribution is MOTSNet - a deep learning, tracking-by-detection architecture for MOTS - deploying a novel mask-pooling layer for improved object association over time. Training MOTSNet with our automatically extracted data leads to significantly improved sMOTSA scores on the novel KITTI MOTS dataset (+1.9%/+7.5% on cars/pedestrians), and MOTSNet improves by +4.1% over previously best methods on the MOTSChallenge dataset. Our most impressive finding is that we can improve over previous best-performing works, even in complete absence of manually annotated MOTS training data.",0
"Our work introduces a new pipeline that automatically generates training data and enhances existing multi-object tracking and segmentation (MOTS) methods. Our proposed track mining algorithm transforms unprocessed street videos into precise MOTS training data that is scalable and eliminates the need for costly and time-consuming manual annotation. Our approach combines state-of-the-art instance segmentation outcomes with optical flow predictions, both trained on automatically extracted training data. Furthermore, we present MOTSNet, a deep learning tracking-by-detection structure for MOTS, which employs a novel mask-pooling layer to improve object identification over time. Using our automatically gathered data to train MOTSNet results in significant improvements in sMOTSA scores on the KITTI MOTS dataset (+1.9%/+7.5% for cars/pedestrians). On the MOTSChallenge dataset, MOTSNet outperforms previous methods by +4.1%. Notably, our research demonstrates that we can surpass the performance of previous approaches even in the absence of manually annotated MOTS training data.",1
"High-quality 3D reconstructions from endoscopy video play an important role in many clinical applications, including surgical navigation where they enable direct video-CT registration. While many methods exist for general multi-view 3D reconstruction, these methods often fail to deliver satisfactory performance on endoscopic video. Part of the reason is that local descriptors that establish pair-wise point correspondences, and thus drive reconstruction, struggle when confronted with the texture-scarce surface of anatomy. Learning-based dense descriptors usually have larger receptive fields enabling the encoding of global information, which can be used to disambiguate matches. In this work, we present an effective self-supervised training scheme and novel loss design for dense descriptor learning. In direct comparison to recent local and dense descriptors on an in-house sinus endoscopy dataset, we demonstrate that our proposed dense descriptor can generalize to unseen patients and scopes, thereby largely improving the performance of Structure from Motion (SfM) in terms of model density and completeness. We also evaluate our method on a public dense optical flow dataset and a small-scale SfM public dataset to further demonstrate the effectiveness and generality of our method. The source code is available at https://github.com/lppllppl920/DenseDescriptorLearning-Pytorch.",0
"Endoscopy video 3D reconstructions are crucial in various clinical applications, particularly surgical navigation, where they facilitate direct video-CT registration. Despite the availability of many methods for multi-view 3D reconstruction, these techniques often fail to provide satisfactory results for endoscopic video. One of the reasons for this failure is that local descriptors, which establish pair-wise point correspondences and drive reconstruction, struggle when dealing with the texture-scarce surfaces of anatomy. Dense descriptors that are learned based on global information can be used to disambiguate matches since they have larger receptive fields. This study introduces a self-supervised training scheme and novel loss design for dense descriptor learning, which significantly improves the performance of Structure from Motion (SfM) in terms of model density and completeness. Our proposed dense descriptor can generalize to unseen patients and scopes, as demonstrated in a direct comparison with recent local and dense descriptors on an in-house sinus endoscopy dataset. Additionally, we evaluate our method on a public dense optical flow dataset and a small-scale SfM public dataset to further demonstrate its effectiveness and generality. The source code is available at https://github.com/lppllppl920/DenseDescriptorLearning-Pytorch.",1
"Particle Imaging Velocimetry (PIV) estimates the flow of fluid by analyzing the motion of injected particles. The problem is challenging as the particles lie at different depths but have similar appearance and tracking a large number of particles is particularly difficult. In this paper, we present a PIV solution that uses densely sampled light field to reconstruct and track 3D particles. We exploit the refocusing capability and focal symmetry constraint of the light field for reliable particle depth estimation. We further propose a new motion-constrained optical flow estimation scheme by enforcing local motion rigidity and the Navier-Stoke constraint. Comprehensive experiments on synthetic and real experiments show that using a single light field camera, our technique can recover dense and accurate 3D fluid flows in small to medium volumes.",0
"The estimation of fluid flow through Particle Imaging Velocimetry (PIV) involves analyzing the motion of particles injected into the fluid. This task is challenging because the particles are situated at varying depths, possess similar appearances, and tracking a large number of them is difficult. In this paper, we present a PIV solution that employs densely sampled light field to reconstruct and track 3D particles. To ensure reliable particle depth estimation, we utilize the refocusing capability and focal symmetry constraint of the light field. Additionally, we introduce a novel motion-constrained optical flow estimation scheme that enforces local motion rigidity and the Navier-Stoke constraint. Our experiments demonstrate that by employing a single light field camera, our technique can recover dense and accurate 3D fluid flows in small to medium volumes.",1
"Deep neural networks have been successfully applied to solving the video-based person re-identification problem with impressive results reported. The existing networks for person re-id are designed to extract discriminative features that preserve the identity information. Usually, whole video frames are fed into the neural networks and all the regions in a frame are equally treated. This may be a suboptimal choice because many regions, e.g., background regions in the video, are not related to the person. Furthermore, the person of interest may be occluded by another person or something else. These unrelated regions may hinder person re-identification. In this paper, we introduce a novel gating mechanism to deep neural networks. Our gating mechanism will learn which regions are helpful for person re-identification and let these regions pass the gate. The unrelated background regions or occluding regions are filtered out by the gate. In each frame, the color channels and optical flow channels provide quite different information. To better leverage such information, we generate one gate using the color channels and another gate using the optical flow channels. These two gates are combined to provide a more reliable gate with a novel fusion method. Experimental results on two major datasets demonstrate the performance improvements due to the proposed gating mechanism.",0
"Impressive results have been achieved through successful application of deep neural networks for solving the video-based person re-identification problem. The existing networks are designed to extract discriminative features that preserve identity information by treating all regions in a frame equally. This approach may not be optimal as many background regions in the video are unrelated to the person of interest, resulting in hindrance to person re-identification. To overcome this problem, this paper proposes a novel gating mechanism that filters out irrelevant regions. The mechanism comprises two gates - one generated using color channels and the other using optical flow channels. The two gates are combined using a novel fusion method to provide a more reliable gate. Experimental results on two major datasets demonstrate the effectiveness of the proposed gating mechanism.",1
"This paper presents baseline results for the Third Facial Micro-Expression Grand Challenge (MEGC 2020). Both macro- and micro-expression intervals in CAS(ME)$^2$ and SAMM Long Videos are spotted by employing the method of Main Directional Maximal Difference Analysis (MDMD). The MDMD method uses the magnitude maximal difference in the main direction of optical flow features to spot facial movements. The single-frame prediction results of the original MDMD method are post-processed into reasonable video intervals. The metric F1-scores of baseline results are evaluated: for CAS(ME)$^2$, the F1-scores are 0.1196 and 0.0082 for macro- and micro-expressions respectively, and the overall F1-score is 0.0376; for SAMM Long Videos, the F1-scores are 0.0629 and 0.0364 for macro- and micro-expressions respectively, and the overall F1-score is 0.0445. The baseline project codes are publicly available at https://github.com/HeyingGithub/Baseline-project-for-MEGC2020_spotting.",0
"The aim of this paper is to present the baseline results of the Third Facial Micro-Expression Grand Challenge (MEGC 2020). The paper utilizes the Main Directional Maximal Difference Analysis (MDMD) method to detect both macro- and micro-expression intervals in CAS(ME)$^2$ and SAMM Long Videos. The MDMD method detects facial movements by using the magnitude maximal difference in the main direction of optical flow features. The original MDMD method's single-frame prediction outcomes are processed into rational video intervals. The metric F1-scores of the baseline results are evaluated, with the F1-scores for CAS(ME)$^2$ being 0.1196 and 0.0082 for macro- and micro-expressions, respectively, and an overall F1-score of 0.0376. On the other hand, for SAMM Long Videos, the F1-scores are 0.0629 and 0.0364 for macro- and micro-expressions, respectively, with an overall F1-score of 0.0445. The codes of the baseline project are available to the public at https://github.com/HeyingGithub/Baseline-project-for-MEGC2020_spotting.",1
"Hand hygiene is one of the most significant factors in preventing hospital acquired infections (HAI) which often be transmitted by medical staffs in contact with patients in the operating room (OR). Hand hygiene monitoring could be important to investigate and reduce the outbreak of infections within the OR. However, an effective monitoring tool for hand hygiene compliance is difficult to develop due to the visual complexity of the OR scene. Recent progress in video understanding with convolutional neural net (CNN) has increased the application of recognition and detection of human actions. Leveraging this progress, we proposed a fully automated hand hygiene monitoring tool of the alcohol-based hand rubbing action of anesthesiologists on OR video using spatio-temporal features with 3D CNN. First, the region of interest (ROI) of anesthesiologists' upper body were detected and cropped. A temporal smoothing filter was applied to the ROIs. Then, the ROIs were given to a 3D CNN and classified into two classes: rubbing hands or other actions. We observed that a transfer learning from Kinetics-400 is beneficial and the optical flow stream was not helpful in our dataset. The final accuracy, precision, recall and F1 score in testing is 0.76, 0.85, 0.65 and 0.74, respectively.",0
"Preventing hospital acquired infections (HAI) is crucial, and one significant factor is hand hygiene. Medical staff who come into contact with patients in the operating room (OR) can transmit these infections, making it important to monitor hand hygiene compliance to reduce outbreaks. However, creating an effective monitoring tool is challenging due to the visual complexity of the OR scene. With recent advancements in video understanding through convolutional neural net (CNN), recognizing and detecting human actions has become more feasible. Using this progress, we propose an automated tool to monitor anesthesiologists' alcohol-based hand rubbing action in the OR using spatio-temporal features with 3D CNN. We detect and crop the region of interest (ROI) of the anesthesiologists' upper body, apply a temporal smoothing filter, and classify the ROIs into two classes: rubbing hands or other actions. Our findings show that transfer learning from Kinetics-400 is useful, while the optical flow stream is not helpful in our dataset. In testing, our tool achieved an accuracy of 0.76, precision of 0.85, recall of 0.65, and F1 score of 0.74.",1
"Fine-grained action recognition datasets exhibit environmental bias, where multiple video sequences are captured from a limited number of environments. Training a model in one environment and deploying in another results in a drop in performance due to an unavoidable domain shift. Unsupervised Domain Adaptation (UDA) approaches have frequently utilised adversarial training between the source and target domains. However, these approaches have not explored the multi-modal nature of video within each domain. In this work we exploit the correspondence of modalities as a self-supervised alignment approach for UDA in addition to adversarial alignment.   We test our approach on three kitchens from our large-scale dataset, EPIC-Kitchens, using two modalities commonly employed for action recognition: RGB and Optical Flow. We show that multi-modal self-supervision alone improves the performance over source-only training by 2.4% on average. We then combine adversarial training with multi-modal self-supervision, showing that our approach outperforms other UDA methods by 3%.",0
"Datasets for fine-grained action recognition face an environmental bias since only a limited number of environments are used to capture multiple video sequences. Training a model in one environment and deploying it in another results in performance degradation due to domain shift which is inevitable. Unsatisfactory Domain Adaptation (UDA) approaches have often employed adversarial training between the source and target domains. However, these approaches have not explored the multi-modal nature of video within each domain. In this study, we employ the correspondence of modalities as a self-supervised alignment method for UDA in addition to adversarial alignment. We conducted an experiment on three kitchens from our large-scale dataset, EPIC-Kitchens, using two modalities commonly employed for action recognition: RGB and Optical Flow. We demonstrate that multi-modal self-supervision alone enhances performance by 2.4% on average compared to source-only training. We then integrate adversarial training with multi-modal self-supervision, indicating that our approach surpasses other UDA methods by 3%.",1
"Over four decades, the majority addresses the problem of optical flow estimation using variational methods. With the advance of machine learning, some recent works have attempted to address the problem using convolutional neural network (CNN) and have showed promising results. FlowNet2, the state-of-the-art CNN, requires over 160M parameters to achieve accurate flow estimation. Our LiteFlowNet2 outperforms FlowNet2 on Sintel and KITTI benchmarks, while being 25.3 times smaller in the model size and 3.1 times faster in the running speed. LiteFlowNet2 is built on the foundation laid by conventional methods and resembles the corresponding roles as data fidelity and regularization in variational methods. We compute optical flow in a spatial-pyramid formulation as SPyNet but through a novel lightweight cascaded flow inference. It provides high flow estimation accuracy through early correction with seamless incorporation of descriptor matching. Flow regularization is used to ameliorate the issue of outliers and vague flow boundaries through feature-driven local convolutions. Our network also owns an effective structure for pyramidal feature extraction and embraces feature warping rather than image warping as practiced in FlowNet2 and SPyNet. Comparing to LiteFlowNet, LiteFlowNet2 improves the optical flow accuracy on Sintel Clean by 23.3%, Sintel Final by 12.8%, KITTI 2012 by 19.6%, and KITTI 2015 by 18.8%, while being 2.2 times faster. Our network protocol and trained models are made publicly available on https://github.com/twhui/LiteFlowNet2.",0
"Variational methods have been used by the majority to address the optical flow estimation problem for over four decades. However, recent works have attempted to solve the problem using convolutional neural networks (CNN) due to the emergence of machine learning, with FlowNet2 being the state-of-the-art CNN. While FlowNet2 requires over 160M parameters for accurate flow estimation, our LiteFlowNet2 outperforms it on Sintel and KITTI benchmarks while being 25.3 times smaller in size and 3.1 times faster in running speed. We have built our network on the foundation laid by conventional methods and incorporated data fidelity and regularization roles similar to those in variational methods. LiteFlowNet2 uses a spatial-pyramid formulation to compute optical flow, similar to SPyNet, but through a lightweight cascaded flow inference method. Our network utilizes flow regularization to address the issue of outliers and vague flow boundaries through feature-driven local convolutions. Additionally, our network has an effective structure for pyramidal feature extraction and uses feature warping instead of image warping to improve flow accuracy. Comparing to LiteFlowNet, LiteFlowNet2 shows a significant improvement in optical flow accuracy on Sintel Clean, Sintel Final, KITTI 2012, and KITTI 2015 while being 2.2 times faster. Our network protocol and trained models are publicly available on https://github.com/twhui/LiteFlowNet2.",1
"Pose tracking is an important problem that requires identifying unique human pose-instances and matching them temporally across different frames of a video. However, existing pose tracking methods are unable to accurately model temporal relationships and require significant computation, often computing the tracks offline. We present an efficient Multi-person Pose Tracking method, KeyTrack, that only relies on keypoint information without using any RGB or optical flow information to track human keypoints in real-time. Keypoints are tracked using our Pose Entailment method, in which, first, a pair of pose estimates is sampled from different frames in a video and tokenized. Then, a Transformer-based network makes a binary classification as to whether one pose temporally follows another. Furthermore, we improve our top-down pose estimation method with a novel, parameter-free, keypoint refinement technique that improves the keypoint estimates used during the Pose Entailment step. We achieve state-of-the-art results on the PoseTrack'17 and the PoseTrack'18 benchmarks while using only a fraction of the computation required by most other methods for computing the tracking information.",0
"The identification and matching of unique human pose instances across different frames of a video, known as pose tracking, is a crucial problem. However, current methods for pose tracking lack accuracy in modeling temporal relationships and entail significant computation, often requiring offline tracking. Our KeyTrack method offers an efficient solution to multi-person pose tracking, relying solely on keypoint information without the use of RGB or optical flow information. Our Pose Entailment technique tracks keypoints by sampling a pair of pose estimates from different frames in a video, tokenizing them, and then using a Transformer-based network to classify whether one pose follows another. Additionally, we have improved our top-down pose estimation technique with a novel, parameter-free keypoint refinement method that enhances the keypoint estimates used during the Pose Entailment process. Our method achieves state-of-the-art results on the PoseTrack'17 and the PoseTrack'18 benchmarks while using only a fraction of the computation required by other tracking methods.",1
"It has been proposed by many researchers that combining deep neural networks with graphical models can create more efficient and better regularized composite models. The main difficulties in implementing this in practice are associated with a discrepancy in suitable learning objectives as well as with the necessity of approximations for the inference. In this work we take one of the simplest inference methods, a truncated max-product Belief Propagation, and add what is necessary to make it a proper component of a deep learning model: We connect it to learning formulations with losses on marginals and compute the backprop operation. This BP-Layer can be used as the final or an intermediate block in convolutional neural networks (CNNs), allowing us to design a hierarchical model composing BP inference and CNNs at different scale levels. The model is applicable to a range of dense prediction problems, is well-trainable and provides parameter-efficient and robust solutions in stereo, optical flow and semantic segmentation.",0
"Numerous researchers have suggested that combining graphical models with deep neural networks can lead to the creation of more effective and well-regulated composite models. However, putting this into practice poses several challenges, such as the mismatch between suitable learning objectives and the need for approximations for inference. This study addresses these issues by incorporating a truncated max-product Belief Propagation as a component of a deep learning model. By connecting it to learning formulations with losses on marginals and computing the backprop operation, we create a BP-Layer that can be used as an intermediate or final block in convolutional neural networks. This allows for the creation of a hierarchical model that incorporates BP inference and CNNs at various scale levels, making it applicable to a wide range of dense prediction problems. The model is easy to train and produces robust and parameter-efficient solutions in stereo, optical flow, and semantic segmentation.",1
"Differentiable image sampling in the form of backward warping has seen broad adoption in tasks like depth estimation and optical flow prediction. In contrast, how to perform forward warping has seen less attention, partly due to additional challenges such as resolving the conflict of mapping multiple pixels to the same target location in a differentiable way. We propose softmax splatting to address this paradigm shift and show its effectiveness on the application of frame interpolation. Specifically, given two input frames, we forward-warp the frames and their feature pyramid representations based on an optical flow estimate using softmax splatting. In doing so, the softmax splatting seamlessly handles cases where multiple source pixels map to the same target location. We then use a synthesis network to predict the interpolation result from the warped representations. Our softmax splatting allows us to not only interpolate frames at an arbitrary time but also to fine tune the feature pyramid and the optical flow. We show that our synthesis approach, empowered by softmax splatting, achieves new state-of-the-art results for video frame interpolation.",0
"The technique of using backward warping for differentiable image sampling has become popular in applications such as depth estimation and optical flow prediction. However, the adoption of forward warping has been limited due to challenges like mapping multiple pixels to a single target location in a differentiable manner. Our proposal, softmax splatting, addresses this issue and is effective for frame interpolation. By forward-warping input frames and their feature pyramid representations using optical flow estimates, our method can handle cases where multiple source pixels map to the same target location. We use a synthesis network to predict the interpolation result and our approach allows for fine-tuning of the feature pyramid and optical flow. Our technique achieves state-of-the-art results for video frame interpolation.",1
"In this paper, we proposed an unsupervised learning method for estimating the optical flow between video frames, especially to solve the occlusion problem. Occlusion is caused by the movement of an object or the movement of the camera, defined as when certain pixels are visible in one video frame but not in adjacent frames. Due to the lack of pixel correspondence between frames in the occluded area, incorrect photometric loss calculation can mislead the optical flow training process. In the video sequence, we found that the occlusion in the forward ($t\rightarrow t+1$) and backward ($t\rightarrow t-1$) frame pairs are usually complementary. That is, pixels that are occluded in subsequent frames are often not occluded in the previous frame and vice versa. Therefore, by using this complementarity, a new weighted loss is proposed to solve the occlusion problem. In addition, we calculate gradients in multiple directions to provide richer supervision information. Our method achieves competitive optical flow accuracy compared to the baseline and some supervised methods on KITTI 2012 and 2015 benchmarks. This source code has been released at https://github.com/jianfenglihg/UnOpticalFlow.git.",0
"An unsupervised learning approach for estimating optical flow between video frames was proposed in this study, with a focus on solving the occlusion problem. This phenomenon occurs when certain pixels are visible in one frame but not in adjacent ones due to object or camera movement. Incorrect photometric loss calculation during occlusion can mislead the optical flow training process, as there is no pixel correspondence between frames in the affected area. Our research determined that occlusions in forward ($t\rightarrow t+1$) and backward ($t\rightarrow t-1$) frame pairs are typically complementary, meaning that pixels not visible in subsequent frames are often visible in the previous frame and vice versa. Utilizing this complementarity, we created a new weighted loss function to address the occlusion problem. We also calculated gradients in multiple directions to provide more comprehensive supervision information. Our method demonstrated competitive optical flow accuracy compared to the baseline and some supervised methods on KITTI 2012 and 2015 benchmarks. The source code is available at https://github.com/jianfenglihg/UnOpticalFlow.git.",1
"Depth from a monocular video can enable billions of devices and robots with a single camera to see the world in 3D. In this paper, we present an approach with a differentiable flow-to-depth layer for video depth estimation. The model consists of a flow-to-depth layer, a camera pose refinement module, and a depth fusion network. Given optical flow and camera pose, our flow-to-depth layer generates depth proposals and the corresponding confidence maps by explicitly solving an epipolar geometry optimization problem. Our flow-to-depth layer is differentiable, and thus we can refine camera poses by maximizing the aggregated confidence in the camera pose refinement module. Our depth fusion network can utilize depth proposals and their confidence maps inferred from different adjacent frames to produce the final depth map. Furthermore, the depth fusion network can additionally take the depth proposals generated by other methods to improve the results further. The experiments on three public datasets show that our approach outperforms state-of-the-art depth estimation methods, and has reasonable cross dataset generalization capability: our model trained on KITTI still performs well on the unseen Waymo dataset.",0
"In this paper, we present a novel method for estimating depth from monocular video that has the potential to revolutionize the way billions of devices and robots perceive the world in 3D. Our approach involves the use of a differentiable flow-to-depth layer that generates depth proposals and corresponding confidence maps by solving an epipolar geometry optimization problem based on optical flow and camera pose. The model also includes a camera pose refinement module that maximizes the aggregated confidence of the flow-to-depth layer, and a depth fusion network that utilizes depth proposals and their confidence maps from adjacent frames to produce the final depth map. Moreover, our depth fusion network can incorporate depth proposals generated by other methods to further improve the results. Our experiments on three public datasets demonstrate that our method outperforms state-of-the-art depth estimation techniques and exhibits reasonable cross-dataset generalization capability, as evidenced by its success when applied to the unseen Waymo dataset after being trained on KITTI.",1
"Applications of satellite data in areas such as weather tracking and modeling, ecosystem monitoring, wildfire detection, and land-cover change are heavily dependent on the trade-offs to spatial, spectral and temporal resolutions of observations. In weather tracking, high-frequency temporal observations are critical and used to improve forecasts, study severe events, and extract atmospheric motion, among others. However, while the current generation of geostationary satellites have hemispheric coverage at 10-15 minute intervals, higher temporal frequency observations are ideal for studying mesoscale severe weather events. In this work, we apply a task specific optical flow approach to temporal up-sampling using deep convolutional neural networks. We apply this technique to 16-bands of GOES-R/Advanced Baseline Imager mesoscale dataset to temporally enhance full disk hemispheric snapshots of different spatial resolutions from 15 minutes to 1 minute. Experiments show the effectiveness of task specific optical flow and multi-scale blocks for interpolating high-frequency severe weather events relative to bilinear and global optical flow baselines. Lastly, we demonstrate strong performance in capturing variability during a convective precipitation events.",0
"The utilization of satellite data in numerous fields, such as weather tracking and modeling, ecosystem monitoring, wildfire detection, and land-cover change, heavily relies on the spatial, spectral, and temporal resolutions of observations. In weather tracking, frequent temporal observations are crucial for improving forecasts, studying severe occurrences, and extracting atmospheric motion. However, while the current geostationary satellites have hemispheric coverage at 10-15 minute intervals, higher temporal frequency observations are more suitable for examining mesoscale severe weather events. In this work, we employ a specialized optical flow method to up-sample temporal data utilizing deep convolutional neural networks. We use this technique to enhance the temporal resolution of full disk hemispheric snapshots with different spatial resolutions from 15 minutes to 1 minute, using 16-bands of GOES-R/Advanced Baseline Imager mesoscale data. Our experiments indicate the effectiveness of task-specific optical flow and multi-scale blocks in interpolating high-frequency severe weather events compared to bilinear and global optical flow methods. Lastly, we demonstrate excellent performance in capturing variability during convective precipitation events.",1
"We address the problem of joint optical flow and camera motion estimation in rigid scenes by incorporating geometric constraints into an unsupervised deep learning framework. Unlike existing approaches which rely on brightness constancy and local smoothness for optical flow estimation, we exploit the global relationship between optical flow and camera motion using epipolar geometry. In particular, we formulate the prediction of optical flow and camera motion as a bi-level optimization problem, consisting of an upper-level problem to estimate the flow that conforms to the predicted camera motion, and a lower-level problem to estimate the camera motion given the predicted optical flow. We use implicit differentiation to enable back-propagation through the lower-level geometric optimization layer independent of its implementation, allowing end-to-end training of the network. With globally-enforced geometric constraints, we are able to improve the quality of the estimated optical flow in challenging scenarios and obtain better camera motion estimates compared to other unsupervised learning methods.",0
"Our focus is on tackling the challenge of jointly estimating optical flow and camera motion in rigid scenes. To achieve this, we have integrated geometric constraints into a deep learning framework that is unsupervised. Our approach differs from existing methods that rely on brightness constancy and local smoothness for optical flow estimation. Instead, we leverage epipolar geometry to establish a global relationship between optical flow and camera motion. Specifically, we have formulated a bi-level optimization problem to predict optical flow and camera motion. The upper-level task involves estimating the flow that aligns with the predicted camera motion, while the lower-level task involves estimating the camera motion based on the predicted optical flow. We have utilized implicit differentiation to enable back-propagation through the lower-level geometric optimization layer, thereby facilitating end-to-end training of the network. Our approach with globally-enforced geometric constraints has led to improved optical flow estimation in challenging scenarios and better camera motion estimates than other unsupervised learning methods.",1
"People identification in video based on the way they walk (i.e. gait) is a relevant task in computer vision using a non-invasive approach. Standard and current approaches typically derive gait signatures from sequences of binary energy maps of subjects extracted from images, but this process introduces a large amount of non-stationary noise, thus, conditioning their efficacy. In contrast, in this paper we focus on the raw pixels, or simple functions derived from them, letting advanced learning techniques to extract relevant features. Therefore, we present a comparative study of different Convolutional Neural Network (CNN) architectures by using three different modalities (i.e. gray pixels, optical flow channels and depth maps) on two widely-adopted and challenging datasets: TUM-GAID and CASIA-B. In addition, we perform a comparative study between different early and late fusion methods used to combine the information obtained from each kind of modalities. Our experimental results suggest that (i) the raw pixel values represent a competitive input modality, compared to the traditional state-of-the-art silhouette-based features (e.g. GEI), since equivalent or better results are obtained; (ii) the fusion of the raw pixel information with information from optical flow and depth maps allows to obtain state-of-the-art results on the gait recognition task with an image resolution several times smaller than the previously reported results; and, (iii) the selection and the design of the CNN architecture are critical points that can make a difference between state-of-the-art results or poor ones.",0
"The identification of individuals in videos based on their gait is a relevant task in computer vision that employs a non-invasive approach. Current approaches rely on deriving gait signatures from sequences of binary energy maps of subjects extracted from images, which introduces significant non-stationary noise, affecting their effectiveness. In contrast, this study examines the use of raw pixels or simple functions derived from them, allowing advanced learning techniques to extract relevant features. The study compares different Convolutional Neural Network (CNN) architectures using three modalities on two widely-adopted datasets. Additionally, it compares different methods of early and late fusion used to combine information from each modality. Results suggest that raw pixel values are competitive with traditional state-of-the-art silhouette-based features, and fusion of pixel information with optical flow and depth maps yields state-of-the-art results with an image resolution several times smaller than previously reported results. Finally, the selection and design of the CNN architecture are critical for achieving state-of-the-art results.",1
"This paper proposes a simple yet effective method for human action recognition in video. The proposed method separately extracts local appearance and motion features using state-of-the-art three-dimensional convolutional neural networks from sampled snippets of a video. These local features are then concatenated to form global representations which are then used to train a linear SVM to perform the action classification using full context of the video, as partial context as used in previous works. The videos undergo two simple proposed preprocessing techniques, optical flow scaling and crop filling. We perform an extensive evaluation on three common benchmark dataset to empirically show the benefit of the SVM, and the two preprocessing steps.",0
"In this paper, a straightforward approach to recognizing human actions in videos is presented. The approach involves extracting local appearance and motion features using advanced three-dimensional convolutional neural networks from selected segments of a video. The local features are combined to create global representations, which are subsequently utilized to train a linear SVM for action classification that uses the complete context of the video rather than partial context as in previous studies. The videos are subjected to two basic preprocessing methods, namely, optical flow scaling and crop filling. To demonstrate the advantages of the SVM and the two preprocessing techniques, a thorough assessment is conducted on three standard benchmark datasets.",1
"While the satellite-based Global Positioning System (GPS) is adequate for some outdoor applications, many other applications are held back by its multi-meter positioning errors and poor indoor coverage. In this paper, we study the feasibility of real-time video-based localization on resource-constrained platforms. Before commencing a localization task, a video-based localization system downloads an offline model of a restricted target environment, such as a set of city streets, or an indoor shopping mall. The system is then able to localize the user within the model, using only video as input.   To enable such a system to run on resource-constrained embedded systems or smartphones, we (a) propose techniques for efficiently building a 3D model of a surveyed path, through frame selection and efficient feature matching, (b) substantially reduce model size by multiple compression techniques, without sacrificing localization accuracy, (c) propose efficient and concurrent techniques for feature extraction and matching to enable online localization, (d) propose a method with interleaved feature matching and optical flow based tracking to reduce the feature extraction and matching time in online localization.   Based on an extensive set of both indoor and outdoor videos, manually annotated with location ground truth, we demonstrate that sub-meter accuracy, at real-time rates, is achievable on smart-phone type platforms, despite challenging video conditions.",0
"Although the Global Positioning System (GPS) that relies on satellites suffices for certain outdoor applications, its imprecise positioning errors of several meters and limited indoor coverage pose obstacles for many other applications. This research examines the feasibility of video-based real-time localization on platforms with limited resources. Prior to initiating the localization task, the video-based localization system downloads an offline model of a restricted target environment, such as city streets or an indoor shopping mall. The system is then able to locate the user within the model, using only video as input. To enable this system to function on embedded systems or smartphones with limited resources, we propose several techniques, including efficient methods for building a 3D model of a surveyed path, compression techniques to reduce model size, concurrent feature extraction and matching techniques for online localization, and a method that uses interleaved feature matching and optical flow-based tracking to reduce feature extraction and matching time. By analyzing a comprehensive collection of both indoor and outdoor videos annotated with location ground truth, we demonstrate that sub-meter accuracy can be achieved on smartphone-type platforms in real-time, despite challenging video conditions.",1
"In this work we present a monocular visual odometry (VO) algorithm which leverages geometry-based methods and deep learning. Most existing VO/SLAM systems with superior performance are based on geometry and have to be carefully designed for different application scenarios. Moreover, most monocular systems suffer from scale-drift issue.Some recent deep learning works learn VO in an end-to-end manner but the performance of these deep systems is still not comparable to geometry-based methods. In this work, we revisit the basics of VO and explore the right way for integrating deep learning with epipolar geometry and Perspective-n-Point (PnP) method. Specifically, we train two convolutional neural networks (CNNs) for estimating single-view depths and two-view optical flows as intermediate outputs. With the deep predictions, we design a simple but robust frame-to-frame VO algorithm (DF-VO) which outperforms pure deep learning-based and geometry-based methods. More importantly, our system does not suffer from the scale-drift issue being aided by a scale consistent single-view depth CNN. Extensive experiments on KITTI dataset shows the robustness of our system and a detailed ablation study shows the effect of different factors in our system.",0
"This paper presents a visual odometry (VO) algorithm for monocular systems that combines geometry-based methods with deep learning. While most VO/SLAM systems with superior performance are based on geometry and require careful design for different applications, monocular systems often experience scale-drift issues. Although recent deep learning approaches learn VO in an end-to-end manner, their performance does not match that of geometry-based methods. This work revisits the fundamentals of VO and integrates deep learning with epipolar geometry and the Perspective-n-Point (PnP) method. Two convolutional neural networks (CNNs) are trained to estimate single-view depths and two-view optical flows as intermediate outputs. These deep predictions are then used to design a simple yet robust frame-to-frame VO algorithm (DF-VO) that outperforms pure deep learning-based and geometry-based methods. A scale consistent single-view depth CNN ensures that the system does not suffer from scale-drift issues. Comprehensive experiments on the KITTI dataset demonstrate the robustness of the system, and a detailed ablation study shows the impact of different factors.",1
"Motion blurry images challenge many computer vision algorithms, e.g, feature detection, motion estimation, or object recognition. Deep convolutional neural networks are state-of-the-art for image deblurring. However, obtaining training data with corresponding sharp and blurry image pairs can be difficult. In this paper, we present a differentiable reblur model for self-supervised motion deblurring, which enables the network to learn from real-world blurry image sequences without relying on sharp images for supervision. Our key insight is that motion cues obtained from consecutive images yield sufficient information to inform the deblurring task. We therefore formulate deblurring as an inverse rendering problem, taking into account the physical image formation process: we first predict two deblurred images from which we estimate the corresponding optical flow. Using these predictions, we re-render the blurred images and minimize the difference with respect to the original blurry inputs. We use both synthetic and real dataset for experimental evaluations. Our experiments demonstrate that self-supervised single image deblurring is really feasible and leads to visually compelling results.",0
"Computer vision algorithms such as feature detection, motion estimation, and object recognition face challenges with motion blurry images. Although deep convolutional neural networks are currently the most advanced method for image deblurring, obtaining training data with both sharp and blurry image pairs can be problematic. To address this issue, we propose a differentiable reblur model for self-supervised motion deblurring in our paper. This model enables the network to learn from real-world blurry image sequences without relying on sharp images for supervision. Our approach relies on motion cues obtained from consecutive images to provide sufficient information for the deblurring task. We formulate deblurring as an inverse rendering problem by considering the physical image formation process. First, we predict two deblurred images from which we estimate the corresponding optical flow. Using these predictions, we re-render the blurred images and minimize the difference with respect to the original blurry inputs. We evaluate our approach using both synthetic and real datasets, and our experiments show that self-supervised single image deblurring is feasible and produces visually compelling results.",1
"We introduce the first very large detection dataset for event cameras. The dataset is composed of more than 39 hours of automotive recordings acquired with a 304x240 ATIS sensor. It contains open roads and very diverse driving scenarios, ranging from urban, highway, suburbs and countryside scenes, as well as different weather and illumination conditions. Manual bounding box annotations of cars and pedestrians contained in the recordings are also provided at a frequency between 1 and 4Hz, yielding more than 255,000 labels in total. We believe that the availability of a labeled dataset of this size will contribute to major advances in event-based vision tasks such as object detection and classification. We also expect benefits in other tasks such as optical flow, structure from motion and tracking, where for example, the large amount of data can be leveraged by self-supervised learning methods.",0
"We are presenting the first ever detection dataset for event cameras that is significantly large. Our dataset comprises of over 39 hours of automotive recordings that were captured with a 304x240 ATIS sensor. It covers a wide variety of driving scenarios such as open roads, urban, highway, suburbs and countryside scenes, and is recorded under diverse weather and illumination conditions. Our dataset also includes manual bounding box annotations of cars and pedestrians that are present in the recordings, and are provided at a frequency between 1 and 4Hz. This results in over 255,000 labels in total. We believe that the availability of such a labeled dataset will lead to significant progress in event-based vision tasks such as object detection and classification. Additionally, we anticipate benefits in other tasks such as optical flow, structure from motion and tracking, where the large data volume can be utilized by self-supervised learning methods.",1
"With the prevalence of RGB-D cameras, multi-modal video data have become more available for human action recognition. One main challenge for this task lies in how to effectively leverage their complementary information. In this work, we propose a Modality Compensation Network (MCN) to explore the relationships of different modalities, and boost the representations for human action recognition. We regard RGB/optical flow videos as source modalities, skeletons as auxiliary modality. Our goal is to extract more discriminative features from source modalities, with the help of auxiliary modality. Built on deep Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks, our model bridges data from source and auxiliary modalities by a modality adaptation block to achieve adaptive representation learning, that the network learns to compensate for the loss of skeletons at test time and even at training time. We explore multiple adaptation schemes to narrow the distance between source and auxiliary modal distributions from different levels, according to the alignment of source and auxiliary data in training. In addition, skeletons are only required in the training phase. Our model is able to improve the recognition performance with source data when testing. Experimental results reveal that MCN outperforms state-of-the-art approaches on four widely-used action recognition benchmarks.",0
"The availability of multi-modal video data for human action recognition has increased with the prevalence of RGB-D cameras. However, effectively utilizing the complementary information remains a significant challenge. To address this issue, we propose the Modality Compensation Network (MCN) which enhances the representations for human action recognition by exploring the relationships between different modalities. Our model considers RGB/optical flow videos as source modalities and skeletons as auxiliary modality. By utilizing deep Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks, our model adapts to the loss of skeletons at test time and even during training by bridging data from source and auxiliary modalities through a modality adaptation block. Multiple adaptation schemes are explored to reduce the distance between source and auxiliary modal distributions from different levels. Furthermore, skeletons are only required during the training phase. The performance of our model surpasses state-of-the-art approaches on four widely-used action recognition benchmarks.",1
"Most of Multiple Object Tracking (MOT) approaches compute individual target features for two subtasks: estimating target-wise motions and conducting pair-wise Re-Identification (Re-ID). Because of the indefinite number of targets among video frames, both subtasks are very difficult to scale up efficiently in end-to-end Deep Neural Networks (DNNs). In this paper, we design an end-to-end DNN tracking approach, Flow-Fuse-Tracker (FFT), that addresses the above issues with two efficient techniques: target flowing and target fusing. Specifically, in target flowing, a FlowTracker DNN module learns the indefinite number of target-wise motions jointly from pixel-level optical flows. In target fusing, a FuseTracker DNN module refines and fuses targets proposed by FlowTracker and frame-wise object detection, instead of trusting either of the two inaccurate sources of target proposal. Because FlowTracker can explore complex target-wise motion patterns and FuseTracker can refine and fuse targets from FlowTracker and detectors, our approach can achieve the state-of-the-art results on several MOT benchmarks. As an online MOT approach, FFT produced the top MOTA of 46.3 on the 2DMOT15, 56.5 on the MOT16, and 56.5 on the MOT17 tracking benchmarks, surpassing all the online and offline methods in existing publications.",0
"The majority of Multiple Object Tracking (MOT) methods calculate individual target features for two tasks: estimating target-wise motions and conducting pair-wise Re-Identification (Re-ID). These subtasks are challenging to scale up efficiently in end-to-end Deep Neural Networks (DNNs) due to the indefinite number of targets in video frames. In this study, we introduce an end-to-end DNN tracking approach, Flow-Fuse-Tracker (FFT), that tackles these issues using two effective techniques: target flowing and target fusing. Target flowing involves using a FlowTracker DNN module to jointly learn the indefinite number of target-wise motions from pixel-level optical flows. Target fusing utilizes a FuseTracker DNN module to refine and fuse targets proposed by FlowTracker and frame-wise object detection, rather than relying on either of the two inaccurate sources of target proposal. Our approach can accomplish state-of-the-art results on various MOT benchmarks because FlowTracker can explore complex target-wise motion patterns, and FuseTracker can refine and fuse targets from FlowTracker and detectors. As an online MOT approach, FFT achieved the highest MOTA of 46.3 on the 2DMOT15, 56.5 on the MOT16, and 56.5 on the MOT17 tracking benchmarks, surpassing all existing online and offline methods in publications.",1
"Video denoising is to remove noise from noise-corrupted data, thus recovering true signals via spatiotemporal processing. Existing approaches for spatiotemporal video denoising tend to suffer from motion blur artifacts, that is, the boundary of a moving object tends to appear blurry especially when the object undergoes a fast motion, causing optical flow calculation to break down. In this paper, we address this challenge by designing a first-image-then-video two-stage denoising neural network, consisting of an image denoising module for spatially reducing intra-frame noise followed by a regular spatiotemporal video denoising module. The intuition is simple yet powerful and effective: the first stage of image denoising effectively reduces the noise level and, therefore, allows the second stage of spatiotemporal denoising for better modeling and learning everywhere, including along the moving object boundaries. This two-stage network, when trained in an end-to-end fashion, yields the state-of-the-art performances on the video denoising benchmark Vimeo90K dataset in terms of both denoising quality and computation. It also enables an unsupervised approach that achieves comparable performance to existing supervised approaches.",0
"The aim of video denoising is to eliminate noise from data that has been corrupted by noise in order to restore the true signals through spatiotemporal processing. However, current methods for spatiotemporal video denoising often experience problems with motion blur artifacts, where the edges of moving objects become blurred, especially when the object is moving quickly, causing the breakdown of optical flow calculation. This paper addresses this challenge by creating a two-stage denoising neural network that first reduces intra-frame noise through an image denoising module and then uses a regular spatiotemporal video denoising module. The idea behind this approach is that the first stage of image denoising effectively decreases the noise level, allowing the second stage of spatiotemporal denoising to model and learn better, even along the boundaries of moving objects. Training this two-stage network in an end-to-end way results in the best performance on the Vimeo90K video denoising benchmark dataset in terms of both denoising quality and computation. Additionally, it allows for an unsupervised approach that achieves similar results to existing supervised approaches.",1
"In this paper, we study the value of using synthetically produced videos as training data for neural networks used for action categorization. Motivated by the fact that texture and background of a video play little to no significant roles in optical flow, we generated simplified texture-less and background-less videos and utilized the synthetic data to train a Temporal Segment Network (TSN). The results demonstrated that augmenting TSN with simplified synthetic data improved the original network accuracy (68.5%), achieving 71.8% on HMDB-51 when adding 4,000 videos and 72.4% when adding 8,000 videos. Also, training using simplified synthetic videos alone on 25 classes of UCF-101 achieved 30.71% when trained on 2500 videos and 52.7% when trained on 5000 videos. Finally, results showed that when reducing the number of real videos of UCF-25 to 10% and combining them with synthetic videos, the accuracy drops to only 85.41%, compared to a drop to 77.4% when no synthetic data is added.",0
"The focus of our research is to examine the effectiveness of using artificially generated videos as a means of training neural networks for action categorization. Our motivation stemmed from the realization that background and texture have minimal impact on optical flow. We created videos without these elements and incorporated them into a Temporal Segment Network (TSN) for training. We found that the inclusion of 4,000 or 8,000 synthetic videos improved TSN's accuracy from 68.5% to 71.8% and 72.4%, respectively, on HMDB-51. Additionally, training on 25 classes of UCF-101 using only simplified synthetic videos resulted in accuracy of 30.71% and 52.7% when trained on 2500 and 5000 videos, respectively. Our results demonstrate that combining real videos with synthetic ones yielded an accuracy of 85.41%, compared to 77.4% without the use of synthetic data when reducing the number of UCF-25 real videos to 10%.",1
"The existing approaches for salient motion segmentation are unable to explicitly learn geometric cues and often give false detections on prominent static objects. We exploit multiview geometric constraints to avoid such shortcomings. To handle the nonrigid background like a sea, we also propose a robust fusion mechanism between motion and appearance-based features. We find dense trajectories, covering every pixel in the video, and propose trajectory-based epipolar distances to distinguish between background and foreground regions. Trajectory epipolar distances are data-independent and can be readily computed given a few features' correspondences between the images. We show that by combining epipolar distances with optical flow, a powerful motion network can be learned. Enabling the network to leverage both of these features, we propose a simple mechanism, we call input-dropout. Comparing the motion-only networks, we outperform the previous state of the art on DAVIS-2016 dataset by 5.2% in the mean IoU score. By robustly fusing our motion network with an appearance network using the input-dropout mechanism, we also outperform the previous methods on DAVIS-2016, 2017 and Segtrackv2 dataset.",0
"Current methods for identifying significant motion are insufficient in their ability to learn geometric cues and often produce incorrect detections on stable objects. To overcome this limitation, we utilize multiview geometric constraints. In addition, we propose a robust fusion mechanism between motion and appearance-based features to handle nonrigid backgrounds like the sea. Our approach involves identifying dense trajectories covering every pixel in the video and using trajectory-based epipolar distances to differentiate between background and foreground regions. These distances can be computed easily based on a few feature correspondences. By combining epipolar distances with optical flow, we can train a powerful motion network using a simple input-dropout mechanism. Compared to motion-only networks, our approach outperforms the previous state-of-the-art on the DAVIS-2016 dataset by 5.2% in the mean IoU score. Moreover, by effectively fusing our motion network with an appearance network using input-dropout, we outperform previous methods on the DAVIS-2016, 2017, and Segtrackv2 datasets.",1
"For a long time, the vision community tries to learn the spatio-temporal representation by combining convolutional neural network together with various temporal models, such as the families of Markov chain, optical flow, RNN and temporal convolution. However, these pipelines consume enormous computing resources due to the alternately learning process for spatial and temporal information. One natural question is whether we can embed the temporal information into the spatial one so the information in the two domains can be jointly learned once-only. In this work, we answer this question by presenting a simple yet powerful operator -- temporal interlacing network (TIN). Instead of learning the temporal features, TIN fuses the two kinds of information by interlacing spatial representations from the past to the future, and vice versa. A differentiable interlacing target can be learned to control the interlacing process. In this way, a heavy temporal model is replaced by a simple interlacing operator. We theoretically prove that with a learnable interlacing target, TIN performs equivalently to the regularized temporal convolution network (r-TCN), but gains 4% more accuracy with 6x less latency on 6 challenging benchmarks. These results push the state-of-the-art performances of video understanding by a considerable margin. Not surprising, the ensemble model of the proposed TIN won the $1^{st}$ place in the ICCV19 - Multi Moments in Time challenge. Code is made available to facilitate further research at https://github.com/deepcs233/TIN",0
"The vision community has been attempting to learn spatio-temporal representation for a while by combining convolutional neural networks with various temporal models, like Markov chain, optical flow, RNN, and temporal convolution. However, these approaches require significant computing resources due to their alternating learning process for spatial and temporal information. Therefore, a natural question arises: can we merge the temporal information with the spatial information to enable joint learning of both domains in a one-time process? This work answers this question by introducing a simple yet powerful operator called the temporal interlacing network (TIN). Instead of learning temporal features, TIN blends spatial representations from the past and future by interlacing them, and a differentiable interlacing target can be learned to control the process. This replaces the heavy temporal model with a simple interlacing operator. Theoretically, TIN performs as well as the regularized temporal convolution network (r-TCN), but with 6x less latency and 4% more accuracy on six challenging benchmarks. These results significantly improve the state-of-the-art performances of video understanding. The proposed TIN ensemble model won the $1^{st}$ place in the ICCV19 - Multi Moments in Time challenge. The code is available for further research at https://github.com/deepcs233/TIN.",1
"We consider the problem of unsupervised camera pose estimation. Given an input video sequence, our goal is to estimate the camera pose (i.e. the camera motion) between consecutive frames. Traditionally, this problem is tackled by placing strict constraints on the transformation vector or by incorporating optical flow through a complex pipeline. We propose an alternative approach that utilizes a compositional re-estimation process for camera pose estimation. Given an input, we first estimate a depth map. Our method then iteratively estimates the camera motion based on the estimated depth map. Our approach significantly improves the predicted camera motion both quantitatively and visually. Furthermore, the re-estimation resolves the problem of out-of-boundaries pixels in a novel and simple way. Another advantage of our approach is that it is adaptable to other camera pose estimation approaches. Experimental analysis on KITTI benchmark dataset demonstrates that our method outperforms existing state-of-the-art approaches in unsupervised camera ego-motion estimation.",0
"Our focus is on unsupervised camera pose estimation, wherein we aim to determine the camera's movement between consecutive frames in an input video sequence. Traditionally, this task was accomplished by imposing rigid constraints on the transformation vector or utilizing optical flow through a complicated pipeline. We suggest a fresh approach that utilizes a compositional re-estimation process for camera pose estimation. Initially, we determine a depth map from the input, after which our method iteratively estimates the camera's motion based on the depth map. Our approach significantly improves the accuracy of predicted camera motion both quantitatively and visually, while also addressing the issue of out-of-boundaries pixels in a novel and straightforward manner. Additionally, our approach is adaptable to other camera pose estimation techniques. Our method outperforms existing state-of-the-art approaches in unsupervised camera ego-motion estimation, as demonstrated by experimental analysis on the KITTI benchmark dataset.",1
"Recently, 3D convolutional networks yield good performance in action recognition. However, optical flow stream is still needed to ensure better performance, the cost of which is very high. In this paper, we propose a fast but effective way to extract motion features from videos utilizing residual frames as the input data in 3D ConvNets. By replacing traditional stacked RGB frames with residual ones, 20.5% and 12.5% points improvements over top-1 accuracy can be achieved on the UCF101 and HMDB51 datasets when trained from scratch. Because residual frames contain little information of object appearance, we further use a 2D convolutional network to extract appearance features and combine them with the results from residual frames to form a two-path solution. In three benchmark datasets, our two-path solution achieved better or comparable performances than those using additional optical flow methods, especially outperformed the state-of-the-art models on Mini-kinetics dataset. Further analysis indicates that better motion features can be extracted using residual frames with 3D ConvNets, and our residual-frame-input path is a good supplement for existing RGB-frame-input models.",0
"The utilization of 3D convolutional networks has led to successful action recognition. However, incorporating optical flow stream remains costly. This study presents a fast and effective approach to extract motion features from videos using residual frames as input data in 3D ConvNets. By replacing traditional stacked RGB frames with residual ones, the accuracy of the UCF101 and HMDB51 datasets improves by 20.5% and 12.5% points, respectively, when trained from scratch. Since residual frames contain little information on object appearance, a 2D convolutional network is used to extract appearance features, which are combined with the results from residual frames to form a two-path solution. Our two-path solution performs better or comparably to models utilizing additional optical flow methods on three benchmark datasets, particularly outperforming state-of-the-art models on the Mini-kinetics dataset. Moreover, better motion features can be extracted using residual frames with 3D ConvNets, and our residual-frame-input path is a valuable supplement to existing RGB-frame-input models.",1
"Event-based vision sensors, such as the Dynamic Vision Sensor (DVS), are ideally suited for real-time motion analysis. The unique properties encompassed in the readings of such sensors provide high temporal resolution, superior sensitivity to light and low latency. These properties provide the grounds to estimate motion extremely reliably in the most sophisticated scenarios but they come at a price - modern event-based vision sensors have extremely low resolution and produce a lot of noise. Moreover, the asynchronous nature of the event stream calls for novel algorithms.   This paper presents a new, efficient approach to object tracking with asynchronous cameras. We present a novel event stream representation which enables us to utilize information about the dynamic (temporal) component of the event stream, and not only the spatial component, at every moment of time. This is done by approximating the 3D geometry of the event stream with a parametric model; as a result, the algorithm is capable of producing the motion-compensated event stream (effectively approximating egomotion), and without using any form of external sensors in extremely low-light and noisy conditions without any form of feature tracking or explicit optical flow computation. We demonstrate our framework on the task of independent motion detection and tracking, where we use the temporal model inconsistencies to locate differently moving objects in challenging situations of very fast motion.",0
"Ideal for real-time motion analysis, event-based vision sensors, like the Dynamic Vision Sensor (DVS), possess unique properties such as high temporal resolution, superior light sensitivity, and low latency. These features make it possible to estimate motion with exceptional reliability, even in complex scenarios. However, the sensors have low resolution and produce a lot of noise. Additionally, their asynchronous event stream requires novel algorithms. In this paper, we present an innovative method for object tracking using asynchronous cameras. Our approach uses a novel event stream representation to leverage information about the temporal component of the event stream at every moment. By approximating the 3D geometry of the event stream with a parametric model, our algorithm can produce motion-compensated event streams, effectively approximating egomotion, without using external sensors, feature tracking, or explicit optical flow computation. We demonstrate the effectiveness of our framework in detecting and tracking independently moving objects in challenging situations involving fast motion and temporal model inconsistencies.",1
"Visual odometry is an essential key for a localization module in SLAM systems. However, previous methods require tuning the system to adapt environment changes. In this paper, we propose a learning-based approach for frame-to-frame monocular visual odometry estimation. The proposed network is only learned by disparity maps for not only covering the environment changes but also solving the scale problem. Furthermore, attention block and skip-ordering scheme are introduced to achieve robust performance in various driving environment. Our network is compared with the conventional methods which use common domain such as color or optical flow. Experimental results confirm that the proposed network shows better performance than other approaches with higher and more stable results.",0
"The use of visual odometry is crucial for SLAM systems to accurately localize objects. However, previous methods required constant adjustments to account for changes in the environment. This research paper introduces a learning-based approach for estimating frame-to-frame monocular visual odometry that eliminates the need for constant tuning. The proposed network is trained using disparity maps, which not only adapts to changes in the environment but also solves the problem of scale. To ensure robust performance in different driving environments, an attention block and skip-ordering scheme are incorporated. Compared to conventional methods that rely on color or optical flow, our network yields better and more consistent results. The experimental results confirm the superior performance of our approach.",1
"VBM3D is an extension to video of the well known image denoising algorithm BM3D, which takes advantage of the sparse representation of stacks of similar patches in a transform domain. The extension is rather straightforward: the similar 2D patches are taken from a spatio-temporal neighborhood which includes neighboring frames. In spite of its simplicity, the algorithm offers a good trade-off between denoising performance and computational complexity. In this work we revisit this method, providing an open-source C++ implementation reproducing the results. A detailed description is given and the choice of parameters is thoroughly discussed. Furthermore, we discuss several extensions of the original algorithm: (1) a multi-scale implementation, (2) the use of 3D patches, (3) the use of optical flow to guide the patch search. These extensions allow to obtain results which are competitive with even the most recent state of the art.",0
"VBM3D is a video denoising algorithm that builds on the popular image denoising algorithm, BM3D. The algorithm takes advantage of the sparse representation of similar patches in a transform domain. However, unlike BM3D, VBM3D utilizes 2D patches from a spatio-temporal neighborhood that includes neighboring frames. Despite its simplicity, VBM3D strikes a good balance between denoising performance and computational complexity. In this study, we present an open-source C++ implementation that reproduces the results of the algorithm. We also provide a detailed description of the algorithm and thoroughly discuss the choice of parameters. Additionally, we explore several extensions of the original algorithm, including a multi-scale implementation, the use of 3D patches, and the use of optical flow to guide the patch search. These extensions enable us to achieve competitive results that rival even the most recent state-of-the-art methods.",1
"Video super-resolution (SR) aims at generating a sequence of high-resolution (HR) frames with plausible and temporally consistent details from their low-resolution (LR) counterparts. The key challenge for video SR lies in the effective exploitation of temporal dependency between consecutive frames. Existing deep learning based methods commonly estimate optical flows between LR frames to provide temporal dependency. However, the resolution conflict between LR optical flows and HR outputs hinders the recovery of fine details. In this paper, we propose an end-to-end video SR network to super-resolve both optical flows and images. Optical flow SR from LR frames provides accurate temporal dependency and ultimately improves video SR performance. Specifically, we first propose an optical flow reconstruction network (OFRnet) to infer HR optical flows in a coarse-to-fine manner. Then, motion compensation is performed using HR optical flows to encode temporal dependency. Finally, compensated LR inputs are fed to a super-resolution network (SRnet) to generate SR results. Extensive experiments have been conducted to demonstrate the effectiveness of HR optical flows for SR performance improvement. Comparative results on the Vid4 and DAVIS-10 datasets show that our network achieves the state-of-the-art performance.",0
"The aim of video super-resolution (SR) is to create a sequence of high-resolution (HR) frames that are both plausible and temporally consistent, starting from their low-resolution (LR) counterparts. The biggest challenge in video SR is making the most of the temporal relationship between consecutive frames. Existing deep learning approaches typically estimate optical flows between LR frames to provide temporal consistency. However, the resolution discrepancy between LR optical flows and HR outputs makes it difficult to restore fine details. In this paper, we propose an end-to-end video SR network that can super-resolve both optical flows and images. Accurate temporal consistency is achieved through optical flow SR from LR frames. Specifically, we introduce an optical flow reconstruction network (OFRnet) that infers HR optical flows in a coarse-to-fine manner. Motion compensation is then performed using HR optical flows to encode temporal consistency. Finally, compensated LR inputs are fed to a super-resolution network (SRnet) to generate SR results. Our extensive experiments demonstrate the effectiveness of HR optical flows in improving SR performance. Comparative results on the Vid4 and DAVIS-10 datasets show that our network achieves state-of-the-art performance.",1
"This paper tackles the problem of real-time semantic segmentation of high definition videos using a hybrid GPU / CPU approach. We propose an Efficient Video Segmentation(EVS) pipeline that combines:   (i) On the CPU, a very fast optical flow method, that is used to exploit the temporal aspect of the video and propagate semantic information from one frame to the next. It runs in parallel with the GPU.   (ii) On the GPU, two Convolutional Neural Networks: A main segmentation network that is used to predict dense semantic labels from scratch, and a Refiner that is designed to improve predictions from previous frames with the help of a fast Inconsistencies Attention Module (IAM). The latter can identify regions that cannot be propagated accurately.   We suggest several operating points depending on the desired frame rate and accuracy. Our pipeline achieves accuracy levels competitive to the existing real-time methods for semantic image segmentation(mIoU above 60%), while achieving much higher frame rates. On the popular Cityscapes dataset with high resolution frames (2048 x 1024), the proposed operating points range from 80 to 1000 Hz on a single GPU and CPU.",0
"The main focus of this article is to address the issue of real-time semantic segmentation of high definition videos with the help of a hybrid GPU/CPU approach. Our proposed solution, the Efficient Video Segmentation (EVS) pipeline, combines two different methods. Firstly, a fast optical flow technique is used on the CPU to exploit the temporal aspect of the video and propagate semantic information from one frame to the next in parallel with the GPU. Secondly, two Convolutional Neural Networks are used on the GPU: a main segmentation network that predicts dense semantic labels from scratch and a Refiner that enhances predictions from previous frames with the aid of a fast Inconsistencies Attention Module (IAM) that can identify regions that cannot be accurately propagated. We suggest different operating points based on the required frame rate and accuracy. Our pipeline delivers competitive accuracy levels to the existing real-time methods for semantic image segmentation (mIoU above 60%), while achieving significantly higher frame rates. On the Cityscapes dataset with high resolution frames (2048 x 1024), the operating points range from 80 to 1000 Hz on a single GPU and CPU.",1
"We describe a technique that automatically generates plausible depth maps from videos using non-parametric depth sampling. We demonstrate our technique in cases where past methods fail (non-translating cameras and dynamic scenes). Our technique is applicable to single images as well as videos. For videos, we use local motion cues to improve the inferred depth maps, while optical flow is used to ensure temporal depth consistency. For training and evaluation, we use a Kinect-based system to collect a large dataset containing stereoscopic videos with known depths. We show that our depth estimation technique outperforms the state-of-the-art on benchmark databases. Our technique can be used to automatically convert a monoscopic video into stereo for 3D visualization, and we demonstrate this through a variety of visually pleasing results for indoor and outdoor scenes, including results from the feature film Charade.",0
"Using non-parametric depth sampling, we have developed a technique that can automatically generate believable depth maps from videos. This approach is effective even in situations where conventional methods have failed, such as dynamic scenes and non-translating cameras. Our technique can be applied to both single images and videos, and in the latter case, we utilize local motion cues to enhance the accuracy of the inferred depth maps. To ensure temporal depth consistency, we employ optical flow. We trained and evaluated our method using a Kinect-based system to create a vast dataset comprising stereoscopic videos with known depths. Our depth estimation technique surpasses state-of-the-art solutions on benchmark databases. Additionally, our technique can be used to transform a monoscopic video into stereo for 3D visualization, which we demonstrate with a variety of impressive results, including scenes from the feature film Charade.",1
"We describe a technique that automatically generates plausible depth maps from videos using non-parametric depth sampling. We demonstrate our technique in cases where past methods fail (non-translating cameras and dynamic scenes). Our technique is applicable to single images as well as videos. For videos, we use local motion cues to improve the inferred depth maps, while optical flow is used to ensure temporal depth consistency. For training and evaluation, we use a Kinect-based system to collect a large dataset containing stereoscopic videos with known depths. We show that our depth estimation technique outperforms the state-of-the-art on benchmark databases. Our technique can be used to automatically convert a monoscopic video into stereo for 3D visualization, and we demonstrate this through a variety of visually pleasing results for indoor and outdoor scenes, including results from the feature film Charade.",0
"Using non-parametric depth sampling, we have developed a technique that generates plausible depth maps from videos automatically. Our method is effective even in cases where previous approaches have failed, such as dynamic scenes and non-translating cameras. Moreover, it is suitable for both single images and videos. To enhance the inferred depth maps in videos, we utilize local motion cues while ensuring temporal depth consistency via optical flow. Our approach is evaluated and trained using a Kinect-based system that collects stereoscopic videos with known depths. The results demonstrate that our depth estimation technique outperforms the current state-of-the-art on benchmark databases. Our technique can also convert monoscopic videos into stereo for 3D visualization, as demonstrated through a range of visually stunning results for indoor and outdoor scenes, including the film Charade.",1
"Event cameras provide a number of benefits over traditional cameras, such as the ability to track incredibly fast motions, high dynamic range, and low power consumption. However, their application into computer vision problems, many of which are primarily dominated by deep learning solutions, has been limited by the lack of labeled training data for events. In this work, we propose a method which leverages the existing labeled data for images by simulating events from a pair of temporal image frames, using a convolutional neural network. We train this network on pairs of images and events, using an adversarial discriminator loss and a pair of cycle consistency losses. The cycle consistency losses utilize a pair of pre-trained self-supervised networks which perform optical flow estimation and image reconstruction from events, and constrain our network to generate events which result in accurate outputs from both of these networks. Trained fully end to end, our network learns a generative model for events from images without the need for accurate modeling of the motion in the scene, exhibited by modeling based methods, while also implicitly modeling event noise. Using this simulator, we train a pair of downstream networks on object detection and 2D human pose estimation from events, using simulated data from large scale image datasets, and demonstrate the networks' abilities to generalize to datasets with real events.",0
"Event cameras offer several advantages compared to traditional cameras, such as the ability to track exceptionally fast movements, low power consumption, and high dynamic range. However, their use in computer vision tasks, which are mostly dominated by deep learning solutions, has been limited due to the scarcity of labeled training data for events. To address this issue, we propose a technique that employs labeled image data to simulate events using a convolutional neural network. Our approach involves training the network on pairs of images and events, utilizing an adversarial discriminator loss and a pair of cycle consistency losses. These cycle consistency losses make use of two pre-trained self-supervised networks that perform optical flow estimation and image reconstruction from events, ensuring that our network generates events that produce accurate outputs from both networks. Our end-to-end training process allows the network to learn a generative model for events without the need for precise scene motion modeling, as is the case with modeling-based approaches, while also implicitly modeling event noise. Using this simulator, we train downstream networks for object detection and 2D human pose estimation from events, using artificial data from large-scale image datasets. We demonstrate that these networks can generalize to datasets with genuine events.",1
"Two-stream networks have achieved great success in video recognition. A two-stream network combines a spatial stream of RGB frames and a temporal stream of Optical Flow to make predictions. However, the temporal redundancy of RGB frames as well as the high-cost of optical flow computation creates challenges for both the performance and efficiency. Recent works instead use modern compressed video modalities as an alternative to the RGB spatial stream and improve the inference speed by orders of magnitudes. Previous works create one stream for each modality which are combined with an additional temporal stream through late fusion. This is redundant since some modalities like motion vectors already contain temporal information. Based on this observation, we propose a compressed domain two-stream network IP TSN for compressed video recognition, where the two streams are represented by the two types of frames (I and P frames) in compressed videos, without needing a separate temporal stream. With this goal, we propose to fully exploit the motion information of P-stream through generalized distillation from optical flow, which largely improves the efficiency and accuracy. Our P-stream runs 60 times faster than using optical flow while achieving higher accuracy. Our full IP TSN, evaluated over public action recognition benchmarks (UCF101, HMDB51 and a subset of Kinetics), outperforms other compressed domain methods by large margins while improving the total inference speed by 20%.",0
"Video recognition has seen great success with the use of two-stream networks, which combine a spatial stream of RGB frames and a temporal stream of Optical Flow to make predictions. However, the redundancy of RGB frames and the high cost of optical flow computation have posed challenges for both performance and efficiency. To address this issue, recent works have explored using compressed video modalities instead of the RGB stream to improve inference speed. Previous approaches have utilized one stream for each modality, including motion vectors, which redundantly contain temporal information. Based on this observation, we propose a compressed domain two-stream network, IP TSN, that represents the two streams using the two types of frames in compressed videos (I and P frames) without requiring a separate temporal stream. Our approach fully exploits the motion information of P-stream through generalized distillation from optical flow, improving both efficiency and accuracy. Our P-stream runs 60 times faster than optical flow while achieving higher accuracy. Our full IP TSN outperforms other compressed domain methods over public action recognition benchmarks (UCF101, HMDB51, and a subset of Kinetics) and improves total inference speed by 20%.",1
"High-resolution nowcasting is an essential tool needed for effective adaptation to climate change, particularly for extreme weather. As Deep Learning (DL) techniques have shown dramatic promise in many domains, including the geosciences, we present an application of DL to the problem of precipitation nowcasting, i.e., high-resolution (1 km x 1 km) short-term (1 hour) predictions of precipitation. We treat forecasting as an image-to-image translation problem and leverage the power of the ubiquitous UNET convolutional neural network. We find this performs favorably when compared to three commonly used models: optical flow, persistence and NOAA's numerical one-hour HRRR nowcasting prediction.",0
"An important tool required for effective adaptation to climate change, particularly for extreme weather, is high-resolution nowcasting. Deep Learning (DL) techniques have demonstrated tremendous potential in various fields, including the geosciences. We have applied DL to the problem of predicting precipitation in high-resolution (1 km x 1 km) and short-term (1 hour) timeframes, which is known as precipitation nowcasting. We approach forecasting as an image-to-image translation challenge and utilize the power of the widely-used UNET convolutional neural network. Our findings indicate that this method outperforms three commonly utilized models, including optical flow, persistence, and NOAA's numerical one-hour HRRR nowcasting prediction.",1
"Learning-based visual odometry and SLAM methods demonstrate a steady improvement over past years. However, collecting ground truth poses to train these methods is difficult and expensive. This could be resolved by training in an unsupervised mode, but there is still a large gap between performance of unsupervised and supervised methods. In this work, we focus on generating synthetic data for deep learning-based visual odometry and SLAM methods that take optical flow as an input. We produce training data in a form of optical flow that corresponds to arbitrary camera movement between a real frame and a virtual frame. For synthesizing data we use depth maps either produced by a depth sensor or estimated from stereo pair. We train visual odometry model on synthetic data and do not use ground truth poses hence this model can be considered unsupervised. Also it can be classified as monocular as we do not use depth maps on inference. We also propose a simple way to convert any visual odometry model into a SLAM method based on frame matching and graph optimization. We demonstrate that both the synthetically-trained visual odometry model and the proposed SLAM method build upon this model yields state-of-the-art results among unsupervised methods on KITTI dataset and shows promising results on a challenging EuRoC dataset.",0
"Over the years, learning-based visual odometry and SLAM methods have shown consistent progress. However, obtaining ground truth poses to train these methods is challenging and costly. To overcome this issue, unsupervised training can be used, but there remains a significant performance gap between unsupervised and supervised methods. This study focuses on creating artificial data for deep learning-based visual odometry and SLAM methods that utilize optical flow as an input. We generate training data in the form of optical flow that corresponds to arbitrary camera movement between a real and virtual frame. Depth maps obtained from a depth sensor or stereo pair are employed to synthesize the data. We train a visual odometry model on the artificially created data without ground truth poses, making it an unsupervised and monocular model. We also propose a straightforward method to convert any visual odometry model into a SLAM method based on frame matching and graph optimization. Our results demonstrate that both the synthetically-trained visual odometry model and the proposed SLAM method based on this model outperform other unsupervised methods on the KITTI dataset and show promise on the challenging EuRoC dataset.",1
"Modern approaches for multi-person pose estimation in video require large amounts of dense annotations. However, labeling every frame in a video is costly and labor intensive. To reduce the need for dense annotations, we propose a PoseWarper network that leverages training videos with sparse annotations (every k frames) to learn to perform dense temporal pose propagation and estimation. Given a pair of video frames---a labeled Frame A and an unlabeled Frame B---we train our model to predict human pose in Frame A using the features from Frame B by means of deformable convolutions to implicitly learn the pose warping between A and B. We demonstrate that we can leverage our trained PoseWarper for several applications. First, at inference time we can reverse the application direction of our network in order to propagate pose information from manually annotated frames to unlabeled frames. This makes it possible to generate pose annotations for the entire video given only a few manually-labeled frames. Compared to modern label propagation methods based on optical flow, our warping mechanism is much more compact (6M vs 39M parameters), and also more accurate (88.7% mAP vs 83.8% mAP). We also show that we can improve the accuracy of a pose estimator by training it on an augmented dataset obtained by adding our propagated poses to the original manual labels. Lastly, we can use our PoseWarper to aggregate temporal pose information from neighboring frames during inference. This allows our system to achieve state-of-the-art pose detection results on the PoseTrack2017 and PoseTrack2018 datasets. Code has been made available at: https://github.com/facebookresearch/PoseWarper.",0
"Current methods for estimating the poses of multiple people in videos require extensive annotations. However, manually labeling every frame is both costly and time-consuming. To alleviate the need for dense annotations, we suggest a PoseWarper network that employs sparsely annotated training videos (every k frames) to learn how to perform dense temporal pose estimation and propagation. Our model trains on a labeled Frame A and an unlabeled Frame B, using deformable convolutions to implicitly learn the pose warping between A and B and predict human pose in Frame A with features from Frame B. The trained PoseWarper can be used to propagate pose information from manually labeled frames to unlabeled frames at inference time, allowing for pose annotations for the entire video with just a few manually labeled frames. Compared to optical flow-based label propagation methods, our warping mechanism is more precise and compact. We can also augment the original manual labels with our propagated poses to improve the accuracy of a pose estimator. Lastly, we can use our PoseWarper to aggregate temporal pose information from neighboring frames during inference, leading to state-of-the-art pose detection results on the PoseTrack2017 and PoseTrack2018 datasets. Source code is available at https://github.com/facebookresearch/PoseWarper.",1
"The optical flow of humans is well known to be useful for the analysis of human action. Recent optical flow methods focus on training deep networks to approach the problem. However, the training data used by them does not cover the domain of human motion. Therefore, we develop a dataset of multi-human optical flow and train optical flow networks on this dataset. We use a 3D model of the human body and motion capture data to synthesize realistic flow fields in both single- and multi-person images. We then train optical flow networks to estimate human flow fields from pairs of images. We demonstrate that our trained networks are more accurate than a wide range of top methods on held-out test data and that they can generalize well to real image sequences. The code, trained models and the dataset are available for research.",0
"The analysis of human action can benefit greatly from the optical flow of humans. Recently, deep networks have been trained to tackle this problem. However, the training data used by these networks is insufficient as it does not encompass the domain of human motion. To address this issue, we have created a dataset that includes multi-human optical flow and trained optical flow networks on it. We have utilized a 3D model of the human body and motion capture data to produce realistic flow fields in both single and multi-person images. These networks have been trained to estimate human flow fields from pairs of images. Our results show that our trained networks outperform a variety of top methods on held-out test data, and they can generalize well to real image sequences. Our code, trained models, and dataset are available for research purposes.",1
"Moving Object Detection (MOD) is a critical task for autonomous vehicles as moving objects represent higher collision risk than static ones. The trajectory of the ego-vehicle is planned based on the future states of detected moving objects. It is quite challenging as the ego-motion has to be modelled and compensated to be able to understand the motion of the surrounding objects. In this work, we propose a real-time end-to-end CNN architecture for MOD utilizing spatio-temporal context to improve robustness. We construct a novel time-aware architecture exploiting temporal motion information embedded within sequential images in addition to explicit motion maps using optical flow images.We demonstrate the impact of our algorithm on KITTI dataset where we obtain an improvement of 8% relative to the baselines. We compare our algorithm with state-of-the-art methods and achieve competitive results on KITTI-Motion dataset in terms of accuracy at three times better run-time. The proposed algorithm runs at 23 fps on a standard desktop GPU targeting deployment on embedded platforms.",0
"Detecting moving objects is a crucial task for autonomous vehicles as they pose a higher risk of collision compared to stationary objects. To plan the trajectory of the vehicle, the motion of detected moving objects must be taken into account. This is challenging as the movement of the vehicle must also be considered to understand the motion of surrounding objects. In this study, we propose a real-time end-to-end CNN architecture for Moving Object Detection that utilizes spatio-temporal context to increase robustness. Our novel time-aware architecture incorporates temporal motion information from sequential images and explicit motion maps using optical flow images. Our algorithm improves the accuracy of detection by 8% relative to baselines on KITTI dataset. We also achieve competitive results on KITTI-Motion dataset with three times better run-time when compared to state-of-the-art methods. The proposed algorithm is designed to run at 23 fps on a standard desktop GPU and is suitable for deployment on embedded platforms.",1
"Deep learning-based video salient object detection has recently achieved great success with its performance significantly outperforming any other unsupervised methods. However, existing data-driven approaches heavily rely on a large quantity of pixel-wise annotated video frames to deliver such promising results. In this paper, we address the semi-supervised video salient object detection task using pseudo-labels. Specifically, we present an effective video saliency detector that consists of a spatial refinement network and a spatiotemporal module. Based on the same refinement network and motion information in terms of optical flow, we further propose a novel method for generating pixel-level pseudo-labels from sparsely annotated frames. By utilizing the generated pseudo-labels together with a part of manual annotations, our video saliency detector learns spatial and temporal cues for both contrast inference and coherence enhancement, thus producing accurate saliency maps. Experimental results demonstrate that our proposed semi-supervised method even greatly outperforms all the state-of-the-art fully supervised methods across three public benchmarks of VOS, DAVIS, and FBMS.",0
"Recently, deep learning-based video salient object detection has been highly successful and has surpassed other unsupervised methods in terms of performance. However, current data-driven approaches rely heavily on a large number of pixel-wise annotated video frames to achieve these promising results. This paper focuses on the semi-supervised video salient object detection task using pseudo-labels. An effective video saliency detector is presented, which includes a spatial refinement network and a spatiotemporal module. Additionally, a novel method for generating pixel-level pseudo-labels from sparsely annotated frames is proposed, which utilizes the same refinement network and motion information in terms of optical flow. By combining the generated pseudo-labels with a portion of manual annotations, the video saliency detector learns spatial and temporal cues for both contrast inference and coherence enhancement. As a result, accurate saliency maps are produced. Experimental results show that the proposed semi-supervised method outperforms all state-of-the-art fully supervised methods for three public benchmarks of VOS, DAVIS, and FBMS.",1
"A major challenge for video semantic segmentation is the lack of labeled data. In most benchmark datasets, only one frame of a video clip is annotated, which makes most supervised methods fail to utilize information from the rest of the frames. To exploit the spatio-temporal information in videos, many previous works use pre-computed optical flows, which encode the temporal consistency to improve the video segmentation. However, the video segmentation and optical flow estimation are still considered as two separate tasks. In this paper, we propose a novel framework for joint video semantic segmentation and optical flow estimation. Semantic segmentation brings semantic information to handle occlusion for more robust optical flow estimation, while the non-occluded optical flow provides accurate pixel-level temporal correspondences to guarantee the temporal consistency of the segmentation. Moreover, our framework is able to utilize both labeled and unlabeled frames in the video through joint training, while no additional calculation is required in inference. Extensive experiments show that the proposed model makes the video semantic segmentation and optical flow estimation benefit from each other and outperforms existing methods under the same settings in both tasks.",0
"The dearth of labeled data poses a significant obstacle in accomplishing video semantic segmentation. In most benchmark datasets, merely one frame of a video clip is annotated, causing supervised methods to falter in utilizing the information from the other frames. To tackle this issue, many previous studies employ precomputed optical flows to exploit the spatio-temporal information. While this enhances video segmentation, it still treats optical flow estimation and video segmentation as separate tasks. In this study, we propose a new framework that jointly tackles video semantic segmentation and optical flow estimation. This approach utilizes semantic segmentation to handle occlusion, making optical flow estimation more robust. Additionally, it employs non-occluded optical flow to ensure accurate pixel-level temporal correspondences, guaranteeing the temporal consistency of the segmentation. Furthermore, our framework trains jointly with both labeled and unlabeled frames, without requiring extra computation during inference. Experimental results show that our proposed model outperforms existing methods in both tasks, demonstrating the mutual benefits of video semantic segmentation and optical flow estimation.",1
"Majority of state-of-the-art monocular depth estimation methods are supervised learning approaches. The success of such approaches heavily depends on the high-quality depth labels which are expensive to obtain. Some recent methods try to learn depth networks by leveraging unsupervised cues from monocular videos which are easier to acquire but less reliable. In this paper, we propose to resolve this dilemma by transferring knowledge from synthetic videos with easily obtainable ground-truth depth labels. Due to the stylish difference between synthetic and real images, we propose a temporally-consistent domain adaptation (TCDA) approach that simultaneously explores labels in the synthetic domain and temporal constraints in the videos to improve style transfer and depth prediction. Furthermore, we make use of the ground-truth optical flow and pose information in the synthetic data to learn moving mask and pose prediction networks. The learned moving masks can filter out moving regions that produces erroneous temporal constraints and the estimated poses provide better initializations for estimating temporal constraints. Experimental results demonstrate the effectiveness of our method and comparable performance against state-of-the-art.",0
"Most of the advanced techniques for estimating depth from a single image rely on supervised learning, which heavily relies on expensive high-quality depth labels. Some recent methods attempt to utilize unsupervised cues from monocular videos, which are easier to obtain but less reliable. In this paper, we propose a solution to this problem by transferring knowledge from synthetic videos that have easily obtainable ground-truth depth labels. However, due to the difference in style between synthetic and real images, we introduce a temporally-consistent domain adaptation (TCDA) approach that simultaneously exploits labels in the synthetic domain and temporal constraints in the videos to improve style transfer and depth prediction. Additionally, we use the ground-truth optical flow and pose information in the synthetic data to learn moving mask and pose prediction networks. The moving masks can filter out moving regions that produce erroneous temporal constraints, while the estimated poses provide better initializations for estimating temporal constraints. Our experimental results demonstrate the efficacy of our approach, which achieves comparable performance to state-of-the-art methods.",1
"We propose a torus model for high-contrast patches of optical flow. Our model is derived from a database of ground-truth optical flow from the computer-generated video \emph{Sintel}, collected by Butler et al.\ in \emph{A naturalistic open source movie for optical flow evaluation}. Using persistent homology and zigzag persistence, popular tools from the field of computational topology, we show that the high-contrast $3\times 3$ patches from this video are well-modeled by a \emph{torus}, a nonlinear 2-dimensional manifold. Furthermore, we show that the optical flow torus model is naturally equipped with the structure of a fiber bundle, related to the statistics of range image patches.",0
"To model high-contrast patches of optical flow, we propose a torus model. This model is based on the ground-truth optical flow database from the computer-generated video \emph{Sintel}, which was gathered by Butler et al. in \emph{A naturalistic open source movie for optical flow evaluation}. Using computational topology tools like persistent homology and zigzag persistence, we demonstrate that the $3\times 3$ patches with high contrast from this video can be accurately represented by a torus, a 2-dimensional nonlinear manifold. Moreover, we establish that the optical flow torus model naturally possesses the structure of a fiber bundle, which is connected to the characteristics of range image patches.",1
"Scene flow is a challenging task aimed at jointly estimating the 3D structure and motion of the sensed environment. Although deep learning solutions achieve outstanding performance in terms of accuracy, these approaches divide the whole problem into standalone tasks (stereo and optical flow) addressing them with independent networks. Such a strategy dramatically increases the complexity of the training procedure and requires power-hungry GPUs to infer scene flow barely at 1 FPS. Conversely, we propose DWARF, a novel and lightweight architecture able to infer full scene flow jointly reasoning about depth and optical flow easily and elegantly trainable end-to-end from scratch. Moreover, since ground truth images for full scene flow are scarce, we propose to leverage on the knowledge learned by networks specialized in stereo or flow, for which much more data are available, to distill proxy annotations. Exhaustive experiments show that i) DWARF runs at about 10 FPS on a single high-end GPU and about 1 FPS on NVIDIA Jetson TX2 embedded at KITTI resolution, with moderate drop in accuracy compared to 10x deeper models, ii) learning from many distilled samples is more effective than from the few, annotated ones available. Code available at: https://github.com/FilippoAleotti/Dwarf-Tensorflow",0
"The task of scene flow involves estimating both the 3D structure and motion of the environment, which is a difficult challenge. While deep learning methods have shown impressive accuracy, the current approaches divide the problem into separate tasks of stereo and optical flow, each tackled by independent networks. This leads to increased complexity during training and high power consumption, resulting in a slow inference speed of only 1 FPS. In contrast, our proposed DWARF architecture is a lightweight solution that can jointly reason about depth and optical flow, and can be trained end-to-end from scratch easily. As ground truth images for full scene flow are limited, we suggest using networks specialized in stereo or flow to distill proxy annotations. Our experiments reveal that DWARF can run at 10 FPS on a high-end GPU and 1 FPS on NVIDIA Jetson TX2 embedded at KITTI resolution, while still maintaining moderate accuracy compared to deeper models. Additionally, learning from many distilled samples is found to be more effective than from a few annotated ones. The code for DWARF is available at https://github.com/FilippoAleotti/Dwarf-Tensorflow.",1
"Spatial-temporal feature learning is of vital importance for video emotion recognition. Previous deep network structures often focused on macro-motion which extends over long time scales, e.g., on the order of seconds. We believe integrating structures capturing information about both micro- and macro-motion will benefit emotion prediction, because human perceive both micro- and macro-expressions. In this paper, we propose to combine micro- and macro-motion features to improve video emotion recognition with a two-stream recurrent network, named MIMAMO (Micro-Macro-Motion) Net. Specifically, smaller and shorter micro-motions are analyzed by a two-stream network, while larger and more sustained macro-motions can be well captured by a subsequent recurrent network. Assigning specific interpretations to the roles of different parts of the network enables us to make choice of parameters based on prior knowledge: choices that turn out to be optimal. One of the important innovations in our model is the use of interframe phase differences rather than optical flow as input to the temporal stream. Compared with the optical flow, phase differences require less computation and are more robust to illumination changes. Our proposed network achieves state of the art performance on two video emotion datasets, the OMG emotion dataset and the Aff-Wild dataset. The most significant gains are for arousal prediction, for which motion information is intuitively more informative. Source code is available at https://github.com/wtomin/MIMAMO-Net.",0
"Recognizing emotions in videos requires the learning of spatial-temporal features. However, previous deep network structures have mainly focused on macro-motion, which occurs over long periods of time. This approach overlooks the importance of micro-expressions, which are also crucial for emotion prediction as humans perceive both types of motion. To address this issue, we propose a two-stream recurrent network, called MIMAMO Net, that combines micro- and macro-motion features. Our network analyzes smaller and shorter micro-motions using a two-stream network and larger and more sustained macro-motions using a subsequent recurrent network. This model assigns specific interpretations to different parts of the network, allowing us to choose optimal parameters based on prior knowledge. One of the key innovations of our model is the use of interframe phase differences instead of optical flow as input to the temporal stream, which is less computationally intensive and more robust to illumination changes. Our proposed network outperforms other models on two video emotion datasets, namely the OMG emotion dataset and the Aff-Wild dataset, especially in predicting arousal. To facilitate further research, we have made the source code available at https://github.com/wtomin/MIMAMO-Net.",1
"Moving object detection is a critical task for autonomous vehicles. As dynamic objects represent higher collision risk than static ones, our own ego-trajectories have to be planned attending to the future states of the moving elements of the scene. Motion can be perceived using temporal information such as optical flow. Conventional optical flow computation is based on camera sensors only, which makes it prone to failure in conditions with low illumination. On the other hand, LiDAR sensors are independent of illumination, as they measure the time-of-flight of their own emitted lasers. In this work, we propose a robust and real-time CNN architecture for Moving Object Detection (MOD) under low-light conditions by capturing motion information from both camera and LiDAR sensors. We demonstrate the impact of our algorithm on KITTI dataset where we simulate a low-light environment creating a novel dataset ""Dark KITTI"". We obtain a 10.1% relative improvement on Dark-KITTI, and a 4.25% improvement on standard KITTI relative to our baselines. The proposed algorithm runs at 18 fps on a standard desktop GPU using $256\times1224$ resolution images.",0
"Detecting moving objects is a critical task for autonomous vehicles as dynamic objects pose a higher risk of collision than static ones. Therefore, the trajectory of the autonomous vehicle must be planned while considering the future states of the moving elements in the scene. Optical flow can be used to perceive motion by using temporal information. However, conventional optical flow computation based on camera sensors is prone to failure in low illumination conditions. In contrast, LiDAR sensors are not affected by illumination as they measure the time-of-flight of their own emitted lasers. This study proposes a real-time CNN architecture that can detect moving objects under low-light conditions by using motion information from both camera and LiDAR sensors. The proposed algorithm was tested on the KITTI dataset, including a new dataset called ""Dark KITTI,"" which simulates a low-light environment. The results show a relative improvement of 10.1% on Dark-KITTI and 4.25% on standard KITTI compared to the baseline. The algorithm can run at 18 fps on a standard desktop GPU using $256\times1224$ resolution images.",1
"It is expensive to generate real-life image labels and there is a domain gap between real-life and simulated images, hence a model trained on the latter cannot adapt to the former. Solving this can totally eliminate the need for labeling real-life datasets completely. Class balanced self-training is one of the existing techniques that attempt to reduce the domain gap. Moreover, augmenting RGB with flow maps has improved performance in simple semantic segmentation and geometry is preserved across domains. Hence, by augmenting images with dense optical flow map, domain adaptation in semantic segmentation can be improved.",0
"Generating labels for real-life images is costly, and there is a discrepancy between real-life and simulated images, making it difficult for a model trained on the latter to adjust to the former. If this problem is resolved, the need to label real-life datasets can be eliminated. To bridge the domain gap, techniques such as class balanced self-training have been developed. Additionally, augmenting RGB with flow maps has been shown to enhance performance in basic semantic segmentation, and geometry is maintained across domains. Thus, incorporating dense optical flow maps in image augmentation can enhance domain adaptation in semantic segmentation.",1
"In this paper we present a novel approach for depth map enhancement from an RGB-D video sequence. The basic idea is to exploit the shading information in the color image. Instead of making assumption about surface albedo or controlled object motion and lighting, we use the lighting variations introduced by casual object movement. We are effectively calculating photometric stereo from a moving object under natural illuminations. The key technical challenge is to establish correspondences over the entire image set. We therefore develop a lighting insensitive robust pixel matching technique that out-performs optical flow method in presence of lighting variations. In addition we present an expectation-maximization framework to recover the surface normal and albedo simultaneously, without any regularization term. We have validated our method on both synthetic and real datasets to show its superior performance on both surface details recovery and intrinsic decomposition.",0
"A new approach to enhance depth maps from RGB-D video sequences is introduced in this paper. The approach utilizes shading information from the color image instead of making assumptions about surface albedo or controlled object motion and lighting. The lighting variations introduced by object movement are used to effectively calculate photometric stereo under natural illuminations. Correspondences are established over the entire image set using a lighting-insensitive, robust pixel matching technique that outperforms optical flow methods in the presence of lighting variations. An expectation-maximization framework is also presented to recover surface normal and albedo simultaneously without the need for regularization terms. The method's superior performance in recovering surface details and intrinsic decomposition is demonstrated through validation on both synthetic and real datasets.",1
"Synthetic visual data can provide practically infinite diversity and rich labels, while avoiding ethical issues with privacy and bias. However, for many tasks, current models trained on synthetic data generalize poorly to real data. The task of 3D human pose estimation is a particularly interesting example of this sim2real problem, because learning-based approaches perform reasonably well given real training data, yet labeled 3D poses are extremely difficult to obtain in the wild, limiting scalability. In this paper, we show that standard neural-network approaches, which perform poorly when trained on synthetic RGB images, can perform well when the data is pre-processed to extract cues about the person's motion, notably as optical flow and the motion of 2D keypoints. Therefore, our results suggest that motion can be a simple way to bridge a sim2real gap when video is available. We evaluate on the 3D Poses in the Wild dataset, the most challenging modern benchmark for 3D pose estimation, where we show full 3D mesh recovery that is on par with state-of-the-art methods trained on real 3D sequences, despite training only on synthetic humans from the SURREAL dataset.",0
"The use of synthetic visual data can offer a vast array of diversity and detailed labels, all while avoiding ethical concerns surrounding privacy and bias. However, current models trained on this type of data may not generalize well to real-world scenarios for certain tasks. An example of this issue can be seen in the difficulty of obtaining labeled 3D poses in natural environments, limiting the scalability of learning-based approaches for 3D human pose estimation. This paper explores the use of pre-processing techniques to extract motion cues, such as optical flow and 2D keypoints, to bridge the gap between synthetic and real data. The results demonstrate that this method can be effective in producing accurate 3D mesh recovery, even when trained solely on synthetic humans. The 3D Poses in the Wild dataset was used to evaluate the performance, which proved to be on par with state-of-the-art methods trained on real 3D sequences.",1
"Architecture optimization, which is a technique for finding an efficient neural network that meets certain requirements, generally reduces to a set of multiple-choice selection problems among alternative sub-structures or parameters. The discrete nature of the selection problem, however, makes this optimization difficult. To tackle this problem we introduce a novel concept of a trainable gate function. The trainable gate function, which confers a differentiable property to discretevalued variables, allows us to directly optimize loss functions that include non-differentiable discrete values such as 0-1 selection. The proposed trainable gate can be applied to pruning. Pruning can be carried out simply by appending the proposed trainable gate functions to each intermediate output tensor followed by fine-tuning the overall model, using any gradient-based training methods. So the proposed method can jointly optimize the selection of the pruned channels while fine-tuning the weights of the pruned model at the same time. Our experimental results demonstrate that the proposed method efficiently optimizes arbitrary neural networks in various tasks such as image classification, style transfer, optical flow estimation, and neural machine translation.",0
"The process of architecture optimization involves identifying an efficient neural network that meets specific criteria, typically through selecting from multiple alternatives in terms of sub-structures or parameters. However, the discrete nature of this selection process can pose challenges for optimization. To address this issue, we have developed a new trainable gate function that imparts differentiability to discrete variables. This enables the optimization of loss functions incorporating non-differentiable 0-1 selection values. Our trainable gate can also be used for pruning, by appending it to intermediate output tensors and fine-tuning the model using gradient-based training methods. This method jointly optimizes channel selection and weight pruning, as demonstrated in our experiments across various tasks such as image classification, style transfer, optical flow estimation, and neural machine translation.",1
"Generating temporal action proposals remains a very challenging problem, where the main issue lies in predicting precise temporal proposal boundaries and reliable action confidence in long and untrimmed real-world videos. In this paper, we propose an efficient and unified framework to generate temporal action proposals named Dense Boundary Generator (DBG), which draws inspiration from boundary-sensitive methods and implements boundary classification and action completeness regression for densely distributed proposals. In particular, the DBG consists of two modules: Temporal boundary classification (TBC) and Action-aware completeness regression (ACR). The TBC aims to provide two temporal boundary confidence maps by low-level two-stream features, while the ACR is designed to generate an action completeness score map by high-level action-aware features. Moreover, we introduce a dual stream BaseNet (DSB) to encode RGB and optical flow information, which helps to capture discriminative boundary and actionness features. Extensive experiments on popular benchmarks ActivityNet-1.3 and THUMOS14 demonstrate the superiority of DBG over the state-of-the-art proposal generator (e.g., MGG and BMN). Our code will be made available upon publication.",0
"The problem of generating temporal action proposals is extremely challenging due to the difficulty of predicting accurate temporal boundaries and reliable action confidence in lengthy and unedited real-world videos. This paper proposes a unified and efficient framework for generating temporal action proposals called Dense Boundary Generator (DBG), which is inspired by boundary-sensitive methods and incorporates boundary classification and action completeness regression for densely distributed proposals. The DBG comprises two modules: Temporal boundary classification (TBC) and Action-aware completeness regression (ACR). TBC provides two temporal boundary confidence maps using low-level two-stream features, while ACR generates an action completeness score map using high-level action-aware features. Additionally, a dual stream BaseNet (DSB) is introduced to encode RGB and optical flow information, which helps capture discriminative boundary and actionness features. Extensive experiments on popular benchmarks ActivityNet-1.3 and THUMOS14 demonstrate that DBG outperforms state-of-the-art proposal generators such as MGG and BMN. The code will be made available after publication.",1
"Many video enhancement algorithms rely on optical flow to register frames in a video sequence. Precise flow estimation is however intractable; and optical flow itself is often a sub-optimal representation for particular video processing tasks. In this paper, we propose task-oriented flow (TOFlow), a motion representation learned in a self-supervised, task-specific manner. We design a neural network with a trainable motion estimation component and a video processing component, and train them jointly to learn the task-oriented flow. For evaluation, we build Vimeo-90K, a large-scale, high-quality video dataset for low-level video processing. TOFlow outperforms traditional optical flow on standard benchmarks as well as our Vimeo-90K dataset in three video processing tasks: frame interpolation, video denoising/deblocking, and video super-resolution.",0
"The use of optical flow in video enhancement algorithms is common, but precise flow estimation is difficult and optical flow is not always the best representation for certain video processing tasks. This paper introduces a new motion representation called task-oriented flow (TOFlow), which is learned in a self-supervised manner specific to the task at hand. A neural network is designed with a trainable motion estimation component and video processing component to jointly learn the TOFlow. The Vimeo-90K dataset is created for evaluating the performance of TOFlow in low-level video processing tasks, such as frame interpolation, video denoising/deblocking, and video super-resolution. Results show that TOFlow outperforms traditional optical flow on standard benchmarks and the Vimeo-90K dataset.",1
"In recent years, artificial intelligence (AI) based on deep learning (DL) has sparked tremendous global interest. DL is widely used today and has expanded into various interesting areas. It is becoming more popular in cross-subject research, such as studies of smart city systems, which combine computer science with engineering applications. Human action detection is one of these areas. Human action detection is an interesting challenge due to its stringent requirements in terms of computing speed and accuracy. High-accuracy real-time object tracking is also considered a significant challenge. This paper integrates the YOLO detection network, which is considered a state-of-the-art tool for real-time object detection, with motion vectors and the Coyote Optimization Algorithm (COA) to construct a real-time human action localization and tracking system. The proposed system starts with the extraction of motion information from a compressed video stream and the extraction of appearance information from RGB frames using an object detector. Then, a fusion step between the two streams is performed, and the results are fed into the proposed action tracking model. The COA is used in object tracking due to its accuracy and fast convergence. The basic foundation of the proposed model is the utilization of motion vectors, which already exist in a compressed video bit stream and provide sufficient information to improve the localization of the target action without requiring high consumption of computational resources compared with other popular methods of extracting motion information, such as optical flows. This advantage allows the proposed approach to be implemented in challenging environments where the computational resources are limited, such as Internet of Things (IoT) systems.",0
"In recent times, there has been a surge of interest globally in artificial intelligence (AI) that is based on deep learning (DL). DL is being widely used in various intriguing areas and has gained popularity in cross-subject research, such as the study of smart city systems that merge computer science with engineering applications. One such area is human action detection, which presents a unique challenge due to its need for computing speed and accuracy. Real-time object tracking with high accuracy is also a significant challenge. To tackle this, the authors of this paper propose a real-time human action localization and tracking system that integrates the YOLO detection network, motion vectors, and the Coyote Optimization Algorithm (COA). The proposed system extracts motion and appearance information from a compressed video stream and RGB frames, respectively, and fuses the results to feed into the action tracking model. The COA is employed in object tracking due to its fast convergence and accuracy. The proposed model leverages motion vectors, which are available in a compressed video bit stream, and require minimal computational resources compared to other methods like optical flows. This makes the proposed system suitable for use in environments with limited computational resources, such as the Internet of Things (IoT) systems.",1
"Fine-grained action detection is an important task with numerous applications in robotics and human-computer interaction. Existing methods typically utilize a two-stage approach including extraction of local spatio-temporal features followed by temporal modeling to capture long-term dependencies. While most recent papers have focused on the latter (long-temporal modeling), here, we focus on producing features capable of modeling fine-grained motion more efficiently. We propose a novel locally-consistent deformable convolution, which utilizes the change in receptive fields and enforces a local coherency constraint to capture motion information effectively. Our model jointly learns spatio-temporal features (instead of using independent spatial and temporal streams). The temporal component is learned from the feature space instead of pixel space, e.g. optical flow. The produced features can be flexibly used in conjunction with other long-temporal modeling networks, e.g. ST-CNN, DilatedTCN, and ED-TCN. Overall, our proposed approach robustly outperforms the original long-temporal models on two fine-grained action datasets: 50 Salads and GTEA, achieving F1 scores of 80.22% and 75.39% respectively.",0
"The detection of fine-grained actions is crucial for various applications such as human-computer interaction and robotics. Current methods typically involve a two-stage process, where local spatio-temporal features are extracted and temporal modeling is used to capture long-term dependencies. While recent studies have mainly focused on long-term modeling, we concentrate on creating features that can efficiently model fine-grained motion. Our approach introduces a novel locally-consistent deformable convolution that enforces a local coherency constraint and utilizes changes in receptive fields to capture motion information effectively. Our model jointly learns spatio-temporal features rather than independent spatial and temporal streams. Instead of pixel space, we learn the temporal component from the feature space, i.e., optical flow. The features generated can be used flexibly with other long-temporal modeling networks, such as ST-CNN, DilatedTCN, and ED-TCN. Our proposed method outperforms the original long-temporal models on two fine-grained action datasets: 50 Salads and GTEA, achieving F1 scores of 80.22% and 75.39%, respectively.",1
"Inspired by the cognitive process of humans and animals, Curriculum Learning (CL) trains a model by gradually increasing the difficulty of the training data. In this paper, we study whether CL can be applied to complex geometry problems like estimating monocular Visual Odometry (VO). Unlike existing CL approaches, we present a novel CL strategy for learning the geometry of monocular VO by gradually making the learning objective more difficult during training. To this end, we propose a novel geometry-aware objective function by jointly optimizing relative and composite transformations over small windows via bounded pose regression loss. A cascade optical flow network followed by recurrent network with a differentiable windowed composition layer, termed CL-VO, is devised to learn the proposed objective. Evaluation on three real-world datasets shows superior performance of CL-VO over state-of-the-art feature-based and learning-based VO.",0
"The process of Curriculum Learning (CL) is modeled after how humans and animals learn, gradually increasing the difficulty of training data. This study explores whether CL can be used for complex geometry problems, specifically estimating monocular Visual Odometry (VO). Unlike previous CL methods, a new strategy is presented for learning monocular VO geometry by gradually increasing the learning objective difficulty during training. A novel geometry-aware objective function is proposed, optimizing relative and composite transformations over small windows using bounded pose regression loss. A CL-VO network is developed, consisting of a cascade optical flow network and recurrent network with a differentiable windowed composition layer to learn the proposed objective. Evaluation on three datasets demonstrates that CL-VO outperforms existing feature-based and learning-based VO models.",1
"The deep learning-based visual tracking algorithms such as MDNet achieve high performance leveraging to the feature extraction ability of a deep neural network. However, the tracking efficiency of these trackers is not very high due to the slow feature extraction for each frame in a video. In this paper, we propose an effective tracking algorithm to alleviate the time-consuming problem. Specifically, we design a deep flow collaborative network, which executes the expensive feature network only on sparse keyframes and transfers the feature maps to other frames via optical flow. Moreover, we raise an effective adaptive keyframe scheduling mechanism to select the most appropriate keyframe. We evaluate the proposed approach on large-scale datasets: OTB2013 and OTB2015. The experiment results show that our algorithm achieves considerable speedup and high precision as well.",0
"High performance is achieved by deep learning-based visual tracking algorithms such as MDNet, which leverage the feature extraction ability of deep neural networks. However, these trackers suffer from low tracking efficiency due to the slow feature extraction process for each video frame. In this study, we present an effective tracking algorithm to address this time-consuming issue. Specifically, we introduce a deep flow collaborative network that only executes the feature network on sparse keyframes and transfers feature maps to other frames using optical flow. Additionally, we propose an adaptive keyframe scheduling mechanism to select the most suitable keyframe. We evaluate our approach on large-scale datasets, OTB2013 and OTB2015, and our experiment results demonstrate considerable speedup and high precision.",1
"Predicting future video frames is extremely challenging, as there are many factors of variation that make up the dynamics of how frames change through time. Previously proposed solutions require complex inductive biases inside network architectures with highly specialized computation, including segmentation masks, optical flow, and foreground and background separation. In this work, we question if such handcrafted architectures are necessary and instead propose a different approach: finding minimal inductive bias for video prediction while maximizing network capacity. We investigate this question by performing the first large-scale empirical study and demonstrate state-of-the-art performance by learning large models on three different datasets: one for modeling object interactions, one for modeling human motion, and one for modeling car driving.",0
"It is an immense challenge to predict upcoming video frames due to numerous variables that contribute to the dynamics of how frames alter over time. Previously suggested solutions entail intricate inductive biases within network architectures that necessitate specialized computing, such as segmentation masks, optical flow, and foreground and background separation. However, in this study, we question whether such pre-designed architectures are indispensable and present an alternative approach: searching for minimal inductive bias in video prediction while maximizing network capacity. To investigate this hypothesis, we conducted the first extensive empirical study and achieved state-of-the-art performance by training extensive models on three distinct datasets: one for object interactions, one for human motion, and one for car driving.",1
"The paper addresses the problem of motion saliency in videos, that is, identifying regions that undergo motion departing from its context. We propose a new unsupervised paradigm to compute motion saliency maps. The key ingredient is the flow inpainting stage. Candidate regions are determined from the optical flow boundaries. The residual flow in these regions is given by the difference between the optical flow and the flow inpainted from the surrounding areas. It provides the cue for motion saliency. The method is flexible and general by relying on motion information only. Experimental results on the DAVIS 2016 benchmark demonstrate that the method compares favourably with state-of-the-art video saliency methods.",0
"The article discusses the issue of identifying motion saliency in videos, which involves detecting areas that exhibit motion distinct from their surroundings. A new unsupervised approach to generating motion saliency maps is proposed, which involves using a flow inpainting stage. Potential regions are identified based on optical flow boundaries, and the residual flow in these regions is calculated by subtracting the optical flow from the inpainted flow in the surrounding areas. This residual flow serves as an indicator of motion saliency. The method is versatile and relies solely on motion information. Results from experiments conducted on the DAVIS 2016 benchmark illustrate that the method performs favorably when compared to other state-of-the-art video saliency techniques.",1
"Robust and computationally efficient anomaly detection in videos is a problem in video surveillance systems. We propose a technique to increase robustness and reduce computational complexity in a Convolutional Neural Network (CNN) based anomaly detector that utilizes the optical flow information of video data. We reduce the complexity of the network by denoising the intermediate layer outputs of the CNN and by using powers-of-two weights, which replaces the computationally expensive multiplication operations with bit-shift operations. Denoising operation during inference forces small valued intermediate layer outputs to zero. The number of zeros in the network significantly increases as a result of denoising, we can implement the CNN about 10% faster than a comparable network while detecting all the anomalies in the testing set. It turns out that denoising operation also provides robustness because the contribution of small intermediate values to the final result is negligible. During training we also generate motion vector images by a Generative Adversarial Network (GAN) to improve the robustness of the overall system. We experimentally observe that the resulting system is robust to background motion.",0
"Video surveillance systems face a challenge in achieving strong and efficient detection of anomalies in videos. To address this issue, we propose a method that utilizes optical flow information and reduces computational complexity in a Convolutional Neural Network (CNN)-based anomaly detector. Our technique involves denoising the intermediate layer outputs of the CNN and using powers-of-two weights, which replaces costly multiplication operations with bit-shift operations. During inference, the denoising operation forces small intermediate layer outputs to zero, resulting in a significant increase in the number of zeros in the network. This enables us to implement the CNN about 10% faster than a comparable network while detecting all anomalies in the testing set. Moreover, the denoising operation enhances the system's robustness as small intermediate values have a negligible contribution to the final result. To further improve the system's robustness, we also generate motion vector images using a Generative Adversarial Network (GAN) during training. Our experimental results demonstrate that the resulting system is robust to background motion.",1
"In this research, Piano performances have been analyzed only based on visual information. Computer vision algorithms, e.g., Hough transform and binary thresholding, have been applied to find where the keyboard and specific keys are located. At the same time, Convolutional Neural Networks(CNNs) has been also utilized to find whether specific keys are pressed or not, and how much intensity the keys are pressed only based on visual information. Especially for detecting intensity, a new method of utilizing spatial, temporal CNNs model is devised. Early fusion technique is especially applied in temporal CNNs architecture to analyze hand movement. We also make a new dataset for training each model. Especially when finding an intensity of a pressed key, both of video frames and their optical flow images are used to train models to find effectiveness.",0
"The study focused on analyzing piano performances through visual information alone. To do this, computer vision algorithms such as Hough transform and binary thresholding were employed to identify the location of the keyboard and specific keys. Additionally, Convolutional Neural Networks (CNNs) were utilized to determine whether certain keys were being pressed and the level of intensity of each key, all based on visual data. A novel spatial temporal CNNs model was developed specifically for intensity detection, with early fusion technique employed in temporal CNNs architecture to track hand movements. A new dataset was created to train each model, with video frames and optical flow images used to train models to effectively detect key intensity.",1
"We introduce a compact network for holistic scene flow estimation, called SENSE, which shares common encoder features among four closely-related tasks: optical flow estimation, disparity estimation from stereo, occlusion estimation, and semantic segmentation. Our key insight is that sharing features makes the network more compact, induces better feature representations, and can better exploit interactions among these tasks to handle partially labeled data. With a shared encoder, we can flexibly add decoders for different tasks during training. This modular design leads to a compact and efficient model at inference time. Exploiting the interactions among these tasks allows us to introduce distillation and self-supervised losses in addition to supervised losses, which can better handle partially labeled real-world data. SENSE achieves state-of-the-art results on several optical flow benchmarks and runs as fast as networks specifically designed for optical flow. It also compares favorably against the state of the art on stereo and scene flow, while consuming much less memory.",0
"We present SENSE, a compact network for comprehensive scene flow estimation. SENSE uses a shared encoder to extract common features for four related tasks: optical flow estimation, stereo disparity estimation, occlusion estimation, and semantic segmentation. Our approach shows that sharing features results in a more efficient and effective network that can leverage the interactions between tasks to handle incomplete data. We can easily incorporate different decoders for each task during training, making the model modular and efficient at inference time. In addition to supervised losses, we introduce distillation and self-supervised losses to better handle partially labeled data. Our method outperforms state-of-the-art optical flow benchmarks and is comparable to the best stereo and scene flow methods while using significantly less memory.",1
"Unsupervised video object segmentation has often been tackled by methods based on recurrent neural networks and optical flow. Despite their complexity, these kinds of approaches tend to favour short-term temporal dependencies and are thus prone to accumulating inaccuracies, which cause drift over time. Moreover, simple (static) image segmentation models, alone, can perform competitively against these methods, which further suggests that the way temporal dependencies are modelled should be reconsidered. Motivated by these observations, in this paper we explore simple yet effective strategies to model long-term temporal dependencies. Inspired by the non-local operators of [70], we introduce a technique to establish dense correspondences between pixel embeddings of a reference ""anchor"" frame and the current one. This allows the learning of pairwise dependencies at arbitrarily long distances without conditioning on intermediate frames. Without online supervision, our approach can suppress the background and precisely segment the foreground object even in challenging scenarios, while maintaining consistent performance over time. With a mean IoU of $81.7\%$, our method ranks first on the DAVIS-2016 leaderboard of unsupervised methods, while still being competitive against state-of-the-art online semi-supervised approaches. We further evaluate our method on the FBMS dataset and the ViSal video saliency dataset, showing results competitive with the state of the art.",0
"Methods for unsupervised video object segmentation have typically relied on recurrent neural networks and optical flow. However, these approaches are complex and tend to prioritize short-term temporal dependencies, leading to inaccuracies and drift over time. Interestingly, simple image segmentation models can perform just as well as these complex methods, highlighting the need to rethink how temporal dependencies are modeled. To address this, we propose a simple yet effective strategy for modeling long-term temporal dependencies. Our approach establishes dense correspondences between pixel embeddings of a reference frame and the current one, inspired by non-local operators. This enables us to learn pairwise dependencies at any distance without conditioning on intermediate frames. Our method achieves excellent results without online supervision and can accurately segment foreground objects, even in challenging scenarios. In fact, we rank first on the DAVIS-2016 leaderboard for unsupervised methods with a mean IoU of 81.7%. We also demonstrate competitive results on the FBMS dataset and the ViSal video saliency dataset.",1
"Action recognition is a key problem in computer vision that labels videos with a set of predefined actions. Capturing both, semantic content and motion, along the video frames is key to achieve high accuracy performance on this task. Most of the state-of-the-art methods rely on RGB frames for extracting the semantics and pre-computed optical flow fields as a motion cue. Then, both are combined using deep neural networks. Yet, it has been argued that such models are not able to leverage the motion information extracted from the optical flow, but instead the optical flow allows for better recognition of people and objects in the video. This urges the need to explore different cues or models that can extract motion in a more informative fashion. To tackle this issue, we propose to explore the predictive coding network, so called PredNet, a recurrent neural network that propagates predictive coding errors across layers and time steps. We analyze whether PredNet can better capture motions in videos by estimating over time the representations extracted from pre-trained networks for action recognition. In this way, the model only relies on the video frames, and does not need pre-processed optical flows as input. We report the effectiveness of our proposed model on UCF101 and HMDB51 datasets.",0
"Recognizing actions in videos is a significant challenge in computer vision as it involves assigning predefined actions to video footage while capturing both the semantic content and motion across frames. Most advanced methods use RGB frames to extract semantics and pre-calculated optical flow fields as a motion cue, and then combine them using deep neural networks. However, it has been suggested that these models cannot take full advantage of the motion information from optical flow, which mainly helps in recognizing people and objects. Therefore, there is a need to explore alternative cues or models that can extract motion more informatively. To address this issue, we propose exploring the recurrent neural network PredNet, which propagates predictive coding errors across layers and time steps, to better capture motion. We assess the effectiveness of this model by estimating over time the representations extracted from pre-trained networks for action recognition, without requiring pre-processed optical flows. Our proposed model demonstrates promising results on the UCF101 and HMDB51 datasets.",1
"Deep neural nets achieve state-of-the-art performance on the problem of optical flow estimation. Since optical flow is used in several safety-critical applications like self-driving cars, it is important to gain insights into the robustness of those techniques. Recently, it has been shown that adversarial attacks easily fool deep neural networks to misclassify objects. The robustness of optical flow networks to adversarial attacks, however, has not been studied so far. In this paper, we extend adversarial patch attacks to optical flow networks and show that such attacks can compromise their performance. We show that corrupting a small patch of less than 1% of the image size can significantly affect optical flow estimates. Our attacks lead to noisy flow estimates that extend significantly beyond the region of the attack, in many cases even completely erasing the motion of objects in the scene. While networks using an encoder-decoder architecture are very sensitive to these attacks, we found that networks using a spatial pyramid architecture are less affected. We analyse the success and failure of attacking both architectures by visualizing their feature maps and comparing them to classical optical flow techniques which are robust to these attacks. We also demonstrate that such attacks are practical by placing a printed pattern into real scenes.",0
"The problem of estimating optical flow is crucial in safety-critical applications such as self-driving cars, and deep neural nets have achieved state-of-the-art performance in this field. However, recent studies have shown that deep neural networks are susceptible to adversarial attacks, which can cause misclassification of objects. Although the robustness of optical flow networks to such attacks has not been studied, we investigate this issue by extending adversarial patch attacks to optical flow networks. Our findings reveal that even a small patch of less than 1% of the image size can significantly compromise the performance of optical flow estimates. These attacks lead to noisy flow estimates that extend beyond the region of the attack, resulting in the motion of objects in the scene being completely erased. We compare the success and failure of attacking encoder-decoder and spatial pyramid architectures, and we show that the latter is less affected. Visualizing feature maps and comparing them to classical optical flow techniques that are robust to these attacks, we analyze the success and failure of attacking both architectures. We also demonstrate the practicality of these attacks by placing a printed pattern into real scenes.",1
"Though machine learning has achieved notable success in modeling sequential and spatial data for speech recognition and in computer vision, applications to remote sensing and climate science problems are seldom considered. In this paper, we demonstrate techniques from unsupervised learning of future video frame prediction, to increase the accuracy of ice flow tracking in multi-spectral satellite images. As the volume of cryosphere data increases in coming years, this is an interesting and important opportunity for machine learning to address a global challenge for climate change, risk management from floods, and conserving freshwater resources. Future frame prediction of ice melt and tracking the optical flow of ice dynamics presents modeling difficulties, due to uncertainties in global temperature increase, changing precipitation patterns, occlusion from cloud cover, rapid melting and glacier retreat due to black carbon aerosol deposition, from wildfires or human fossil emissions. We show the adversarial learning method helps improve the accuracy of tracking the optical flow of ice dynamics compared to existing methods in climate science. We present a dataset, IceNet, to encourage machine learning research and to help facilitate further applications in the areas of cryospheric science and climate change.",0
"Although machine learning has been successful in modeling sequential and spatial data for speech recognition and computer vision, it is rarely considered for remote sensing and climate science problems. This study showcases techniques from unsupervised learning, specifically future video frame prediction, to enhance ice flow tracking accuracy in multi-spectral satellite images. As the amount of cryosphere data grows, machine learning has the potential to address global challenges related to climate change, flood risk management, and freshwater conservation. Predicting ice melt and tracking ice dynamics present modeling obstacles due to various uncertainties such as global temperature increase, changing precipitation patterns, occlusion from cloud cover, and glacier retreat caused by black carbon aerosol deposition, wildfires, or human fossil emissions. Using the adversarial learning method, we demonstrate that the accuracy of tracking the optical flow of ice dynamics can be improved compared to existing methods in climate science. Moreover, we provide a dataset, IceNet, to encourage machine learning research and promote further applications in the fields of cryospheric science and climate change.",1
"Recently unsupervised learning of depth from videos has made remarkable progress and the results are comparable to fully supervised methods in outdoor scenes like KITTI. However, there still exist great challenges when directly applying this technology in indoor environments, e.g., large areas of non-texture regions like white wall, more complex ego-motion of handheld camera, transparent glasses and shiny objects. To overcome these problems, we propose a new optical-flow based training paradigm which reduces the difficulty of unsupervised learning by providing a clearer training target and handles the non-texture regions. Our experimental evaluation demonstrates that the result of our method is comparable to fully supervised methods on the NYU Depth V2 benchmark. To the best of our knowledge, this is the first quantitative result of purely unsupervised learning method reported on indoor datasets.",0
"Remarkable progress has been made in unsupervised learning of depth from videos, with results comparable to fully supervised methods in outdoor scenes like KITTI. However, applying this technology directly in indoor environments presents great challenges, such as large areas of non-textured regions like white walls, more complex ego-motion of handheld cameras, transparent glasses and shiny objects. To overcome these issues, a new optical-flow based training paradigm is proposed which reduces the difficulty of unsupervised learning by providing a clearer training target and handling non-textured regions. Experimental evaluation shows that the results of this method are comparable to fully supervised methods on the NYU Depth V2 benchmark and is the first quantitative result of purely unsupervised learning method reported on indoor datasets.",1
"We address the challenging task of video-based person re-identification. Recent works have shown that splitting the video sequences into clips and then aggregating clip based similarity is appropriate for the task. We show that using a learned clip similarity aggregation function allows filtering out hard clip pairs, e.g. where the person is not clearly visible, is in a challenging pose, or where the poses in the two clips are too different to be informative. This allows the method to focus on clip-pairs which are more informative for the task. We also introduce the use of 3D CNNs for video-based re-identification and show their effectiveness by performing equivalent to previous works, which use optical flow in addition to RGB, while using RGB inputs only. We give quantitative results on three challenging public benchmarks and show better or competitive performance. We also validate our method qualitatively.",0
"Our focus is on the complex task of identifying individuals from video footage. Previous studies have demonstrated that dividing video sequences into clips and then amalgamating similarity based on these clips is suitable for this purpose. We present a novel approach that utilizes a learned function for the aggregation of clip similarity. Our method filters out difficult clip pairs, such as those in which the person is in an awkward position, not visible clearly, or the poses in the two clips are too different. This enables our approach to prioritize clip-pairs that provide more relevant information for the task at hand. We also introduce the use of 3D CNNs for video-based re-identification and demonstrate their effectiveness by achieving the same level of performance as prior techniques that employ both optical flow and RGB inputs, while using only RGB inputs. Our method outperforms or competes favorably with previous methods on three challenging public benchmarks, and we confirm our approach's validity through qualitative validation.",1
"Driver drowsiness increases crash risk, leading to substantial road trauma each year. Drowsiness detection methods have received considerable attention, but few studies have investigated the implementation of a detection approach on a mobile phone. Phone applications reduce the need for specialised hardware and hence, enable a cost-effective roll-out of the technology across the driving population. While it has been shown that three-dimensional (3D) operations are more suitable for spatiotemporal feature learning, current methods for drowsiness detection commonly use frame-based, multi-step approaches. However, computationally expensive techniques that achieve superior results on action recognition benchmarks (e.g. 3D convolutions, optical flow extraction) create bottlenecks for real-time, safety-critical applications on mobile devices. Here, we show how depthwise separable 3D convolutions, combined with an early fusion of spatial and temporal information, can achieve a balance between high prediction accuracy and real-time inference requirements. In particular, increased accuracy is achieved when assessment requires motion information, for example, when sunglasses conceal the eyes. Further, a custom TensorFlow-based smartphone application shows the true impact of various approaches on inference times and demonstrates the effectiveness of real-time monitoring based on out-of-sample data to alert a drowsy driver. Our model is pre-trained on ImageNet and Kinetics and fine-tuned on a publicly available Driver Drowsiness Detection dataset. Fine-tuning on large naturalistic driving datasets could further improve accuracy to obtain robust in-vehicle performance. Overall, our research is a step towards practical deep learning applications, potentially preventing micro-sleeps and reducing road trauma.",0
"Every year, drowsiness in drivers leads to a significant increase in the risk of crashes and causes substantial road trauma. While there has been significant attention towards drowsiness detection methods, few studies have examined how to implement detection approaches on mobile phones. The use of phone applications reduces the need for specialized hardware, making it more cost-effective to implement across the population of drivers. Current methods for drowsiness detection commonly use frame-based, multi-step approaches, but these can be computationally expensive and create bottlenecks for real-time, safety-critical applications on mobile devices. In this study, the authors show how depthwise separable 3D convolutions, combined with an early fusion of spatial and temporal information, can achieve a balance between high prediction accuracy and real-time inference requirements. The authors also demonstrate the effectiveness of real-time monitoring based on out-of-sample data to alert a drowsy driver using a custom TensorFlow-based smartphone application. Overall, this research is a step towards practical deep learning applications that can potentially prevent micro-sleeps and reduce road trauma.",1
"The automatic detection and tracking of general objects (like persons, animals or cars), text and logos in a video is crucial for many video understanding tasks, and usually real-time processing as required. We propose OmniTrack, an efficient and robust algorithm which is able to automatically detect and track objects, text as well as brand logos in real-time. It combines a powerful deep learning based object detector (YoloV3) with high-quality optical flow methods. Based on the reference YoloV3 C++ implementation, we did some important performance optimizations which will be described. The major steps in the training procedure for the combined detector for text and logo will be presented. We will describe then the OmniTrack algorithm, consisting of the phases preprocessing, feature calculation, prediction, matching and update. Several performance optimizations have been implemented there as well, like doing the object detection and optical flow calculation asynchronously. Experiments show that the proposed algorithm runs in real-time for standard definition ($720x576$) video on a PC with a Quadro RTX 5000 GPU.",0
"Detecting and tracking various objects, such as people, animals, and cars, along with text and logos in videos in real-time is crucial for numerous video understanding tasks. Our proposed solution, OmniTrack, is an efficient and robust algorithm that combines a powerful deep learning-based object detector (YoloV3) with high-quality optical flow methods. We have made significant performance optimizations to the reference YoloV3 C++ implementation and present the major steps in the training procedure for the combined detector for text and logo. The OmniTrack algorithm comprises preprocessing, feature calculation, prediction, matching, and update phases, with several performance optimizations, including asynchronous object detection and optical flow calculation. Our experiments demonstrate that the proposed algorithm can run in real-time for standard definition ($720x576$) video on a PC with a Quadro RTX 5000 GPU.",1
"Deep video action recognition models have been highly successful in recent years but require large quantities of manually annotated data, which are expensive and laborious to obtain. In this work, we investigate the generation of synthetic training data for video action recognition, as synthetic data have been successfully used to supervise models for a variety of other computer vision tasks. We propose an interpretable parametric generative model of human action videos that relies on procedural generation, physics models and other components of modern game engines. With this model we generate a diverse, realistic, and physically plausible dataset of human action videos, called PHAV for ""Procedural Human Action Videos"". PHAV contains a total of 39,982 videos, with more than 1,000 examples for each of 35 action categories. Our video generation approach is not limited to existing motion capture sequences: 14 of these 35 categories are procedurally defined synthetic actions. In addition, each video is represented with 6 different data modalities, including RGB, optical flow and pixel-level semantic labels. These modalities are generated almost simultaneously using the Multiple Render Targets feature of modern GPUs. In order to leverage PHAV, we introduce a deep multi-task (i.e. that considers action classes from multiple datasets) representation learning architecture that is able to simultaneously learn from synthetic and real video datasets, even when their action categories differ. Our experiments on the UCF-101 and HMDB-51 benchmarks suggest that combining our large set of synthetic videos with small real-world datasets can boost recognition performance. Our approach also significantly outperforms video representations produced by fine-tuning state-of-the-art unsupervised generative models of videos.",0
"Recent years have seen great success in deep video action recognition models, but the process is reliant on obtaining large amounts of manually annotated data, which is both expensive and laborious. This study aims to explore the use of synthetic training data for video action recognition, as synthetic data has been effectively employed in supervising models for various computer vision tasks. To achieve this, a parametric generative model of human action videos has been proposed, utilizing procedural generation, physics models, and other components of modern game engines. This model generates a dataset of human action videos called PHAV that encompasses 39,982 videos, with over 1,000 examples for each of 35 action categories. The video generation approach allows for the creation of procedurally defined synthetic actions and offers six different data modalities for each video, including RGB, optical flow, and pixel-level semantic labels. These modalities are generated simultaneously via the Multiple Render Targets feature of modern GPUs. To leverage PHAV, a deep multi-task representation learning architecture has been introduced, capable of learning from synthetic and real video datasets simultaneously, even when their action categories differ. Experiments on the UCF-101 and HMDB-51 benchmarks demonstrate that combining the large set of synthetic videos with small real-world datasets can improve recognition performance. Additionally, the proposed approach outperforms video representations produced by fine-tuning state-of-the-art unsupervised generative models of videos.",1
"Video salient object detection aims at discovering the most visually distinctive objects in a video. How to effectively take object motion into consideration during video salient object detection is a critical issue. Existing state-of-the-art methods either do not explicitly model and harvest motion cues or ignore spatial contexts within optical flow images. In this paper, we develop a multi-task motion guided video salient object detection network, which learns to accomplish two sub-tasks using two sub-networks, one sub-network for salient object detection in still images and the other for motion saliency detection in optical flow images. We further introduce a series of novel motion guided attention modules, which utilize the motion saliency sub-network to attend and enhance the sub-network for still images. These two sub-networks learn to adapt to each other by end-to-end training. Experimental results demonstrate that the proposed method significantly outperforms existing state-of-the-art algorithms on a wide range of benchmarks. We hope our simple and effective approach will serve as a solid baseline and help ease future research in video salient object detection. Code and models will be made available.",0
"The objective of video salient object detection is to identify the most visually prominent objects in a video. A crucial issue in this process is how to consider object motion effectively. Existing methods either overlook motion cues or neglect spatial contexts in optical flow images. We present a motion-guided video salient object detection network that accomplishes two sub-tasks using two sub-networks. One sub-network identifies salient objects in still images, and the other detects motion saliency in optical flow images. We also introduce novel motion-guided attention modules that employ the motion saliency sub-network to enhance the still image sub-network. These two sub-networks learn from each other through end-to-end training. We have conducted experiments that demonstrate our proposed method outperforms state-of-the-art algorithms in various benchmarks. Our approach provides a solid foundation that can facilitate future research in video salient object detection. We will make our code and models available.",1
"In this work we propose a capsule-based approach for semi-supervised video object segmentation. Current video object segmentation methods are frame-based and often require optical flow to capture temporal consistency across frames which can be difficult to compute. To this end, we propose a video based capsule network, CapsuleVOS, which can segment several frames at once conditioned on a reference frame and segmentation mask. This conditioning is performed through a novel routing algorithm for attention-based efficient capsule selection. We address two challenging issues in video object segmentation: 1) segmentation of small objects and 2) occlusion of objects across time. The issue of segmenting small objects is addressed with a zooming module which allows the network to process small spatial regions of the video. Apart from this, the framework utilizes a novel memory module based on recurrent networks which helps in tracking objects when they move out of frame or are occluded. The network is trained end-to-end and we demonstrate its effectiveness on two benchmark video object segmentation datasets; it outperforms current offline approaches on the Youtube-VOS dataset while having a run-time that is almost twice as fast as competing methods. The code is publicly available at https://github.com/KevinDuarte/CapsuleVOS.",0
"Our work proposes a capsule-based method for semi-supervised video object segmentation, which addresses challenges faced by current frame-based approaches that rely on optical flow for capturing temporal consistency. Our CapsuleVOS network can segment multiple frames simultaneously, conditioned on a reference frame and its segmentation mask, using a novel routing algorithm for efficient capsule selection. We tackle two major issues in video object segmentation - small object segmentation and object occlusion across time. For small objects, we use a zooming module, and for occlusion, we utilize a memory module based on recurrent networks. The end-to-end trained network outperforms existing offline approaches on two benchmark datasets and is almost twice as fast as competing methods. Interested parties can access our code on https://github.com/KevinDuarte/CapsuleVOS.",1
"Synthetic data is an increasingly popular tool for training deep learning models, especially in computer vision but also in other areas. In this work, we attempt to provide a comprehensive survey of the various directions in the development and application of synthetic data. First, we discuss synthetic datasets for basic computer vision problems, both low-level (e.g., optical flow estimation) and high-level (e.g., semantic segmentation), synthetic environments and datasets for outdoor and urban scenes (autonomous driving), indoor scenes (indoor navigation), aerial navigation, simulation environments for robotics, applications of synthetic data outside computer vision (in neural programming, bioinformatics, NLP, and more); we also survey the work on improving synthetic data development and alternative ways to produce it such as GANs. Second, we discuss in detail the synthetic-to-real domain adaptation problem that inevitably arises in applications of synthetic data, including synthetic-to-real refinement with GAN-based models and domain adaptation at the feature/model level without explicit data transformations. Third, we turn to privacy-related applications of synthetic data and review the work on generating synthetic datasets with differential privacy guarantees. We conclude by highlighting the most promising directions for further work in synthetic data studies.",0
"Synthetic data has become a popular means of training deep learning models, particularly in computer vision and other fields. This study aims to comprehensively survey the development and application of synthetic data. The first section examines synthetic datasets for basic computer vision tasks, synthetic environments for autonomous driving, indoor navigation, aerial navigation, robotics, and applications beyond computer vision. It also explores ways to improve synthetic data development and alternative production methods such as GANs. The second section delves into the synthetic-to-real domain adaptation problem and discusses refinement with GAN-based models and domain adaptation without explicit data transformations. The third section focuses on privacy-related applications of synthetic data and reviews generating synthetic datasets with differential privacy guarantees. Finally, the study concludes by highlighting promising directions for future synthetic data research.",1
"We propose a novel conditional GAN (cGAN) model for continuous fine-grained human action segmentation, that utilises multi-modal data and learned scene context information. The proposed approach utilises two GANs: termed Action GAN and Auxiliary GAN, where the Action GAN is trained to operate over the current RGB frame while the Auxiliary GAN utilises supplementary information such as depth or optical flow. The goal of both GANs is to generate similar `action codes', a vector representation of the current action. To facilitate this process a context extractor that incorporates data and recent outputs from both modes is used to extract context information to aid recognition. The result is a recurrent GAN architecture which learns a task specific loss function from multiple feature modalities. Extensive evaluations on variants of the proposed model to show the importance of utilising different information streams such as context and auxiliary information in the proposed network; and show that our model is capable of outperforming state-of-the-art methods for three widely used datasets: 50 Salads, MERL Shopping and Georgia Tech Egocentric Activities, comprising both static and dynamic camera settings.",0
"Our proposed approach for continuous fine-grained human action segmentation involves a new conditional GAN model (cGAN) that incorporates multi-modal data and scene context information. This approach involves two separate GANs, namely the Action GAN and Auxiliary GAN. The Action GAN is designed to operate on the current RGB frame, while the Auxiliary GAN utilizes supplementary information like depth or optical flow. The objective of both GANs is to generate similar action codes, which are vector representations of the current action. To aid recognition, a context extractor is used, which incorporates data and recent outputs from both modes to extract context information. This results in a recurrent GAN architecture that learns a task-specific loss function from multiple feature modalities. Our proposed model has been evaluated on three widely used datasets: 50 Salads, MERL Shopping and Georgia Tech Egocentric Activities, which comprise both static and dynamic camera settings. Extensive evaluations on variants of the proposed model demonstrate the importance of utilizing different information streams such as context and auxiliary information in the proposed network. Moreover, our model has been shown to outperform state-of-the-art methods.",1
"This paper proposes a vision-based fire and smoke segmentation system which use spatial, temporal and motion information to extract the desired regions from the video frames. The fusion of information is done using multiple features such as optical flow, divergence and intensity values. These features extracted from the images are used to segment the pixels into different classes in an unsupervised way. A comparative analysis is done by using multiple clustering algorithms for segmentation. Here the Markov Random Field performs more accurately than other segmentation algorithms since it characterizes the spatial interactions of pixels using a finite number of parameters. It builds a probabilistic image model that selects the most likely labeling using the maximum a posteriori (MAP) estimation. This unsupervised approach is tested on various images and achieves a frame-wise fire detection rate of 95.39%. Hence this method can be used for early detection of fire in real-time and it can be incorporated into an indoor or outdoor surveillance system.",0
"This paper presents a system for fire and smoke segmentation that relies on visual cues such as spatial, temporal, and motion information to identify relevant regions in video frames. To achieve this, various features are used, including optical flow, divergence, and intensity values. The unsupervised segmentation process involves clustering algorithms, with Markov Random Field proving to be the most accurate due to its ability to model spatial interactions between pixels efficiently. The method achieves a high frame-wise fire detection rate of 95.39%, making it suitable for real-time use in indoor or outdoor surveillance systems for early fire detection.",1
"Infrared human action recognition has many advantages, i.e., it is insensitive to illumination change, appearance variability, and shadows. Existing methods for infrared action recognition are either based on spatial or local temporal information, however, the global temporal information, which can better describe the movements of body parts across the whole video, is not considered. In this letter, we propose a novel global temporal representation named optical-flow stacked difference image (OFSDI) and extract robust and discriminative feature from the infrared action data by considering the local, global, and spatial temporal information together. Due to the small size of the infrared action dataset, we first apply convolutional neural networks on local, spatial, and global temporal stream respectively to obtain efficient convolutional feature maps from the raw data rather than train a classifier directly. Then these convolutional feature maps are aggregated into effective descriptors named three-stream trajectory-pooled deep-convolutional descriptors by trajectory-constrained pooling. Furthermore, we improve the robustness of these features by using the locality-constrained linear coding (LLC) method. With these features, a linear support vector machine (SVM) is adopted to classify the action data in our scheme. We conduct the experiments on infrared action recognition datasets InfAR and NTU RGB+D. The experimental results show that the proposed approach outperforms the representative state-of-the-art handcrafted features and deep learning features based methods for the infrared action recognition.",0
"Infrared human action recognition is advantageous as it is not affected by changes in illumination, appearance, or shadows. However, current methods for infrared action recognition only consider spatial or local temporal information, neglecting the global temporal information that can better describe the movements of body parts across the entire video. To address this issue, we propose a new global temporal representation called optical-flow stacked difference image (OFSDI) and extract robust and discriminative features from the infrared action data by considering local, global, and spatial temporal information together. To account for the small size of the infrared action dataset, we use convolutional neural networks to obtain efficient convolutional feature maps from the raw data. These feature maps are then aggregated into effective descriptors using trajectory-constrained pooling and improved using the locality-constrained linear coding (LLC) method. Finally, we use a linear support vector machine (SVM) to classify the action data. Our approach outperforms state-of-the-art methods for infrared action recognition on datasets InfAR and NTU RGB+D.",1
"We propose a novel video inpainting algorithm that simultaneously hallucinates missing appearance and motion (optical flow) information, building upon the recent 'Deep Image Prior' (DIP) that exploits convolutional network architectures to enforce plausible texture in static images. In extending DIP to video we make two important contributions. First, we show that coherent video inpainting is possible without a priori training. We take a generative approach to inpainting based on internal (within-video) learning without reliance upon an external corpus of visual data to train a one-size-fits-all model for the large space of general videos. Second, we show that such a framework can jointly generate both appearance and flow, whilst exploiting these complementary modalities to ensure mutual consistency. We show that leveraging appearance statistics specific to each video achieves visually plausible results whilst handling the challenging problem of long-term consistency.",0
"Our proposal introduces a unique video inpainting technique that can simultaneously replace missing appearance and motion (optical flow) data. This builds on the 'Deep Image Prior' (DIP) method which uses convolutional network architectures to create realistic texture in static images. We extend DIP to video and make two significant contributions. Firstly, we demonstrate that coherent video inpainting is feasible without the need for pre-training. Our generative approach to inpainting relies on internal learning, without the use of a general model for all videos. Secondly, we showcase a framework that can generate both appearance and flow, while ensuring mutual consistency between the two modalities. By utilizing appearance statistics specific to each video, we achieve results that are visually plausible and can handle the challenging problem of long-term consistency.",1
"Recently, it is increasingly popular to equip mobile RGB cameras with Time-of-Flight (ToF) sensors for active depth sensing. However, for off-the-shelf ToF sensors, one must tackle two problems in order to obtain high-quality depth with respect to the RGB camera, namely 1) online calibration and alignment; and 2) complicated error correction for ToF depth sensing. In this work, we propose a framework for jointly alignment and refinement via deep learning. First, a cross-modal optical flow between the RGB image and the ToF amplitude image is estimated for alignment. The aligned depth is then refined via an improved kernel predicting network that performs kernel normalization and applies the bias prior to the dynamic convolution. To enrich our data for end-to-end training, we have also synthesized a dataset using tools from computer graphics. Experimental results demonstrate the effectiveness of our approach, achieving state-of-the-art for ToF refinement.",0
"Lately, it has become increasingly popular to add Time-of-Flight (ToF) sensors to mobile RGB cameras to enable active depth sensing. However, achieving high-quality depth with respect to the RGB camera using off-the-shelf ToF sensors requires addressing two problems- online calibration and alignment, and complicated error correction for ToF depth sensing. Therefore, we propose a framework that uses deep learning to jointly align and refine the depth. Firstly, we estimate a cross-modal optical flow between the RGB and ToF amplitude images to align them. The aligned depth is then refined using an improved kernel predicting network that performs kernel normalization and applies the bias prior to the dynamic convolution. To enrich our data for end-to-end training, we have created a synthesized dataset using computer graphics tools. Our experimental results demonstrate that our approach is effective and achieves state-of-the-art results for ToF refinement.",1
"The goal of this study is to develop and analyze multimodal models for predicting experienced affective responses of viewers watching movie clips. We develop hybrid multimodal prediction models based on both the video and audio of the clips. For the video content, we hypothesize that both image content and motion are crucial features for evoked emotion prediction. To capture such information, we extract features from RGB frames and optical flow using pre-trained neural networks. For the audio model, we compute an enhanced set of low-level descriptors including intensity, loudness, cepstrum, linear predictor coefficients, pitch and voice quality. Both visual and audio features are then concatenated to create audio-visual features, which are used to predict the evoked emotion. To classify the movie clips into the corresponding affective response categories, we propose two approaches based on deep neural network models. The first one is based on fully connected layers without memory on the time component, the second incorporates the sequential dependency with a long short-term memory recurrent neural network (LSTM). We perform a thorough analysis of the importance of each feature set. Our experiments reveal that in our set-up, predicting emotions at each time step independently gives slightly better accuracy performance than with the LSTM. Interestingly, we also observe that the optical flow is more informative than the RGB in videos, and overall, models using audio features are more accurate than those based on video features when making the final prediction of evoked emotions.",0
"The aim of this investigation is to establish and assess multimodal models that can predict the emotional responses of viewers while watching movie clips. These hybrid multimodal prediction models utilize both the video and audio components of the clips. Our hypothesis is that both the image content and motion of the video are crucial in predicting the evoked emotions. To capture this information, we extract features from the RGB frames and optical flow using pre-trained neural networks. For the audio model, we compute an enhanced set of low-level descriptors including intensity, loudness, cepstrum, linear predictor coefficients, pitch and voice quality. These visual and audio features are combined to create audio-visual features, which are then used to predict the evoked emotions. To classify the movie clips into the corresponding affective response categories, we propose two approaches based on deep neural network models. The first one is based on fully connected layers without memory on the time component, while the second one incorporates the sequential dependency with a long short-term memory recurrent neural network (LSTM). We conduct a thorough analysis of the importance of each feature set and find that predicting emotions at each time step independently gives slightly better accuracy performance than with the LSTM. Interestingly, we also discover that the optical flow is more informative than the RGB in videos, and overall, models utilizing audio features are more accurate than those based on video features when predicting evoked emotions.",1
"We focus on the word-level visual lipreading, which requires to decode the word from the speaker's video. Recently, many state-of-the-art visual lipreading methods explore the end-to-end trainable deep models, involving the use of 2D convolutional networks (e.g., ResNet) as the front-end visual feature extractor and the sequential model (e.g., Bi-LSTM or Bi-GRU) as the back-end. Although a deep 2D convolution neural network can provide informative image-based features, it ignores the temporal motion existing between the adjacent frames. In this work, we investigate the spatial-temporal capacity power of I3D (Inflated 3D ConvNet) for visual lipreading. We demonstrate that, after pre-trained on the large-scale video action recognition dataset (e.g., Kinetics), our models show a considerable improvement of performance on the task of lipreading. A comparison between a set of video model architectures and input data representation is also reported. Our extensive experiments on LRW shows that a two-stream I3D model with RGB video and optical flow as the inputs achieves the state-of-the-art performance.",0
"Our focus is on word-level visual lipreading, which involves decoding words from a speaker's video. Recently, modern visual lipreading techniques have utilized end-to-end trainable deep models, utilizing front-end visual feature extractors like 2D convolutional networks (e.g., ResNet) and back-ends like sequential models (e.g., Bi-LSTM or Bi-GRU). While deep 2D convolution neural networks can provide informative image-based features, they overlook the temporal motion between adjacent frames. Our study investigates the spatial-temporal capacity power of I3D (Inflated 3D ConvNet) in visual lipreading. We show that our models demonstrate considerable performance improvement after being pre-trained on large-scale video action recognition datasets (e.g., Kinetics). We also report a comparison between various video model architectures and input data representations. Our comprehensive experiments on LRW reveal that a two-stream I3D model with RGB video and optical flow inputs achieves state-of-the-art performance.",1
"Predicting depth from a monocular video sequence is an important task for autonomous driving. Although it has advanced considerably in the past few years, recent methods based on convolutional neural networks (CNNs) discard temporal coherence in the video sequence and estimate depth independently for each frame, which often leads to undesired inconsistent results over time. To address this problem, we propose to memorize temporal consistency in the video sequence, and leverage it for the task of depth prediction. To this end, we introduce a two-stream CNN with a flow-guided memory module, where each stream encodes visual and temporal features, respectively. The memory module, implemented using convolutional gated recurrent units (ConvGRUs), inputs visual and temporal features sequentially together with optical flow tailored to our task. It memorizes trajectories of individual features selectively and propagates spatial information over time, enforcing a long-term temporal consistency to prediction results. We evaluate our method on the KITTI benchmark dataset in terms of depth prediction accuracy, temporal consistency and runtime, and achieve a new state of the art. We also provide an extensive experimental analysis, clearly demonstrating the effectiveness of our approach to memorizing temporal consistency for depth prediction.",0
"The task of predicting depth from a monocular video sequence is crucial for autonomous driving. While convolutional neural network (CNN) methods have progressed significantly in recent years, they often produce inconsistent results due to the lack of temporal coherence in the video sequence. To overcome this issue, we suggest the implementation of a two-stream CNN with a flow-guided memory module that encodes visual and temporal features separately. The memory module, which employs ConvGRUs, selectively memorizes trajectories of individual features and propagates spatial information over time to enforce long-term temporal consistency in prediction results. Our approach achieves a new state of the art in terms of depth prediction accuracy, temporal consistency, and runtime on the KITTI benchmark dataset. Additionally, we provide extensive experimental analysis to demonstrate the effectiveness of our method in memorizing temporal consistency for depth prediction.",1
"We present a method for decomposing the 3D scene flow observed from a moving stereo rig into stationary scene elements and dynamic object motion. Our unsupervised learning framework jointly reasons about the camera motion, optical flow, and 3D motion of moving objects. Three cooperating networks predict stereo matching, camera motion, and residual flow, which represents the flow component due to object motion and not from camera motion. Based on rigid projective geometry, the estimated stereo depth is used to guide the camera motion estimation, and the depth and camera motion are used to guide the residual flow estimation. We also explicitly estimate the 3D scene flow of dynamic objects based on the residual flow and scene depth. Experiments on the KITTI dataset demonstrate the effectiveness of our approach and show that our method outperforms other state-of-the-art algorithms on the optical flow and visual odometry tasks.",0
"Our approach involves breaking down the 3D scene flow that a moving stereo rig observes into two distinct parts: stationary scene elements and dynamic object motion. We achieve this through an unsupervised learning framework that takes into account camera motion, optical flow, and 3D motion of moving objects. Three networks work together to predict stereo matching, camera motion, and residual flow, which captures object motion and not camera motion. Using rigid projective geometry, we use the estimated stereo depth to guide the estimation of camera motion and residual flow. Additionally, we explicitly calculate the 3D scene flow of dynamic objects based on the residual flow and scene depth. Our experiments with the KITTI dataset show that our method is more effective than other state-of-the-art algorithms for optical flow and visual odometry tasks.",1
"Dashboard cameras capture a tremendous amount of driving scene video each day. These videos are purposefully coupled with vehicle sensing data, such as from the speedometer and inertial sensors, providing an additional sensing modality for free. In this work, we leverage the large-scale unlabeled yet naturally paired data for visual representation learning in the driving scenario. A representation is learned in an end-to-end self-supervised framework for predicting dense optical flow from a single frame with paired sensing data. We postulate that success on this task requires the network to learn semantic and geometric knowledge in the ego-centric view. For example, forecasting a future view to be seen from a moving vehicle requires an understanding of scene depth, scale, and movement of objects. We demonstrate that our learned representation can benefit other tasks that require detailed scene understanding and outperforms competing unsupervised representations on semantic segmentation.",0
"Every day, dashboard cameras record a large amount of footage of driving scenes. This footage is coupled with vehicle sensing data, such as speedometer and inertial sensors, to provide an additional source of sensing. The purpose of this study is to make use of this vast amount of unlabeled, naturally paired data for visual representation learning in driving scenarios. An end-to-end self-supervised framework is used to learn a representation that can predict dense optical flow from a single frame with paired sensing data. The hypothesis is that success in this task requires the network to learn both semantic and geometric knowledge in the ego-centric view. For instance, predicting a future view from a moving vehicle necessitates an understanding of scene depth, scale, and the movement of objects. The findings demonstrate that this learned representation is beneficial for other tasks that require detailed scene comprehension and outperforms other unsupervised representations in semantic segmentation.",1
"We propose a learning-based method that solves monocular stereo and can be extended to fuse depth information from multiple target frames. Given two unconstrained images from a monocular camera with known intrinsic calibration, our network estimates relative camera poses and the depth map of the source image. The core contribution of the proposed method is threefold. First, a network is tailored for static scenes that jointly estimates the optical flow and camera motion. By the joint estimation, the optical flow search space is gradually reduced resulting in an efficient and accurate flow estimation. Second, a novel triangulation layer is proposed to encode the estimated optical flow and camera motion while avoiding common numerical issues caused by epipolar. Third, beyond two-view depth estimation, we further extend the above networks to fuse depth information from multiple target images and estimate the depth map of the source image. To further benefit the research community, we introduce tools to generate photorealistic structure-from-motion datasets such that deep networks can be well trained and evaluated. The proposed method is compared with previous methods and achieves state-of-the-art results within less time. Images from real-world applications and Google Earth are used to demonstrate the generalization ability of the method.",0
"Our proposed method utilizes learning to solve monocular stereo and has the ability to combine depth information from multiple target frames. The system estimates the relative camera poses and depth map of a source image using two unconstrained images from a monocular camera with a known intrinsic calibration. Our method has three key contributions. Firstly, we have designed a network specifically for static scenes that jointly estimates optical flow and camera motion, which reduces the search space and leads to more accurate flow estimation. Secondly, we have introduced a novel triangulation layer that encodes the estimated optical flow and camera motion while avoiding numerical issues arising from epipolar. Thirdly, we have extended the network to fuse depth information from several target images to estimate the depth map of the source image. Additionally, we have created photorealistic structure-from-motion datasets to facilitate the training and evaluation of deep networks. Our method achieves state-of-the-art results in less time than previous methods, as demonstrated by real-world and Google Earth images. Our method's generalization ability is also shown in the test results.",1
"We present GLNet, a self-supervised framework for learning depth, optical flow, camera pose and intrinsic parameters from monocular video - addressing the difficulty of acquiring realistic ground-truth for such tasks. We propose three contributions: 1) we design new loss functions that capture multiple geometric constraints (eg. epipolar geometry) as well as an adaptive photometric loss that supports multiple moving objects, rigid and non-rigid, 2) we extend the model such that it predicts camera intrinsics, making it applicable to uncalibrated video, and 3) we propose several online refinement strategies that rely on the symmetry of our self-supervised loss in training and testing, in particular optimizing model parameters and/or the output of different tasks, thus leveraging their mutual interactions. The idea of jointly optimizing the system output, under all geometric and photometric constraints can be viewed as a dense generalization of classical bundle adjustment. We demonstrate the effectiveness of our method on KITTI and Cityscapes, where we outperform previous self-supervised approaches on multiple tasks. We also show good generalization for transfer learning in YouTube videos.",0
"GLNet is a framework that uses self-supervision to learn depth, optical flow, camera pose, and intrinsic parameters from monocular video. This is particularly useful because obtaining realistic ground-truth for these tasks is difficult. Our framework has three main contributions. Firstly, we have designed new loss functions that capture multiple geometric constraints, including epipolar geometry, and an adaptive photometric loss that supports multiple types of moving objects, including rigid and non-rigid. Secondly, we have extended the model to predict camera intrinsics, making it suitable for uncalibrated video. Finally, we have proposed several online refinement strategies that leverage the symmetry of our self-supervised loss in both training and testing. These strategies optimize model parameters and/or the output of different tasks, taking advantage of their mutual interactions. This approach can be seen as a dense generalization of classical bundle adjustment. Our method has demonstrated excellent performance on KITTI and Cityscapes, outperforming previous self-supervised approaches on multiple tasks. Additionally, we have shown that our framework can generalize well for transfer learning in YouTube videos.",1
"Dense prediction tasks typically employ encoder-decoder architectures, but the prevalent convolutions in the decoder are not image-adaptive and can lead to boundary artifacts. Different generalized convolution operations have been introduced to counteract this. We go beyond these by leveraging guidance data to redefine their inherent notion of proximity. Our proposed network layer builds on the permutohedral lattice, which performs sparse convolutions in a high-dimensional space allowing for powerful non-local operations despite small filters. Multiple features with different characteristics span this permutohedral space. In contrast to prior work, we learn these features in a task-specific manner by generalizing the basic permutohedral operations to learnt feature representations. As the resulting objective is complex, a carefully designed framework and learning procedure are introduced, yielding rich feature embeddings in practice. We demonstrate the general applicability of our approach in different joint upsampling tasks. When adding our network layer to state-of-the-art networks for optical flow and semantic segmentation, boundary artifacts are removed and the accuracy is improved.",0
"Tasks that require dense predictions typically use encoder-decoder architectures, but the convolutions in the decoder can cause boundary artifacts because they are not image-adaptive. To address this, researchers have introduced various generalized convolution operations. However, we propose a new approach that uses guidance data to redefine proximity. Our network layer builds on the permutohedral lattice, which performs sparse convolutions in a high-dimensional space and allows for powerful non-local operations with small filters. In contrast to previous work, we learn features in a task-specific manner by generalizing permutohedral operations to learned feature representations. We introduce a carefully designed framework and learning procedure to yield rich feature embeddings in practice, demonstrating the general applicability of our approach in different joint upsampling tasks. By adding our network layer to state-of-the-art networks for optical flow and semantic segmentation, we remove boundary artifacts and improve accuracy.",1
"Prediction and interpolation for long-range video data involves the complex task of modeling motion trajectories for each visible object, occlusions and dis-occlusions, as well as appearance changes due to viewpoint and lighting. Optical flow based techniques generalize but are suitable only for short temporal ranges. Many methods opt to project the video frames to a low dimensional latent space, achieving long-range predictions. However, these latent representations are often non-interpretable, and therefore difficult to manipulate. This work poses video prediction and interpolation as unsupervised latent structure inference followed by a temporal prediction in this latent space. The latent representations capture foreground semantics without explicit supervision such as keypoints or poses. Further, as each landmark can be mapped to a coordinate indicating where a semantic part is positioned, we can reliably interpolate within the coordinate domain to achieve predictable motion interpolation. Given an image decoder capable of mapping these landmarks back to the image domain, we are able to achieve high-quality long-range video interpolation and extrapolation by operating on the landmark representation space.",0
"The task of predicting and interpolating long-range video data is complicated and involves modeling motion trajectories for each visible object, as well as accounting for occlusions, dis-occlusions, and appearance changes. Although optical flow techniques can generalize, they are only suitable for short temporal ranges. To achieve long-range predictions, many methods project video frames to a low dimensional latent space. However, these latent representations are often difficult to manipulate and interpret. This study proposes an unsupervised approach to video prediction and interpolation by inferring latent structures and then making temporal predictions in this space. The proposed method captures foreground semantics without explicit supervision and allows reliable interpolation within the coordinate domain for predictable motion interpolation. By mapping landmarks back to the image domain, high-quality long-range video interpolation and extrapolation can be achieved through operating on the landmark representation space.",1
"Motion estimation (ME) and motion compensation (MC) have been widely used for classical video frame interpolation systems over the past decades. Recently, a number of data-driven frame interpolation methods based on convolutional neural networks have been proposed. However, existing learning based methods typically estimate either flow or compensation kernels, thereby limiting performance on both computational efficiency and interpolation accuracy. In this work, we propose a motion estimation and compensation driven neural network for video frame interpolation. A novel adaptive warping layer is developed to integrate both optical flow and interpolation kernels to synthesize target frame pixels. This layer is fully differentiable such that both the flow and kernel estimation networks can be optimized jointly. The proposed model benefits from the advantages of motion estimation and compensation methods without using hand-crafted features. Compared to existing methods, our approach is computationally efficient and able to generate more visually appealing results. Furthermore, the proposed MEMC-Net can be seamlessly adapted to several video enhancement tasks, e.g., super-resolution, denoising, and deblocking. Extensive quantitative and qualitative evaluations demonstrate that the proposed method performs favorably against the state-of-the-art video frame interpolation and enhancement algorithms on a wide range of datasets.",0
"For many years, classical video frame interpolation systems have relied heavily on motion estimation (ME) and motion compensation (MC). However, recent advancements have introduced data-driven frame interpolation methods that use convolutional neural networks. Despite this progress, existing learning-based methods are limited in their ability to estimate either flow or compensation kernels, resulting in decreased computational efficiency and interpolation accuracy. To address these limitations, we propose a motion estimation and compensation driven neural network that utilizes an adaptive warping layer to integrate both optical flow and interpolation kernels. This layer is fully differentiable, allowing for joint optimization of flow and kernel estimation networks without the need for hand-crafted features. Our approach offers superior computational efficiency and generates more visually appealing results compared to existing methods. Additionally, our MEMC-Net model can be seamlessly adapted to various video enhancement tasks, such as super-resolution, denoising, and deblocking. Our extensive quantitative and qualitative evaluations demonstrate that the proposed method outperforms state-of-the-art video frame interpolation and enhancement algorithms across a wide range of datasets.",1
"Pedestrian action recognition and intention prediction is one of the core issues in the field of autonomous driving. In this research field, action recognition is one of the key technologies. A large number of scholars have done a lot of work to im-prove the accuracy of the algorithm for the task. However, there are relatively few studies and improvements in the computational complexity of algorithms and sys-tem real-time. In the autonomous driving application scenario, the real-time per-formance and ultra-low latency of the algorithm are extremely important evalua-tion indicators, which are directly related to the availability and safety of the au-tonomous driving system. To this end, we construct a bypass enhanced RGB flow model, which combines the previous two-branch algorithm to extract RGB feature information and optical flow feature information respectively. In the train-ing phase, the two branches are merged by distillation method, and the bypass enhancement is combined in the inference phase to ensure accuracy. The real-time behavior of the behavior recognition algorithm is significantly improved on the premise that the accuracy does not decrease. Experiments confirm the superiority and effectiveness of our algorithm.",0
"The recognition and prediction of pedestrian actions is a critical issue in the field of autonomous driving, with action recognition being a key technology. While many scholars have worked on improving the algorithm's accuracy, few studies have focused on reducing the computational complexity of algorithms and ensuring real-time system performance. In autonomous driving scenarios, real-time performance and ultra-low latency are crucial evaluation indicators that directly impact the availability and safety of the autonomous driving system. To address this, we developed a bypass enhanced RGB flow model that merges two-branch algorithms to extract RGB and optical flow feature information and improve accuracy. During training, we merged the two branches using the distillation method, and in the inference phase, we added bypass enhancement to ensure accuracy. Our algorithm significantly improves real-time behavior recognition without decreasing accuracy, as confirmed by experiments.",1
"Due to better video quality and higher frame rate, the performance of multiple object tracking issues has been greatly improved in recent years. However, in real application scenarios, camera motion and noisy per frame detection results degrade the performance of trackers significantly. High-speed and high-quality multiple object trackers are still in urgent demand. In this paper, we propose a new multiple object tracker following the popular tracking-by-detection scheme. We tackle the camera motion problem with an optical flow network and utilize an auxiliary tracker to deal with the missing detection problem. Besides, we use both the appearance and motion information to improve the matching quality. The experimental results on the VisDrone-MOT dataset show that our approach can improve the performance of multiple object tracking significantly while achieving a high efficiency.",0
"In recent years, the performance of multiple object tracking issues has seen a significant improvement due to better video quality and higher frame rate. However, when it comes to real application scenarios, the performance of trackers is significantly degraded by camera motion and noisy per frame detection results. Therefore, the demand for high-speed and high-quality multiple object trackers is still urgent. This paper proposes a new multiple object tracker that follows the popular tracking-by-detection scheme. To tackle the camera motion problem, an optical flow network is utilized, while an auxiliary tracker is employed to deal with the missing detection problem. Moreover, both appearance and motion information are used to improve the matching quality. The experimental results on the VisDrone-MOT dataset indicate that our approach can significantly enhance the performance of multiple object tracking while maintaining high efficiency.",1
"This paper presents a novel obstacle avoidance system for road robots equipped with RGB-D sensor that captures scenes of its way forward. The purpose of the system is to have road robots move around autonomously and constantly without any collision even with small obstacles, which are often missed by existing solutions. For each input RGB-D image, the system uses a new two-stage semantic segmentation network followed by the morphological processing to generate the accurate semantic map containing road and obstacles. Based on the map, the local path planning is applied to avoid possible collision. Additionally, optical flow supervision and motion blurring augmented training scheme is applied to improve temporal consistency between adjacent frames and overcome the disturbance caused by camera shake. Various experiments are conducted to show that the proposed architecture obtains high performance both in indoor and outdoor scenarios.",0
"This article introduces an innovative obstacle avoidance system for road robots that are equipped with RGB-D sensors to capture images of their surroundings. The aim of the system is to enable road robots to move independently and without collision, even with small obstacles that may be overlooked by current solutions. To achieve this, a new two-stage semantic segmentation network is used to generate an accurate semantic map of the road and any obstacles in each input RGB-D image. Local path planning is then applied based on the map to avoid potential collisions. The system also employs optical flow supervision and a motion blurring augmented training scheme to enhance temporal consistency between neighboring frames and counteract camera shake interference. Various experiments have shown that the proposed architecture delivers excellent performance in both indoor and outdoor settings.",1
"Detecting action units (AUs) on human faces is challenging because various AUs make subtle facial appearance change over various regions at different scales. Current works have attempted to recognize AUs by emphasizing important regions. However, the incorporation of expert prior knowledge into region definition remains under-exploited, and current AU detection approaches do not use regional convolutional neural networks (R-CNN) with expert prior knowledge to directly focus on AU-related regions adaptively. By incorporating expert prior knowledge, we propose a novel R-CNN based model named AU R-CNN. The proposed solution offers two main contributions: (1) AU R-CNN directly observes different facial regions, where various AUs are located. Specifically, we define an AU partition rule which encodes the expert prior knowledge into the region definition and RoI-level label definition. This design produces considerably better detection performance than existing approaches. (2) We integrate various dynamic models (including convolutional long short-term memory, two stream network, conditional random field, and temporal action localization network) into AU R-CNN and then investigate and analyze the reason behind the performance of dynamic models. Experiment results demonstrate that \textit{only} static RGB image information and no optical flow-based AU R-CNN surpasses the one fused with dynamic models. AU R-CNN is also superior to traditional CNNs that use the same backbone on varying image resolutions. State-of-the-art recognition performance of AU detection is achieved. The complete network is end-to-end trainable. Experiments on BP4D and DISFA datasets show the effectiveness of our approach. The implementation code is available online.",0
"Facial appearance changes due to various action units (AUs) can be subtle and occur over different regions and scales, making it difficult to detect AUs on human faces. While some approaches have attempted to recognize AUs by emphasizing important regions, they have not fully utilized expert prior knowledge to define these regions. Our proposed solution, AU R-CNN, addresses this issue by incorporating expert prior knowledge into region and RoI-level label definitions. This results in better detection performance than existing approaches. Additionally, we integrate dynamic models into AU R-CNN and find that even without optical flow-based information, AU R-CNN with only static RGB image information outperforms the model fused with dynamic models. Our end-to-end trainable network achieves state-of-the-art recognition performance on BP4D and DISFA datasets. The implementation code is available online.",1
"We present a 3D Convolutional Neural Networks (CNNs) based single shot detector for spatial-temporal action detection tasks. Our model includes: (1) two short-term appearance and motion streams, with single RGB and optical flow image input separately, in order to capture the spatial and temporal information for the current frame; (2) two long-term 3D ConvNet based stream, working on sequences of continuous RGB and optical flow images to capture the context from past frames. Our model achieves strong performance for action detection in video and can be easily integrated into any current two-stream action detection methods. We report a frame-mAP of 71.30% on the challenging UCF101-24 actions dataset, achieving the state-of-the-art result of the one-stage methods. To the best of our knowledge, our work is the first system that combined 3D CNN and SSD in action detection tasks.",0
"We have developed a single shot detector for spatial-temporal action detection tasks using 3D Convolutional Neural Networks (CNNs). Our model comprises two short-term appearance and motion streams that operate on single RGB and optical flow image inputs to capture current frame spatial and temporal information, as well as two long-term 3D ConvNet based streams that work on sequences of continuous RGB and optical flow images to capture context from past frames. Our model demonstrates strong performance for action detection in video and can be easily incorporated into existing two-stream action detection methods. The UCF101-24 actions dataset yielded a frame-mAP of 71.30%, representing a state-of-the-art result for one-stage methods. To the best of our knowledge, our work represents the first system to combine 3D CNN and SSD in action detection tasks.",1
"In the recent year, state-of-the-art for facial micro-expression recognition have been significantly advanced by deep neural networks. The robustness of deep learning has yielded promising performance beyond that of traditional handcrafted approaches. Most works in literature emphasized on increasing the depth of networks and employing highly complex objective functions to learn more features. In this paper, we design a Shallow Triple Stream Three-dimensional CNN (STSTNet) that is computationally light whilst capable of extracting discriminative high level features and details of micro-expressions. The network learns from three optical flow features (i.e., optical strain, horizontal and vertical optical flow fields) computed based on the onset and apex frames of each video. Our experimental results demonstrate the effectiveness of the proposed STSTNet, which obtained an unweighted average recall rate of 0.7605 and unweighted F1-score of 0.7353 on the composite database consisting of 442 samples from the SMIC, CASME II and SAMM databases.",0
"Deep neural networks have made significant advancements in facial micro-expression recognition in recent years, surpassing traditional handcrafted methods. Previous literature has focused on increasing network depth and using complex objective functions to extract more features. However, our paper proposes a Shallow Triple Stream Three-dimensional CNN (STSTNet) that is computationally efficient yet still able to extract discriminative features and details of micro-expressions. STSTNet learns from three optical flow features, namely optical strain, horizontal and vertical optical flow fields, based on onset and apex frames of each video. Our experimental results demonstrate the effectiveness of STSTNet, achieving an unweighted average recall rate of 0.7605 and unweighted F1-score of 0.7353 on a composite database of 442 samples from SMIC, CASME II and SAMM databases.",1
"Avoiding bottleneck situations in crowds is critical for the safety and comfort of people at large events or in public transportation. Based on the work of Lagrangian motion analysis we propose a novel video-based bottleneckdetector by identifying characteristic stowage patterns in crowd-movements captured by optical flow fields. The Lagrangian framework allows to assess complex timedependent crowd-motion dynamics at large temporal scales near the bottleneck by two dimensional Lagrangian fields. In particular we propose long-term temporal filtered Finite Time Lyapunov Exponents (FTLE) fields that provide towards a more global segmentation of the crowd movements and allows to capture its deformations when a crowd is passing a bottleneck. Finally, these deformations are used for an automatic spatio-temporal detection of such situations. The performance of the proposed approach is shown in extensive evaluations on the existing J\""ulich and AGORASET datasets, that we have updated with ground truth data for spatio-temporal bottleneck analysis.",0
"Ensuring the safety and comfort of individuals in crowded areas and public transportation requires the avoidance of bottleneck situations. Our proposed video-based bottleneck detector utilizes Lagrangian motion analysis to identify stowage patterns in crowd movements captured by optical flow fields. This framework enables the assessment of complex time-dependent crowd-motion dynamics at large temporal scales near the bottleneck using two-dimensional Lagrangian fields. We suggest the use of long-term temporal filtered Finite Time Lyapunov Exponents (FTLE) fields to achieve a more comprehensive segmentation of the crowd movements and to capture its deformations as it passes through a bottleneck. These deformations are then utilized to automatically detect spatio-temporal bottleneck situations. Our approach's efficacy is supported by extensive evaluations on the existing J\""ulich and AGORASET datasets, which we have updated with ground truth data for spatio-temporal bottleneck analysis.",1
"Event cameras are vision sensors that record asynchronous streams of per-pixel brightness changes, referred to as ""events"". They have appealing advantages over frame-based cameras for computer vision, including high temporal resolution, high dynamic range, and no motion blur. Due to the sparse, non-uniform spatiotemporal layout of the event signal, pattern recognition algorithms typically aggregate events into a grid-based representation and subsequently process it by a standard vision pipeline, e.g., Convolutional Neural Network (CNN). In this work, we introduce a general framework to convert event streams into grid-based representations through a sequence of differentiable operations. Our framework comes with two main advantages: (i) allows learning the input event representation together with the task dedicated network in an end to end manner, and (ii) lays out a taxonomy that unifies the majority of extant event representations in the literature and identifies novel ones. Empirically, we show that our approach to learning the event representation end-to-end yields an improvement of approximately 12% on optical flow estimation and object recognition over state-of-the-art methods.",0
"Vision sensors known as event cameras capture asynchronous streams of per-pixel changes in brightness, called ""events"", which offer numerous benefits over frame-based cameras for computer vision tasks, such as high dynamic range, high temporal resolution, and no motion blur. However, due to the sparse and non-uniform spatiotemporal layout of the event signal, pattern recognition algorithms typically aggregate events into a grid-based representation and process them using a standard vision pipeline, like Convolutional Neural Network (CNN). In this study, we present a general framework that utilizes a sequence of differentiable operations to convert event streams into grid-based representations. Our framework has two significant advantages: it enables learning the input event representation and the task-dedicated network together in an end-to-end manner, and it unifies a majority of existing event representations in the literature and identifies new ones. Our empirical results demonstrate that our approach to learning the event representation end-to-end enhances optical flow estimation and object recognition by approximately 12% compared to state-of-the-art methods.",1
"Video objection detection (VID) has been a rising research direction in recent years. A central issue of VID is the appearance degradation of video frames caused by fast motion. This problem is essentially ill-posed for a single frame. Therefore, aggregating features from other frames becomes a natural choice. Existing methods rely heavily on optical flow or recurrent neural networks for feature aggregation. However, these methods emphasize more on the temporally nearby frames. In this work, we argue that aggregating features in the full-sequence level will lead to more discriminative and robust features for video object detection. To achieve this goal, we devise a novel Sequence Level Semantics Aggregation (SELSA) module. We further demonstrate the close relationship between the proposed method and the classic spectral clustering method, providing a novel view for understanding the VID problem. We test the proposed method on the ImageNet VID and the EPIC KITCHENS dataset and achieve new state-of-the-art results. Our method does not need complicated postprocessing methods such as Seq-NMS or Tubelet rescoring, which keeps the pipeline simple and clean.",0
"In recent years, Video Objection Detection (VID) has become a popular research direction. A major challenge in VID is the degradation of video frames due to fast motion, which is an ill-posed problem for a single frame. As a result, existing methods rely on optical flow or recurrent neural networks to aggregate features from temporally nearby frames. However, these methods may not be optimal as they fail to capture the full sequence of frames. In this study, we propose a new approach called Sequence Level Semantics Aggregation (SELSA) module, which aggregates features at the full-sequence level to enhance the discriminative and robust features for VID. We also demonstrate the connection between our approach and the classic spectral clustering method. Our method achieves state-of-the-art results on the ImageNet VID and EPIC KITCHENS datasets, without the need for complicated postprocessing methods such as Seq-NMS or Tubelet rescoring, thus keeping the pipeline simple and straightforward.",1
"As a milestone for video object segmentation, one-shot video object segmentation (OSVOS) has achieved a large margin compared to the conventional optical-flow based methods regarding to the segmentation accuracy. Its excellent performance mainly benefit from the three-step training mechanism, that are: (1) acquiring object features on the base dataset (i.e. ImageNet), (2) training the parent network on the training set of the target dataset (i.e. DAVIS-2016) to be capable of differentiating the object of interest from the background. (3) online fine-tuning the interested object on the first frame of the target test set to overfit its appearance, then the model can be utilized to segment the same object in the rest frames of that video. In this paper, we argue that for the step (2), OSVOS has the limitation to 'overemphasize' the generic semantic object information while 'dilute' the instance cues of the object(s), which largely block the whole training process. Through adding a common module, video loss, which we formulate with various forms of constraints (including weighted BCE loss, high-dimensional triplet loss, as well as a novel mixed instance-aware video loss), to train the parent network in the step (2), the network is then better prepared for the step (3), i.e. online fine-tuning on the target instance. Through extensive experiments using different network structures as the backbone, we show that the proposed video loss module can improve the segmentation performance significantly, compared to that of OSVOS. Meanwhile, since video loss is a common module, it can be generalized to other fine-tuning based methods and similar vision tasks such as depth estimation and saliency detection.",0
"The one-shot video object segmentation (OSVOS) has surpassed traditional optical-flow based techniques in terms of segmentation accuracy and is considered a milestone in video object segmentation. It owes its outstanding performance to a three-step training mechanism. Firstly, it acquires object features on the ImageNet dataset. Secondly, it trains the parent network on the training set of the target dataset (DAVIS-2016) to differentiate the object of interest from the background. Finally, it fine-tunes the interested object on the first frame of the target test set to overfit its appearance and segment the same object in the rest of the frames. However, we contend that OSVOS has limitations in the second step, as it tends to overemphasize generic semantic object information and dilute instance cues, hindering the entire training process. To address this, we introduce a common module called video loss, which includes various forms of constraints. This module significantly improves segmentation performance, as demonstrated by experiments using different network structures. Moreover, video loss can also be applied to other fine-tuning based methods and similar vision tasks, such as depth estimation and saliency detection.",1
"This draft summarizes some basics about geometric computer vision needed to implement efficient computer vision algorithms for applications that use measurements from at least one digital camera mounted on a moving platform with a special focus on automotive applications processing image streams taken from cameras mounted on a car. Our intention is twofold: On the one hand, we would like to introduce well-known basic geometric relations in a compact way that can also be found in lecture books about geometric computer vision like [1, 2]. On the other hand, we would like to share some experience about subtleties that should be taken into account in order to set up quite simple but robust and fast vision algorithms that are able to run in real time. We added a conglomeration of literature, we found to be relevant when implementing basic algorithms like optical flow, visual odometry and structure from motion. The reader should get some feeling about how the estimates of these algorithms are interrelated, which parts of the algorithms are critical in terms of robustness and what kind of additional assumptions can be useful to constrain the solution space of the underlying usually non-convex optimization problems.",0
"The purpose of this draft is to provide an overview of the fundamental concepts of geometric computer vision that are necessary for implementing efficient algorithms for computer vision applications involving measurements taken from a digital camera mounted on a moving platform, with a particular focus on automotive applications that process image streams captured by cameras on a car. Our aim is two-fold: Firstly, we aim to give a concise introduction to the commonly known geometric relations that can be found in textbooks on geometric computer vision, such as [1, 2]. Secondly, we aim to share our experience regarding the nuances that need to be considered when creating simple yet robust and fast vision algorithms that can operate in real time. We have included relevant literature on implementing basic algorithms like optical flow, visual odometry and structure from motion. The reader will gain an understanding of how the estimates of these algorithms are interconnected, which parts of the algorithms are crucial for robustness, and what assumptions can be used to limit the solution space of the underlying non-convex optimization problems.",1
"In this paper, we revive the use of old-fashioned handcrafted video representations for action recognition and put new life into these techniques via a CNN-based hallucination step. Despite of the use of RGB and optical flow frames, the I3D model (amongst others) thrives on combining its output with the Improved Dense Trajectory (IDT) and extracted with its low-level video descriptors encoded via Bag-of-Words (BoW) and Fisher Vectors (FV). Such a fusion of CNNs and handcrafted representations is time-consuming due to pre-processing, descriptor extraction, encoding and tuning parameters. Thus, we propose an end-to-end trainable network with streams which learn the IDT-based BoW/FV representations at the training stage and are simple to integrate with the I3D model. Specifically, each stream takes I3D feature maps ahead of the last 1D conv. layer and learns to `translate' these maps to BoW/FV representations. Thus, our model can hallucinate and use such synthesized BoW/FV representations at the testing stage. We show that even features of the entire I3D optical flow stream can be hallucinated thus simplifying the pipeline. Our model saves 20-55h of computations and yields state-of-the-art results on four publicly available datasets.",0
"In this article, we bring back the use of traditional handcrafted video representations for recognizing actions and enhance these techniques with a CNN-based hallucination process. Despite the use of RGB and optical flow frames, the I3D model (and others) rely on combining their output with the Improved Dense Trajectory (IDT) and low-level video descriptors, encoded via Bag-of-Words (BoW) and Fisher Vectors (FV). However, this fusion of CNNs and handcrafted representations is time-consuming due to pre-processing, descriptor extraction, encoding, and parameter tuning. Thus, we propose an end-to-end trainable network with streams that learn the IDT-based BoW/FV representations during training and are easy to integrate with the I3D model. Each stream takes I3D feature maps before the last 1D convolutional layer and learns to transform these maps into BoW/FV representations. This allows our model to synthesize and use BoW/FV representations during testing, even for the entire I3D optical flow stream, simplifying the pipeline. Our model saves up to 55 hours of computations and produces state-of-the-art results on four publicly available datasets.",1
"Transferring image-based object detectors to the domain of videos remains a challenging problem. Previous efforts mostly exploit optical flow to propagate features across frames, aiming to achieve a good trade-off between accuracy and efficiency. However, introducing an extra model to estimate optical flow can significantly increase the overall model size. The gap between optical flow and high-level features can also hinder it from establishing spatial correspondence accurately. Instead of relying on optical flow, this paper proposes a novel module called Progressive Sparse Local Attention (PSLA), which establishes the spatial correspondence between features across frames in a local region with progressively sparser stride and uses the correspondence to propagate features. Based on PSLA, Recursive Feature Updating (RFU) and Dense Feature Transforming (DenseFT) are proposed to model temporal appearance and enrich feature representation respectively in a novel video object detection framework. Experiments on ImageNet VID show that our method achieves the best accuracy compared to existing methods with smaller model size and acceptable runtime speed.",0
"The task of transferring object detectors from images to videos is still a difficult one. Past attempts have mainly relied on optical flow to transfer features between frames, with the aim of striking a balance between accuracy and efficiency. However, incorporating an additional model to estimate optical flow can significantly increase the overall size of the model. Moreover, the difference between optical flow and high-level features can impede accurate spatial correspondence. This paper proposes a new module called Progressive Sparse Local Attention (PSLA) that establishes spatial correspondence between features in a local region across frames using progressively sparser strides. This correspondence is then used to transfer features. Recursive Feature Updating (RFU) and Dense Feature Transforming (DenseFT) are introduced to model temporal appearance and enhance feature representation, respectively, in a new video object detection framework. Our method surpasses existing approaches in accuracy with a smaller model size and reasonable run time, as demonstrated by experiments on ImageNet VID.",1
"We study the video super-resolution (SR) problem for facilitating video analytics tasks, e.g. action recognition, instead of for visual quality. The popular action recognition methods based on convolutional networks, exemplified by two-stream networks, are not directly applicable on video of low spatial resolution. This can be remedied by performing video SR prior to recognition, which motivates us to improve the SR procedure for recognition accuracy. Tailored for two-stream action recognition networks, we propose two video SR methods for the spatial and temporal streams respectively. On the one hand, we observe that regions with action are more important to recognition, and we propose an optical-flow guided weighted mean-squared-error loss for our spatial-oriented SR (SoSR) network to emphasize the reconstruction of moving objects. On the other hand, we observe that existing video SR methods incur temporal discontinuity between frames, which also worsens the recognition accuracy, and we propose a siamese network for our temporal-oriented SR (ToSR) training that emphasizes the temporal continuity between consecutive frames. We perform experiments using two state-of-the-art action recognition networks and two well-known datasets--UCF101 and HMDB51. Results demonstrate the effectiveness of our proposed SoSR and ToSR in improving recognition accuracy.",0
"Our focus is on using video super-resolution (SR) to aid video analytics tasks, such as action recognition, rather than improving visual quality. Popular convolutional network-based action recognition methods, like two-stream networks, are not suitable for low spatial resolution videos. To address this, we aim to enhance the SR process for better recognition accuracy. We propose two video SR methods, one for the spatial stream and the other for the temporal stream, tailored to two-stream action recognition networks. Our spatial-oriented SR (SoSR) network prioritizes reconstructing moving objects using an optical-flow guided weighted mean-squared-error loss, while our temporal-oriented SR (ToSR) network emphasizes the temporal continuity between consecutive frames. We evaluate our proposed methods on UCF101 and HMDB51 datasets using two state-of-the-art action recognition networks and demonstrate their effectiveness in improving recognition accuracy.",1
"When a deep neural network is trained on data with only image-level labeling, the regions activated in each image tend to identify only a small region of the target object. We propose a method of using videos automatically harvested from the web to identify a larger region of the target object by using temporal information, which is not present in the static image. The temporal variations in a video allow different regions of the target object to be activated. We obtain an activated region in each frame of a video, and then aggregate the regions from successive frames into a single image, using a warping technique based on optical flow. The resulting localization maps cover more of the target object, and can then be used as proxy ground-truth to train a segmentation network. This simple approach outperforms existing methods under the same level of supervision, and even approaches relying on extra annotations. Based on VGG-16 and ResNet 101 backbones, our method achieves the mIoU of 65.0 and 67.4, respectively, on PASCAL VOC 2012 test images, which represents a new state-of-the-art.",0
"When data with only image-level labeling is used to train a deep neural network, the regions activated in each image typically only identify a small part of the target object. To address this limitation, we propose a novel approach that utilizes videos obtained from the internet to identify a larger region of the target object by taking advantage of the temporal information present in videos. By analyzing the temporal variations in a video, different regions of the target object can be activated. Our approach involves identifying an activated region in each frame of a video and then combining these regions from successive frames into a single image using a warping technique based on optical flow. This results in localization maps that cover more of the target object and can be used as a proxy ground-truth to train a segmentation network. Our approach outperforms existing methods that use the same level of supervision and even those that rely on extra annotations. Using VGG-16 and ResNet 101 backbones, our method achieves a mIoU of 65.0 and 67.4, respectively, on PASCAL VOC 2012 test images, which represents a new state-of-the-art.",1
"Anomaly detection plays in many fields of research, along with the strongly related task of outlier detection, a very important role. Especially within the context of the automated analysis of video material recorded by surveillance cameras, abnormal situations can be of very different nature. For this purpose this work investigates Generative-Adversarial-Network-based methods (GAN) for anomaly detection related to surveillance applications. The focus is on the usage of static camera setups, since this kind of camera is one of the most often used and belongs to the lower price segment. In order to address this task, multiple subtasks are evaluated, including the influence of existing optical flow methods for the incorporation of short-term temporal information, different forms of network setups and losses for GANs, and the use of morphological operations for further performance improvement. With these extension we achieved up to 2.4% better results. Furthermore, the final method reduced the anomaly detection error for GAN-based methods by about 42.8%.",0
"An important role is played by anomaly detection in various fields of research, as well as the closely related task of outlier detection. Unusual situations can take many forms, particularly in the context of analyzing automated surveillance camera footage. This study focuses on using Generative-Adversarial-Network-based methods (GAN) for anomaly detection in surveillance applications. Specifically, the study examines the use of static camera setups, which are commonly used and less expensive. Multiple subtasks are evaluated in order to address this task, including the impact of optical flow methods for incorporating short-term temporal information, different network setups and losses for GANs, and the use of morphological operations to improve performance. By incorporating these extensions, the results improved by up to 2.4%. Furthermore, the final method was able to reduce the anomaly detection error for GAN-based methods by about 42.8%.",1
"In this paper we present mono-stixels, a compact environment representation specially designed for dynamic street scenes. Mono-stixels are a novel approach to estimate stixels from a monocular camera sequence instead of the traditionally used stereo depth measurements. Our approach jointly infers the depth, motion and semantic information of the dynamic scene as a 1D energy minimization problem based on optical flow estimates, pixel-wise semantic segmentation and camera motion. The optical flow of a stixel is described by a homography. By applying the mono-stixel model the degrees of freedom of a stixel-homography are reduced to only up to two degrees of freedom. Furthermore, we exploit a scene model and semantic information to handle moving objects. In our experiments we use the public available DeepFlow for optical flow estimation and FCN8s for the semantic information as inputs and show on the KITTI 2015 dataset that mono-stixels provide a compact and reliable depth reconstruction of both the static and moving parts of the scene. Thereby, mono-stixels overcome the limitation to static scenes of previous structure-from-motion approaches.",0
"The article introduces mono-stixels, a new way of representing dynamic street scenes in a compact form. Unlike traditional methods that rely on stereo depth measurements, mono-stixels estimate stixels from a monocular camera sequence. The process involves jointly determining the depth, motion, and semantic information of the scene using optical flow, semantic segmentation, and camera motion. By using a homography to describe the optical flow of a stixel, the mono-stixel model reduces the degrees of freedom to only two. Additionally, the approach incorporates a scene model and semantic information to handle moving objects. The study used DeepFlow and FCN8s as inputs in experiments and demonstrated that mono-stixels provide a reliable depth reconstruction of both static and moving parts of the scene. This overcomes the limitation of previous structure-from-motion approaches, which were only able to handle static scenes.",1
"Images of static scenes submerged beneath a wavy water surface exhibit severe non-rigid distortions. The physics of water flow suggests that water surfaces possess spatio-temporal smoothness and temporal periodicity. Hence they possess a sparse representation in the 3D discrete Fourier (DFT) basis. Motivated by this, we pose the task of restoration of such video sequences as a compressed sensing (CS) problem. We begin by tracking a few salient feature points across the frames of a video sequence of the submerged scene. Using these point trajectories, we show that the motion fields at all other (non-tracked) points can be effectively estimated using a typical CS solver. This by itself is a novel contribution in the field of non-rigid motion estimation. We show that this method outperforms state of the art algorithms for underwater image restoration. We further consider a simple optical flow algorithm based on local polynomial expansion of the image frames (PEOF). Surprisingly, we demonstrate that PEOF is more efficient and often outperforms all the state of the art methods in terms of numerical measures. Finally, we demonstrate that a two-stage approach consisting of the CS step followed by PEOF much more accurately preserves the image structure and improves the (visual as well as numerical) video quality as compared to just the PEOF stage.",0
"When viewing still scenes underwater with a wavy surface, the resulting images are subject to significant distortions. However, the smoothness and periodicity of water flow suggest that these surfaces can be represented sparsely in the 3D discrete Fourier basis. To restore such videos, we approach the problem using compressed sensing. First, we track key feature points across the video frames and use these trajectories to estimate the motion fields of non-tracked points with a CS solver, which is a novel contribution to non-rigid motion estimation. This method proves to be more effective than existing algorithms for underwater image restoration. We also explore a simple optical flow algorithm based on local polynomial expansion of the image frames (PEOF), which surprisingly outperforms state-of-the-art methods in terms of numerical measures. Finally, we demonstrate that a two-stage approach combining the CS and PEOF steps results in significantly improved image structure and video quality compared to using PEOF alone.",1
"Egocentric action anticipation consists in understanding which objects the camera wearer will interact with in the near future and which actions they will perform. We tackle the problem proposing an architecture able to anticipate actions at multiple temporal scales using two LSTMs to 1) summarize the past, and 2) formulate predictions about the future. The input video is processed considering three complimentary modalities: appearance (RGB), motion (optical flow) and objects (object-based features). Modality-specific predictions are fused using a novel Modality ATTention (MATT) mechanism which learns to weigh modalities in an adaptive fashion. Extensive evaluations on two large-scale benchmark datasets show that our method outperforms prior art by up to +7% on the challenging EPIC-Kitchens dataset including more than 2500 actions, and generalizes to EGTEA Gaze+. Our approach is also shown to generalize to the tasks of early action recognition and action recognition. Our method is ranked first in the public leaderboard of the EPIC-Kitchens egocentric action anticipation challenge 2019. Please see our web pages for code and examples: http://iplab.dmi.unict.it/rulstm - https://github.com/fpv-iplab/rulstm.",0
"The concept of egocentric action anticipation involves predicting the objects that a camera wearer will interact with and the actions they will perform in the near future. To address this challenge, we propose an architecture that utilizes two LSTMs to summarize past events and make future predictions across multiple temporal scales. Our approach incorporates three different modalities (RGB appearance, optical flow motion, and object-based features) and employs a Modality ATTention (MATT) mechanism to adaptively weigh the predictions from each modality. We have extensively evaluated our method on two benchmark datasets and achieved superior performance compared to prior methods, including a 7% improvement on the challenging EPIC-Kitchens dataset. Furthermore, our approach can be generalized to other related tasks such as early action recognition and action recognition. Our method is currently ranked first on the public leaderboard for the EPIC-Kitchens egocentric action anticipation challenge 2019. More information about our code and examples can be found on our web pages: http://iplab.dmi.unict.it/rulstm and https://github.com/fpv-iplab/rulstm.",1
"In this paper, we develop a modified differential Structure from Motion (SfM) algorithm that can estimate relative pose from two consecutive frames despite of Rolling Shutter (RS) artifacts. In particular, we show that under constant velocity assumption, the errors induced by the rolling shutter effect can be easily rectified by a linear scaling operation on each optical flow. We further propose a 9-point algorithm to recover the relative pose of a rolling shutter camera that undergoes constant acceleration motion. We demonstrate that the dense depth maps recovered from the relative pose of the RS camera can be used in a RS-aware warping for image rectification to recover high-quality Global Shutter (GS) images. Experiments on both synthetic and real RS images show that our RS-aware differential SfM algorithm produces more accurate results on relative pose estimation and 3D reconstruction from images distorted by RS effect compared to standard SfM algorithms that assume a GS camera model. We also demonstrate that our RS-aware warping for image rectification method outperforms state-of-the-art commercial software products, i.e. Adobe After Effects and Apple Imovie, at removing RS artifacts.",0
"The aim of this research is to develop an altered differential Structure from Motion (SfM) algorithm that can estimate relative pose from two consecutive frames, even in the presence of Rolling Shutter (RS) artifacts. Our study suggests that the errors induced by the rolling shutter effect can be rectified by a linear scaling operation on each optical flow, provided that a constant velocity assumption is maintained. Additionally, we propose a 9-point algorithm that can recover the relative pose of a rolling shutter camera that undergoes constant acceleration motion. The recovered dense depth maps can be utilized in a RS-aware warping technique for image rectification to obtain high-quality Global Shutter (GS) images. Our experiments on synthetic and real RS images indicate that our RS-aware differential SfM algorithm provides more precise results on relative pose estimation and 3D reconstruction from images distorted by RS effect as compared to standard SfM algorithms that assume a GS camera model. Furthermore, our RS-aware warping for image rectification method outperforms commercial software products such as Adobe After Effects and Apple Imovie in removing RS artifacts.",1
"We address the challenging task of foreground object discovery and segmentation in video. We introduce an efficient solution, suitable for both unsupervised and supervised scenarios, based on a spacetime graph representation of the video sequence. We ensure a fine grained representation with one-to-one correspondences between graph nodes and video pixels. We formulate the task as a spectral clustering problem by exploiting the spatio-temporal consistency between the scene elements in terms of motion and appearance. Graph nodes that belong to the main object of interest should form a strong cluster, as they are linked through long range optical flow chains and have similar motion and appearance features along those chains. On one hand, the optimization problem aims to maximize the segmentation clustering score based on the motion structure through space and time. On the other hand, the segmentation should be consistent with respect to node features. Our approach leads to a graph formulation in which the segmentation solution becomes the principal eigenvector of a novel Feature-Motion matrix. While the actual matrix is not computed explicitly, the proposed algorithm efficiently computes, in a few iteration steps, the principal eigenvector that captures the segmentation of the main object in the video. The proposed algorithm, GO-VOS, produces a global optimum solution and, consequently, it does not depend on initialization. In practice, GO-VOS achieves state of the art results on three challenging datasets used in current literature: DAVIS, SegTrack and YouTube-Objects.",0
"Our objective is to tackle the demanding task of identifying and segmenting foreground objects in videos. To accomplish this, we have devised a proficient solution that is applicable to both supervised and unsupervised scenarios. Our approach involves representing the video sequence as a spacetime graph, ensuring that each graph node corresponds to a single video pixel for a more detailed representation. We have formulated the problem as a spectral clustering task by taking into account the motion and appearance consistency among the scene elements. Our method aims to have the graph nodes belonging to the main subject form a robust cluster, linked through long-range optical flow chains and sharing similar motion and appearance features. We have introduced GO-VOS, which leads to a graph formulation where the segmentation solution is the main eigenvector of a novel Feature-Motion matrix. Our algorithm efficiently computes this principal eigenvector, resulting in a global optimal solution that is not dependent on initialization. GO-VOS has achieved impressive results on several challenging datasets such as DAVIS, SegTrack, and YouTube-Objects, making it a state-of-the-art solution in the current literature.",1
"In this paper, we propose a convolutional layer inspired by optical flow algorithms to learn motion representations. Our representation flow layer is a fully-differentiable layer designed to capture the `flow' of any representation channel within a convolutional neural network for action recognition. Its parameters for iterative flow optimization are learned in an end-to-end fashion together with the other CNN model parameters, maximizing the action recognition performance. Furthermore, we newly introduce the concept of learning `flow of flow' representations by stacking multiple representation flow layers. We conducted extensive experimental evaluations, confirming its advantages over previous recognition models using traditional optical flows in both computational speed and performance. Code/models available here: https://piergiaj.github.io/rep-flow-site/",0
"The paper proposes a convolutional layer that is influenced by optical flow algorithms, which aims to acquire motion representations. The representation flow layer is created to be fully-differentiable and can capture the flow of any representation channel within a convolutional neural network for action recognition. The layer's parameters for iterative flow optimization are learned simultaneously with the other CNN model parameters in an end-to-end manner to enhance the action recognition performance. Additionally, the paper introduces the concept of learning the flow of flow representations by stacking multiple representation flow layers. The study conducted extensive experimental evaluations that demonstrated its superiority over previous recognition models using traditional optical flows in terms of both computational speed and performance. Code/models are available at https://piergiaj.github.io/rep-flow-site/.",1
"Video deblurring is a challenging task due to the spatially variant blur caused by camera shake, object motions, and depth variations, etc. Existing methods usually estimate optical flow in the blurry video to align consecutive frames or approximate blur kernels. However, they tend to generate artifacts or cannot effectively remove blur when the estimated optical flow is not accurate. To overcome the limitation of separate optical flow estimation, we propose a Spatio-Temporal Filter Adaptive Network (STFAN) for the alignment and deblurring in a unified framework. The proposed STFAN takes both blurry and restored images of the previous frame as well as blurry image of the current frame as input, and dynamically generates the spatially adaptive filters for the alignment and deblurring. We then propose the new Filter Adaptive Convolutional (FAC) layer to align the deblurred features of the previous frame with the current frame and remove the spatially variant blur from the features of the current frame. Finally, we develop a reconstruction network which takes the fusion of two transformed features to restore the clear frames. Both quantitative and qualitative evaluation results on the benchmark datasets and real-world videos demonstrate that the proposed algorithm performs favorably against state-of-the-art methods in terms of accuracy, speed as well as model size.",0
"The removal of blur from videos is difficult because of the varying types of blur resulting from camera shake, object movements, and changes in depth. Existing methods often use optical flow estimation to align frames or approximate blur kernels, but these methods can result in artifacts or be ineffective if the optical flow estimation is inaccurate. To address these limitations, we propose the Spatio-Temporal Filter Adaptive Network (STFAN), which combines alignment and deblurring into a single framework. The STFAN takes input from both blurry and restored images of the previous frame, as well as the blurry image of the current frame, and uses dynamically generated spatially adaptive filters to align and deblur the frames. We also introduce the Filter Adaptive Convolutional (FAC) layer to align the deblurred features of the previous frame with the current frame and remove spatially variant blur from the features of the current frame. Finally, we develop a reconstruction network that fuses the transformed features to restore clear frames. Our algorithm outperforms state-of-the-art methods in terms of accuracy, speed, and model size on benchmark datasets and real-world videos.",1
"This paper presents novel techniques for recovering 3D dense scene flow, based on differential analysis of 4D light fields. The key enabling result is a per-ray linear equation, called the ray flow equation, that relates 3D scene flow to 4D light field gradients. The ray flow equation is invariant to 3D scene structure and applicable to a general class of scenes, but is under-constrained (3 unknowns per equation). Thus, additional constraints must be imposed to recover motion. We develop two families of scene flow algorithms by leveraging the structural similarity between ray flow and optical flow equations: local 'Lucas-Kanade' ray flow and global 'Horn-Schunck' ray flow, inspired by corresponding optical flow methods. We also develop a combined local-global method by utilizing the correspondence structure in the light fields. We demonstrate high precision 3D scene flow recovery for a wide range of scenarios, including rotation and non-rigid motion. We analyze the theoretical and practical performance limits of the proposed techniques via the light field structure tensor, a 3x3 matrix that encodes the local structure of light fields. We envision that the proposed analysis and algorithms will lead to design of future light-field cameras that are optimized for motion sensing, in addition to depth sensing.",0
"New techniques for recovering 3D dense scene flow are presented in this paper. The techniques are based on differential analysis of 4D light fields and include a per-ray linear equation called the ray flow equation that relates 3D scene flow to 4D light field gradients. While the ray flow equation is invariant to 3D scene structure and applicable to various scenes, it is under-constrained, with three unknowns per equation. Additional constraints are therefore necessary to recover motion. Two families of scene flow algorithms are developed by leveraging the similarity between ray flow and optical flow equations: local 'Lucas-Kanade' ray flow and global 'Horn-Schunck' ray flow, inspired by corresponding optical flow methods. A combined local-global method is also developed, utilizing the correspondence structure in the light fields. The techniques demonstrate high precision 3D scene flow recovery for a wide range of scenarios, including rotation and non-rigid motion. The theoretical and practical performance limits of the techniques are analyzed via the light field structure tensor, a 3x3 matrix encoding the local structure of light fields. The proposed analysis and algorithms are expected to lead to the design of future light-field cameras optimized for motion sensing, in addition to depth sensing.",1
"Most of current Convolution Neural Network (CNN) based methods for optical flow estimation focus on learning optical flow on synthetic datasets with groundtruth, which is not practical. In this paper, we propose an unsupervised optical flow estimation framework named PCLNet. It uses pyramid Convolution LSTM (ConvLSTM) with the constraint of adjacent frame reconstruction, which allows flexibly estimating multi-frame optical flows from any video clip. Besides, by decoupling motion feature learning and optical flow representation, our method avoids complex short-cut connections used in existing frameworks while improving accuracy of optical flow estimation. Moreover, different from those methods using specialized CNN architectures for capturing motion, our framework directly learns optical flow from the features of generic CNNs and thus can be easily embedded in any CNN based frameworks for other tasks. Extensive experiments have verified that our method not only estimates optical flow effectively and accurately, but also obtains comparable performance on action recognition.",0
"Currently, most Convolutional Neural Network (CNN) based methods utilized for estimating optical flow concentrate on acquiring knowledge about optical flow through synthetic datasets with groundtruth, which is not practical. This research proposes an unsupervised optical flow estimation framework called PCLNet, which uses a pyramid Convolution LSTM (ConvLSTM) with the constraint of adjacent frame reconstruction. This allows for flexible estimation of multi-frame optical flows from any video clip. Additionally, our method decouples motion feature learning and optical flow representation, avoiding the complex short-cut connections used in existing frameworks while improving the accuracy of optical flow estimation. Unlike other methods using specialized CNN architectures for capturing motion, our framework learns optical flow directly from the features of generic CNNs, making it easy to embed in any CNN based frameworks for other tasks. Extensive experiments have shown that our method not only estimates optical flow effectively and accurately, but also achieves comparable performance on action recognition.",1
"Video stabilization algorithms are of greater importance nowadays with the prevalence of hand-held devices which unavoidably produce videos with undesirable shaky motions. In this paper we propose a data-driven online video stabilization method along with a paired dataset for deep learning. The network processes each unsteady frame progressively in a multi-scale manner, from low resolution to high resolution, and then outputs an affine transformation to stabilize the frame. Different from conventional methods which require explicit feature tracking or optical flow estimation, the underlying stabilization process is learned implicitly from the training data, and the stabilization process can be done online. Since there are limited public video stabilization datasets available, we synthesized unstable videos with different extent of shake that simulate real-life camera movement. Experiments show that our method is able to outperform other stabilization methods in several unstable samples while remaining comparable in general. Also, our method is tested on complex contents and found robust enough to dampen these samples to some extent even it was not explicitly trained in the contents.",0
"As hand-held devices are becoming increasingly common, the need for video stabilization algorithms is greater than ever. This article proposes an online video stabilization method that uses a paired dataset for deep learning. The network processes each shaky frame progressively in a multi-scale manner, starting from low resolution to high resolution, and then provides an affine transformation to stabilize the frame. Unlike traditional methods that require explicit feature tracking or optical flow estimation, this approach implicitly learns the stabilization process from the training data and can be done online. To address the lack of public video stabilization datasets, unstable videos with varying degrees of shake were synthesized to simulate real-life camera movement. The results show that the proposed method outperforms other stabilization techniques in several unstable samples while remaining comparable in general. Additionally, the approach was tested on complex content and found to be robust enough to dampen the shaking to some extent, even though it was not specifically trained on that content.",1
"We focus on the word-level visual lipreading, which requires recognizing the word being spoken, given only the video but not the audio. State-of-the-art methods explore the use of end-to-end neural networks, including a shallow (up to three layers) 3D convolutional neural network (CNN) + a deep 2D CNN (e.g., ResNet) as the front-end to extract visual features, and a recurrent neural network (e.g., bidirectional LSTM) as the back-end for classification. In this work, we propose to replace the shallow 3D CNNs + deep 2D CNNs front-end with recent successful deep 3D CNNs --- two-stream (i.e., grayscale video and optical flow streams) I3D. We evaluate different combinations of front-end and back-end modules with the grayscale video and optical flow inputs on the LRW dataset. The experiments show that, compared to the shallow 3D CNNs + deep 2D CNNs front-end, the deep 3D CNNs front-end with pre-training on the large-scale image and video datasets (e.g., ImageNet and Kinetics) can improve the classification accuracy. Also, we demonstrate that using the optical flow input alone can achieve comparable performance as using the grayscale video as input. Moreover, the two-stream network using both the grayscale video and optical flow inputs can further improve the performance. Overall, our two-stream I3D front-end with a Bi-LSTM back-end results in an absolute improvement of 5.3% over the previous art on the LRW dataset.",0
"Our research focuses on recognizing spoken words through visual lipreading without the use of audio. To achieve this, current methods use end-to-end neural networks consisting of a shallow 3D CNN along with a deep 2D CNN to extract visual features, and a recurrent neural network for classification. However, we propose replacing this front-end with the successful deep 3D CNNs, specifically the two-stream I3D, which uses grayscale video and optical flow streams. We conducted experiments on the LRW dataset using different front-end and back-end modules with grayscale video and optical flow inputs. Results show that the deep 3D CNN front-end with pre-training on large-scale datasets such as ImageNet and Kinetics can significantly improve accuracy compared to the previous shallow 3D CNN and deep 2D CNN approach. We also found that using optical flow input alone can achieve comparable performance to grayscale video input, while using both inputs further improves accuracy. Our two-stream I3D front-end and Bi-LSTM back-end achieved a 5.3% improvement over previous methods on the LRW dataset.",1
"Many road accidents occur due to distracted drivers. Today, driver monitoring is essential even for the latest autonomous vehicles to alert distracted drivers in order to take over control of the vehicle in case of emergency. In this paper, a spatio-temporal approach is applied to classify drivers' distraction level and movement decisions using convolutional neural networks (CNNs). We approach this problem as action recognition to benefit from temporal information in addition to spatial information. Our approach relies on features extracted from sparsely selected frames of an action using a pre-trained BN-Inception network. Experiments show that our approach outperforms the state-of-the art results on the Distracted Driver Dataset (96.31%), with an accuracy of 99.10% for 10-class classification while providing real-time performance. We also analyzed the impact of fusion using RGB and optical flow modalities with a very recent data level fusion strategy. The results on the Distracted Driver and Brain4Cars datasets show that fusion of these modalities further increases the accuracy.",0
"The number of road accidents caused by distracted drivers is high. Even the latest autonomous vehicles require driver monitoring to alert distracted drivers and enable the vehicle to be taken over in case of emergency. This research paper proposes a spatio-temporal approach to classify drivers' distraction level and movement decisions using convolutional neural networks (CNNs) as action recognition. The approach uses pre-trained BN-Inception network to extract features from sparsely selected frames of an action. The experiments show that the proposed approach outperforms the state-of-the-art results on the Distracted Driver Dataset with an accuracy of 99.10% for 10-class classification while providing real-time performance. The impact of fusion using RGB and optical flow modalities with a recent data level fusion strategy is also analyzed, and the results on the Distracted Driver and Brain4Cars datasets show an increase in accuracy.",1
"Optical Flow (OF) and depth are commonly used for visual odometry since they provide sufficient information about camera ego-motion in a rigid scene. We reformulate the problem of ego-motion estimation as a problem of motion estimation of a 3D-scene with respect to a static camera. The entire scene motion can be represented as a combination of motions of its visible points. Using OF and depth we estimate a motion of each point in terms of 6DoF and represent results in the form of motion maps, each one addressing single degree of freedom. In this work we provide motion maps as inputs to a deep neural network that predicts 6DoF of scene motion. Through our evaluation on outdoor and indoor datasets we show that utilizing motion maps leads to accuracy improvement in comparison with naive stacking of depth and OF. Another contribution of our work is a novel network architecture that efficiently exploits motion maps and outperforms learnable RGB/RGB-D baselines.",0
"Visual odometry commonly relies on Optical Flow (OF) and depth to obtain sufficient information on camera ego-motion within a rigid scene. We propose a reformulation of ego-motion estimation as a 3D-scene motion estimation problem with respect to a static camera. The motion of the entire scene can be represented as a combination of visible point motions. By utilizing OF and depth, we estimate the motion of each point in terms of 6DoF and present the results as motion maps for each degree of freedom. Our approach involves using these motion maps as inputs to a deep neural network that predicts 6DoF of scene motion. Our evaluation on indoor and outdoor datasets shows that our method utilizing motion maps yields better accuracy compared to the naive approach of stacking depth and OF. Additionally, our work introduces a novel network architecture that efficiently exploits motion maps and performs better than learnable RGB/RGB-D baselines.",1
"In this technical report we investigate speed estimation of the ego-vehicle on the KITTI benchmark using state-of-the-art deep neural network based optical flow and single-view depth prediction methods. Using a straightforward intuitive approach and approximating a single scale factor, we evaluate several application schemes of the deep networks and formulate meaningful conclusions such as: combining depth information with optical flow improves speed estimation accuracy as opposed to using optical flow alone; the quality of the deep neural network methods influences speed estimation performance; using the depth and optical flow results from smaller crops of wide images degrades performance. With these observations in mind, we achieve a RMSE of less than 1 m/s for vehicle speed estimation using monocular images as input from recordings of the KITTI benchmark. Limitations and possible future directions are discussed as well.",0
"The purpose of this technical report is to explore the estimation of the ego-vehicle's speed on the KITTI benchmark by utilizing advanced deep neural network based optical flow and single-view depth prediction methods. We conducted an investigation into various application schemes of the deep networks and concluded that the combination of depth information with optical flow leads to more accurate speed estimation compared to using optical flow alone. Additionally, the quality of the deep neural network methods has a significant impact on speed estimation performance. We also found that the usage of smaller crops of wide images for depth and optical flow results reduced performance. By implementing these findings, we achieved a RMSE of less than 1 m/s for vehicle speed estimation using monocular images from the KITTI benchmark. We also discussed the limitations of our study and potential areas for future research.",1
"Learning to estimate 3D geometry in a single frame and optical flow from consecutive frames by watching unlabeled videos via deep convolutional network has made significant progress recently. Current state-of-the-art (SoTA) methods treat the two tasks independently. One typical assumption of the existing depth estimation methods is that the scenes contain no independent moving objects. while object moving could be easily modeled using optical flow. In this paper, we propose to address the two tasks as a whole, i.e. to jointly understand per-pixel 3D geometry and motion. This eliminates the need of static scene assumption and enforces the inherent geometrical consistency during the learning process, yielding significantly improved results for both tasks. We call our method as ""Every Pixel Counts++"" or ""EPC++"". Specifically, during training, given two consecutive frames from a video, we adopt three parallel networks to predict the camera motion (MotionNet), dense depth map (DepthNet), and per-pixel optical flow between two frames (OptFlowNet) respectively. The three types of information are fed into a holistic 3D motion parser (HMP), and per-pixel 3D motion of both rigid background and moving objects are disentangled and recovered. Comprehensive experiments were conducted on datasets with different scenes, including driving scenario (KITTI 2012 and KITTI 2015 datasets), mixed outdoor/indoor scenes (Make3D) and synthetic animation (MPI Sintel dataset). Performance on the five tasks of depth estimation, optical flow estimation, odometry, moving object segmentation and scene flow estimation shows that our approach outperforms other SoTA methods. Code will be available at: https://github.com/chenxuluo/EPC.",0
"Recent advancements in deep convolutional networks have enabled significant progress in learning to estimate 3D geometry in a single frame and optical flow from consecutive frames by watching unlabeled videos. However, current state-of-the-art methods treat these tasks independently and assume that scenes contain no independent moving objects. We propose an approach that jointly understands per-pixel 3D geometry and motion to eliminate this assumption and enforce geometrical consistency during the learning process. Our method, called ""Every Pixel Counts++"" or ""EPC++"", utilizes three parallel networks to predict camera motion, dense depth map, and per-pixel optical flow between two frames, which are then fed into a holistic 3D motion parser to disentangle and recover per-pixel 3D motion of both rigid background and moving objects. We conducted comprehensive experiments on datasets with different scenes and found that our approach outperforms other state-of-the-art methods in five tasks: depth estimation, optical flow estimation, odometry, moving object segmentation, and scene flow estimation. Our code is available on GitHub at https://github.com/chenxuluo/EPC.",1
"We present Accel, a novel semantic video segmentation system that achieves high accuracy at low inference cost by combining the predictions of two network branches: (1) a reference branch that extracts high-detail features on a reference keyframe, and warps these features forward using frame-to-frame optical flow estimates, and (2) an update branch that computes features of adjustable quality on the current frame, performing a temporal update at each video frame. The modularity of the update branch, where feature subnetworks of varying layer depth can be inserted (e.g. ResNet-18 to ResNet-101), enables operation over a new, state-of-the-art accuracy-throughput trade-off spectrum. Over this curve, Accel models achieve both higher accuracy and faster inference times than the closest comparable single-frame segmentation networks. In general, Accel significantly outperforms previous work on efficient semantic video segmentation, correcting warping-related error that compounds on datasets with complex dynamics. Accel is end-to-end trainable and highly modular: the reference network, the optical flow network, and the update network can each be selected independently, depending on application requirements, and then jointly fine-tuned. The result is a robust, general system for fast, high-accuracy semantic segmentation on video.",0
"The Accel system is a new approach to semantic video segmentation that achieves high accuracy with low inference cost. The system combines the predictions of two network branches: (1) a reference branch that extracts detailed features on a reference keyframe and uses optical flow estimates to warp these features forward, and (2) an update branch that computes adjustable quality features on the current frame, updating them at each video frame. The update branch is modular, allowing for the insertion of different feature subnetworks, resulting in a spectrum of accuracy-throughput trade-offs. The Accel system outperforms previous methods on efficient semantic video segmentation, correcting warping-related errors on complex datasets. The system is end-to-end trainable and highly modular, with each network component being selected independently and then fine-tuned. Overall, Accel is a robust system for fast, high-accuracy semantic video segmentation.",1
"Appearance and motion are two key components to depict and characterize the video content. Currently, the two-stream models have achieved state-of-the-art performances on video classification. However, extracting motion information, specifically in the form of optical flow features, is extremely computationally expensive, especially for large-scale video classification. In this paper, we propose a motion hallucination network, namely MoNet, to imagine the optical flow features from the appearance features, with no reliance on the optical flow computation. Specifically, MoNet models the temporal relationships of the appearance features and exploits the contextual relationships of the optical flow features with concurrent connections. Extensive experimental results demonstrate that the proposed MoNet can effectively and efficiently hallucinate the optical flow features, which together with the appearance features consistently improve the video classification performances. Moreover, MoNet can help cutting down almost a half of computational and data-storage burdens for the two-stream video classification. Our code is available at: https://github.com/YongyiTang92/MoNet-Features.",0
"Depicting and characterizing video content requires attention to both appearance and motion. While two-stream models have achieved impressive results in video classification, the extraction of motion information in the form of optical flow features is computationally intensive, particularly for large-scale video classification. In this study, we propose a motion hallucination network, MoNet, that can generate optical flow features from appearance features without relying on optical flow computation. MoNet models the temporal and contextual relationships between appearance and optical flow features, yielding improved video classification performance. Additionally, MoNet can cut computational and data storage burdens in half for two-stream video classification. Our code is available at: https://github.com/YongyiTang92/MoNet-Features.",1
"The goal of this paper is to detect the spatio-temporal extent of an action. The two-stream detection network based on RGB and flow provides state-of-the-art accuracy at the expense of a large model-size and heavy computation. We propose to embed RGB and optical-flow into a single two-in-one stream network with new layers. A motion condition layer extracts motion information from flow images, which is leveraged by the motion modulation layer to generate transformation parameters for modulating the low-level RGB features. The method is easily embedded in existing appearance- or two-stream action detection networks, and trained end-to-end. Experiments demonstrate that leveraging the motion condition to modulate RGB features improves detection accuracy. With only half the computation and parameters of the state-of-the-art two-stream methods, our two-in-one stream still achieves impressive results on UCF101-24, UCFSports and J-HMDB.",0
"The aim of this study is to identify the time and space involved in an action. While the two-stream detection network employing RGB and flow is highly accurate, it requires a large model-size and heavy computation. To address this issue, we propose a two-in-one stream network that incorporates RGB and optical-flow using novel layers. The motion condition layer extracts motion data from flow images, which is utilized by the motion modulation layer to create transformation parameters for modifying the low-level RGB features. This approach can be easily integrated into existing appearance or two-stream action detection networks and can be trained end-to-end. Results show that using motion conditions to modify RGB features enhances detection accuracy. Furthermore, our two-in-one stream network achieves remarkable results on UCF101-24, UCFSports, and J-HMDB, while having only half the computation and parameters of the current two-stream methods.",1
"Convolutional neural networks (CNNs) can model complicated non-linear relations between images. However, they are notoriously sensitive to small changes in the input. Most CNNs trained to describe image-to-image mappings generate temporally unstable results when applied to video sequences, leading to flickering artifacts and other inconsistencies over time. In order to use CNNs for video material, previous methods have relied on estimating dense frame-to-frame motion information (optical flow) in the training and/or the inference phase, or by exploring recurrent learning structures. We take a different approach to the problem, posing temporal stability as a regularization of the cost function. The regularization is formulated to account for different types of motion that can occur between frames, so that temporally stable CNNs can be trained without the need for video material or expensive motion estimation. The training can be performed as a fine-tuning operation, without architectural modifications of the CNN. Our evaluation shows that the training strategy leads to large improvements in temporal smoothness. Moreover, for small datasets the regularization can help in boosting the generalization performance to a much larger extent than what is possible with na\""ive augmentation strategies.",0
"Complicated non-linear relations between images can be modeled by Convolutional neural networks (CNNs). However, they are highly sensitive to minor changes in the input, which makes them unsuitable for generating temporally stable results when applied to video sequences. This leads to inconsistencies over time and flickering artifacts. In previous methods, dense frame-to-frame motion information or recurrent learning structures were used to address this issue. We, on the other hand, propose a different approach by incorporating temporal stability as a regularization of the cost function. This regularization takes into account different types of motion between frames, allowing the training of temporally stable CNNs without the need for video material or costly motion estimation. This training technique can be implemented as a fine-tuning operation without any modifications to the CNN's architecture. Our evaluation indicates that this approach leads to significant improvements in temporal smoothness. Additionally, for smaller datasets, the regularization can enhance the generalization performance far beyond the capabilities of naive augmentation strategies.",1
"We present a new method to learn video representations from unlabeled data. Given large-scale unlabeled video data, the objective is to benefit from such data by learning a generic and transferable representation space that can be directly used for a new task such as zero/few-shot learning. We formulate our unsupervised representation learning as a multi-modal, multi-task learning problem, where the representations are also shared across different modalities via distillation. Further, we also introduce the concept of finding a better loss function to train such multi-task multi-modal representation space using an evolutionary algorithm; our method automatically searches over different combinations of loss functions capturing multiple (self-supervised) tasks and modalities. Our formulation allows for the distillation of audio, optical flow and temporal information into a single, RGB-based convolutional neural network. We also compare the effects of using additional unlabeled video data and evaluate our representation learning on standard public video datasets.",0
"A novel approach for acquiring video representations from unlabelled data is introduced in this study. The goal is to leverage large-scale unlabelled video data to acquire a universal and adaptable representation space that can be applied directly to fresh tasks, such as zero/few-shot learning. Our unsupervised representation learning process is modelled as a multi-modal, multi-task learning problem, where representations are shared between different modalities through distillation. Additionally, we propose the idea of discovering a superior loss function for training the multi-modal, multi-task representation space using an evolutionary algorithm. Our approach allows for the fusion of audio, optical flow, and temporal information into a single, RGB-based convolutional neural network. Finally, we examine the effects of employing additional unlabelled video data and evaluate the performance of our representation learning on standard public video datasets.",1
"We address the problem of temporal activity detection in continuous, untrimmed video streams. This is a difficult task that requires extracting meaningful spatio-temporal features to capture activities, accurately localizing the start and end times of each activity. We introduce a new model, Region Convolutional 3D Network (R-C3D), which encodes the video streams using a three-dimensional fully convolutional network, then generates candidate temporal regions containing activities and finally classifies selected regions into specific activities. Computation is saved due to the sharing of convolutional features between the proposal and the classification pipelines. We further improve the detection performance by efficiently integrating an optical flow based motion stream with the original RGB stream. The two-stream network is jointly optimized by fusing the flow and RGB feature maps at different levels. Additionally, the training stage incorporates an online hard example mining strategy to address the extreme foreground-background imbalance typically observed in any detection pipeline. Instead of heuristically sampling the candidate segments for the final activity classification stage, we rank them according to their performance and only select the worst performers to update the model. This improves the model without heavy hyper-parameter tuning. Extensive experiments on three benchmark datasets are carried out to show superior performance over existing temporal activity detection methods. Our model achieves state-of-the-art results on the THUMOS'14 and Charades datasets. We further demonstrate that our model is a general temporal activity detection framework that does not rely on assumptions about particular dataset properties by evaluating our approach on the ActivityNet dataset.",0
"Our focus is on detecting activities in continuous, untrimmed video streams, a challenging task that requires extracting meaningful spatio-temporal features to accurately localize the start and end times of each activity. To address this, we propose a new model called Region Convolutional 3D Network (R-C3D) that uses a three-dimensional fully convolutional network to encode the video streams. This generates candidate temporal regions containing activities, which are then classified into specific activities. Our approach saves computation by sharing convolutional features between the proposal and classification pipelines. To improve detection performance, we also integrate an optical flow based motion stream with the original RGB stream, and jointly optimize the two-stream network by fusing the flow and RGB feature maps at different levels. Additionally, we incorporate an online hard example mining strategy during training to address foreground-background imbalance. Instead of heuristically sampling the candidate segments, we rank them according to their performance and only select the worst performers to update the model, which improves the model without requiring heavy hyper-parameter tuning. We conduct extensive experiments on three benchmark datasets to demonstrate the superior performance of our approach over existing methods, achieving state-of-the-art results on the THUMOS'14 and Charades datasets. Importantly, our approach is a general temporal activity detection framework that does not rely on assumptions about particular dataset properties, as shown by our evaluation on the ActivityNet dataset.",1
"Anomaly detection in crowd videos has become a popular area of research for the computer vision community. Several existing methods generally perform a prior training about the scene with or without the use of labeled data. However, it is difficult to always guarantee the availability of prior data, especially, for scenarios like remote area surveillance. To address such challenge, we propose an adaptive training-less system capable of detecting anomaly on-the-fly while dynamically estimating and adjusting response based on certain parameters. This makes our system both training-less and adaptive in nature. Our pipeline consists of three main components, namely, adaptive 3D-DCT model for multi-object detection-based association, local motion structure description through saliency modulated optic flow, and anomaly detection based on earth movers distance (EMD). The proposed model, despite being training-free, is found to achieve comparable performance with several state-of-the-art methods on the publicly available UCSD, UMN, CHUK-Avenue and ShanghaiTech datasets.",0
"The detection of anomalies in crowd videos has become a popular research area for the computer vision community. Most existing methods require prior training of the scene, with or without labeled data. However, obtaining prior data can be challenging, especially in scenarios such as remote area surveillance. To overcome this issue, we propose an adaptive training-less system that can detect anomalies on-the-fly and adjust its response dynamically based on specific parameters. This approach makes our system both adaptive and training-less. Our system is comprised of three main components: an adaptive 3D-DCT model for multi-object detection-based association, local motion structure description using saliency modulated optic flow, and anomaly detection based on earth movers distance (EMD). Despite being training-free, our proposed model achieves comparable performance with several state-of-the-art methods on publicly available datasets such as UCSD, UMN, CHUK-Avenue, and ShanghaiTech.",1
"In this dissertation, I present my work towards exploring temporal information for better video understanding. Specifically, I have worked on two problems: action recognition and semantic segmentation. For action recognition, I have proposed a framework, termed hidden two-stream networks, to learn an optimal motion representation that does not require the computation of optical flow. My framework alleviates several challenges faced in video classification, such as learning motion representations, real-time inference, multi-framerate handling, generalizability to unseen actions, etc. For semantic segmentation, I have introduced a general framework that uses video prediction models to synthesize new training samples. By scaling up the training dataset, my trained models are more accurate and robust than previous models even without modifications to the network architectures or objective functions. I believe videos have much more potential to be mined, and temporal information is one of the most important cues for machines to perceive the visual world better.",0
"This dissertation details my investigation into temporal information as a means to enhance video understanding. My research focuses on action recognition and semantic segmentation, both of which present unique challenges. To address the issue of learning motion representations without requiring optical flow computation, I propose a framework called hidden two-stream networks for action recognition. This framework improves real-time inference, multi-framerate handling, and generalizability to unseen actions. For semantic segmentation, I introduce a general framework that leverages video prediction models to synthesize new training samples, thereby scaling up the training dataset and increasing model accuracy and robustness without modifying network architectures or objective functions. I believe that temporal information is a critical component in enabling machines to better perceive the visual world, and that videos offer a wealth of untapped potential for further exploration.",1
"Mathematical optimization is widely used in various research fields. With a carefully-designed objective function, mathematical optimization can be quite helpful in solving many problems. However, objective functions are usually hand-crafted and designing a good one can be quite challenging. In this paper, we propose a novel framework to learn the objective function based on a neural net-work. The basic idea is to consider the neural network as an objective function, and the input as an optimization variable. For the learning of objective function from the training data, two processes are conducted: In the inner process, the optimization variable (the input of the network) are optimized to minimize the objective function (the network output), while fixing the network weights. In the outer process, on the other hand, the weights are optimized based on how close the final solution of the inner process is to the desired solution. After learning the objective function, the solution for the test set is obtained in the same manner of the inner process. The potential and applicability of our approach are demonstrated by the experiments on toy examples and a computer vision task, optical flow.",0
"Mathematical optimization is commonly utilized across various research fields to solve problems by creating a well-designed objective function. Nonetheless, crafting a suitable objective function can be challenging as it typically involves manual intervention. This paper presents an innovative framework that employs a neural network to learn the objective function. The neural network is treated as the objective function while the input is considered as the optimization variable. The learning process is carried out in two steps - the inner process optimizes the input to minimize the objective function while fixing the network weights. The outer process optimizes the network weights based on how closely the final result of the inner process aligns with the desired solution. The approach is demonstrated through experiments on toy examples and a computer vision task of optical flow, showcasing its potential and applicability.",1
"Precipitation nowcasting is a short-range forecast of rain/snow (up to 2 hours), often displayed on top of the geographical map by the weather service. Modern precipitation nowcasting algorithms rely on the extrapolation of observations by ground-based radars via optical flow techniques or neural network models. Dependent on these radars, typical nowcasting is limited to the regions around their locations. We have developed a method for precipitation nowcasting based on geostationary satellite imagery and incorporated the resulting data into the Yandex.Weather precipitation map (including an alerting service with push notifications for products in the Yandex ecosystem), thus expanding its coverage and paving the way to a truly global nowcasting service.",0
"The weather service often displays a short-range forecast of rain or snow, known as precipitation nowcasting, for up to 2 hours on top of a geographical map. Modern algorithms rely on the extrapolation of ground-based radar observations using optical flow techniques or neural network models. However, the nowcasting is generally limited to regions around the radar's location. To overcome this limitation, we have developed a method that utilizes geostationary satellite imagery and integrated it into the Yandex.Weather precipitation map. This has expanded the coverage area and opened up the possibility of creating a truly global nowcasting service. Additionally, we have included an alerting service with push notifications for products in the Yandex ecosystem.",1
"Stereo matching and flow estimation are two essential tasks for scene understanding, spatially in 3D and temporally in motion. Existing approaches have been focused on the unsupervised setting due to the limited resource to obtain the large-scale ground truth data. To construct a self-learnable objective, co-related tasks are often linked together to form a joint framework. However, the prior work usually utilizes independent networks for each task, thus not allowing to learn shared feature representations across models. In this paper, we propose a single and principled network to jointly learn spatiotemporal correspondence for stereo matching and flow estimation, with a newly designed geometric connection as the unsupervised signal for temporally adjacent stereo pairs. We show that our method performs favorably against several state-of-the-art baselines for both unsupervised depth and flow estimation on the KITTI benchmark dataset.",0
"In order to comprehend scenes in both 3D space and motion, stereo matching and flow estimation are vital tasks. However, due to the lack of access to large-scale ground truth data, the current focus has been on unsupervised methods. To create a self-teachable goal, correlated tasks are commonly linked together to form a joint framework. However, previous approaches have employed independent networks for each task, resulting in an inability to learn shared feature representations across models. This study introduces a unified and systematic network that learns spatiotemporal correspondence for stereo matching and flow estimation, utilizing a newly developed geometric connection as the unsupervised signal for temporally adjacent stereo pairs. Our method produces results that surpass those of various state-of-the-art baselines for unsupervised depth and flow estimation on the KITTI benchmark dataset.",1
"Modern optical flow methods make use of salient scene feature points detected and matched within the scene as a basis for sparse-to-dense optical flow estimation. Current feature detectors however either give sparse, non uniform point clouds (resulting in flow inaccuracies) or lack the efficiency for frame-rate real-time applications. In this work we use the novel Dense Gradient Based Features (DeGraF) as the input to a sparse-to-dense optical flow scheme. This consists of three stages: 1) efficient detection of uniformly distributed Dense Gradient Based Features (DeGraF); 2) feature tracking via robust local optical flow; and 3) edge preserving flow interpolation to recover overall dense optical flow. The tunable density and uniformity of DeGraF features yield superior dense optical flow estimation compared to other popular feature detectors within this three stage pipeline. Furthermore, the comparable speed of feature detection also lends itself well to the aim of real-time optical flow recovery. Evaluation on established real-world benchmark datasets show test performance in an autonomous vehicle setting where DeGraF-Flow shows promising results in terms of accuracy with competitive computational efficiency among non-GPU based methods, including a marked increase in speed over the conceptually similar EpicFlow approach.",0
"Modern methods for optical flow rely on identifying and matching salient feature points in a scene to estimate sparse-to-dense optical flow. However, existing feature detectors can produce sparse, non-uniform point clouds that lead to inaccuracies in the flow or lack the efficiency for real-time applications. To address this, we propose using Dense Gradient Based Features (DeGraF) as input to a sparse-to-dense optical flow scheme. Our three-stage approach involves detecting uniformly distributed DeGraF features, tracking them with local optical flow, and using edge-preserving flow interpolation to recover overall dense optical flow. Compared to other feature detectors, DeGraF features yield superior dense optical flow estimation and faster detection, making it well-suited for real-time optical flow recovery. Our evaluation on real-world benchmark datasets shows promising results, particularly in an autonomous vehicle setting where DeGraF-Flow offers competitive accuracy and computational efficiency, outperforming the EpicFlow approach.",1
"Video representation is a key challenge in many computer vision applications such as video classification, video captioning, and video surveillance. In this paper, we propose a novel approach for video representation that captures meaningful information including motion and appearance from a sequence of video frames and compacts it into a single image. To this end, we compute the optical flow and use it in a least squares optimization to find a new image, the so-called Flow Profile Image (FPI). This image encodes motions as well as foreground appearance information while background information is removed. The quality of this image is validated in activity recognition experiments and the results are compared with other video representation techniques such as dynamic images [1] and eigen images [2]. The experimental results as well as visual quality confirm that FPIs can be successfully used in video processing applications.",0
"Several computer vision applications, including video classification, video captioning, and video surveillance, face the challenge of video representation. Our paper introduces a new method for video representation that captures significant information, like motion and appearance, from a sequence of video frames and compresses it into a single image. We accomplish this by using optical flow and a least squares optimization to generate the Flow Profile Image (FPI), which retains foreground appearance and motion information while eliminating background information. We tested the FPI's quality in activity recognition experiments and compared it to other video representation techniques, such as dynamic images and eigen images. The results of our experiments confirm that FPIs are a successful tool for video processing applications, both visually and experimentally.",1
"Video inpainting, which aims at filling in missing regions of a video, remains challenging due to the difficulty of preserving the precise spatial and temporal coherence of video contents. In this work we propose a novel flow-guided video inpainting approach. Rather than filling in the RGB pixels of each frame directly, we consider video inpainting as a pixel propagation problem. We first synthesize a spatially and temporally coherent optical flow field across video frames using a newly designed Deep Flow Completion network. Then the synthesized flow field is used to guide the propagation of pixels to fill up the missing regions in the video. Specifically, the Deep Flow Completion network follows a coarse-to-fine refinement to complete the flow fields, while their quality is further improved by hard flow example mining. Following the guide of the completed flow, the missing video regions can be filled up precisely. Our method is evaluated on DAVIS and YouTube-VOS datasets qualitatively and quantitatively, achieving the state-of-the-art performance in terms of inpainting quality and speed.",0
"The challenge of maintaining precise spatial and temporal coherence in video inpainting, which fills in missing areas of a video, has yet to be overcome. In this study, we present a fresh approach to video inpainting that employs flow guidance. Rather than filling in the RGB pixels of each frame directly, we view video inpainting as a pixel propagation issue. We begin by creating a coherent optical flow field across video frames using a newly developed Deep Flow Completion network. The synthesized flow field is then used to direct the propagation of pixels to fill in the missing regions of the video. The Deep Flow Completion network follows a coarse-to-fine refinement to complete the flow fields, while hard flow example mining improves their quality. The missing video regions can be precisely filled in by following the completed flow. Our method is evaluated on DAVIS and YouTube-VOS datasets both qualitatively and quantitatively, and it achieves state-of-the-art performance in terms of inpainting quality and speed.",1
"In this paper, we present a new inpainting framework for recovering missing regions of video frames. Compared with image inpainting, performing this task on video presents new challenges such as how to preserving temporal consistency and spatial details, as well as how to handle arbitrary input video size and length fast and efficiently. Towards this end, we propose a novel deep learning architecture which incorporates ConvLSTM and optical flow for modeling the spatial-temporal consistency in videos. It also saves much computational resource such that our method can handle videos with larger frame size and arbitrary length streamingly in real-time. Furthermore, to generate an accurate optical flow from corrupted frames, we propose a robust flow generation module, where two sources of flows are fed and a flow blending network is trained to fuse them. We conduct extensive experiments to evaluate our method in various scenarios and different datasets, both qualitatively and quantitatively. The experimental results demonstrate the superior of our method compared with the state-of-the-art inpainting approaches.",0
"This paper introduces a novel framework for video frame inpainting that addresses the challenges of maintaining spatial and temporal consistency, as well as handling varying input video sizes and lengths efficiently. Our approach incorporates ConvLSTM and optical flow to model the spatial-temporal consistency in videos while minimizing computational resources. Additionally, we propose a robust flow generation module that blends two sources of flows to generate accurate optical flow from corrupted frames. Our method is evaluated through extensive experiments using different datasets and scenarios, demonstrating its superiority compared to existing inpainting approaches both qualitatively and quantitatively.",1
"Motion has shown to be useful for video understanding, where motion is typically represented by optical flow. However, computing flow from video frames is very time-consuming. Recent works directly leverage the motion vectors and residuals readily available in the compressed video to represent motion at no cost. While this avoids flow computation, it also hurts accuracy since the motion vector is noisy and has substantially reduced resolution, which makes it a less discriminative motion representation. To remedy these issues, we propose a lightweight generator network, which reduces noises in motion vectors and captures fine motion details, achieving a more Discriminative Motion Cue (DMC) representation. Since optical flow is a more accurate motion representation, we train the DMC generator to approximate flow using a reconstruction loss and a generative adversarial loss, jointly with the downstream action classification task. Extensive evaluations on three action recognition benchmarks (HMDB-51, UCF-101, and a subset of Kinetics) confirm the effectiveness of our method. Our full system, consisting of the generator and the classifier, is coined as DMC-Net which obtains high accuracy close to that of using flow and runs two orders of magnitude faster than using optical flow at inference time.",0
"The use of motion is beneficial for video comprehension, with optical flow being the typical representation. However, calculating flow from video frames is time-intensive. Recent studies utilize readily available motion vectors and residuals in compressed video to represent motion at no cost. However, this approach negatively impacts accuracy due to the noisy and lower resolution motion vector, which is a less distinct motion representation. To address these issues, we propose a lightweight generator network that decreases noise in motion vectors and captures fine motion details, achieving a more Discriminative Motion Cue (DMC) representation. We train the DMC generator to approximate flow using a reconstruction loss and a generative adversarial loss, jointly with the downstream action classification task. Our method is evaluated on three action recognition benchmarks (HMDB-51, UCF-101, and a subset of Kinetics) and confirms its effectiveness. Our full system, DMC-Net, comprising the generator and classifier, achieves high accuracy close to that of using flow and runs two orders of magnitude faster than using optical flow at inference time.",1
"In this paper, we adapt the geodesic distance-based recursive filter to the sparse data interpolation problem. The proposed technique is general and can be easily applied to any kind of sparse data. We demonstrate the superiority over other interpolation techniques in three experiments for qualitative and quantitative evaluation.   In addition, we compare our method with the popular interpolation algorithm presented in the EpicFlow optical flow paper that is intuitively motivated by a similar geodesic distance principle. The comparison shows that our algorithm is more accurate and considerably faster than the EpicFlow interpolation technique.",0
"The geodesic distance-based recursive filter is modified in this paper to address the issue of sparse data interpolation. Our proposed approach is versatile and can be implemented for any type of sparse data. Through three experiments, we prove its superiority over other interpolation techniques in both qualitative and quantitative evaluations. Furthermore, we conduct a comparison between our method and the EpicFlow optical flow paper's widely used interpolation algorithm inspired by a similar geodesic distance principle. Our analysis indicates that our algorithm is more precise and significantly faster than the EpicFlow interpolation technique.",1
"Multiview stereo aims to reconstruct scene depth from images acquired by a camera under arbitrary motion. Recent methods address this problem through deep learning, which can utilize semantic cues to deal with challenges such as textureless and reflective regions. In this paper, we present a convolutional neural network called DPSNet (Deep Plane Sweep Network) whose design is inspired by best practices of traditional geometry-based approaches for dense depth reconstruction. Rather than directly estimating depth and/or optical flow correspondence from image pairs as done in many previous deep learning methods, DPSNet takes a plane sweep approach that involves building a cost volume from deep features using the plane sweep algorithm, regularizing the cost volume via a context-aware cost aggregation, and regressing the dense depth map from the cost volume. The cost volume is constructed using a differentiable warping process that allows for end-to-end training of the network. Through the effective incorporation of conventional multiview stereo concepts within a deep learning framework, DPSNet achieves state-of-the-art reconstruction results on a variety of challenging datasets.",0
"The objective of multiview stereo is to determine the depth of a scene based on images captured by a camera in motion. Recent techniques employ deep learning to handle problems such as textureless and reflective areas by utilizing semantic indicators. This study introduces DPSNet (Deep Plane Sweep Network), a convolutional neural network that follows the successful approaches of conventional geometry-based methods for dense depth reconstruction. Instead of directly estimating depth or optical flow correspondence from image pairs like previous deep learning methods, DPSNet uses a plane sweep approach. This method entails constructing a cost volume from deep features using the plane sweep algorithm, regulating the cost volume using context-aware cost aggregation, and predicting the dense depth map from the cost volume. The cost volume is generated using a differentiable warping procedure that enables end-to-end network training. By skillfully incorporating standard multiview stereo principles into a deep learning system, DPSNet achieves superior reconstruction outcomes on several challenging datasets.",1
"Processing and fusing information among multi-modal is a very useful technique for achieving high performance in many computer vision problems. In order to tackle multi-modal information more effectively, we introduce a novel framework for multi-modal fusion: Cross-modal Message Passing (CMMP). Specifically, we propose a cross-modal message passing mechanism to fuse two-stream network for action recognition, which composes of an appearance modal network (RGB image) and a motion modal (optical flow image) network. The objectives of individual networks in this framework are two-fold: a standard classification objective and a competing objective. The classification object ensures that each modal network predicts the true action category while the competing objective encourages each modal network to outperform the other one. We quantitatively show that the proposed CMMP fuses the traditional two-stream network more effectively, and outperforms all existing two-stream fusion method on UCF-101 and HMDB-51 datasets.",0
"To achieve high performance in computer vision problems, it is useful to process and merge information among multiple modes. To effectively handle multi-modal information, a new framework called Cross-modal Message Passing (CMMP) is introduced. This framework proposes a mechanism for fusing two-stream networks for action recognition, which includes an appearance modal network (RGB image) and a motion modal (optical flow image) network. The individual networks in this framework have two objectives: a standard classification objective and a competing objective. The classification objective ensures that each modal network predicts the accurate action category, while the competing objective encourages each modal network to outperform the other. Our quantitative analysis shows that CMMP fuses the traditional two-stream network more effectively, and outperforms all existing two-stream fusion methods on UCF-101 and HMDB-51 datasets.",1
In this paper we present a self-supervised method for representation learning utilizing two different modalities. Based on the observation that cross-modal information has a high semantic meaning we propose a method to effectively exploit this signal. For our approach we utilize video data since it is available on a large scale and provides easily accessible modalities given by RGB and optical flow. We demonstrate state-of-the-art performance on highly contested action recognition datasets in the context of self-supervised learning. We show that our feature representation also transfers to other tasks and conduct extensive ablation studies to validate our core contributions. Code and model can be found at https://github.com/nawidsayed/Cross-and-Learn.,0
"This paper introduces a technique for learning representations through self-supervision that leverages two modalities. The approach capitalizes on the fact that cross-modal data holds significant semantic value and proposes a way to effectively utilize this signal. To implement the method, we use video data, which offers readily accessible RGB and optical flow modalities and is widely available. The approach achieves top-notch performance on competitive action recognition datasets in the realm of self-supervised learning. Our results also indicate that our feature representation can transfer to other tasks. We conducted rigorous experiments to validate our main contributions. The code and model can be found at https://github.com/nawidsayed/Cross-and-Learn.",1
"We propose a light-weight video frame interpolation algorithm. Our key innovation is an instance-level supervision that allows information to be learned from the high-resolution version of similar objects. Our experiment shows that the proposed method can generate state-of-the-art results across different datasets, with fractional computation resources (time and memory) of competing methods. Given two image frames, a cascade network creates an intermediate frame with 1) a flow-warping module that computes coarse bi-directional optical flow and creates an interpolated image via flow-based warping, followed by 2) an image synthesis module to make fine-scale corrections. In the learning stage, object detection proposals are generated on the interpolated image.Lower resolution objects are zoomed into, and the learning algorithms using an adversarial loss trained on high-resolution objects to guide the system towards the instance-level refinement corrects details of object shape and boundaries.",0
"Our proposed video frame interpolation algorithm is lightweight and innovative. We use instance-level supervision to learn information from high-resolution versions of similar objects. Our method outperforms competing algorithms on various datasets while using a fraction of the computation resources. The algorithm works by using a cascade network to create an intermediate frame. This involves a flow-warping module to compute coarse bi-directional optical flow and create an interpolated image, followed by an image synthesis module for fine-scale corrections. During the learning stage, object detection proposals are generated on the interpolated image. Lower resolution objects are zoomed in, and the learning algorithms use an adversarial loss to train on high-resolution objects. This guides the system towards instance-level refinement, correcting details of object shape and boundaries.",1
"This paper presents a novel approach for segmenting moving objects in unconstrained environments using guided convolutional neural networks. This guiding process relies on foreground masks from independent algorithms (i.e. state-of-the-art algorithms) to implement an attention mechanism that incorporates the spatial location of foreground and background to compute their separated representations. Our approach initially extracts two kinds of features for each frame using colour and optical flow information. Such features are combined following a multiplicative scheme to benefit from their complementarity. These unified colour and motion features are later processed to obtain the separated foreground and background representations. Then, both independent representations are concatenated and decoded to perform foreground segmentation. Experiments conducted on the challenging DAVIS 2016 dataset demonstrate that our guided representations not only outperform non-guided, but also recent and top-performing video object segmentation algorithms.",0
"In this paper, an innovative method is proposed for segmenting moving objects in unconstrained environments through the use of guided convolutional neural networks. This method involves incorporating an attention mechanism that utilizes foreground masks from independent algorithms. The attention mechanism takes into account the spatial location of the foreground and background to generate separate representations. The proposed approach initially extracts two types of features from each frame, namely colour and optical flow information. These features are then combined using a multiplicative scheme to optimize their complementarity. The resulting unified features are further processed to obtain separate foreground and background representations. These independent representations are concatenated and decoded to perform foreground segmentation. Results from experiments conducted on the challenging DAVIS 2016 dataset reveal that the guided representations not only surpass non-guided methods, but also outperform recent and top-performing video object segmentation algorithms.",1
"The problem of Scene flow estimation in depth videos has been attracting attention of researchers of robot vision, due to its potential application in various areas of robotics. The conventional scene flow methods are difficult to use in reallife applications due to their long computational overhead. We propose a conditional adversarial network SceneFlowGAN for scene flow estimation. The proposed SceneFlowGAN uses loss function at two ends: both generator and descriptor ends. The proposed network is the first attempt to estimate scene flow using generative adversarial networks, and is able to estimate both the optical flow and disparity from the input stereo images simultaneously. The proposed method is experimented on a large RGB-D benchmark sceneflow dataset.",0
"The estimation of Scene flow in depth videos has captured the interest of researchers in the field of robot vision, as it has the potential to be applied in various areas of robotics. However, conventional scene flow methods have limited practicality due to their extensive computational requirements. To address this issue, we introduce SceneFlowGAN, a conditional adversarial network that utilizes loss functions at both the generator and descriptor ends. SceneFlowGAN is the first method to incorporate generative adversarial networks for scene flow estimation and can estimate optical flow and disparity simultaneously from input stereo images. We evaluate the effectiveness of our proposed method on a large RGB-D benchmark sceneflow dataset.",1
"We present an implementation of a new approach to diffeomorphic non-rigid registration of medical images. The method is based on optical flow and warps images via gradient flow with the standard $L^2$ inner product. To compute the transformation, we rely on accelerated optimisation on the manifold of diffeomorphisms. We achieve regularity properties of Sobolev gradient flows, which are expensive to compute, owing to a novel method of averaging the gradients in time rather than space. We successfully register brain MRI and challenging abdominal CT scans at speeds orders of magnitude faster than previous approaches. We make our code available in a public repository: https://github.com/dgrzech/fastreg",0
"Our research introduces a novel technique for non-rigid registration of medical images using diffeomorphism. The approach utilizes optical flow and applies image warping through gradient flow using the standard $L^2$ inner product. In order to compute the transformation, we employ accelerated optimization on the manifold of diffeomorphisms. Our method achieves Sobolev gradient flow regularity properties, which can be computationally expensive, by utilizing a new time-based approach for averaging gradients instead of spatial averaging. We were able to register brain MRI and challenging abdominal CT scans at significantly faster speeds than previous methods. Our code is publicly available at https://github.com/dgrzech/fastreg.",1
"Ghosting artifacts caused by moving objects or misalignments is a key challenge in high dynamic range (HDR) imaging for dynamic scenes. Previous methods first register the input low dynamic range (LDR) images using optical flow before merging them, which are error-prone and cause ghosts in results. A very recent work tries to bypass optical flows via a deep network with skip-connections, however, which still suffers from ghosting artifacts for severe movement. To avoid the ghosting from the source, we propose a novel attention-guided end-to-end deep neural network (AHDRNet) to produce high-quality ghost-free HDR images. Unlike previous methods directly stacking the LDR images or features for merging, we use attention modules to guide the merging according to the reference image. The attention modules automatically suppress undesired components caused by misalignments and saturation and enhance desirable fine details in the non-reference images. In addition to the attention model, we use dilated residual dense block (DRDB) to make full use of the hierarchical features and increase the receptive field for hallucinating the missing details. The proposed AHDRNet is a non-flow-based method, which can also avoid the artifacts generated by optical-flow estimation error. Experiments on different datasets show that the proposed AHDRNet can achieve state-of-the-art quantitative and qualitative results.",0
"The main challenge in dynamic scenes for high dynamic range (HDR) imaging is the appearance of ghosting artifacts caused by moving objects or misalignments. Previous techniques have attempted to register low dynamic range (LDR) images using optical flow before merging them, but this approach is error-prone and can result in ghosting in the final image. While a recent deep network approach with skip-connections attempted to bypass optical flows, it still suffers from ghosting artifacts in cases of severe movement. To address this issue, we introduce a novel attention-guided end-to-end deep neural network (AHDRNet) that produces high-quality HDR images free of ghosting artifacts. Unlike previous methods that directly stack LDR images or features for merging, our approach uses attention modules to guide the merging process based on the reference image. These modules automatically suppress undesired components caused by misalignments and saturation while enhancing desirable fine details in the non-reference images. Additionally, we use a dilated residual dense block (DRDB) to make full use of hierarchical features and increase the receptive field for hallucinating missing details. The proposed AHDRNet is a non-flow-based approach that avoids optical-flow estimation errors and can achieve state-of-the-art quantitative and qualitative results, as demonstrated by experiments on various datasets.",1
"We present a self-supervised learning approach for optical flow. Our method distills reliable flow estimations from non-occluded pixels, and uses these predictions as ground truth to learn optical flow for hallucinated occlusions. We further design a simple CNN to utilize temporal information from multiple frames for better flow estimation. These two principles lead to an approach that yields the best performance for unsupervised optical flow learning on the challenging benchmarks including MPI Sintel, KITTI 2012 and 2015. More notably, our self-supervised pre-trained model provides an excellent initialization for supervised fine-tuning. Our fine-tuned models achieve state-of-the-art results on all three datasets. At the time of writing, we achieve EPE=4.26 on the Sintel benchmark, outperforming all submitted methods.",0
"A self-supervised learning technique for optical flow is introduced in this study. The method involves extracting dependable flow estimations from non-occluded pixels, which serve as the ground truth for learning optical flow in the presence of hallucinated occlusions. Furthermore, a basic CNN is developed to incorporate temporal information from multiple frames to enhance flow estimation. These two principles result in an approach that exhibits superior performance in unsupervised optical flow learning, as demonstrated on the challenging benchmarks of MPI Sintel, KITTI 2012, and 2015. Importantly, the pre-trained self-supervised model serves as an excellent starting point for supervised fine-tuning, which leads to state-of-the-art results on all three datasets. At the time of writing, the method achieves EPE=4.26 on the Sintel benchmark, surpassing all other submitted methods.",1
"We present a self-supervised approach to estimate flow in camera image and top-view grid map sequences using fully convolutional neural networks in the domain of automated driving. We extend existing approaches for self-supervised optical flow estimation by adding a regularizer expressing motion consistency assuming a static environment. However, as this assumption is violated for other moving traffic participants we also estimate a mask to scale this regularization. Adding a regularization towards motion consistency improves convergence and flow estimation accuracy. Furthermore, we scale the errors due to spatial flow inconsistency by a mask that we derive from the motion mask. This improves accuracy in regions where the flow drastically changes due to a better separation between static and dynamic environment. We apply our approach to optical flow estimation from camera image sequences, validate on odometry estimation and suggest a method to iteratively increase optical flow estimation accuracy using the generated motion masks. Finally, we provide quantitative and qualitative results based on the KITTI odometry and tracking benchmark for scene flow estimation based on grid map sequences. We show that we can improve accuracy and convergence when applying motion and spatial consistency regularization.",0
"Our study introduces a self-supervised method for estimating flow in camera image and top-view grid map sequences using fully convolutional neural networks in the field of automated driving. Our approach enhances existing self-supervised optical flow estimation methods by including a regularizer that expresses motion consistency under the assumption of a static environment. However, since this assumption is invalid for other moving traffic participants, we also predict a mask to adjust this regularization. By introducing a regularization towards motion consistency, we improve convergence and flow estimation accuracy. Additionally, we improve accuracy in regions with significant flow changes by scaling errors due to spatial flow inconsistency through a mask derived from the motion mask. We apply our technique to optical flow estimation from camera image sequences, validate it on odometry estimation, and present a method to gradually improve optical flow estimation accuracy using the generated motion masks. Finally, we offer quantitative and qualitative results based on the KITTI odometry and tracking benchmark for scene flow estimation based on grid map sequences, demonstrating that motion and spatial consistency regularization improves accuracy and convergence.",1
"Explicit representations of the global match distributions of pixel-wise correspondences between pairs of images are desirable for uncertainty estimation and downstream applications. However, the computation of the match density for each pixel may be prohibitively expensive due to the large number of candidates. In this paper, we propose Hierarchical Discrete Distribution Decomposition (HD^3), a framework suitable for learning probabilistic pixel correspondences in both optical flow and stereo matching. We decompose the full match density into multiple scales hierarchically, and estimate the local matching distributions at each scale conditioned on the matching and warping at coarser scales. The local distributions can then be composed together to form the global match density. Despite its simplicity, our probabilistic method achieves state-of-the-art results for both optical flow and stereo matching on established benchmarks. We also find the estimated uncertainty is a good indication of the reliability of the predicted correspondences.",0
"For uncertainty estimation and downstream applications, it is desirable to have explicit representations of the global match distributions of pixel-wise correspondences between pairs of images. However, calculating the match density for each pixel can be too expensive due to the large number of candidates. In this article, we introduce the Hierarchical Discrete Distribution Decomposition (HD^3), which is a framework suitable for learning probabilistic pixel correspondences in optical flow and stereo matching. We decompose the full match density into multiple scales hierarchically and estimate the local matching distributions at each scale based on the matching and warping at coarser scales. The local distributions can then be composed to form the global match density. Despite its simplicity, our probabilistic approach achieves state-of-the-art results for both optical flow and stereo matching on established benchmarks. We also observe that the estimated uncertainty is a reliable indication of the predicted correspondences' reliability.",1
"Event cameras are novel vision sensors that output pixel-level brightness changes (""events"") instead of traditional video frames. These asynchronous sensors offer several advantages over traditional cameras, such as, high temporal resolution, very high dynamic range, and no motion blur. To unlock the potential of such sensors, motion compensation methods have been recently proposed. We present a collection and taxonomy of twenty two objective functions to analyze event alignment in motion compensation approaches (Fig. 1). We call them Focus Loss Functions since they have strong connections with functions used in traditional shape-from-focus applications. The proposed loss functions allow bringing mature computer vision tools to the realm of event cameras. We compare the accuracy and runtime performance of all loss functions on a publicly available dataset, and conclude that the variance, the gradient and the Laplacian magnitudes are among the best loss functions. The applicability of the loss functions is shown on multiple tasks: rotational motion, depth and optical flow estimation. The proposed focus loss functions allow to unlock the outstanding properties of event cameras.",0
"Instead of traditional video frames, event cameras provide pixel-level brightness changes and are considered novel vision sensors. These sensors have several advantages over traditional cameras, including high temporal resolution, no motion blur, and a very high dynamic range. To fully utilize the potential of event cameras, motion compensation methods have been recently introduced. In this study, we present a taxonomy of twenty-two objective functions that analyze event alignment in motion compensation approaches. These functions are called Focus Loss Functions and are related to the functions used in traditional shape-from-focus applications. By using these loss functions, we can apply mature computer vision tools to event cameras. We evaluated the accuracy and runtime performance of all loss functions on a publicly available dataset, and found that the variance, gradient, and Laplacian magnitudes are among the best. The proposed focus loss functions can be applied to various tasks, including rotational motion, depth, and optical flow estimation. By using these functions, we can fully realize the exceptional properties of event cameras.",1
"Crowd gatherings at social and cultural events are increasing in leaps and bounds with the increase in population. Surveillance through computer vision and expert decision making systems can help to understand the crowd phenomena at large gatherings. Understanding crowd phenomena can be helpful in early identification of unwanted incidents and their prevention. Motion flow is one of the important crowd phenomena that can be instrumental in describing the crowd behavior. Flows can be useful in understanding instabilities in the crowd. However, extracting motion flows is a challenging task due to randomness in crowd movement and limitations of the sensing device. Moreover, low-level features such as optical flow can be misleading if the randomness is high. In this paper, we propose a new model based on Langevin equation to analyze the linear dominant flows in videos of densely crowded scenarios. We assume a force model with three components, namely external force, confinement/drift force, and disturbance force. These forces are found to be sufficient to describe the linear or near-linear motion in dense crowd videos. The method is significantly faster as compared to existing popular crowd segmentation methods. The evaluation of the proposed model has been carried out on publicly available datasets as well as using our dataset. It has been observed that the proposed method is able to estimate and segment the linear flows in the dense crowd with better accuracy as compared to state-of-the-art techniques with substantial decrease in the computational overhead.",0
"The rise in population has led to a surge in crowds at social and cultural events. To comprehend this phenomenon, computer vision and expert decision-making systems can be employed for surveillance. By understanding the behavior of crowds, it is possible to prevent unwanted incidents. The motion flow of crowds is an essential aspect of their conduct and can help identify instabilities. However, this is a challenging task due to the randomness in movement and limitations of sensing devices. Optical flow, which is a low-level feature, can be misleading when there is high randomness. To tackle this issue, we propose a new model based on Langevin equation to analyze linear dominant flows in videos of densely crowded scenarios. The model uses three forces, namely external force, confinement/drift force, and disturbance force, to describe linear or near-linear motion in dense crowd videos. Our approach is much faster than existing crowd segmentation methods and shows better accuracy in estimating and segmenting linear flows in dense crowds with less computational overhead. We evaluated our model using publicly available datasets and our own dataset.",1
"Video object removal is a challenging task in video processing that often requires massive human efforts. Given the mask of the foreground object in each frame, the goal is to complete (inpaint) the object region and generate a video without the target object. While recently deep learning based methods have achieved great success on the image inpainting task, they often lead to inconsistent results between frames when applied to videos. In this work, we propose a novel learning-based Video Object Removal Network (VORNet) to solve the video object removal task in a spatio-temporally consistent manner, by combining the optical flow warping and image-based inpainting model. Experiments are done on our Synthesized Video Object Removal (SVOR) dataset based on the YouTube-VOS video segmentation dataset, and both the objective and subjective evaluation demonstrate that our VORNet generates more spatially and temporally consistent videos compared with existing methods.",0
"Removing objects from videos is a difficult task that usually requires a lot of human effort. The objective is to remove the foreground object from each frame and create a video without it by completing the object region. Although deep learning-based methods have been successful in image inpainting, they tend to produce inconsistent results when applied to videos. To address this issue, we propose a new Video Object Removal Network (VORNet) that uses optical flow warping and image-based inpainting to achieve spatio-temporal consistency. We evaluated VORNet on our Synthesized Video Object Removal (SVOR) dataset, which is based on the YouTube-VOS video segmentation dataset. Both objective and subjective evaluations demonstrate that our method generates videos that are more spatially and temporally consistent than existing approaches.",1
"We propose an adversarial contextual model for detecting moving objects in images. A deep neural network is trained to predict the optical flow in a region using information from everywhere else but that region (context), while another network attempts to make such context as uninformative as possible. The result is a model where hypotheses naturally compete with no need for explicit regularization or hyper-parameter tuning. Although our method requires no supervision whatsoever, it outperforms several methods that are pre-trained on large annotated datasets. Our model can be thought of as a generalization of classical variational generative region-based segmentation, but in a way that avoids explicit regularization or solution of partial differential equations at run-time.",0
"Our proposed approach for detecting moving objects in images is an adversarial contextual model. This involves training a deep neural network to predict optical flow in a region, using contextual information from all other areas except the region of interest. Simultaneously, another network attempts to render this context as uninformative as possible. The outcome of this process is a model that allows hypotheses to compete naturally, without the need for explicit regularization or hyper-parameter tuning. While our method requires no supervision, it outperforms pre-trained approaches that have been trained on large annotated datasets. Our model can be considered an extension of classical variational generative region-based segmentation, but without the need for explicit regularization or solving partial differential equations in real-time.",1
"In the last few years, convolutional neural networks (CNNs) have demonstrated increasing success at learning many computer vision tasks including dense estimation problems such as optical flow and stereo matching. However, the joint prediction of these tasks, called scene flow, has traditionally been tackled using slow classical methods based on primitive assumptions which fail to generalize. The work presented in this paper overcomes these drawbacks efficiently (in terms of speed and accuracy) by proposing PWOC-3D, a compact CNN architecture to predict scene flow from stereo image sequences in an end-to-end supervised setting. Further, large motion and occlusions are well-known problems in scene flow estimation. PWOC-3D employs specialized design decisions to explicitly model these challenges. In this regard, we propose a novel self-supervised strategy to predict occlusions from images (learned without any labeled occlusion data). Leveraging several such constructs, our network achieves competitive results on the KITTI benchmark and the challenging FlyingThings3D dataset. Especially on KITTI, PWOC-3D achieves the second place among end-to-end deep learning methods with 48 times fewer parameters than the top-performing method.",0
"Over the past few years, convolutional neural networks (CNNs) have been increasingly successful in learning various computer vision tasks, including dense estimation problems like optical flow and stereo matching. However, traditional methods for joint prediction of these tasks, known as scene flow, have been slow and based on primitive assumptions that fail to generalize. In this paper, the authors propose PWOC-3D, a compact CNN architecture that efficiently predicts scene flow from stereo image sequences in an end-to-end supervised setting. The network addresses the challenges of large motion and occlusions by employing specialized design decisions and a novel self-supervised strategy to predict occlusions from images without labeled occlusion data. The authors demonstrate the effectiveness of their approach on the KITTI benchmark and the FlyingThings3D dataset, achieving competitive results with significantly fewer parameters than the top-performing method in the KITTI benchmark.",1
"This paper digs deeper into factors that influence egocentric gaze. Instead of training deep models for this purpose in a blind manner, we propose to inspect factors that contribute to gaze guidance during daily tasks. Bottom-up saliency and optical flow are assessed versus strong spatial prior baselines. Task-specific cues such as vanishing point, manipulation point, and hand regions are analyzed as representatives of top-down information. We also look into the contribution of these factors by investigating a simple recurrent neural model for ego-centric gaze prediction. First, deep features are extracted for all input video frames. Then, a gated recurrent unit is employed to integrate information over time and to predict the next fixation. We also propose an integrated model that combines the recurrent model with several top-down and bottom-up cues. Extensive experiments over multiple datasets reveal that (1) spatial biases are strong in egocentric videos, (2) bottom-up saliency models perform poorly in predicting gaze and underperform spatial biases, (3) deep features perform better compared to traditional features, (4) as opposed to hand regions, the manipulation point is a strong influential cue for gaze prediction, (5) combining the proposed recurrent model with bottom-up cues, vanishing points and, in particular, manipulation point results in the best gaze prediction accuracy over egocentric videos, (6) the knowledge transfer works best for cases where the tasks or sequences are similar, and (7) task and activity recognition can benefit from gaze prediction. Our findings suggest that (1) there should be more emphasis on hand-object interaction and (2) the egocentric vision community should consider larger datasets including diverse stimuli and more subjects.",0
"Instead of blindly training deep models to understand the factors that influence egocentric gaze, this paper proposes a thorough investigation into the elements that contribute to gaze guidance during daily tasks. The research compares bottom-up saliency and optical flow to strong spatial prior baselines and analyzes task-specific cues such as vanishing point, manipulation point, and hand regions as representatives of top-down information. A simple recurrent neural model is used to predict ego-centric gaze, with extensive experiments revealing that deep features perform better than traditional features, and the manipulation point is a strong cue for gaze prediction. The study also suggests that combining the recurrent model with bottom-up cues, vanishing points, and the manipulation point results in the best gaze prediction accuracy over egocentric videos. Additionally, the research suggests that there should be more emphasis on hand-object interaction, and the egocentric vision community should consider larger datasets that include diverse stimuli and more subjects. Finally, the findings suggest that task and activity recognition can benefit from gaze prediction.",1
"Effective spatiotemporal feature representation is crucial to the video-based action recognition task. Focusing on discriminate spatiotemporal feature learning, we propose Information Fused Temporal Transformation Network (IF-TTN) for action recognition on top of popular Temporal Segment Network (TSN) framework. In the network, Information Fusion Module (IFM) is designed to fuse the appearance and motion features at multiple ConvNet levels for each video snippet, forming a short-term video descriptor. With fused features as inputs, Temporal Transformation Networks (TTN) are employed to model middle-term temporal transformation between the neighboring snippets following a sequential order. As TSN itself depicts long-term temporal structure by segmental consensus, the proposed network comprehensively considers multiple granularity temporal features. Our IF-TTN achieves the state-of-the-art results on two most popular action recognition datasets: UCF101 and HMDB51. Empirical investigation reveals that our architecture is robust to the input motion map quality. Replacing optical flow with the motion vectors from compressed video stream, the performance is still comparable to the flow-based methods while the testing speed is 10x faster.",0
"The ability to represent spatiotemporal features effectively is essential for recognizing actions in videos. To address the need for discriminative spatiotemporal feature learning, we propose the Information Fused Temporal Transformation Network (IF-TTN) for action recognition, which builds on the Temporal Segment Network (TSN) framework. Our network includes an Information Fusion Module (IFM) that combines appearance and motion features at different ConvNet levels to create a short-term video descriptor. We then use Temporal Transformation Networks (TTN) to model middle-term temporal changes between neighboring snippets in sequence. By combining these features, our network considers multiple granularity temporal features, adding to the long-term temporal structure of TSN's segmental consensus. IF-TTN outperforms other methods on two major action recognition datasets-UFC101 and HMDB51. Our empirical investigation shows that our architecture is robust to motion map quality and can even replace optical flow with motion vectors from compressed video streams, resulting in comparable performance but 10x faster testing speed.",1
"Deep learning approaches to optical flow estimation have seen rapid progress over the recent years. One common trait of many networks is that they refine an initial flow estimate either through multiple stages or across the levels of a coarse-to-fine representation. While leading to more accurate results, the downside of this is an increased number of parameters. Taking inspiration from both classical energy minimization approaches as well as residual networks, we propose an iterative residual refinement (IRR) scheme based on weight sharing that can be combined with several backbone networks. It reduces the number of parameters, improves the accuracy, or even achieves both. Moreover, we show that integrating occlusion prediction and bi-directional flow estimation into our IRR scheme can further boost the accuracy. Our full network achieves state-of-the-art results for both optical flow and occlusion estimation across several standard datasets.",0
"Over the past few years, there has been significant progress in deep learning methods for optical flow estimation. Many of these networks share a common feature, refining an initial flow estimate through multiple stages or levels of a coarse-to-fine representation. While this approach can improve accuracy, it also increases the number of parameters required. Drawing from classical energy minimization techniques and residual networks, we introduce an iterative residual refinement (IRR) scheme that uses weight sharing and can be combined with various backbone networks. This scheme reduces the number of parameters and improves accuracy or achieves both. Additionally, we demonstrate that integrating occlusion prediction and bi-directional flow estimation into our IRR scheme can further enhance accuracy. Our complete network achieves the best results for optical flow and occlusion estimation on several standard datasets.",1
"Recent advances in 3D human shape estimation build upon parametric representations that model very well the shape of the naked body, but are not appropriate to represent the clothing geometry. In this paper, we present an approach to model dressed humans and predict their geometry from single images. We contribute in three fundamental aspects of the problem, namely, a new dataset, a novel shape parameterization algorithm and an end-to-end deep generative network for predicting shape.   First, we present 3DPeople, a large-scale synthetic dataset with 2.5 Million photo-realistic images of 80 subjects performing 70 activities and wearing diverse outfits. Besides providing textured 3D meshes for clothes and body, we annotate the dataset with segmentation masks, skeletons, depth, normal maps and optical flow. All this together makes 3DPeople suitable for a plethora of tasks.   We then represent the 3D shapes using 2D geometry images. To build these images we propose a novel spherical area-preserving parameterization algorithm based on the optimal mass transportation method. We show this approach to improve existing spherical maps which tend to shrink the elongated parts of the full body models such as the arms and legs, making the geometry images incomplete.   Finally, we design a multi-resolution deep generative network that, given an input image of a dressed human, predicts his/her geometry image (and thus the clothed body shape) in an end-to-end manner. We obtain very promising results in jointly capturing body pose and clothing shape, both for synthetic validation and on the wild images.",0
"Recent advancements in 3D human shape estimation have built upon parametric models that accurately represent the shape of the unclothed body, but are not suitable for representing clothing geometry. This paper presents an approach to model clothed humans and predict their geometry from a single image, contributing to three fundamental aspects of the problem: a new dataset, a novel shape parameterization algorithm, and an end-to-end deep generative network for predicting shape. The 3DPeople dataset is introduced, consisting of 2.5 million photo-realistic images of 80 subjects performing 70 activities and wearing diverse outfits, along with annotated textured 3D meshes for clothes and body, segmentation masks, skeletons, depth, normal maps, and optical flow. The 3D shapes are represented using 2D geometry images created through a novel spherical area-preserving parameterization algorithm based on the optimal mass transportation method. The approach improves upon existing spherical maps that tend to shrink elongated parts of full body models. Finally, a multi-resolution deep generative network is designed to predict geometry images of clothed humans from input images, capturing body pose and clothing shape effectively for both synthetic validation and wild images. The results are highly promising.",1
"We address the problem of recovering the 3D geometry of a human face from a set of facial images in multiple views. While recent studies have shown impressive progress in 3D Morphable Model (3DMM) based facial reconstruction, the settings are mostly restricted to a single view. There is an inherent drawback in the single-view setting: the lack of reliable 3D constraints can cause unresolvable ambiguities. We in this paper explore 3DMM-based shape recovery in a different setting, where a set of multi-view facial images are given as input. A novel approach is proposed to regress 3DMM parameters from multi-view inputs with an end-to-end trainable Convolutional Neural Network (CNN). Multiview geometric constraints are incorporated into the network by establishing dense correspondences between different views leveraging a novel self-supervised view alignment loss. The main ingredient of the view alignment loss is a differentiable dense optical flow estimator that can backpropagate the alignment errors between an input view and a synthetic rendering from another input view, which is projected to the target view through the 3D shape to be inferred. Through minimizing the view alignment loss, better 3D shapes can be recovered such that the synthetic projections from one view to another can better align with the observed image. Extensive experiments demonstrate the superiority of the proposed method over other 3DMM methods.",0
"The focus of our study is on the restoration of the 3D structure of a human face using multiple facial images. While recent research has made remarkable advancements in 3D Morphable Model (3DMM) based facial reconstruction, most of these studies are limited to a single viewpoint. This approach has a significant disadvantage as the absence of reliable 3D constraints can lead to unsolvable ambiguities. In our paper, we explore a different approach to 3DMM-based shape recovery, where a collection of multi-view facial images is used as input. To achieve this, we propose a new method to estimate the 3DMM parameters from multi-view inputs using a trainable Convolutional Neural Network (CNN). We incorporate multi-view geometric constraints into the network by creating dense correspondences between different views, using a novel self-supervised view alignment loss. The view alignment loss function uses a differentiable dense optical flow estimator to backpropagate the alignment errors between an input view and a synthetic rendering from another input view. By minimizing the view alignment loss, we can recover better 3D shapes such that the synthetic projections from one view to another can better align with the observed image. Our experiments demonstrate the superiority of our proposed method over other 3DMM methods.",1
"Unsupervised deep learning for optical flow computation has achieved promising results. Most existing deep-net based methods rely on image brightness consistency and local smoothness constraint to train the networks. Their performance degrades at regions where repetitive textures or occlusions occur. In this paper, we propose Deep Epipolar Flow, an unsupervised optical flow method which incorporates global geometric constraints into network learning. In particular, we investigate multiple ways of enforcing the epipolar constraint in flow estimation. To alleviate a ``chicken-and-egg'' type of problem encountered in dynamic scenes where multiple motions may be present, we propose a low-rank constraint as well as a union-of-subspaces constraint for training. Experimental results on various benchmarking datasets show that our method achieves competitive performance compared with supervised methods and outperforms state-of-the-art unsupervised deep-learning methods.",0
"Optical flow computation through unsupervised deep learning has shown promising outcomes. However, current deep-net based techniques rely heavily on image brightness consistency and local smoothness constraint for network training. These methods tend to fail in areas with repetitive textures or occlusions. This study introduces Deep Epipolar Flow, an unsupervised optical flow approach that includes global geometric constraints into network learning. Various strategies to enforce the epipolar constraint in flow estimation are explored. To tackle the issue of multiple motions in dynamic scenes, a low-rank constraint and a union-of-subspaces constraint are proposed for training. Experimental results on various benchmarking datasets demonstrate that our approach delivers performance that is comparable to supervised methods and surpasses current unsupervised deep-learning methods.",1
"Dense pixel matching is important for many computer vision tasks such as disparity and flow estimation. We present a robust, unified descriptor network that considers a large context region with high spatial variance. Our network has a very large receptive field and avoids striding layers to maintain spatial resolution. These properties are achieved by creating a novel neural network layer that consists of multiple, parallel, stacked dilated convolutions (SDC). Several of these layers are combined to form our SDC descriptor network. In our experiments, we show that our SDC features outperform state-of-the-art feature descriptors in terms of accuracy and robustness. In addition, we demonstrate the superior performance of SDC in state-of-the-art stereo matching, optical flow and scene flow algorithms on several famous public benchmarks.",0
"For tasks such as estimating disparity and flow, it is crucial to have accurate pixel matching. Our solution is a descriptor network that takes into account a wide context area with significant spatial variance. By employing a unique neural network layer, which involves multiple, parallel, stacked dilated convolutions, we were able to achieve a large receptive field while maintaining spatial resolution. Our experiments proved that our SDC features surpassed the accuracy and stability of other feature descriptors. Furthermore, we exhibited SDC's superior performance in several renowned public benchmarks for stereo matching, optical flow, and scene flow algorithms.",1
"Recently, deep image compression has shown a big progress in terms of coding efficiency and image quality improvement. However, relatively less attention has been put on video compression using deep learning networks. In the paper, we first propose a deep learning based bi-predictive coding network, called BP-DVC Net, for video compression. Learned from the lesson of the conventional video coding, a B-frame coding structure is incorporated in our BP-DVC Net. While the bi-predictive coding in the conventional video codecs requires to transmit to decoder sides the motion vectors for block motion and the residues from prediction, our BP-DVC Net incorporates optical flow estimation networks in both encoder and decoder sides so as not to transmit the motion information to the decoder sides for coding efficiency improvement. Also, a bi-prediction network in the BP-DVC Net is proposed and used to precisely predict the current frame and to yield the resulting residues as small as possible. Furthermore, our BP-DVC Net allows for the compressive feature maps to be entropy-coded using the temporal context among the feature maps of adjacent frames. The BP-DVC Net has an end-to-end video compression architecture with newly designed flow and prediction losses. Experimental results show that the compression performance of our proposed method is comparable to those of H.264, HEVC in terms of PSNR and MS-SSIM.",0
"In recent times, significant advancements have been made in deep image compression regarding coding efficiency and image quality enhancement. However, there has been comparatively less focus on video compression using deep learning networks. This paper presents the introduction of a deep learning-based bi-predictive coding network, called BP-DVC Net, for video compression. We have incorporated a B-frame coding structure in our BP-DVC Net, guided by the traditional video coding's teachings. Unlike bi-predictive coding in conventional video codecs that require transmitting motion vectors for block motion and prediction residues to the decoder, our BP-DVC Net has optical flow estimation networks in both encoder and decoder sides, eliminating the need to transmit motion information for coding efficiency improvement. Additionally, we propose a bi-prediction network in the BP-DVC Net that enables precise prediction of the current frame, resulting in minimal residues. Our BP-DVC Net allows compressive feature maps to be entropy-coded using the temporal context among the feature maps of adjacent frames. The proposed method has an end-to-end video compression architecture with newly designed flow and prediction losses. Experimental results show that our method's compression performance is comparable to that of H.264 and HEVC regarding PSNR and MS-SSIM.",1
"Unsupervised learning for geometric perception (depth, optical flow, etc.) is of great interest to autonomous systems. Recent works on unsupervised learning have made considerable progress on perceiving geometry; however, they usually ignore the coherence of objects and perform poorly under scenarios with dark and noisy environments. In contrast, supervised learning algorithms, which are robust, require large labeled geometric dataset. This paper introduces SIGNet, a novel framework that provides robust geometry perception without requiring geometrically informative labels. Specifically, SIGNet integrates semantic information to make depth and flow predictions consistent with objects and robust to low lighting conditions. SIGNet is shown to improve upon the state-of-the-art unsupervised learning for depth prediction by 30% (in squared relative error). In particular, SIGNet improves the dynamic object class performance by 39% in depth prediction and 29% in flow prediction. Our code will be made available at https://github.com/mengyuest/SIGNet",0
"Autonomous systems are highly interested in unsupervised learning for geometric perception such as depth and optical flow. While recent unsupervised learning efforts have made significant progress in perceiving geometry, they tend to neglect object coherence and perform poorly in low light and noisy conditions. On the other hand, supervised learning algorithms are robust but require large geometric datasets with labeled information. To overcome these limitations, this paper introduces SIGNet, a novel framework that integrates semantic information to provide strong geometry perception without the need for geometric labels. SIGNet makes depth and flow predictions consistent with objects and robust in low light conditions. The results show that SIGNet outperforms the state-of-the-art unsupervised learning for depth prediction by 30% in squared relative error. SIGNet also improves dynamic object class performance by 39% in depth prediction and 29% in flow prediction. The code for SIGNet is available at https://github.com/mengyuest/SIGNet.",1
"In semantic video segmentation the goal is to acquire consistent dense semantic labelling across image frames. To this end, recent approaches have been reliant on manually arranged operations applied on top of static semantic segmentation networks - with the most prominent building block being the optical flow able to provide information about scene dynamics. Related to that is the line of research concerned with speeding up static networks by approximating expensive parts of them with cheaper alternatives, while propagating information from previous frames. In this work we attempt to come up with generalisation of those methods, and instead of manually designing contextual blocks that connect per-frame outputs, we propose a neural architecture search solution, where the choice of operations together with their sequential arrangement are being predicted by a separate neural network. We showcase that such generalisation leads to stable and accurate results across common benchmarks, such as CityScapes and CamVid datasets. Importantly, the proposed methodology takes only 2 GPU-days, finds high-performing cells and does not rely on the expensive optical flow computation.",0
"The aim of semantic video segmentation is to obtain consistent and dense semantic labelling throughout image frames. Recent approaches have relied on manual operations applied onto static semantic segmentation networks, with optical flow being the most prominent building block to provide scene dynamics. Another line of research has focused on speeding up static networks by approximating costly parts with cheaper alternatives while passing on information from previous frames. In this study, we aim to generalize these methods by proposing a neural architecture search solution instead of manually designing contextual blocks to connect per-frame outputs. The choice and sequence of operations are predicted by a separate neural network. Our method produces stable and accurate results on common benchmarks like CityScapes and CamVid datasets. Crucially, our methodology only takes 2 GPU-days and doesn't require expensive optical flow computation, while still discovering high-performing cells.",1
"Owing to the development and advancement of artificial intelligence, numerous works were established in the human facial expression recognition system. Meanwhile, the detection and classification of micro-expressions are attracting attentions from various research communities in the recent few years. In this paper, we first review the processes of a conventional optical-flow-based recognition system, which comprised of facial landmarks annotations, optical flow guided images computation, features extraction and emotion class categorization. Secondly, a few approaches have been proposed to improve the feature extraction part, such as exploiting GAN to generate more image samples. Particularly, several variations of optical flow are computed in order to generate optimal images to lead to high recognition accuracy. Next, GAN, a combination of Generator and Discriminator, is utilized to generate new ""fake"" images to increase the sample size. Thirdly, a modified state-of-the-art Convolutional neural networks is proposed. To verify the effectiveness of the the proposed method, the results are evaluated on spontaneous micro-expression databases, namely SMIC, CASME II and SAMM. Both the F1-score and accuracy performance metrics are reported in this paper.",0
"Due to the advancements in artificial intelligence, there has been a surge of interest in the field of human facial expression recognition, with a recent focus on detecting and classifying micro-expressions. In this article, we provide a review of the conventional optical-flow-based recognition system, which includes facial landmark annotations, optical flow guided images computation, features extraction, and emotion class categorization. Additionally, we discuss various methods to enhance the feature extraction process, such as using GAN to generate more image samples and computing several variations of optical flow to generate optimal images. We also propose a modified convolutional neural network for improved performance. The proposed method is evaluated on spontaneous micro-expression databases (SMIC, CASME II, and SAMM) using F1-score and accuracy performance metrics.",1
"We introduce multigrid Predictive Filter Flow (mgPFF), a framework for unsupervised learning on videos. The mgPFF takes as input a pair of frames and outputs per-pixel filters to warp one frame to the other. Compared to optical flow used for warping frames, mgPFF is more powerful in modeling sub-pixel movement and dealing with corruption (e.g., motion blur). We develop a multigrid coarse-to-fine modeling strategy that avoids the requirement of learning large filters to capture large displacement. This allows us to train an extremely compact model (4.6MB) which operates in a progressive way over multiple resolutions with shared weights. We train mgPFF on unsupervised, free-form videos and show that mgPFF is able to not only estimate long-range flow for frame reconstruction and detect video shot transitions, but also readily amendable for video object segmentation and pose tracking, where it substantially outperforms the published state-of-the-art without bells and whistles. Moreover, owing to mgPFF's nature of per-pixel filter prediction, we have the unique opportunity to visualize how each pixel is evolving during solving these tasks, thus gaining better interpretability.",0
"Our new framework for unsupervised learning on videos is called multigrid Predictive Filter Flow (mgPFF). By taking a pair of frames as input and outputting per-pixel filters, mgPFF is more effective than optical flow in handling sub-pixel movement and corruption (such as motion blur). Our multigrid coarse-to-fine modeling strategy eliminates the need for large filters and allows for compact models (4.6MB) with shared weights. We trained mgPFF on unsupervised, free-form videos and found that it excels at estimating long-range flow, detecting video shot transitions, and performing video object segmentation and pose tracking. Additionally, mgPFF's per-pixel filter prediction enables better interpretability and visualization of pixel evolution during these tasks. Overall, mgPFF outperforms state-of-the-art methods without the need for additional features.",1
"We introduce a self-supervised method for learning visual correspondence from unlabeled video. The main idea is to use cycle-consistency in time as free supervisory signal for learning visual representations from scratch. At training time, our model learns a feature map representation to be useful for performing cycle-consistent tracking. At test time, we use the acquired representation to find nearest neighbors across space and time. We demonstrate the generalizability of the representation -- without finetuning -- across a range of visual correspondence tasks, including video object segmentation, keypoint tracking, and optical flow. Our approach outperforms previous self-supervised methods and performs competitively with strongly supervised methods.",0
"Our method introduces a way to learn visual correspondence from unlabeled video through self-supervision. The key concept is to leverage cycle-consistency in time to establish a free supervisory signal that facilitates the learning of visual representations from scratch. During training, our model acquires a feature map representation that is useful for cycle-consistent tracking. During testing, we apply this representation to identify nearest neighbors across space and time. We demonstrate that the learned representation is highly generalizable, without the need for finetuning, across a variety of visual correspondence tasks, including optical flow, keypoint tracking, and video object segmentation. Our approach outperforms previous self-supervised methods and is competitive with strongly supervised methods.",1
"Video frame interpolation aims to synthesize nonexistent frames in-between the original frames. While significant advances have been made from the recent deep convolutional neural networks, the quality of interpolation is often reduced due to large object motion or occlusion. In this work, we propose a video frame interpolation method which explicitly detects the occlusion by exploring the depth information. Specifically, we develop a depth-aware flow projection layer to synthesize intermediate flows that preferably sample closer objects than farther ones. In addition, we learn hierarchical features to gather contextual information from neighboring pixels. The proposed model then warps the input frames, depth maps, and contextual features based on the optical flow and local interpolation kernels for synthesizing the output frame. Our model is compact, efficient, and fully differentiable. Quantitative and qualitative results demonstrate that the proposed model performs favorably against state-of-the-art frame interpolation methods on a wide variety of datasets.",0
"The objective of video frame interpolation is to generate additional frames between the original frames. Despite the advancements in deep convolutional neural networks, the quality of the interpolation is often compromised by significant object motion or occlusion. This research proposes a video frame interpolation technique that detects occlusion by utilizing depth information. Specifically, a depth-aware flow projection layer is developed to create intermediate flows that prioritize sampling of closer objects. Additionally, hierarchical features are learned to gather contextual information from neighboring pixels. The proposed model then warps input frames, depth maps, and contextual features based on optical flow and local interpolation kernels to synthesize the output frame. The model is efficient, compact, and fully differentiable. Results demonstrate that the proposed model performs better than state-of-the-art frame interpolation methods on various datasets.",1
"With superiorities on low cost, portability, and free of radiation, echocardiogram is a widely used imaging modality for left ventricle (LV) function quantification. However, automatic LV segmentation and motion tracking is still a challenging task. In addition to fuzzy border definition, low contrast, and abounding artifacts on typical ultrasound images, the shape and size of the LV change significantly in a cardiac cycle. In this work, we propose a temporal affine network (TAN) to perform image analysis in a warped image space, where the shape and size variations due to the cardiac motion as well as other artifacts are largely compensated. Furthermore, we perform three frequent echocardiogram interpretation tasks simultaneously: standard cardiac plane recognition, LV landmark detection, and LV segmentation. Instead of using three networks with one dedicating to each task, we use a multi-task network to perform three tasks simultaneously. Since three tasks share the same encoder, the compact network improves the segmentation accuracy with more supervision. The network is further finetuned with optical flow adjusted annotations to enhance motion coherence in the segmentation result. Experiments on 1,714 2D echocardiographic sequences demonstrate that the proposed method achieves state-of-the-art segmentation accuracy with real-time efficiency.",0
"Echocardiogram is a widely used imaging technique for measuring left ventricle (LV) function due to its low cost, portability, and lack of radiation. However, automatic LV segmentation and motion tracking remains a challenging task due to fuzzy border definition, low contrast, and abundant artifacts in typical ultrasound images, as well as significant changes in LV shape and size during a cardiac cycle. To address these challenges, we propose a temporal affine network (TAN) that performs image analysis in a warped image space to compensate for cardiac motion and other artifacts. Additionally, we use a single multi-task network to simultaneously perform frequent echocardiogram interpretation tasks such as standard cardiac plane recognition, LV landmark detection, and LV segmentation. This approach improves segmentation accuracy by using a compact network that shares the same encoder for all three tasks. To further enhance motion coherence in the segmentation result, the network is fine-tuned with optical flow adjusted annotations. Our experiments on 1,714 2D echocardiographic sequences demonstrate that our proposed method achieves state-of-the-art segmentation accuracy with real-time efficiency.",1
"Learning descriptive spatio-temporal object models from data is paramount for the task of semi-supervised video object segmentation. Most existing approaches mainly rely on models that estimate the segmentation mask based on a reference mask at the first frame (aided sometimes by optical flow or the previous mask). These models, however, are prone to fail under rapid appearance changes or occlusions due to their limitations in modelling the temporal component. On the other hand, very recently, other approaches learned long-term features using a convolutional LSTM to leverage the information from all previous video frames. Even though these models achieve better temporal representations, they still have to be fine-tuned for every new video sequence. In this paper, we present an intermediate solution and devise a novel GAN architecture, FaSTGAN, to learn spatio-temporal object models over finite temporal windows. To achieve this, we concentrate all the heavy computational load to the training phase with two critics that enforce spatial and temporal mask consistency over the last K frames. Then at test time, we only use a relatively light regressor, which reduces the inference time considerably. As a result, our approach combines a high resiliency to sudden geometric and photometric object changes with efficiency at test time (no need for fine-tuning nor post-processing). We demonstrate that the accuracy of our method is on par with state-of-the-art techniques on the challenging YouTube-VOS and DAVIS datasets, while running at 32 fps, about 4x faster than the closest competitor.",0
"Developing descriptive spatio-temporal object models through data is crucial in semi-supervised video object segmentation. However, many current methods rely heavily on models that estimate segmentation masks based on a reference mask from the first frame. These models are not effective in cases of rapid appearance changes or occlusions due to their inability to model the temporal component. Recently, other approaches have used convolutional LSTMs to learn long-term features and improve temporal representations, but these models still require fine-tuning for each new video sequence. We propose a new solution, FaSTGAN, which learns spatio-temporal object models over finite temporal windows using a GAN architecture. During the training phase, heavy computational load is concentrated on two critics enforcing spatial and temporal mask consistency over the last K frames. At test time, only a relatively light regressor is used, reducing inference time significantly. Our approach achieves both high resiliency to sudden geometric and photometric object changes and efficiency at test time without the need for fine-tuning or post-processing. We demonstrate that our method is on par with state-of-the-art techniques on challenging datasets and runs at 32 fps, approximately 4x faster than the closest competitor.",1
"Successive frames of a video are highly redundant, and the most popular object detection methods do not take advantage of this fact. Using multiple consecutive frames can improve detection of small objects or difficult examples and can improve speed and detection consistency in a video sequence, for instance by interpolating features between frames. In this work, a novel approach is introduced to perform online video object detection using two consecutive frames of video sequences involving road users. Two new models, RetinaNet-Double and RetinaNet-Flow, are proposed, based respectively on the concatenation of a target frame with a preceding frame, and the concatenation of the optical flow with the target frame. The models are trained and evaluated on three public datasets. Experiments show that using a preceding frame improves performance over single frame detectors, but using explicit optical flow usually does not.",0
"Most object detection methods do not consider the high redundancy of successive frames in a video. However, utilizing multiple consecutive frames can enhance the detection of challenging objects and improve speed and consistency in a video sequence. This can be achieved by interpolating features between frames. The current study proposes a new approach to online video object detection using two consecutive frames of road user video sequences. Two models, RetinaNet-Double and RetinaNet-Flow, are introduced, which rely on the concatenation of a preceding frame with a target frame and the concatenation of optical flow with the target frame, respectively. The models are trained and evaluated on three public datasets, and results indicate that using a preceding frame can boost performance compared to single frame detectors, but explicit optical flow does not necessarily contribute to improved performance.",1
"The combination of spiking neural networks and event-based vision sensors holds the potential of highly efficient and high-bandwidth optical flow estimation. This paper presents the first hierarchical spiking architecture in which motion (direction and speed) selectivity emerges in an unsupervised fashion from the raw stimuli generated with an event-based camera. A novel adaptive neuron model and stable spike-timing-dependent plasticity formulation are at the core of this neural network governing its spike-based processing and learning, respectively. After convergence, the neural architecture exhibits the main properties of biological visual motion systems, namely feature extraction and local and global motion perception. Convolutional layers with input synapses characterized by single and multiple transmission delays are employed for feature and local motion perception, respectively; while global motion selectivity emerges in a final fully-connected layer. The proposed solution is validated using synthetic and real event sequences. Along with this paper, we provide the cuSNN library, a framework that enables GPU-accelerated simulations of large-scale spiking neural networks. Source code and samples are available at https://github.com/tudelft/cuSNN.",0
"The potential for highly efficient and high-bandwidth optical flow estimation can be achieved through a combination of spiking neural networks and event-based vision sensors. This study introduces the first hierarchical spiking architecture that produces motion selectivity (direction and speed) in an unsupervised manner using raw stimuli from an event-based camera. The neural network comprises an adaptive neuron model and stable spike-timing-dependent plasticity formulation, which govern its spike-based processing and learning, respectively. The architecture demonstrates feature extraction, local and global motion perception, and includes convolutional layers with input synapses characterized by single and multiple transmission delays for feature and local motion perception, while global motion selectivity emerges in a final fully-connected layer. Synthetic and real event sequences validate the proposed solution. Additionally, the cuSNN library is provided, allowing for GPU-accelerated simulations of large-scale spiking neural networks. The source code and samples are available at https://github.com/tudelft/cuSNN.",1
"We present a method for human pose tracking that is based on learning spatiotemporal relationships among joints. Beyond generating the heatmap of a joint in a given frame, our system also learns to predict the offset of the joint from a neighboring joint in the frame. Additionally, it is trained to predict the displacement of the joint from its position in the previous frame, in a manner that can account for possibly changing joint appearance, unlike optical flow. These relational cues in the spatial domain and temporal domain are inferred in a robust manner by attending only to relevant areas in the video frames. By explicitly learning and exploiting these joint relationships, our system achieves state-of-the-art performance on standard benchmarks for various pose tracking tasks including 3D body pose tracking in RGB video, 3D hand pose tracking in depth sequences, and 3D hand gesture tracking in RGB video.",0
"A technique for tracking human posture is presented, which involves learning spatiotemporal connections among joints. In addition to creating joint heatmaps for each frame, the system is also capable of predicting the joint's offset from its neighboring joint in the frame. Furthermore, the system forecasts the joint's displacement from its previous frame position, accounting for any possible joint appearance changes. By attending only to relevant areas in video frames, these relational cues in both the spatial and temporal domains are inferred in a reliable manner. The system achieves state-of-the-art results on standard benchmarks for different pose tracking tasks, including 3D body pose tracking in RGB video, 3D hand pose tracking in depth sequences, and 3D hand gesture tracking in RGB video, by explicitly learning and exploiting these joint relationships.",1
"Facial micro-expressions are subtle and involuntary expressions that can reveal concealed emotions. Micro-expressions are an invaluable source of information in application domains such as lie detection, mental health, sentiment analysis and more. One of the biggest challenges in this field of research is the small amount of available spontaneous micro-expression data. However, spontaneous data collection is burdened by time-consuming and expensive annotation. Hence, methods are needed which can reduce the amount of data that annotators have to review. This paper presents a novel micro-expression spotting method using a recurrent neural network (RNN) on optical flow features. We extract Histogram of Oriented Optical Flow (HOOF) features to encode the temporal changes in selected face regions. Finally, the RNN spots short intervals which are likely to contain occurrences of relevant facial micro-movements. The proposed method is evaluated on the SAMM database. Any chance of subject bias is eliminated by training the RNN using Leave-One-Subject-Out cross-validation. Comparing the spotted intervals with the labeled data shows that the method produced 1569 false positives while obtaining a recall of 0.4654. The initial results show that the proposed method would reduce the video length by a factor of 3.5, while still retaining almost half of the relevant micro-movements. Lastly, as the model gets more data, it becomes better at detecting intervals, which makes the proposed method suitable for supporting the annotation process.",0
"Facial micro-expressions are subtle involuntary expressions that can reveal hidden emotions. They are useful in various fields like lie detection, mental health, and sentiment analysis. The main challenge in researching micro-expressions is the limited availability of spontaneous data, which is time-consuming and expensive to annotate. Therefore, there is a need for methods that can reduce the data annotators have to review. This paper introduces a new micro-expression spotting method using a recurrent neural network on optical flow features. Histogram of Oriented Optical Flow features is used to encode the temporal changes in selected face regions. The RNN identifies short intervals that may contain relevant facial micro-movements. The proposed method is assessed on the SAMM database using Leave-One-Subject-Out cross-validation to eliminate any subject bias. The results indicate that the proposed method could lessen the video length by 3.5 times while retaining almost half of the relevant micro-movements. Additionally, as the model gains more data, it improves its ability to detect intervals, making it suitable for supporting the annotation process.",1
"Current benchmarks for optical flow algorithms evaluate the estimation quality by comparing their predicted flow field with the ground truth, and additionally may compare interpolated frames, based on these predictions, with the correct frames from the actual image sequences. For the latter comparisons, objective measures such as mean square errors are applied. However, for applications like image interpolation, the expected user's quality of experience cannot be fully deduced from such simple quality measures. Therefore, we conducted a subjective quality assessment study by crowdsourcing for the interpolated images provided in one of the optical flow benchmarks, the Middlebury benchmark. We used paired comparisons with forced choice and reconstructed absolute quality scale values according to Thurstone's model using the classical least squares method. The results give rise to a re-ranking of 141 participating algorithms w.r.t. visual quality of interpolated frames mostly based on optical flow estimation. Our re-ranking result shows the necessity of visual quality assessment as another evaluation metric for optical flow and frame interpolation benchmarks.",0
"Optical flow algorithms are currently evaluated based on their ability to estimate flow fields by comparing them with the ground truth. Additionally, the algorithms may be assessed by comparing interpolated frames based on their predictions with the correct frames from actual image sequences using objective measures like mean square errors. However, such simple quality measures cannot fully deduce the expected user experience for applications such as image interpolation. To address this, we conducted a subjective quality assessment study by crowdsourcing interpolated images from the Middlebury benchmark using paired comparisons with forced choice and Thurstone's model. We reconstructed absolute quality scale values using the classical least squares method to re-rank 141 participating algorithms based on the visual quality of interpolated frames mainly estimated by optical flow. Our results highlight the need for visual quality assessment as another evaluation metric for optical flow and frame interpolation benchmarks.",1
"Many semantic video analysis tasks can benefit from multiple, heterogenous signals. For example, in addition to the original RGB input sequences, sequences of optical flow are usually used to boost the performance of human action recognition in videos. To learn from these heterogenous input sources, existing methods reply on two-stream architectural designs that contain independent, parallel streams of Recurrent Neural Networks (RNNs). However, two-stream RNNs do not fully exploit the reciprocal information contained in the multiple signals, let alone exploit it in a recurrent manner. To this end, we propose in this paper a novel recurrent architecture, termed Coupled Recurrent Network (CRN), to deal with multiple input sources. In CRN, the parallel streams of RNNs are coupled together. Key design of CRN is a Recurrent Interpretation Block (RIB) that supports learning of reciprocal feature representations from multiple signals in a recurrent manner. Different from RNNs which stack the training loss at each time step or the last time step, we propose an effective and efficient training strategy for CRN. Experiments show the efficacy of the proposed CRN. In particular, we achieve the new state of the art on the benchmark datasets of human action recognition and multi-person pose estimation.",0
"The usage of various signals can be advantageous in semantic video analysis tasks. Apart from the initial RGB input sequences, optical flow sequences are typically utilized to enhance human action recognition in videos. Currently, existing methods use two-stream architectural designs that consist of self-reliant, parallel streams of Recurrent Neural Networks (RNNs) to learn from these different input sources. However, this approach does not fully utilize the interdependent information present in the multiple signals in a recurrent manner. To address this, we propose a novel recurrent architecture, called Coupled Recurrent Network (CRN), in this paper. CRN couples the parallel streams of RNNs together and contains a Recurrent Interpretation Block (RIB) that enables the learning of reciprocal feature representations from multiple signals in a recurrent fashion. Unlike RNNs, which stack the training loss at each time step or the final time step, we recommend an efficient training strategy for CRN. Our experiments demonstrate the effectiveness of CRN, and we achieve state-of-the-art results on benchmark datasets for human action recognition and multi-person pose estimation.",1
"This paper presents a semi-supervised learning framework to train a keypoint detector using multiview image streams given the limited labeled data (typically $<$4\%). We leverage the complementary relationship between multiview geometry and visual tracking to provide three types of supervisionary signals to utilize the unlabeled data: (1) keypoint detection in one view can be supervised by other views via the epipolar geometry; (2) a keypoint moves smoothly over time where its optical flow can be used to temporally supervise consecutive image frames to each other; (3) visible keypoint in one view is likely to be visible in the adjacent view. We integrate these three signals in a differentiable fashion to design a new end-to-end neural network composed of three pathways. This design allows us to extensively use the unlabeled data to train the keypoint detector. We show that our approach outperforms existing detectors including DeepLabCut tailored to the keypoint detection of non-human species such as monkeys, dogs, and mice.",0
"The aim of this paper is to introduce a semi-supervised learning framework that can effectively train a keypoint detector using multiview image streams despite limited labeled data (usually less than 4%). To achieve this, we take advantage of the complementary relationship between multiview geometry and visual tracking to provide three different types of supervisory signals that utilize the unlabeled data. Firstly, we use epipolar geometry to supervise keypoint detection in one view by other views. Secondly, we use optical flow to supervise consecutive image frames to each other by tracking the smooth movement of keypoints over time. Lastly, we use the visibility of keypoints in one view to predict their visibility in adjacent views. We combine these signals in a differentiable manner to create an end-to-end neural network with three pathways that can extensively use the unlabeled data to train the keypoint detector. Our results show that our approach outperforms existing detectors, including DeepLabCut, which is specifically designed for the keypoint detection of non-human species such as dogs, monkeys, and mice.",1
"Recent geometric methods need reliable estimates of 3D motion parameters to procure accurate dense depth map of a complex dynamic scene from monocular images \cite{kumar2017monocular, ranftl2016dense}. Generally, to estimate \textbf{precise} measurements of relative 3D motion parameters and to validate its accuracy using image data is a challenging task. In this work, we propose an alternative approach that circumvents the 3D motion estimation requirement to obtain a dense depth map of a dynamic scene. Given per-pixel optical flow correspondences between two consecutive frames and, the sparse depth prior for the reference frame, we show that, we can effectively recover the dense depth map for the successive frames without solving for 3D motion parameters. Our method assumes a piece-wise planar model of a dynamic scene, which undergoes rigid transformation locally, and as-rigid-as-possible transformation globally between two successive frames. Under our assumption, we can avoid the explicit estimation of 3D rotation and translation to estimate scene depth. In essence, our formulation provides an unconventional way to think and recover the dense depth map of a complex dynamic scene which is incremental and motion free in nature. Our proposed method does not make object level or any other high-level prior assumption about the dynamic scene, as a result, it is applicable to a wide range of scenarios. Experimental results on the benchmarks dataset show the competence of our approach for multiple frames.",0
"Accurate dense depth maps of complex dynamic scenes can be obtained using recent geometric methods, but they require reliable estimates of 3D motion parameters. However, obtaining precise measurements of relative 3D motion parameters and validating their accuracy using image data is challenging. To address this issue, we propose an alternative approach that does not require 3D motion estimation to obtain a dense depth map of a dynamic scene. Instead, we use per-pixel optical flow correspondences between two consecutive frames and a sparse depth prior for the reference frame to effectively recover the dense depth map for the successive frames. Our method assumes a piece-wise planar model of the scene that undergoes rigid transformation locally and as-rigid-as-possible transformation globally between two successive frames. This formulation provides an unconventional way to think and recover dense depth maps that is incremental and motion-free in nature and applicable to a wide range of scenarios. Our proposed method does not make object level or any other high-level prior assumption about the dynamic scene. Experimental results on benchmark datasets demonstrate the competence of our approach for multiple frames.",1
"In multi-person videos, especially team sport videos, a semantic event is usually represented as a confrontation between two teams of players, which can be represented as collective motion. In broadcast basketball videos, specific camera motions are used to present specific events. Therefore, a semantic event in broadcast basketball videos is closely related to both the global motion (camera motion) and the collective motion. A semantic event in basketball videos can be generally divided into three stages: pre-event, event occurrence (event-occ), and post-event. In this paper, we propose an ontology-based global and collective motion pattern (On_GCMP) algorithm for basketball event classification. First, a two-stage GCMP based event classification scheme is proposed. The GCMP is extracted using optical flow. The two-stage scheme progressively combines a five-class event classification algorithm on event-occs and a two-class event classification algorithm on pre-events. Both algorithms utilize sequential convolutional neural networks (CNNs) and long short-term memory (LSTM) networks to extract the spatial and temporal features of GCMP for event classification. Second, we utilize post-event segments to predict success/failure using deep features of images in the video frames (RGB_DF_VF) based algorithms. Finally the event classification results and success/failure classification results are integrated to obtain the final results. To evaluate the proposed scheme, we collected a new dataset called NCAA+, which is automatically obtained from the NCAA dataset by extending the fixed length of video clips forward and backward of the corresponding semantic events. The experimental results demonstrate that the proposed scheme achieves the mean average precision of 58.10% on NCAA+. It is higher by 6.50% than state-of-the-art on NCAA.",0
"The representation of a semantic event in multi-person videos, particularly in team sports videos, is often depicted as a confrontation between two teams of players through collective motion. Specific camera movements are employed in broadcast basketball videos to showcase particular events. Hence, a semantic event in broadcast basketball videos is closely linked to both global and collective motions and is divided into three stages: pre-event, event occurrence, and post-event. In this paper, we propose an algorithm called the ontology-based global and collective motion pattern (On_GCMP) for basketball event classification. We suggest a two-stage GCMP-based event classification scheme that employs optical flow to extract the GCMP. The first stage is a five-class event classification algorithm on event occurrences, while the second stage is a two-class event classification algorithm on pre-events. Both stages use sequential convolutional neural networks (CNNs) and long short-term memory (LSTM) networks to extract spatial and temporal features of the GCMP for event classification. Additionally, we employ post-event segments to predict success or failure using RGB_DF_VF-based algorithms. Finally, we integrate the event classification results with the success/failure classification results to obtain the final outcome. We evaluate the proposed scheme using a new dataset called NCAA+. The results show that our scheme achieves a mean average precision of 58.10%, which is 6.50% higher than state-of-the-art on NCAA.",1
"In this paper, several variants of two-stream architectures for temporal action proposal generation in long, untrimmed videos are presented. Inspired by the recent advances in the field of human action recognition utilizing 3D convolutions in combination with two-stream networks and based on the Single-Stream Temporal Action Proposals (SST) architecture, four different two-stream architectures utilizing sequences of images on one stream and sequences of images of optical flow on the other stream are subsequently investigated. The four architectures fuse the two separate streams at different depths in the model; for each of them, a broad range of parameters is investigated systematically as well as an optimal parametrization is empirically determined. The experiments on the THUMOS'14 dataset show that all four two-stream architectures are able to outperform the original single-stream SST and achieve state of the art results. Additional experiments revealed that the improvements are not restricted to a single method of calculating optical flow by exchanging the formerly used method of Brox with FlowNet2 and still achieving improvements.",0
"This paper presents various versions of two-stream architectures for generating proposals for temporal action in lengthy, untrimmed videos. These architectures are inspired by recent advancements in human action recognition that use 3D convolutions with two-stream networks. They build on the Single-Stream Temporal Action Proposals (SST) architecture and investigate four different two-stream architectures. These architectures use sequences of images for one stream and optical flow images for the other stream. The two streams are fused at different depths in the model, and a wide range of parameters are analyzed for each architecture to determine the optimal parametrization empirically. Experiments on the THUMOS'14 dataset show that all four two-stream architectures outperform the original single-stream SST and achieve state-of-the-art results. Additional experiments reveal that the improvements are not limited to a particular optical flow calculation method, as exchanging the previously used Brox method with FlowNet2 still results in improvements.",1
"Inferring the relations between two images is an important class of tasks in computer vision. Examples of such tasks include computing optical flow and stereo disparity. We treat the relation inference tasks as a machine learning problem and tackle it with neural networks. A key to the problem is learning a representation of relations. We propose a new neural network module, contrast association unit (CAU), which explicitly models the relations between two sets of input variables. Due to the non-negativity of the weights in CAU, we adopt a multiplicative update algorithm for learning these weights. Experiments show that neural networks with CAUs are more effective in learning five fundamental image transformations than conventional neural networks.",0
"The identification of connections between two images is a significant set of duties within the field of computer vision. Optical flow and stereo disparity calculations are examples of such duties. We view these inference tasks as a machine learning issue and address it via neural networks. The key to the problem is developing a representation of these connections. Our solution is the contrast association unit (CAU), a novel neural network component that models the relationship between two input variables. Learning the weights within CAU requires a multiplicative update algorithm due to the non-negativity of the weights. Our experiments demonstrate that neural networks equipped with CAUs are more adept at learning five fundamental image transformations compared to traditional neural networks.",1
"We address the unsupervised learning of several interconnected problems in low-level vision: single view depth prediction, camera motion estimation, optical flow, and segmentation of a video into the static scene and moving regions. Our key insight is that these four fundamental vision problems are coupled through geometric constraints. Consequently, learning to solve them together simplifies the problem because the solutions can reinforce each other. We go beyond previous work by exploiting geometry more explicitly and segmenting the scene into static and moving regions. To that end, we introduce Competitive Collaboration, a framework that facilitates the coordinated training of multiple specialized neural networks to solve complex problems. Competitive Collaboration works much like expectation-maximization, but with neural networks that act as both competitors to explain pixels that correspond to static or moving regions, and as collaborators through a moderator that assigns pixels to be either static or independently moving. Our novel method integrates all these problems in a common framework and simultaneously reasons about the segmentation of the scene into moving objects and the static background, the camera motion, depth of the static scene structure, and the optical flow of moving objects. Our model is trained without any supervision and achieves state-of-the-art performance among joint unsupervised methods on all sub-problems.",0
"Our focus is on unsupervised learning of low-level vision problems, specifically single view depth prediction, camera motion estimation, optical flow, and segmentation of a video into static and moving regions. We recognize that these four fundamental vision problems are interconnected through geometric constraints and propose that solving them together simplifies the problem since the solutions can reinforce each other. Unlike previous work, we explicitly exploit geometry and segment the scene into static and moving regions. Our Competitive Collaboration framework facilitates the coordinated training of multiple specialized neural networks to solve complex problems. Our novel method integrates all these problems into a common framework and simultaneously addresses segmentation, camera motion, depth, and optical flow. Our model achieves state-of-the-art performance among joint unsupervised methods on all sub-problems, without any supervision.",1
"This paper proposes the novel task of video generation conditioned on a SINGLE semantic label map, which provides a good balance between flexibility and quality in the generation process. Different from typical end-to-end approaches, which model both scene content and dynamics in a single step, we propose to decompose this difficult task into two sub-problems. As current image generation methods do better than video generation in terms of detail, we synthesize high quality content by only generating the first frame. Then we animate the scene based on its semantic meaning to obtain the temporally coherent video, giving us excellent results overall. We employ a cVAE for predicting optical flow as a beneficial intermediate step to generate a video sequence conditioned on the initial single frame. A semantic label map is integrated into the flow prediction module to achieve major improvements in the image-to-video generation process. Extensive experiments on the Cityscapes dataset show that our method outperforms all competing methods.",0
"In this paper, we introduce a new video generation task that uses a single semantic label map for improved flexibility and quality. Instead of the typical method of modeling both scene content and dynamics in a single step, we propose to break down the task into two sub-problems. To achieve high-quality content, we generate only the first frame using current image generation methods, which excel in detail. We then animate the scene based on its semantic meaning to produce a temporally coherent video, yielding excellent results overall. We utilize a cVAE to predict optical flow as an intermediary step to generate a video sequence conditioned on the initial single frame. By integrating a semantic label map into the flow prediction module, we achieve significant improvements in the image-to-video generation process. Our experiments on the Cityscapes dataset demonstrate that our method outperforms all other competing methods.",1
"Synthetic Aperture Vector Flow Imaging (SA-VFI) can visualize complex cardiac and vascular blood flow patterns at high temporal resolution with a large field of view. Convolutional neural networks (CNNs) are commonly used in image and video recognition and classification. However, most recently presented CNNs also allow for making per-pixel predictions as needed in optical flow velocimetry. To our knowledge we demonstrate here for the first time a CNN architecture to produce 2D full flow field predictions from high frame rate SA ultrasound images using supervised learning. The CNN was initially trained using CFD-generated and augmented noiseless SA ultrasound data of a realistic vessel geometry. Subsequently, a mix of noisy simulated and real \textit{in vivo} acquisitions were added to increase the network's robustness. The resulting flow field of the CNN resembled the ground truth accurately with an endpoint-error percentage between 6.5\% to 14.5\%. Furthermore, when confronted with an unknown geometry of an arterial bifurcation, the CNN was able to predict an accurate flow field indicating its ability for generalization. Remarkably, the CNN also performed well for rotational flows, which usually requires advanced, computationally intensive VFI methods. We have demonstrated that convolutional neural networks can be used to estimate complex multidirectional flow.",0
"With Synthetic Aperture Vector Flow Imaging (SA-VFI), it is possible to observe intricate blood flow patterns in the heart and blood vessels with a large field of view and high temporal resolution. Convolutional neural networks (CNNs) are frequently used for image and video recognition and classification; however, new CNNs are also capable of making per-pixel predictions, as seen in optical flow velocimetry. We present, to our knowledge, the first CNN architecture that uses supervised learning to produce 2D full flow field predictions from high frame rate SA ultrasound images. The CNN was initially trained using noiseless SA ultrasound data generated by CFD and augmented with realistic vessel geometry. To increase the network's robustness, a combination of simulated and real in vivo acquisitions were introduced. The CNN's resulting flow field closely resembled the ground truth, with an endpoint-error percentage ranging from 6.5% to 14.5%. Additionally, the CNN was able to predict accurate flow fields for an unknown arterial bifurcation geometry, indicating its ability to generalize. Remarkably, the CNN also performed well for rotational flows, which typically require advanced, computationally intensive VFI methods. Our study demonstrates the potential of convolutional neural networks to estimate complex multidirectional flow.",1
"Dynamic scenes that contain both object motion and egomotion are a challenge for monocular visual odometry (VO). Another issue with monocular VO is the scale ambiguity, i.e. these methods cannot estimate scene depth and camera motion in real scale. Here, we propose a learning based approach to predict camera motion parameters directly from optic flow, by marginalizing depthmap variations and outliers. This is achieved by learning a sparse overcomplete basis set of egomotion in an autoencoder network, which is able to eliminate irrelevant components of optic flow for the task of camera parameter or motionfield estimation. The model is trained using a sparsity regularizer and a supervised egomotion loss, and achieves the state-of-the-art performances on trajectory prediction and camera rotation prediction tasks on KITTI and Virtual KITTI datasets, respectively. The sparse latent space egomotion representation learned by the model is robust and requires only 5% of the hidden layer neurons to maintain the best trajectory prediction accuracy on KITTI dataset. Additionally, in presence of depth information, the proposed method demonstrates faithful object velocity prediction for wide range of object sizes and speeds by global compensation of predicted egomotion and a divisive normalization procedure.",0
"Monocular visual odometry faces difficulties when trying to process dynamic scenes containing both object and egomotion. It also struggles with scale ambiguity, as it is unable to estimate scene depth and camera motion in real scale. To address these issues, we propose a learning-based approach that predicts camera motion parameters directly from optic flow, using an autoencoder network to learn a sparse overcomplete basis set of egomotion. This eliminates irrelevant components of optic flow and outliers, achieved through a sparsity regularizer and supervised egomotion loss. Our model achieves state-of-the-art performance on KITTI and Virtual KITTI datasets for trajectory and camera rotation prediction tasks. The sparse latent space egomotion representation is robust and requires only 5% of hidden layer neurons for the best trajectory prediction accuracy on KITTI. When depth information is present, our method demonstrates accurate object velocity prediction for various sizes and speeds through global compensation of predicted egomotion and divisive normalization.",1
"The goal of video segmentation is to turn video data into a set of concrete motion clusters that can be easily interpreted as building blocks of the video. There are some works on similar topics like detecting scene cuts in a video, but there is few specific research on clustering video data into the desired number of compact segments. It would be more intuitive, and more efficient, to work with perceptually meaningful entity obtained from a low-level grouping process which we call it superframe. This paper presents a new simple and efficient technique to detect superframes of similar content patterns in videos. We calculate the similarity of content-motion to obtain the strength of change between consecutive frames. With the help of existing optical flow technique using deep models, the proposed method is able to perform more accurate motion estimation efficiently. We also propose two criteria for measuring and comparing the performance of different algorithms on various databases. Experimental results on the videos from benchmark databases have demonstrated the effectiveness of the proposed method.",0
"The objective of video segmentation is to transform video data into distinct motion clusters that can be easily comprehended as fundamental elements of the video. While some research has been done on related topics, such as detecting scene cuts in a video, there has been limited research on clustering video data into a specific number of compact segments. It would be more logical and efficient to use perceptually meaningful entities, which we refer to as superframes, obtained from a low-level grouping process. This article presents a new and efficient approach for identifying superframes of comparable content patterns in videos. The method involves calculating the similarity of content-motion to determine the strength of change between consecutive frames, along with utilizing deep models and optical flow techniques to perform more precise motion estimation. In addition, the article suggests two criteria for evaluating the performance of different algorithms on various databases. The proposed method's effectiveness was demonstrated through experimental results on videos from benchmark databases.",1
"Identifying human behaviors is a challenging research problem due to the complexity and variation of appearances and postures, the variation of camera settings, and view angles. In this paper, we try to address the problem of human behavior identification by introducing a novel motion descriptor based on statistical features. The method first divide the video into N number of temporal segments. Then for each segment, we compute dense optical flow, which provides instantaneous velocity information for all the pixels. We then compute Histogram of Optical Flow (HOOF) weighted by the norm and quantized into 32 bins. We then compute statistical features from the obtained HOOF forming a descriptor vector of 192- dimensions. We then train a non-linear multi-class SVM that classify different human behaviors with the accuracy of 72.1%. We evaluate our method by using publicly available human action data set. Experimental results shows that our proposed method out performs state of the art methods.",0
"The task of identifying human behaviors is a complex research problem due to the variability in appearances, postures, camera settings, and view angles. This paper proposes a new approach to address this challenge by introducing a motion descriptor based on statistical features. The technique involves dividing the video into N temporal segments, computing dense optical flow for each segment, and generating a Histogram of Optical Flow (HOOF) that is weighted by norm and quantized into 32 bins. Statistical features are then computed from the HOOF to form a 192-dimensional descriptor vector. A non-linear multi-class SVM is trained using this descriptor to classify different human behaviors, achieving an accuracy of 72.1%. The proposed method is evaluated using a publicly available human action dataset, and the experimental results show that it outperforms existing state-of-the-art methods.",1
"Two-stream architecture have shown strong performance in video classification task. The key idea is to learn spatio-temporal features by fusing convolutional networks spatially and temporally. However, there are some problems within such architecture. First, it relies on optical flow to model temporal information, which are often expensive to compute and store. Second, it has limited ability to capture details and local context information for video data. Third, it lacks explicit semantic guidance that greatly decrease the classification performance. In this paper, we proposed a new two-stream based deep framework for video classification to discover spatial and temporal information only from RGB frames, moreover, the multi-scale pyramid attention (MPA) layer and the semantic adversarial learning (SAL) module is introduced and integrated in our framework. The MPA enables the network capturing global and local feature to generate a comprehensive representation for video, and the SAL can make this representation gradually approximate to the real video semantics in an adversarial manner. Experimental results on two public benchmarks demonstrate our proposed methods achieves state-of-the-art results on standard video datasets.",0
"The use of two-stream architecture has demonstrated exceptional performance in video classification tasks by combining convolutional networks spatially and temporally to learn spatio-temporal features. Nonetheless, this architecture poses several issues. Firstly, it relies on optical flow, which is both expensive to calculate and store, to model temporal information. Secondly, it has limited capacity to capture details and local context information in video data. Lastly, it lacks explicit semantic guidance, which significantly reduces classification performance. To address these problems, we introduce a new two-stream-based deep framework for video classification that solely detects spatial and temporal information from RGB frames. Furthermore, we integrate the multi-scale pyramid attention (MPA) layer and the semantic adversarial learning (SAL) module into our framework. The MPA layer enables the network to capture global and local features, producing a comprehensive video representation, while the SAL module progressively approximates this representation to the actual video semantics in an adversarial manner. Experimental results on two public benchmarks demonstrate the superiority of our proposed method in achieving state-of-the-art results on standard video datasets.",1
"Face anti-spoofing is significant to the security of face recognition systems. Previous works on depth supervised learning have proved the effectiveness for face anti-spoofing. Nevertheless, they only considered the depth as an auxiliary supervision in the single frame. Different from these methods, we develop a new method to estimate depth information from multiple RGB frames and propose a depth-supervised architecture which can efficiently encodes spatiotemporal information for presentation attack detection. It includes two novel modules: optical flow guided feature block (OFFB) and convolution gated recurrent units (ConvGRU) module, which are designed to extract short-term and long-term motion to discriminate living and spoofing faces. Extensive experiments demonstrate that the proposed approach achieves state-of-the-art results on four benchmark datasets, namely OULU-NPU, SiW, CASIA-MFSD, and Replay-Attack.",0
"The security of face recognition systems relies heavily on face anti-spoofing. Previous research on depth supervised learning has shown that it is an effective method for face anti-spoofing, but it has only been used as an auxiliary supervision in a single frame. Our new approach estimates depth information from multiple RGB frames and utilizes a depth-supervised architecture that encodes spatiotemporal information for presentation attack detection. This method includes two innovative modules, the optical flow guided feature block (OFFB) and convolution gated recurrent units (ConvGRU) module, which extract short-term and long-term motion to distinguish between living and spoofing faces. Our approach has been extensively tested and has achieved state-of-the-art results on four benchmark datasets: OULU-NPU, SiW, CASIA-MFSD, and Replay-Attack.",1
"State-of-the-art neural network models estimate large displacement optical flow in multi-resolution and use warping to propagate the estimation between two resolutions. Despite their impressive results, it is known that there are two problems with the approach. First, the multi-resolution estimation of optical flow fails in situations where small objects move fast. Second, warping creates artifacts when occlusion or dis-occlusion happens. In this paper, we propose a new neural network module, Deformable Cost Volume, which alleviates the two problems. Based on this module, we designed the Deformable Volume Network (Devon) which can estimate multi-scale optical flow in a single high resolution. Experiments show Devon is more suitable in handling small objects moving fast and achieves comparable results to the state-of-the-art methods in public benchmarks.",0
"Cutting-edge neural network models utilize warping to propagate large displacement optical flow estimation across multiple resolutions. While these models have proven impressive, they are known to face two issues. Firstly, multi-resolution estimation struggles to track the movement of small objects at high speeds. Secondly, warping can create artifacts during occlusion or dis-occlusion. To address these problems, we suggest a novel neural network component, the Deformable Cost Volume, which mitigates these issues. We have developed the Deformable Volume Network (Devon) based on this component, allowing for the estimation of multi-scale optical flow at a single high resolution. Our experiments demonstrate Devon's superior ability to handle fast-moving small objects and achieve equivalent results to state-of-the-art techniques in public benchmarks.",1
"Predicting the future location of vehicles is essential for safety-critical applications such as advanced driver assistance systems (ADAS) and autonomous driving. This paper introduces a novel approach to simultaneously predict both the location and scale of target vehicles in the first-person (egocentric) view of an ego-vehicle. We present a multi-stream recurrent neural network (RNN) encoder-decoder model that separately captures both object location and scale and pixel-level observations for future vehicle localization. We show that incorporating dense optical flow improves prediction results significantly since it captures information about motion as well as appearance change. We also find that explicitly modeling future motion of the ego-vehicle improves the prediction accuracy, which could be especially beneficial in intelligent and automated vehicles that have motion planning capability. To evaluate the performance of our approach, we present a new dataset of first-person videos collected from a variety of scenarios at road intersections, which are particularly challenging moments for prediction because vehicle trajectories are diverse and dynamic.",0
"For safety-critical applications like advanced driver assistance systems (ADAS) and autonomous driving, it is crucial to predict the future location of vehicles. This study proposes a new technique that can predict both the location and scale of target vehicles in the first-person view of an ego-vehicle at the same time. The approach employs a multi-stream recurrent neural network (RNN) encoder-decoder model that captures object location and scale separately, along with pixel-level observations for future vehicle localization. The inclusion of dense optical flow enhances the prediction outcomes significantly by capturing information about both motion and appearance changes. Furthermore, explicitly modeling the future motion of the ego-vehicle improves prediction accuracy, which is especially useful in intelligent and automated vehicles with motion planning capability. To test the effectiveness of the approach, a new dataset of first-person videos captured from various scenarios at road intersections is presented, which is challenging due to the diverse and dynamic nature of vehicle trajectories.",1
"In this work we present a lightweight, unsupervised learning pipeline for \textit{dense} depth, optical flow and egomotion estimation from sparse event output of the Dynamic Vision Sensor (DVS). To tackle this low level vision task, we use a novel encoder-decoder neural network architecture - ECN.   Our work is the first monocular pipeline that generates dense depth and optical flow from sparse event data only. The network works in self-supervised mode and has just 150k parameters. We evaluate our pipeline on the MVSEC self driving dataset and present results for depth, optical flow and and egomotion estimation. Due to the lightweight design, the inference part of the network runs at 250 FPS on a single GPU, making the pipeline ready for realtime robotics applications. Our experiments demonstrate significant improvements upon previous works that used deep learning on event data, as well as the ability of our pipeline to perform well during both day and night.",0
"We introduce a simple and unsupervised learning pipeline for estimating dense depth, optical flow, and egomotion from sparse event output of the Dynamic Vision Sensor (DVS). Our approach employs a novel encoder-decoder neural network architecture (ECN) for low-level vision tasks. Notably, our pipeline is the first of its kind to generate dense depth and optical flow solely from sparse event data. Operating in self-supervised mode, the network has a lightweight design with only 150k parameters. We evaluate the performance of our pipeline on the MVSEC self-driving dataset for depth, optical flow, and egomotion estimation, and demonstrate its ability to significantly outperform existing deep learning models, while also being effective in both day and night conditions. Moreover, the network runs at 250 FPS on a single GPU, which makes it ideal for real-time robotics applications.",1
"We present DDFlow, a data distillation approach to learning optical flow estimation from unlabeled data. The approach distills reliable predictions from a teacher network, and uses these predictions as annotations to guide a student network to learn optical flow. Unlike existing work relying on hand-crafted energy terms to handle occlusion, our approach is data-driven, and learns optical flow for occluded pixels. This enables us to train our model with a much simpler loss function, and achieve a much higher accuracy. We conduct a rigorous evaluation on the challenging Flying Chairs, MPI Sintel, KITTI 2012 and 2015 benchmarks, and show that our approach significantly outperforms all existing unsupervised learning methods, while running at real time.",0
"DDFlow is a novel technique for teaching optical flow estimation using unlabeled data. Our method distills accurate predictions from a teacher network and employs them as annotations to guide a student network in learning the optical flow. Unlike conventional approaches that use hand-crafted energy terms to address occlusion, our data-driven approach learns optical flow for obscured pixels. This simplifies the loss function and enhances the accuracy of our model. We evaluate our approach on challenging benchmarks such as Flying Chairs, MPI Sintel, KITTI 2012, and 2015, and demonstrate that it outperforms all existing unsupervised learning methods in real-time.",1
"In this work, we propose a novel transformation for events from an event camera that is equivariant to optical flow under convolutions in the 3-D spatiotemporal domain. Events are generated by changes in the image, which are typically due to motion, either of the camera or the scene. As a result, different motions result in a different set of events. For learning based tasks based on a static scene such as classification which directly use the events, we must either rely on the learning method to learn the underlying object distinct from the motion, or to memorize all possible motions for each object with extensive data augmentation. Instead, we propose a novel transformation of the input event data which normalizes the $x$ and $y$ positions by the timestamp of each event. We show that this transformation generates a representation of the events that is equivariant to this motion when the optical flow is constant, allowing a deep neural network to learn the classification task without the need for expensive data augmentation. We test our method on the event based N-MNIST dataset, as well as a novel dataset N-MOVING-MNIST, with significantly more variety in motion compared to the standard N-MNIST dataset. In all sequences, we demonstrate that our transformed network is able to achieve similar or better performance compared to a network with a standard volumetric event input, and performs significantly better when the test set has a larger set of motions than seen at training.",0
"Our work proposes a unique approach to transform events generated by an event camera. This transformation is equivariant to optical flow when convolutions are applied in the 3-D spatiotemporal domain. Events are produced due to image changes caused by camera or scene motion, resulting in a distinct set of events for different motions. For learning tasks, such as classification, that rely on events, we face the challenge of either relying on the learning method to distinguish objects from motion or memorizing all possible motions for each object with extensive data augmentation. Our solution involves normalizing the $x$ and $y$ positions of each event by its timestamp, resulting in a motion-equivariant representation that does not require costly data augmentation. We validate our method on the N-MNIST dataset and a novel N-MOVING-MNIST dataset, demonstrating improved performance for the latter, especially when dealing with a larger set of motions in the test set. Our transformed network performs similarly or better than a network with volumetric event input.",1
"This paper proposes a novel framework to reconstruct the dynamic magnetic resonance images (DMRI) with motion compensation (MC). Due to the inherent motion effects during DMRI acquisition, reconstruction of DMRI using motion estimation/compensation (ME/MC) has been studied under a compressed sensing (CS) scheme. In this paper, by embedding the intensity-based optical flow (OF) constraint into the traditional CS scheme, we are able to couple the DMRI reconstruction with motion field estimation. The formulated optimization problem is solved by a primal-dual algorithm with linesearch due to its efficiency when dealing with non-differentiable problems. With the estimated motion field, the DMRI reconstruction is refined through MC. By employing the multi-scale coarse-to-fine strategy, we are able to update the variables(temporal image sequences and motion vectors) and to refine the image reconstruction alternately. Moreover, the proposed framework is capable of handling a wide class of prior information (regularizations) for DMRI reconstruction, such as sparsity, low rank and total variation. Experiments on various DMRI data, ranging from in vivo lung to cardiac dataset, validate the reconstruction quality improvement using the proposed scheme in comparison to several state-of-the-art algorithms.",0
"This paper presents a new approach for reconstructing dynamic magnetic resonance images (DMRI) by compensating for motion. The reconstruction of DMRI with motion estimation/compensation (ME/MC) has been previously studied under a compressed sensing (CS) scheme due to motion effects during DMRI acquisition. The proposed method incorporates an intensity-based optical flow (OF) constraint into the traditional CS scheme to couple DMRI reconstruction with motion field estimation. The optimization problem is solved using a primal-dual algorithm with linesearch, which efficiently handles non-differentiable problems. The DMRI reconstruction is refined through MC using the estimated motion field and a multi-scale coarse-to-fine strategy updates the variables and refines the image reconstruction alternatively. The proposed framework accommodates various prior information (regularizations) for DMRI reconstruction, including sparsity, low rank, and total variation. Experiments on various DMRI data validate the effectiveness of the proposed scheme compared to several state-of-the-art algorithms.",1
"In this work, we propose a method that combines unsupervised deep learning predictions for optical flow and monocular disparity with a model based optimization procedure for instantaneous camera pose. Given the flow and disparity predictions from the network, we apply a RANSAC outlier rejection scheme to find an inlier set of flows and disparities, which we use to solve for the relative camera pose in a least squares fashion. We show that this pipeline is fully differentiable, allowing us to combine the pose with the network outputs as an additional unsupervised training loss to further refine the predicted flows and disparities. This method not only allows us to directly regress relative pose from the network outputs, but also automatically segments away pixels that do not fit the rigid scene assumptions that many unsupervised structure from motion methods apply, such as on independently moving objects. We evaluate our method on the KITTI dataset, and demonstrate state of the art results, even in the presence of challenging independently moving objects.",0
"Our proposed approach combines unsupervised deep learning predictions for optical flow and monocular disparity with a model-based optimization procedure to determine instantaneous camera pose. By applying a RANSAC outlier rejection scheme to the flow and disparity predictions, we can identify an inlier set and solve for the relative camera pose using the least squares method. The pipeline is entirely differentiable, enabling us to incorporate the pose as an additional unsupervised training loss and refine the predicted flows and disparities. This approach not only enables us to directly regress relative pose from the network outputs but also automatically eliminates pixels that do not conform to the rigid scene assumptions that many unsupervised structure from motion methods impose, such as independently moving objects. We evaluate our method on the KITTI dataset and achieve state-of-the-art results, even in the presence of challenging independently moving objects.",1
"State-of-the-art methods for video action recognition commonly use an ensemble of two networks: the spatial stream, which takes RGB frames as input, and the temporal stream, which takes optical flow as input. In recent work, both of these streams consist of 3D Convolutional Neural Networks, which apply spatiotemporal filters to the video clip before performing classification. Conceptually, the temporal filters should allow the spatial stream to learn motion representations, making the temporal stream redundant. However, we still see significant benefits in action recognition performance by including an entirely separate temporal stream, indicating that the spatial stream is ""missing"" some of the signal captured by the temporal stream. In this work, we first investigate whether motion representations are indeed missing in the spatial stream of 3D CNNs. Second, we demonstrate that these motion representations can be improved by distillation, by tuning the spatial stream to predict the outputs of the temporal stream, effectively combining both models into a single stream. Finally, we show that our Distilled 3D Network (D3D) achieves performance on par with two-stream approaches, using only a single model and with no need to compute optical flow.",0
"Cutting-edge techniques for recognizing actions in videos commonly employ a duo of networks: the spatial stream that accepts RGB frames and the temporal stream that takes in optical flow. Recently, both these streams have adopted 3D Convolutional Neural Networks, which use spatiotemporal filters to classify video clips. In theory, the temporal filters should enable the spatial stream to learn motion representations, rendering the temporal stream unnecessary. However, even with a separate temporal stream, action recognition performance sees significant improvements, suggesting that the spatial stream is missing some of the information captured by the temporal stream. In this study, we explore whether the spatial stream of 3D CNNs is indeed missing motion representations and demonstrate that these representations can be enhanced through distillation, which involves fine-tuning the spatial stream to predict the temporal stream's outputs, effectively fusing both models into a single stream. Finally, we prove that our Distilled 3D Network (D3D) achieves performance equivalent to two-stream approaches with just one model and no requirement for optical flow computation.",1
"Despite the progress within the last decades, weather forecasting is still a challenging and computationally expensive task. Current satellite-based approaches to predict thunderstorms are usually based on the analysis of the observed brightness temperatures in different spectral channels and emit a warning if a critical threshold is reached. Recent progress in data science however demonstrates that machine learning can be successfully applied to many research fields in science, especially in areas dealing with large datasets. We therefore present a new approach to the problem of predicting thunderstorms based on machine learning. The core idea of our work is to use the error of two-dimensional optical flow algorithms applied to images of meteorological satellites as a feature for machine learning models. We interpret that optical flow error as an indication of convection potentially leading to thunderstorms and lightning. To factor in spatial proximity we use various manual convolution steps. We also consider effects such as the time of day or the geographic location. We train different tree classifier models as well as a neural network to predict lightning within the next few hours (called nowcasting in meteorology) based on these features. In our evaluation section we compare the predictive power of the different models and the impact of different features on the classification result. Our results show a high accuracy of 96% for predictions over the next 15 minutes which slightly decreases with increasing forecast period but still remains above 83% for forecasts of up to five hours. The high false positive rate of nearly 6% however needs further investigation to allow for an operational use of our approach.",0
"Although there has been progress in weather forecasting in recent years, it remains a complex and computationally challenging task. Presently, satellite-based approaches for predicting thunderstorms rely on analyzing observed brightness temperatures in various spectral channels, emitting a warning if a critical threshold is reached. However, advances in data science have shown that machine learning can be successfully applied to many scientific fields, particularly those dealing with large datasets. As such, we propose a new approach to predicting thunderstorms using machine learning. Our approach utilizes the error from two-dimensional optical flow algorithms applied to meteorological satellite images as a feature for machine learning models. This error serves as an indication of convection that may lead to thunderstorms and lightning. To account for spatial proximity, we incorporate various convolution steps. Additionally, we consider other factors such as time of day and geographic location. We train various tree classifier models and a neural network to predict lightning within the next few hours (known as nowcasting in meteorology) based on these features. In our evaluation, we compare the predictive power of different models and the impact of different features on the classification results. Our results indicate a high accuracy of 96% for predictions over the next 15 minutes, which slightly decreases with increasing forecast period but remains above 83% for forecasts up to five hours. However, the high false positive rate of nearly 6% requires further investigation before our approach can be used operationally.",1
"In this paper, we propose a data-driven visual rhythm prediction method, which overcomes the previous works' deficiency that predictions are made primarily by human-crafted hard rules. In our approach, we first extract features including original frames and their residuals, optical flow, scene change, and body pose. These visual features will be next taken into an end-to-end neural network as inputs. Here we observe that there are some slight misaligning between features over the timeline and assume that this is due to the distinctions between how different features are computed. To solve this problem, the extracted features are aligned by an elaborately designed layer, which can also be applied to other models suffering from mismatched features, and boost performance. Then these aligned features are fed into sequence labeling layers implemented with BiLSTM and CRF to predict the onsets. Due to the lack of existing public training and evaluation set, we experiment on a dataset constructed by ourselves based on professionally edited Music Videos (MVs), and the F1 score of our approach reaches 79.6.",0
"This paper presents a novel approach for predicting visual rhythm using data-driven methods. Unlike previous works that rely on human-crafted rules, our method utilizes various visual features such as original frames, residuals, optical flow, scene change, and body pose. These features are fed into an end-to-end neural network after being aligned to address any misalignments over time. To predict onsets, the aligned features are passed through sequence labeling layers implemented with BiLSTM and CRF. Our approach was evaluated on a dataset constructed from professionally edited Music Videos (MVs), as no existing public training and evaluation set was available. The F1 score achieved by our method is 79.6, indicating its effectiveness in visual rhythm prediction.",1
"Exploration bonus derived from the novelty of the states in an environment has become a popular approach to motivate exploration for deep reinforcement learning agents in the past few years. Recent methods such as curiosity-driven exploration usually estimate the novelty of new observations by the prediction errors of their system dynamics models. Due to the capacity limitation of the models and difficulty of performing next-frame prediction, however, these methods typically fail to balance between exploration and exploitation in high-dimensional observation tasks, resulting in the agents forgetting the visited paths and exploring those states repeatedly. Such inefficient exploration behavior causes significant performance drops, especially in large environments with sparse reward signals. In this paper, we propose to introduce the concept of optical flow estimation from the field of computer vision to deal with the above issue. We propose to employ optical flow estimation errors to examine the novelty of new observations, such that agents are able to memorize and understand the visited states in a more comprehensive fashion. We compare our method against the previous approaches in a number of experimental experiments. Our results indicate that the proposed method appears to deliver superior and long-lasting performance than the previous methods. We further provide a set of comprehensive ablative analysis of the proposed method, and investigate the impact of optical flow estimation on the learning curves of the DRL agents.",0
"In recent years, using the novelty of states in an environment as an exploration bonus has become a popular technique for motivating deep reinforcement learning agents to explore. However, methods like curiosity-driven exploration that estimate the novelty of new observations based on prediction errors of system dynamics models often struggle to balance exploration and exploitation in high-dimensional observation tasks. This leads to inefficient exploration behavior where agents forget visited paths and repeatedly explore the same states. This can significantly impact performance, particularly in large environments with sparse reward signals. To address this issue, we propose using optical flow estimation from computer vision to examine the novelty of new observations. This allows agents to memorize and understand visited states more comprehensively. Our experiments show that our proposed method outperforms previous approaches and delivers superior and long-lasting performance. We also provide a comprehensive analysis of the proposed method and investigate the impact of optical flow estimation on the learning curves of DRL agents.",1
This paper describes the design and implementation of a ground-related odometry sensor suitable for micro aerial vehicles. The sensor is based on a ground-facing camera and a single-board Linux-based embedded computer with a multimedia System on a Chip (SoC). The SoC features a hardware video encoder which is used to estimate the optical flow online. The optical flow is then used in combination with a distance sensor to estimate the vehicle's velocity. The proposed sensor is compared to a similar existing solution and evaluated in both indoor and outdoor environments.,0
"In this article, the development and execution of a ground-related odometry sensor that is appropriate for micro aerial vehicles is explained. The sensor uses a ground-facing camera and a single-board Linux-based embedded computer with a multimedia System on a Chip (SoC). The SoC has a hardware video encoder that is utilized to determine the optical flow in real-time. The optical flow is combined with a distance sensor to determine the velocity of the vehicle. The sensor is compared to a similar present solution and tested in both indoor and outdoor settings.",1
"In this paper, we propose the use of a semantic image, an improved representation for video analysis, principally in combination with Inception networks. The semantic image is obtained by applying localized sparse segmentation using global clustering (LSSGC) prior to the approximate rank pooling which summarizes the motion characteristics in single or multiple images. It incorporates the background information by overlaying a static background from the window onto the subsequent segmented frames. The idea is to improve the action-motion dynamics by focusing on the region which is important for action recognition and encoding the temporal variances using the frame ranking method. We also propose the sequential combination of Inception-ResNetv2 and long-short-term memory network (LSTM) to leverage the temporal variances for improved recognition performance. Extensive analysis has been carried out on UCF101 and HMDB51 datasets which are widely used in action recognition studies. We show that (i) the semantic image generates better activations and converges faster than its original variant, (ii) using segmentation prior to approximate rank pooling yields better recognition performance, (iii) The use of LSTM leverages the temporal variance information from approximate rank pooling to model the action behavior better than the base network, (iv) the proposed representations can be adaptive as they can be used with existing methods such as temporal segment networks to improve the recognition performance, and (v) our proposed four-stream network architecture comprising of semantic images and semantic optical flows achieves state-of-the-art performance, 95.9% and 73.5% recognition accuracy on UCF101 and HMDB51, respectively.",0
"This paper suggests utilizing a semantic image for video analysis, particularly in conjunction with Inception networks. The semantic image is created through localized sparse segmentation using global clustering (LSSGC) before approximate rank pooling, which summarizes motion characteristics in one or more images. By overlaying a static background from the window onto subsequent segmented frames, the background information is incorporated. The objective is to improve action-motion dynamics by focusing on the crucial action recognition region and encoding temporal variances via frame ranking. Additionally, we propose combining Inception-ResNetv2 with a long-short-term memory network (LSTM) sequentially to leverage temporal variances for superior recognition performance. Extensive testing on the widely used UCF101 and HMDB51 datasets demonstrates that the semantic image generates better activations and converges faster than the original variant, segmentation before approximate rank pooling leads to better recognition performance, LSTM improves action behavior modeling compared to the base network, the proposed representations can be adaptive with existing methods to enhance recognition performance, and our four-stream network architecture featuring semantic images and semantic optical flows attains state-of-the-art performance, with recognition accuracy of 95.9% and 73.5% on UCF101 and HMDB51, respectively.",1
"Visual SLAM shows significant progress in recent years due to high attention from vision community but still, challenges remain for low-textured environments. Feature based visual SLAMs do not produce reliable camera and structure estimates due to insufficient features in a low-textured environment. Moreover, existing visual SLAMs produce partial reconstruction when the number of 3D-2D correspondences is insufficient for incremental camera estimation using bundle adjustment. This paper presents Edge SLAM, a feature based monocular visual SLAM which mitigates the above mentioned problems. Our proposed Edge SLAM pipeline detects edge points from images and tracks those using optical flow for point correspondence. We further refine these point correspondences using geometrical relationship among three views. Owing to our edge-point tracking, we use a robust method for two-view initialization for bundle adjustment. Our proposed SLAM also identifies the potential situations where estimating a new camera into the existing reconstruction is becoming unreliable and we adopt a novel method to estimate the new camera reliably using a local optimization technique. We present an extensive evaluation of our proposed SLAM pipeline with most popular open datasets and compare with the state-of-the art. Experimental result indicates that our Edge SLAM is robust and works reliably well for both textured and less-textured environment in comparison to existing state-of-the-art SLAMs.",0
"Although visual SLAM has made significant progress in recent years and garnered high attention from the vision community, challenges persist in low-textured environments. This is due to the unreliable camera and structure estimates produced by feature-based visual SLAMs, which struggle with insufficient features. Additionally, existing visual SLAMs often only produce partial reconstructions when there are not enough 3D-2D correspondences for incremental camera estimation using bundle adjustment. To address these issues, we propose Edge SLAM, a feature-based monocular visual SLAM that detects edge points from images and tracks them using optical flow for point correspondence. Our pipeline refines these point correspondences using the geometrical relationship among three views and uses a robust method for two-view initialization for bundle adjustment. Our SLAM also identifies potential situations where estimating a new camera into the existing reconstruction is becoming unreliable and adopts a novel method to estimate the new camera reliably using a local optimization technique. We extensively evaluate our proposed SLAM pipeline with popular open datasets and compare it with the state-of-the-art, showing that our Edge SLAM is robust and works reliably well for both textured and less-textured environments.",1
"Convolutional Neural Networks (CNN) are successfully used for various visual perception tasks including bounding box object detection, semantic segmentation, optical flow, depth estimation and visual SLAM. Generally these tasks are independently explored and modeled. In this paper, we present a joint multi-task network design for learning object detection and semantic segmentation simultaneously. The main motivation is to achieve real-time performance on a low power embedded SOC by sharing of encoder for both the tasks. We construct an efficient architecture using a small ResNet10 like encoder which is shared for both decoders. Object detection uses YOLO v2 like decoder and semantic segmentation uses FCN8 like decoder. We evaluate the proposed network in two public datasets (KITTI, Cityscapes) and in our private fisheye camera dataset, and demonstrate that joint network provides the same accuracy as that of separate networks. We further optimize the network to achieve 30 fps for 1280x384 resolution image.",0
"Convolutional Neural Networks (CNN) have proven to be effective in various visual perception tasks, such as bounding box object detection, semantic segmentation, optical flow, depth estimation, and visual SLAM. Typically, these tasks are studied and modeled independently. In this study, we introduce a multi-task network design that simultaneously learns object detection and semantic segmentation. The primary objective is to achieve real-time performance on a low power embedded SOC by using a shared encoder for both tasks. We created an efficient architecture using a small ResNet10-like encoder and a YOLOv2-like decoder for object detection and an FCN8-like decoder for semantic segmentation. We evaluated the proposed network on two publicly available datasets (KITTI, Cityscapes) and our private fisheye camera dataset. Our results demonstrate that the joint network provides similar accuracy to separate networks. Additionally, we optimized the network to achieve a frame rate of 30 fps for a 1280x384 resolution image.",1
"Motion is a dominant cue in automated driving systems. Optical flow is typically computed to detect moving objects and to estimate depth using triangulation. In this paper, our motivation is to leverage the existing dense optical flow to improve the performance of semantic segmentation. To provide a systematic study, we construct four different architectures which use RGB only, flow only, RGBF concatenated and two-stream RGB + flow. We evaluate these networks on two automotive datasets namely Virtual KITTI and Cityscapes using the state-of-the-art flow estimator FlowNet v2. We also make use of the ground truth optical flow in Virtual KITTI to serve as an ideal estimator and a standard Farneback optical flow algorithm to study the effect of noise. Using the flow ground truth in Virtual KITTI, two-stream architecture achieves the best results with an improvement of 4% IoU. As expected, there is a large improvement for moving objects like trucks, vans and cars with 38%, 28% and 6% increase in IoU. FlowNet produces an improvement of 2.4% in average IoU with larger improvement in the moving objects corresponding to 26%, 11% and 5% in trucks, vans and cars. In Cityscapes, flow augmentation provided an improvement for moving objects like motorcycle and train with an increase of 17% and 7% in IoU.",0
"Automated driving systems rely heavily on motion as a key indicator. Optical flow is used to detect moving objects and estimate depth through triangulation. The purpose of this study is to enhance semantic segmentation by utilizing the dense optical flow already in place. Four different architectures were constructed, including RGB only, flow only, RGBF concatenated, and two-stream RGB + flow. These networks were tested on two automotive datasets, Virtual KITTI and Cityscapes, using FlowNet v2 as the flow estimator. Ground truth optical flow in Virtual KITTI was used as an ideal estimator, and the Farneback optical flow algorithm was used to examine the impact of noise. The two-stream architecture achieved the best results with a 4% IoU improvement when using flow ground truth in Virtual KITTI. Moving objects, such as trucks, vans, and cars, saw the most significant improvement in IoU with an increase of 38%, 28%, and 6%, respectively. FlowNet produced an average IoU improvement of 2.4%, with a larger improvement for moving objects, corresponding to 26%, 11%, and 5% in trucks, vans, and cars. In Cityscapes, flow augmentation provided an improvement in IoU for moving objects, such as motorcycles and trains, with an increase of 17% and 7%, respectively.",1
"We propose a new self-supervised approach to image feature learning from motion cue. This new approach leverages recent advances in deep learning in two directions: 1) the success of training deep neural network in estimating optical flow in real data using synthetic flow data; and 2) emerging work in learning image features from motion cues, such as optical flow. Building on these, we demonstrate that image features can be learned in self-supervision by first training an optical flow estimator with synthetic flow data, and then learning image features from the estimated flows in real motion data. We demonstrate and evaluate this approach on an image segmentation task. Using the learned image feature representation, the network performs significantly better than the ones trained from scratch in few-shot segmentation tasks.",0
"Our proposal introduces a novel method for self-supervised image feature learning through motion cue. This approach capitalizes on recent advancements in deep learning in two key areas: 1) the ability to effectively train deep neural networks to estimate optical flow in real-world scenarios using synthetic flow data; and 2) the emergence of research focused on learning image features from motion cues, such as optical flow. By combining these advancements, we showcase that self-supervised image features can be learned by initially training an optical flow estimator with synthetic flow data and subsequently extracting image features from the estimated flows in actual motion data. We evaluate this methodology on an image segmentation task and demonstrate that the network performs significantly better than those trained from scratch in few-shot segmentation tasks, utilizing the learned image feature representation.",1
"We present a system for learning motion of independently moving objects from stereo videos. The only human annotation used in our system are 2D object bounding boxes which introduce the notion of objects to our system. Unlike prior learning based work which has focused on predicting dense pixel-wise optical flow field and/or a depth map for each image, we propose to predict object instance specific 3D scene flow maps and instance masks from which we are able to derive the motion direction and speed for each object instance. Our network takes the 3D geometry of the problem into account which allows it to correlate the input images. We present experiments evaluating the accuracy of our 3D flow vectors, as well as depth maps and projected 2D optical flow where our jointly learned system outperforms earlier approaches trained for each task independently.",0
"Our system utilizes stereo videos to learn the motion of independently moving objects, with the only human annotation being 2D object bounding boxes which introduce object recognition. Rather than focusing on predicting pixel-wise optical flow fields and depth maps, we propose predicting 3D scene flow maps and instance masks specific to each object instance, which provides the direction and speed of motion for each object instance. Our network considers the 3D geometry of the problem, enabling correlation of input images. We conducted experiments to evaluate the accuracy of our 3D flow vectors, depth maps, and projected 2D optical flow, which outperformed earlier approaches trained for each task independently.",1
"Advanced video classification systems decode video frames to derive the necessary texture and motion representations for ingestion and analysis by spatio-temporal deep convolutional neural networks (CNNs). However, when considering visual Internet-of-Things applications, surveillance systems and semantic crawlers of large video repositories, the video capture and the CNN-based semantic analysis parts do not tend to be co-located. This necessitates the transport of compressed video over networks and incurs significant overhead in bandwidth and energy consumption, thereby significantly undermining the deployment potential of such systems. In this paper, we investigate the trade-off between the encoding bitrate and the achievable accuracy of CNN-based video classification models that directly ingest AVC/H.264 and HEVC encoded videos. Instead of retaining entire compressed video bitstreams and applying complex optical flow calculations prior to CNN processing, we only retain motion vector and select texture information at significantly-reduced bitrates and apply no additional processing prior to CNN ingestion. Based on three CNN architectures and two action recognition datasets, we achieve 11%-94% saving in bitrate with marginal effect on classification accuracy. A model-based selection between multiple CNNs increases these savings further, to the point where, if up to 7% loss of accuracy can be tolerated, video classification can take place with as little as 3 kbps for the transport of the required compressed video information to the system implementing the CNN models.",0
"Video classification systems that are advanced use decoding of video frames to create the appropriate texture and motion representations. This is then analyzed by spatio-temporal deep convolutional neural networks (CNNs). However, when considering visual IoT applications, surveillance systems, and semantic crawlers for large video repositories, the video capture and semantic analysis are typically not in the same location. This requires compressed video to be transported over networks, leading to high bandwidth and energy consumption, which hinders the deployment potential of these systems. This paper examines the balance between encoding bitrate and CNN-based video classification models' achievable accuracy, using AVC/H.264 and HEVC encoded videos. We only retain motion vector and select texture information at significantly-reduced bitrates, with no additional processing prior to CNN ingestion. By using three CNN architectures and two action recognition datasets, we achieve 11%-94% savings in bitrate with minimal effect on classification accuracy. By choosing between multiple CNNs, the savings can be increased further, allowing video classification to occur with as little as 3 kbps for transporting the necessary compressed video information to the CNN model implementation, provided that a loss of accuracy of up to 7% is acceptable.",1
"Dynamic imaging is a recently proposed action description paradigm for simultaneously capturing motion and temporal evolution information, particularly in the context of deep convolutional neural networks (CNNs). Compared with optical flow for motion characterization, dynamic imaging exhibits superior efficiency and compactness. Inspired by the success of dynamic imaging in RGB video, this study extends it to the depth domain. To better exploit three-dimensional (3D) characteristics, multi-view dynamic images are proposed. In particular, the raw depth video is densely projected with respect to different virtual imaging viewpoints by rotating the virtual camera within the 3D space. Subsequently, dynamic images are extracted from the obtained multi-view depth videos and multi-view dynamic images are thus constructed from these images. Accordingly, more view-tolerant visual cues can be involved. A novel CNN model is then proposed to perform feature learning on multi-view dynamic images. Particularly, the dynamic images from different views share the same convolutional layers but correspond to different fully connected layers. This is aimed at enhancing the tuning effectiveness on shallow convolutional layers by alleviating the gradient vanishing problem. Moreover, as the spatial occurrence variation of the actions may impair the CNN, an action proposal approach is also put forth. In experiments, the proposed approach can achieve state-of-the-art performance on three challenging datasets.",0
"A new approach to describing motion and temporal evolution, called dynamic imaging, has been proposed for deep convolutional neural networks (CNNs). Compared to optical flow, dynamic imaging is more efficient and compact. This study extends the use of dynamic imaging to the depth domain by proposing multi-view dynamic images which capture 3D characteristics by rotating a virtual camera within the 3D space. A novel CNN model is then proposed to perform feature learning on multi-view dynamic images, using shared convolutional layers and different fully connected layers. To address the spatial occurrence variation of the actions, an action proposal approach is also suggested. The proposed approach shows superior performance on three challenging datasets.",1
"Recognizing actions in ice hockey using computer vision poses challenges due to bulky equipment and inadequate image quality. A novel two-stream framework has been designed to improve action recognition accuracy for hockey using three main components. First, pose is estimated via the Part Affinity Fields model to extract meaningful cues from the player. Second, optical flow (using LiteFlowNet) is used to extract temporal features. Third, pose and optical flow streams are fused and passed to fully-connected layers to estimate the hockey player's action. A novel publicly available dataset named HARPET (Hockey Action Recognition Pose Estimation, Temporal) was created, composed of sequences of annotated actions and pose of hockey players including their hockey sticks as an extension of human body pose. Three contributions are recognized. (1) The novel two-stream architecture achieves 85% action recognition accuracy, with the inclusion of optical flows increasing accuracy by about 10%. (2) The unique localization of hand-held objects (e.g., hockey sticks) as part of pose increases accuracy by about 13%. (3) For pose estimation, a bigger and more general dataset, MSCOCO, is successfully used for transfer learning to a smaller and more specific dataset, HARPET, achieving a PCKh of 87%.",0
"Difficulties arise when attempting to recognize actions in ice hockey using computer vision due to the cumbersome equipment worn by players and substandard image quality. In order to enhance the accuracy of action recognition for hockey, a new two-stream framework has been developed, consisting of three primary components. Firstly, meaningful cues are extracted from the player by estimating their pose through the Part Affinity Fields model. Secondly, temporal features are extracted through the utilization of optical flow, utilizing LiteFlowNet. Thirdly, the pose and optical flow streams are merged and transmitted to fully-connected layers to evaluate the hockey player's action. A new publicly available dataset named HARPET (Hockey Action Recognition Pose Estimation, Temporal) has been created, comprising sequences of annotated actions and the pose of hockey players, including their hockey sticks as an extension of the human body pose. There are three identified contributions. Firstly, the novel two-stream architecture obtains an 85% action recognition accuracy, with the addition of optical flows increasing accuracy by approximately 10%. Secondly, accuracy increases by around 13% by localizing hand-held objects such as hockey sticks as part of the pose. Finally, for pose estimation, a larger and more general dataset, MSCOCO, is employed for transfer learning to a smaller and more specific dataset, HARPET, resulting in a PCKh of 87%.",1
"Three-dimensional (3D) biomedical image sets are often acquired with in-plane pixel spacings that are far less than the out-of-plane spacings between images. The resultant anisotropy, which can be detrimental in many applications, can be decreased using image interpolation. Optical flow and/or other registration-based interpolators have proven useful in such interpolation roles in the past. When acquired images are comprised of signals that describe the flow velocity of fluids, additional information is available to guide the interpolation process. In this paper, we present an optical-flow based framework for image interpolation that also minimizes resultant divergence in the interpolated data.",0
"Biomedical image sets in 3D are frequently obtained with in-plane pixel spacings that differ significantly from the out-of-plane spacings between images. This creates an anisotropic effect that can be disadvantageous in numerous applications. Image interpolation can help reduce this effect. In previous instances, optical flow and/or other registration-based interpolators have proved beneficial in this regard. When images obtained consist of signals that describe fluid flow velocity, extra information can be utilized to direct the interpolation process. Our research presents an optical-flow based approach for image interpolation that also lessens the divergence in the interpolated data.",1
"Optical flow estimation can be formulated as an end-to-end supervised learning problem, which yields estimates with a superior accuracy-runtime tradeoff compared to alternative methodology. In this paper, we make such networks estimate their local uncertainty about the correctness of their prediction, which is vital information when building decisions on top of the estimations. For the first time we compare several strategies and techniques to estimate uncertainty in a large-scale computer vision task like optical flow estimation. Moreover, we introduce a new network architecture utilizing the Winner-Takes-All loss and show that this can provide complementary hypotheses and uncertainty estimates efficiently with a single forward pass and without the need for sampling or ensembles. Finally, we demonstrate the quality of the different uncertainty estimates, which is clearly above previous confidence measures on optical flow and allows for interactive frame rates.",0
"The accuracy-runtime tradeoff of alternative methods can be surpassed by end-to-end supervised learning for optical flow estimation. This paper focuses on networks that estimate local uncertainty for their predictions, which is crucial for decision-making. The study compares several techniques for estimating uncertainty in optical flow estimation and introduces a new network architecture using the Winner-Takes-All loss. This architecture efficiently provides complementary hypotheses and uncertainty estimates in a single forward pass without the need for sampling or ensembles. The results demonstrate that the quality of uncertainty estimates is superior to previous confidence measures and allows for interactive frame rates.",1
"In this work, we propose a novel framework for unsupervised learning for event cameras that learns motion information from only the event stream. In particular, we propose an input representation of the events in the form of a discretized volume that maintains the temporal distribution of the events, which we pass through a neural network to predict the motion of the events. This motion is used to attempt to remove any motion blur in the event image. We then propose a loss function applied to the motion compensated event image that measures the motion blur in this image. We train two networks with this framework, one to predict optical flow, and one to predict egomotion and depths, and evaluate these networks on the Multi Vehicle Stereo Event Camera dataset, along with qualitative results from a variety of different scenes.",0
"Our work presents a new framework for unsupervised learning for event cameras. Our approach focuses on learning motion information solely from the event stream. To achieve this, we suggest an input representation of the events that maintains their temporal distribution in the form of a discretized volume. We use a neural network to predict the motion of the events and utilize this to remove any motion blur in the event image. We then apply a loss function to the motion compensated event image to measure the motion blur. We have trained two networks using this framework, one for predicting optical flow and the other for predicting egomotion and depths. We have evaluated these networks on the Multi Vehicle Stereo Event Camera dataset and have provided qualitative results from various scenes.",1
"Image warping is a necessary step in many multimedia applications such as texture mapping, image-based rendering, panorama stitching, image resizing and optical flow computation etc. Traditionally, color image warping interpolation is performed in each color channel independently. In this paper, we show that the warping quality can be significantly enhanced by exploiting the cross-channel correlation. We design a warping scheme that integrates intra-channel interpolation with cross-channel variation at very low computational cost, which is required for interactive multimedia applications on mobile devices. The effectiveness and efficiency of our method are validated by extensive experiments.",0
"Many multimedia applications require image warping for tasks like image resizing, panorama stitching, and optical flow computation. Typically, color image warping interpolation is performed independently per color channel. However, this paper presents a new approach that leverages cross-channel correlation to improve warping quality. Our warping scheme combines intra-channel interpolation and cross-channel variation, resulting in significant enhancements without requiring high computational costs. This makes our method suitable for interactive multimedia applications on mobile devices. Our approach has been extensively tested and validated for both effectiveness and efficiency.",1
"A safe and robust on-road navigation system is a crucial component of achieving fully automated vehicles. NVIDIA recently proposed an End-to-End algorithm that can directly learn steering commands from raw pixels of a front camera by using one convolutional neural network. In this paper, we leverage auxiliary information aside from raw images and design a novel network structure, called Auxiliary Task Network (ATN), to help boost the driving performance while maintaining the advantage of minimal training data and an End-to-End training method. In this network, we introduce human prior knowledge into vehicle navigation by transferring features from image recognition tasks. Image semantic segmentation is applied as an auxiliary task for navigation. We consider temporal information by introducing an LSTM module and optical flow to the network. Finally, we combine vehicle kinematics with a sensor fusion step. We discuss the benefits of our method over state-of-the-art visual navigation methods both in the Udacity simulation environment and on the real-world Comma.ai dataset.",0
"To achieve complete automation of vehicles, a reliable and secure on-road navigation system is essential. NVIDIA has recently proposed an End-to-End algorithm that allows direct learning of steering commands from raw camera pixels using a single convolutional neural network. However, we have developed a new network structure, called Auxiliary Task Network (ATN), that incorporates additional information besides raw images to enhance driving performance while retaining minimal training data and an End-to-End training approach. Our network integrates human prior knowledge into vehicle navigation by transferring features from image recognition tasks and utilizing image semantic segmentation as an auxiliary task for navigation. We also introduce temporal information through an LSTM module and optical flow integration, and then combine vehicle kinematics with a sensor fusion step. Our method outperforms other visual navigation techniques in both the Udacity simulation environment and the real-world Comma.ai dataset.",1
"We propose a methodology to extend the concept of Two-Stream Convolutional Networks to perform end-to-end learning for self-driving cars with temporal cues. The system has the ability to learn spatiotemporal features by simultaneously mapping raw images and pre-calculated optical flows directly to steering commands. Although optical flows encode temporal-rich information, we found that 2D-CNNs are prone to capturing features only as spatial representations. We show how the use of Multitask Learning favors the learning of temporal features via inductive transfer from a shared spatiotemporal representation. Preliminary results demonstrate a competitive improvement of 30% in prediction accuracy and stability compared to widely used regression methods trained on the Comma.ai dataset.",0
"Our proposed methodology aims to expand the Two-Stream Convolutional Networks' concept and enable end-to-end learning for self-driving cars utilizing temporal cues. The system can learn spatiotemporal features by simultaneously applying raw images and pre-calculated optical flows directly to steering commands. However, we discovered that 2D-CNNs tend to capture features solely as spatial representations and are not efficient in encoding temporal-rich information. To overcome this limitation, we incorporated Multitask Learning to enhance the learning of temporal features through inductive transfer from a shared spatiotemporal representation. Our preliminary results demonstrate a significant improvement of 30% in prediction accuracy and stability compared to commonly used regression methods trained on the Comma.ai dataset.",1
"Anomaly detection is a challenging problem in intelligent video surveillance. Most existing methods are computation consuming, which cannot satisfy the real-time requirement. In this paper, we propose a real-time anomaly detection framework with low computational complexity and high efficiency. A new feature, named Histogram of Magnitude Optical Flow (HMOF), is proposed to capture the motion of video patches. Compared with existing feature descriptors, HMOF is more sensitive to motion magnitude and more efficient to distinguish anomaly information. The HMOF features are computed for foreground patches, and are reconstructed by the auto-encoder for better clustering. Then, we use Gaussian Mixture Model (GMM) Classifiers to distinguish anomalies from normal activities in videos. Experimental results show that our framework outperforms state-of-the-art methods, and can reliably detect anomalies in real-time.",0
"The task of detecting anomalies in intelligent video surveillance is difficult, and existing methods are often too computationally intensive to meet the real-time requirements. This paper proposes a new framework for real-time anomaly detection that is both efficient and low in computational complexity. The framework introduces a novel feature descriptor, Histogram of Magnitude Optical Flow (HMOF), which is more sensitive to motion magnitude and better suited to identifying anomalies. HMOF features are computed for foreground patches, reconstructed with an auto-encoder, and then classified using Gaussian Mixture Model (GMM) classifiers. Experimental results demonstrate that our framework surpasses current state-of-the-art methods and can reliably detect anomalies in real-time.",1
"Video super-resolution (VSR) aims to restore a photo-realistic high-resolution (HR) video frame from both its corresponding low-resolution (LR) frame (reference frame) and multiple neighboring frames (supporting frames). Due to varying motion of cameras or objects, the reference frame and each support frame are not aligned. Therefore, temporal alignment is a challenging yet important problem for VSR. Previous VSR methods usually utilize optical flow between the reference frame and each supporting frame to wrap the supporting frame for temporal alignment. Therefore, the performance of these image-level wrapping-based models will highly depend on the prediction accuracy of optical flow, and inaccurate optical flow will lead to artifacts in the wrapped supporting frames, which also will be propagated into the reconstructed HR video frame. To overcome the limitation, in this paper, we propose a temporal deformable alignment network (TDAN) to adaptively align the reference frame and each supporting frame at the feature level without computing optical flow. The TDAN uses features from both the reference frame and each supporting frame to dynamically predict offsets of sampling convolution kernels. By using the corresponding kernels, TDAN transforms supporting frames to align with the reference frame. To predict the HR video frame, a reconstruction network taking aligned frames and the reference frame is utilized. Experimental results demonstrate the effectiveness of the proposed TDAN-based VSR model.",0
"The goal of Video super-resolution (VSR) is to enhance the quality of low-resolution (LR) video frames by generating high-resolution (HR) frames using both the reference frame and neighboring supporting frames. However, achieving temporal alignment between the reference frame and each supporting frame is not an easy task due to differences in camera or object motion. Previous VSR methods use optical flow to wrap the supporting frames for temporal alignment, but this approach relies heavily on the accuracy of the prediction of optical flow, which can lead to visual artifacts in the reconstructed HR video frame. To overcome this limitation, this study proposes a Temporal Deformable Alignment Network (TDAN) that aligns the reference and supporting frames at the feature level without using optical flow. The TDAN predicts offsets of sampling convolution kernels based on features from both the reference and supporting frames, which are then used to transform the supporting frames to align with the reference frame. The aligned frames are then used in a reconstruction network to predict the HR video frame. The proposed TDAN-based VSR model is effective, as demonstrated by experimental results.",1
"The field of automatic video generation has received a boost thanks to the recent Generative Adversarial Networks (GANs). However, most existing methods cannot control the contents of the generated video using a text caption, losing their usefulness to a large extent. This particularly affects human videos due to their great variety of actions and appearances. This paper presents Conditional Flow and Texture GAN (CFT-GAN), a GAN-based video generation method from action-appearance captions. We propose a novel way of generating video by encoding a caption (e.g., ""a man in blue jeans is playing golf"") in a two-stage generation pipeline. Our CFT-GAN uses such caption to generate an optical flow (action) and a texture (appearance) for each frame. As a result, the output video reflects the content specified in the caption in a plausible way. Moreover, to train our method, we constructed a new dataset for human video generation with captions. We evaluated the proposed method qualitatively and quantitatively via an ablation study and a user study. The results demonstrate that CFT-GAN is able to successfully generate videos containing the action and appearances indicated in the captions.",0
"Recently, Generative Adversarial Networks (GANs) have given a boost to the field of automatic video generation. However, the majority of existing methods are unable to accurately control the content of the generated video using text captions, which greatly reduces their usefulness, especially for human videos with diverse actions and appearances. In response, this study introduces a GAN-based video generation method called Conditional Flow and Texture GAN (CFT-GAN) that utilizes action-appearance captions. The CFT-GAN generates video by encoding a caption (e.g., ""a man in blue jeans is playing golf"") in a two-stage generation process that produces an optical flow (action) and a texture (appearance) for each frame. As a result, the output video reflects the content specified in the caption in a plausible manner. To train the CFT-GAN method, a new dataset for human video generation with captions was constructed. Qualitative and quantitative evaluation of the proposed method was conducted through an ablation study and a user study, both of which demonstrated CFT-GAN's capability to effectively generate videos containing the actions and appearances indicated in the captions.",1
"Abnormal driving behaviour is one of the leading cause of terrible traffic accidents endangering human life. Therefore, study on driving behaviour surveillance has become essential to traffic security and public management. In this paper, we conduct this promising research and employ a two stream CNN framework for video-based driving behaviour recognition, in which spatial stream CNN captures appearance information from still frames, whilst temporal stream CNN captures motion information with pre-computed optical flow displacement between a few adjacent video frames. We investigate different spatial-temporal fusion strategies to combine the intra frame static clues and inter frame dynamic clues for final behaviour recognition. So as to validate the effectiveness of the designed spatial-temporal deep learning based model, we create a simulated driving behaviour dataset, containing 1237 videos with 6 different driving behavior for recognition. Experiment result shows that our proposed method obtains noticeable performance improvements compared to the existing methods.",0
"The prevalence of abnormal driving behavior is a leading cause of traffic accidents that put human lives at risk. Thus, research on monitoring driving behavior has become crucial for traffic safety and public administration. This study presents a promising investigation into driving behavior recognition using a two-stream CNN framework for video analysis. The spatial stream CNN captures appearance information from static frames, while the temporal stream CNN captures motion information from pre-computed optical flow displacement between adjacent video frames. Various spatial-temporal fusion strategies are examined to combine the static and dynamic clues for accurate behavior recognition. To validate the effectiveness of our deep learning-based model, we create a simulated driving behavior dataset with 1237 videos and six different driving behaviors. Experimental results indicate that our proposed method outperforms existing methods in terms of performance.",1
"First-person (egocentric) and third person (exocentric) videos are drastically different in nature. The relationship between these two views have been studied in recent years, however, it has yet to be fully explored. In this work, we introduce two datasets (synthetic and natural/real) containing simultaneously recorded egocentric and exocentric videos. We also explore relating the two domains (egocentric and exocentric) in two aspects. First, we synthesize images in the egocentric domain from the exocentric domain using a conditional generative adversarial network (cGAN). We show that with enough training data, our network is capable of hallucinating how the world would look like from an egocentric perspective, given an exocentric video. Second, we address the cross-view retrieval problem across the two views. Given an egocentric query frame (or its momentary optical flow), we retrieve its corresponding exocentric frame (or optical flow) from a gallery set. We show that using synthetic data could be beneficial in retrieving real data. We show that performing domain adaptation from the synthetic domain to the natural/real domain, is helpful in tasks such as retrieval. We believe that the presented datasets and the proposed baselines offer new opportunities for further research in this direction. The code and dataset are publicly available.",0
"The dissimilarities between first-person (egocentric) and third person (exocentric) videos are significant, and although researchers have studied the relationship between these two perspectives, there is still much to be explored. This study presents two datasets - one synthetic and the other natural/real - that contain both egocentric and exocentric videos. The study focuses on two aspects of relating these two perspectives: firstly, synthesizing egocentric images from exocentric videos using a conditional generative adversarial network (cGAN); this can be achieved with sufficient training data. Secondly, addressing the cross-view retrieval problem by retrieving corresponding exocentric frames from an egocentric query frame or its optical flow in a gallery set. The study shows that synthetic data can be useful in retrieving real data, and domain adaptation from synthetic to natural/real domains is useful in tasks such as retrieval. The presented datasets and baselines offer new avenues for further research and are publicly available, along with the code.",1
"To date, top-performing optical flow estimation methods only take pairs of consecutive frames into account. While elegant and appealing, the idea of using more than two frames has not yet produced state-of-the-art results. We present a simple, yet effective fusion approach for multi-frame optical flow that benefits from longer-term temporal cues. Our method first warps the optical flow from previous frames to the current, thereby yielding multiple plausible estimates. It then fuses the complementary information carried by these estimates into a new optical flow field. At the time of writing, our method ranks first among published results in the MPI Sintel and KITTI 2015 benchmarks. Our models will be available on https://github.com/NVlabs/PWC-Net.",0
"Up until now, the most successful optical flow estimation methods have only utilized a pair of consecutive frames. Although the concept of incorporating more than two frames is intriguing, it has not yet produced superior outcomes. Our approach for multi-frame optical flow is uncomplicated, yet effective, as it capitalizes on longer-term temporal cues. Initially, our method shifts the optical flow from earlier frames to the present, generating various probable estimates. It then merges the supplementary information conveyed by these estimates to create a new optical flow field. Currently, our method is ranked first among published outcomes in the MPI Sintel and KITTI 2015 benchmarks. Our models will be made available at https://github.com/NVlabs/PWC-Net.",1
"The success of deep learning research has catapulted deep models into production systems that our society is becoming increasingly dependent on, especially in the image and video domains. However, recent work has shown that these largely uninterpretable models exhibit glaring security vulnerabilities in the presence of an adversary. In this work, we develop a powerful untargeted adversarial attack for action recognition systems in both white-box and black-box settings. Action recognition models differ from image-classification models in that their inputs contain a temporal dimension, which we explicitly target in the attack. Drawing inspiration from image classifier attacks, we create new attacks which achieve state-of-the-art success rates on a two-stream classifier trained on the UCF-101 dataset. We find that our attacks can significantly degrade a model's performance with sparsely and imperceptibly perturbed examples. We also demonstrate the transferability of our attacks to black-box action recognition systems.",0
"The success of deep learning research has led to the widespread use of deep models in production systems, particularly in the image and video fields. However, recent research has revealed that these models, which are often difficult to interpret, are highly vulnerable to security breaches when faced with an adversary. In this study, we develop a potent untargeted adversarial attack that can be used on both white-box and black-box action recognition systems. Unlike image-classification models, action recognition models have a temporal dimension in their inputs, which we specifically target in our attack. Drawing inspiration from attacks on image classifiers, we create new attacks that achieve exceptional success rates on a two-stream classifier trained on the UCF-101 dataset. Our attacks can significantly impair a model's performance with only sparsely and imperceptibly perturbed examples. Furthermore, we demonstrate the transferability of our attacks to black-box action recognition systems.",1
"Convolutional networks optimized for accuracy on challenging, dense prediction tasks are prohibitively slow to run on each frame in a video. The spatial similarity of nearby video frames, however, suggests opportunity to reuse computation. Existing work has explored basic feature reuse and feature warping based on optical flow, but has encountered limits to the speedup attainable with these techniques. In this paper, we present a new, two part approach to accelerating inference on video. First, we propose a fast feature propagation technique that utilizes the block motion vectors present in compressed video (e.g. H.264 codecs) to cheaply propagate features from frame to frame. Second, we develop a novel feature estimation scheme, termed feature interpolation, that fuses features propagated from enclosing keyframes to render accurate feature estimates, even at sparse keyframe frequencies. We evaluate our system on the Cityscapes and CamVid datasets, comparing to both a frame-by-frame baseline and related work. We find that we are able to substantially accelerate segmentation on video, achieving near real-time frame rates (20.1 frames per second) on large images (960 x 720 pixels), while maintaining competitive accuracy. This represents an improvement of almost 6x over the single-frame baseline and 2.5x over the fastest prior work.",0
"Running convolutional networks optimized for accuracy on dense prediction tasks on each frame of a video is too slow. However, because nearby video frames are spatially similar, there is an opportunity to reuse computation. Prior research has explored basic feature reuse and feature warping based on optical flow but has found that these techniques have limitations in terms of speedup. This paper proposes a new approach to accelerate video inference in two parts. First, a fast feature propagation technique is proposed that utilizes block motion vectors in compressed video to propagate features from frame to frame inexpensively. Second, a novel feature estimation scheme called feature interpolation is developed, which fuses features propagated from enclosing keyframes to produce accurate feature estimates, even at sparse keyframe frequencies. The system is evaluated on the Cityscapes and CamVid datasets, and it achieves near real-time frame rates (20.1 frames per second) on large images (960 x 720 pixels) while maintaining competitive accuracy. This represents an improvement of almost 6x over the single-frame baseline and 2.5x over the fastest prior work.",1
"Understanding the world around us and making decisions about the future is a critical component to human intelligence. As autonomous systems continue to develop, their ability to reason about the future will be the key to their success. Semantic anticipation is a relatively under-explored area for which autonomous vehicles could take advantage of (e.g., forecasting pedestrian trajectories). Motivated by the need for real-time prediction in autonomous systems, we propose to decompose the challenging semantic forecasting task into two subtasks: current frame segmentation and future optical flow prediction. Through this decomposition, we built an efficient, effective, low overhead model with three main components: flow prediction network, feature-flow aggregation LSTM, and end-to-end learnable warp layer. Our proposed method achieves state-of-the-art accuracy on short-term and moving objects semantic forecasting while simultaneously reducing model parameters by up to 95% and increasing efficiency by greater than 40x.",0
"Human intelligence relies on comprehending our surroundings and making informed decisions for the future. With the advancement of autonomous systems, their ability to predict future events will be crucial for their success. Autonomous vehicles could benefit from exploring the underdeveloped area of semantic anticipation, particularly in predicting pedestrian movements. To address the real-time prediction needs of autonomous systems, we suggest breaking down the complex task of semantic forecasting into two subtasks: segmenting the current frame and predicting future optical flow. Our model comprises a flow prediction network, a feature-flow aggregation LSTM, and an end-to-end learnable warp layer, which results in a highly efficient and effective model. Our proposed approach sets a new standard of precision for short-term and moving objects semantic forecasting while reducing model parameters by up to 95% and increasing efficiency by over 40 times.",1
"Real-time motion detection in non-stationary scenes is a difficult task due to dynamic background, changing foreground appearance and limited computational resource. These challenges degrade the performance of the existing methods in practical applications. In this paper, an optical flow based framework is proposed to address this problem. By applying a novel strategy to utilize optical flow, we enable our method being free of model constructing, training or updating and can be performed efficiently. Besides, a dual judgment mechanism with adaptive intervals and adaptive thresholds is designed to heighten the system's adaptation to different situations. In experiment part, we quantitatively and qualitatively validate the effectiveness and feasibility of our method with videos in various scene conditions. The experimental results show that our method adapts itself to different situations and outperforms the state-of-the-art real-time methods, indicating the advantages of our optical flow based method.",0
"Real-time motion detection in non-stationary scenes presents a complex challenge due to dynamic background, limited computational resources, and altering foreground appearance. The performance of current methods is hindered by these difficulties in practical applications. To address this issue, this paper proposes an optical flow based framework that overcomes the need for model constructing, training, or updating while remaining highly efficient. Additionally, a dual judgment mechanism with adaptive intervals and thresholds is implemented to improve the system's adaptability to different scenarios. Through quantitative and qualitative experiments, our method is shown to outperform state-of-the-art real-time methods and effectively adapt to various scene conditions, demonstrating the advantages of our optical flow based approach.",1
"Obtained by moving object detection, the foreground mask result is unshaped and can not be directly used in most subsequent processes. In this paper, we focus on this problem and address it by constructing an optical flow based moving foreground analysis framework. During the processing procedure, the foreground masks are analyzed and segmented through two complementary clustering algorithms. As a result, we obtain the instance-level information like the number, location and size of moving objects. The experimental result show that our method adapts itself to the problem and performs well enough for practical applications.",0
"The outcome of object detection through movement is an unstructured foreground mask that is unsuitable for direct use in most further procedures. This issue is the focus of this paper, which proposes a moving foreground analysis framework based on optical flow. The framework involves analyzing and segmenting foreground masks using two complementary clustering algorithms, resulting in instance-level information such as the count, position, and dimensions of moving objects. Experimental results demonstrate that this method is well-suited to the problem and performs satisfactorily for practical purposes.",1
"The performance of optical flow algorithms greatly depends on the specifics of the content and the application for which it is used. Existing and well established optical flow datasets are limited to rather particular contents from which none is close to crowd behavior analysis; whereas such applications heavily utilize optical flow. We introduce a new optical flow dataset exploiting the possibilities of a recent video engine to generate sequences with ground-truth optical flow for large crowds in different scenarios. We break with the development of the last decade of introducing ever increasing displacements to pose new difficulties. Instead we focus on real-world surveillance scenarios where numerous small, partly independent, non rigidly moving objects observed over a long temporal range pose a challenge. By evaluating different optical flow algorithms, we find that results of established datasets can not be transferred to these new challenges. In exhaustive experiments we are able to provide new insight into optical flow for crowd analysis. Finally, the results have been validated on the real-world UCF crowd tracking benchmark while achieving competitive results compared to more sophisticated state-of-the-art crowd tracking approaches.",0
"The effectiveness of optical flow algorithms is heavily influenced by the content and intended use of the application. Optical flow datasets that currently exist are not suitable for analyzing crowd behavior, despite the fact that this is a common use case for optical flow. To address this, we have developed a new optical flow dataset that utilizes a video engine to create ground-truth sequences for large crowds in different scenarios. Rather than increasing the displacements to pose new challenges, we focus on real-world surveillance scenarios where many small, independent, and non-rigid objects are observed over a long period of time. Through extensive experimentation, we have discovered that traditional optical flow algorithms cannot be used for these new challenges. Our results provide valuable insight into optical flow for crowd analysis and have been validated on the UCF crowd tracking benchmark alongside more advanced state-of-the-art approaches.",1
"Modern optical flow methods are often composed of a cascade of many independent steps or formulated as a black box neural network that is hard to interpret and analyze. In this work we seek for a plain, interpretable, but learnable solution. We propose a novel inpainting based algorithm that approaches the problem in three steps: feature selection and matching, selection of supporting points and energy based inpainting. To facilitate the inference we propose an optimization layer that allows to backpropagate through 10K iterations of a first-order method without any numerical or memory problems. Compared to recent state-of-the-art networks, our modular CNN is very lightweight and competitive with other, more involved, inpainting based methods.",0
"Nowadays, optical flow techniques are often composed of multiple independent stages or formulated as a complex neural network that is challenging to comprehend and evaluate. Therefore, we aim to develop a straightforward and comprehensible approach that can be learned. Our proposed solution is a novel algorithm that employs three steps: selecting and matching features, choosing support points, and energy-based inpainting. To facilitate inference, we created an optimization layer that allows backpropagation through 10K iterations of a first-order method without any numerical or memory issues. Our modular CNN is lightweight and competitive with other more complicated inpainting-based methods when compared to recent state-of-the-art networks.",1
"The training of many existing end-to-end steering angle prediction models heavily relies on steering angles as the supervisory signal. Without learning from much richer contexts, these methods are susceptible to the presence of sharp road curves, challenging traffic conditions, strong shadows, and severe lighting changes. In this paper, we considerably improve the accuracy and robustness of predictions through heterogeneous auxiliary networks feature mimicking, a new and effective training method that provides us with much richer contextual signals apart from steering direction. Specifically, we train our steering angle predictive model by distilling multi-layer knowledge from multiple heterogeneous auxiliary networks that perform related but different tasks, e.g., image segmentation or optical flow estimation. As opposed to multi-task learning, our method does not require expensive annotations of related tasks on the target set. This is made possible by applying contemporary off-the-shelf networks on the target set and mimicking their features in different layers after transformation. The auxiliary networks are discarded after training without affecting the runtime efficiency of our model. Our approach achieves a new state-of-the-art on Udacity and Comma.ai, outperforming the previous best by a large margin of 12.8% and 52.1%, respectively. Encouraging results are also shown on Berkeley Deep Drive (BDD) dataset.",0
"Many end-to-end steering angle prediction models rely heavily on steering angles as their supervisory signal during training. However, these models are vulnerable to sharp road curves, challenging traffic conditions, strong shadows, and severe lighting changes because they don't learn from a broader context. In this paper, we introduce a new and effective training method called heterogeneous auxiliary networks feature mimicking to significantly improve the accuracy and robustness of predictions. Our approach involves training a steering angle predictive model by distilling multi-layer knowledge from various heterogeneous auxiliary networks that perform different tasks like image segmentation or optical flow estimation. Unlike multi-task learning, our method doesn't require expensive annotations of related tasks on the target set. Instead, we use off-the-shelf networks on the target set and mimic their features in different layers after transformation. After training, we discard the auxiliary networks without affecting our model's runtime efficiency. Our approach achieves a new state-of-the-art on Udacity and Comma.ai, surpassing the previous best by 12.8% and 52.1%, respectively. Encouraging results are also shown on Berkeley Deep Drive (BDD) dataset.",1
"Video style transfer is a useful component for applications such as augmented reality, non-photorealistic rendering, and interactive games. Many existing methods use optical flow to preserve the temporal smoothness of the synthesized video. However, the estimation of optical flow is sensitive to occlusions and rapid motions. Thus, in this work, we introduce a novel evolve-sync loss computed by evolvements to replace optical flow. Using this evolve-sync loss, we build an adversarial learning framework, termed as Video Style Transfer Generative Adversarial Network (VST-GAN), which improves upon the MGAN method for image style transfer for more efficient video style transfer. We perform extensive experimental evaluations of our method and show quantitative and qualitative improvements over the state-of-the-art methods.",0
"The implementation of video style transfer is beneficial for various applications, including augmented reality, non-photorealistic rendering, and interactive games. While numerous methods utilize optical flow to maintain the smoothness of the video synthesis, optical flow's accuracy can be affected by occlusions and rapid motions. Therefore, this study proposes a novel evolve-sync loss that uses evolvements instead of optical flow. By incorporating this evolve-sync loss, the Video Style Transfer Generative Adversarial Network (VST-GAN) framework is developed to enhance the efficiency of video style transfer beyond the MGAN method for image style transfer. Our experimental evaluations demonstrate significant quantitative and qualitative enhancements compared to the current state-of-the-art methods.",1
"Recovering structure and motion parameters given a image pair or a sequence of images is a well studied problem in computer vision. This is often achieved by employing Structure from Motion (SfM) or Simultaneous Localization and Mapping (SLAM) algorithms based on the real-time requirements. Recently, with the advent of Convolutional Neural Networks (CNNs) researchers have explored the possibility of using machine learning techniques to reconstruct the 3D structure of a scene and jointly predict the camera pose. In this work, we present a framework that achieves state-of-the-art performance on single image depth prediction for both indoor and outdoor scenes. The depth prediction system is then extended to predict optical flow and ultimately the camera pose and trained end-to-end. Our motion estimation framework outperforms the previous motion prediction systems and we also demonstrate that the state-of-the-art metric depths can be further improved using the knowledge of pose.",0
"The task of retrieving structure and motion parameters from an image pair or sequence of images has been extensively researched in the field of computer vision. Traditionally, this is accomplished through the use of real-time Structure from Motion (SfM) or Simultaneous Localization and Mapping (SLAM) algorithms. However, with the emergence of Convolutional Neural Networks (CNNs), researchers have started to explore the possibility of applying machine learning techniques to reconstruct a scene's 3D structure and predict camera pose. Our study introduces a framework that attains exceptional performance in single image depth prediction for both outdoor and indoor scenes. We then expand the depth prediction system to include optical flow and camera pose prediction, which are trained in an end-to-end manner. Our motion estimation framework surpasses prior motion prediction systems, and we also demonstrate that incorporating pose knowledge can further improve the state-of-the-art metric depths.",1
"Two optical flow estimation problems are addressed: i) occlusion estimation and handling, and ii) estimation from image sequences longer than two frames. The proposed ContinualFlow method estimates occlusions before flow, avoiding the use of flow corrupted by occlusions for their estimation. We show that providing occlusion masks as an additional input to flow estimation improves the standard performance metric by more than 25\% on both KITTI and Sintel. As a second contribution, a novel method for incorporating information from past frames into flow estimation is introduced. The previous frame flow serves as an input to occlusion estimation and as a prior in occluded regions, i.e. those without visual correspondences. By continually using the previous frame flow, ContinualFlow performance improves further by 18\% on KITTI and 7\% on Sintel, achieving top performance on KITTI and Sintel.",0
"The article focuses on solving two issues related to optical flow estimation: i) dealing with occlusions, and ii) estimating flow from image sequences comprising more than two frames. The ContinualFlow approach proposed in this study first estimates occlusions and then calculates flow, which prevents the use of flow that may be corrupted by occlusions during estimation. The authors demonstrate that adding occlusion masks as an extra input to flow estimation enhances the standard performance metric by over 25\% for both KITTI and Sintel. Additionally, the research introduces a new strategy for integrating information from past frames into flow estimation. The flow of the previous frame is used as input for occlusion estimation and as a prior in occluded regions, where there are no visual correspondences. ContinualFlow's performance improves by 18\% on KITTI and 7\% on Sintel by continually utilizing the previous frame flow, resulting in top performance on both datasets.",1
We address the problem of motion estimation in images operating in the frequency domain. A method is presented which extends phase correlation to handle multiple motions present in an area. Our scheme is based on a novel Bilateral-Phase Correlation (BLPC) technique that incorporates the concept and principles of Bilateral Filters retaining the motion boundaries by taking into account the difference both in value and distance in a manner very similar to Gaussian convolution. The optical flow is obtained by applying the proposed method at certain locations selected based on the present motion differences and then performing non-uniform interpolation in a multi-scale iterative framework. Experiments with several well-known datasets with and without ground-truth show that our scheme outperforms recently proposed state-of-the-art phase correlation based optical flow methods.,0
"Our focus is on motion estimation in images using frequency domain operations. To address the issue of multiple motions in a given area, we present a new approach that expands phase correlation. Our method, called Bilateral-Phase Correlation (BLPC), is based on the principles of Bilateral Filters. BLPC effectively preserves motion boundaries by considering both value and distance differences, similar to Gaussian convolution. We obtain optical flow by applying BLPC at specific locations based on motion differences and performing non-uniform interpolation within a multi-scale iterative framework. Our experiments, conducted on various datasets with and without ground-truth, demonstrate that our scheme outperforms the latest phase correlation-based optical flow methods.",1
"Analyzing videos of human actions involves understanding the temporal relationships among video frames. State-of-the-art action recognition approaches rely on traditional optical flow estimation methods to pre-compute motion information for CNNs. Such a two-stage approach is computationally expensive, storage demanding, and not end-to-end trainable. In this paper, we present a novel CNN architecture that implicitly captures motion information between adjacent frames. We name our approach hidden two-stream CNNs because it only takes raw video frames as input and directly predicts action classes without explicitly computing optical flow. Our end-to-end approach is 10x faster than its two-stage baseline. Experimental results on four challenging action recognition datasets: UCF101, HMDB51, THUMOS14 and ActivityNet v1.2 show that our approach significantly outperforms the previous best real-time approaches.",0
"The examination of human actions in videos requires an understanding of the chronological connections between video frames. The most advanced approaches for recognizing actions rely on traditional optical flow estimation methods to calculate motion information for CNNs. However, this two-stage method is resource-intensive, requires significant storage, and cannot be trained end-to-end. This study introduces a new CNN architecture that implicitly captures motion information between consecutive frames. Our approach, called hidden two-stream CNNs, predicts action classes directly from raw video frames, without explicitly computing optical flow. Our end-to-end process is ten times faster than the two-stage baseline. Our method significantly outperforms the previous best real-time approaches on four challenging action recognition datasets: UCF101, HMDB51, THUMOS14, and ActivityNet v1.2.",1
"Current state-of-the-art approaches to video understanding adopt temporal jittering to simulate analyzing the video at varying frame rates. However, this does not work well for multirate videos, in which actions or subactions occur at different speeds. The frame sampling rate should vary in accordance with the different motion speeds. In this work, we propose a simple yet effective strategy, termed random temporal skipping, to address this situation. This strategy effectively handles multirate videos by randomizing the sampling rate during training. It is an exhaustive approach, which can potentially cover all motion speed variations. Furthermore, due to the large temporal skipping, our network can see video clips that originally cover over 100 frames. Such a time range is enough to analyze most actions/events. We also introduce an occlusion-aware optical flow learning method that generates improved motion maps for human action recognition. Our framework is end-to-end trainable, runs in real-time, and achieves state-of-the-art performance on six widely adopted video benchmarks.",0
"To simulate analyzing a video at varying frame rates, current state-of-the-art approaches to video understanding use temporal jittering. However, this approach is not effective for multirate videos where actions or subactions occur at different speeds. To handle this situation, we propose a strategy called random temporal skipping which randomizes the sampling rate during training. This approach is exhaustive and can potentially cover all motion speed variations. Additionally, our network can analyze video clips that originally cover over 100 frames due to the large temporal skipping, which is sufficient for most actions/events. We also introduce an occlusion-aware optical flow learning method that improves motion maps for human action recognition. Our framework is end-to-end trainable, runs in real-time, and achieves state-of-the-art performance on six widely adopted video benchmarks.",1
"Modern large displacement optical flow algorithms usually use an initialization by either sparse descriptor matching techniques or dense approximate nearest neighbor fields. While the latter have the advantage of being dense, they have the major disadvantage of being very outlier-prone as they are not designed to find the optical flow, but the visually most similar correspondence. In this article we present a dense correspondence field approach that is much less outlier-prone and thus much better suited for optical flow estimation than approximate nearest neighbor fields. Our approach does not require explicit regularization, smoothing (like median filtering) or a new data term. Instead we solely rely on patch matching techniques and a novel multi-scale matching strategy. We also present enhancements for outlier filtering. We show that our approach is better suited for large displacement optical flow estimation than modern descriptor matching techniques. We do so by initializing EpicFlow with our approach instead of their originally used state-of-the-art descriptor matching technique. We significantly outperform the original EpicFlow on MPI-Sintel, KITTI 2012, KITTI 2015 and Middlebury. In this extended article of our former conference publication we further improve our approach in matching accuracy as well as runtime and present more experiments and insights.",0
"Large displacement optical flow algorithms today typically use either sparse descriptor matching techniques or dense approximate nearest neighbor fields for initialization. Although the latter are dense, they are highly susceptible to outliers since they are not designed to identify optical flow but rather the most visually similar correspondence. In this article, we introduce a dense correspondence field approach that is less prone to outliers and therefore more suitable for optical flow estimation than approximate nearest neighbor fields. Our approach does not require regularization, smoothing, or new data terms. Instead, we rely solely on patch matching techniques and a novel multi-scale matching strategy, along with enhancements for outlier filtering. We demonstrate that our approach is better suited for optical flow estimation than modern descriptor matching techniques by using it to initialize EpicFlow instead of their previous state-of-the-art descriptor matching technique. We significantly outperform the original EpicFlow on MPI-Sintel, KITTI 2012, KITTI 2015, and Middlebury. In this extended article, we further improve our approach in matching accuracy and runtime while presenting more experiments and insights.",1
"Video super-resolution (SR) aims to generate a sequence of high-resolution (HR) frames with plausible and temporally consistent details from their low-resolution (LR) counterparts. The generation of accurate correspondence plays a significant role in video SR. It is demonstrated by traditional video SR methods that simultaneous SR of both images and optical flows can provide accurate correspondences and better SR results. However, LR optical flows are used in existing deep learning based methods for correspondence generation. In this paper, we propose an end-to-end trainable video SR framework to super-resolve both images and optical flows. Specifically, we first propose an optical flow reconstruction network (OFRnet) to infer HR optical flows in a coarse-to-fine manner. Then, motion compensation is performed according to the HR optical flows. Finally, compensated LR inputs are fed to a super-resolution network (SRnet) to generate the SR results. Extensive experiments demonstrate that HR optical flows provide more accurate correspondences than their LR counterparts and improve both accuracy and consistency performance. Comparative results on the Vid4 and DAVIS-10 datasets show that our framework achieves the state-of-the-art performance.",0
"The goal of video super-resolution (SR) is to create a series of high-resolution (HR) frames that have realistic and consistent details based on their low-resolution (LR) versions. Accurate correspondence is crucial in achieving successful video SR. While traditional video SR methods have shown that simultaneous SR of both images and optical flows can lead to precise correspondences and better SR results, existing deep learning techniques rely on LR optical flows for correspondence generation. This paper proposes an end-to-end trainable video SR framework that can super-resolve both images and optical flows. The framework includes an optical flow reconstruction network (OFRnet) that uses a coarse-to-fine approach to infer HR optical flows, motion compensation based on HR optical flows, and a super-resolution network (SRnet) that generates the SR results using compensated LR inputs. Experimental results demonstrate that HR optical flows lead to more accurate correspondences than LR optical flows, and improve both accuracy and consistency performance. Additionally, the proposed framework achieves state-of-the-art performance on the Vid4 and DAVIS-10 datasets.",1
"In this work, we propose a mask propagation network to treat the video segmentation problem as a concept of the guided instance segmentation. Similar to most MaskTrack based video segmentation methods, our method takes the mask probability map of previous frame and the appearance of current frame as inputs, and predicts the mask probability map for the current frame. Specifically, we adopt the Xception backbone based DeepLab v3+ model as the probability map predictor in our prediction pipeline. Besides, instead of the full image and the original mask probability, our network takes the region of interest of the instance, and the new mask probability which warped by the optical flow between the previous and current frames as the inputs. We also ensemble the modified One-Shot Video Segmentation Network to make the final predictions in order to retrieve and segment the missing instance.",0
"Our work presents a mask propagation network that addresses the video segmentation problem by utilizing the guided instance segmentation concept. Our approach is similar to other MaskTrack-based video segmentation methods, where we use the mask probability map of the previous frame and the appearance of the current frame as inputs to predict the mask probability map for the current frame. To achieve this, we leverage the Xception backbone-based DeepLab v3+ model as the probability map predictor in our prediction pipeline. We also modify the inputs of our network by using the region of interest of the instance and the new mask probability that's been warped by the optical flow between the previous and current frames. Furthermore, we enhance our predictions by ensembling the modified One-Shot Video Segmentation Network to retrieve and segment the missing instance.",1
"This paper addresses the challenge of dense pixel correspondence estimation between two images. This problem is closely related to optical flow estimation task where ConvNets (CNNs) have recently achieved significant progress. While optical flow methods produce very accurate results for the small pixel translation and limited appearance variation scenarios, they hardly deal with the strong geometric transformations that we consider in this work. In this paper, we propose a coarse-to-fine CNN-based framework that can leverage the advantages of optical flow approaches and extend them to the case of large transformations providing dense and subpixel accurate estimates. It is trained on synthetic transformations and demonstrates very good performance to unseen, realistic, data. Further, we apply our method to the problem of relative camera pose estimation and demonstrate that the model outperforms existing dense approaches.",0
"The main focus of this article is the difficulty of accurately estimating dense pixel correspondence between two images, which is closely related to the task of estimating optical flow. Although ConvNets have made significant progress in optical flow estimation, they are not well-equipped to handle strong geometric transformations. To address this issue, we propose a CNN-based framework that combines the strengths of optical flow methods and extends them to large transformation scenarios, producing precise and subpixel accurate results. Our model is trained on synthetic data and performs well on real-world data. Additionally, we apply our method to the task of relative camera pose estimation and demonstrate its superiority over existing dense approaches.",1
"Quantitative assessment of left ventricle (LV) function from cine MRI has significant diagnostic and prognostic value for cardiovascular disease patients. The temporal movement of LV provides essential information on the contracting/relaxing pattern of heart, which is keenly evaluated by clinical experts in clinical practice. Inspired by the expert way of viewing Cine MRI, we propose a new CNN module that is able to incorporate the temporal information into LV segmentation from cine MRI. In the proposed CNN, the optical flow (OF) between neighboring frames is integrated and aggregated at feature level, such that temporal coherence in cardiac motion can be taken into account during segmentation. The proposed module is integrated into the U-net architecture without need of additional training. Furthermore, dilated convolution is introduced to improve the spatial accuracy of segmentation. Trained and tested on the Cardiac Atlas database, the proposed network resulted in a Dice index of 95% and an average perpendicular distance of 0.9 pixels for the middle LV contour, significantly outperforming the original U-net that processes each frame individually. Notably, the proposed method improved the temporal coherence of LV segmentation results, especially at the LV apex and base where the cardiac motion is difficult to follow.",0
"Cine MRI is a valuable tool in the diagnosis and prognosis of cardiovascular disease patients as it provides essential information on the left ventricle's (LV) contracting and relaxing pattern. Clinical experts evaluate this temporal movement of the LV in their practice. To incorporate this temporal information into LV segmentation from cine MRI, a new CNN module is proposed that integrates optical flow (OF) between neighboring frames at a feature level. This allows for the consideration of temporal coherence in cardiac motion during segmentation. The proposed module is integrated into the U-net architecture without additional training and uses dilated convolution to improve spatial accuracy. The proposed network was trained and tested on the Cardiac Atlas database, resulting in a Dice index of 95% and an average perpendicular distance of 0.9 pixels for the middle LV contour. This outperformed the original U-net and improved the temporal coherence of LV segmentation, especially at the LV apex and base where tracking cardiac motion can be challenging.",1
"Edge detection has made significant progress with the help of deep Convolutional Networks (ConvNet). These ConvNet based edge detectors have approached human level performance on standard benchmarks. We provide a systematical study of these detectors' outputs. We show that the detection results did not accurately localize edge pixels, which can be adversarial for tasks that require crisp edge inputs. As a remedy, we propose a novel refinement architecture to address the challenging problem of learning a crisp edge detector using ConvNet. Our method leverages a top-down backward refinement pathway, and progressively increases the resolution of feature maps to generate crisp edges. Our results achieve superior performance, surpassing human accuracy when using standard criteria on BSDS500, and largely outperforming state-of-the-art methods when using more strict criteria. More importantly, we demonstrate the benefit of crisp edge maps for several important applications in computer vision, including optical flow estimation, object proposal generation and semantic segmentation.",0
"The use of deep Convolutional Networks (ConvNet) has led to significant advancements in edge detection, with ConvNet-based detectors achieving performance levels similar to that of human beings on standard benchmarks. In this research, we conducted a systematic study of these detectors' outputs and found that they failed to accurately identify edge pixels, which can be detrimental for tasks that require precise edge inputs. To address this issue, we propose a novel refinement architecture that utilizes a top-down backward refinement pathway to progressively increase the resolution of feature maps and generate crisp edges. Our results demonstrate superior performance, surpassing human accuracy on BSDS500 using standard criteria and outperforming state-of-the-art methods when using more stringent criteria. Additionally, we highlight the benefits of crisp edge maps for several critical applications in computer vision, including optical flow estimation, object proposal generation, and semantic segmentation.",1
"Learning depth and optical flow via deep neural networks by watching videos has made significant progress recently. In this paper, we jointly solve the two tasks by exploiting the underlying geometric rules within stereo videos. Specifically, given two consecutive stereo image pairs from a video, we first estimate depth, camera ego-motion and optical flow from three neural networks. Then the whole scene is decomposed into moving foreground and static background by compar- ing the estimated optical flow and rigid flow derived from the depth and ego-motion. We propose a novel consistency loss to let the optical flow learn from the more accurate rigid flow in static regions. We also design a rigid alignment module which helps refine ego-motion estimation by using the estimated depth and optical flow. Experiments on the KITTI dataset show that our results significantly outperform other state-of- the-art algorithms. Source codes can be found at https: //github.com/baidu-research/UnDepthflow",0
"The use of deep neural networks to learn depth and optical flow from videos has made considerable advancements recently. This study aims to address both tasks simultaneously by utilizing the geometric principles available in stereo videos. The approach involves estimating depth, camera ego-motion, and optical flow using three neural networks on two consecutive stereo image pairs. The scene is then separated into a moving foreground and a static background by comparing the estimated optical flow with the rigid flow derived from the depth and ego-motion. To enable the optical flow to learn from the more reliable rigid flow in the static regions, a novel consistency loss is proposed. Additionally, a rigid alignment module is designed to refine the ego-motion estimation by employing the estimated depth and optical flow. The experiments conducted on the KITTI dataset show that the proposed method outperforms other state-of-the-art algorithms. The source codes are available on https://github.com/baidu-research/UnDepthflow.",1
"Models optimized for accuracy on single images are often prohibitively slow to run on each frame in a video. Recent work exploits the use of optical flow to warp image features forward from select keyframes, as a means to conserve computation on video. This approach, however, achieves only limited speedup, even when optimized, due to the accuracy degradation introduced by repeated forward warping, and the inference cost of optical flow estimation. To address these problems, we propose a new scheme that propagates features using the block motion vectors (BMV) present in compressed video (e.g. H.264 codecs), instead of optical flow, and bi-directionally warps and fuses features from enclosing keyframes to capture scene context on each video frame. Our technique, interpolation-BMV, enables us to accurately estimate the features of intermediate frames, while keeping inference costs low. We evaluate our system on the CamVid and Cityscapes datasets, comparing to both a strong single-frame baseline and related work. We find that we are able to substantially accelerate segmentation on video, achieving near real-time frame rates (20+ frames per second) on large images (e.g. 960 x 720 pixels), while maintaining competitive accuracy. This represents an improvement of almost 6x over the single-frame baseline and 2.5x over the fastest prior work.",0
"Models designed to optimize accuracy on individual images are often too slow to be used on every frame of a video. Recent studies have utilized optical flow to shift image features forward from selected keyframes in order to conserve computational resources when working with video. However, even with optimization, this method only achieves limited speedup due to the accuracy loss caused by repeated forward warping and the high cost of optical flow estimation. To address these issues, we introduce a new approach that propagates features using the block motion vectors (BMV) present in compressed video (such as H.264 codecs) instead of optical flow. Our interpolation-BMV technique bi-directionally warps and integrates features from enclosing keyframes to capture scene context on each video frame, allowing us to accurately estimate the features of intermediate frames while keeping inference costs low. We evaluate our system on the CamVid and Cityscapes datasets, comparing it to both a strong single-frame baseline and related work. Our results show that we are able to significantly enhance segmentation on video, achieving near real-time frame rates (20+ frames per second) on large images (e.g. 960 x 720 pixels) while maintaining competitive accuracy. This represents an improvement of almost 6x over the single-frame baseline and 2.5x over previous fastest methods.",1
"Deep convolutional neural networks (DCNN) have recently shown promising results in low-level computer vision problems such as optical flow and disparity estimation, but still, have much room to further improve their performance. In this paper, we propose a novel sub-pixel convolution-based encoder-decoder network for optical flow and disparity estimations, which can extend FlowNetS and DispNet by replacing the deconvolution layers with sup-pixel convolution blocks. By using sub-pixel refinement and estimation on the decoder stages instead of deconvolution, we can significantly improve the estimation accuracy for optical flow and disparity, even with reduced numbers of parameters. We show a supervised end-to-end training of our proposed networks for optical flow and disparity estimations, and an unsupervised end-to-end training for monocular depth and pose estimations. In order to verify the effectiveness of our proposed networks, we perform intensive experiments for (i) optical flow and disparity estimations, and (ii) monocular depth and pose estimations. Throughout the extensive experiments, our proposed networks outperform the baselines such as FlowNetS and DispNet in terms of estimation accuracy and training times.",0
"Although deep convolutional neural networks (DCNN) have exhibited promising outcomes in low-level computer vision tasks like optical flow and disparity estimation, there is still ample space for them to enhance their performance. This study suggests a groundbreaking encoder-decoder network based on sub-pixel convolution for optical flow and disparity estimations. It expands upon FlowNetS and DispNet by substituting the deconvolution layers with sub-pixel convolution blocks to refine and estimate the decoder stages. This approach significantly improves the accuracy of optical flow and disparity estimations, even with fewer parameters. The proposed networks are trained end-to-end using supervised and unsupervised techniques for optical flow and disparity estimations, as well as monocular depth and pose estimations. Through extensive experimentation, our networks outperform baselines like FlowNetS and DispNet in terms of estimation accuracy and training times.",1
"Anticipating future events is an important prerequisite towards intelligent behavior. Video forecasting has been studied as a proxy task towards this goal. Recent work has shown that to predict semantic segmentation of future frames, forecasting at the semantic level is more effective than forecasting RGB frames and then segmenting these. In this paper we consider the more challenging problem of future instance segmentation, which additionally segments out individual objects. To deal with a varying number of output labels per image, we develop a predictive model in the space of fixed-sized convolutional features of the Mask R-CNN instance segmentation model. We apply the ""detection head'"" of Mask R-CNN on the predicted features to produce the instance segmentation of future frames. Experiments show that this approach significantly improves over strong baselines based on optical flow and repurposed instance segmentation architectures.",0
"Being able to predict future events is essential for exhibiting intelligent behavior. Video forecasting has been researched as a means to achieve this objective. Recent studies have revealed that to forecast the semantic segmentation of future frames, it is more effective to forecast at the semantic level rather than forecasting RGB frames and then segmenting them. This paper aims to tackle the more challenging task of future instance segmentation, which involves segmenting out individual objects as well. To address the issue of varying output labels per image, we have developed a predictive model in the space of fixed-sized convolutional features of the Mask R-CNN instance segmentation model. To produce the instance segmentation of future frames, we have utilized the ""detection head"" of Mask R-CNN on the predicted features. Our experiments have shown that this approach significantly outperforms strong baselines based on optical flow and repurposed instance segmentation architectures.",1
"Plenoptic cameras offer a cost effective solution to capture light fields by multiplexing multiple views on a single image sensor. However, the high angular resolution is achieved at the expense of reducing the spatial resolution of each view by orders of magnitude compared to the raw sensor image. While light field super-resolution is still at an early stage, the field of single image super-resolution (SISR) has recently known significant advances with the use of deep learning techniques. This paper describes a simple framework allowing us to leverage state-of-the-art SISR techniques into light fields, while taking into account specific light field geometrical constraints. The idea is to first compute a representation compacting most of the light field energy into as few components as possible. This is achieved by aligning the light field using optical flows and then by decomposing the aligned light field using singular value decomposition (SVD). The principal basis captures the information that is coherent across all the views, while the other basis contain the high angular frequencies. Super-resolving this principal basis using an SISR method allows us to super-resolve all the information that is coherent across the entire light field. This framework allows the proposed light field super-resolution method to inherit the benefits of the SISR method used. Experimental results show that the proposed method is competitive, and most of the time superior, to recent light field super-resolution methods in terms of both PSNR and SSIM quality metrics, with a lower complexity.",0
"Plenoptic cameras are a cost-effective way to capture light fields by combining multiple views onto one image sensor. However, the high angular resolution comes at the cost of reducing the spatial resolution of each view significantly compared to the raw sensor image. Although light field super-resolution is still in the early stages, single image super-resolution (SISR) has made significant strides recently with the use of deep learning techniques. This paper presents a straightforward framework that enables the integration of state-of-the-art SISR techniques into light fields while considering specific light field geometrical constraints. The concept is to first create a compact representation that compresses most of the light field energy into as few components as possible. This is accomplished by aligning the light field using optical flows and decomposing the aligned light field using singular value decomposition (SVD). The principal basis captures the information that is coherent across all views, while the other basis contains the high angular frequencies. Super-resolving this principal basis using an SISR method enables us to super-resolve all information that is coherent across the entire light field. This framework allows the proposed light field super-resolution method to derive the benefits of the SISR method used. Experimental results demonstrate that the proposed method is competitive and, in most cases, superior to recent light field super-resolution methods in terms of both PSNR and SSIM quality metrics, with lower complexity.",1
"Detecting the occlusion from stereo images or video frames is important to many computer vision applications. Previous efforts focus on bundling it with the computation of disparity or optical flow, leading to a chicken-and-egg problem. In this paper, we leverage convolutional neural network to liberate the occlusion detection task from the interleaved, traditional calculation framework. We propose a Symmetric Network (SymmNet) to directly exploit information from an image pair, without estimating disparity or motion in advance. The proposed network is structurally left-right symmetric to learn the binocular occlusion simultaneously, aimed at jointly improving both results. The comprehensive experiments show that our model achieves state-of-the-art results on detecting the stereo and motion occlusion.",0
"Many computer vision applications require the detection of occlusion from stereo images or video frames. Previous attempts to address this issue involved combining it with disparity or optical flow computation, which created a chicken-and-egg problem. This paper introduces a new approach that uses a convolutional neural network to separate occlusion detection from traditional calculations. We propose a Symmetric Network (SymmNet) that directly utilizes image pair information without estimating disparity or motion beforehand. The SymmNet is designed to learn binocular occlusion simultaneously and improve both results. Our model achieves state-of-the-art results in detecting stereo and motion occlusion, as demonstrated in comprehensive experiments.",1
"The difficulty of annotating training data is a major obstacle to using CNNs for low-level tasks in video. Synthetic data often does not generalize to real videos, while unsupervised methods require heuristic losses. Proxy tasks can overcome these issues, and start by training a network for a task for which annotation is easier or which can be trained unsupervised. The trained network is then fine-tuned for the original task using small amounts of ground truth data. Here, we investigate frame interpolation as a proxy task for optical flow. Using real movies, we train a CNN unsupervised for temporal interpolation. Such a network implicitly estimates motion, but cannot handle untextured regions. By fine-tuning on small amounts of ground truth flow, the network can learn to fill in homogeneous regions and compute full optical flow fields. Using this unsupervised pre-training, our network outperforms similar architectures that were trained supervised using synthetic optical flow.",0
"Annotating training data is a significant challenge when using CNNs for low-level tasks in video. Synthetic data lacks generalizability to real videos, while unsupervised methods require heuristic losses. To overcome these issues, proxy tasks can be used, starting with training a network for a task that is easier to annotate or can be trained unsupervised. The network is then fine-tuned with small amounts of ground truth data for the original task. In this study, we explore frame interpolation as a proxy task for optical flow. We train a CNN unsupervised for temporal interpolation with real movies, which implicitly estimates motion but struggles with untextured regions. By fine-tuning on small amounts of ground truth flow, the network learns to fill in homogeneous regions and compute full optical flow fields. Our network, with its unsupervised pre-training, outperforms similar architectures that were trained supervised using synthetic optical flow.",1
"We investigate two crucial and closely related aspects of CNNs for optical flow estimation: models and training. First, we design a compact but effective CNN model, called PWC-Net, according to simple and well-established principles: pyramidal processing, warping, and cost volume processing. PWC-Net is 17 times smaller in size, 2 times faster in inference, and 11\% more accurate on Sintel final than the recent FlowNet2 model. It is the winning entry in the optical flow competition of the robust vision challenge. Next, we experimentally analyze the sources of our performance gains. In particular, we use the same training procedure of PWC-Net to retrain FlowNetC, a sub-network of FlowNet2. The retrained FlowNetC is 56\% more accurate on Sintel final than the previously trained one and even 5\% more accurate than the FlowNet2 model. We further improve the training procedure and increase the accuracy of PWC-Net on Sintel by 10\% and on KITTI 2012 and 2015 by 20\%. Our newly trained model parameters and training protocols will be available on https://github.com/NVlabs/PWC-Net",0
"The focus of our investigation is on two key aspects of CNNs for optical flow estimation: the models and the training. Initially, we created a compact yet efficient CNN model, named PWC-Net, which adheres to well-established principles such as pyramidal processing, warping, and cost volume processing. Compared to the recent FlowNet2 model, PWC-Net is 17 times smaller in size, 2 times faster in inference, and 11% more accurate on Sintel final. This model was the winner of the optical flow competition in the robust vision challenge. We then conducted an experimental analysis to determine the reasons for our improved performance. We used the same training procedure for PWC-Net to retrain FlowNetC, a sub-network of FlowNet2. The retrained FlowNetC achieved 56% greater accuracy on Sintel final than its previous training, and even surpassed the accuracy of the FlowNet2 model by 5%. We further improved the training procedure, resulting in a 10% increase in accuracy on Sintel and a 20% increase on KITTI 2012 and 2015 for PWC-Net. Our newly trained model parameters and training protocols can be found on https://github.com/NVlabs/PWC-Net.",1
"Currently, the most common motion representation for action recognition is optical flow. Optical flow is based on particle tracking which adheres to a Lagrangian perspective on dynamics. In contrast to the Lagrangian perspective, the Eulerian model of dynamics does not track, but describes local changes. For video, an Eulerian phase-based motion representation, using complex steerable filters, has been successfully employed recently for motion magnification and video frame interpolation. Inspired by these previous works, here, we proposes learning Eulerian motion representations in a deep architecture for action recognition. We learn filters in the complex domain in an end-to-end manner. We design these complex filters to resemble complex Gabor filters, typically employed for phase-information extraction. We propose a phase-information extraction module, based on these complex filters, that can be used in any network architecture for extracting Eulerian representations. We experimentally analyze the added value of Eulerian motion representations, as extracted by our proposed phase extraction module, and compare with existing motion representations based on optical flow, on the UCF101 dataset.",0
"At present, optical flow is the most commonly used method for representing motion in action recognition. This method is based on particle tracking and follows the Lagrangian perspective on dynamics. However, the Eulerian model of dynamics provides a different approach that does not involve tracking but instead focuses on describing local changes. Recently, a successful approach to motion magnification and video frame interpolation has been the use of an Eulerian phase-based motion representation employing complex steerable filters. Building on this work, we propose a deep architecture for action recognition that learns Eulerian motion representations. In this approach, we learn filters in the complex domain in an end-to-end manner, designing them to resemble complex Gabor filters that are typically used for extracting phase information. Our proposed phase-information extraction module, based on these complex filters, can be used in any network architecture to extract Eulerian representations. We conduct experiments to evaluate the added value of our proposed approach, comparing it with existing motion representations based on optical flow, using the UCF101 dataset.",1
"The wide availability of Commercial Off-The-Shelf (COTS) electronics that can withstand Low Earth Orbit conditions has opened avenue for wide deployment of CubeSats and small-satellites. CubeSats thanks to their low developmental and launch costs offer new opportunities for rapidly demonstrating on-orbit surveillance capabilities. In our earlier work, we proposed development of SWIMSat (Space based Wide-angle Imaging of Meteors) a 3U CubeSat demonstrator that is designed to observe illuminated objects entering the Earth's atmosphere. The spacecraft would operate autonomously using a smart camera with vision algorithms to detect, track and report of objects. Several CubeSats can track an object in a coordinated fashion to pinpoint an object's trajectory. An extension of this smart camera capability is to track unilluminated objects utilizing capabilities we have been developing to track and navigate to Near Earth Objects (NEOs). This extension enables detecting and tracking objects that can't readily be detected by humans. The system maintains a dense star map of the night sky and performs round the clock observations. Standard optical flow algorithms are used to obtain trajectories of all moving objects in the camera field of view. Through a process of elimination, certain stars maybe occluded by a transiting unilluminated object which is then used to first detect and obtain a trajectory of the object. Using multiple cameras observing the event from different points of view, it may be possible then to triangulate the position of the object in space and obtain its orbital trajectory. In this work, the performance of our space object detection algorithm coupled with a spacecraft guidance, navigation, and control system is demonstrated.",0
"The availability of Commercial Off-The-Shelf (COTS) electronics that can withstand Low Earth Orbit conditions has paved the way for the widespread deployment of CubeSats and small-satellites. CubeSats offer new opportunities for demonstrating surveillance capabilities at a low developmental and launch cost. In previous work, we proposed the development of SWIMSat, a 3U CubeSat demonstrator designed to observe illuminated objects entering the Earth's atmosphere. The spacecraft operates autonomously using a smart camera with vision algorithms to detect, track, and report objects. By coordinating several CubeSats, an object's trajectory can be pinpointed. The smart camera can also be used to track unilluminated objects using Near Earth Object (NEO) tracking and navigation capabilities. The system maintains a dense star map of the night sky and performs round the clock observations. Optical flow algorithms are used to obtain trajectories of all moving objects in the camera field of view. An unilluminated object may occlude certain stars, allowing for detection and trajectory determination. Multiple cameras observing the event may triangulate the object's position in space and obtain its orbital trajectory. Our work demonstrates the performance of the space object detection algorithm coupled with a spacecraft guidance, navigation, and control system.",1
"Learning long-term spatial-temporal features are critical for many video analysis tasks. However, existing video segmentation methods predominantly rely on static image segmentation techniques, and methods capturing temporal dependency for segmentation have to depend on pretrained optical flow models, leading to suboptimal solutions for the problem. End-to-end sequential learning to explore spatialtemporal features for video segmentation is largely limited by the scale of available video segmentation datasets, i.e., even the largest video segmentation dataset only contains 90 short video clips. To solve this problem, we build a new large-scale video object segmentation dataset called YouTube Video Object Segmentation dataset (YouTube-VOS). Our dataset contains 4,453 YouTube video clips and 94 object categories. This is by far the largest video object segmentation dataset to our knowledge and has been released at http://youtube-vos.org. We further evaluate several existing state-of-the-art video object segmentation algorithms on this dataset which aims to establish baselines for the development of new algorithms in the future.",0
"For many video analysis tasks, it is crucial to acquire long-term spatial-temporal features. However, current video segmentation methods primarily rely on static image segmentation techniques. Methods that capture temporal dependency for segmentation have to depend on pre-trained optical flow models, leading to suboptimal solutions for the problem. The scale of available video segmentation datasets limits end-to-end sequential learning to explore spatial-temporal features for video segmentation. Even the largest video segmentation dataset only contains 90 short video clips. To address this issue, we have created a new large-scale video object segmentation dataset called the YouTube Video Object Segmentation dataset (YouTube-VOS). Our dataset comprises 4,453 YouTube video clips and 94 object categories. This is the largest video object segmentation dataset to our knowledge and has been released at http://youtube-vos.org. Furthermore, we evaluate several existing state-of-the-art video object segmentation algorithms on this dataset to establish baselines for the development of new algorithms in the future.",1
"We present an unsupervised learning framework for simultaneously training single-view depth prediction and optical flow estimation models using unlabeled video sequences. Existing unsupervised methods often exploit brightness constancy and spatial smoothness priors to train depth or flow models. In this paper, we propose to leverage geometric consistency as additional supervisory signals. Our core idea is that for rigid regions we can use the predicted scene depth and camera motion to synthesize 2D optical flow by backprojecting the induced 3D scene flow. The discrepancy between the rigid flow (from depth prediction and camera motion) and the estimated flow (from optical flow model) allows us to impose a cross-task consistency loss. While all the networks are jointly optimized during training, they can be applied independently at test time. Extensive experiments demonstrate that our depth and flow models compare favorably with state-of-the-art unsupervised methods.",0
"A framework for unsupervised learning is presented in this study, which trains single-view depth prediction and optical flow estimation models simultaneously using unlabeled video sequences. Existing unsupervised methods typically utilize brightness constancy and spatial smoothness priors for training depth or flow models. To provide additional supervisory signals, we propose leveraging geometric consistency. Our core idea is to use the predicted scene depth and camera motion to synthesize 2D optical flow for rigid regions by backprojecting the induced 3D scene flow. A cross-task consistency loss can be imposed by comparing the rigid flow, estimated from depth prediction and camera motion, with the flow predicted from the optical flow model. Although the networks are jointly optimized during training, they can be independently applied at test time. Extensive experiments demonstrate the superiority of our depth and flow models over state-of-the-art unsupervised methods.",1
"Compositing is one of the most important editing operations for images and videos. The process of improving the realism of composite results is often called harmonization. Previous approaches for harmonization mainly focus on images. In this work, we take one step further to attack the problem of video harmonization. Specifically, we train a convolutional neural network in an adversarial way, exploiting a pixel-wise disharmony discriminator to achieve more realistic harmonized results and introducing a temporal loss to increase temporal consistency between consecutive harmonized frames. Thanks to the pixel-wise disharmony discriminator, we are also able to relieve the need of input foreground masks. Since existing video datasets which have ground-truth foreground masks and optical flows are not sufficiently large, we propose a simple yet efficient method to build up a synthetic dataset supporting supervised training of the proposed adversarial network. Experiments show that training on our synthetic dataset generalizes well to the real-world composite dataset. Also, our method successfully incorporates temporal consistency during training and achieves more harmonious results than previous methods.",0
"One of the most crucial editing operations for images and videos is compositing. The process of enhancing the realism of composite outcomes is commonly referred to as harmonization, with previous methods focusing mainly on images. However, this study takes a step further by addressing the problem of video harmonization. The authors use an adversarial approach to train a convolutional neural network, utilizing a pixel-wise disharmony discriminator to produce more authentic harmonized results. Additionally, they introduce a temporal loss to ensure temporal consistency between consecutive harmonized frames. The pixel-wise disharmony discriminator also eliminates the need for input foreground masks. However, due to the lack of video datasets with ground-truth foreground masks and optical flows, the authors propose a straightforward yet effective method for creating a synthetic dataset to support supervised training of the proposed adversarial network. The experiment results demonstrate that training on the synthetic dataset generalizes well to the real-world composite dataset and that the proposed method is superior to previous approaches in achieving harmonious results while incorporating temporal consistency during training.",1
"Unsupervised video segmentation plays an important role in a wide variety of applications from object identification to compression. However, to date, fast motion, motion blur and occlusions pose significant challenges. To address these challenges for unsupervised video segmentation, we develop a novel saliency estimation technique as well as a novel neighborhood graph, based on optical flow and edge cues. Our approach leads to significantly better initial foreground-background estimates and their robust as well as accurate diffusion across time. We evaluate our proposed algorithm on the challenging DAVIS, SegTrack v2 and FBMS-59 datasets. Despite the usage of only a standard edge detector trained on 200 images, our method achieves state-of-the-art results outperforming deep learning based methods in the unsupervised setting. We even demonstrate competitive results comparable to deep learning based methods in the semi-supervised setting on the DAVIS dataset.",0
"Unsupervised video segmentation is crucial in various applications that range from object identification to compression. However, challenges are posed by fast motion, motion blur, and occlusions. To overcome these challenges, we introduce a new saliency estimation technique and neighborhood graph based on optical flow and edge cues for unsupervised video segmentation. Our method results in better initial foreground-background estimates, and it diffuses them accurately across time. We tested our algorithm on the difficult DAVIS, SegTrack v2, and FBMS-59 datasets. Despite using only a standard edge detector trained on 200 images, our approach outperforms deep learning methods in the unsupervised setting and is competitive with deep learning methods in the semi-supervised setting on the DAVIS dataset.",1
"The convolutional neural network model for optical flow estimation usually outputs a low-resolution(LR) optical flow field. To obtain the corresponding full image resolution,interpolation and variational approach are the most common options, which do not effectively improve the results. With the motivation of various convolutional neural network(CNN) structures succeeded in single image super-resolution(SISR) task, an end-to-end convolutional neural network is proposed to reconstruct the high resolution(HR) optical flow field from initial LR optical flow with the guidence of the first frame used in optical flow estimation. Our optical flow super-resolution(OFSR) problem differs from the general SISR problem in two main aspects. Firstly, the optical flow includes less texture information than image so that the SISR CNN structures can't be directly used in our OFSR problem. Secondly, the initial LR optical flow data contains estimation error, while the LR image data for SISR is generally a bicubic downsampled, blurred, and noisy version of HR ground truth. We evaluate the proposed approach on two different optical flow estimation mehods and show that it can not only obtain the full image resolution, but generate more accurate optical flow field (Accuracy improve 15% on FlyingChairs, 13% on MPI Sintel) with sharper edges than the estimation result of original method.",0
"When estimating optical flow using a convolutional neural network model, the resulting optical flow field is typically of low resolution. Common methods used to obtain full image resolution, such as interpolation and variational approaches, are ineffective at improving results. To address this issue, an end-to-end convolutional neural network has been proposed to reconstruct a high-resolution optical flow field from the initial low-resolution optical flow field, with guidance from the first frame used in optical flow estimation. This optical flow super-resolution (OFSR) approach differs from the general single image super-resolution (SISR) problem in two ways: optical flow includes less texture information than images, and the initial low-resolution optical flow data contains estimation errors. The proposed approach was evaluated on two different optical flow estimation methods and showed improved accuracy (15% on FlyingChairs and 13% on MPI Sintel) with sharper edges compared to the original method.",1
"Learning long-term spatial-temporal features are critical for many video analysis tasks. However, existing video segmentation methods predominantly rely on static image segmentation techniques, and methods capturing temporal dependency for segmentation have to depend on pretrained optical flow models, leading to suboptimal solutions for the problem. End-to-end sequential learning to explore spatial-temporal features for video segmentation is largely limited by the scale of available video segmentation datasets, i.e., even the largest video segmentation dataset only contains 90 short video clips. To solve this problem, we build a new large-scale video object segmentation dataset called YouTube Video Object Segmentation dataset (YouTube-VOS). Our dataset contains 3,252 YouTube video clips and 78 categories including common objects and human activities. This is by far the largest video object segmentation dataset to our knowledge and we have released it at https://youtube-vos.org. Based on this dataset, we propose a novel sequence-to-sequence network to fully exploit long-term spatial-temporal information in videos for segmentation. We demonstrate that our method is able to achieve the best results on our YouTube-VOS test set and comparable results on DAVIS 2016 compared to the current state-of-the-art methods. Experiments show that the large scale dataset is indeed a key factor to the success of our model.",0
"The acquisition of long-term spatial-temporal features is crucial for numerous video analysis tasks. However, the majority of existing video segmentation techniques rely on static image segmentation methods, and those that capture temporal dependency for segmentation must rely on pretrained optical flow models, which results in suboptimal solutions. End-to-end sequential learning to investigate spatial-temporal features for video segmentation is restricted by the limited scale of available video segmentation datasets. Even the largest video segmentation dataset only encompasses 90 short video clips. To solve this issue, we have created a new comprehensive video object segmentation dataset called the YouTube Video Object Segmentation dataset (YouTube-VOS). Our dataset contains 3,252 YouTube video clips and 78 categories, incorporating common objects and human activities. To our knowledge, this is the most extensive video object segmentation dataset available, and it can be found at https://youtube-vos.org. Using this dataset, we have developed a unique sequence-to-sequence network to fully explore the long-term spatial-temporal information in videos for segmentation. Our approach achieves the best results on our YouTube-VOS test set and produces comparable results on DAVIS 2016 compared to the current state-of-the-art techniques. Our experiments have shown that the large-scale dataset is a crucial element in the success of our model.",1
"In this paper we propose a novel approach to estimate dense optical flow from sparse lidar data acquired on an autonomous vehicle. This is intended to be used as a drop-in replacement of any image-based optical flow system when images are not reliable due to e.g. adverse weather conditions or at night. In order to infer high resolution 2D flows from discrete range data we devise a three-block architecture of multiscale filters that combines multiple intermediate objectives, both in the lidar and image domain. To train this network we introduce a dataset with approximately 20K lidar samples of the Kitti dataset which we have augmented with a pseudo ground-truth image-based optical flow computed using FlowNet2. We demonstrate the effectiveness of our approach on Kitti, and show that despite using the low-resolution and sparse measurements of the lidar, we can regress dense optical flow maps which are at par with those estimated with image-based methods.",0
"This paper presents a new method for estimating dense optical flow from sparse lidar data acquired on an autonomous vehicle. The proposed approach can be used as a substitute for image-based optical flow systems in cases where images are unreliable, such as adverse weather conditions or nighttime. To achieve high resolution 2D flows from discrete range data, a three-block architecture of multiscale filters is designed, which combines multiple intermediate objectives in both the lidar and image domain. To train the network, a dataset with approximately 20K lidar samples of the Kitti dataset is introduced, which is augmented with a pseudo ground-truth image-based optical flow computed using FlowNet2. The effectiveness of the approach is demonstrated on Kitti, and it is shown that despite using low-resolution and sparse lidar measurements, the method can regress dense optical flow maps that are comparable to those estimated using image-based methods.",1
"Perception technologies in Autonomous Driving are experiencing their golden age due to the advances in Deep Learning. Yet, most of these systems rely on the semantically rich information of RGB images. Deep Learning solutions applied to the data of other sensors typically mounted on autonomous cars (e.g. lidars or radars) are not explored much. In this paper we propose a novel solution to understand the dynamics of moving vehicles of the scene from only lidar information. The main challenge of this problem stems from the fact that we need to disambiguate the proprio-motion of the 'observer' vehicle from that of the external 'observed' vehicles. For this purpose, we devise a CNN architecture which at testing time is fed with pairs of consecutive lidar scans. However, in order to properly learn the parameters of this network, during training we introduce a series of so-called pretext tasks which also leverage on image data. These tasks include semantic information about vehicleness and a novel lidar-flow feature which combines standard image-based optical flow with lidar scans. We obtain very promising results and show that including distilled image information only during training, allows improving the inference results of the network at test time, even when image data is no longer used.",0
"Autonomous Driving is currently experiencing a boom in Perception technologies thanks to the advancements in Deep Learning. However, most of these systems rely solely on semantically rich RGB images. Little research has been conducted on applying Deep Learning solutions to the data from other sensors found on autonomous cars, such as lidars or radars. This paper presents a novel solution to comprehend the movement of vehicles in a scene using only lidar information. The primary challenge is distinguishing between the observer vehicle's proprio-motion and that of the external observed vehicles. To tackle this challenge, a CNN architecture is used, which is given pairs of consecutive lidar scans during testing. However, to train the network's parameters adequately, a series of pretext tasks is introduced that uses image data, including semantic information about vehicleness and a novel lidar-flow feature that merges standard image-based optical flow with lidar scans. The results are highly encouraging, demonstrating that incorporating distilled image information during training can improve the network's inference performance at test time, even without the use of image data.",1
"Scene flow describes 3D motion in a 3D scene. It can either be modeled as a single task, or it can be reconstructed from the auxiliary tasks of stereo depth and optical flow estimation. While the second method can achieve real-time performance by using real-time auxiliary methods, it will typically produce non-dense results. In this representation of a basic combination approach for scene flow estimation, we will tackle the problem of non-density by interpolation.",0
"The concept of scene flow pertains to the movement of objects in a three-dimensional setting. It may be viewed as a solo undertaking or created from additional tasks like stereo depth and optical flow estimation. The latter approach is capable of real-time performance through the use of concurrent auxiliary methods, but the outcome is often not thorough. Our strategy, which combines these methods for estimating scene flow, addresses the issue of incompleteness by employing interpolation.",1
"Anomaly detection through video analysis is of great importance to detect any anomalous vehicle/human behavior at a traffic intersection. While most existing works use neural networks and conventional machine learning methods based on provided dataset, we will use object recognition (Faster R-CNN) to identify objects labels and their corresponding location in the video scene as the first step to implement anomaly detection. Then, the optical flow will be utilized to identify adaptive traffic flows in each region of the frame. Basically, we propose an alternative method for unusual activity detection using an adaptive anomaly detection framework. Compared to the baseline method described in the reference paper, our method is more efficient and yields the comparable accuracy.",0
"Detecting abnormal vehicle or human behavior at a traffic intersection is crucial, and video analysis is a common method used for this task. However, most existing approaches rely on neural networks and traditional machine learning techniques with pre-existing datasets. Our proposed approach utilizes object recognition (Faster R-CNN) to identify object labels and their locations in the video footage as an initial step towards anomaly detection. Then, we apply optical flow to identify traffic patterns in each area of the frame. Our method offers an adaptive anomaly detection framework that presents a feasible alternative to detecting unusual activity. When compared to a baseline method outlined in the reference paper, our approach is more efficient and produces comparable accuracy results.",1
"We present a method to reconstruct the three-dimensional trajectory of a moving instance of a known object category using stereo video data. We track the two-dimensional shape of objects on pixel level exploiting instance-aware semantic segmentation techniques and optical flow cues. We apply Structure from Motion (SfM) techniques to object and background images to determine for each frame initial camera poses relative to object instances and background structures. We refine the initial SfM results by integrating stereo camera constraints exploiting factor graphs. We compute the object trajectory by combining object and background camera pose information. In contrast to stereo matching methods, our approach leverages temporal adjacent views for object point triangulation. As opposed to monocular trajectory reconstruction approaches, our method shows no degenerated cases. We evaluate our approach using publicly available video data of vehicles in urban scenes.",0
"Our method utilizes stereo video data to reconstruct the three-dimensional trajectory of a moving object category that is already known. By implementing instance-aware semantic segmentation techniques and optical flow cues, we track the object's two-dimensional shape on a pixel level. We use Structure from Motion (SfM) techniques to determine initial camera poses relative to object instances and background structures in each frame. Afterward, we integrate stereo camera constraints and factor graphs to refine our initial SfM results. By combining object and background camera pose information, we compute the object trajectory. Our approach is different from stereo matching methods as we use temporal adjacent views for object point triangulation. Moreover, our method does not experience any degenerated cases, unlike monocular trajectory reconstruction approaches. We demonstrate our method's effectiveness using publicly available video data of vehicles in urban scenes.",1
"We propose a self-supervised learning method to jointly reason about spatial and temporal context for video recognition. Recent self-supervised approaches have used spatial context [9, 34] as well as temporal coherency [32] but a combination of the two requires extensive preprocessing such as tracking objects through millions of video frames [59] or computing optical flow to determine frame regions with high motion [30]. We propose to combine spatial and temporal context in one self-supervised framework without any heavy preprocessing. We divide multiple video frames into grids of patches and train a network to solve jigsaw puzzles on these patches from multiple frames. So the network is trained to correctly identify the position of a patch within a video frame as well as the position of a patch over time. We also propose a novel permutation strategy that outperforms random permutations while significantly reducing computational and memory constraints. We use our trained network for transfer learning tasks such as video activity recognition and demonstrate the strength of our approach on two benchmark video action recognition datasets without using a single frame from these datasets for unsupervised pretraining of our proposed video jigsaw network.",0
"Our approach proposes a self-supervised learning method that combines spatial and temporal context for video recognition without the need for extensive preprocessing. Previous self-supervised methods have separately used spatial context and temporal coherency, but our method aims to merge the two into a single framework. To achieve this, we divide multiple video frames into patches and train a network to solve jigsaw puzzles on these patches from multiple frames, enabling the network to identify both the position of a patch within a video frame and over time. Additionally, we introduce a novel permutation strategy that outperforms random permutations while reducing computational and memory constraints. Our trained network can be used for transfer learning tasks such as video activity recognition and has demonstrated strong performance on two benchmark video action recognition datasets without the need for unsupervised pretraining.",1
"We propose a novel representation for dense pixel-wise estimation tasks using CNNs that boosts accuracy and reduces training time, by explicitly exploiting joint coarse-and-fine reasoning. The coarse reasoning is performed over a discrete classification space to obtain a general rough solution, while the fine details of the solution are obtained over a continuous regression space. In our approach both components are jointly estimated, which proved to be beneficial for improving estimation accuracy. Additionally, we propose a new network architecture, which combines coarse and fine components by treating the fine estimation as a refinement built on top of the coarse solution, and therefore adding details to the general prediction. We apply our approach to the challenging problem of optical flow estimation and empirically validate it against state-of-the-art CNN-based solutions trained from scratch and tested on large optical flow datasets.",0
"Our proposal involves a unique method of representing dense pixel-wise estimation tasks using CNNs. This method enhances accuracy and reduces training time by taking advantage of joint coarse-and-fine reasoning. By utilizing a discrete classification space, the coarse reasoning provides a general solution, while a continuous regression space is used to obtain fine details. Our approach combines both components, leading to improved estimation accuracy. Additionally, we introduce a new network architecture that treats fine estimation as a refinement of the coarse solution, adding more details to the general prediction. We tested our approach on the challenging task of optical flow estimation and compared it with state-of-the-art CNN-based solutions, which were trained from scratch and tested on large optical flow datasets. Our results validate the effectiveness of our approach.",1
"Optical flow refers to the visual motion observed between two consecutive images. Since the degree of freedom is typically much larger than the constraints imposed by the image observations, the straightforward formulation of optical flow as an inverse problem is ill-posed. Standard approaches to determine optical flow rely on formulating and solving an optimization problem that contains both a data fidelity term and a regularization term, the latter effectively resolves the otherwise ill-posedness of the inverse problem. In this work, we depart from the deterministic formalism, and instead treat optical flow as a statistical inverse problem. We discuss how a classical optical flow solution can be interpreted as a point estimate in this more general framework. The statistical approach, whose ""solution"" is a distribution of flow fields, which we refer to as Bayesian optical flow, allows not only ""point"" estimates (e.g., the computation of average flow field), but also statistical estimates (e.g., quantification of uncertainty) that are beyond any standard method for optical flow. As application, we benchmark Bayesian optical flow together with uncertainty quantification using several types of prescribed ground-truth flow fields and images.",0
"The term ""optical flow"" pertains to the visual motion that occurs between two consecutive images. However, due to the significant degree of freedom, the straightforward approach of treating optical flow as an inverse problem is not feasible. Instead, most methods rely on an optimization problem that involves a data fidelity term and a regularization term to resolve the ill-posedness of the inverse problem. In this study, we opt for a statistical approach to optical flow, treating it as a statistical inverse problem. This allows for both point estimates and statistical estimates, including quantification of uncertainty, which are not possible with traditional methods. We refer to this approach as Bayesian optical flow and demonstrate its effectiveness by applying it to various types of ground-truth flow fields and images.",1
"Recent work has shown that convolutional neural networks (CNNs) can be used to estimate optical flow with high quality and fast runtime. This makes them preferable for real-world applications. However, such networks require very large training datasets. Engineering the training data is difficult and/or laborious. This paper shows how to augment a network trained on an existing synthetic dataset with large amounts of additional unlabelled data. In particular, we introduce a selection mechanism to assemble from multiple estimates a joint optical flow field, which outperforms that of all input methods. The latter can be used as proxy-ground-truth to train a network on real-world data and to adapt it to specific domains of interest. Our experimental results show that the performance of networks improves considerably, both, in cross-domain and in domain-specific scenarios. As a consequence, we obtain state-of-the-art results on the KITTI benchmarks.",0
"A recent study has demonstrated that convolutional neural networks (CNNs) can be used for quick and high-quality optical flow estimation, making them the preferred choice for practical applications. However, these networks necessitate large training datasets, which can be challenging and time-consuming to engineer. This study aims to address this issue by demonstrating how to enhance a network trained on a synthetic dataset with unlabelled data. A selection mechanism is introduced to generate a joint optical flow field from multiple estimates, which outperforms all other input methods. This joint field can serve as proxy-ground-truth for training a network on real-world data and adapting it to specific domains of interest. The study's findings indicate that the network's performance improves significantly in cross-domain and domain-specific scenarios, resulting in state-of-the-art results on the KITTI benchmarks.",1
"Learning to estimate 3D geometry in a single image by watching unlabeled videos via deep convolutional network has made significant process recently. Current state-of-the-art (SOTA) methods, are based on the learning framework of rigid structure-from-motion, where only 3D camera ego motion is modeled for geometry estimation.However, moving objects also exist in many videos, e.g. moving cars in a street scene. In this paper, we tackle such motion by additionally incorporating per-pixel 3D object motion into the learning framework, which provides holistic 3D scene flow understanding and helps single image geometry estimation. Specifically, given two consecutive frames from a video, we adopt a motion network to predict their relative 3D camera pose and a segmentation mask distinguishing moving objects and rigid background. An optical flow network is used to estimate dense 2D per-pixel correspondence. A single image depth network predicts depth maps for both images. The four types of information, i.e. 2D flow, camera pose, segment mask and depth maps, are integrated into a differentiable holistic 3D motion parser (HMP), where per-pixel 3D motion for rigid background and moving objects are recovered. We design various losses w.r.t. the two types of 3D motions for training the depth and motion networks, yielding further error reduction for estimated geometry. Finally, in order to solve the 3D motion confusion from monocular videos, we combine stereo images into joint training. Experiments on KITTI 2015 dataset show that our estimated geometry, 3D motion and moving object masks, not only are constrained to be consistent, but also significantly outperforms other SOTA algorithms, demonstrating the benefits of our approach.",0
"Recent progress has been made in the field of estimating 3D geometry in a single image through the use of deep convolutional networks and unlabeled videos. However, current state-of-the-art methods are limited to modeling only the 3D camera ego motion for geometry estimation using a rigid structure-from-motion framework. This approach fails to account for moving objects in videos, such as cars in a street scene. This paper proposes a solution to this problem by incorporating per-pixel 3D object motion into the learning framework, which allows for holistic 3D scene flow understanding and improved single image geometry estimation. To achieve this, the paper adopts a motion network to predict relative 3D camera pose, a segmentation mask for moving objects and rigid background, and optical flow network to estimate dense 2D per-pixel correspondence. These four types of information are integrated into a differentiable holistic 3D motion parser (HMP) to recover per-pixel 3D motion for both rigid background and moving objects. The proposed approach is trained using various losses to further reduce error in estimated geometry. To address the 3D motion confusion from monocular videos, stereo images are combined into joint training. Experiments on KITTI 2015 dataset demonstrate that the proposed approach outperforms other state-of-the-art algorithms and improves the consistency of estimated geometry, 3D motion, and moving object masks.",1
"Event-based cameras have shown great promise in a variety of situations where frame based cameras suffer, such as high speed motions and high dynamic range scenes. However, developing algorithms for event measurements requires a new class of hand crafted algorithms. Deep learning has shown great success in providing model free solutions to many problems in the vision community, but existing networks have been developed with frame based images in mind, and there does not exist the wealth of labeled data for events as there does for images for supervised training. To these points, we present EV-FlowNet, a novel self-supervised deep learning pipeline for optical flow estimation for event based cameras. In particular, we introduce an image based representation of a given event stream, which is fed into a self-supervised neural network as the sole input. The corresponding grayscale images captured from the same camera at the same time as the events are then used as a supervisory signal to provide a loss function at training time, given the estimated flow from the network. We show that the resulting network is able to accurately predict optical flow from events only in a variety of different scenes, with performance competitive to image based networks. This method not only allows for accurate estimation of dense optical flow, but also provides a framework for the transfer of other self-supervised methods to the event-based domain.",0
"Event-based cameras have demonstrated their potential in situations where frame-based cameras struggle due to high speed motions and high dynamic range scenes. However, developing algorithms for event measurements requires a new set of hand-crafted algorithms, which poses a challenge. Though deep learning has been successful in providing model-free solutions to many problems in the vision community, existing networks are designed for frame-based images. Moreover, there is a lack of labeled data for events for supervised training. To address these issues, we introduce EV-FlowNet, a novel self-supervised deep learning pipeline for optical flow estimation for event-based cameras. Our approach involves using an image-based representation of an event stream, which is then fed into a self-supervised neural network as the sole input. The corresponding grayscale images captured from the same camera at the same time as the events are used as a supervisory signal to provide a loss function at training time. The estimated flow from the network is then used to predict optical flow accurately from events only in different scenes, with performance comparable to image-based networks. This approach not only allows for accurate estimation of dense optical flow, but also provides a framework for transferring other self-supervised methods to the event-based domain.",1
"Occlusions play an important role in disparity and optical flow estimation, since matching costs are not available in occluded areas and occlusions indicate depth or motion boundaries. Moreover, occlusions are relevant for motion segmentation and scene flow estimation. In this paper, we present an efficient learning-based approach to estimate occlusion areas jointly with disparities or optical flow. The estimated occlusions and motion boundaries clearly improve over the state-of-the-art. Moreover, we present networks with state-of-the-art performance on the popular KITTI benchmark and good generic performance. Making use of the estimated occlusions, we also show improved results on motion segmentation and scene flow estimation.",0
"The estimation of occlusions is crucial in determining disparities and optical flow, as these areas lack matching costs and signify depth or motion boundaries. Furthermore, occlusions are significant in motion segmentation and scene flow estimation. This paper proposes an effective learning-based method to estimate occlusions in conjunction with disparities or optical flow. The estimated occlusions and motion boundaries surpass the current state-of-the-art. Additionally, our networks perform exceptionally well on the KITTI benchmark and have good generic performance. The use of estimated occlusions also leads to improved motion segmentation and scene flow estimation outcomes.",1
"The electroencephalography classifier is the most important component of brain-computer interface based systems. There are two major problems hindering the improvement of it. First, traditional methods do not fully exploit multimodal information. Second, large-scale annotated EEG datasets are almost impossible to acquire because biological data acquisition is challenging and quality annotation is costly. Herein, we propose a novel deep transfer learning approach to solve these two problems. First, we model cognitive events based on EEG data by characterizing the data using EEG optical flow, which is designed to preserve multimodal EEG information in a uniform representation. Second, we design a deep transfer learning framework which is suitable for transferring knowledge by joint training, which contains a adversarial network and a special loss function. The experiments demonstrate that our approach, when applied to EEG classification tasks, has many advantages, such as robustness and accuracy.",0
"The most critical element of brain-computer interface systems is the electroencephalography (EEG) classifier. However, there are two significant challenges that impede its advancement. Firstly, traditional methods fail to exploit multimodal information adequately. Secondly, it is almost impossible to obtain large-scale annotated EEG datasets due to the difficulties associated with biological data acquisition and expensive quality annotation. To address these problems, we propose a new approach to deep transfer learning. We model cognitive events based on EEG data by characterizing the data using EEG optical flow, which preserves multimodal EEG information in a uniform representation. Additionally, we design a deep transfer learning framework that facilitates knowledge transfer through joint training, which involves an adversarial network and a specific loss function. Our experiments show that our approach offers many benefits, including robustness and accuracy, when applied to EEG classification tasks.",1
"In interventional radiology, short video sequences of vein structure in motion are captured in order to help medical personnel identify vascular issues or plan intervention. Semantic segmentation can greatly improve the usefulness of these videos by indicating exact position of vessels and instruments, thus reducing the ambiguity. We propose a real-time segmentation method for these tasks, based on U-Net network trained in a Siamese architecture from automatically generated annotations. We make use of noisy low level binary segmentation and optical flow to generate multi class annotations that are successively improved in a multistage segmentation approach. We significantly improve the performance of a state of the art U-Net at the processing speeds of 90fps.",0
"To aid medical professionals in identifying vascular problems or devising treatment plans, interventional radiologists capture brief videos of vein structures in motion. Semantic segmentation can enhance the effectiveness of these videos by providing precise information on the location of vessels and instruments, thereby reducing any ambiguity. Our proposal is a segmentation technique that works in real-time and is based on a U-Net network trained using a Siamese architecture achieved from automatically generated annotations. We utilize low-level binary segmentation and optical flow to produce multi-class annotations that are progressively refined in a multi-stage segmentation approach. We significantly enhance the performance of the existing U-Net technology at processing speeds of 90fps.",1
"Interest point descriptors have fueled progress on almost every problem in computer vision. Recent advances in deep neural networks have enabled task-specific learned descriptors that outperform hand-crafted descriptors on many problems. We demonstrate that commonly used metric learning approaches do not optimally leverage the feature hierarchies learned in a Convolutional Neural Network (CNN), especially when applied to the task of geometric feature matching. While a metric loss applied to the deepest layer of a CNN, is often expected to yield ideal features irrespective of the task, in fact the growing receptive field as well as striding effects cause shallower features to be better at high precision matching tasks. We leverage this insight together with explicit supervision at multiple levels of the feature hierarchy for better regularization, to learn more effective descriptors in the context of geometric matching tasks. Further, we propose to use activation maps at different layers of a CNN, as an effective and principled replacement for the multi-resolution image pyramids often used for matching tasks. We propose concrete CNN architectures employing these ideas, and evaluate them on multiple datasets for 2D and 3D geometric matching as well as optical flow, demonstrating state-of-the-art results and generalization across datasets.",0
"Almost all computer vision problems have benefited from the use of interest point descriptors. While hand-crafted descriptors have been used in the past, recent developments in deep neural networks have resulted in task-specific learned descriptors that outperform their predecessors on many tasks. However, commonly used metric learning approaches do not sufficiently take advantage of the feature hierarchies learned in a Convolutional Neural Network (CNN), especially for geometric feature matching. Although a metric loss applied to the deepest layer of a CNN is assumed to produce optimal features, in reality, shallower features are better suited for high precision matching tasks due to the growing receptive field and striding effects. To improve regularization and learn more effective descriptors for geometric matching tasks, we utilize explicit supervision at multiple levels of the feature hierarchy and leverage this insight. We also propose using activation maps at different layers of a CNN as a replacement for multi-resolution image pyramids in matching tasks. We present specific CNN architectures incorporating these ideas and demonstrate their state-of-the-art performance and generalization across datasets in 2D and 3D geometric matching and optical flow.",1
"Applying image processing algorithms independently to each frame of a video often leads to undesired inconsistent results over time. Developing temporally consistent video-based extensions, however, requires domain knowledge for individual tasks and is unable to generalize to other applications. In this paper, we present an efficient end-to-end approach based on deep recurrent network for enforcing temporal consistency in a video. Our method takes the original unprocessed and per-frame processed videos as inputs to produce a temporally consistent video. Consequently, our approach is agnostic to specific image processing algorithms applied on the original video. We train the proposed network by minimizing both short-term and long-term temporal losses as well as the perceptual loss to strike a balance between temporal stability and perceptual similarity with the processed frames. At test time, our model does not require computing optical flow and thus achieves real-time speed even for high-resolution videos. We show that our single model can handle multiple and unseen tasks, including but not limited to artistic style transfer, enhancement, colorization, image-to-image translation and intrinsic image decomposition. Extensive objective evaluation and subject study demonstrate that the proposed approach performs favorably against the state-of-the-art methods on various types of videos.",0
"When image processing algorithms are applied independently to each frame of a video, inconsistent results may occur over time. While developing video-based extensions for temporal consistency requires domain knowledge for individual tasks, these extensions cannot be generalized to other applications. This paper introduces an efficient end-to-end approach for enforcing temporal consistency in a video using a deep recurrent network. The method takes both the original unprocessed and per-frame processed videos as inputs to produce a temporally consistent video, without relying on specific image processing algorithms applied to the original video. During training, the proposed network minimizes both short-term and long-term temporal losses, as well as the perceptual loss, to balance temporal stability and perceptual similarity with the processed frames. At test time, the model achieves real-time speed even for high-resolution videos without the need for computing optical flow. The approach can handle multiple and unseen tasks, such as artistic style transfer, enhancement, colorization, image-to-image translation, and intrinsic image decomposition. The proposed approach outperforms state-of-the-art methods on various types of videos, as demonstrated through extensive objective evaluation and subject study.",1
"Spatio-temporal representations in frame sequences play an important role in the task of action recognition. Previously, a method of using optical flow as a temporal information in combination with a set of RGB images that contain spatial information has shown great performance enhancement in the action recognition tasks. However, it has an expensive computational cost and requires two-stream (RGB and optical flow) framework. In this paper, we propose MFNet (Motion Feature Network) containing motion blocks which make it possible to encode spatio-temporal information between adjacent frames in a unified network that can be trained end-to-end. The motion block can be attached to any existing CNN-based action recognition frameworks with only a small additional cost. We evaluated our network on two of the action recognition datasets (Jester and Something-Something) and achieved competitive performances for both datasets by training the networks from scratch.",0
"The recognition of actions relies heavily on spatio-temporal representations found in frame sequences. An approach previously used to enhance performance in this area integrates temporal information through the use of optical flow, along with a set of RGB images that provide spatial information. However, this method is computationally expensive and requires a two-stream framework. This paper introduces MFNet, a Motion Feature Network that incorporates motion blocks to encode spatio-temporal information between adjacent frames in a single network that can be trained end-to-end. The motion block can be added to any existing CNN-based action recognition framework with minimal added cost. Our network was evaluated on two action recognition datasets (Jester and Something-Something), and we achieved competitive performance on both by training the networks from scratch.",1
"Despite many advances in deep-learning based semantic segmentation, performance drop due to distribution mismatch is often encountered in the real world. Recently, a few domain adaptation and active learning approaches have been proposed to mitigate the performance drop. However, very little attention has been made toward leveraging information in videos which are naturally captured in most camera systems. In this work, we propose to leverage ""motion prior"" in videos for improving human segmentation in a weakly-supervised active learning setting. By extracting motion information using optical flow in videos, we can extract candidate foreground motion segments (referred to as motion prior) potentially corresponding to human segments. We propose to learn a memory-network-based policy model to select strong candidate segments (referred to as strong motion prior) through reinforcement learning. The selected segments have high precision and are directly used to finetune the model. In a newly collected surveillance camera dataset and a publicly available UrbanStreet dataset, our proposed method improves the performance of human segmentation across multiple scenes and modalities (i.e., RGB to Infrared (IR)). Last but not least, our method is empirically complementary to existing domain adaptation approaches such that additional performance gain is achieved by combining our weakly-supervised active learning approach with domain adaptation approaches.",0
"In spite of significant advancements in semantic segmentation using deep learning, real-world scenarios often result in performance drops due to distribution mismatches. To address this issue, domain adaptation and active learning strategies have been proposed, but these have paid little attention to the valuable information available in videos captured by most camera systems. This paper proposes the utilization of ""motion prior"" in videos to improve human segmentation in weakly-supervised active learning settings. Optical flow is used to extract motion information, which identifies candidate foreground motion segments potentially corresponding to human segments. A memory-network-based policy model is proposed to select strong candidate segments through reinforcement learning, which are then used to finetune the model. The proposed methodology has been tested on a newly collected surveillance camera dataset and a publicly available UrbanStreet dataset, where it has exhibited improved performance in human segmentation across multiple scenes and modalities (i.e., RGB to Infrared (IR)). Furthermore, combining this weakly-supervised active learning approach with domain adaptation strategies has resulted in additional performance gains.",1
"Estimation of 3D motion in a dynamic scene from a temporal pair of images is a core task in many scene understanding problems. In real world applications, a dynamic scene is commonly captured by a moving camera (i.e., panning, tilting or hand-held), increasing the task complexity because the scene is observed from different view points. The main challenge is the disambiguation of the camera motion from scene motion, which becomes more difficult as the amount of rigidity observed decreases, even with successful estimation of 2D image correspondences. Compared to other state-of-the-art 3D scene flow estimation methods, in this paper we propose to \emph{learn} the rigidity of a scene in a supervised manner from a large collection of dynamic scene data, and directly infer a rigidity mask from two sequential images with depths. With the learned network, we show how we can effectively estimate camera motion and projected scene flow using computed 2D optical flow and the inferred rigidity mask. For training and testing the rigidity network, we also provide a new semi-synthetic dynamic scene dataset (synthetic foreground objects with a real background) and an evaluation split that accounts for the percentage of observed non-rigid pixels. Through our evaluation we show the proposed framework outperforms current state-of-the-art scene flow estimation methods in challenging dynamic scenes.",0
"The task of estimating 3D motion in a dynamic scene from a pair of images is crucial in various scene understanding problems. When dealing with real-world applications, a moving camera is often used to capture the dynamic scene, which increases the complexity of the task due to the various viewpoints of the scene. The primary difficulty arises in distinguishing the camera motion from the scene motion, which becomes more challenging when the level of rigidity in the scene decreases, even if 2D image correspondences are accurately estimated. In contrast to other 3D scene flow estimation methods, we propose a supervised learning approach to learn the scene's rigidity from a vast collection of dynamic scene data. Our method directly infers a rigidity mask from two sequential images with depths, enabling us to estimate camera motion and projected scene flow effectively. To train and evaluate the rigidity network, we introduce a new semi-synthetic dynamic scene dataset and an evaluation split that accounts for the percentage of observed non-rigid pixels. Our evaluation demonstrates that the proposed framework outperforms current state-of-the-art methods for scene flow estimation in challenging dynamic scenes.",1
"We use large amounts of unlabeled video to learn models for visual tracking without manual human supervision. We leverage the natural temporal coherency of color to create a model that learns to colorize gray-scale videos by copying colors from a reference frame. Quantitative and qualitative experiments suggest that this task causes the model to automatically learn to track visual regions. Although the model is trained without any ground-truth labels, our method learns to track well enough to outperform the latest methods based on optical flow. Moreover, our results suggest that failures to track are correlated with failures to colorize, indicating that advancing video colorization may further improve self-supervised visual tracking.",0
"Our approach involves utilizing a significant amount of unlabeled video data to develop visual tracking models without any manual supervision. We capitalize on the inherent temporal consistency of color to construct a model that can colorize gray-scale videos by imitating colors from a reference frame. Our quantitative and qualitative experiments indicate that this task enables the model to learn to track visual regions automatically. Despite being trained without any ground-truth labels, our method performs better than the latest optical flow-based techniques in tracking. Additionally, our findings suggest that unsuccessful tracking is related to unsuccessful colorization, implying that enhancing video colorization may further enhance self-supervised visual tracking.",1
"Classical computation of optical flow involves generic priors (regularizers) that capture rudimentary statistics of images, but not long-range correlations or semantics. On the other hand, fully supervised methods learn the regularity in the annotated data, without explicit regularization and with the risk of overfitting. We seek to learn richer priors on the set of possible flows that are statistically compatible with an image. Once the prior is learned in a supervised fashion, one can easily learn the full map to infer optical flow directly from two or more images, without any need for (additional) supervision. We introduce a novel architecture, called Conditional Prior Network (CPN), and show how to train it to yield a conditional prior. When used in conjunction with a simple optical flow architecture, the CPN beats all variational methods and all unsupervised learning-based ones using the same data term. It performs comparably to fully supervised ones, that however are fine-tuned to a particular dataset. Our method, on the other hand, performs well even when transferred between datasets.",0
"Optical flow computation in classical methods involves regularizers that capture basic image statistics, but not long-range correlations or semantics. However, fully supervised methods learn regularity in annotated data without explicit regularization, which can lead to overfitting. Our aim is to learn richer priors that are statistically compatible with possible flows from an image. By learning the prior in a supervised manner, we can infer optical flow directly from two or more images without additional supervision. We propose a new architecture, called Conditional Prior Network (CPN), to generate a conditional prior and show how to train it. When combined with a simple optical flow architecture, CPN outperforms all variational and unsupervised learning-based methods using the same data term. Our method performs as well as fully supervised methods that are fine-tuned to a specific dataset, but ours works well even when transferred between datasets.",1
"In this paper our objectives are, first, networks that can embed audio and visual inputs into a common space that is suitable for cross-modal retrieval; and second, a network that can localize the object that sounds in an image, given the audio signal. We achieve both these objectives by training from unlabelled video using only audio-visual correspondence (AVC) as the objective function. This is a form of cross-modal self-supervision from video.   To this end, we design new network architectures that can be trained for cross-modal retrieval and localizing the sound source in an image, by using the AVC task. We make the following contributions: (i) show that audio and visual embeddings can be learnt that enable both within-mode (e.g. audio-to-audio) and between-mode retrieval; (ii) explore various architectures for the AVC task, including those for the visual stream that ingest a single image, or multiple images, or a single image and multi-frame optical flow; (iii) show that the semantic object that sounds within an image can be localized (using only the sound, no motion or flow information); and (iv) give a cautionary tale on how to avoid undesirable shortcuts in the data preparation.",0
"The aim of this paper is twofold: firstly, to create networks that can effectively combine audio and visual inputs to enable cross-modal retrieval, and secondly, to develop a network that can determine the location of a sound source within an image using only the accompanying audio signal. To achieve these objectives, we use unlabelled video to train our networks with audio-visual correspondence (AVC) as the sole objective function, thereby employing a form of cross-modal self-supervision. In order to accomplish this, we have developed novel network architectures that can be trained to perform cross-modal retrieval and sound source localization through the AVC task. Our contributions include demonstrating the ability to learn audio and visual embeddings that facilitate both within-mode and between-mode retrieval, exploring various architectures for the AVC task, and showing that sound sources can be accurately localized within images using only the accompanying audio. We also provide guidance on how to avoid potential data preparation shortcuts that could compromise the effectiveness of our approach.",1
"This paper proposes the first non-flow-based deep framework for high dynamic range (HDR) imaging of dynamic scenes with large-scale foreground motions. In state-of-the-art deep HDR imaging, input images are first aligned using optical flows before merging, which are still error-prone due to occlusion and large motions. In stark contrast to flow-based methods, we formulate HDR imaging as an image translation problem without optical flows. Moreover, our simple translation network can automatically hallucinate plausible HDR details in the presence of total occlusion, saturation and under-exposure, which are otherwise almost impossible to recover by conventional optimization approaches. Our framework can also be extended for different reference images. We performed extensive qualitative and quantitative comparisons to show that our approach produces excellent results where color artifacts and geometric distortions are significantly reduced compared to existing state-of-the-art methods, and is robust across various inputs, including images without radiometric calibration.",0
"This paper introduces a novel deep framework for high dynamic range (HDR) imaging of dynamic scenes with large-scale foreground motions. Unlike existing deep HDR imaging methods that rely on optical flows for aligning input images, which are prone to errors due to occlusion and large motions, our approach formulates HDR imaging as an image translation problem without optical flows. Our translation network can automatically generate plausible HDR details even in the presence of total occlusion, saturation, and under-exposure, which are difficult to recover using conventional optimization techniques. Additionally, our framework can be extended for different reference images. We conducted extensive qualitative and quantitative comparisons to demonstrate that our approach yields superior results with fewer color artifacts and geometric distortions compared to existing state-of-the-art methods. Furthermore, our framework is robust across various inputs, including images without radiometric calibration.",1
"We propose a Spatiotemporal Sampling Network (STSN) that uses deformable convolutions across time for object detection in videos. Our STSN performs object detection in a video frame by learning to spatially sample features from the adjacent frames. This naturally renders the approach robust to occlusion or motion blur in individual frames. Our framework does not require additional supervision, as it optimizes sampling locations directly with respect to object detection performance. Our STSN outperforms the state-of-the-art on the ImageNet VID dataset and compared to prior video object detection methods it uses a simpler design, and does not require optical flow data for training.",0
"To detect objects in videos while accounting for occlusion and motion blur, we introduce the Spatiotemporal Sampling Network (STSN). The STSN leverages deformable convolutions across time to spatially sample features from adjacent frames for object detection in a given frame. Unlike other methods, our framework does not need additional supervision as it optimizes sampling locations directly for improved object detection performance. Additionally, our STSN achieves better results than the current state-of-the-art on the ImageNet VID dataset, despite employing a simpler design and not relying on optical flow data for training.",1
"Electroencephalography (EEG) has become the most significant input signal for brain computer interface (BCI) based systems. However, it is very difficult to obtain satisfactory classification accuracy due to traditional methods can not fully exploit multimodal information. Herein, we propose a novel approach to modeling cognitive events from EEG data by reducing it to a video classification problem, which is designed to preserve the multimodal information of EEG. In addition, optical flow is introduced to represent the variant information of EEG. We train a deep neural network (DNN) with convolutional neural network (CNN) and recurrent neural network (RNN) for the EEG classification task by using EEG video and optical flow. The experiments demonstrate that our approach has many advantages, such as more robustness and more accuracy in EEG classification tasks. According to our approach, we designed a mixed BCI-based rehabilitation support system to help stroke patients perform some basic operations.",0
"The use of electroencephalography (EEG) has become crucial in brain computer interface (BCI) systems, but the accuracy of classification is often unsatisfactory due to the inability of traditional methods to fully utilize multimodal information. To address this issue, we propose a novel approach that treats EEG data as a video classification problem to retain the multimodal information. We also incorporate optical flow to capture variant information in the EEG. Our approach trains a deep neural network (DNN) that combines convolutional neural network (CNN) and recurrent neural network (RNN) with EEG video and optical flow, resulting in a more robust and accurate EEG classification. We applied our approach to design a mixed BCI-based rehabilitation support system for stroke patients to perform basic operations.",1
"The optical flow of humans is well known to be useful for the analysis of human action. Given this, we devise an optical flow algorithm specifically for human motion and show that it is superior to generic flow methods. Designing a method by hand is impractical, so we develop a new training database of image sequences with ground truth optical flow. For this we use a 3D model of the human body and motion capture data to synthesize realistic flow fields. We then train a convolutional neural network to estimate human flow fields from pairs of images. Since many applications in human motion analysis depend on speed, and we anticipate mobile applications, we base our method on SpyNet with several modifications. We demonstrate that our trained network is more accurate than a wide range of top methods on held-out test data and that it generalizes well to real image sequences. When combined with a person detector/tracker, the approach provides a full solution to the problem of 2D human flow estimation. Both the code and the dataset are available for research.",0
"The analysis of human action is commonly aided by the optical flow of humans. To improve on generic flow methods, we have created an optical flow algorithm specifically for human motion. As it is not practical to design a method by hand, we have developed a new training database of image sequences with ground truth optical flow. Our approach involves using a 3D model of the human body and motion capture data to synthesize realistic flow fields. We then train a convolutional neural network to estimate human flow fields from pairs of images. To cater to the need for speed and mobile applications, we have based our method on SpyNet with several modifications. Our trained network has proven to be more accurate than other top methods on held-out test data and generalizes well to real image sequences. When combined with a person detector/tracker, our approach provides a comprehensive solution to the problem of 2D human flow estimation. The code and dataset are both available for research purposes.",1
"The general ability to analyze and classify the 3D kinematics of the human form is an essential step in the development of socially adept humanoid robots. A variety of different types of signals can be used by machines to represent and characterize actions such as RGB videos, infrared maps, and optical flow. In particular, skeleton sequences provide a natural 3D kinematic description of human motions and can be acquired in real time using RGB+D cameras. Moreover, skeleton sequences are generalizable to characterize the motions of both humans and humanoid robots. The Globally Optimal Reparameterization Algorithm (GORA) is a novel, recently proposed algorithm for signal alignment in which signals are reparameterized to a globally optimal universal standard timescale (UST). Here, we introduce a variant of GORA for humanoid action recognition with skeleton sequences, which we call GORA-S. We briefly review the algorithm's mathematical foundations and contextualize them in the problem of action recognition with skeleton sequences. Subsequently, we introduce GORA-S and discuss parameters and numerical techniques for its effective implementation. We then compare its performance with that of the DTW and FastDTW algorithms, in terms of computational efficiency and accuracy in matching skeletons. Our results show that GORA-S attains a complexity that is significantly less than that of any tested DTW method. In addition, it displays a favorable balance between speed and accuracy that remains invariant under changes in skeleton sampling frequency, lending it a degree of versatility that could make it well-suited for a variety of action recognition tasks.",0
"Developing socially adept humanoid robots requires the ability to analyze and classify the 3D kinematics of human movements. Machines can use various types of signals, such as RGB videos, infrared maps, and optical flow, to represent and characterize actions. Skeleton sequences provide a natural 3D kinematic description of human motions and can be obtained in real time using RGB+D cameras. They are also generalizable to both humans and humanoid robots. The Globally Optimal Reparameterization Algorithm (GORA) is a recent algorithm for signal alignment that reparameterizes signals to a globally optimal universal standard timescale (UST). In this study, we introduce GORA-S, a variant of GORA for recognizing humanoid actions with skeleton sequences. We explain the algorithm's mathematical foundations and its contextualization in the problem of action recognition. We then present GORA-S and discuss its parameters and numerical techniques for effective implementation. We compare its performance with that of the DTW and FastDTW algorithms in terms of computational efficiency and accuracy in matching skeletons. Our findings show that GORA-S is significantly less complex than any tested DTW method and achieves a favorable balance between speed and accuracy, which remains consistent regardless of skeleton sampling frequency. This versatility makes GORA-S suitable for various action recognition tasks.",1
"Optical flow, semantic segmentation, and surface normals represent different information modalities, yet together they bring better cues for scene understanding problems. In this paper, we study the influence between the three modalities: how one impacts on the others and their efficiency in combination. We employ a modular approach using a convolutional refinement network which is trained supervised but isolated from RGB images to enforce joint modality features. To assist the training process, we create a large-scale synthetic outdoor dataset that supports dense annotation of semantic segmentation, optical flow, and surface normals. The experimental results show positive influence among the three modalities, especially for objects' boundaries, region consistency, and scene structures.",0
"The combination of optical flow, semantic segmentation, and surface normals provides better cues for understanding scenes, despite each representing different information. Our paper explores the interplay between these modalities, examining how they impact one another and the benefits of their integration. To achieve this, we use a modular approach with a convolutional refinement network trained in isolation from RGB images to enforce joint modality features. To aid in training, we create a large-scale synthetic outdoor dataset with dense annotations of semantic segmentation, optical flow, and surface normals. Our experiments demonstrate that the three modalities positively influence each other, particularly in terms of objects' boundaries, region consistency, and scene structures.",1
"We propose a novel method for learning convolutional neural image representations without manual supervision. We use motion cues in the form of optical flow, to supervise representations of static images. The obvious approach of training a network to predict flow from a single image can be needlessly difficult due to intrinsic ambiguities in this prediction task. We instead propose a much simpler learning goal: embed pixels such that the similarity between their embeddings matches that between their optical flow vectors. At test time, the learned deep network can be used without access to video or flow information and transferred to tasks such as image classification, detection, and segmentation. Our method, which significantly simplifies previous attempts at using motion for self-supervision, achieves state-of-the-art results in self-supervision using motion cues, competitive results for self-supervision in general, and is overall state of the art in self-supervised pretraining for semantic image segmentation, as demonstrated on standard benchmarks.",0
"Our proposed method enables the acquisition of convolutional neural image representations without manual supervision. We leverage motion cues in the form of optical flow to supervise static image representations. The conventional approach of training a network to predict flow from a single image can be arduous due to inherent ambiguities in this prediction task. Instead, we suggest a simpler learning objective: embedding pixels such that their embeddings' similarity matches that of their optical flow vectors. The deep network learned can be applied during testing without access to video or flow information and transferred to tasks like image classification, detection, and segmentation. Our method streamlines previous attempts at motion self-supervision, achieving state-of-the-art results for self-supervision using motion cues, competitive outcomes for self-supervision in general, and overall state-of-the-art self-supervised pretraining for semantic image segmentation, as proven on standard benchmarks.",1
"Given two consecutive frames, video interpolation aims at generating intermediate frame(s) to form both spatially and temporally coherent video sequences. While most existing methods focus on single-frame interpolation, we propose an end-to-end convolutional neural network for variable-length multi-frame video interpolation, where the motion interpretation and occlusion reasoning are jointly modeled. We start by computing bi-directional optical flow between the input images using a U-Net architecture. These flows are then linearly combined at each time step to approximate the intermediate bi-directional optical flows. These approximate flows, however, only work well in locally smooth regions and produce artifacts around motion boundaries. To address this shortcoming, we employ another U-Net to refine the approximated flow and also predict soft visibility maps. Finally, the two input images are warped and linearly fused to form each intermediate frame. By applying the visibility maps to the warped images before fusion, we exclude the contribution of occluded pixels to the interpolated intermediate frame to avoid artifacts. Since none of our learned network parameters are time-dependent, our approach is able to produce as many intermediate frames as needed. We use 1,132 video clips with 240-fps, containing 300K individual video frames, to train our network. Experimental results on several datasets, predicting different numbers of interpolated frames, demonstrate that our approach performs consistently better than existing methods.",0
"Video interpolation involves generating intermediate frames between two consecutive frames to create a coherent video sequence that is both spatially and temporally consistent. While most current techniques focus on interpolating a single frame, we propose an end-to-end convolutional neural network that can interpolate multiple frames of variable length. Our approach combines motion interpretation and occlusion reasoning in a joint model. We first compute bi-directional optical flow between the input images using a U-Net architecture. However, these approximations are not always successful in regions that are not locally smooth and produce artifacts around motion boundaries. Thus, we use another U-Net to refine the approximated flow and predict soft visibility maps. Finally, we warp and linearly fuse the two input images to create each intermediate frame. By applying the visibility maps to the warped images before fusion, we eliminate the contribution of occluded pixels to the interpolated intermediate frame, thereby avoiding any artifacts. Since none of our learned network parameters are time-dependent, our approach can generate an unlimited number of intermediate frames. We trained our network on 1,132 video clips with 240-fps, containing 300K individual video frames and achieved consistently better results than existing methods when predicting different numbers of interpolated frames on several datasets.",1
"Real-time moving object detection in unconstrained scenes is a difficult task due to dynamic background, changing foreground appearance and limited computational resource. In this paper, an optical flow based moving object detection framework is proposed to address this problem. We utilize homography matrixes to online construct a background model in the form of optical flow. When judging out moving foregrounds from scenes, a dual-mode judge mechanism is designed to heighten the system's adaptation to challenging situations. In experiment part, two evaluation metrics are redefined for more properly reflecting the performance of methods. We quantitatively and qualitatively validate the effectiveness and feasibility of our method with videos in various scene conditions. The experimental results show that our method adapts itself to different situations and outperforms the state-of-the-art methods, indicating the advantages of optical flow based methods.",0
"Detecting moving objects in real-time in unconstrained scenes is a challenging undertaking due to factors such as dynamic background, altering foreground appearance, and limited computational resources. To tackle this issue, we propose an optical flow-based framework for detecting moving objects. This framework employs homography matrices to construct an online background model in the form of optical flow. To improve the system's ability to adapt to difficult situations when identifying moving foregrounds in scenes, we design a dual-mode judge mechanism. In the experimentation phase, we redefine two evaluation metrics to more accurately reflect method performance. We demonstrate the effectiveness and feasibility of our approach through quantitative and qualitative analysis of videos captured in various scene conditions. Our experimental results indicate that our method is adaptable to different situations and surpasses state-of-the-art methods, highlighting the advantages of optical flow-based approaches.",1
"Motion representation plays a vital role in human action recognition in videos. In this study, we introduce a novel compact motion representation for video action recognition, named Optical Flow guided Feature (OFF), which enables the network to distill temporal information through a fast and robust approach. The OFF is derived from the definition of optical flow and is orthogonal to the optical flow. The derivation also provides theoretical support for using the difference between two frames. By directly calculating pixel-wise spatiotemporal gradients of the deep feature maps, the OFF could be embedded in any existing CNN based video action recognition framework with only a slight additional cost. It enables the CNN to extract spatiotemporal information, especially the temporal information between frames simultaneously. This simple but powerful idea is validated by experimental results. The network with OFF fed only by RGB inputs achieves a competitive accuracy of 93.3% on UCF-101, which is comparable with the result obtained by two streams (RGB and optical flow), but is 15 times faster in speed. Experimental results also show that OFF is complementary to other motion modalities such as optical flow. When the proposed method is plugged into the state-of-the-art video action recognition framework, it has 96:0% and 74:2% accuracy on UCF-101 and HMDB-51 respectively. The code for this project is available at https://github.com/kevin-ssy/Optical-Flow-Guided-Feature.",0
"The role of motion representation in human action recognition within videos is of great importance. In this study, a new and compact motion representation for video action recognition is introduced, called Optical Flow guided Feature (OFF). This approach enables the network to distill temporal information quickly and reliably. The OFF is based on the definition of optical flow and is orthogonal to it, providing theoretical support for using the difference between two frames. By calculating pixel-wise spatiotemporal gradients of deep feature maps, the OFF can be easily incorporated into any existing CNN-based video action recognition framework with only marginal additional cost. This allows the CNN to extract spatiotemporal information, especially the temporal information between frames, simultaneously. The experimental results validate this simple yet powerful idea, with the network achieving a competitive accuracy of 93.3% on UCF-101 when fed only by RGB inputs. This result is comparable to the outcome obtained by two streams (RGB and optical flow) but is 15 times faster in speed. The study also shows that the OFF is complementary to other motion modalities such as optical flow. When the proposed method is plugged into the state-of-the-art video action recognition framework, it achieves 96.0% and 74.2% accuracy on UCF-101 and HMDB-51, respectively. The code for this project is available at https://github.com/kevin-ssy/Optical-Flow-Guided-Feature.",1
"In this paper, we present supervision-by-registration, an unsupervised approach to improve the precision of facial landmark detectors on both images and video. Our key observation is that the detections of the same landmark in adjacent frames should be coherent with registration, i.e., optical flow. Interestingly, the coherency of optical flow is a source of supervision that does not require manual labeling, and can be leveraged during detector training. For example, we can enforce in the training loss function that a detected landmark at frame$_{t-1}$ followed by optical flow tracking from frame$_{t-1}$ to frame$_t$ should coincide with the location of the detection at frame$_t$. Essentially, supervision-by-registration augments the training loss function with a registration loss, thus training the detector to have output that is not only close to the annotations in labeled images, but also consistent with registration on large amounts of unlabeled videos. End-to-end training with the registration loss is made possible by a differentiable Lucas-Kanade operation, which computes optical flow registration in the forward pass, and back-propagates gradients that encourage temporal coherency in the detector. The output of our method is a more precise image-based facial landmark detector, which can be applied to single images or video. With supervision-by-registration, we demonstrate (1) improvements in facial landmark detection on both images (300W, ALFW) and video (300VW, Youtube-Celebrities), and (2) significant reduction of jittering in video detections.",0
"The aim of this paper is to introduce a new approach called supervision-by-registration that enhances the accuracy of facial landmark detectors in images and videos. The approach utilizes the coherency of optical flow as a source of supervision without the need for manual labeling. The key idea is to ensure that the detections of the same facial landmark in adjacent frames are consistent with registration. This is achieved by incorporating a registration loss into the training loss function, which encourages temporal coherency in the detector. The registration loss is enabled by a differentiable Lucas-Kanade operation that computes optical flow registration in the forward pass and back-propagates gradients. The output of this approach is a more precise facial landmark detector that can be applied to both images and videos. The results of the experiments conducted demonstrate significant improvements in facial landmark detection on both images and videos, as well as a reduction in jittering in video detections.",1
"This paper addresses spatio-temporal localization of human actions in video. In order to localize actions in time, we propose a recurrent localization network (RecLNet) designed to model the temporal structure of actions on the level of person tracks. Our model is trained to simultaneously recognize and localize action classes in time and is based on two layer gated recurrent units (GRU) applied separately to two streams, i.e. appearance and optical flow streams. When used together with state-of-the-art person detection and tracking, our model is shown to improve substantially spatio-temporal action localization in videos. The gain is shown to be mainly due to improved temporal localization. We evaluate our method on two recent datasets for spatio-temporal action localization, UCF101-24 and DALY, demonstrating a significant improvement of the state of the art.",0
"The focus of this paper is on the identification of the time and location of human actions in videos. To achieve this, we introduce a recurrent localization network (RecLNet) that has been specifically designed to capture the temporal structure of actions at the level of person tracks. Our model is trained to simultaneously recognize and pinpoint action classes in time, utilizing two-layer gated recurrent units (GRU) that are applied separately to appearance and optical flow streams. When combined with cutting-edge person detection and tracking techniques, our model considerably enhances spatio-temporal action localization in videos, primarily due to improved temporal localization. We test our approach on two recent datasets for spatio-temporal action localization, UCF101-24 and DALY, and demonstrate a significant improvement over previous state-of-the-art methods.",1
"De-fencing is to eliminate the captured fence on an image or a video, providing a clear view of the scene. It has been applied for many purposes including assisting photographers and improving the performance of computer vision algorithms such as object detection and recognition. However, the state-of-the-art de-fencing methods have limited performance caused by the difficulty of fence segmentation and also suffer from the motion of the camera or objects. To overcome these problems, we propose a novel method consisting of segmentation using convolutional neural networks and a fast/robust recovery algorithm. The segmentation algorithm using convolutional neural network achieves significant improvement in the accuracy of fence segmentation. The recovery algorithm using optical flow produces plausible de-fenced images and videos. The proposed method is experimented on both our diverse and complex dataset and publicly available datasets. The experimental results demonstrate that the proposed method achieves the state-of-the-art performance for both segmentation and content recovery.",0
"De-fencing refers to the removal of fences from images or videos in order to obtain a clear view of the scene. This technique has been employed for various purposes, including aiding photographers and enhancing the performance of object detection and recognition algorithms in computer vision. However, current de-fencing methods have limited efficacy due to difficulties in fence segmentation, as well as camera or object motion. To address these challenges, we propose a new approach that incorporates convolutional neural networks for segmentation and a fast and robust recovery algorithm. Our segmentation algorithm significantly improves fence segmentation accuracy, while the recovery algorithm, which utilizes optical flow, produces realistic de-fenced images and videos. We validate our method using both our own diverse and complex datasets and publicly available datasets, and the experimental results demonstrate that our approach outperforms existing methods in both segmentation and content recovery.",1
"We present a compact but effective CNN model for optical flow, called PWC-Net. PWC-Net has been designed according to simple and well-established principles: pyramidal processing, warping, and the use of a cost volume. Cast in a learnable feature pyramid, PWC-Net uses the cur- rent optical flow estimate to warp the CNN features of the second image. It then uses the warped features and features of the first image to construct a cost volume, which is processed by a CNN to estimate the optical flow. PWC-Net is 17 times smaller in size and easier to train than the recent FlowNet2 model. Moreover, it outperforms all published optical flow methods on the MPI Sintel final pass and KITTI 2015 benchmarks, running at about 35 fps on Sintel resolution (1024x436) images. Our models are available on https://github.com/NVlabs/PWC-Net.",0
"Introducing PWC-Net, a CNN model for optical flow that is both compact and effective. The model has been designed based on well-established principles, including pyramidal processing, warping, and the use of a cost volume. PWC-Net utilizes a learnable feature pyramid, which enables it to warp the CNN features of the second image using the current optical flow estimate. It then combines the warped features with those of the first image to create a cost volume that is analyzed by a CNN in order to estimate the optical flow. Compared to the recent FlowNet2 model, PWC-Net is 17 times smaller and easier to train, while still outperforming all other published optical flow methods on the MPI Sintel final pass and KITTI 2015 benchmarks. The model runs at a speed of approximately 35 fps on Sintel resolution images (1024x436) and is available for download on https://github.com/NVlabs/PWC-Net.",1
"This note describes the details of our solution to the dense-captioning events in videos task of ActivityNet Challenge 2018. Specifically, we solve this problem with a two-stage way, i.e., first temporal event proposal and then sentence generation. For temporal event proposal, we directly leverage the three-stage workflow in [13, 16]. For sentence generation, we capitalize on LSTM-based captioning framework with temporal attention mechanism (dubbed as LSTM-T). Moreover, the input visual sequence to the LSTM-based video captioning model is comprised of RGB and optical flow images. At inference, we adopt a late fusion scheme to fuse the two LSTM-based captioning models for sentence generation.",0
"Our approach to the dense-captioning events in videos task for the ActivityNet Challenge 2018 is detailed in this note. We employ a two-stage method consisting of temporal event proposal followed by sentence generation. Our temporal event proposal utilizes the three-stage workflow from [13, 16]. For sentence generation, we utilize the LSTM-based captioning framework with temporal attention mechanism, known as LSTM-T. Additionally, we use both RGB and optical flow images as input to the LSTM-based video captioning model. During inference, we use a late fusion scheme to combine the outputs of the two LSTM-based captioning models for sentence generation.",1
"Deep learning is ubiquitous across many areas areas of computer vision. It often requires large scale datasets for training before being fine-tuned on small-to-medium scale problems. Activity, or, in other words, action recognition, is one of many application areas of deep learning. While there exist many Convolutional Neural Network architectures that work with the RGB and optical flow frames, training on the time sequences of 3D body skeleton joints is often performed via recurrent networks such as LSTM.   In this paper, we propose a new representation which encodes sequences of 3D body skeleton joints in texture-like representations derived from mathematically rigorous kernel methods. Such a representation becomes the first layer in a standard CNN network e.g., ResNet-50, which is then used in the supervised domain adaptation pipeline to transfer information from the source to target dataset. This lets us leverage the available Kinect-based data beyond training on a single dataset and outperform simple fine-tuning on any two datasets combined in a naive manner. More specifically, in this paper we utilize the overlapping classes between datasets. We associate datapoints of the same class via so-called commonality, known from the supervised domain adaptation. We demonstrate state-of-the-art results on three publicly available benchmarks.",0
"Deep learning is widely used in various computer vision applications and is usually trained on large datasets before being fine-tuned for smaller-scale tasks. Action recognition, or activity, is just one of the many areas where deep learning is applied. While existing Convolutional Neural Network architectures work with RGB and optical flow frames, recurrent networks like LSTM are used for training on time sequences of 3D body skeleton joints. This paper presents a new representation that encodes sequences of 3D body skeleton joints using texture-like representations derived from kernel methods. This representation is used as the first layer in a standard CNN network, such as ResNet-50, in a supervised domain adaptation pipeline that transfers information from the source to target dataset. This approach allows for the use of Kinect-based data beyond a single dataset and outperforms simple fine-tuning on any two datasets combined. The method utilizes overlapping classes between datasets and associates data points of the same class through commonality. The proposed approach achieves state-of-the-art results on three publicly available benchmarks.",1
"We consider the problem of learning to play first-person shooter (FPS) video games using raw screen images as observations and keyboard inputs as actions. The high-dimensionality of the observations in this type of applications leads to prohibitive needs of training data for model-free methods, such as the deep Q-network (DQN), and its recurrent variant DRQN. Thus, recent works focused on learning low-dimensional representations that may reduce the need for data. This paper presents a new and efficient method for learning such representations. Salient segments of consecutive frames are detected from their optical flow, and clustered based on their feature descriptors. The clusters typically correspond to different discovered categories of objects. Segments detected in new frames are then classified based on their nearest clusters. Because only a few categories are relevant to a given task, the importance of a category is defined as the correlation between its occurrence and the agent's performance. The result is encoded as a vector indicating objects that are in the frame and their locations, and used as a side input to DRQN. Experiments on the game Doom provide a good evidence for the benefit of this approach.",0
"This article addresses the challenge of learning to play first-person shooter (FPS) video games by utilizing raw screen images as observations and keyboard inputs as actions. However, the high dimensionality of these observations requires a large amount of data for model-free methods like deep Q-network (DQN) and its recurrent counterpart, DRQN. Consequently, recent research has focused on developing low-dimensional representations that can reduce the need for data. To this end, the authors propose a novel and efficient technique that involves detecting important segments of consecutive frames using optical flow and clustering them based on their feature descriptors. These clusters correspond to different categories of objects. The method then classifies new segments based on their nearest clusters, considering only a few relevant categories that are associated with the agent's performance. The result is a vector that encodes the objects in the frame and their locations, which is used as a side input to DRQN. The authors demonstrate the effectiveness of this approach through experiments conducted on the game Doom.",1
"In this paper, we propose to improve the traditional use of RNNs by employing a many to many model for video classification. We analyze the importance of modeling spatial layout and temporal encoding for daily living action recognition. Many RGB methods focus only on short term temporal information obtained from optical flow. Skeleton based methods on the other hand show that modeling long term skeleton evolution improves action recognition accuracy. In this work, we propose a deep-temporal LSTM architecture which extends standard LSTM and allows better encoding of temporal information. In addition, we propose to fuse 3D skeleton geometry with deep static appearance. We validate our approach on public available CAD60, MSRDailyActivity3D and NTU-RGB+D, achieving competitive performance as compared to the state-of-the art.",0
"The aim of this paper is to enhance the conventional use of RNNs for video classification by utilizing a many-to-many model. The focus is on determining the significance of spatial layout and temporal encoding in recognizing daily living actions. While RGB techniques concentrate on short-term temporal information obtained from optical flow, skeleton-based methods illustrate that long-term skeleton evolution modeling improves the accuracy of action recognition. To this end, a deep-temporal LSTM architecture that extends the standard LSTM and enables better encoding of temporal information is proposed. Additionally, 3D skeleton geometry is fused with deep static appearance. The proposed method is assessed on publicly available datasets, including CAD60, MSRDailyActivity3D, and NTU-RGB+D, and achieves performance comparable to state-of-the-art methods.",1
"In this paper, we introduce our submissions for the tasks of trimmed activity recognition (Kinetics) and trimmed event recognition (Moments in Time) for Activitynet Challenge 2018. In the two tasks, non-local neural networks and temporal segment networks are implemented as our base models. Multi-modal cues such as RGB image, optical flow and acoustic signal have also been used in our method. We also propose new non-local-based models for further improvement on the recognition accuracy. The final submissions after ensembling the models achieve 83.5% top-1 accuracy and 96.8% top-5 accuracy on the Kinetics validation set, 35.81% top-1 accuracy and 62.59% top-5 accuracy on the MIT validation set.",0
"The purpose of this paper is to present our entries for the trimmed activity recognition (Kinetics) and trimmed event recognition (Moments in Time) tasks in the Activitynet Challenge 2018. To achieve our goals, we utilized non-local neural networks and temporal segment networks as our primary models and incorporated multi-modal cues such as RGB image, optical flow, and acoustic signal. Additionally, we proposed novel non-local-based models to further enhance recognition accuracy. Our final submissions, which combined the various models, resulted in top-1 accuracy of 83.5% and top-5 accuracy of 96.8% on the Kinetics validation set and top-1 accuracy of 35.81% and top-5 accuracy of 62.59% on the MIT validation set.",1
"Temporal coherence is a valuable source of information in the context of optical flow estimation. However, finding a suitable motion model to leverage this information is a non-trivial task. In this paper we propose an unsupervised online learning approach based on a convolutional neural network (CNN) that estimates such a motion model individually for each frame. By relating forward and backward motion these learned models not only allow to infer valuable motion information based on the backward flow, they also help to improve the performance at occlusions, where a reliable prediction is particularly useful. Moreover, our learned models are spatially variant and hence allow to estimate non-rigid motion per construction. This, in turns, allows to overcome the major limitation of recent rigidity-based approaches that seek to improve the estimation by incorporating additional stereo/SfM constraints. Experiments demonstrate the usefulness of our new approach. They not only show a consistent improvement of up to 27% for all major benchmarks (KITTI 2012, KITTI 2015, MPI Sintel) compared to a baseline without prediction, they also show top results for the MPI Sintel benchmark -- the one of the three benchmarks that contains the largest amount of non-rigid motion.",0
"Optical flow estimation can benefit greatly from temporal coherence. However, finding an appropriate motion model to utilize this information is a challenging task. This study proposes an unsupervised online learning method based on a convolutional neural network (CNN) to individually estimate a motion model for each frame. These models relate forward and backward motion, which not only allows for valuable motion information to be inferred based on backward flow, but also improves performance at occlusions where reliable prediction is particularly useful. Additionally, our learned models are spatially variant, enabling estimation of non-rigid motion, overcoming the major limitation of recent rigidity-based approaches that require additional stereo/SfM constraints for improvement. Our experiments demonstrate the usefulness of this new approach, showing consistent improvements of up to 27% for all major benchmarks (KITTI 2012, KITTI 2015, MPI Sintel) compared to a baseline without prediction, and top results for the MPI Sintel benchmark, which includes the largest amount of non-rigid motion among the three benchmarks.",1
"We present an algorithm (SOFAS) to estimate the optical flow of events generated by a dynamic vision sensor (DVS). Where traditional cameras produce frames at a fixed rate, DVSs produce asynchronous events in response to intensity changes with a high temporal resolution. Our algorithm uses the fact that events are generated by edges in the scene to not only estimate the optical flow but also to simultaneously segment the image into objects which are travelling at the same velocity. This way it is able to avoid the aperture problem which affects other implementations such as Lucas-Kanade. Finally, we show that SOFAS produces more accurate results than traditional optic flow algorithms.",0
"Introducing SOFAS, an algorithm designed to determine the optical flow of dynamic vision sensor (DVS)-generated events. Unlike traditional cameras, DVSs produce asynchronous events in response to intensity changes, providing a high temporal resolution. Our algorithm utilizes the fact that events are generated by edges in the scene to not only estimate optical flow but also segment the image into objects moving at the same speed. This approach circumvents issues such as the aperture problem found in other methods like Lucas-Kanade. Ultimately, our results demonstrate that SOFAS offers higher accuracy than traditional optic flow algorithms.",1
"Making predictions of future frames is a critical challenge in autonomous driving research. Most of the existing methods for video prediction attempt to generate future frames in simple and fixed scenes. In this paper, we propose a novel and effective optical flow conditioned method for the task of video prediction with an application to complex urban scenes. In contrast with previous work, the prediction model only requires video sequences and optical flow sequences for training and testing. Our method uses the rich spatial-temporal features in video sequences. The method takes advantage of the motion information extracting from optical flow maps between neighbor images as well as previous images. Empirical evaluations on the KITTI dataset and the Cityscapes dataset demonstrate the effectiveness of our method.",0
"The challenge of predicting future frames is crucial in the field of autonomous driving research. The current techniques for video prediction aim to generate future frames in straightforward and unchanging environments. However, this paper presents a new and efficient approach that relies on optical flow to predict videos in complex urban settings. Unlike previous methods, this approach only requires video and optical flow sequences for both training and testing. By utilizing the spatial-temporal features in video sequences, our method extracts motion information from optical flow maps between adjacent and previous images. Our approach has been evaluated on the KITTI and Cityscapes datasets, and the results demonstrate its effectiveness.",1
"Existing methods to recognize actions in static images take the images at their face value, learning the appearances---objects, scenes, and body poses---that distinguish each action class. However, such models are deprived of the rich dynamic structure and motions that also define human activity. We propose an approach that hallucinates the unobserved future motion implied by a single snapshot to help static-image action recognition. The key idea is to learn a prior over short-term dynamics from thousands of unlabeled videos, infer the anticipated optical flow on novel static images, and then train discriminative models that exploit both streams of information. Our main contributions are twofold. First, we devise an encoder-decoder convolutional neural network and a novel optical flow encoding that can translate a static image into an accurate flow map. Second, we show the power of hallucinated flow for recognition, successfully transferring the learned motion into a standard two-stream network for activity recognition. On seven datasets, we demonstrate the power of the approach. It not only achieves state-of-the-art accuracy for dense optical flow prediction, but also consistently enhances recognition of actions and dynamic scenes.",0
"The current methods for identifying actions in still images only rely on the visual features of the image, such as objects, scenes, and body positions, to distinguish between different types of actions. However, this approach fails to take into account the dynamic nature of human activity. In order to address this issue, we propose a new technique that extrapolates the future motion of a single image to aid in identifying actions. Our method involves learning the short-term dynamics from unlabeled videos and using this information to predict the optical flow of new static images. We then combine this information with discriminative models to improve action recognition accuracy. Our approach consists of two main contributions: a convolutional neural network and a novel optical flow encoding that accurately translates static images into flow maps. We demonstrate the effectiveness of our approach on seven datasets, achieving state-of-the-art accuracy for optical flow prediction and improving recognition of actions and dynamic scenes.",1
"In crowded scenes, detection and localization of abnormal behaviors is challenging in that high-density people make object segmentation and tracking extremely difficult. We associate the optical flows of multiple frames to capture short-term trajectories and introduce the histogram-based shape descriptor referred to as shape contexts to describe such short-term trajectories. Furthermore, we propose a K-NN similarity-based statistical model to detect anomalies over time and space, which is an unsupervised one-class learning algorithm requiring no clustering nor any prior assumption. Firstly, we retrieve the K-NN samples from the training set in regard to the testing sample, and then use the similarities between every pair of the K-NN samples to construct a Gaussian model. Finally, the probabilities of the similarities from the testing sample to the K-NN samples under the Gaussian model are calculated in the form of a joint probability. Abnormal events can be detected by judging whether the joint probability is below predefined thresholds in terms of time and space, separately. Such a scheme can adapt to the whole scene, since the probability computed as such is not affected by motion distortions arising from perspective distortion. We conduct experiments on real-world surveillance videos, and the results demonstrate that the proposed method can reliably detect and locate the abnormal events in the video sequences, outperforming the state-of-the-art approaches.",0
"Detecting and localizing abnormal behaviors in crowded scenes can be difficult due to the high density of people, making object segmentation and tracking challenging. To address this issue, we use the optical flows of multiple frames to capture short-term trajectories and introduce a histogram-based shape descriptor called shape contexts. We also propose an unsupervised one-class learning algorithm, the K-NN similarity-based statistical model, to detect anomalies over time and space without the need for clustering or prior assumptions. The algorithm retrieves K-NN samples from the training set and constructs a Gaussian model using similarities between each pair of samples. The joint probability of the similarities from the testing sample to the K-NN samples is calculated, and abnormal events are detected by comparing the joint probability to predefined thresholds in terms of time and space. This approach is not affected by motion distortions arising from perspective distortion and can adapt to the entire scene. Experiments on real-world surveillance videos show that our proposed method outperforms state-of-the-art approaches in reliably detecting and locating abnormal events in video sequences.",1
"This research mainly emphasizes on traffic detection thus essentially involving object detection and classification. The particular work discussed here is motivated from unsatisfactory attempts of re-using well known pre-trained object detection networks for domain specific data. In this course, some trivial issues leading to prominent performance drop are identified and ways to resolve them are discussed. For example, some simple yet relevant tricks regarding data collection and sampling prove to be very beneficial. Also, introducing a blur net to deal with blurred real time data is another important factor promoting performance elevation. We further study the neural network design issues for beneficial object classification and involve shared, region-independent convolutional features. Adaptive learning rates to deal with saddle points are also investigated and an average covariance matrix based pre-conditioned approach is proposed. We also introduce the use of optical flow features to accommodate orientation information. Experimental results demonstrate that this results in a steady rise in the performance rate.",0
"This research focuses on traffic detection, which involves object detection and classification. The study aims to improve the use of pre-trained object detection networks for domain-specific data. The research identifies common issues that lead to a drop in performance and proposes solutions, such as tricks for data collection and sampling. To handle blurred real-time data, a blur net is introduced, which significantly improves performance. The study also explores neural network design issues for object classification, including shared, region-independent convolutional features and adaptive learning rates to deal with saddle points. To accommodate orientation information, the study introduces the use of optical flow features. Experimental results show a steady increase in performance rate.",1
Optical flow estimation with convolutional neural networks (CNNs) has recently solved various tasks of computer vision successfully. In this paper we adapt a state-of-the-art approach for optical flow estimation to omnidirectional images. We investigate CNN architectures to determine high motion variations caused by the geometry of fish-eye images. Further we determine the qualitative influence of texture on the non-rigid object to the motion vectors. For evaluation of the results we create ground truth motion fields synthetically. The ground truth contains cubes with static background. We test variations of pre-trained FlowNet 2.0 architectures by indicating common error metrics. We generate competitive results for the motion of the foreground with inhomogeneous texture on the moving object.,0
"Recently, convolutional neural networks (CNNs) have been successful in various computer vision tasks for optical flow estimation. This study adapts a cutting-edge CNN approach for estimating optical flow to omnidirectional images. The study explores CNN architectures to determine high motion variations caused by the fish-eye image geometry. Additionally, the study examines the impact of texture on non-rigid objects on motion vectors. The study evaluates the results using synthetically-created ground truth motion fields, which consist of cubes with a static background. By applying common error metrics, the study tests different variations of pre-trained FlowNet 2.0 architectures and achieves competitive results for the motion of the foreground with inhomogeneous texture on the moving object.",1
"The recent advances in Deep Convolutional Neural Networks (DCNNs) have shown extremely good results for video human action classification, however, action detection is still a challenging problem. The current action detection approaches follow a complex pipeline which involves multiple tasks such as tube proposals, optical flow, and tube classification. In this work, we present a more elegant solution for action detection based on the recently developed capsule network. We propose a 3D capsule network for videos, called VideoCapsuleNet: a unified network for action detection which can jointly perform pixel-wise action segmentation along with action classification. The proposed network is a generalization of capsule network from 2D to 3D, which takes a sequence of video frames as input. The 3D generalization drastically increases the number of capsules in the network, making capsule routing computationally expensive. We introduce capsule-pooling in the convolutional capsule layer to address this issue which makes the voting algorithm tractable. The routing-by-agreement in the network inherently models the action representations and various action characteristics are captured by the predicted capsules. This inspired us to utilize the capsules for action localization and the class-specific capsules predicted by the network are used to determine a pixel-wise localization of actions. The localization is further improved by parameterized skip connections with the convolutional capsule layers and the network is trained end-to-end with a classification as well as localization loss. The proposed network achieves sate-of-the-art performance on multiple action detection datasets including UCF-Sports, J-HMDB, and UCF-101 (24 classes) with an impressive ~20% improvement on UCF-101 and ~15% improvement on J-HMDB in terms of v-mAP scores.",0
"While Deep Convolutional Neural Networks (DCNNs) have demonstrated remarkable success in video human action classification, action detection remains a challenging task. The existing approaches for action detection are complex, involving multiple tasks such as tube proposals, optical flow, and tube classification. To simplify this process, we propose a more elegant solution based on a 3D capsule network called VideoCapsuleNet, which can jointly perform pixel-wise action segmentation and action classification. This network is a generalization of the capsule network from 2D to 3D and takes a sequence of video frames as input. However, the 3D generalization drastically increases the number of capsules in the network, making capsule routing computationally expensive. We address this issue by introducing capsule-pooling in the convolutional capsule layer to make the voting algorithm tractable. By utilizing the capsules for action localization, we achieve state-of-the-art performance on multiple action detection datasets, including UCF-Sports, J-HMDB, and UCF-101 (24 classes), with a significant improvement in v-mAP scores. The proposed network is trained end-to-end with a classification and localization loss and further improved by parameterized skip connections with the convolutional capsule layers.",1
"Good temporal representations are crucial for video understanding, and the state-of-the-art video recognition framework is based on two-stream networks. In such framework, besides the regular ConvNets responsible for RGB frame inputs, a second network is introduced to handle the temporal representation, usually the optical flow (OF). However, OF or other task-oriented flow is computationally costly, and is thus typically pre-computed. Critically, this prevents the two-stream approach from being applied to reinforcement learning (RL) applications such as video game playing, where the next state depends on current state and action choices. Inspired by the early vision systems of mammals and insects, we propose a fast event-driven representation (EDR) that models several major properties of early retinal circuits: (1) logarithmic input response, (2) multi-timescale temporal smoothing to filter noise, and (3) bipolar (ON/OFF) pathways for primitive event detection[12]. Trading off the directional information for fast speed (> 9000 fps), EDR en-ables fast real-time inference/learning in video applications that require interaction between an agent and the world such as game-playing, virtual robotics, and domain adaptation. In this vein, we use EDR to demonstrate performance improvements over state-of-the-art reinforcement learning algorithms for Atari games, something that has not been possible with pre-computed OF. Moreover, with UCF-101 video action recognition experiments, we show that EDR performs near state-of-the-art in accuracy while achieving a 1,500x speedup in input representation processing, as compared to optical flow.",0
"The ability to accurately represent time in video is crucial for understanding it, and two-stream networks are currently the most advanced framework for video recognition. These networks use regular ConvNets for RGB frames and a second network, often based on optical flow, to handle temporal representation. However, this approach is computationally expensive and precludes its use in reinforcement learning applications, which require rapid interaction between an agent and its environment. To address this, we propose an event-driven representation (EDR) inspired by early vision systems in mammals and insects, which enables fast real-time inference/learning in video applications that require interaction. Using EDR, we demonstrate performance improvements over state-of-the-art reinforcement learning algorithms for Atari games and near state-of-the-art accuracy for UCF-101 video action recognition experiments, while achieving a 1,500x speedup in input representation processing compared to optical flow.",1
"FlowNet2, the state-of-the-art convolutional neural network (CNN) for optical flow estimation, requires over 160M parameters to achieve accurate flow estimation. In this paper we present an alternative network that outperforms FlowNet2 on the challenging Sintel final pass and KITTI benchmarks, while being 30 times smaller in the model size and 1.36 times faster in the running speed. This is made possible by drilling down to architectural details that might have been missed in the current frameworks: (1) We present a more effective flow inference approach at each pyramid level through a lightweight cascaded network. It not only improves flow estimation accuracy through early correction, but also permits seamless incorporation of descriptor matching in our network. (2) We present a novel flow regularization layer to ameliorate the issue of outliers and vague flow boundaries by using a feature-driven local convolution. (3) Our network owns an effective structure for pyramidal feature extraction and embraces feature warping rather than image warping as practiced in FlowNet2. Our code and trained models are available at https://github.com/twhui/LiteFlowNet .",0
"FlowNet2 is a convolutional neural network (CNN) that uses over 160M parameters to achieve precise optical flow estimation. However, we introduce an alternative network that performs better than FlowNet2 on the challenging Sintel final pass and KITTI benchmarks, while being 30 times smaller in the model size and 1.36 times faster in the running speed. To achieve this, we focus on architectural details that may have been overlooked in current frameworks. Firstly, we present a lightweight cascaded network that improves flow inference accuracy and allows for seamless incorporation of descriptor matching. Secondly, we introduce a flow regularization layer that uses a feature-driven local convolution to address the issue of outliers and vague flow boundaries. Finally, our network adopts an effective structure for pyramidal feature extraction and feature warping, unlike the image warping employed in FlowNet2. Our code and trained models are available at https://github.com/twhui/LiteFlowNet.",1
"Conventional image motion based structure from motion methods first compute optical flow, then solve for the 3D motion parameters based on the epipolar constraint, and finally recover the 3D geometry of the scene. However, errors in optical flow due to regularization can lead to large errors in 3D motion and structure. This paper investigates whether performance and consistency can be improved by avoiding optical flow estimation in the early stages of the structure from motion pipeline, and it proposes a new direct method based on image gradients (normal flow) only. The main idea lies in a reformulation of the positive-depth constraint, which allows the use of well-known minimization techniques to solve for 3D motion. The 3D motion estimate is then refined and structure estimated adding a regularization based on depth. Experimental comparisons on standard synthetic datasets and the real-world driving benchmark dataset KITTI using three different optic flow algorithms show that the method achieves better accuracy in all but one case. Furthermore, it outperforms existing normal flow based 3D motion estimation techniques. Finally, the recovered 3D geometry is shown to be also very accurate.",0
"Current structure from motion methods reliant on image motion typically start by computing optical flow, then solving for 3D motion parameters based on the epipolar constraint, and ultimately recovering the 3D scene geometry. However, errors arising from regularization in optical flow can result in significant inaccuracies in 3D motion and structure. This study aims to enhance performance and consistency by bypassing optical flow estimation in the early stages of the structure from motion pipeline. Instead, it proposes a novel direct method based solely on image gradients (normal flow) to improve accuracy. The positive-depth constraint is reformulated to enable the use of well-established minimization techniques to solve for 3D motion. The refined 3D motion estimate is then utilized to estimate the structure while also incorporating depth-based regularization. The approach is compared to three different optic flow methods on standard synthetic datasets and the KITTI driving benchmark dataset. Results indicate improved accuracy in all but one case and superior performance to existing normal flow-based 3D motion estimation techniques. Furthermore, the recovered 3D geometry is demonstrated to be highly accurate.",1
"Dynamic Vision Sensors (DVS), which output asynchronous log intensity change events, have potential applications in high-speed robotics, autonomous cars and drones. The precise event timing, sparse output, and wide dynamic range of the events are well suited for optical flow, but conventional optical flow (OF) algorithms are not well matched to the event stream data. This paper proposes an event-driven OF algorithm called adaptive block-matching optical flow (ABMOF). ABMOF uses time slices of accumulated DVS events. The time slices are adaptively rotated based on the input events and OF results. Compared with other methods such as gradient-based OF, ABMOF can efficiently be implemented in compact logic circuits. Results show that ABMOF achieves comparable accuracy to conventional standards such as Lucas-Kanade (LK). The main contributions of our paper are new adaptive time-slice rotation methods that ensure the generated slices have sufficient features for matching,including a feedback mechanism that controls the generated slices to have average slice displacement within the block search range. An LK method using our adapted slices is also implemented. The ABMOF accuracy is compared with this LK method on natural scene data including sparse and dense texture, high dynamic range, and fast motion exceeding 30,000 pixels per second.The paper dataset and source code are available from http://sensors.ini.uzh.ch/databases.html.",0
"The potential applications of Dynamic Vision Sensors (DVS) are in high-speed robotics, autonomous cars, and drones due to their ability to output asynchronous log intensity change events. However, conventional optical flow (OF) algorithms are not well suited to the precise event timing, sparse output, and wide dynamic range of the events. To address this issue, an event-driven OF algorithm called adaptive block-matching optical flow (ABMOF) is proposed in this paper. ABMOF uses time slices of accumulated DVS events that are adaptively rotated based on the input events and OF results. Compared to other methods, ABMOF can be efficiently implemented in compact logic circuits. Results demonstrate that ABMOF achieves similar accuracy to conventional standards such as Lucas-Kanade (LK). The paper presents new adaptive time-slice rotation methods that ensure the generated slices have sufficient features for matching and a feedback mechanism that controls the generated slices to have an average slice displacement within the block search range. An LK method using the adapted slices is also implemented. ABMOF accuracy is compared to the LK method on natural scene data with various characteristics, and the paper dataset and source code are available from http://sensors.ini.uzh.ch/databases.html.",1
"When a person attempts to conceal an emotion, the genuine emotion is manifest as a micro-expression. Exploration of automatic facial micro-expression recognition systems is relatively new in the computer vision domain. This is due to the difficulty in implementing optimal feature extraction methods to cope with the subtlety and brief motion characteristics of the expression. Most of the existing approaches extract the subtle facial movements based on hand-crafted features. In this paper, we address the micro-expression recognition task with a convolutional neural network (CNN) architecture, which well integrates the features extracted from each video. A new feature descriptor, Optical Flow Features from Apex frame Network (OFF-ApexNet) is introduced. This feature descriptor combines the optical ow guided context with the CNN. Firstly, we obtain the location of the apex frame from each video sequence as it portrays the highest intensity of facial motion among all frames. Then, the optical ow information are attained from the apex frame and a reference frame (i.e., onset frame). Finally, the optical flow features are fed into a pre-designed CNN model for further feature enhancement as well as to carry out the expression classification. To evaluate the effectiveness of OFF-ApexNet, comprehensive evaluations are conducted on three public spontaneous micro-expression datasets (i.e., SMIC, CASME II and SAMM). The promising recognition result suggests that the proposed method can optimally describe the significant micro-expression details. In particular, we report that, in a multi-database with leave-one-subject-out cross-validation experimental protocol, the recognition performance reaches 74.60% of recognition accuracy and F-measure of 71.04%. We also note that this is the first work that performs cross-dataset validation on three databases in this domain.",0
"The manifestation of a genuine emotion as a micro-expression occurs when someone tries to conceal it. However, automatic facial micro-expression recognition systems are relatively new in the computer vision field. This is because of the complexity in implementing feature extraction methods to account for the subtle and brief motion characteristics of the expression. Most existing approaches use hand-crafted features to extract the subtle facial movements. In this study, we propose a convolutional neural network (CNN) architecture that integrates the features extracted from each video to address the micro-expression recognition task. We introduce a new feature descriptor called Optical Flow Features from Apex frame Network (OFF-ApexNet), which combines optical flow guided context with CNN. We obtain the apex frame location from each video sequence, which portrays the highest intensity of facial motion among all frames. Then, we attain the optical flow information from the apex frame and a reference frame (onset frame). Finally, we feed the optical flow features into a pre-designed CNN model for further feature enhancement and expression classification. The effectiveness of OFF-ApexNet is evaluated on three public spontaneous micro-expression datasets (SMIC, CASME II, and SAMM). The results suggest that the proposed method can optimally describe the significant micro-expression details. In a multi-database with leave-one-subject-out cross-validation experimental protocol, the recognition performance reaches 74.60% of recognition accuracy and F-measure of 71.04%. This is the first work that performs cross-dataset validation on three databases in this domain.",1
"Using a layered representation for motion estimation has the advantage of being able to cope with discontinuities and occlusions. In this paper, we learn to estimate optical flow by combining a layered motion representation with deep learning. Instead of pre-segmenting the image to layers, the proposed approach automatically generates a layered representation of optical flow using the proposed soft-mask module. The essential components of the soft-mask module are maxout and fuse operations, which enable a disjoint layered representation of optical flow and more accurate flow estimation. We show that by using masks the motion estimate results in a quadratic function of input features in the output layer. The proposed soft-mask module can be added to any existing optical flow estimation networks by replacing their flow output layer. In this work, we use FlowNet as the base network to which we add the soft-mask module. The resulting network is tested on three well-known benchmarks with both supervised and unsupervised flow estimation tasks. Evaluation results show that the proposed network achieve better results compared with the original FlowNet.",0
"The use of a layered representation in motion estimation is advantageous as it can handle discontinuities and occlusions. This study combines deep learning with a layered motion representation to estimate optical flow. Rather than pre-segmenting the image into layers, the proposed approach uses a soft-mask module to automatically generate a layered representation of optical flow. The soft-mask module includes maxout and fuse operations, which enable a disjoint layered representation of optical flow and more accurate flow estimation. The study demonstrates that using masks results in a quadratic function of input features in the output layer. The soft-mask module can easily be incorporated into existing optical flow estimation networks by replacing their flow output layer. In this study, the soft-mask module is added to FlowNet, resulting in better performance on three popular benchmarks for both supervised and unsupervised flow estimation tasks.",1
"Optical Flow algorithms are of high importance for many applications. Recently, the Flow Field algorithm and its modifications have shown remarkable results, as they have been evaluated with top accuracy on different data sets. In our analysis of the algorithm we have found that it produces accurate sparse matches, but there is room for improvement in the interpolation. Thus, we propose in this paper FlowFields++, where we combine the accurate matches of Flow Fields with a robust interpolation. In addition, we propose improved variational optimization as post-processing. Our new algorithm is evaluated on the challenging KITTI and MPI Sintel data sets with public top results on both benchmarks.",0
"Many applications rely heavily on Optical Flow algorithms. The Flow Field algorithm and its variations have recently demonstrated exceptional performance, achieving top accuracy in various data sets. Our assessment of the algorithm revealed that it generates precise sparse matches, but its interpolation has potential for enhancement. Consequently, we introduce FlowFields++, which combines Flow Fields' accurate matches with a sturdy interpolation in this paper. Furthermore, we suggest post-processing utilizing enhanced variational optimization. Our novel algorithm achieved outstanding results on the KITTI and MPI Sintel data sets, earning top marks in both benchmarks.",1
"Convolutional neural networks (CNNs) handle the case where filters extend beyond the image boundary using several heuristics, such as zero, repeat or mean padding. These schemes are applied in an ad-hoc fashion and, being weakly related to the image content and oblivious of the target task, result in low output quality at the boundary. In this paper, we propose a simple and effective improvement that learns the boundary handling itself. At training-time, the network is provided with a separate set of explicit boundary filters. At testing-time, we use these filters which have learned to extrapolate features at the boundary in an optimal way for the specific task. Our extensive evaluation, over a wide range of architectural changes (variations of layers, feature channels, or both), shows how the explicit filters result in improved boundary handling. Consequently, we demonstrate an improvement of 5% to 20% across the board of typical CNN applications (colorization, de-Bayering, optical flow, and disparity estimation).",0
"The handling of filters extending beyond the image boundary in convolutional neural networks (CNNs) is typically done using heuristics like zero, repeat, or mean padding, which are applied in an ad-hoc manner and result in low output quality at the boundary due to weak relation to image content and lack of consideration for the target task. This paper proposes a simple and effective solution to this problem by introducing a separate set of explicit boundary filters that are learned by the network during training. These filters are then used at testing-time to extrapolate features at the boundary in an optimal way for the specific task, resulting in improved boundary handling. The evaluation, conducted across a wide range of architectural changes, shows that the explicit filters lead to a significant improvement of 5% to 20% across typical CNN applications such as colorization, de-Bayering, optical flow, and disparity estimation.",1
"In recent years, many publications showed that convolutional neural network based features can have a superior performance to engineered features. However, not much effort was taken so far to extract local features efficiently for a whole image. In this paper, we present an approach to compute patch-based local feature descriptors efficiently in presence of pooling and striding layers for whole images at once. Our approach is generic and can be applied to nearly all existing network architectures. This includes networks for all local feature extraction tasks like camera calibration, Patchmatching, optical flow estimation and stereo matching. In addition, our approach can be applied to other patch-based approaches like sliding window object detection and recognition. We complete our paper with a speed benchmark of popular CNN based feature extraction approaches applied on a whole image, with and without our speedup, and example code (for Torch) that shows how an arbitrary CNN architecture can be easily converted by our approach.",0
"Publications in recent years have highlighted that features based on convolutional neural networks tend to outperform engineered features. However, there has been a lack of effort in efficiently extracting local features for entire images. Our paper proposes an approach that efficiently computes patch-based local feature descriptors for whole images in the presence of pooling and striding layers. This approach is generic and can be applied to a wide range of existing network architectures for various local feature extraction tasks, including camera calibration, Patchmatching, optical flow estimation, and stereo matching. Additionally, it can also be applied to other patch-based approaches such as sliding window object detection and recognition. To demonstrate the efficacy of our approach, we conduct a speed benchmark on popular CNN-based feature extraction approaches applied to whole images, with and without our speedup, and provide example code (for Torch) that showcases how to easily convert an arbitrary CNN architecture using our approach.",1
"Spatio-temporal contexts are crucial in understanding human actions in videos. Recent state-of-the-art Convolutional Neural Network (ConvNet) based action recognition systems frequently involve 3D spatio-temporal ConvNet filters, chunking videos into fixed length clips and Long Short Term Memory (LSTM) networks. Such architectures are designed to take advantage of both short term and long term temporal contexts, but also requires the accumulation of a predefined number of video frames (e.g., to construct video clips for 3D ConvNet filters, to generate enough inputs for LSTMs). For applications that require low-latency online predictions of fast-changing action scenes, a new action recognition system is proposed in this paper. Termed ""Weighted Multi-Region Convolutional Neural Network"" (WMR ConvNet), the proposed system is LSTM-free, and is based on 2D ConvNet that does not require the accumulation of video frames for 3D ConvNet filtering. Unlike early 2D ConvNets that are based purely on RGB frames and optical flow frames, the WMR ConvNet is designed to simultaneously capture multiple spatial and short term temporal cues (e.g., human poses, occurrences of objects in the background) with both the primary region (foreground) and secondary regions (mostly background). On both the UCF101 and HMDB51 datasets, the proposed WMR ConvNet achieves the state-of-the-art performance among competing low-latency algorithms. Furthermore, WMR ConvNet even outperforms the 3D ConvNet based C3D algorithm that requires video frame accumulation. In an ablation study with the optical flow ConvNet stream removed, the ablated WMR ConvNet nevertheless outperforms competing algorithms.",0
"To comprehend human actions in videos, spatio-temporal contexts are critical. Currently, advanced action recognition systems that employ Convolutional Neural Networks (ConvNets) frequently use 3D spatio-temporal ConvNet filters, divide videos into fixed length clips, and Long Short Term Memory (LSTM) networks. This approach benefits from both short and long term temporal contexts, but necessitates the collection of a predetermined number of video frames. For applications that require prompt online predictions of swiftly changing action scenes, a new action recognition system is introduced. The proposed Weighted Multi-Region Convolutional Neural Network (WMR ConvNet) does not depend on LSTM and is based on a 2D ConvNet that does not require video frame accumulation. Unlike early 2D ConvNets, WMR ConvNet is designed to capture multiple spatial and short term temporal cues in both the primary region (foreground) and secondary regions (mostly background). WMR ConvNet achieves the state-of-the-art performance on the UCF101 and HMDB51 datasets among competing low-latency algorithms. Additionally, WMR ConvNet outperforms the 3D ConvNet based C3D algorithm that requires video frame accumulation. Even with the optical flow ConvNet stream removed, the ablated WMR ConvNet outperforms competing algorithms in an ablation study.",1
"Despite the significant progress that has been made on estimating optical flow recently, most estimation methods, including classical and deep learning approaches, still have difficulty with multi-scale estimation, real-time computation, and/or occlusion reasoning. In this paper, we introduce dilated convolution and occlusion reasoning into unsupervised optical flow estimation to address these issues. The dilated convolution allows our network to avoid upsampling via deconvolution and the resulting gridding artifacts. Dilated convolution also results in a smaller memory footprint which speeds up interference. The occlusion reasoning prevents our network from learning incorrect deformations due to occluded image regions during training. Our proposed method outperforms state-of-the-art unsupervised approaches on the KITTI benchmark. We also demonstrate its generalization capability by applying it to action recognition in video.",0
"Although there has been considerable advancement in optical flow estimation, most classical and deep learning techniques struggle with multi-scale estimation, real-time computation, and/or occlusion reasoning. To tackle these challenges, we have incorporated dilated convolution and occlusion reasoning into our unsupervised optical flow estimation. Dilated convolution negates the need for upsampling with deconvolution and prevents gridding artifacts while also reducing the memory footprint for faster interference. Additionally, occlusion reasoning prevents our network from learning erroneous deformations caused by obscured image regions during training. Our proposed method surpasses the performance of existing unsupervised approaches on the KITTI benchmark and demonstrates its versatility through its use in action recognition in video.",1
"Object tracking is a hot topic in computer vision. Thanks to the booming of the very high resolution (VHR) remote sensing techniques, it is now possible to track targets of interests in satellite videos. However, since the targets in the satellite videos are usually too small compared with the entire image, and too similar with the background, most state-of-the-art algorithms failed to track the target in satellite videos with a satisfactory accuracy. Due to the fact that optical flow shows the great potential to detect even the slight movement of the targets, we proposed a multi-frame optical flow tracker (MOFT) for object tracking in satellite videos. The Lucas-Kanade optical flow method was fused with the HSV color system and integral image to track the targets in the satellite videos, while multi-frame difference method was utilized in the optical flow tracker for a better interpretation. The experiments with three VHR remote sensing satellite video datasets indicate that compared with state-of-the-art object tracking algorithms, the proposed method can track the target more accurately.",0
"Computer vision has a keen interest in object tracking, particularly with the recent advancements in very high resolution (VHR) remote sensing techniques. These techniques have made it possible to track targets of interest in satellite videos. However, the small size of the targets in relation to the entire image, as well as their similarity to the background, often results in unsatisfactory accuracy for most state-of-the-art algorithms. Our proposed solution is a multi-frame optical flow tracker (MOFT) that utilizes the Lucas-Kanade optical flow method fused with the HSV color system and integral image to track targets in satellite videos. Additionally, the optical flow tracker integrates a multi-frame difference method for better interpretation. Through experiments with three VHR remote sensing satellite video datasets, our method has proven to track targets more accurately than state-of-the-art object tracking algorithms.",1
"From the frame/clip-level feature learning to the video-level representation building, deep learning methods in action recognition have developed rapidly in recent years. However, current methods suffer from the confusion caused by partial observation training, or without end-to-end learning, or restricted to single temporal scale modeling and so on. In this paper, we build upon two-stream ConvNets and propose Deep networks with Temporal Pyramid Pooling (DTPP), an end-to-end video-level representation learning approach, to address these problems. Specifically, at first, RGB images and optical flow stacks are sparsely sampled across the whole video. Then a temporal pyramid pooling layer is used to aggregate the frame-level features which consist of spatial and temporal cues. Lastly, the trained model has compact video-level representation with multiple temporal scales, which is both global and sequence-aware. Experimental results show that DTPP achieves the state-of-the-art performance on two challenging video action datasets: UCF101 and HMDB51, either by ImageNet pre-training or Kinetics pre-training.",0
"Recent years have seen a rapid evolution of deep learning methods in action recognition, from frame/clip-level feature learning to video-level representation building. However, existing approaches suffer from partial observation training, restricted single temporal scale modeling, and a lack of end-to-end learning. To overcome these issues, this paper proposes an end-to-end video-level representation learning approach called Deep networks with Temporal Pyramid Pooling (DTPP), which builds upon two-stream ConvNets. DTPP uses sparsely sampled RGB images and optical flow stacks to aggregate frame-level features with spatial and temporal cues using a temporal pyramid pooling layer. The resulting trained model has a compact video-level representation with multiple temporal scales that is both global and sequence-aware. Experimental results demonstrate that DTPP achieves state-of-the-art performance on challenging video action datasets UCF101 and HMDB51, with either ImageNet or Kinetics pre-training.",1
"Optimized scene representation is an important characteristic of a framework for detecting abnormalities on live videos. One of the challenges for detecting abnormalities in live videos is real-time detection of objects in a non-parametric way. Another challenge is to efficiently represent the state of objects temporally across frames. In this paper, a Gibbs sampling based heuristic model referred to as Temporal Unknown Incremental Clustering (TUIC) has been proposed to cluster pixels with motion. Pixel motion is first detected using optical flow and a Bayesian algorithm has been applied to associate pixels belonging to similar cluster in subsequent frames. The algorithm is fast and produces accurate results in $\Theta(kn)$ time, where $k$ is the number of clusters and $n$ the number of pixels. Our experimental validation with publicly available datasets reveals that the proposed framework has good potential to open-up new opportunities for real-time traffic analysis.",0
"A framework for detecting abnormalities in live videos requires an optimized scene representation, which presents some challenges. Firstly, it is crucial to detect objects in real-time without relying on parametric methods. Secondly, it is necessary to represent the temporal state of objects efficiently across frames. To address these challenges, this paper proposes a heuristic model called Temporal Unknown Incremental Clustering (TUIC), which utilizes Gibbs sampling to cluster pixels with motion. The model detects pixel motion through optical flow and employs a Bayesian algorithm to group pixels belonging to similar clusters in subsequent frames. The algorithm is both fast and accurate, operating in $\Theta(kn)$ time, where $k$ is the number of clusters and $n$ is the number of pixels. Experimental validation using publicly available datasets demonstrates the framework's potential for real-time traffic analysis.",1
"Motion boundary detection is a crucial yet challenging problem. Prior methods focus on analyzing the gradients and distributions of optical flow fields, or use hand-crafted features for motion boundary learning. In this paper, we propose the first dedicated end-to-end deep learning approach for motion boundary detection, which we term as MoBoNet. We introduce a refinement network structure which takes source input images, initial forward and backward optical flows as well as corresponding warping errors as inputs and produces high-resolution motion boundaries. Furthermore, we show that the obtained motion boundaries, through a fusion sub-network we design, can in turn guide the optical flows for removing the artifacts. The proposed MoBoNet is generic and works with any optical flows. Our motion boundary detection and the refined optical flow estimation achieve results superior to the state of the art.",0
"Detecting motion boundaries is a challenging yet crucial task, and previous methods have primarily focused on analyzing gradients and optical flow fields or using hand-crafted features. This paper presents MoBoNet, the first deep learning approach dedicated to motion boundary detection. MoBoNet utilizes a refinement network structure that takes input images, forward and backward optical flows, and warping errors to produce high-resolution motion boundaries. Additionally, MoBoNet incorporates a fusion sub-network that uses the motion boundaries to improve the optical flow estimation. MoBoNet is a generic approach that works with any optical flows and outperforms existing methods in both motion boundary detection and refined optical flow estimation.",1
"We introduce a two-stream model for dynamic texture synthesis. Our model is based on pre-trained convolutional networks (ConvNets) that target two independent tasks: (i) object recognition, and (ii) optical flow prediction. Given an input dynamic texture, statistics of filter responses from the object recognition ConvNet encapsulate the per-frame appearance of the input texture, while statistics of filter responses from the optical flow ConvNet model its dynamics. To generate a novel texture, a randomly initialized input sequence is optimized to match the feature statistics from each stream of an example texture. Inspired by recent work on image style transfer and enabled by the two-stream model, we also apply the synthesis approach to combine the texture appearance from one texture with the dynamics of another to generate entirely novel dynamic textures. We show that our approach generates novel, high quality samples that match both the framewise appearance and temporal evolution of input texture. Finally, we quantitatively evaluate our texture synthesis approach with a thorough user study.",0
"Our approach to dynamic texture synthesis involves a two-stream model that utilizes pre-trained Convolutional Networks (ConvNets) for object recognition and optical flow prediction. The ConvNets' filter responses' statistics capture the per-frame appearance and dynamics of the input texture separately. To create a new texture, we initiate a random input sequence and optimize it to match the feature statistics from each stream of a given example texture. This method also allows us to combine the appearance of one texture with the dynamics of another to generate entirely novel dynamic textures. We demonstrate that our approach generates high-quality samples that match both the framewise appearance and temporal evolution of the input texture. Finally, we quantitatively evaluate our texture synthesis approach with a comprehensive user study.",1
"Despite recent advances, estimating optical flow remains a challenging problem in the presence of illumination change, large occlusions or fast movement. In this paper, we propose a novel optical flow estimation framework which can provide accurate dense correspondence and occlusion localization through a multi-scale generalized plane matching approach. In our method, we regard the scene as a collection of planes at multiple scales, and for each such plane, compensate motion in consensus to improve match quality. We estimate the square patch plane distortion using a robust plane model detection method and iteratively apply a plane matching scheme within a multi-scale framework. During the flow estimation process, our enhanced plane matching method also clearly localizes the occluded regions. In experiments on MPI-Sintel datasets, our method robustly estimated optical flow from given noisy correspondences, and also revealed the occluded regions accurately. Compared to other state-of-the-art optical flow methods, our method shows accurate occlusion localization, comparable optical flow quality, and better thin object detection.",0
"Although there have been recent advancements, the task of estimating optical flow is still difficult when factors such as changes in lighting, large obstructions, or fast movement are present. This paper introduces a new framework for estimating optical flow that can accurately determine dense correspondence and identify occlusions through a multi-scale generalized plane matching technique. The approach considers the scene as a group of planes at various scales, and motion is compensated for each plane to enhance match accuracy. A robust plane model detection method is used to estimate square patch plane distortion, and a plane matching scheme is iteratively applied within a multi-scale framework. The improved plane matching method also effectively locates occluded regions during the flow estimation process. Experimental results on MPI-Sintel datasets demonstrate that our method robustly estimates optical flow from noisy correspondences and accurately identifies occluded regions. Compared to other state-of-the-art optical flow methods, our approach offers precise occlusion localization, comparable optical flow quality, and improved detection of thin objects.",1
"In this paper, a new video classification methodology is proposed which can be applied in both first and third person videos. The main idea behind the proposed strategy is to capture complementary information of appearance and motion efficiently by performing two independent streams on the videos. The first stream is aimed to capture long-term motions from shorter ones by keeping track of how elements in optical flow images have changed over time. Optical flow images are described by pre-trained networks that have been trained on large scale image datasets. A set of multi-channel time series are obtained by aligning descriptions beside each other. For extracting motion features from these time series, PoT representation method plus a novel pooling operator is followed due to several advantages. The second stream is accomplished to extract appearance features which are vital in the case of video classification. The proposed method has been evaluated on both first and third-person datasets and results present that the proposed methodology reaches the state of the art successfully.",0
"This paper introduces a new approach for classifying videos, suitable for both first and third person perspectives. The strategy is designed to efficiently capture appearance and motion information by having two independent streams for analyzing the videos. The first stream focuses on identifying long-term changes in motion by tracking the changes in optical flow images over time, using pre-trained networks from large image datasets. This produces a set of multi-channel time series, which are then processed using the PoT representation method and a novel pooling operator to extract motion features. The second stream is dedicated to extracting appearance features, which are important for video classification. The proposed methodology has been tested on both first and third-person datasets and has successfully achieved state-of-the-art results.",1
"Using deep learning, this paper addresses the problem of joint object boundary detection and boundary motion estimation in videos, which we named boundary flow estimation. Boundary flow is an important mid-level visual cue as boundaries characterize objects spatial extents, and the flow indicates objects motions and interactions. Yet, most prior work on motion estimation has focused on dense object motion or feature points that may not necessarily reside on boundaries. For boundary flow estimation, we specify a new fully convolutional Siamese network (FCSN) that jointly estimates object-level boundaries in two consecutive frames. Boundary correspondences in the two frames are predicted by the same FCSN with a new, unconventional deconvolution approach. Finally, the boundary flow estimate is improved with an edgelet-based filtering. Evaluation is conducted on three tasks: boundary detection in videos, boundary flow estimation, and optical flow estimation. On boundary detection, we achieve the state-of-the-art performance on the benchmark VSB100 dataset. On boundary flow estimation, we present the first results on the Sintel training dataset. For optical flow estimation, we run the recent approach CPMFlow but on the augmented input with our boundary-flow matches, and achieve significant performance improvement on the Sintel benchmark.",0
"This paper tackles the problem of detecting object boundaries and estimating boundary motion in videos through deep learning, which is referred to as boundary flow estimation. Boundary flow is a significant visual cue that characterizes an object's spatial extents and its interactions with other objects. However, previous research on motion estimation has primarily focused on dense object motion or feature points that may not be located on boundaries. To estimate boundary flow, the paper introduces a new fully convolutional Siamese network (FCSN) that jointly predicts object-level boundaries in two consecutive frames. The same FCSN predicts boundary correspondences in both frames using an unconventional deconvolution approach. An edgelet-based filter is then employed to enhance the boundary flow estimate. The paper evaluates the proposed approach on three tasks: boundary detection in videos, boundary flow estimation, and optical flow estimation. The state-of-the-art performance is achieved on the VSB100 dataset for boundary detection. Results on the Sintel training dataset are presented for boundary flow estimation, and a significant performance improvement is achieved on the Sintel benchmark for optical flow estimation using the proposed approach.",1
"We present a unifying framework to solve several computer vision problems with event cameras: motion, depth and optical flow estimation. The main idea of our framework is to find the point trajectories on the image plane that are best aligned with the event data by maximizing an objective function: the contrast of an image of warped events. Our method implicitly handles data association between the events, and therefore, does not rely on additional appearance information about the scene. In addition to accurately recovering the motion parameters of the problem, our framework produces motion-corrected edge-like images with high dynamic range that can be used for further scene analysis. The proposed method is not only simple, but more importantly, it is, to the best of our knowledge, the first method that can be successfully applied to such a diverse set of important vision tasks with event cameras.",0
"A framework has been presented that can solve various computer vision problems using event cameras, such as depth, motion, and optical flow estimation. The central concept of this framework is to identify the point trajectories on the image plane that are best aligned with the event data. This is achieved by maximizing an objective function, which is the contrast of a warped event image. The framework handles data association between events, eliminating the need for additional scene appearance information. The framework is capable of accurately recovering motion parameters and producing motion-corrected, high dynamic range, edge-like images that can be used for further scene analysis. This method is not only simple, but also the first of its kind in successfully applying to a diverse set of critical vision tasks with event cameras.",1
"It has been recently shown that a convolutional neural network can learn optical flow estimation with unsupervised learning. However, the performance of the unsupervised methods still has a relatively large gap compared to its supervised counterpart. Occlusion and large motion are some of the major factors that limit the current unsupervised learning of optical flow methods. In this work we introduce a new method which models occlusion explicitly and a new warping way that facilitates the learning of large motion. Our method shows promising results on Flying Chairs, MPI-Sintel and KITTI benchmark datasets. Especially on KITTI dataset where abundant unlabeled samples exist, our unsupervised method outperforms its counterpart trained with supervised learning.",0
"Recently, it was demonstrated that a convolutional neural network can acquire knowledge of optical flow estimation through unsupervised learning. Nonetheless, the unsupervised methods still display a relatively substantial disparity in performance compared to their supervised equivalent. Unsupervised learning of optical flow methods is limited by major factors such as occlusion and large motion. In this study, we introduce a novel method that explicitly models occlusion and a new warping technique that facilitates learning of large motion. Our approach exhibits encouraging outcomes on the Flying Chairs, MPI-Sintel, and KITTI benchmark datasets. Particularly, on the KITTI dataset where an abundance of unlabeled samples is present, our unsupervised method surpasses its supervised learning counterpart.",1
"Despite the recent success of end-to-end learned representations, hand-crafted optical flow features are still widely used in video analysis tasks. To fill this gap, we propose TVNet, a novel end-to-end trainable neural network, to learn optical-flow-like features from data. TVNet subsumes a specific optical flow solver, the TV-L1 method, and is initialized by unfolding its optimization iterations as neural layers. TVNet can therefore be used directly without any extra learning. Moreover, it can be naturally concatenated with other task-specific networks to formulate an end-to-end architecture, thus making our method more efficient than current multi-stage approaches by avoiding the need to pre-compute and store features on disk. Finally, the parameters of the TVNet can be further fine-tuned by end-to-end training. This enables TVNet to learn richer and task-specific patterns beyond exact optical flow. Extensive experiments on two action recognition benchmarks verify the effectiveness of the proposed approach. Our TVNet achieves better accuracies than all compared methods, while being competitive with the fastest counterpart in terms of features extraction time.",0
"Although end-to-end learned representations have been successful recently, hand-crafted optical flow features are still extensively used for video analysis tasks. To address this gap, we introduce TVNet, a new neural network that can be trained end-to-end to learn optical-flow-like features from data. TVNet incorporates the TV-L1 method, a specific optical flow solver, and is initialized by unfolding its optimization iterations as neural layers. TVNet can be used directly without additional learning and can be combined with other task-specific networks to create an end-to-end structure, which improves the efficiency of our method by removing the need to pre-compute and store features on disk. Additionally, the parameters of TVNet can be fine-tuned by end-to-end training, allowing it to learn richer and task-specific patterns beyond exact optical flow. Our approach is verified by extensive experiments on two action recognition benchmarks. Our TVNet outperforms all compared methods while still being competitive with the fastest counterpart in terms of feature extraction time.",1
"Video frame interpolation algorithms typically estimate optical flow or its variations and then use it to guide the synthesis of an intermediate frame between two consecutive original frames. To handle challenges like occlusion, bidirectional flow between the two input frames is often estimated and used to warp and blend the input frames. However, how to effectively blend the two warped frames still remains a challenging problem. This paper presents a context-aware synthesis approach that warps not only the input frames but also their pixel-wise contextual information and uses them to interpolate a high-quality intermediate frame. Specifically, we first use a pre-trained neural network to extract per-pixel contextual information for input frames. We then employ a state-of-the-art optical flow algorithm to estimate bidirectional flow between them and pre-warp both input frames and their context maps. Finally, unlike common approaches that blend the pre-warped frames, our method feeds them and their context maps to a video frame synthesis neural network to produce the interpolated frame in a context-aware fashion. Our neural network is fully convolutional and is trained end to end. Our experiments show that our method can handle challenging scenarios such as occlusion and large motion and outperforms representative state-of-the-art approaches.",0
"Algorithms used for video frame interpolation typically estimate optical flow or similar variables to generate a middle frame between two consecutive original frames. However, challenges like occlusion require bidirectional flow estimation to warp and blend input frames. Nonetheless, effectively blending the warped frames remains a difficult task. This study proposes a context-aware synthesis approach that warps the input frames and their pixel-wise contextual information to interpolate a high-quality intermediate frame. The approach involves using a pre-trained neural network to extract per-pixel contextual information, estimating bidirectional flow through an optical flow algorithm, pre-warping input frames and their context maps, and using a video frame synthesis neural network to produce the interpolated frame in a context-aware manner. The neural network is fully convolutional and trained end to end. The experiments show that the proposed method handles challenging scenarios like occlusion and large motion and outperforms state-of-the-art approaches.",1
"Variational inference has experienced a recent surge in popularity owing to stochastic approaches, which have yielded practical tools for a wide range of model classes. A key benefit is that stochastic variational inference obviates the tedious process of deriving analytical expressions for closed-form variable updates. Instead, one simply needs to derive the gradient of the log-posterior, which is often much easier. Yet for certain model classes, the log-posterior itself is difficult to optimize using standard gradient techniques. One such example are random field models, where optimization based on gradient linearization has proven popular, since it speeds up convergence significantly and can avoid poor local optima. In this paper we propose stochastic variational inference with gradient linearization (SVIGL). It is similarly convenient as standard stochastic variational inference - all that is required is a local linearization of the energy gradient. Its benefit over stochastic variational inference with conventional gradient methods is a clear improvement in convergence speed, while yielding comparable or even better variational approximations in terms of KL divergence. We demonstrate the benefits of SVIGL in three applications: Optical flow estimation, Poisson-Gaussian denoising, and 3D surface reconstruction.",0
"Stochastic approaches have made variational inference increasingly popular, as they have produced practical tools for various models. One of the main advantages of stochastic variational inference is that it eliminates the need for deriving analytical expressions for closed-form variable updates, and instead, only requires deriving the gradient of the log-posterior, which is often easier. However, for certain model classes, optimizing the log-posterior using standard gradient techniques is challenging. Random field models are one such example, where optimization based on gradient linearization has been popular due to its ability to speed up convergence and avoid poor local optima. This paper proposes stochastic variational inference with gradient linearization (SVIGL), which is as convenient as standard stochastic variational inference, with the only requirement being a local linearization of the energy gradient. The main benefit of SVIGL over stochastic variational inference with conventional gradient methods is that it significantly improves convergence speed while providing comparable or even better variational approximations in terms of KL divergence. The benefits of SVIGL are demonstrated in three applications: Optical flow estimation, Poisson-Gaussian denoising, and 3D surface reconstruction.",1
"This paper addresses the problem of video object segmentation, where the initial object mask is given in the first frame of an input video. We propose a novel spatio-temporal Markov Random Field (MRF) model defined over pixels to handle this problem. Unlike conventional MRF models, the spatial dependencies among pixels in our model are encoded by a Convolutional Neural Network (CNN). Specifically, for a given object, the probability of a labeling to a set of spatially neighboring pixels can be predicted by a CNN trained for this specific object. As a result, higher-order, richer dependencies among pixels in the set can be implicitly modeled by the CNN. With temporal dependencies established by optical flow, the resulting MRF model combines both spatial and temporal cues for tackling video object segmentation. However, performing inference in the MRF model is very difficult due to the very high-order dependencies. To this end, we propose a novel CNN-embedded algorithm to perform approximate inference in the MRF. This algorithm proceeds by alternating between a temporal fusion step and a feed-forward CNN step. When initialized with an appearance-based one-shot segmentation CNN, our model outperforms the winning entries of the DAVIS 2017 Challenge, without resorting to model ensembling or any dedicated detectors.",0
"The problem of video object segmentation, which involves the initial object mask being provided in the first frame of a video input, is the focus of this paper. A unique spatio-temporal Markov Random Field (MRF) model is introduced in order to address this issue. Unlike typical MRF models, the spatial relations among pixels in our model are encoded by a Convolutional Neural Network (CNN). Based on a given object, a CNN trained for that specific object can predict the probability of labeling a set of spatially adjacent pixels. As a result, the CNN implicitly models higher-order and richer dependencies among pixels in the set. Combining both spatial and temporal cues, the resulting MRF model uses optical flow to establish temporal dependencies for video object segmentation. However, performing inference in the MRF model is challenging because of the very high-order dependencies. Therefore, we propose a novel CNN-embedded algorithm to perform approximate inference in the MRF. This algorithm alternates between a temporal fusion step and a feed-forward CNN step. Our model outperforms the winning entries of the DAVIS 2017 Challenge when initialized with an appearance-based one-shot segmentation CNN, without resorting to model ensembling or any dedicated detectors.",1
"The finding that very large networks can be trained efficiently and reliably has led to a paradigm shift in computer vision from engineered solutions to learning formulations. As a result, the research challenge shifts from devising algorithms to creating suitable and abundant training data for supervised learning. How to efficiently create such training data? The dominant data acquisition method in visual recognition is based on web data and manual annotation. Yet, for many computer vision problems, such as stereo or optical flow estimation, this approach is not feasible because humans cannot manually enter a pixel-accurate flow field. In this paper, we promote the use of synthetically generated data for the purpose of training deep networks on such tasks.We suggest multiple ways to generate such data and evaluate the influence of dataset properties on the performance and generalization properties of the resulting networks. We also demonstrate the benefit of learning schedules that use different types of data at selected stages of the training process.",0
"The discovery that large networks can be trained effectively and dependably has altered the computer vision paradigm from engineered solutions to learning formulations. Consequently, the challenge for researchers has shifted from developing algorithms to producing appropriate and abundant training data for supervised learning. The most common method of data acquisition for visual recognition is web data and manual annotation, but this approach is not feasible for certain computer vision problems, such as stereo or optical flow estimation, due to the impossibility of humans manually inputting a pixel-precise flow field. This paper advocates for the use of synthetically generated data for training deep networks on such tasks and proposes various methods for generating such data. We also assess how dataset properties affect the performance and generalization properties of the resulting networks. Additionally, we demonstrate the usefulness of learning schedules that employ various types of data at specific stages of the training process.",1
"Traditional approaches to interpolate/extrapolate frames in a video sequence require accurate pixel correspondences between images, e.g., using optical flow. Their results stem on the accuracy of optical flow estimation, and could generate heavy artifacts when flow estimation failed. Recently methods using auto-encoder has shown impressive progress, however they are usually trained for specific interpolation/extrapolation settings and lack of flexibility and In order to reduce these limitations, we propose a unified network to parameterize the interest frame position and therefore infer interpolate/extrapolate frames within the same framework. To achieve this, we introduce a transitive consistency loss to better regularize the network. We adopt a multi-scale structure for the network so that the parameters can be shared across multi-layers. Our approach avoids expensive global optimization of optical flow methods, and is efficient and flexible for video interpolation/extrapolation applications. Experimental results have shown that our method performs favorably against state-of-the-art methods.",0
"Conventional methods for interpolating or extrapolating frames in a video sequence rely on precise pixel correspondences between images, typically via optical flow. However, the accuracy of these approaches depends heavily on the estimation of optical flow and can result in significant artifacts when estimation fails. While recent auto-encoder methods have made impressive progress, they are often limited by specific interpolation/extrapolation settings and lack flexibility. To address these limitations, we propose a unified network that can parameterize the position of the frames of interest and thus infer interpolation/extrapolation frames within the same framework. To achieve this, we introduce a transitive consistency loss to better regulate the network. Our multi-scale network structure allows parameters to be shared across multiple layers, reducing the need for expensive global optimization of optical flow methods. Our approach is efficient, flexible, and suitable for video interpolation/extrapolation applications. Our experimental results demonstrate that our method outperforms state-of-the-art methods.",1
"Accurate prediction of traffic signal duration for roadway junction is a challenging problem due to the dynamic nature of traffic flows. Though supervised learning can be used, parameters may vary across roadway junctions. In this paper, we present a computer vision guided expert system that can learn the departure rate of a given traffic junction modeled using traditional queuing theory. First, we temporally group the optical flow of the moving vehicles using Dirichlet Process Mixture Model (DPMM). These groups are referred to as tracklets or temporal clusters. Tracklet features are then used to learn the dynamic behavior of a traffic junction, especially during on/off cycles of a signal. The proposed queuing theory based approach can predict the signal open duration for the next cycle with higher accuracy when compared with other popular features used for tracking. The hypothesis has been verified on two publicly available video datasets. The results reveal that the DPMM based features are better than existing tracking frameworks to estimate $\mu$. Thus, signal duration prediction is more accurate when tested on these datasets.The method can be used for designing intelligent operator-independent traffic control systems for roadway junctions at cities and highways.",0
"The dynamic nature of traffic flows makes it difficult to accurately predict the duration of traffic signals at roadway junctions. While supervised learning can be utilized, the parameters may differ among junctions. This study proposes a computer vision-based expert system that can learn the departure rate of a specific traffic junction modeled using traditional queuing theory. First, the moving vehicles' optical flow is temporally grouped using Dirichlet Process Mixture Model (DPMM) to create tracklets or temporal clusters. The tracklet features are then used to learn the traffic junction's dynamic behavior, particularly during signal on/off cycles. The proposed queuing theory approach can predict the signal open duration for the next cycle with greater accuracy than other popular tracking features. The hypothesis was tested on two publicly available video datasets, and the results demonstrated that the DPMM-based features were superior to current tracking frameworks in estimating $\mu$. Consequently, the method can be applied to develop intelligent operator-independent traffic control systems for roadway junctions in cities and highways.",1
"Anomaly detection in videos refers to the identification of events that do not conform to expected behavior. However, almost all existing methods tackle the problem by minimizing the reconstruction errors of training data, which cannot guarantee a larger reconstruction error for an abnormal event. In this paper, we propose to tackle the anomaly detection problem within a video prediction framework. To the best of our knowledge, this is the first work that leverages the difference between a predicted future frame and its ground truth to detect an abnormal event. To predict a future frame with higher quality for normal events, other than the commonly used appearance (spatial) constraints on intensity and gradient, we also introduce a motion (temporal) constraint in video prediction by enforcing the optical flow between predicted frames and ground truth frames to be consistent, and this is the first work that introduces a temporal constraint into the video prediction task. Such spatial and motion constraints facilitate the future frame prediction for normal events, and consequently facilitate to identify those abnormal events that do not conform the expectation. Extensive experiments on both a toy dataset and some publicly available datasets validate the effectiveness of our method in terms of robustness to the uncertainty in normal events and the sensitivity to abnormal events.",0
"The detection of anomalies in videos involves identifying events that deviate from expected behavior. However, current methods focus on minimizing reconstruction errors of training data, which may not necessarily result in a larger reconstruction error for abnormal events. This study proposes a novel approach to address anomaly detection within a video prediction framework. It is the first work to utilize the difference between predicted and ground truth frames to identify abnormal events. To improve the quality of future frame predictions for normal events, the study introduces a motion constraint that enforces consistency between predicted and ground truth frames' optical flow. This is the first work to introduce a temporal constraint in video prediction. The proposed spatial and motion constraints aid in predicting future frames for normal events and detecting abnormal events that do not conform to expectations. Extensive experiments on toy and publicly available datasets demonstrate the effectiveness of the proposed method in terms of robustness to uncertainty in normal events and sensitivity to abnormal events.",1
"We propose GeoNet, a jointly unsupervised learning framework for monocular depth, optical flow and ego-motion estimation from videos. The three components are coupled by the nature of 3D scene geometry, jointly learned by our framework in an end-to-end manner. Specifically, geometric relationships are extracted over the predictions of individual modules and then combined as an image reconstruction loss, reasoning about static and dynamic scene parts separately. Furthermore, we propose an adaptive geometric consistency loss to increase robustness towards outliers and non-Lambertian regions, which resolves occlusions and texture ambiguities effectively. Experimentation on the KITTI driving dataset reveals that our scheme achieves state-of-the-art results in all of the three tasks, performing better than previously unsupervised methods and comparably with supervised ones.",0
"Our suggestion is GeoNet - an unsupervised learning framework that jointly estimates monocular depth, optical flow, and ego-motion from videos. The three components are interlinked by the 3D scene geometry, which our framework learns in an end-to-end fashion. We extract geometric relationships from the predictions of individual modules and blend them into an image reconstruction loss, which separately reasons about static and dynamic scene parts. Additionally, we introduce an adaptive geometric consistency loss that enhances robustness towards outliers and non-Lambertian regions, addressing occlusions and texture ambiguities effectively. Our experimentation on the KITTI driving dataset demonstrates that our approach outperforms previous unsupervised methods and performs comparably with supervised ones, achieving state-of-the-art results in all three tasks.",1
"Visual feature clustering is one of the cost-effective approaches to segment objects in videos. However, the assumptions made for developing the existing algorithms prevent them from being used in situations like segmenting an unknown number of static and moving objects under heavy camera movements. This paper addresses the problem by introducing a clustering approach based on superpixels and short-term Histogram of Oriented Optical Flow (HOOF). Salient Dither Pattern Feature (SDPF) is used as the visual feature to track the flow and Simple Linear Iterative Clustering (SLIC) is used for obtaining the superpixels. This new clustering approach is based on merging superpixels by comparing short term local HOOF and a color cue to form high-level semantic segments. The new approach was compared with one of the latest feature clustering approaches based on K-Means in eight-dimensional space and the results revealed that the new approach is better by means of consistency, completeness, and spatial accuracy. Further, the new approach completely solved the problem of not knowing the number of objects in a scene.",0
"One way to segment objects in videos in a cost-effective manner is through visual feature clustering. However, current algorithms have limitations that prevent them from being useful in scenarios where there are unknown quantities of static and moving objects amidst heavy camera movements. To address this issue, this study presents a clustering approach that utilizes superpixels and short-term Histogram of Oriented Optical Flow (HOOF). The approach involves using Salient Dither Pattern Feature (SDPF) as the visual feature for tracking flow, and Simple Linear Iterative Clustering (SLIC) for obtaining superpixels. The new approach involves merging superpixels based on comparing short term local HOOF and color cues in order to create high-level semantic segments. When compared to the K-Means-based feature clustering approach in eight-dimensional space, the new method demonstrated better results in terms of consistency, completeness, and spatial accuracy. Additionally, the new approach successfully resolved the issue of unknown object quantities in a scene.",1
"Discriminative correlation filters (DCF) with deep convolutional features have achieved favorable performance in recent tracking benchmarks. However, most of existing DCF trackers only consider appearance features of current frame, and hardly benefit from motion and inter-frame information. The lack of temporal information degrades the tracking performance during challenges such as partial occlusion and deformation. In this work, we focus on making use of the rich flow information in consecutive frames to improve the feature representation and the tracking accuracy. Firstly, individual components, including optical flow estimation, feature extraction, aggregation and correlation filter tracking are formulated as special layers in network. To the best of our knowledge, this is the first work to jointly train flow and tracking task in a deep learning framework. Then the historical feature maps at predefined intervals are warped and aggregated with current ones by the guiding of flow. For adaptive aggregation, we propose a novel spatial-temporal attention mechanism. Extensive experiments are performed on four challenging tracking datasets: OTB2013, OTB2015, VOT2015 and VOT2016, and the proposed method achieves superior results on these benchmarks.",0
"Recent tracking benchmarks have shown that discriminative correlation filters (DCF) with deep convolutional features are effective. However, most current DCF trackers only focus on the appearance features of the current frame and do not utilize motion and inter-frame information. This lack of temporal information results in poor tracking performance during challenging situations such as partial occlusion and deformation. To address this issue, this study proposes the use of flow information in consecutive frames to improve feature representation and tracking accuracy. The study introduces optical flow estimation, feature extraction, aggregation and correlation filter tracking as special layers in the network. This work is the first to jointly train flow and tracking tasks in a deep learning framework. Furthermore, the study proposes a novel spatial-temporal attention mechanism for adaptive aggregation. Historical feature maps at predefined intervals are warped and aggregated with current ones using flow as a guide. The proposed method achieves superior results on OTB2013, OTB2015, VOT2015 and VOT2016 tracking datasets in extensive experiments.",1
"Designing a scheme that can achieve a good performance in predicting single person activities and group activities is a challenging task. In this paper, we propose a novel robust and efficient human activity recognition scheme called ReHAR, which can be used to handle single person activities and group activities prediction. First, we generate an optical flow image for each video frame. Then, both video frames and their corresponding optical flow images are fed into a Single Frame Representation Model to generate representations. Finally, an LSTM is used to pre- dict the final activities based on the generated representations. The whole model is trained end-to-end to allow meaningful representations to be generated for the final activity recognition. We evaluate ReHAR using two well-known datasets: the NCAA Basketball Dataset and the UCFSports Action Dataset. The experimental results show that the pro- posed ReHAR achieves a higher activity recognition accuracy with an order of magnitude shorter computation time compared to the state-of-the-art methods.",0
"Coming up with a scheme that can effectively predict both individual and group activities is a difficult undertaking. This paper introduces a new, durable, and efficient human activity recognition system called ReHAR, which is capable of handling both single-person and group activity prediction. The process involves creating an optical flow image for each video frame, feeding both the video frames and their corresponding optical flow images into a Single Frame Representation Model to create representations, and ultimately using an LSTM to predict the final activities based on these representations. The entire model is trained end-to-end to produce meaningful representations for accurate activity recognition. ReHAR is tested on two well-known datasets, the NCAA Basketball Dataset and the UCFSports Action Dataset, and the results show improved accuracy and computation time compared to state-of-the-art methods.",1
"We propose a method for unsupervised video object segmentation by transferring the knowledge encapsulated in image-based instance embedding networks. The instance embedding network produces an embedding vector for each pixel that enables identifying all pixels belonging to the same object. Though trained on static images, the instance embeddings are stable over consecutive video frames, which allows us to link objects together over time. Thus, we adapt the instance networks trained on static images to video object segmentation and incorporate the embeddings with objectness and optical flow features, without model retraining or online fine-tuning. The proposed method outperforms state-of-the-art unsupervised segmentation methods in the DAVIS dataset and the FBMS dataset.",0
"Our suggested approach involves utilizing image-based instance embedding networks to achieve unsupervised video object segmentation. By generating an embedding vector for each pixel, the instance embedding network enables the identification of all pixels pertaining to the same object. Although the network has been trained on still images, the instance embeddings remain consistent across consecutive video frames, facilitating the linking of objects over time. We have successfully adapted the instance networks for video object segmentation by integrating the embeddings with objectness and optical flow features, without requiring any model retraining or online fine-tuning. Our methodology has demonstrated superior performance compared to existing unsupervised segmentation methods in both the DAVIS and FBMS datasets.",1
"Two-stream networks have been very successful for solving the problem of action detection. However, prior work using two-stream networks train both streams separately, which prevents the network from exploiting regularities between the two streams. Moreover, unlike the visual stream, the dominant forms of optical flow computation typically do not maximally exploit GPU parallelism. We present a real-time end-to-end trainable two-stream network for action detection. First, we integrate the optical flow computation in our framework by using Flownet2. Second, we apply early fusion for the two streams and train the whole pipeline jointly end-to-end. Finally, for better network initialization, we transfer from the task of action recognition to action detection by pre-training our framework using the recently released large-scale Kinetics dataset. Our experimental results show that training the pipeline jointly end-to-end with fine-tuning the optical flow for the objective of action detection improves detection performance significantly. Additionally, we observe an improvement when initializing with parameters pre-trained using Kinetics. Last, we show that by integrating the optical flow computation, our framework is more efficient, running at real-time speeds (up to 31 fps).",0
"The problem of action detection has been effectively solved by two-stream networks. However, previous methods have trained the two streams separately, which hinders the network's ability to utilize regularities between them. Additionally, optical flow computation, unlike the visual stream, does not fully exploit GPU parallelism. We have devised a real-time, end-to-end trainable two-stream network for action detection. Firstly, we have integrated Flownet2 optical flow computation into our framework. Secondly, we have employed early fusion for the two streams and trained the entire pipeline end-to-end. Finally, to achieve better network initialization, we have pre-trained our framework using the large-scale Kinetics dataset for action recognition to action detection transfer. Our experiments show that joint end-to-end training and fine-tuning optical flow significantly improves detection performance. We have also observed that pre-training with Kinetics parameters has a positive impact. Lastly, our framework is more efficient, running at real-time speeds (up to 31 fps), thanks to the integration of optical flow computation.",1
"This paper documents the winning entry at the CVPR2017 vehicle velocity estimation challenge. Velocity estimation is an emerging task in autonomous driving which has not yet been thoroughly explored. The goal is to estimate the relative velocity of a specific vehicle from a sequence of images. In this paper, we present a light-weight approach for directly regressing vehicle velocities from their trajectories using a multilayer perceptron. Another contribution is an explorative study of features for monocular vehicle velocity estimation. We find that light-weight trajectory based features outperform depth and motion cues extracted from deep ConvNets, especially for far-distance predictions where current disparity and optical flow estimators are challenged significantly. Our light-weight approach is real-time capable on a single CPU and outperforms all competing entries in the velocity estimation challenge. On the test set, we report an average error of 1.12 m/s which is comparable to a (ground-truth) system that combines LiDAR and radar techniques to achieve an error of around 0.71 m/s.",0
"The focus of this paper is on the winning entry of the CVPR2017 vehicle velocity estimation challenge. Although velocity estimation is a relatively new task in autonomous driving, it is an important one that requires accurate assessment of the relative velocity of a specific vehicle from a sequence of images. The authors present a lightweight approach to directly estimate vehicle velocities by using a multilayer perceptron to regress their trajectories. Additionally, they conduct a study of features for monocular vehicle velocity estimation and discover that trajectory-based features are more effective than depth and motion cues from deep ConvNets, particularly for far-distance prediction where existing disparity and optical flow estimators face significant challenges. The authors' method is real-time capable on a single CPU and has outperformed all other entries in the velocity estimation challenge. The average error on the test set is 1.12 m/s, which is comparable to a (ground-truth) system that combines LiDAR and radar techniques to achieve an error of around 0.71 m/s.",1
"Even with the recent advances in convolutional neural networks (CNN) in various visual recognition tasks, the state-of-the-art action recognition system still relies on hand crafted motion feature such as optical flow to achieve the best performance. We propose a multitask learning model ActionFlowNet to train a single stream network directly from raw pixels to jointly estimate optical flow while recognizing actions with convolutional neural networks, capturing both appearance and motion in a single model. We additionally provide insights to how the quality of the learned optical flow affects the action recognition. Our model significantly improves action recognition accuracy by a large margin 31% compared to state-of-the-art CNN-based action recognition models trained without external large scale data and additional optical flow input. Without pretraining on large external labeled datasets, our model, by well exploiting the motion information, achieves competitive recognition accuracy to the models trained with large labeled datasets such as ImageNet and Sport-1M.",0
"Despite the advances made in convolutional neural networks (CNN) for visual recognition tasks, the current leading action recognition system still relies on manually crafted motion features like optical flow to achieve optimal performance. Our proposed multitask learning model, ActionFlowNet, aims to train a single stream network directly from raw pixels to jointly estimate optical flow and recognize actions using CNNs. This model captures both appearance and motion in a single entity. We also examine how the quality of the learned optical flow affects action recognition and demonstrate that our model significantly improves accuracy by 31% compared to state-of-the-art CNN-based action recognition models that don't use external large-scale data and extra optical flow input. Without pre-training on large external labeled datasets, our model utilizes motion information to achieve competitive accuracy with models trained using datasets like ImageNet and Sport-1M.",1
"Despite recent interest and advances in facial micro-expression research, there is still plenty room for improvement in terms of micro-expression recognition. Conventional feature extraction approaches for micro-expression video consider either the whole video sequence or a part of it, for representation. However, with the high-speed video capture of micro-expressions (100-200 fps), are all frames necessary to provide a sufficiently meaningful representation? Is the luxury of data a bane to accurate recognition? A novel proposition is presented in this paper, whereby we utilize only two images per video: the apex frame and the onset frame. The apex frame of a video contains the highest intensity of expression changes among all frames, while the onset is the perfect choice of a reference frame with neutral expression. A new feature extractor, Bi-Weighted Oriented Optical Flow (Bi-WOOF) is proposed to encode essential expressiveness of the apex frame. We evaluated the proposed method on five micro-expression databases: CAS(ME)$^2$, CASME II, SMIC-HS, SMIC-NIR and SMIC-VIS. Our experiments lend credence to our hypothesis, with our proposed technique achieving a state-of-the-art F1-score recognition performance of 61% and 62% in the high frame rate CASME II and SMIC-HS databases respectively.",0
"Although there have been recent advancements in facial micro-expression research, there is still room for improvement in terms of micro-expression recognition. Traditional techniques for extracting features from micro-expression video either use the entire video sequence or a portion of it for representation. However, with micro-expressions being captured at a high-speed rate (100-200 fps), it is unclear whether all frames are necessary for a meaningful representation and whether the abundance of data hinders accurate recognition. This paper proposes a new approach that utilizes only two images per video: the apex frame and the onset frame. The apex frame contains the highest intensity of expression changes, while the onset frame serves as a reference frame with a neutral expression. The paper also introduces a new feature extractor, Bi-Weighted Oriented Optical Flow (Bi-WOOF), to encode the essential expressiveness of the apex frame. The proposed method was evaluated on five micro-expression databases, with the results demonstrating its superiority over existing methods, achieving a state-of-the-art F1-score recognition performance of 61% and 62% in the high frame rate CASME II and SMIC-HS databases, respectively.",1
"Wood-composite materials are widely used today as they homogenize humidity related directional deformations. Quantification of these deformations as coefficients is important for construction and engineering and topic of current research but still a manual process.   This work introduces a novel computer vision approach that automatically extracts these properties directly from scans of the wooden specimens, taken at different humidity levels during the long lasting humidity conditioning process. These scans are used to compute a humidity dependent deformation field for each pixel, from which the desired coefficients can easily be calculated.   The overall method includes automated registration of the wooden blocks, numerical optimization to compute a variational optical flow field which is further used to calculate dense strain fields and finally the engineering coefficients and their variance throughout the wooden blocks. The methods regularization is fully parameterizable which allows to model and suppress artifacts due to surface appearance changes of the specimens from mold, cracks, etc. that typically arise in the conditioning process.",0
"Nowadays, wood-composite materials are extensively utilized due to their ability to standardize humidity-related directional distortions. It is crucial in construction and engineering to quantify these distortions as coefficients, which is currently a manual process and an area of active research. This study presents a new computer vision technique that automatically captures these attributes directly from scans of the wooden pieces, taken at different humidity levels during a prolonged humidity conditioning process. These scans enable the computation of a humidity-dependent distortion field for each pixel, from which the desired coefficients can be easily determined. The entire method includes the automated alignment of the wooden blocks, the numerical optimization of a variational optical flow field, the calculation of dense strain fields, and ultimately the engineering coefficients and their variance throughout the wooden blocks. The regularization of the method is completely customizable, enabling the modeling and suppression of artifacts resulting from surface appearance changes such as mold and cracks that commonly occur during the conditioning process.",1
"In this paper, we present an unsupervised learning framework for analyzing activities and interactions in surveillance videos. In our framework, three levels of video events are connected by Hierarchical Dirichlet Process (HDP) model: low-level visual features, simple atomic activities, and multi-agent interactions. Atomic activities are represented as distribution of low-level features, while complicated interactions are represented as distribution of atomic activities. This learning process is unsupervised. Given a training video sequence, low-level visual features are extracted based on optic flow and then clustered into different atomic activities and video clips are clustered into different interactions. The HDP model automatically decide the number of clusters, i.e. the categories of atomic activities and interactions. Based on the learned atomic activities and interactions, a training dataset is generated to train the Gaussian Process (GP) classifier. Then the trained GP models work in newly captured video to classify interactions and detect abnormal events in real time. Furthermore, the temporal dependencies between video events learned by HDP-Hidden Markov Models (HMM) are effectively integrated into GP classifier to enhance the accuracy of the classification in newly captured videos. Our framework couples the benefits of the generative model (HDP) with the discriminant model (GP). We provide detailed experiments showing that our framework enjoys favorable performance in video event classification in real-time in a crowded traffic scene.",0
"This paper introduces a framework for unsupervised learning to analyze activities and interactions in surveillance videos. The framework utilizes the Hierarchical Dirichlet Process (HDP) model to connect three levels of video events: low-level visual features, simple atomic activities, and multi-agent interactions. The atomic activities and interactions are represented as distributions of low-level features and atomic activities, respectively. The HDP model automatically determines the number of clusters and categories of atomic activities and interactions. The learned atomic activities and interactions are used to generate a training dataset to train the Gaussian Process (GP) classifier, which is then utilized to classify interactions and detect abnormal events in real time. The temporal dependencies between video events are integrated into the GP classifier using HDP-Hidden Markov Models (HMM) to enhance classification accuracy. Experimental results demonstrate that the framework performs well in video event classification in real time, particularly in crowded traffic scenes, by combining the generative model (HDP) with the discriminant model (GP).",1
"Human gait or walking manner is a biometric feature that allows identification of a person when other biometric features such as the face or iris are not visible. In this paper, we present a new pose-based convolutional neural network model for gait recognition. Unlike many methods that consider the full-height silhouette of a moving person, we consider the motion of points in the areas around human joints. To extract motion information, we estimate the optical flow between consecutive frames. We propose a deep convolutional model that computes pose-based gait descriptors. We compare different network architectures and aggregation methods and experimentally assess various sets of body parts to determine which are the most important for gait recognition. In addition, we investigate the generalization ability of the developed algorithms by transferring them between datasets. The results of these experiments show that our approach outperforms state-of-the-art methods.",0
"The way a person walks, known as their gait or walking manner, can be used as a biometric feature to identify them in situations where other biometric features like the face or iris are not visible. This paper presents a new model for gait recognition using a pose-based convolutional neural network. Rather than considering the full-height silhouette of a moving person, this model focuses on the motion of points around human joints to extract motion information using optical flow between consecutive frames. The proposed model computes pose-based gait descriptors and compares different network architectures and aggregation methods. The study also determines the most important body parts for gait recognition and investigates the generalization ability of the developed algorithms by transferring them between datasets. The results show that this approach outperforms current methods.",1
"Computer vision researchers have been expecting that neural networks have spatial transformation ability to eliminate the interference caused by geometric distortion for a long time. Emergence of spatial transformer network makes dream come true. Spatial transformer network and its variants can handle global displacement well, but lack the ability to deal with local spatial variance. Hence how to achieve a better manner of deformation in the neural network has become a pressing matter of the moment. To address this issue, we analyze the advantages and disadvantages of approximation theory and optical flow theory, then we combine them to propose a novel way to achieve image deformation and implement it with a hierarchical convolutional neural network. This new approach solves for a linear deformation along with an optical flow field to model image deformation. In the experiments of cluttered MNIST handwritten digits classification and image plane alignment, our method outperforms baseline methods by a large margin.",0
"For a long time, computer vision researchers have been anticipating neural networks to possess spatial transformation abilities that can eliminate interference from geometric distortion. The emergence of the spatial transformer network has made this dream a reality. Although the spatial transformer network and its variations are capable of handling global displacement, they lack the ability to address local spatial variance. This has become a pressing matter, and to overcome this issue, we have analyzed the advantages and disadvantages of the approximation theory and optical flow theory. By combining them, we propose a new approach to achieve image deformation, which we implement with a hierarchical convolutional neural network. Our method solves for a linear deformation along with an optical flow field to model image deformation, outperforming baseline methods by a large margin in experiments involving cluttered MNIST handwritten digits classification and image plane alignment.",1
"In this paper, we present a new method for detecting road users in an urban environment which leads to an improvement in multiple object tracking. Our method takes as an input a foreground image and improves the object detection and segmentation. This new image can be used as an input to trackers that use foreground blobs from background subtraction. The first step is to create foreground images for all the frames in an urban video. Then, starting from the original blobs of the foreground image, we merge the blobs that are close to one another and that have similar optical flow. The next step is extracting the edges of the different objects to detect multiple objects that might be very close (and be merged in the same blob) and to adjust the size of the original blobs. At the same time, we use the optical flow to detect occlusion of objects that are moving in opposite directions. Finally, we make a decision on which information we keep in order to construct a new foreground image with blobs that can be used for tracking. The system is validated on four videos of an urban traffic dataset. Our method improves the recall and precision metrics for the object detection task compared to the vanilla background subtraction method and improves the CLEAR MOT metrics in the tracking tasks for most videos.",0
"The purpose of this paper is to introduce a novel approach for identifying road users in urban settings, with the aim of enhancing multiple object tracking. The new method uses an initial foreground image and improves object detection and segmentation. This updated image can then be utilized as an input for trackers that rely on foreground blobs from background subtraction. The process begins by generating foreground images for all frames in an urban video. Next, we merge nearby blobs with similar optical flow, and detect object edges to identify objects that may be too close and adjust the size of the original blobs. Additionally, we use optical flow to detect occlusion of objects moving in opposite directions. Ultimately, we decide which information to retain and construct a new foreground image with blobs suitable for tracking. We tested the system on four urban traffic videos, and our method demonstrated improved recall and precision metrics for object detection, compared to the vanilla background subtraction approach. Furthermore, the CLEAR MOT metrics for tracking tasks were also improved for most of the videos.",1
"Most of the crowd abnormal event detection methods rely on complex hand-crafted features to represent the crowd motion and appearance. Convolutional Neural Networks (CNN) have shown to be a powerful tool with excellent representational capacities, which can leverage the need for hand-crafted features. In this paper, we show that keeping track of the changes in the CNN feature across time can facilitate capturing the local abnormality. We specifically propose a novel measure-based method which allows measuring the local abnormality in a video by combining semantic information (inherited from existing CNN models) with low-level Optical-Flow. One of the advantage of this method is that it can be used without the fine-tuning costs. The proposed method is validated on challenging abnormality detection datasets and the results show the superiority of our method compared to the state-of-the-art methods.",0
"The majority of methods for detecting abnormal events in crowds rely on intricate manual features to represent their movement and appearance. Convolutional Neural Networks (CNNs) have proven to be highly effective, negating the need for such features. This study demonstrates that by monitoring changes in CNN features over time, it becomes easier to identify local abnormalities. The study proposes a novel measure-based method that combines semantic information with low-level Optical-Flow to measure local abnormality in videos. One benefit of this method is that it doesn't require costly fine-tuning. The proposed method is tested on rigorous abnormality detection datasets, and the results demonstrate its superiority over state-of-the-art methods.",1
"In recent years, deep neural network approaches have naturally extended to the video domain, in their simplest case by aggregating per-frame classifications as a baseline for action recognition. A majority of the work in this area extends from the imaging domain, leading to visual-feature heavy approaches on temporal data. To address this issue we introduce ""Let's Dance"", a 1000 video dataset (and growing) comprised of 10 visually overlapping dance categories that require motion for their classification. We stress the important of human motion as a key distinguisher in our work given that, as we show in this work, visual information is not sufficient to classify motion-heavy categories. We compare our datasets' performance using imaging techniques with UCF-101 and demonstrate this inherent difficulty. We present a comparison of numerous state-of-the-art techniques on our dataset using three different representations (video, optical flow and multi-person pose data) in order to analyze these approaches. We discuss the motion parameterization of each of them and their value in learning to categorize online dance videos. Lastly, we release this dataset (and its three representations) for the research community to use.",0
"Over the last few years, deep neural network methods have naturally expanded into the realm of videos. In its simplest form, action recognition is based on per-frame classifications. Much of the work in this area has been inspired by the imaging domain, resulting in visually-intensive approaches to temporal data. To overcome this limitation, we present ""Let's Dance,"" a dataset of 1000 videos (and increasing) that features ten dance categories with visual overlap, with motion being the key factor for classification. Our work emphasizes the importance of human motion as a critical distinguishing factor, as we demonstrate the insufficiency of visual information for motion-heavy categories. We compare our dataset's performance to UCF-101 using imaging techniques, highlighting the inherent difficulty. We analyze various state-of-the-art techniques on our dataset using three different representations (video, optical flow, and multi-person pose data) and discuss the motion parameterization of each approach, as well as their value in online dance video categorization. Lastly, we make this dataset (along with its three representations) available to the research community.",1
"Motion blur is a fundamental problem in computer vision as it impacts image quality and hinders inference. Traditional deblurring algorithms leverage the physics of the image formation model and use hand-crafted priors: they usually produce results that better reflect the underlying scene, but present artifacts. Recent learning-based methods implicitly extract the distribution of natural images directly from the data and use it to synthesize plausible images. Their results are impressive, but they are not always faithful to the content of the latent image. We present an approach that bridges the two. Our method fine-tunes existing deblurring neural networks in a self-supervised fashion by enforcing that the output, when blurred based on the optical flow between subsequent frames, matches the input blurry image. We show that our method significantly improves the performance of existing methods on several datasets both visually and in terms of image quality metrics. The supplementary material is https://goo.gl/nYPjEQ",0
"The issue of motion blur is a fundamental concern in computer vision because it negatively impacts image quality and impedes inference. Traditional deblurring algorithms rely on the physics of the image formation model and incorporate hand-crafted priors, which generally result in images that better reflect the original scene but may have artifacts. Recent learning-based approaches extract the distribution of natural images from the data to synthesize believable images, but these images may not always be faithful to the latent image. Our method combines both approaches by fine-tuning existing deblurring neural networks in a self-supervised manner. We enforce that the output, when blurred based on the optical flow between subsequent frames, matches the input blurry image. Our approach has demonstrated significant improvement in the performance of existing deblurring methods on various datasets, both visually and in terms of image quality metrics. Further details can be found in the supplementary material at https://goo.gl/nYPjEQ.",1
"Scene flow is a description of real world motion in 3D that contains more information than optical flow. Because of its complexity there exists no applicable variant for real-time scene flow estimation in an automotive or commercial vehicle context that is sufficiently robust and accurate. Therefore, many applications estimate the 2D optical flow instead. In this paper, we examine the combination of top-performing state-of-the-art optical flow and stereo disparity algorithms in order to achieve a basic scene flow. On the public KITTI Scene Flow Benchmark we demonstrate the reasonable accuracy of the combination approach and show its speed in computation.",0
"Scene flow is a 3D representation of actual motion that provides more comprehensive data than optical flow. However, due to its intricacy, there is no viable option for real-time scene flow estimation in the automotive or commercial vehicle industry that is both reliable and precise. Consequently, many applications rely on approximating the 2D optical flow instead. This study investigates the fusion of cutting-edge optical flow and stereo disparity algorithms to create a basic scene flow. Our results on the KITTI Scene Flow Benchmark indicate that this combination approach is reasonably accurate and computationally efficient.",1
"Light field imaging has recently known a regain of interest due to the availability of practical light field capturing systems that offer a wide range of applications in the field of computer vision. However, capturing high-resolution light fields remains technologically challenging since the increase in angular resolution is often accompanied by a significant reduction in spatial resolution. This paper describes a learning-based spatial light field super-resolution method that allows the restoration of the entire light field with consistency across all sub-aperture images. The algorithm first uses optical flow to align the light field and then reduces its angular dimension using low-rank approximation. We then consider the linearly independent columns of the resulting low-rank model as an embedding, which is restored using a deep convolutional neural network (DCNN). The super-resolved embedding is then used to reconstruct the remaining sub-aperture images. The original disparities are restored using inverse warping where missing pixels are approximated using a novel light field inpainting algorithm. Experimental results show that the proposed method outperforms existing light field super-resolution algorithms, achieving PSNR gains of 0.23 dB over the second best performing method. This performance can be further improved using iterative back-projection as a post-processing step.",0
"The recent availability of practical light field capturing systems has renewed interest in light field imaging due to its potential applications in computer vision. However, capturing high-resolution light fields is still a challenging task, as increasing angular resolution often comes at the expense of spatial resolution. This paper presents a learning-based approach to spatial light field super-resolution, which restores the entire light field with consistency across all sub-aperture images. The algorithm aligns the light field using optical flow and reduces its angular dimension using low-rank approximation. It then restores the embedding using a deep convolutional neural network, which is used to reconstruct the remaining sub-aperture images. The original disparities are restored using inverse warping, and missing pixels are approximated using a novel light field inpainting algorithm. The proposed method outperforms existing light field super-resolution algorithms, achieving PSNR gains of 0.23 dB over the second best performing method, and iterative back-projection can further improve performance.",1
"Most of the top performing action recognition methods use optical flow as a ""black box"" input. Here we take a deeper look at the combination of flow and action recognition, and investigate why optical flow is helpful, what makes a flow method good for action recognition, and how we can make it better. In particular, we investigate the impact of different flow algorithms and input transformations to better understand how these affect a state-of-the-art action recognition method. Furthermore, we fine tune two neural-network flow methods end-to-end on the most widely used action recognition dataset (UCF101). Based on these experiments, we make the following five observations: 1) optical flow is useful for action recognition because it is invariant to appearance, 2) optical flow methods are optimized to minimize end-point-error (EPE), but the EPE of current methods is not well correlated with action recognition performance, 3) for the flow methods tested, accuracy at boundaries and at small displacements is most correlated with action recognition performance, 4) training optical flow to minimize classification error instead of minimizing EPE improves recognition performance, and 5) optical flow learned for the task of action recognition differs from traditional optical flow especially inside the human body and at the boundary of the body. These observations may encourage optical flow researchers to look beyond EPE as a goal and guide action recognition researchers to seek better motion cues, leading to a tighter integration of the optical flow and action recognition communities.",0
"The majority of the highest performing methods for action recognition use optical flow as a ""black box"" input. However, in this study, we delve deeper into the combination of flow and action recognition. We examine the benefits of optical flow and explore what makes a flow method effective for action recognition, as well as how we can enhance its performance. Specifically, we analyze the impact of different flow algorithms and input transformations to gain a better understanding of how they influence a state-of-the-art action recognition method. Additionally, we fine tune two neural-network flow methods end-to-end using the most widely used action recognition dataset (UCF101). Through these experiments, we draw five key observations: 1) Optical flow is advantageous for action recognition because it is impervious to appearance, 2) Optical flow methods are designed to minimize end-point-error (EPE), but the EPE of current methods is not strongly correlated with action recognition performance, 3) Among the flow methods tested, accuracy at boundaries and small displacements is most closely linked with action recognition performance, 4) Training optical flow to minimize classification error rather than EPE enhances recognition performance, and 5) Optical flow learned for action recognition differs from traditional optical flow, particularly within the human body and at the body's boundary. These findings should encourage optical flow researchers to expand their focus beyond EPE and encourage action recognition researchers to seek better motion cues, resulting in a more seamless integration of the optical flow and action recognition communities.",1
"Video image datasets are playing an essential role in design and evaluation of traffic vision algorithms. Nevertheless, a longstanding inconvenience concerning image datasets is that manually collecting and annotating large-scale diversified datasets from real scenes is time-consuming and prone to error. For that virtual datasets have begun to function as a proxy of real datasets. In this paper, we propose to construct large-scale artificial scenes for traffic vision research and generate a new virtual dataset called ""ParallelEye"". First of all, the street map data is used to build 3D scene model of Zhongguancun Area, Beijing. Then, the computer graphics, virtual reality, and rule modeling technologies are utilized to synthesize large-scale, realistic virtual urban traffic scenes, in which the fidelity and geography match the real world well. Furthermore, the Unity3D platform is used to render the artificial scenes and generate accurate ground-truth labels, e.g., semantic/instance segmentation, object bounding box, object tracking, optical flow, and depth. The environmental conditions in artificial scenes can be controlled completely. As a result, we present a viable implementation pipeline for constructing large-scale artificial scenes for traffic vision research. The experimental results demonstrate that this pipeline is able to generate photorealistic virtual datasets with low modeling time and high accuracy labeling.",0
"The use of video image datasets is crucial for designing and assessing traffic vision algorithms. However, manually collecting and annotating real-world datasets can be time-consuming and prone to errors. To overcome this, virtual datasets are being utilized as a substitute for real ones. To this end, we propose creating large-scale artificial scenes for traffic vision research and introduce a virtual dataset named ""ParallelEye."" Our approach involves using street map data to construct a 3D model of the Zhongguancun Area in Beijing, followed by synthesizing realistic virtual urban traffic scenes using computer graphics, virtual reality, and rule modeling technologies. The Unity3D platform is used to render these artificial scenes and generate accurate ground-truth labels, including semantic/instance segmentation, object bounding box, object tracking, optical flow, and depth. The environmental conditions in these scenes can be fully controlled, resulting in a viable implementation pipeline for constructing large-scale artificial scenes for traffic vision research. Our experimental results show that this pipeline can generate photorealistic virtual datasets with low modeling time and high accuracy labeling.",1
"We investigate video classification via a two-stream convolutional neural network (CNN) design that directly ingests information extracted from compressed video bitstreams. Our approach begins with the observation that all modern video codecs divide the input frames into macroblocks (MBs). We demonstrate that selective access to MB motion vector (MV) information within compressed video bitstreams can also provide for selective, motion-adaptive, MB pixel decoding (a.k.a., MB texture decoding). This in turn allows for the derivation of spatio-temporal video activity regions at extremely high speed in comparison to conventional full-frame decoding followed by optical flow estimation. In order to evaluate the accuracy of a video classification framework based on such activity data, we independently train two CNN architectures on MB texture and MV correspondences and then fuse their scores to derive the final classification of each test video. Evaluation on two standard datasets shows that the proposed approach is competitive to the best two-stream video classification approaches found in the literature. At the same time: (i) a CPU-based realization of our MV extraction is over 977 times faster than GPU-based optical flow methods; (ii) selective decoding is up to 12 times faster than full-frame decoding; (iii) our proposed spatial and temporal CNNs perform inference at 5 to 49 times lower cloud computing cost than the fastest methods from the literature.",0
"Our research focuses on video classification using a two-stream CNN design that takes data from compressed video bitstreams. Our method utilizes macroblocks (MBs) that modern video codecs use to divide input frames. We discovered that selective access to MB motion vector (MV) information within compressed video bitstreams can allow for selective, motion-adaptive, MB pixel decoding. This technique lets us obtain spatio-temporal video activity regions at a much faster rate than conventional full-frame decoding followed by optical flow estimation. To test our video classification framework's accuracy using this activity data, we trained two CNN architectures independently on MB texture and MV correspondences. We then combined their scores to classify each test video. Our approach is competitive with the best two-stream video classification methods in the literature. Additionally, our MV extraction is over 977 times faster than GPU-based optical flow methods, selective decoding is up to 12 times faster than full-frame decoding, and our proposed spatial and temporal CNNs perform inference at 5 to 49 times lower cloud computing cost than the fastest methods in the literature.",1
"This work proposes a novel deep network architecture to solve the camera Ego-Motion estimation problem. A motion estimation network generally learns features similar to Optical Flow (OF) fields starting from sequences of images. This OF can be described by a lower dimensional latent space. Previous research has shown how to find linear approximations of this space. We propose to use an Auto-Encoder network to find a non-linear representation of the OF manifold. In addition, we propose to learn the latent space jointly with the estimation task, so that the learned OF features become a more robust description of the OF input. We call this novel architecture LS-VO.   The experiments show that LS-VO achieves a considerable increase in performances in respect to baselines, while the number of parameters of the estimation network only slightly increases.",0
"In this work, a new deep network architecture is presented for solving the problem of estimating camera Ego-Motion. Typically, a motion estimation network learns features similar to Optical Flow (OF) fields from a sequence of images, which can be represented by a lower dimensional latent space. Previous studies have demonstrated how to find linear approximations of this space, but we propose using an Auto-Encoder network to discover a non-linear representation of the OF manifold. Furthermore, we suggest jointly learning the latent space with the estimation task to improve the robustness of the learned OF features. This innovative architecture is called LS-VO. Experimental results indicate that LS-VO significantly outperforms baseline methods, while only slightly increasing the number of parameters in the estimation network.",1
"While deep feature learning has revolutionized techniques for static-image understanding, the same does not quite hold for video processing. Architectures and optimization techniques used for video are largely based off those for static images, potentially underutilizing rich video information. In this work, we rethink both the underlying network architecture and the stochastic learning paradigm for temporal data. To do so, we draw inspiration from classic theory on linear dynamic systems for modeling time series. By extending such models to include nonlinear mappings, we derive a series of novel recurrent neural networks that sequentially make top-down predictions about the future and then correct those predictions with bottom-up observations. Predictive-corrective networks have a number of desirable properties: (1) they can adaptively focus computation on ""surprising"" frames where predictions require large corrections, (2) they simplify learning in that only ""residual-like"" corrective terms need to be learned over time and (3) they naturally decorrelate an input data stream in a hierarchical fashion, producing a more reliable signal for learning at each layer of a network. We provide an extensive analysis of our lightweight and interpretable framework, and demonstrate that our model is competitive with the two-stream network on three challenging datasets without the need for computationally expensive optical flow.",0
"Although deep feature learning has transformed static-image comprehension, it has not had the same impact on video processing. Video architectures and optimization techniques have largely been built on those used for static images, which may overlook valuable video data. This research reconsiders the network architecture and stochastic learning paradigm for temporal data. Linear dynamic systems theory for modeling time series inspires the new models, which are recurrent neural networks that predict future frames and then adjust those predictions with current observations. Predictive-corrective networks have various benefits, including the ability to focus computation on ""surprising"" frames, simplify learning, and generate a more reliable signal at each layer of the network. We provide an extensive evaluation of our lightweight and interpretable framework, which performs competitively with the two-stream network on three challenging datasets without requiring computationally expensive optical flow.",1
"This paper proposes a deep learning model to efficiently detect salient regions in videos. It addresses two important issues: (1) deep video saliency model training with the absence of sufficiently large and pixel-wise annotated video data, and (2) fast video saliency training and detection. The proposed deep video saliency network consists of two modules, for capturing the spatial and temporal saliency information, respectively. The dynamic saliency model, explicitly incorporating saliency estimates from the static saliency model, directly produces spatiotemporal saliency inference without time-consuming optical flow computation. We further propose a novel data augmentation technique that simulates video training data from existing annotated image datasets, which enables our network to learn diverse saliency information and prevents overfitting with the limited number of training videos. Leveraging our synthetic video data (150K video sequences) and real videos, our deep video saliency model successfully learns both spatial and temporal saliency cues, thus producing accurate spatiotemporal saliency estimate. We advance the state-of-the-art on the DAVIS dataset (MAE of .06) and the FBMS dataset (MAE of .07), and do so with much improved speed (2fps with all steps).",0
"The aim of this paper is to suggest an effective deep learning model for identifying significant regions in videos, which targets two main concerns. Firstly, the lack of sufficiently large video data with pixel-wise annotations for deep video saliency model training, and secondly, the need for fast video saliency training and detection. The proposed deep video saliency network comprises of two modules for capturing spatial and temporal saliency information separately. The dynamic saliency model incorporates saliency estimates from the static saliency model, enabling it to generate spatiotemporal saliency inference without the time-consuming optical flow computation. Additionally, a novel data augmentation technique is proposed to simulate video training data from current annotated image datasets, which helps in the diverse learning of saliency information and avoids overfitting with the limited number of training videos. By utilizing our synthetic and real video data, our deep video saliency model successfully recognizes both spatial and temporal saliency cues, resulting in an accurate spatiotemporal saliency estimate. Our approach improves the state-of-the-art on the DAVIS dataset (MAE of .06) and the FBMS dataset (MAE of .07) while also achieving much faster speed (2fps with all steps).",1
"This paper presents a novel method for detecting scene changes from a pair of images with a difference of camera viewpoints using a dense optical flow based change detection network. In the case that camera poses of input images are fixed or known, such as with surveillance and satellite cameras, the pixel correspondence between the images captured at different times can be known. Hence, it is possible to comparatively accurately detect scene changes between the images by modeling the appearance of the scene. On the other hand, in case of cameras mounted on a moving object, such as ground and aerial vehicles, we must consider the spatial correspondence between the images captured at different times. However, it can be difficult to accurately estimate the camera pose or 3D model of a scene, owing to the scene changes or lack of imagery. To solve this problem, we propose a change detection convolutional neural network utilizing dense optical flow between input images to improve the robustness to the difference between camera viewpoints. Our evaluation based on the panoramic change detection dataset shows that the proposed method outperforms state-of-the-art change detection algorithms.",0
"A new technique is presented in this paper for identifying changes in a scene using a dense optical flow based detection network, when comparing two images with different camera viewpoints. When the cameras are fixed or known, such as with surveillance and satellite cameras, the appearance of the scene can be modeled to accurately detect differences between the images. However, when cameras are mounted on moving objects, such as ground and aerial vehicles, spatial correspondence between the images must be considered, which can be difficult due to changes in the scene or lack of imagery. To address this issue, a change detection convolutional neural network is proposed that utilizes dense optical flow to improve robustness to differences in camera viewpoints. Results from the panoramic change detection dataset show that this method outperforms other state-of-the-art change detection algorithms.",1
"Current state-of-the-art solutions for motion capture from a single camera are optimization driven: they optimize the parameters of a 3D human model so that its re-projection matches measurements in the video (e.g. person segmentation, optical flow, keypoint detections etc.). Optimization models are susceptible to local minima. This has been the bottleneck that forced using clean green-screen like backgrounds at capture time, manual initialization, or switching to multiple cameras as input resource. In this work, we propose a learning based motion capture model for single camera input. Instead of optimizing mesh and skeleton parameters directly, our model optimizes neural network weights that predict 3D shape and skeleton configurations given a monocular RGB video. Our model is trained using a combination of strong supervision from synthetic data, and self-supervision from differentiable rendering of (a) skeletal keypoints, (b) dense 3D mesh motion, and (c) human-background segmentation, in an end-to-end framework. Empirically we show our model combines the best of both worlds of supervised learning and test-time optimization: supervised learning initializes the model parameters in the right regime, ensuring good pose and surface initialization at test time, without manual effort. Self-supervision by back-propagating through differentiable rendering allows (unsupervised) adaptation of the model to the test data, and offers much tighter fit than a pretrained fixed model. We show that the proposed model improves with experience and converges to low-error solutions where previous optimization methods fail.",0
"At present, the leading solutions for capturing motion from a single camera are driven by optimization. This involves optimizing the parameters of a 3D human model to match measurements in the video, such as person segmentation, optical flow, and keypoint detections. However, optimization models can be limited by local minima, which has resulted in the use of clean backgrounds, manual initialization, or multiple cameras. To overcome these challenges, we introduce a motion capture model that is based on learning and designed for single camera input. Our model optimizes neural network weights to predict 3D shape and skeleton configurations from a monocular RGB video, rather than optimizing mesh and skeleton parameters directly. It is trained using strong supervision from synthetic data and self-supervision from differentiable rendering. This approach combines the benefits of both supervised learning and test-time optimization, resulting in good pose and surface initialization at test time without manual effort. Our model also enables unsupervised adaptation to test data and offers a much tighter fit than a pretrained fixed model, improving with experience and converging to low-error solutions where previous optimization methods fail.",1
"We study the problem of segmenting moving objects in unconstrained videos. Given a video, the task is to segment all the objects that exhibit independent motion in at least one frame. We formulate this as a learning problem and design our framework with three cues: (i) independent object motion between a pair of frames, which complements object recognition, (ii) object appearance, which helps to correct errors in motion estimation, and (iii) temporal consistency, which imposes additional constraints on the segmentation. The framework is a two-stream neural network with an explicit memory module. The two streams encode appearance and motion cues in a video sequence respectively, while the memory module captures the evolution of objects over time, exploiting the temporal consistency. The motion stream is a convolutional neural network trained on synthetic videos to segment independently moving objects in the optical flow field. The module to build a 'visual memory' in video, i.e., a joint representation of all the video frames, is realized with a convolutional recurrent unit learned from a small number of training video sequences.   For every pixel in a frame of a test video, our approach assigns an object or background label based on the learned spatio-temporal features as well as the 'visual memory' specific to the video. We evaluate our method extensively on three benchmarks, DAVIS, Freiburg-Berkeley motion segmentation dataset and SegTrack. In addition, we provide an extensive ablation study to investigate both the choice of the training data and the influence of each component in the proposed framework.",0
"Our focus is on segmenting moving objects in videos with no constraints. The objective is to segment all objects showing independent motion in at least one frame. We approach this as a learning problem and create a framework with three cues: (i) independent object motion, (ii) object appearance, and (iii) temporal consistency. Our framework is a two-stream neural network with a memory module that represents the evolution of objects over time, ensuring temporal consistency. The motion stream is trained on synthetic videos to segment independently moving objects in the optical flow field. The memory module builds a 'visual memory' of all video frames using a convolutional recurrent unit. Our approach assigns an object or background label to every pixel in a test video, based on learned spatio-temporal features and the video's 'visual memory.' We evaluate our method on three benchmarks, DAVIS, Freiburg-Berkeley motion segmentation dataset, and SegTrack, and conduct an ablation study to examine our framework's components and training data choice.",1
"Learning to represent and generate videos from unlabeled data is a very challenging problem. To generate realistic videos, it is important not only to ensure that the appearance of each frame is real, but also to ensure the plausibility of a video motion and consistency of a video appearance in the time direction. The process of video generation should be divided according to these intrinsic difficulties. In this study, we focus on the motion and appearance information as two important orthogonal components of a video, and propose Flow-and-Texture-Generative Adversarial Networks (FTGAN) consisting of FlowGAN and TextureGAN. In order to avoid a huge annotation cost, we have to explore a way to learn from unlabeled data. Thus, we employ optical flow as motion information to generate videos. FlowGAN generates optical flow, which contains only the edge and motion of the videos to be begerated. On the other hand, TextureGAN specializes in giving a texture to optical flow generated by FlowGAN. This hierarchical approach brings more realistic videos with plausible motion and appearance consistency. Our experiments show that our model generates more plausible motion videos and also achieves significantly improved performance for unsupervised action classification in comparison to previous GAN works. In addition, because our model generates videos from two independent information, our model can generate new combinations of motion and attribute that are not seen in training data, such as a video in which a person is doing sit-up in a baseball ground.",0
"Generating and representing videos from unlabeled data is an arduous task as it requires ensuring the realism of every frame, the plausibility of video motion, and consistency of video appearance in the time direction. Therefore, the video generation process should be divided based on these intrinsic difficulties. The current study focuses on motion and appearance information as two crucial orthogonal components of a video, and proposes Flow-and-Texture-Generative Adversarial Networks (FTGAN), comprising FlowGAN and TextureGAN. To avoid incurring huge annotation costs, optical flow is employed as motion information for generating videos. FlowGAN generates optical flow containing only the edge and motion of the videos to be generated, while TextureGAN provides a texture to the optical flow generated by FlowGAN. This hierarchical approach results in more realistic videos with plausible motion and appearance consistency. The model's effectiveness is demonstrated by its ability to generate videos with new combinations of motion and attribute that are not present in the training data, such as a video depicting a person doing sit-ups on a baseball field. Furthermore, the model outperforms previous GAN models in generating videos with more plausible motion and achieving significantly improved performance in unsupervised action classification.",1
"Optical flow estimation in the rainy scenes is challenging due to background degradation introduced by rain streaks and rain accumulation effects in the scene. Rain accumulation effect refers to poor visibility of remote objects due to the intense rainfall. Most existing optical flow methods are erroneous when applied to rain sequences because the conventional brightness constancy constraint (BCC) and gradient constancy constraint (GCC) generally break down in this situation. Based on the observation that the RGB color channels receive raindrop radiance equally, we introduce a residue channel as a new data constraint to reduce the effect of rain streaks. To handle rain accumulation, our method decomposes the image into a piecewise-smooth background layer and a high-frequency detail layer. It also enforces the BCC on the background layer only. Results on both synthetic dataset and real images show that our algorithm outperforms existing methods on different types of rain sequences. To our knowledge, this is the first optical flow method specifically dealing with rain.",0
"Rainy scenes pose a challenge for optical flow estimation due to degradation of the background caused by rain streaks and accumulation. The latter results in poor visibility of distant objects due to heavy rainfall. Traditional optical flow methods are not reliable for such sequences as the brightness constancy constraint (BCC) and gradient constancy constraint (GCC) are often invalid in this scenario. We propose a novel approach that utilizes the equal distribution of raindrop radiance among RGB color channels to introduce a residue channel, which reduces the impact of rain streaks. Additionally, we decompose the image into a piecewise-smooth background layer and a high-frequency detail layer to handle rain accumulation and enforce BCC only on the background layer. Our algorithm outperforms existing methods on both synthetic and real images of various types of rain sequences, making it the first optical flow method that specifically addresses the issue of rain.",1
"In the era of end-to-end deep learning, many advances in computer vision are driven by large amounts of labeled data. In the optical flow setting, however, obtaining dense per-pixel ground truth for real scenes is difficult and thus such data is rare. Therefore, recent end-to-end convolutional networks for optical flow rely on synthetic datasets for supervision, but the domain mismatch between training and test scenarios continues to be a challenge. Inspired by classical energy-based optical flow methods, we design an unsupervised loss based on occlusion-aware bidirectional flow estimation and the robust census transform to circumvent the need for ground truth flow. On the KITTI benchmarks, our unsupervised approach outperforms previous unsupervised deep networks by a large margin, and is even more accurate than similar supervised methods trained on synthetic datasets alone. By optionally fine-tuning on the KITTI training data, our method achieves competitive optical flow accuracy on the KITTI 2012 and 2015 benchmarks, thus in addition enabling generic pre-training of supervised networks for datasets with limited amounts of ground truth.",0
"Computer vision advancements in the age of end-to-end deep learning rely heavily on large amounts of labeled data. However, obtaining dense per-pixel ground truth for real-life scenes in the optical flow setting is an arduous task, making such data scarce. Consequently, end-to-end convolutional networks for optical flow rely on synthetic datasets for supervision, but the domain mismatch between training and testing scenarios proves challenging. Drawing inspiration from traditional energy-based optical flow methods, we have devised an unsupervised loss based on occlusion-aware bidirectional flow estimation and the robust census transform to overcome the need for ground truth flow. Our unsupervised approach far surpasses earlier unsupervised deep networks on the KITTI benchmarks and performs even better than similar supervised methods trained solely on synthetic datasets. By optionally fine-tuning on the KITTI training data, our approach achieves competitive optical flow accuracy on the KITTI 2012 and 2015 benchmarks, enabling generic pre-training of supervised networks for datasets with limited ground truth.",1
"One of the key challenges of visual perception is to extract abstract models of 3D objects and object categories from visual measurements, which are affected by complex nuisance factors such as viewpoint, occlusion, motion, and deformations. Starting from the recent idea of viewpoint factorization, we propose a new approach that, given a large number of images of an object and no other supervision, can extract a dense object-centric coordinate frame. This coordinate frame is invariant to deformations of the images and comes with a dense equivariant labelling neural network that can map image pixels to their corresponding object coordinates. We demonstrate the applicability of this method to simple articulated objects and deformable objects such as human faces, learning embeddings from random synthetic transformations or optical flow correspondences, all without any manual supervision.",0
"Visual perception faces a significant challenge in extracting abstract models of 3D objects and object categories from visual measurements. This task is complicated by various factors such as viewpoint, occlusion, motion, and deformations. Our proposed approach builds upon the viewpoint factorization concept and can extract a dense object-centric coordinate frame from a large number of images of an object without any supervision. This coordinate frame is invariant to image deformations and accompanied by a neural network that can label corresponding object coordinates. We have successfully demonstrated the effectiveness of this method on simple articulated and deformable objects, including human faces, by learning embeddings from synthetic transformations or optical flow correspondences without requiring any manual supervision.",1
"We present a method to reconstruct the three-dimensional trajectory of a moving instance of a known object category in monocular video data. We track the two-dimensional shape of objects on pixel level exploiting instance-aware semantic segmentation techniques and optical flow cues. We apply Structure from Motion techniques to object and background images to determine for each frame camera poses relative to object instances and background structures. By combining object and background camera pose information, we restrict the object trajectory to a one-parameter family of possible solutions. We compute a ground representation by fusing background structures and corresponding semantic segmentations. This allows us to determine an object trajectory consistent to image observations and reconstructed environment model. Our method is robust to occlusion and handles temporarily stationary objects. We show qualitative results using drone imagery. Due to the lack of suitable benchmark datasets we present a new dataset to evaluate the quality of reconstructed three-dimensional object trajectories. The video sequences contain vehicles in urban areas and are rendered using the path-tracing render engine Cycles to achieve realistic results. We perform a quantitative evaluation of the presented approach using this dataset. Our algorithm achieves an average reconstruction-to-ground-truth distance of 0.31 meter.",0
"In this paper, we introduce a technique for reconstructing the movement of a known object category in three dimensions using monocular video data. Our approach involves tracking the two-dimensional shape of objects using semantic segmentation and optical flow, and applying Structure from Motion techniques to determine camera poses relative to object instances and background structures. By combining information from both object and background camera poses, we can narrow down the possible solutions for the object trajectory. We also use a ground representation to ensure consistency between the object trajectory and the reconstructed environment model. Our method is capable of handling occlusion and stationary objects, as demonstrated through qualitative results using drone imagery. To evaluate the quality of our approach, we introduce a new dataset containing videos of vehicles in urban areas rendered using the Cycles render engine. Quantitative evaluation of our algorithm on this dataset shows an average reconstruction-to-ground-truth distance of 0.31 meter.",1
"We present a no reference (NR) quality assessment algorithm for assessing the perceptual quality of natural stereoscopic 3D (S3D) videos. This work is inspired by our finding that the joint statistics of the subband coefficients of motion (optical flow or motion vector magnitude) and depth (disparity map) of natural S3D videos possess a unique signature. Specifically, we empirically show that the joint statistics of the motion and depth subband coefficients of S3D video frames can be modeled accurately using a Bivariate Generalized Gaussian Distribution (BGGD). We then demonstrate that the parameters of the BGGD model possess the ability to discern quality variations in S3D videos. Therefore, the BGGD model parameters are employed as motion and depth quality features. In addition to these features, we rely on a frame level spatial quality feature that is computed using a robust off the shelf NR image quality assessment (IQA) algorithm. These frame level motion, depth and spatial features are consolidated and used with the corresponding S3D video's difference mean opinion score (DMOS) labels for supervised learning using support vector regression (SVR). The overall quality of an S3D video is computed by averaging the frame level quality predictions of the constituent video frames. The proposed algorithm, dubbed Video QUality Evaluation using MOtion and DEpth Statistics (VQUEMODES) is shown to outperform the state of the art methods when evaluated over the IRCCYN and LFOVIA S3D subjective quality assessment databases.",0
"Our study introduces a no-reference quality assessment algorithm that evaluates the perceived quality of natural stereoscopic 3D (S3D) videos. Our research stems from our discovery that the joint statistics of motion (optical flow or motion vector magnitude) and depth (disparity map) subband coefficients of natural S3D videos are distinct. To be specific, we establish that the joint statistics of motion and depth subband coefficients of S3D video frames can be precisely modeled using a Bivariate Generalized Gaussian Distribution (BGGD). We then prove that the parameters of the BGGD model can differentiate quality differences in S3D videos. As a result, we employ the BGGD model parameters as motion and depth quality features. Along with these features, we use a spatial quality feature computed by a robust off-the-shelf no-reference image quality assessment (IQA) algorithm. We combine these frame-level motion, depth, and spatial features with the corresponding S3D video's difference mean opinion score (DMOS) labels for supervised learning using support vector regression (SVR). We calculate the overall quality of an S3D video by averaging the frame-level quality predictions of the constituent video frames. Our proposed algorithm, known as Video QUality Evaluation using MOtion and DEpth Statistics (VQUEMODES), outperforms existing methods when tested on the IRCCYN and LFOVIA S3D subjective quality assessment databases.",1
"Computational saliency models for still images have gained significant popularity in recent years. Saliency prediction from videos, on the other hand, has received relatively little interest from the community. Motivated by this, in this work, we study the use of deep learning for dynamic saliency prediction and propose the so-called spatio-temporal saliency networks. The key to our models is the architecture of two-stream networks where we investigate different fusion mechanisms to integrate spatial and temporal information. We evaluate our models on the DIEM and UCF-Sports datasets and present highly competitive results against the existing state-of-the-art models. We also carry out some experiments on a number of still images from the MIT300 dataset by exploiting the optical flow maps predicted from these images. Our results show that considering inherent motion information in this way can be helpful for static saliency estimation.",0
"Recently, there has been a significant increase in the use of computational saliency models for still images. However, there has been less attention given to predicting saliency in videos. In this study, we explore the use of deep learning for dynamic saliency prediction and introduce the spatio-temporal saliency networks. Our models utilize two-stream networks that incorporate various fusion mechanisms for integrating spatial and temporal information. We assess the effectiveness of our models on the DIEM and UCF-Sports datasets and obtain favorable results compared to existing state-of-the-art models. Additionally, we experiment with still images from the MIT300 dataset by utilizing optical flow maps to incorporate inherent motion information for static saliency estimation. Our findings suggest that this approach can be beneficial for estimating static saliency.",1
"Small flying robots can perform landing maneuvers using bio-inspired optical flow by maintaining a constant divergence. However, optical flow is typically estimated from frame sequences recorded by standard miniature cameras. This requires processing full images on-board, limiting the update rate of divergence measurements, and thus the speed of the control loop and the robot. Event-based cameras overcome these limitations by only measuring pixel-level brightness changes at microsecond temporal accuracy, hence providing an efficient mechanism for optical flow estimation. This paper presents, to the best of our knowledge, the first work integrating event-based optical flow estimation into the control loop of a flying robot. We extend an existing 'local plane fitting' algorithm to obtain an improved and more computationally efficient optical flow estimation method, valid for a wide range of optical flow velocities. This method is validated for real event sequences. In addition, a method for estimating the divergence from event-based optical flow is introduced, which accounts for the aperture problem. The developed algorithms are implemented in a constant divergence landing controller on-board of a quadrotor. Experiments show that, using event-based optical flow, accurate divergence estimates can be obtained over a wide range of speeds. This enables the quadrotor to perform very fast landing maneuvers.",0
"By adopting bio-inspired optical flow, small flying robots can execute landing maneuvers while maintaining a constant divergence. However, this technique typically relies on full image processing with standard miniature cameras, which limits the update rate of divergence measurements and, consequently, the control loop and the robot's speed. To overcome these challenges, event-based cameras only measure pixel-level brightness changes with microsecond temporal accuracy, providing an efficient mechanism for optical flow estimation. This paper presents the first integration of event-based optical flow estimation into a flying robot's control loop. We enhance the 'local plane fitting' algorithm to obtain an improved and more computationally efficient optical flow estimation method, applicable for a wide range of optical flow velocities. This method is validated using actual event sequences, and we introduce a method for estimating divergence from event-based optical flow that addresses the aperture problem. The algorithms are implemented in a quadrotor's constant divergence landing controller, and experiments demonstrate that accurate divergence estimates can be obtained at high speeds, enabling very fast landing maneuvers.",1
"Video classification is highly important with wide applications, such as video search and intelligent surveillance. Video naturally consists of static and motion information, which can be represented by frame and optical flow. Recently, researchers generally adopt the deep networks to capture the static and motion information \textbf{\emph{separately}}, which mainly has two limitations: (1) Ignoring the coexistence relationship between spatial and temporal attention, while they should be jointly modelled as the spatial and temporal evolutions of video, thus discriminative video features can be extracted.(2) Ignoring the strong complementarity between static and motion information coexisted in video, while they should be collaboratively learned to boost each other. For addressing the above two limitations, this paper proposes the approach of two-stream collaborative learning with spatial-temporal attention (TCLSTA), which consists of two models: (1) Spatial-temporal attention model: The spatial-level attention emphasizes the salient regions in frame, and the temporal-level attention exploits the discriminative frames in video. They are jointly learned and mutually boosted to learn the discriminative static and motion features for better classification performance. (2) Static-motion collaborative model: It not only achieves mutual guidance on static and motion information to boost the feature learning, but also adaptively learns the fusion weights of static and motion streams, so as to exploit the strong complementarity between static and motion information to promote video classification. Experiments on 4 widely-used datasets show that our TCLSTA approach achieves the best performance compared with more than 10 state-of-the-art methods.",0
"Classifying videos has numerous practical applications, including video search and intelligent surveillance. Videos contain both static and motion information, represented by frames and optical flow, respectively. Researchers have typically used deep networks to capture static and motion information separately, but this approach has limitations. Firstly, it neglects the relationship between spatial and temporal attention, which should be jointly modeled to extract discriminative video features. Secondly, it disregards the complementary nature of static and motion information in videos, which should be collaboratively learned to enhance their respective strengths. To address these limitations, this paper proposes the two-stream collaborative learning with spatial-temporal attention (TCLSTA) approach. TCLSTA comprises two models: the spatial-temporal attention model, which jointly learns spatial and temporal attention to extract discriminative static and motion features, and the static-motion collaborative model, which adaptively learns fusion weights to exploit the complementarity between static and motion information. Experiments on four commonly-used datasets demonstrate that TCLSTA outperforms over 10 state-of-the-art methods.",1
"The ability of predicting the future is important for intelligent systems, e.g. autonomous vehicles and robots to plan early and make decisions accordingly. Future scene parsing and optical flow estimation are two key tasks that help agents better understand their environments as the former provides dense semantic information, i.e. what objects will be present and where they will appear, while the latter provides dense motion information, i.e. how the objects will move. In this paper, we propose a novel model to simultaneously predict scene parsing and optical flow in unobserved future video frames. To our best knowledge, this is the first attempt in jointly predicting scene parsing and motion dynamics. In particular, scene parsing enables structured motion prediction by decomposing optical flow into different groups while optical flow estimation brings reliable pixel-wise correspondence to scene parsing. By exploiting this mutually beneficial relationship, our model shows significantly better parsing and motion prediction results when compared to well-established baselines and individual prediction models on the large-scale Cityscapes dataset. In addition, we also demonstrate that our model can be used to predict the steering angle of the vehicles, which further verifies the ability of our model to learn latent representations of scene dynamics.",0
"Intelligent systems such as autonomous vehicles and robots require the ability to predict the future in order to plan ahead and make informed decisions. To better understand their surroundings, agents rely on two key tasks: future scene parsing, which provides information on the objects present and their locations, and optical flow estimation, which provides insight into how the objects will move. This paper presents a new model that can simultaneously predict both scene parsing and optical flow in future video frames. This approach is unique in its joint prediction of these two tasks. By combining the benefits of scene parsing and optical flow estimation, the model outperforms established baselines and individual prediction models on the Cityscapes dataset. Furthermore, the model's ability to predict the steering angle of vehicles demonstrates its capacity to learn latent representations of scene dynamics.",1
"We address unsupervised optical flow estimation for ego-centric motion. We argue that optical flow can be cast as a geometrical warping between two successive video frames and devise a deep architecture to estimate such transformation in two stages. First, a dense pixel-level flow is computed with a geometric prior imposing strong spatial constraints. Such prior is typical of driving scenes, where the point of view is coherent with the vehicle motion. We show how such global transformation can be approximated with an homography and how spatial transformer layers can be employed to compute the flow field implied by such transformation. The second stage then refines the prediction feeding a second deeper network. A final reconstruction loss compares the warping of frame X(t) with the subsequent frame X(t+1) and guides both estimates. The model, which we named TransFlow, performs favorably compared to other unsupervised algorithms, and shows better generalization compared to supervised methods with a 3x reduction in error on unseen data.",0
"Our focus is on unsupervised optical flow estimation for ego-centric movement. We propose that optical flow is a geometrical transformation between consecutive video frames, and we introduce a complex design to calculate this transformation in two phases. Initially, we generate a dense flow at a pixel level, utilizing a geometric prior that enforces spatial constraints, which is particularly useful for driving scenes where the viewpoint aligns with vehicle motion. We explain how a homography can approximate this global transformation, and we illustrate how spatial transformer layers can compute the flow field generated by this transformation. In the second phase, we refine our prediction by leveraging a deeper network. We use a final reconstruction loss that compares the warping of frame X(t) to the subsequent frame X(t+1) and guides both estimates. Our model, known as TransFlow, outperforms other unsupervised algorithms and exhibits better generalization than supervised techniques, reducing errors by 3x on unseen data.",1
"This paper describes a fully spike-based neural network for optical flow estimation from Dynamic Vision Sensor data. A low power embedded implementation of the method which combines the Asynchronous Time-based Image Sensor with IBM's TrueNorth Neurosynaptic System is presented. The sensor generates spikes with sub-millisecond resolution in response to scene illumination changes. These spike are processed by a spiking neural network running on TrueNorth with a 1 millisecond resolution to accurately determine the order and time difference of spikes from neighboring pixels, and therefore infer the velocity. The spiking neural network is a variant of the Barlow Levick method for optical flow estimation. The system is evaluated on two recordings for which ground truth motion is available, and achieves an Average Endpoint Error of 11% at an estimated power budget of under 80mW for the sensor and computation.",0
"In this paper, a spike-based neural network is detailed for estimating optical flow from data obtained by the Dynamic Vision Sensor. The method is implemented in an embedded form, which utilizes the Asynchronous Time-based Image Sensor along with IBM's TrueNorth Neurosynaptic System. The sensor responds to changes in scene illumination by producing spikes with sub-millisecond resolution. These spikes are then fed into a spiking neural network running on TrueNorth, which processes the data with a 1 millisecond resolution. By accurately determining the order and time difference of spikes from neighboring pixels, the velocity of the scene can be inferred. The spiking neural network is a modified version of the Barlow Levick method for optical flow estimation. The system is tested on two recordings with known motion, achieving an Average Endpoint Error of 11% while utilizing less than 80mW of power for both the sensor and computation.",1
"In the context of online Robust Principle Component Analysis (RPCA) for the video foreground-background separation, we propose a compressive online RPCA with optical flow that separates recursively a sequence of frames into sparse (foreground) and low-rank (background) components. Our method considers a small set of measurements taken per data vector (frame), which is different from conventional batch RPCA, processing all the data directly. The proposed method also incorporates multiple prior information, namely previous foreground and background frames, to improve the separation and then updates the prior information for the next frame. Moreover, the foreground prior frames are improved by estimating motions between the previous foreground frames using optical flow and compensating the motions to achieve higher quality foreground prior. The proposed method is applied to online video foreground and background separation from compressive measurements. The visual and quantitative results show that our method outperforms the existing methods.",0
"We suggest a compressive online RPCA method with optical flow for separating video foreground-background in online contexts. Our approach recursively divides a sequence of frames into low-rank (background) and sparse (foreground) components. Unlike conventional batch RPCA methods that process all data at once, we use a minimal set of measurements per frame. Additionally, we incorporate prior information from previous foreground and background frames to enhance separation and update it for the next frame. We also improve the foreground prior frames by estimating optical flow between previous foreground frames, compensating for motion to obtain higher quality foreground prior. We apply our technique to online video foreground-background separation from compressive measurements, and the results demonstrate superior performance compared to existing methods.",1
"Given two consecutive frames from a pair of stereo cameras, 3D scene flow methods simultaneously estimate the 3D geometry and motion of the observed scene. Many existing approaches use superpixels for regularization, but may predict inconsistent shapes and motions inside rigidly moving objects. We instead assume that scenes consist of foreground objects rigidly moving in front of a static background, and use semantic cues to produce pixel-accurate scene flow estimates. Our cascaded classification framework accurately models 3D scenes by iteratively refining semantic segmentation masks, stereo correspondences, 3D rigid motion estimates, and optical flow fields. We evaluate our method on the challenging KITTI autonomous driving benchmark, and show that accounting for the motion of segmented vehicles leads to state-of-the-art performance.",0
"3D scene flow methods can estimate the 3D geometry and motion of a scene based on two consecutive frames from a pair of stereo cameras. Although many approaches use superpixels for regularization, they may produce inconsistent shapes and motions within rigidly moving objects. Our method assumes that foreground objects are rigidly moving in front of a static background and utilizes semantic cues to generate accurate scene flow estimates at the pixel level. We utilize a cascaded classification framework that improves 3D scene modeling by refining semantic segmentation masks, stereo correspondences, 3D rigid motion estimates, and optical flow fields iteratively. Our method is evaluated on the challenging KITTI autonomous driving benchmark, and we demonstrate that by considering the motion of segmented vehicles, we can achieve state-of-the-art performance.",1
"With the goal of making high-resolution forecasts of regional rainfall, precipitation nowcasting has become an important and fundamental technology underlying various public services ranging from rainstorm warnings to flight safety. Recently, the Convolutional LSTM (ConvLSTM) model has been shown to outperform traditional optical flow based methods for precipitation nowcasting, suggesting that deep learning models have a huge potential for solving the problem. However, the convolutional recurrence structure in ConvLSTM-based models is location-invariant while natural motion and transformation (e.g., rotation) are location-variant in general. Furthermore, since deep-learning-based precipitation nowcasting is a newly emerging area, clear evaluation protocols have not yet been established. To address these problems, we propose both a new model and a benchmark for precipitation nowcasting. Specifically, we go beyond ConvLSTM and propose the Trajectory GRU (TrajGRU) model that can actively learn the location-variant structure for recurrent connections. Besides, we provide a benchmark that includes a real-world large-scale dataset from the Hong Kong Observatory, a new training loss, and a comprehensive evaluation protocol to facilitate future research and gauge the state of the art.",0
"The significance of precipitation nowcasting lies in its ability to generate high-resolution forecasts of regional rainfall, which in turn serves as a fundamental technology for an array of public services such as rainstorm warnings and flight safety. While the Convolutional LSTM (ConvLSTM) model has proven to be better than traditional optical flow based methods for precipitation nowcasting, its convolutional recurrence structure is location-invariant while natural motion and transformations are location-variant. Additionally, due to the novelty of deep-learning-based precipitation nowcasting, there is a lack of clear evaluation protocols. To tackle these issues, we propose Trajectory GRU (TrajGRU), a new model that can learn the location-variant structure for recurrent connections. We also introduce a benchmark that comprises a large-scale dataset from the Hong Kong Observatory, a new training loss, and a comprehensive evaluation protocol to encourage further research and assess the current state of the art.",1
"Convolutional neural networks (CNNs) have been widely used over many areas in compute vision. Especially in classification. Recently, FlowNet and several works on opti- cal estimation using CNNs shows the potential ability of CNNs in doing per-pixel regression. We proposed several CNNs network architectures that can estimate optical flow, and fully unveiled the intrinsic different between these structures.",0
"CNNs have gained extensive usage in various fields of computer vision, particularly in classification. The potential of CNNs in performing per-pixel regression has been demonstrated through recent works such as FlowNet and optical estimation using CNNs. Our proposal includes several network architectures based on CNNs that can estimate optical flow. Additionally, we have fully exposed the intrinsic differences among these structures.",1
"Semantic video segmentation is challenging due to the sheer amount of data that needs to be processed and labeled in order to construct accurate models. In this paper we present a deep, end-to-end trainable methodology to video segmentation that is capable of leveraging information present in unlabeled data in order to improve semantic estimates. Our model combines a convolutional architecture and a spatio-temporal transformer recurrent layer that are able to temporally propagate labeling information by means of optical flow, adaptively gated based on its locally estimated uncertainty. The flow, the recognition and the gated temporal propagation modules can be trained jointly, end-to-end. The temporal, gated recurrent flow propagation component of our model can be plugged into any static semantic segmentation architecture and turn it into a weakly supervised video processing one. Our extensive experiments in the challenging CityScapes and Camvid datasets, and based on multiple deep architectures, indicate that the resulting model can leverage unlabeled temporal frames, next to a labeled one, in order to improve both the video segmentation accuracy and the consistency of its temporal labeling, at no additional annotation cost and with little extra computation.",0
"The amount of data that must be processed and labeled to create accurate semantic video segmentation models is a difficult challenge. To address this issue, our paper proposes a trainable methodology for video segmentation that utilizes unlabeled data to improve semantic estimates. Our model integrates a convolutional architecture and a spatio-temporal transformer recurrent layer, which propagates labeling information through optical flow and adapts to local uncertainty. The flow, recognition, and gated temporal propagation modules can be trained together in an end-to-end manner. Our model's gated recurrent flow propagation component can be added to any static semantic segmentation architecture to create a weakly supervised video processing system. We conducted extensive experiments on CityScapes and Camvid datasets using various deep architectures, and found that our model can improve video segmentation accuracy and temporal labeling consistency without additional annotation cost or significant computational overhead by leveraging unlabeled temporal frames.",1
"We present a benchmark suite for visual perception. The benchmark is based on more than 250K high-resolution video frames, all annotated with ground-truth data for both low-level and high-level vision tasks, including optical flow, semantic instance segmentation, object detection and tracking, object-level 3D scene layout, and visual odometry. Ground-truth data for all tasks is available for every frame. The data was collected while driving, riding, and walking a total of 184 kilometers in diverse ambient conditions in a realistic virtual world. To create the benchmark, we have developed a new approach to collecting ground-truth data from simulated worlds without access to their source code or content. We conduct statistical analyses that show that the composition of the scenes in the benchmark closely matches the composition of corresponding physical environments. The realism of the collected data is further validated via perceptual experiments. We analyze the performance of state-of-the-art methods for multiple tasks, providing reference baselines and highlighting challenges for future research. The supplementary video can be viewed at https://youtu.be/T9OybWv923Y",0
"A benchmark suite has been introduced for visual perception, utilizing over 250K high-resolution video frames that have been annotated with ground-truth data for low-level and high-level vision tasks. These tasks include optical flow, semantic instance segmentation, object detection and tracking, object-level 3D scene layout, and visual odometry. The ground-truth data for each task is available for every frame. The data was collected in a realistic virtual world while driving, riding, and walking a total of 184 kilometers in diverse ambient conditions. Our approach to collecting ground-truth data from simulated worlds without access to their source code or content has created this benchmark. Statistical analyses demonstrate that the scenes in the benchmark closely resemble those in corresponding physical environments. Perceptual experiments validate the realism of the collected data. State-of-the-art methods for multiple tasks are analyzed, providing reference baselines and highlighting future research challenges. A supplementary video is accessible at https://youtu.be/T9OybWv923Y.",1
"This paper proposes an end-to-end trainable network, SegFlow, for simultaneously predicting pixel-wise object segmentation and optical flow in videos. The proposed SegFlow has two branches where useful information of object segmentation and optical flow is propagated bidirectionally in a unified framework. The segmentation branch is based on a fully convolutional network, which has been proved effective in image segmentation task, and the optical flow branch takes advantage of the FlowNet model. The unified framework is trained iteratively offline to learn a generic notion, and fine-tuned online for specific objects. Extensive experiments on both the video object segmentation and optical flow datasets demonstrate that introducing optical flow improves the performance of segmentation and vice versa, against the state-of-the-art algorithms.",0
"In this paper, SegFlow, a network that is trainable end-to-end, is proposed. The network can predict object segmentation and optical flow pixel-wise simultaneously in videos. SegFlow has two branches that allow useful information of object segmentation and optical flow to be propagated bidirectionally in a unified framework. The segmentation branch of the network is based on a fully convolutional network that has been proven effective in image segmentation tasks. Meanwhile, the optical flow branch leverages the FlowNet model. The unified framework is trained offline iteratively to learn a generic notion and fine-tuned online for specific objects. Extensive experiments conducted on both video object segmentation and optical flow datasets demonstrate that introducing optical flow improves the segmentation's performance and vice versa compared to the state-of-the-art algorithms.",1
"Today's general-purpose deep convolutional neural networks (CNN) for image classification and object detection are trained offline on large static datasets. Some applications, however, will require training in real-time on live video streams with a human-in-the-loop. We refer to this class of problem as Time-ordered Online Training (ToOT) - these problems will require a consideration of not only the quantity of incoming training data, but the human effort required to tag and use it. In this paper, we define training benefit as a metric to measure the effectiveness of a sequence in using each user interaction. We demonstrate and evaluate a system tailored to performing ToOT in the field, capable of training an image classifier on a live video stream through minimal input from a human operator. We show that by exploiting the time-ordered nature of the video stream through optical flow-based object tracking, we can increase the effectiveness of human actions by about 8 times.",0
"At present, deep convolutional neural networks (CNN) that are designed for image classification and object detection are typically trained offline using large, fixed datasets. However, certain applications require real-time training on live video feeds with human input. This type of problem is referred to as Time-ordered Online Training (ToOT), which necessitates careful consideration of both the quantity of incoming training data and the amount of effort required from humans to tag and utilize it. This paper introduces a metric called ""training benefit,"" which measures the effectiveness of a sequence in utilizing user interactions. The authors present a system that is tailored to perform ToOT in the field, capable of training an image classifier on a live video stream with minimal input from a human operator. They demonstrate that by leveraging the time-ordered nature of the video stream via optical flow-based object tracking, the effectiveness of human actions can be increased by approximately eight times.",1
"For many movement disorders, such as Parkinson's disease and ataxia, disease progression is visually assessed by a clinician using a numerical disease rating scale. These tests are subjective, time-consuming, and must be administered by a professional. This can be problematic where specialists are not available, or when a patient is not consistently evaluated by the same clinician. We present an automated method for quantifying the severity of motion impairment in patients with ataxia, using only video recordings. We consider videos of the finger-to-nose test, a common movement task used as part of the assessment of ataxia progression during the course of routine clinical checkups.   Our method uses neural network-based pose estimation and optical flow techniques to track the motion of the patient's hand in a video recording. We extract features that describe qualities of the motion such as speed and variation in performance. Using labels provided by an expert clinician, we train a supervised learning model that predicts severity according to the Brief Ataxia Rating Scale (BARS). The performance of our system is comparable to that of a group of ataxia specialists in terms of mean error and correlation, and our system's predictions were consistently within the range of inter-rater variability. This work demonstrates the feasibility of using computer vision and machine learning to produce consistent and clinically useful measures of motor impairment.",0
"To evaluate the progression of movement disorders like Parkinson's disease and ataxia, clinicians use a subjective numerical rating scale, which is time-consuming and requires a professional to administer. This poses a problem when specialists are unavailable or if a patient is not consistently evaluated by the same clinician. However, we have developed an automated method that can assess the severity of motion impairment in ataxia patients using video recordings of the finger-to-nose test. Our approach utilizes neural network-based pose estimation and optical flow techniques to track hand motion in the video and extract features that describe its speed and variation in performance. We then use these features to train a supervised learning model that predicts disease severity according to the Brief Ataxia Rating Scale (BARS). Our system's performance is comparable to that of a group of ataxia specialists and consistently within the range of inter-rater variability. This work demonstrates the potential for computer vision and machine learning to produce reliable and clinically useful measures of motor impairment.",1
"In this paper we address the abnormality detection problem in crowded scenes. We propose to use Generative Adversarial Nets (GANs), which are trained using normal frames and corresponding optical-flow images in order to learn an internal representation of the scene normality. Since our GANs are trained with only normal data, they are not able to generate abnormal events. At testing time the real data are compared with both the appearance and the motion representations reconstructed by our GANs and abnormal areas are detected by computing local differences. Experimental results on challenging abnormality detection datasets show the superiority of the proposed method compared to the state of the art in both frame-level and pixel-level abnormality detection tasks.",0
"The focus of our paper is detecting abnormalities in crowded scenes. Our proposed solution involves utilizing Generative Adversarial Nets (GANs), which are trained using normal frames and corresponding optical-flow images. This allows the GANs to learn an internal representation of what is considered normal in the scene. By training our GANs with only normal data, they cannot generate abnormal events. During testing, real data is compared to both the appearance and motion representations reconstructed by the GANs. Abnormal areas are identified by computing local differences. Results from our experiments on challenging abnormality detection datasets demonstrate that our method outperforms the state of the art in both frame-level and pixel-level abnormality detection tasks.",1
"This paper proposes a two-stream flow-guided convolutional attention networks for action recognition in videos. The central idea is that optical flows, when properly compensated for the camera motion, can be used to guide attention to the human foreground. We thus develop cross-link layers from the temporal network (trained on flows) to the spatial network (trained on RGB frames). These cross-link layers guide the spatial-stream to pay more attention to the human foreground areas and be less affected by background clutter. We obtain promising performances with our approach on the UCF101, HMDB51 and Hollywood2 datasets.",0
"In this paper, a novel approach to action recognition in videos is presented, which involves utilizing two-stream flow-guided convolutional attention networks. The main concept behind this method is that optical flows can guide attention towards the human foreground, provided that camera motion is suitably compensated for. To achieve this, cross-link layers are developed from the temporal network (which is trained on flows) to the spatial network (which is trained on RGB frames). These cross-link layers help the spatial-stream to focus more on human foreground areas and be less influenced by background clutter. The results obtained using this approach on the UCF101, HMDB51, and Hollywood2 datasets are highly promising.",1
"Activity recognition from long unstructured egocentric photo-streams has several applications in assistive technology such as health monitoring and frailty detection, just to name a few. However, one of its main technical challenges is to deal with the low frame rate of wearable photo-cameras, which causes abrupt appearance changes between consecutive frames. In consequence, important discriminatory low-level features from motion such as optical flow cannot be estimated. In this paper, we present a batch-driven approach for training a deep learning architecture that strongly rely on Long short-term units to tackle this problem. We propose two different implementations of the same approach that process a photo-stream sequence using batches of fixed size with the goal of capturing the temporal evolution of high-level features. The main difference between these implementations is that one explicitly models consecutive batches by overlapping them. Experimental results over a public dataset acquired by three users demonstrate the validity of the proposed architectures to exploit the temporal evolution of convolutional features over time without relying on event boundaries.",0
"Assistive technology, such as health monitoring and frailty detection, can greatly benefit from activity recognition in unstructured egocentric photo-streams. However, the low frame rate of wearable photo-cameras poses a major technical challenge as it causes sudden appearance changes between frames and hinders the estimation of important low-level features like optical flow. This paper presents a batch-driven approach that employs Long short-term units in deep learning architecture to address this issue. Two variations of this approach are proposed, both of which process photo-stream sequences in fixed-size batches to capture the evolution of high-level features over time. The difference between the two variations lies in the explicit modeling of consecutive batches via overlapping. Experiments conducted on a public dataset from three users demonstrate the effectiveness of these architectures in exploiting the temporal evolution of convolutional features without relying on event boundaries.",1
"We propose Stereo Direct Sparse Odometry (Stereo DSO) as a novel method for highly accurate real-time visual odometry estimation of large-scale environments from stereo cameras. It jointly optimizes for all the model parameters within the active window, including the intrinsic/extrinsic camera parameters of all keyframes and the depth values of all selected pixels. In particular, we propose a novel approach to integrate constraints from static stereo into the bundle adjustment pipeline of temporal multi-view stereo. Real-time optimization is realized by sampling pixels uniformly from image regions with sufficient intensity gradient. Fixed-baseline stereo resolves scale drift. It also reduces the sensitivities to large optical flow and to rolling shutter effect which are known shortcomings of direct image alignment methods. Quantitative evaluation demonstrates that the proposed Stereo DSO outperforms existing state-of-the-art visual odometry methods both in terms of tracking accuracy and robustness. Moreover, our method delivers a more precise metric 3D reconstruction than previous dense/semi-dense direct approaches while providing a higher reconstruction density than feature-based methods.",0
"Stereo Direct Sparse Odometry (Stereo DSO) is a new approach for accurately estimating visual odometry in real-time for large-scale environments using stereo cameras. It optimizes all the model parameters within the active window, including intrinsic/extrinsic camera parameters and depth values of selected pixels. To achieve real-time optimization, the method uniformly samples pixels from image regions with sufficient intensity gradient. The use of fixed-baseline stereo helps to resolve scale drift, and reduces sensitivity to optical flow and rolling shutter effect. Compared to existing state-of-the-art visual odometry methods, Stereo DSO outperforms in terms of tracking accuracy and robustness. Additionally, it provides a more precise metric 3D reconstruction than previous dense/semi-dense direct approaches, while maintaining a higher reconstruction density than feature-based methods. The proposed method also incorporates static stereo constraints into the bundle adjustment pipeline of temporal multi-view stereo.",1
"Human action recognition involves the characterization of human actions through the automated analysis of video data and is integral in the development of smart computer vision systems. However, several challenges like dynamic backgrounds, camera stabilization, complex actions, occlusions etc. make action recognition in a real time and robust fashion difficult. Several complex approaches exist but are computationally intensive. This paper presents a novel approach of using a combination of good features along with iterative optical flow algorithm to compute feature vectors which are classified using a multilayer perceptron (MLP) network. The use of multiple features for motion descriptors enhances the quality of tracking. Resilient backpropagation algorithm is used for training the feedforward neural network reducing the learning time. The overall system accuracy is improved by optimizing the various parameters of the multilayer perceptron network.",0
"The identification of human actions through automated video analysis, known as human action recognition, is crucial to the development of intelligent computer vision systems. However, challenges such as dynamic backgrounds, camera stabilization, complex actions, and occlusions make real-time and robust action recognition difficult. Although various complex methods exist, they are computationally demanding. This study introduces a new approach that combines effective features with iterative optical flow algorithms to calculate feature vectors, which are classified using a multilayer perceptron (MLP) network. The use of multiple features for motion descriptors enhances tracking quality, while the resilient backpropagation algorithm reduces learning time during training of the feedforward neural network. The multilayer perceptron network's various parameters are optimized to improve the overall system accuracy.",1
"The temporal component of videos provides an important clue for activity recognition, as a number of activities can be reliably recognized based on the motion information. In view of that, this work proposes a novel temporal stream for two-stream convolutional networks based on images computed from the optical flow magnitude and orientation, named Magnitude-Orientation Stream (MOS), to learn the motion in a better and richer manner. Our method applies simple nonlinear transformations on the vertical and horizontal components of the optical flow to generate input images for the temporal stream. Experimental results, carried on two well-known datasets (HMDB51 and UCF101), demonstrate that using our proposed temporal stream as input to existing neural network architectures can improve their performance for activity recognition. Results demonstrate that our temporal stream provides complementary information able to improve the classical two-stream methods, indicating the suitability of our approach to be used as a temporal video representation.",0
"Recognizing activities in videos is greatly aided by the temporal component, which contains valuable motion information for reliable recognition. To enhance the learning of motion, this study introduces a new temporal stream, called Magnitude-Orientation Stream (MOS), for two-stream convolutional networks. This stream is based on optical flow magnitude and orientation images that undergo nonlinear transformations on their vertical and horizontal components to generate input images. Our experimental results, using the HMDB51 and UCF101 datasets, show that using our MOS stream as input for existing neural network architectures can improve their activity recognition performance. This suggests that our MOS stream provides complementary information that enhances the classical two-stream methods, making it a suitable temporal video representation.",1
"Optical flow estimation remains challenging due to untextured areas, motion boundaries, occlusions, and more. Thus, the estimated flow is not equally reliable across the image. To that end, post-hoc confidence measures have been introduced to assess the per-pixel reliability of the flow. We overcome the artificial separation of optical flow and confidence estimation by introducing a method that jointly predicts optical flow and its underlying uncertainty. Starting from common energy-based formulations, we rely on the corresponding posterior distribution of the flow given the images. We derive a variational inference scheme based on mean field, which incorporates best practices from energy minimization. An uncertainty measure is obtained along the flow at every pixel as the (marginal) entropy of the variational distribution. We demonstrate the flexibility of our probabilistic approach by applying it to two different energies and on two benchmarks. We not only obtain flow results that are competitive with the underlying energy minimization approach, but also a reliable uncertainty measure that significantly outperforms existing post-hoc approaches.",0
"Although optical flow estimation is a difficult task due to various factors such as untextured regions, motion boundaries, and occlusions, confidence measures have been introduced to assess the reliability of the estimated flow on a per-pixel basis. However, the separation between optical flow and confidence estimation has been a limitation. In this study, we propose a method that jointly predicts optical flow and its underlying uncertainty. Using energy-based formulations and the corresponding posterior distribution of the flow given the images, we derive a variational inference scheme based on mean field that incorporates best practices from energy minimization. Our method provides an uncertainty measure along the flow at each pixel as the (marginal) entropy of the variational distribution. We apply our probabilistic approach to two different energies and two benchmarks, obtaining flow results that are comparable to the underlying energy minimization method, as well as a reliable uncertainty measure that outperforms existing post-hoc approaches.",1
"We introduce the concept of ""dynamic image"", a novel compact representation of videos useful for video analysis, particularly in combination with convolutional neural networks (CNNs). A dynamic image encodes temporal data such as RGB or optical flow videos by using the concept of `rank pooling'. The idea is to learn a ranking machine that captures the temporal evolution of the data and to use the parameters of the latter as a representation. When a linear ranking machine is used, the resulting representation is in the form of an image, which we call dynamic because it summarizes the video dynamics in addition of appearance. This is a powerful idea because it allows to convert any video to an image so that existing CNN models pre-trained for the analysis of still images can be immediately extended to videos. We also present an efficient and effective approximate rank pooling operator, accelerating standard rank pooling algorithms by orders of magnitude, and formulate that as a CNN layer. This new layer allows generalizing dynamic images to dynamic feature maps. We demonstrate the power of the new representations on standard benchmarks in action recognition achieving state-of-the-art performance.",0
"We propose a novel approach called ""dynamic image"" which is a compact representation of videos that is useful for video analysis, especially when combined with convolutional neural networks (CNNs). The dynamic image uses the concept of rank pooling to encode temporal data such as RGB or optical flow videos. The rank pooling approach involves learning a ranking machine that captures the temporal evolution of the data and uses its parameters as a representation. When a linear ranking machine is employed, the resulting representation is in the form of an image that summarizes the video dynamics in addition to appearance, hence the term dynamic image. This approach is powerful because it enables the conversion of any video to an image, making it possible to extend existing CNN models trained for the analysis of still images to videos. Additionally, we present an efficient and effective rank pooling operator that accelerates standard rank pooling algorithms by orders of magnitude, and formulate it as a CNN layer. This new layer allows the generalization of dynamic images to dynamic feature maps. We demonstrate the effectiveness of this new approach on standard benchmarks in action recognition and achieve state-of-the-art performance.",1
"Optical flow estimation is one of the most studied problems in computer vision, yet recent benchmark datasets continue to reveal problem areas of today's approaches. Occlusions have remained one of the key challenges. In this paper, we propose a symmetric optical flow method to address the well-known chicken-and-egg relation between optical flow and occlusions. In contrast to many state-of-the-art methods that consider occlusions as outliers, possibly filtered out during post-processing, we highlight the importance of joint occlusion reasoning in the optimization and show how to utilize occlusion as an important cue for estimating optical flow. The key feature of our model is to fully exploit the symmetry properties that characterize optical flow and occlusions in the two consecutive images. Specifically through utilizing forward-backward consistency and occlusion-disocclusion symmetry in the energy, our model jointly estimates optical flow in both forward and backward direction, as well as consistent occlusion maps in both views. We demonstrate significant performance benefits on standard benchmarks, especially from the occlusion-disocclusion symmetry. On the challenging KITTI dataset we report the most accurate two-frame results to date.",0
"Although optical flow estimation is a widely studied problem in computer vision, there are still areas of improvement in current approaches as indicated by recent benchmark datasets. One of the main challenges that persists is the issue of occlusions. This paper proposes a new symmetric optical flow method that addresses the chicken-and-egg problem between optical flow and occlusions. Unlike other state-of-the-art techniques that treat occlusions as outliers and remove them during post-processing, our model emphasizes the importance of simultaneously considering occlusion reasoning during optimization. Our approach utilizes symmetry properties that exist between optical flow and occlusions in two consecutive images. By incorporating forward-backward consistency and occlusion-disocclusion symmetry in the energy, our model estimates optical flow in both directions and consistent occlusion maps in both views. Our results demonstrate a significant improvement in performance, particularly from the occlusion-disocclusion symmetry. We achieve the most accurate two-frame results to date on the challenging KITTI dataset.",1
"We introduce an approach to integrate segmentation information within a convolutional neural network (CNN). This counter-acts the tendency of CNNs to smooth information across regions and increases their spatial precision. To obtain segmentation information, we set up a CNN to provide an embedding space where region co-membership can be estimated based on Euclidean distance. We use these embeddings to compute a local attention mask relative to every neuron position. We incorporate such masks in CNNs and replace the convolution operation with a ""segmentation-aware"" variant that allows a neuron to selectively attend to inputs coming from its own region. We call the resulting network a segmentation-aware CNN because it adapts its filters at each image point according to local segmentation cues. We demonstrate the merit of our method on two widely different dense prediction tasks, that involve classification (semantic segmentation) and regression (optical flow). Our results show that in semantic segmentation we can match the performance of DenseCRFs while being faster and simpler, and in optical flow we obtain clearly sharper responses than networks that do not use local attention masks. In both cases, segmentation-aware convolution yields systematic improvements over strong baselines. Source code for this work is available online at http://cs.cmu.edu/~aharley/segaware.",0
"We propose an innovative approach to incorporating segmentation information into a convolutional neural network (CNN) to address the issue of information smoothing across regions and enhance spatial accuracy. To obtain segmentation information, we establish a CNN to generate embeddings that estimate region co-membership according to Euclidean distance. We use these embeddings to calculate a local attention mask for every neuron position, which we integrate into CNNs via a ""segmentation-aware"" variant that enables a neuron to selectively attend to inputs from its own region. Our resulting network, referred to as a segmentation-aware CNN, is capable of adapting its filters at each image point based on local segmentation cues. We validate our method on two diverse dense prediction tasks, including semantic segmentation and optical flow regression, and demonstrate that our technique outperforms strong baselines in both cases. Specifically, our method achieves comparable performance to DenseCRFs in semantic segmentation and produces significantly sharper responses than networks that do not use local attention masks in optical flow regression. Our work is publicly available online at http://cs.cmu.edu/~aharley/segaware.",1
"Human actions captured in video sequences are three-dimensional signals characterizing visual appearance and motion dynamics. To learn action patterns, existing methods adopt Convolutional and/or Recurrent Neural Networks (CNNs and RNNs). CNN based methods are effective in learning spatial appearances, but are limited in modeling long-term motion dynamics. RNNs, especially Long Short-Term Memory (LSTM), are able to learn temporal motion dynamics. However, naively applying RNNs to video sequences in a convolutional manner implicitly assumes that motions in videos are stationary across different spatial locations. This assumption is valid for short-term motions but invalid when the duration of the motion is long.   In this work, we propose Lattice-LSTM (L2STM), which extends LSTM by learning independent hidden state transitions of memory cells for individual spatial locations. This method effectively enhances the ability to model dynamics across time and addresses the non-stationary issue of long-term motion dynamics without significantly increasing the model complexity. Additionally, we introduce a novel multi-modal training procedure for training our network. Unlike traditional two-stream architectures which use RGB and optical flow information as input, our two-stream model leverages both modalities to jointly train both input gates and both forget gates in the network rather than treating the two streams as separate entities with no information about the other. We apply this end-to-end system to benchmark datasets (UCF-101 and HMDB-51) of human action recognition. Experiments show that on both datasets, our proposed method outperforms all existing ones that are based on LSTM and/or CNNs of similar model complexities.",0
"Visual appearance and motion dynamics captured in video sequences are three-dimensional signals that represent human actions. To learn patterns in these actions, Convolutional and/or Recurrent Neural Networks (CNNs and RNNs) are used. CNNs are effective for learning spatial appearances but have limitations in modeling long-term motion dynamics. RNNs, specifically Long Short-Term Memory (LSTM), can learn temporal motion dynamics. However, applying RNNs to videos in a convolutional manner assumes that motions in videos are stationary across different spatial locations, which is valid for short-term motions but not for long-term motions. This work proposes Lattice-LSTM (L2STM), which extends LSTM by learning independent hidden state transitions of memory cells for individual spatial locations. This method effectively models dynamics across time and addresses the non-stationary issue of long-term motion dynamics without increasing the model complexity significantly. A novel multi-modal training procedure is also introduced, which leverages both RGB and optical flow information to jointly train input and forget gates in the network. Experiments on benchmark datasets (UCF-101 and HMDB-51) show that the proposed L2STM method outperforms all existing methods based on LSTM and/or CNNs with similar model complexities.",1
"Video deblurring is a challenging problem as the blur is complex and usually caused by the combination of camera shakes, object motions, and depth variations. Optical flow can be used for kernel estimation since it predicts motion trajectories. However, the estimates are often inaccurate in complex scenes at object boundaries, which are crucial in kernel estimation. In this paper, we exploit semantic segmentation in each blurry frame to understand the scene contents and use different motion models for image regions to guide optical flow estimation. While existing pixel-wise blur models assume that the blur kernel is the same as optical flow during the exposure time, this assumption does not hold when the motion blur trajectory at a pixel is different from the estimated linear optical flow. We analyze the relationship between motion blur trajectory and optical flow, and present a novel pixel-wise non-linear kernel model to account for motion blur. The proposed blur model is based on the non-linear optical flow, which describes complex motion blur more effectively. Extensive experiments on challenging blurry videos demonstrate the proposed algorithm performs favorably against the state-of-the-art methods.",0
"Deblurring videos is a difficult task due to the intricate nature of the blur, which is often caused by camera shakes, object movements, and depth variations. Although optical flow can estimate motion trajectories and be used for kernel estimation, it is often inaccurate at object boundaries in complex scenes, which are crucial for accurate kernel estimation. In this study, we utilize semantic segmentation in blurry frames to comprehend the scene's contents and apply different motion models to image regions to improve optical flow estimation. Existing pixel-wise blur models assume that the blur kernel is the same as optical flow during exposure time, but this assumption is flawed when the motion blur trajectory at a pixel differs from the estimated linear optical flow. We examine the relationship between motion blur trajectory and optical flow and introduce a unique pixel-wise non-linear kernel model to account for motion blur. The proposed blur model is based on non-linear optical flow, which more effectively describes complex motion blur. Our algorithm's extensive experiments on challenging blurry videos demonstrate its superior performance compared to state-of-the-art methods.",1
"In this work, we propose a technique to convert CNN models for semantic segmentation of static images into CNNs for video data. We describe a warping method that can be used to augment existing architectures with very little extra computational cost. This module is called NetWarp and we demonstrate its use for a range of network architectures. The main design principle is to use optical flow of adjacent frames for warping internal network representations across time. A key insight of this work is that fast optical flow methods can be combined with many different CNN architectures for improved performance and end-to-end training. Experiments validate that the proposed approach incurs only little extra computational cost, while improving performance, when video streams are available. We achieve new state-of-the-art results on the CamVid and Cityscapes benchmark datasets and show consistent improvements over different baseline networks. Our code and models will be available at http://segmentation.is.tue.mpg.de",0
"Our work introduces a method to transform CNN models designed for semantic segmentation of static images into CNNs for video data. We present an augmentation technique called NetWarp that utilizes a warping method to enhance existing architectures without incurring significant additional computational costs. By exploiting optical flow of adjacent frames, we can warp internal network representations across time, which leads to improved performance and end-to-end training. Our research demonstrates that this approach can be applied to various CNN architectures and combined with fast optical flow methods. Experiments show that the proposed method results in only minimal additional computational costs but achieves state-of-the-art results on the CamVid and Cityscapes benchmark datasets, consistently outperforming different baseline networks. Our code and models will be available at http://segmentation.is.tue.mpg.de.",1
"The ability to predict and therefore to anticipate the future is an important attribute of intelligence. It is also of utmost importance in real-time systems, e.g. in robotics or autonomous driving, which depend on visual scene understanding for decision making. While prediction of the raw RGB pixel values in future video frames has been studied in previous work, here we introduce the novel task of predicting semantic segmentations of future frames. Given a sequence of video frames, our goal is to predict segmentation maps of not yet observed video frames that lie up to a second or further in the future. We develop an autoregressive convolutional neural network that learns to iteratively generate multiple frames. Our results on the Cityscapes dataset show that directly predicting future segmentations is substantially better than predicting and then segmenting future RGB frames. Prediction results up to half a second in the future are visually convincing and are much more accurate than those of a baseline based on warping semantic segmentations using optical flow.",0
"Intelligence encompasses the ability to foretell and anticipate future events, which is a crucial attribute, especially in real-time systems like robotics or autonomous driving. These systems rely on visual scene comprehension to make decisions. While previous research has focused on predicting raw RGB pixel values in upcoming video frames, we introduce a new task of predicting semantic segmentations of forthcoming frames. Our objective is to forecast segmentation maps of video frames that have not yet been observed, up to a second or more in the future, using a sequence of video frames. To achieve this, we develop an autoregressive convolutional neural network that learns to generate multiple frames iteratively. Our findings on the Cityscapes dataset indicate that directly predicting future segmentations outperforms predicting and segmenting future RGB frames. The prediction outcomes up to half a second in the future are visually persuasive and are more precise than a baseline approach based on warping semantic segmentations using optical flow.",1
"We address the problem of synthesizing new video frames in an existing video, either in-between existing frames (interpolation), or subsequent to them (extrapolation). This problem is challenging because video appearance and motion can be highly complex. Traditional optical-flow-based solutions often fail where flow estimation is challenging, while newer neural-network-based methods that hallucinate pixel values directly often produce blurry results. We combine the advantages of these two methods by training a deep network that learns to synthesize video frames by flowing pixel values from existing ones, which we call deep voxel flow. Our method requires no human supervision, and any video can be used as training data by dropping, and then learning to predict, existing frames. The technique is efficient, and can be applied at any video resolution. We demonstrate that our method produces results that both quantitatively and qualitatively improve upon the state-of-the-art.",0
"Our research focuses on the challenge of generating new video frames within an existing video, either between existing frames (interpolation) or following them (extrapolation). This task is complicated due to the intricate appearance and motion involved in video creation. Traditional optical-flow-based techniques often fail when flow estimation is difficult, while newer neural-network-based methods that directly generate pixel values can result in blurry outcomes. To overcome these limitations, we have developed a novel approach called deep voxel flow, which merges the strengths of both methods. Our method involves training a deep network to flow pixel values from existing frames to synthesize new video frames. This technique is efficient and requires no human supervision, making it adaptable to any video resolution and using any video as training data. Our results demonstrate that our approach is superior to the current state-of-the-art, both quantitatively and qualitatively.",1
"Standard video frame interpolation methods first estimate optical flow between input frames and then synthesize an intermediate frame guided by motion. Recent approaches merge these two steps into a single convolution process by convolving input frames with spatially adaptive kernels that account for motion and re-sampling simultaneously. These methods require large kernels to handle large motion, which limits the number of pixels whose kernels can be estimated at once due to the large memory demand. To address this problem, this paper formulates frame interpolation as local separable convolution over input frames using pairs of 1D kernels. Compared to regular 2D kernels, the 1D kernels require significantly fewer parameters to be estimated. Our method develops a deep fully convolutional neural network that takes two input frames and estimates pairs of 1D kernels for all pixels simultaneously. Since our method is able to estimate kernels and synthesizes the whole video frame at once, it allows for the incorporation of perceptual loss to train the neural network to produce visually pleasing frames. This deep neural network is trained end-to-end using widely available video data without any human annotation. Both qualitative and quantitative experiments show that our method provides a practical solution to high-quality video frame interpolation.",0
"The conventional method for frame interpolation involves estimating optical flow between input frames and then creating an intermediate frame based on the motion. However, recent approaches have combined these two steps into a single convolution process that uses spatially adaptive kernels to handle motion and re-sampling at the same time. Unfortunately, this method requires large kernels to handle large motion, which limits the number of pixels that can be estimated at once due to the high memory demand. To solve this problem, this paper proposes a new approach that formulates frame interpolation as local separable convolution using pairs of 1D kernels. This method requires significantly fewer parameters to be estimated compared to regular 2D kernels. The proposed approach also incorporates a deep fully convolutional neural network that estimates pairs of 1D kernels for all pixels simultaneously. The network can be trained using widely available video data without human annotation and includes perceptual loss to produce visually pleasing frames. Qualitative and quantitative experiments demonstrate that this method provides a practical solution for high-quality video frame interpolation.",1
"Future frame prediction in videos is a promising avenue for unsupervised video representation learning. Video frames are naturally generated by the inherent pixel flows from preceding frames based on the appearance and motion dynamics in the video. However, existing methods focus on directly hallucinating pixel values, resulting in blurry predictions. In this paper, we develop a dual motion Generative Adversarial Net (GAN) architecture, which learns to explicitly enforce future-frame predictions to be consistent with the pixel-wise flows in the video through a dual-learning mechanism. The primal future-frame prediction and dual future-flow prediction form a closed loop, generating informative feedback signals to each other for better video prediction. To make both synthesized future frames and flows indistinguishable from reality, a dual adversarial training method is proposed to ensure that the future-flow prediction is able to help infer realistic future-frames, while the future-frame prediction in turn leads to realistic optical flows. Our dual motion GAN also handles natural motion uncertainty in different pixel locations with a new probabilistic motion encoder, which is based on variational autoencoders. Extensive experiments demonstrate that the proposed dual motion GAN significantly outperforms state-of-the-art approaches on synthesizing new video frames and predicting future flows. Our model generalizes well across diverse visual scenes and shows superiority in unsupervised video representation learning.",0
"The prediction of future frames in videos has potential for unsupervised video representation learning. Pixel flows from previous frames generate video frames naturally, based on appearance and motion dynamics. However, current methods focus on direct pixel value hallucination, resulting in unclear predictions. In this study, we introduce a dual motion Generative Adversarial Net (GAN) architecture that explicitly enforces consistency between future-frame predictions and pixel-wise flows in the video. A primal future-frame prediction and dual future-flow prediction form a closed loop, providing feedback signals for better video prediction. To ensure both synthesized future frames and flows are realistic, we propose a dual adversarial training method. Our dual motion GAN handles natural motion uncertainty in different pixel locations with a new probabilistic motion encoder based on variational autoencoders. Experiments show that our proposed method outperforms state-of-the-art approaches in synthesizing new video frames and predicting future flows. Our model demonstrates strong generalization across diverse visual scenes, highlighting its superiority in unsupervised video representation learning.",1
"Real-time occlusion handling is a major problem in outdoor mixed reality system because it requires great computational cost mainly due to the complexity of the scene. Using only segmentation, it is difficult to accurately render a virtual object occluded by complex objects such as trees, bushes etc. In this paper, we propose a novel occlusion handling method for real-time, outdoor, and omni-directional mixed reality system using only the information from a monocular image sequence. We first present a semantic segmentation scheme for predicting the amount of visibility for different type of objects in the scene. We also simultaneously calculate a foreground probability map using depth estimation derived from optical flow. Finally, we combine the segmentation result and the probability map to render the computer generated object and the real scene using a visibility-based rendering method. Our results show great improvement in handling occlusions compared to existing blending based methods.",0
"Real-time occlusion management poses a significant challenge for outdoor mixed reality systems, as the complexity of the scene requires extensive computational resources. Segmentation alone is insufficient for accurately rendering virtual objects obscured by intricate structures like trees and shrubs. To address this issue, we propose an innovative occlusion handling technique for omni-directional mixed reality systems that employs only monocular image sequence data. Our method involves using a semantic segmentation scheme to predict object visibility levels and a foreground probability map generated via depth estimation from optical flow. We then merge the segmentation results and probability map using a visibility-based rendering approach to blend the computer-generated objects with the real scene. Our approach outperforms existing blending methods in occlusion management, as demonstrated by our results.",1
"Shot boundary detection (SBD) is an important pre-processing step for video manipulation. Here, each segment of frames is classified as either sharp, gradual or no transition. Current SBD techniques analyze hand-crafted features and attempt to optimize both detection accuracy and processing speed. However, the heavy computations of optical flow prevents this. To achieve this aim, we present an SBD technique based on spatio-temporal Convolutional Neural Networks (CNN). Since current datasets are not large enough to train an accurate SBD CNN, we present a new dataset containing more than 3.5 million frames of sharp and gradual transitions. The transitions are generated synthetically using image compositing models. Our dataset contain additional 70,000 frames of important hard-negative no transitions. We perform the largest evaluation to date for one SBD algorithm, on real and synthetic data, containing more than 4.85 million frames. In comparison to the state of the art, we outperform dissolve gradual detection, generate competitive performance for sharp detections and produce significant improvement in wipes. In addition, we are up to 11 times faster than the state of the art.",0
"The detection of shot boundaries (SBD) is crucial in preparing videos for editing. To do this, frames are categorized as having sharp, gradual, or no transitions. Current SBD methods rely on analyzing hand-crafted features, but this is hindered by the computational demands of optical flow. To address this, we propose an SBD approach using spatio-temporal Convolutional Neural Networks (CNN). However, current datasets are not sufficient to train an accurate SBD CNN, so we introduce a new dataset with over 3.5 million frames of sharp and gradual transitions generated synthetically using image compositing models, as well as an additional 70,000 frames of important hard-negative no transitions. We evaluate our SBD algorithm on real and synthetic data, which includes more than 4.85 million frames, making it the largest evaluation to date. Our results demonstrate superior performance for dissolve gradual detection, comparable performance for sharp detections, and significant improvement in wipes compared to state-of-the-art methods. Additionally, our approach is up to 11 times faster than the current state-of-the-art.",1
"There is an inherent need for autonomous cars, drones, and other robots to have a notion of how their environment behaves and to anticipate changes in the near future. In this work, we focus on anticipating future appearance given the current frame of a video. Existing work focuses on either predicting the future appearance as the next frame of a video, or predicting future motion as optical flow or motion trajectories starting from a single video frame. This work stretches the ability of CNNs (Convolutional Neural Networks) to predict an anticipation of appearance at an arbitrarily given future time, not necessarily the next video frame. We condition our predicted future appearance on a continuous time variable that allows us to anticipate future frames at a given temporal distance, directly from the input video frame. We show that CNNs can learn an intrinsic representation of typical appearance changes over time and successfully generate realistic predictions at a deliberate time difference in the near future.",0
"Autonomous vehicles, drones, and other robots require an understanding of their surroundings and the ability to predict changes in the future. This study focuses on predicting the future appearance of a video based on the current frame. Previous research has focused on predicting future appearance as the next video frame, or predicting motion trajectories from a single frame. This study expands on the capabilities of CNNs by predicting appearance at any given future time, not just the next frame. The predictions are based on a continuous time variable, allowing for anticipation of future frames at specific intervals from the input frame. The study demonstrates that CNNs can learn to accurately predict typical appearance changes over time, resulting in realistic predictions for a set time in the future.",1
"As an important and challenging problem in computer vision, learning based optical flow estimation aims to discover the intrinsic correspondence structure between two adjacent video frames through statistical learning. Therefore, a key issue to solve in this area is how to effectively model the multi-scale correspondence structure properties in an adaptive end-to-end learning fashion. Motivated by this observation, we propose an end-to-end multi-scale correspondence structure learning (MSCSL) approach for optical flow estimation. In principle, the proposed MSCSL approach is capable of effectively capturing the multi-scale inter-image-correlation correspondence structures within a multi-level feature space from deep learning. Moreover, the proposed MSCSL approach builds a spatial Conv-GRU neural network model to adaptively model the intrinsic dependency relationships among these multi-scale correspondence structures. Finally, the above procedures for correspondence structure learning and multi-scale dependency modeling are implemented in a unified end-to-end deep learning framework. Experimental results on several benchmark datasets demonstrate the effectiveness of the proposed approach.",0
"Optical flow estimation is a significant and difficult problem in computer vision, which involves discovering the intrinsic correspondence structure between two consecutive video frames through statistical learning. The main challenge in this field is to develop an adaptive end-to-end learning approach that effectively models the multi-scale correspondence structure properties. To address this challenge, we propose an end-to-end multi-scale correspondence structure learning (MSCSL) approach that can capture the multi-scale inter-image-correlation correspondence structures within a multi-level feature space from deep learning. Additionally, we use a spatial Conv-GRU neural network model to adaptively model the intrinsic dependency relationships among these multi-scale correspondence structures. Lastly, we implement the above procedures in a unified end-to-end deep learning framework. Our approach is experimentally verified on several benchmark datasets, demonstrating its effectiveness.",1
We propose a method for large displacement optical flow in which local matching costs are learned by a convolutional neural network (CNN) and a smoothness prior is imposed by a conditional random field (CRF). We tackle the computation- and memory-intensive operations on the 4D cost volume by a min-projection which reduces memory complexity from quadratic to linear and binary descriptors for efficient matching. This enables evaluation of the cost on the fly and allows to perform learning and CRF inference on high resolution images without ever storing the 4D cost volume. To address the problem of learning binary descriptors we propose a new hybrid learning scheme. In contrast to current state of the art approaches for learning binary CNNs we can compute the exact non-zero gradient within our model. We compare several methods for training binary descriptors and show results on public available benchmarks.,0
"Our proposed technique for optical flow with large displacement involves using a convolutional neural network (CNN) to learn local matching costs, while a conditional random field (CRF) imposes a smoothness constraint. We handle the computationally and memory-intensive operations on the 4D cost volume by using binary descriptors for efficient matching and a min-projection to reduce memory complexity. This allows for cost evaluation on the fly and enables learning and CRF inference on high resolution images without storing the 4D cost volume. To address the challenge of learning binary descriptors, we introduce a hybrid learning approach that computes the exact non-zero gradient within our model. We compare various methods for training binary descriptors and present results on publicly available benchmarks.",1
"Classical approaches for estimating optical flow have achieved rapid progress in the last decade. However, most of them are too slow to be applied in real-time video analysis. Due to the great success of deep learning, recent work has focused on using CNNs to solve such dense prediction problems. In this paper, we investigate a new deep architecture, Densely Connected Convolutional Networks (DenseNet), to learn optical flow. This specific architecture is ideal for the problem at hand as it provides shortcut connections throughout the network, which leads to implicit deep supervision. We extend current DenseNet to a fully convolutional network to learn motion estimation in an unsupervised manner. Evaluation results on three standard benchmarks demonstrate that DenseNet is a better fit than other widely adopted CNN architectures for optical flow estimation.",0
"Over the past decade, traditional methods of estimating optical flow have made considerable advancements, but their slow processing times make them impractical for real-time video analysis. To address this issue, recent research has focused on utilizing CNNs to solve dense prediction problems. In this study, we explore the use of Densely Connected Convolutional Networks (DenseNet), a new deep architecture that is well-suited for optical flow estimation due to its shortcut connections throughout the network and implicit deep supervision. We have extended the current DenseNet to a fully convolutional network for unsupervised motion estimation. Our evaluation on three standard benchmarks shows that DenseNet outperforms other commonly used CNN architectures for optical flow estimation.",1
"We propose a new multi-frame method for efficiently computing scene flow (dense depth and optical flow) and camera ego-motion for a dynamic scene observed from a moving stereo camera rig. Our technique also segments out moving objects from the rigid scene. In our method, we first estimate the disparity map and the 6-DOF camera motion using stereo matching and visual odometry. We then identify regions inconsistent with the estimated camera motion and compute per-pixel optical flow only at these regions. This flow proposal is fused with the camera motion-based flow proposal using fusion moves to obtain the final optical flow and motion segmentation. This unified framework benefits all four tasks - stereo, optical flow, visual odometry and motion segmentation leading to overall higher accuracy and efficiency. Our method is currently ranked third on the KITTI 2015 scene flow benchmark. Furthermore, our CPU implementation runs in 2-3 seconds per frame which is 1-3 orders of magnitude faster than the top six methods. We also report a thorough evaluation on challenging Sintel sequences with fast camera and object motion, where our method consistently outperforms OSF [Menze and Geiger, 2015], which is currently ranked second on the KITTI benchmark.",0
"A new technique has been proposed for efficiently computing scene flow, dense depth, optical flow, and camera ego-motion for a dynamic scene viewed from a moving stereo camera rig, while also identifying and segmenting out moving objects from the rigid scene. Initially, the disparity map and 6-DOF camera motion are estimated using stereo matching and visual odometry. Next, optical flow is computed per-pixel only in regions inconsistent with the estimated camera motion. Fusion moves are then used to combine this flow proposal with the camera motion-based flow proposal, resulting in the final optical flow and motion segmentation. This unified framework enhances the accuracy and efficiency of all four tasks - stereo, optical flow, visual odometry, and motion segmentation. The proposed method ranks third on the KITTI 2015 scene flow benchmark and runs on CPU in 2-3 seconds per frame, which is 1-3 orders of magnitude faster than the top six methods. In addition, the proposed method consistently outperforms the second-ranked OSF [Menze and Geiger, 2015] on challenging Sintel sequences with fast camera and object motion, according to a comprehensive evaluation.",1
"Learning approaches have shown great success in the task of super-resolving an image given a low resolution input. Video super-resolution aims for exploiting additionally the information from multiple images. Typically, the images are related via optical flow and consecutive image warping. In this paper, we provide an end-to-end video super-resolution network that, in contrast to previous works, includes the estimation of optical flow in the overall network architecture. We analyze the usage of optical flow for video super-resolution and find that common off-the-shelf image warping does not allow video super-resolution to benefit much from optical flow. We rather propose an operation for motion compensation that performs warping from low to high resolution directly. We show that with this network configuration, video super-resolution can benefit from optical flow and we obtain state-of-the-art results on the popular test sets. We also show that the processing of whole images rather than independent patches is responsible for a large increase in accuracy.",0
"The utilization of learning approaches has proven to be highly successful in the task of enhancing the resolution of an image when provided with a low-resolution input. Video super-resolution, on the other hand, seeks to make use of information from several images by way of optical flow and consecutive image warping. Our paper presents an all-in-one video super-resolution network that differs from previous work in that it incorporates the estimation of optical flow within the network architecture. We have evaluated the effectiveness of optical flow for video super-resolution, and have discovered that common image warping techniques do not allow for much benefit. Instead, we suggest a motion compensation operation that enables direct warping from low to high resolution. Our findings demonstrate that this network configuration allows video super-resolution to take advantage of optical flow, resulting in state-of-the-art outcomes on popular test sets. Furthermore, we have concluded that processing whole images, rather than independent patches, is responsible for a significant increase in accuracy.",1
"We study the unsupervised learning of CNNs for optical flow estimation using proxy ground truth data. Supervised CNNs, due to their immense learning capacity, have shown superior performance on a range of computer vision problems including optical flow prediction. They however require the ground truth flow which is usually not accessible except on limited synthetic data. Without the guidance of ground truth optical flow, unsupervised CNNs often perform worse as they are naturally ill-conditioned. We therefore propose a novel framework in which proxy ground truth data generated from classical approaches is used to guide the CNN learning. The models are further refined in an unsupervised fashion using an image reconstruction loss. Our guided learning approach is competitive with or superior to state-of-the-art approaches on three standard benchmark datasets yet is completely unsupervised and can run in real time.",0
"The focus of our research is on unsupervised learning of CNNs for optical flow estimation, where we use proxy ground truth data. Although supervised CNNs have demonstrated exceptional performance in various computer vision tasks, including optical flow prediction, their dependence on ground truth flow limits their applicability to synthetic data. Unsupervised CNNs, on the other hand, typically perform poorly due to their natural ill-conditioning in the absence of ground truth optical flow. To address this limitation, we present a unique framework that employs proxy ground truth data generated through traditional methods to guide CNN learning. Our approach also involves refining the models through unsupervised learning with an image reconstruction loss. Despite being unsupervised and capable of real-time operation, our guided learning approach is competitive or superior to state-of-the-art methods on three standard benchmark datasets.",1
"In this paper, we present YoTube-a novel network fusion framework for searching action proposals in untrimmed videos, where each action proposal corresponds to a spatialtemporal video tube that potentially locates one human action. Our method consists of a recurrent YoTube detector and a static YoTube detector, where the recurrent YoTube explores the regression capability of RNN for candidate bounding boxes predictions using learnt temporal dynamics and the static YoTube produces the bounding boxes using rich appearance cues in a single frame. Both networks are trained using rgb and optical flow in order to fully exploit the rich appearance, motion and temporal context, and their outputs are fused to produce accurate and robust proposal boxes. Action proposals are finally constructed by linking these boxes using dynamic programming with a novel trimming method to handle the untrimmed video effectively and efficiently. Extensive experiments on the challenging UCF-101 and UCF-Sports datasets show that our proposed technique obtains superior performance compared with the state-of-the-art.",0
"The purpose of this research is to introduce a new network fusion framework called YoTube which is designed to search for action proposals in untrimmed videos. Each action proposal corresponds to a spatial-temporal video tube that may contain a human action. Our method consists of two detectors: a recurrent YoTube detector and a static YoTube detector. The recurrent YoTube detector uses RNN to predict candidate bounding boxes based on learned temporal dynamics, while the static YoTube detector produces bounding boxes using rich appearance cues in a single frame. Both networks are trained using rgb and optical flow to fully exploit appearance, motion, and temporal context. The outputs of both detectors are combined to produce accurate and robust proposal boxes. Action proposals are then constructed by linking these boxes using dynamic programming and a new trimming method to effectively and efficiently handle untrimmed videos. Our experiments on the challenging UCF-101 and UCF-Sports datasets show that our proposed technique outperforms the state-of-the-art.",1
"We propose a novel method for temporally pooling frames in a video for the task of human action recognition. The method is motivated by the observation that there are only a small number of frames which, together, contain sufficient information to discriminate an action class present in a video, from the rest. The proposed method learns to pool such discriminative and informative frames, while discarding a majority of the non-informative frames in a single temporal scan of the video. Our algorithm does so by continuously predicting the discriminative importance of each video frame and subsequently pooling them in a deep learning framework. We show the effectiveness of our proposed pooling method on standard benchmarks where it consistently improves on baseline pooling methods, with both RGB and optical flow based Convolutional networks. Further, in combination with complementary video representations, we show results that are competitive with respect to the state-of-the-art results on two challenging and publicly available benchmark datasets.",0
"We have developed a new approach to pool frames in a video for the purpose of recognizing human actions. Our method is based on the observation that a small subset of frames contains enough information to distinguish an action class from others. By predicting the importance of each frame, our method selectively pools informative frames while discarding non-informative ones in a single pass. This is achieved through a deep learning framework. Our method outperforms existing pooling methods on standard benchmarks using both RGB and optical flow based Convolutional networks. Additionally, when combined with complementary video representations, our method achieves competitive results compared to the state-of-the-art on two challenging public benchmark datasets.",1
"Intra-operative measurements of tissue shape and multi/ hyperspectral information have the potential to provide surgical guidance and decision making support. We report an optical probe based system to combine sparse hyperspectral measurements and spectrally-encoded structured lighting (SL) for surface measurements. The system provides informative signals for navigation with a surgical interface. By rapidly switching between SL and white light (WL) modes, SL information is combined with structure-from-motion (SfM) from white light images, based on SURF feature detection and Lucas-Kanade (LK) optical flow to provide quasi-dense surface shape reconstruction with known scale in real-time. Furthermore, ""super-spectral-resolution"" was realized, whereby the RGB images and sparse hyperspectral data were integrated to recover dense pixel-level hyperspectral stacks, by using convolutional neural networks to upscale the wavelength dimension. Validation and demonstration of this system is reported on ex vivo/in vivo animal/ human experiments.",0
"The use of intra-operative measurement of tissue shape and multi/hyperspectral information can offer valuable support for surgical decision-making and guidance. Our study presents an optical probe-based system that combines spectrally-encoded structured lighting (SL) and sparse hyperspectral measurements for surface measurements. This system provides useful signals for navigation and has a surgical interface. By switching between SL and white light (WL) modes, SL information is combined with structure-from-motion (SfM) from white light images, which uses SURF feature detection and Lucas-Kanade (LK) optical flow for quasi-dense surface shape reconstruction in real-time. Additionally, we achieved ""super-spectral-resolution"" by integrating RGB images and sparse hyperspectral data to recover dense pixel-level hyperspectral stacks. This was done using convolutional neural networks to upscale the wavelength dimension. Our system was validated and demonstrated in animal/human experiments conducted ex vivo and in vivo.",1
"Rapid and low power computation of optical flow (OF) is potentially useful in robotics. The dynamic vision sensor (DVS) event camera produces quick and sparse output, and has high dynamic range, but conventional OF algorithms are frame-based and cannot be directly used with event-based cameras. Previous DVS OF methods do not work well with dense textured input and are designed for implementation in logic circuits. This paper proposes a new block-matching based DVS OF algorithm which is inspired by motion estimation methods used for MPEG video compression. The algorithm was implemented both in software and on FPGA. For each event, it computes the motion direction as one of 9 directions. The speed of the motion is set by the sample interval. Results show that the Average Angular Error can be improved by 30\% compared with previous methods. The OF can be calculated on FPGA with 50\,MHz clock in 0.2\,us per event (11 clock cycles), 20 times faster than a Java software implementation running on a desktop PC. Sample data is shown that the method works on scenes dominated by edges, sparse features, and dense texture.",0
"In the field of robotics, there is potential for optical flow (OF) computation to be both rapid and low power. The dynamic vision sensor (DVS) event camera is a quick and sparse output option that also has high dynamic range. However, conventional OF algorithms are frame-based and cannot be used directly with event-based cameras. Previous DVS OF methods were not effective with dense textured input and were designed for implementation in logic circuits. This study introduces a new block-matching based DVS OF algorithm inspired by motion estimation methods used for MPEG video compression. The algorithm was implemented in both software and on an FPGA. With each event, the motion direction is computed as one of nine directions, and the motion speed is set by the sample interval. Results show that the Average Angular Error can be improved by 30\% compared to previous methods. The FPGA can calculate OF with a 50 MHz clock in 0.2 us per event (11 clock cycles), 20 times faster than a Java software implementation on a desktop PC. Sample data is presented to demonstrate that this method works on scenes dominated by edges, sparse features, and dense texture.",1
"This work presents a supervised learning based approach to the computer vision problem of frame interpolation. The presented technique could also be used in the cartoon animations since drawing each individual frame consumes a noticeable amount of time. The most existing solutions to this problem use unsupervised methods and focus only on real life videos with already high frame rate. However, the experiments show that such methods do not work as well when the frame rate becomes low and object displacements between frames becomes large. This is due to the fact that interpolation of the large displacement motion requires knowledge of the motion structure thus the simple techniques such as frame averaging start to fail. In this work the deep convolutional neural network is used to solve the frame interpolation problem. In addition, it is shown that incorporating the prior information such as optical flow improves the interpolation quality significantly.",0
"A supervised learning approach to the computer vision issue of frame interpolation is presented in this work. This technique can also be applied to cartoon animations, which can be time-consuming to draw each frame individually. Most current solutions to this problem rely on unsupervised methods and only focus on high frame rate videos. However, experiments reveal that such methods do not perform as well when the frame rate decreases and object displacements between frames become significant. This is because interpolation of large displacement motion necessitates knowledge of the motion structure, causing simple techniques such as frame averaging to fail. The frame interpolation problem is solved using a deep convolutional neural network in this work. Furthermore, it is demonstrated that incorporating prior knowledge such as optical flow leads to a significant improvement in interpolation quality.",1
"Webly-supervised learning has recently emerged as an alternative paradigm to traditional supervised learning based on large-scale datasets with manual annotations. The key idea is that models such as CNNs can be learned from the noisy visual data available on the web. In this work we aim to exploit web data for video understanding tasks such as action recognition and detection. One of the main problems in webly-supervised learning is cleaning the noisy labeled data from the web. The state-of-the-art paradigm relies on training a first classifier on noisy data that is then used to clean the remaining dataset. Our key insight is that this procedure biases the second classifier towards samples that the first one understands. Here we train two independent CNNs, a RGB network on web images and video frames and a second network using temporal information from optical flow. We show that training the networks independently is vastly superior to selecting the frames for the flow classifier by using our RGB network. Moreover, we show benefits in enriching the training set with different data sources from heterogeneous public web databases. We demonstrate that our framework outperforms all other webly-supervised methods on two public benchmarks, UCF-101 and Thumos'14.",0
"Recently, there has been a rise in the use of webly-supervised learning, which offers an alternative to traditional supervised learning that relies on large-scale manually annotated datasets. The central concept of this approach is that models like CNNs can be trained using the web's noisy visual data. Our goal is to apply this approach to video understanding tasks, such as action recognition and detection, by utilizing web data. However, the main issue with webly-supervised learning is removing the noisy labeled data from the web. The current state-of-the-art method involves training a classifier on noisy data, which is then used to clean the remaining dataset. However, we discovered that this process biases the second classifier towards samples that the first one understands. We solved this problem by training two separate CNNs: an RGB network for web images and video frames, and a second network that uses temporal information from optical flow. Our approach of training the networks independently is much better than selecting frames for the flow classifier using our RGB network. Additionally, we benefit from enriching the training set with various data sources from diverse public web databases. Our framework outperforms all other webly-supervised methods on two public benchmarks, UCF-101 and Thumos'14.",1
"Accurate detection of the myocardial infarction (MI) area is crucial for early diagnosis planning and follow-up management. In this study, we propose an end-to-end deep-learning algorithm framework (OF-RNN ) to accurately detect the MI area at the pixel level. Our OF-RNN consists of three different function layers: the heart localization layers, which can accurately and automatically crop the region-of-interest (ROI) sequences, including the left ventricle, using the whole cardiac magnetic resonance image sequences; the motion statistical layers, which are used to build a time-series architecture to capture two types of motion features (at the pixel-level) by integrating the local motion features generated by long short-term memory-recurrent neural networks and the global motion features generated by deep optical flows from the whole ROI sequence, which can effectively characterize myocardial physiologic function; and the fully connected discriminate layers, which use stacked auto-encoders to further learn these features, and they use a softmax classifier to build the correspondences from the motion features to the tissue identities (infarction or not) for each pixel. Through the seamless connection of each layer, our OF-RNN can obtain the area, position, and shape of the MI for each patient. Our proposed framework yielded an overall classification accuracy of 94.35% at the pixel level, from 114 clinical subjects. These results indicate the potential of our proposed method in aiding standardized MI assessments.",0
"Precise identification of the myocardial infarction (MI) region is critical for early diagnosis planning and follow-up management. Our study presents a deep-learning algorithm framework (OF-RNN) that can accurately detect the MI area at the pixel level. The OF-RNN comprises three function layers: heart localization layers, motion statistical layers, and fully connected discriminate layers. The heart localization layers crop the region-of-interest (ROI) sequences, including the left ventricle, from the whole cardiac magnetic resonance image sequences. The motion statistical layers capture two types of motion features (at the pixel-level) to effectively characterize myocardial physiologic function. The fully connected discriminate layers use stacked auto-encoders to learn motion features and a softmax classifier to build correspondences from motion features to tissue identities (infarction or not) for each pixel. By seamlessly integrating each layer, our OF-RNN can determine the area, position, and shape of the MI for each patient. We obtained an overall classification accuracy of 94.35% at the pixel level, from 114 clinical subjects, indicating the potential of our method in aiding standardized MI assessments.",1
"Predicting an interaction before it is fully executed is very important in applications such as human-robot interaction and video surveillance. In a two-human interaction scenario, there often contextual dependency structure between the global interaction context of the two humans and the local context of the different body parts of each human. In this paper, we propose to learn the structure of the interaction contexts, and combine it with the spatial and temporal information of a video sequence for a better prediction of the interaction class. The structural models, including the spatial and the temporal models, are learned with Long Short Term Memory (LSTM) networks to capture the dependency of the global and local contexts of each RGB frame and each optical flow image, respectively. LSTM networks are also capable of detecting the key information from the global and local interaction contexts. Moreover, to effectively combine the structural models with the spatial and temporal models for interaction prediction, a ranking score fusion method is also introduced to automatically compute the optimal weight of each model for score fusion. Experimental results on the BIT Interaction and the UT-Interaction datasets clearly demonstrate the benefits of the proposed method.",0
"Anticipating an interaction prior to its full execution holds significant importance in various applications like video surveillance and human-robot interaction. In situations where two humans interact, the global interaction context of both individuals and the local context of their different body parts are contextually dependent. In this study, we suggest acquiring knowledge about the interaction context structure and integrating it with spatial and temporal information of a video sequence to enhance interaction classification predictions. The structural models, comprising spatial and temporal models, are trained using Long Short Term Memory (LSTM) networks to capture the dependence of global and local contexts of each RGB frame and optical flow image, respectively. LSTM networks can also identify crucial information from the global and local interaction contexts. Additionally, a ranking score fusion approach is introduced to combine the structural models with spatial and temporal models effectively, automatically computing the optimal weight for each model for score fusion. Results obtained from experiments on the BIT Interaction and the UT-Interaction datasets clearly indicate the advantages of the proposed technique.",1
"Typical human actions last several seconds and exhibit characteristic spatio-temporal structure. Recent methods attempt to capture this structure and learn action representations with convolutional neural networks. Such representations, however, are typically learned at the level of a few video frames failing to model actions at their full temporal extent. In this work we learn video representations using neural networks with long-term temporal convolutions (LTC). We demonstrate that LTC-CNN models with increased temporal extents improve the accuracy of action recognition. We also study the impact of different low-level representations, such as raw values of video pixels and optical flow vector fields and demonstrate the importance of high-quality optical flow estimation for learning accurate action models. We report state-of-the-art results on two challenging benchmarks for human action recognition UCF101 (92.7%) and HMDB51 (67.2%).",0
"Human actions typically last for several seconds and possess a distinct spatio-temporal structure. Recently, convolutional neural network-based methods have been used to capture this structure and learn action representations. However, these representations are generally learned at the level of a few video frames and are unable to model actions in their entirety. To address this issue, we have developed a method to learn video representations using neural networks with long-term temporal convolutions (LTC). Our experiments show that LTC-CNN models with extended temporal extents lead to improved accuracy in action recognition. We have also investigated the impact of different low-level representations, including raw video pixel values and optical flow vector fields. Our results demonstrate that high-quality optical flow estimation is crucial for learning accurate action models. We have achieved state-of-the-art performance on two challenging benchmarks for human action recognition, UCF101 (92.7%) and HMDB51 (67.2%).",1
"Infrared (IR) imaging has the potential to enable more robust action recognition systems compared to visible spectrum cameras due to lower sensitivity to lighting conditions and appearance variability. While the action recognition task on videos collected from visible spectrum imaging has received much attention, action recognition in IR videos is significantly less explored. Our objective is to exploit imaging data in this modality for the action recognition task. In this work, we propose a novel two-stream 3D convolutional neural network (CNN) architecture by introducing the discriminative code layer and the corresponding discriminative code loss function. The proposed network processes IR image and the IR-based optical flow field sequences. We pretrain the 3D CNN model on the visible spectrum Sports-1M action dataset and finetune it on the Infrared Action Recognition (InfAR) dataset. To our best knowledge, this is the first application of the 3D CNN to action recognition in the IR domain. We conduct an elaborate analysis of different fusion schemes (weighted average, single and double-layer neural nets) applied to different 3D CNN outputs. Experimental results demonstrate that our approach can achieve state-of-the-art average precision (AP) performances on the InfAR dataset: (1) the proposed two-stream 3D CNN achieves the best reported 77.5% AP, and (2) our 3D CNN model applied to the optical flow fields achieves the best reported single stream 75.42% AP.",0
"The use of infrared (IR) imaging can result in more reliable action recognition systems compared to visible spectrum cameras, as IR is less affected by lighting conditions and appearance variations. While there has been significant research on action recognition using visible spectrum videos, the potential of IR videos in this task has not been fully explored. Therefore, our aim is to leverage IR imaging data for action recognition. To accomplish this, we propose a new two-stream 3D convolutional neural network (CNN) architecture that includes a discriminative code layer and its corresponding loss function. The proposed network processes both IR images and IR-based optical flow field sequences. We pretrain the 3D CNN model on the Sports-1M action dataset, which uses visible spectrum videos, and then fine-tune it on the Infrared Action Recognition (InfAR) dataset. Our approach is the first application of 3D CNN to action recognition in the IR domain. We analyze different fusion schemes, including weighted average, single and double-layer neural nets, applied to different 3D CNN outputs. Our experimental results show that our approach achieves state-of-the-art average precision (AP) performances on the InfAR dataset. Specifically, the proposed two-stream 3D CNN achieves the best reported 77.5% AP, while our 3D CNN model applied to the optical flow fields achieves the best reported single stream 75.42% AP.",1
"We present a method to perform online Multiple Object Tracking (MOT) of known object categories in monocular video data. Current Tracking-by-Detection MOT approaches build on top of 2D bounding box detections. In contrast, we exploit state-of-the-art instance aware semantic segmentation techniques to compute 2D shape representations of target objects in each frame. We predict position and shape of segmented instances in subsequent frames by exploiting optical flow cues. We define an affinity matrix between instances of subsequent frames which reflects locality and visual similarity. The instance association is solved by applying the Hungarian method. We evaluate different configurations of our algorithm using the MOT 2D 2015 train dataset. The evaluation shows that our tracking approach is able to track objects with high relative motions. In addition, we provide results of our approach on the MOT 2D 2015 test set for comparison with previous works. We achieve a MOTA score of 32.1.",0
"A technique for performing online Multiple Object Tracking (MOT) of known object categories in monocular video data is introduced. Current Tracking-by-Detection MOT methods rely on 2D bounding box detections, while this approach leverages the latest instance aware semantic segmentation techniques to generate 2D shape representations of target objects in each frame. Optical flow cues are utilized to predict the position and shape of segmented instances in subsequent frames. An affinity matrix reflecting locality and visual similarity between instances of subsequent frames is defined, with instance association solved by the Hungarian method. The MOT 2D 2015 train dataset is used to evaluate different algorithm configurations, demonstrating the ability to track objects with high relative motions. Results on the MOT 2D 2015 test set are also provided for comparison with previous works, achieving a MOTA score of 32.1.",1
"We propose a novel approach based on deep Convolutional Neural Networks (CNN) to recognize human actions in still images by predicting the future motion, and detecting the shape and location of the salient parts of the image. We make the following major contributions to this important area of research: (i) We use the predicted future motion in the static image (Walker et al., 2015) as a means of compensating for the missing temporal information, while using the saliency map to represent the the spatial information in the form of location and shape of what is predicted as significant. (ii) We cast action classification in static images as a domain adaptation problem by transfer learning. We first map the input static image to a new domain that we refer to as the Predicted Optical Flow-Saliency Map domain (POF-SM), and then fine-tune the layers of a deep CNN model trained on classifying the ImageNet dataset to perform action classification in the POF-SM domain. (iii) We tested our method on the popular Willow dataset. But unlike existing methods, we also tested on a more realistic and challenging dataset of over 2M still images that we collected and labeled by taking random frames from the UCF-101 video dataset. We call our dataset the UCF Still Image dataset or UCFSI-101 in short. Our results outperform the state of the art.",0
"Our proposed method for recognizing human actions in still images utilizes deep Convolutional Neural Networks (CNN) and involves predicting future motion, detecting the shape and location of significant parts of the image using a saliency map. We make significant contributions to this field, including compensating for missing temporal information by using the predicted future motion while utilizing the saliency map to represent spatial information. We also approach action classification in static images as a domain adaptation problem through transfer learning. We map the input image to a new domain called the Predicted Optical Flow-Saliency Map domain (POF-SM) and fine-tune a deep CNN model trained on classifying the ImageNet dataset to classify actions in the POF-SM domain. Our method was tested on the Willow dataset and a more challenging dataset of over 2 million still images, which we collected and labeled from the UCF-101 video dataset, called the UCF Still Image dataset (UCFSI-101). Our results exceed the state of the art.",1
"Recent progress in style transfer on images has focused on improving the quality of stylized images and speed of methods. However, real-time methods are highly unstable resulting in visible flickering when applied to videos. In this work we characterize the instability of these methods by examining the solution set of the style transfer objective. We show that the trace of the Gram matrix representing style is inversely related to the stability of the method. Then, we present a recurrent convolutional network for real-time video style transfer which incorporates a temporal consistency loss and overcomes the instability of prior methods. Our networks can be applied at any resolution, do not re- quire optical flow at test time, and produce high quality, temporally consistent stylized videos in real-time.",0
"The recent advancements in the field of style transfer for images have been centered around enhancing the quality and speed of the stylized images. However, the real-time techniques exhibit high instability, leading to noticeable flickering when implemented on videos. This study delves into the instability of such methods by analyzing the solution set of the style transfer objective. The research demonstrates that the trace of the Gram matrix representing style is inversely proportional to the stability of the method. As a solution to this problem, a recurrent convolutional network is proposed for real-time video style transfer. The network incorporates a temporal consistency loss and overcomes the instability issues of previous methods. The networks can be employed at any resolution and do not require optical flow during testing. They generate high-quality and temporally consistent stylized videos in real-time.",1
"Given a visual history, multiple future outcomes for a video scene are equally probable, in other words, the distribution of future outcomes has multiple modes. Multimodality is notoriously hard to handle by standard regressors or classifiers: the former regress to the mean and the latter discretize a continuous high dimensional output space. In this work, we present stochastic neural network architectures that handle such multimodality through stochasticity: future trajectories of objects, body joints or frames are represented as deep, non-linear transformations of random (as opposed to deterministic) variables. Such random variables are sampled from simple Gaussian distributions whose means and variances are parametrized by the output of convolutional encoders over the visual history. We introduce novel convolutional architectures for predicting future body joint trajectories that outperform fully connected alternatives \cite{DBLP:journals/corr/WalkerDGH16}. We introduce stochastic spatial transformers through optical flow warping for predicting future frames, which outperform their deterministic equivalents \cite{DBLP:journals/corr/PatrauceanHC15}. Training stochastic networks involves an intractable marginalization over stochastic variables. We compare various training schemes that handle such marginalization through a) straightforward sampling from the prior, b) conditional variational autoencoders \cite{NIPS2015_5775,DBLP:journals/corr/WalkerDGH16}, and, c) a proposed K-best-sample loss that penalizes the best prediction under a fixed ""prediction budget"". We show experimental results on object trajectory prediction, human body joint trajectory prediction and video prediction under varying future uncertainty, validating quantitatively and qualitatively our architectural choices and training schemes.",0
"The distribution of future outcomes for a video scene can have multiple modes when viewed through a visual history, resulting in equal probability for multiple outcomes. This poses a challenge for standard classifiers and regressors that either regress to the mean or discretize a continuous high-dimensional output space. To address this, we propose using stochastic neural network architectures that represent future trajectories of objects, body joints, or frames as deep non-linear transformations of random variables. These random variables are sampled from Gaussian distributions with means and variances parametrized by convolutional encoders over the visual history. We introduce novel convolutional architectures for predicting future body joint trajectories and stochastic spatial transformers through optical flow warping for predicting future frames, both of which outperform their deterministic counterparts. However, training stochastic networks involves intractable marginalization over stochastic variables, which we address through various training schemes, including sampling from the prior, conditional variational autoencoders, and a proposed K-best-sample loss that penalizes the best prediction under a fixed ""prediction budget."" Our experimental results demonstrate the effectiveness of our proposed approach for object trajectory prediction, human body joint trajectory prediction, and video prediction under varying future uncertainty.",1
"The optical flow of natural scenes is a combination of the motion of the observer and the independent motion of objects. Existing algorithms typically focus on either recovering motion and structure under the assumption of a purely static world or optical flow for general unconstrained scenes. We combine these approaches in an optical flow algorithm that estimates an explicit segmentation of moving objects from appearance and physical constraints. In static regions we take advantage of strong constraints to jointly estimate the camera motion and the 3D structure of the scene over multiple frames. This allows us to also regularize the structure instead of the motion. Our formulation uses a Plane+Parallax framework, which works even under small baselines, and reduces the motion estimation to a one-dimensional search problem, resulting in more accurate estimation. In moving regions the flow is treated as unconstrained, and computed with an existing optical flow method. The resulting Mostly-Rigid Flow (MR-Flow) method achieves state-of-the-art results on both the MPI-Sintel and KITTI-2015 benchmarks.",0
"An optical flow algorithm that takes into account both the observer's motion and the independent motion of objects in natural scenes is proposed. Previous algorithms have focused on either a completely static world or general unconstrained scenes. The proposed algorithm uses appearance and physical constraints to estimate an explicit segmentation of moving objects. For static regions, the algorithm jointly estimates the camera motion and 3D structure of the scene over multiple frames, allowing for structure regularization instead of just motion. The Plane+Parallax framework is used, which reduces the motion estimation to a one-dimensional search problem and works well under small baselines. For moving regions, an existing optical flow method is used. The resulting Mostly-Rigid Flow (MR-Flow) method achieves state-of-the-art results on both the MPI-Sintel and KITTI-2015 benchmarks.",1
"We propose a novel superpixel-based multi-view convolutional neural network for semantic image segmentation. The proposed network produces a high quality segmentation of a single image by leveraging information from additional views of the same scene. Particularly in indoor videos such as captured by robotic platforms or handheld and bodyworn RGBD cameras, nearby video frames provide diverse viewpoints and additional context of objects and scenes. To leverage such information, we first compute region correspondences by optical flow and image boundary-based superpixels. Given these region correspondences, we propose a novel spatio-temporal pooling layer to aggregate information over space and time. We evaluate our approach on the NYU--Depth--V2 and the SUN3D datasets and compare it to various state-of-the-art single-view and multi-view approaches. Besides a general improvement over the state-of-the-art, we also show the benefits of making use of unlabeled frames during training for multi-view as well as single-view prediction.",0
"Our proposal is a superpixel-based multi-view convolutional neural network designed for semantic image segmentation. By utilizing information from multiple views of the same scene, our network can produce high-quality segmentation of a single image. This is especially useful for indoor videos captured by robotic platforms or handheld and bodyworn RGBD cameras, where nearby video frames provide diverse viewpoints and additional context of objects and scenes. First, we compute region correspondences using optical flow and image boundary-based superpixels. Then, we introduce a spatio-temporal pooling layer to aggregate information over space and time. Our approach is evaluated on the NYU-Depth-V2 and SUN3D datasets and compared to various state-of-the-art single-view and multi-view approaches. We also demonstrate the benefits of incorporating unlabeled frames during training for both multi-view and single-view prediction. Overall, our approach offers a significant improvement over existing methods.",1
"We propose SfM-Net, a geometry-aware neural network for motion estimation in videos that decomposes frame-to-frame pixel motion in terms of scene and object depth, camera motion and 3D object rotations and translations. Given a sequence of frames, SfM-Net predicts depth, segmentation, camera and rigid object motions, converts those into a dense frame-to-frame motion field (optical flow), differentiably warps frames in time to match pixels and back-propagates. The model can be trained with various degrees of supervision: 1) self-supervised by the re-projection photometric error (completely unsupervised), 2) supervised by ego-motion (camera motion), or 3) supervised by depth (e.g., as provided by RGBD sensors). SfM-Net extracts meaningful depth estimates and successfully estimates frame-to-frame camera rotations and translations. It often successfully segments the moving objects in the scene, even though such supervision is never provided.",0
"Our proposed solution, SfM-Net, is a neural network that is conscious of geometry and can estimate motion in videos by breaking down the pixel motion from one frame to the next in terms of object and scene depth, camera movement, and 3D object translations and rotations. SfM-Net can predict depth, segmentation, rigid object and camera motions given a sequence of frames, and then convert these into a dense frame-to-frame motion field (optical flow), warp frames in time differentiably, and back-propagate. The model can be trained with varying levels of supervision, including self-supervision via re-projection photometric error (completely unsupervised), supervised by ego-motion (camera motion), or supervised by depth (e.g., from RGBD sensors). SfM-Net can derive sensible depth estimations, effectively estimate frame-to-frame camera movements, and often segment moving objects in the scene, without any supervision.",1
"Automated Facial Expression Recognition (FER) has been a challenging task for decades. Many of the existing works use hand-crafted features such as LBP, HOG, LPQ, and Histogram of Optical Flow (HOF) combined with classifiers such as Support Vector Machines for expression recognition. These methods often require rigorous hyperparameter tuning to achieve good results. Recently Deep Neural Networks (DNN) have shown to outperform traditional methods in visual object recognition. In this paper, we propose a two-part network consisting of a DNN-based architecture followed by a Conditional Random Field (CRF) module for facial expression recognition in videos. The first part captures the spatial relation within facial images using convolutional layers followed by three Inception-ResNet modules and two fully-connected layers. To capture the temporal relation between the image frames, we use linear chain CRF in the second part of our network. We evaluate our proposed network on three publicly available databases, viz. CK+, MMI, and FERA. Experiments are performed in subject-independent and cross-database manners. Our experimental results show that cascading the deep network architecture with the CRF module considerably increases the recognition of facial expressions in videos and in particular it outperforms the state-of-the-art methods in the cross-database experiments and yields comparable results in the subject-independent experiments.",0
"For decades, recognizing facial expressions using automated technology has been a difficult task. Previous methods have utilized hand-crafted features like LBP, HOG, LPQ, and Histogram of Optical Flow (HOF) in combination with Support Vector Machines to detect expressions, but these methods require extensive hyperparameter tuning to achieve satisfactory outcomes. However, recent advancements in Deep Neural Networks (DNN) have proven to be more effective than traditional techniques in visual object recognition. This study proposes a two-part network for recognizing facial expressions in videos, consisting of a DNN-based architecture followed by a Conditional Random Field (CRF) module. The first part captures the spatial relation of facial images through convolutional layers, Inception-ResNet modules, and fully-connected layers. The second part utilizes a linear chain CRF to capture the temporal relation between image frames. The proposed network was evaluated on three publicly available databases, and the results show that incorporating the CRF module significantly improves the recognition of facial expressions in videos, surpassing state-of-the-art methods in cross-database experiments and producing comparable results in subject-independent experiments.",1
"We present an optical flow estimation approach that operates on the full four-dimensional cost volume. This direct approach shares the structural benefits of leading stereo matching pipelines, which are known to yield high accuracy. To this day, such approaches have been considered impractical due to the size of the cost volume. We show that the full four-dimensional cost volume can be constructed in a fraction of a second due to its regularity. We then exploit this regularity further by adapting semi-global matching to the four-dimensional setting. This yields a pipeline that achieves significantly higher accuracy than state-of-the-art optical flow methods while being faster than most. Our approach outperforms all published general-purpose optical flow methods on both Sintel and KITTI 2015 benchmarks.",0
"We introduce a method for estimating optical flow using the complete four-dimensional cost volume, which shares the advantageous structure of leading stereo matching pipelines and provides high accuracy. Despite being considered impractical in the past due to the cost volume's size, we demonstrate that it can be generated in a fraction of a second thanks to its regularity. We take advantage of this regularity by adapting semi-global matching to the four-dimensional setting. As a result, our pipeline not only performs faster than most state-of-the-art optical flow techniques but also yields significantly higher accuracy. Our approach surpasses all published general-purpose optical flow methods on both Sintel and KITTI 2015 benchmarks.",1
"The ability to amplify or reduce subtle image changes over time is useful in contexts such as video editing, medical video analysis, product quality control and sports. In these contexts there is often large motion present which severely distorts current video amplification methods that magnify change linearly. In this work we propose a method to cope with large motions while still magnifying small changes. We make the following two observations: i) large motions are linear on the temporal scale of the small changes; ii) small changes deviate from this linearity. We ignore linear motion and propose to magnify acceleration. Our method is pure Eulerian and does not require any optical flow, temporal alignment or region annotations. We link temporal second-order derivative filtering to spatial acceleration magnification. We apply our method to moving objects where we show motion magnification and color magnification. We provide quantitative as well as qualitative evidence for our method while comparing to the state-of-the-art.",0
"Being able to enhance or diminish subtle changes in images over time is valuable in various fields, including video editing, medical video analysis, product quality control, and sports. These fields often involve significant motion, which can distort current video amplification techniques that amplify changes in a linear fashion. In this study, we propose a method to address large motions while still amplifying small changes. We note that large motions are linear when viewed over the same temporal scale as small changes, whereas small changes deviate from this linearity. We disregard linear motion and suggest amplifying acceleration instead. Our approach is entirely Eulerian and does not necessitate optical flow, temporal alignment, or region annotations. We connect temporal second-order derivative filtering to spatial acceleration amplification. We apply our technique to moving objects, showing motion and color magnification. We offer both quantitative and qualitative evidence for our method, contrasting it with the current state of the art.",1
"It is difficult to recover the motion field from a real-world footage given a mixture of camera shake and other photometric effects. In this paper we propose a hybrid framework by interleaving a Convolutional Neural Network (CNN) and a traditional optical flow energy. We first conduct a CNN architecture using a novel learnable directional filtering layer. Such layer encodes the angle and distance similarity matrix between blur and camera motion, which is able to enhance the blur features of the camera-shake footages. The proposed CNNs are then integrated into an iterative optical flow framework, which enable the capability of modelling and solving both the blind deconvolution and the optical flow estimation problems simultaneously. Our framework is trained end-to-end on a synthetic dataset and yields competitive precision and performance against the state-of-the-art approaches.",0
"Recovering the motion field from real-world footage that has a mixture of camera shake and other photometric effects is a challenging task. To address this, we present a hybrid framework that combines a Convolutional Neural Network (CNN) and traditional optical flow energy. Our approach begins by creating a CNN architecture that uses a novel learnable directional filtering layer. This layer encodes the angle and distance similarity matrix between blur and camera motion, enhancing the blur features of the camera-shake footages. We then integrate the proposed CNNs into an iterative optical flow framework, which enables the simultaneous modeling and solving of both the blind deconvolution and optical flow estimation problems. Our framework is trained end-to-end on a synthetic dataset and achieves competitive precision and performance compared to state-of-the-art methods.",1
"Dynamic scene understanding is a challenging problem and motion segmentation plays a crucial role in solving it. Incorporating semantics and motion enhances the overall perception of the dynamic scene. For applications of outdoor robotic navigation, joint learning methods have not been extensively used for extracting spatio-temporal features or adding different priors into the formulation. The task becomes even more challenging without stereo information being incorporated. This paper proposes an approach to fuse semantic features and motion clues using CNNs, to address the problem of monocular semantic motion segmentation. We deduce semantic and motion labels by integrating optical flow as a constraint with semantic features into dilated convolution network. The pipeline consists of three main stages i.e Feature extraction, Feature amplification and Multi Scale Context Aggregation to fuse the semantics and flow features. Our joint formulation shows significant improvements in monocular motion segmentation over the state of the art methods on challenging KITTI tracking dataset.",0
"Solving the problem of dynamic scene understanding is difficult and motion segmentation is integral to this process. The perception of a dynamic scene can be improved by incorporating both semantics and motion. In outdoor robotic navigation applications, joint learning methods have not been widely used to extract spatio-temporal features or to add different priors to problem formulation. The absence of stereo information makes this task even more challenging. This paper proposes a solution to the problem of monocular semantic motion segmentation by fusing semantic features and motion clues using CNNs. We integrate optical flow as a constraint with semantic features into a dilated convolution network to deduce semantic and motion labels. The pipeline involves three stages: feature extraction, feature amplification, and multi-scale context aggregation to fuse the semantics and flow features. Our joint formulation provides significant improvements in monocular motion segmentation over state-of-the-art methods on the challenging KITTI tracking dataset.",1
We propose a variational approach to obtain super-resolution images from multiple low-resolution frames extracted from video clips. First the displacement between the low-resolution frames and the reference frame are computed by an optical flow algorithm. Then a low-rank model is used to construct the reference frame in high-resolution by incorporating the information of the low-resolution frames. The model has two terms: a 2-norm data fidelity term and a nuclear-norm regularization term. Alternating direction method of multipliers is used to solve the model. Comparison of our methods with other models on synthetic and real video clips show that our resulting images are more accurate with less artifacts. It also provides much finer and discernable details.,0
"To obtain high-resolution images from low-resolution frames extracted from video clips, we suggest a variational approach. Initially, the optical flow algorithm calculates the displacement between the low-resolution frames and the reference frame. Following this, we employ a low-rank model with two terms: a 2-norm data fidelity term and a nuclear-norm regularization term to create the high-resolution reference frame. To solve the model, we use the alternating direction method of multipliers. In comparison to other models, our approach produces more precise images with fewer artifacts and enhances the finer details. The accuracy of our proposed method is demonstrated through experiments on both synthetic and real video clips.",1
"Initializing optical flow field by either sparse descriptor matching or dense patch matches has been proved to be particularly useful for capturing large displacements. In this paper, we present a pyramidal gradient matching approach that can provide dense matches for highly accurate and efficient optical flow estimation. A novel contribution of our method is that image gradient is used to describe image patches and proved to be able to produce robust matching. Therefore, our method is more efficient than methods that adopt special features (like SIFT) or patch distance metric. Moreover, we find that image gradient is scalable for optical flow estimation, which means we can use different levels of gradient feature (for example, full gradients or only direction information of gradients) to obtain different complexity without dramatic changes in accuracy. Another contribution is that we uncover the secrets of limited PatchMatch through a thorough analysis and design a pyramidal matching framework based these secrets. Our pyramidal matching framework is aimed at robust gradient matching and effective to grow inliers and reject outliers. In this framework, we present some special enhancements for outlier filtering in gradient matching. By initializing EpicFlow with our matches, experimental results show that our method is efficient and robust (ranking 1st on both clean pass and final pass of MPI Sintel dataset among published methods).",0
"The use of sparse descriptor matching or dense patch matches has been proven effective for capturing large displacements in initializing the optical flow field. In this study, we introduce a pyramidal gradient matching approach that can provide dense matches for highly accurate and efficient optical flow estimation. Our method utilizes image gradient to describe image patches, resulting in robust matching without the need for special features or patch distance metrics. Additionally, our approach allows for scalability in optical flow estimation through the use of different levels of gradient features, without significant changes in accuracy. We also present a pyramidal matching framework that is designed to grow inliers and reject outliers, with special enhancements for outlier filtering in gradient matching. Our method is efficient and robust, achieving the top rank on both clean pass and final pass of MPI Sintel dataset among published methods after initializing EpicFlow with our matches.",1
"In this paper we formulate structure from motion as a learning problem. We train a convolutional network end-to-end to compute depth and camera motion from successive, unconstrained image pairs. The architecture is composed of multiple stacked encoder-decoder networks, the core part being an iterative network that is able to improve its own predictions. The network estimates not only depth and motion, but additionally surface normals, optical flow between the images and confidence of the matching. A crucial component of the approach is a training loss based on spatial relative differences. Compared to traditional two-frame structure from motion methods, results are more accurate and more robust. In contrast to the popular depth-from-single-image networks, DeMoN learns the concept of matching and, thus, better generalizes to structures not seen during training.",0
"The aim of this study is to present a novel approach to structure from motion by framing it as a learning challenge. Our proposed methodology involves training a convolutional network end-to-end, with the objective of computing both depth and camera motion from unconstrained image pairs. The network architecture consists of several stacked encoder-decoder networks, with the iterative network being the main component that is capable of enhancing its own predictions. In addition to depth and motion, the network also estimates surface normals, optical flow, and matching confidence. The training process utilizes a spatial relative differences-based loss function, which is a crucial element of the approach, resulting in more accurate and robust results compared to traditional two-frame structure from motion methods. Unlike depth-from-single-image networks, DeMoN, our proposed approach, learns the concept of matching, making it more adaptable to unencountered structures during training.",1
"The problem of determining whether an object is in motion, irrespective of camera motion, is far from being solved. We address this challenging task by learning motion patterns in videos. The core of our approach is a fully convolutional network, which is learned entirely from synthetic video sequences, and their ground-truth optical flow and motion segmentation. This encoder-decoder style architecture first learns a coarse representation of the optical flow field features, and then refines it iteratively to produce motion labels at the original high-resolution. We further improve this labeling with an objectness map and a conditional random field, to account for errors in optical flow, and also to focus on moving ""things"" rather than ""stuff"". The output label of each pixel denotes whether it has undergone independent motion, i.e., irrespective of camera motion. We demonstrate the benefits of this learning framework on the moving object segmentation task, where the goal is to segment all objects in motion. Our approach outperforms the top method on the recently released DAVIS benchmark dataset, comprising real-world sequences, by 5.6%. We also evaluate on the Berkeley motion segmentation database, achieving state-of-the-art results.",0
"Finding a solution to determine if an object is in motion without considering camera movement is a complicated issue. Our approach to tackle this challenge involves studying motion patterns in videos. Our method relies on a fully convolutional network that we train exclusively on synthetic video sequences. We use ground-truth optical flow and motion segmentation to teach the network. The network learns to recognize coarse features of the optical flow field and then iteratively refines them to produce motion labels with high resolution. We enhance the labeling process by adding an objectness map and a conditional random field to account for optical flow errors and focus on moving objects rather than stationary ones. Our labeling system distinguishes independent motion of each pixel regardless of camera motion. We applied our learning framework to the moving object segmentation task, which aims to segment all moving objects. Our approach surpasses the top method by 5.6% on the DAVIS benchmark dataset, which consists of real-world sequences. We also achieved state-of-the-art results on the Berkeley motion segmentation database.",1
"CNN-based optical flow estimation has attracted attention recently, mainly due to its impressively high frame rates. These networks perform well on synthetic datasets, but they are still far behind the classical methods in real-world videos. This is because there is no ground truth optical flow for training these networks on real data. In this paper, we boost CNN-based optical flow estimation in real scenes with the help of the freely available self-supervised task of next-frame prediction. To this end, we train the network in a hybrid way, providing it with a mixture of synthetic and real videos. With the help of a sample-variant multi-tasking architecture, the network is trained on different tasks depending on the availability of ground-truth. We also experiment with the prediction of ""next-flow"" instead of estimation of the current flow, which is intuitively closer to the task of next-frame prediction and yields favorable results. We demonstrate the improvement in optical flow estimation on the real-world KITTI benchmark. Additionally, we test the optical flow indirectly in an action classification scenario. As a side product of this work, we report significant improvements over state-of-the-art in the task of next-frame prediction.",0
"Recently, CNN-based optical flow estimation has gained attention for its high frame rates. However, these networks have been found to be less effective on real-world videos than classical methods because there is no ground truth optical flow for training on actual data. In this study, we aimed to enhance CNN-based optical flow estimation in real scenes by utilizing the freely available self-supervised task of next-frame prediction. We hybrid-trained the network with a combination of synthetic and real videos and used a sample-variant multi-tasking architecture to train the network on different tasks based on ground-truth availability. Instead of estimating the current flow, we experimented with predicting ""next-flow,"" which is closer to the task of next-frame prediction and yields positive outcomes. We demonstrate our improved optical flow estimation on the KITTI benchmark and indirectly test it in an action classification scenario. This study also reports significant enhancements over state-of-the-art in the next-frame prediction task.",1
"Training of Convolutional Neural Networks (CNNs) on long video sequences is computationally expensive due to the substantial memory requirements and the massive number of parameters that deep architectures demand. Early fusion of video frames is thus a standard technique, in which several consecutive frames are first agglomerated into a compact representation, and then fed into the CNN as an input sample. For this purpose, a summarization approach that represents a set of consecutive RGB frames by a single dynamic image to capture pixel dynamics is proposed recently. In this paper, we introduce a novel ordered representation of consecutive optical flow frames as an alternative and argue that this representation captures the action dynamics more effectively than RGB frames. We provide intuitions on why such a representation is better for action recognition. We validate our claims on standard benchmark datasets and demonstrate that using summaries of flow images lead to significant improvements over RGB frames while achieving accuracy comparable to the state-of-the-art on UCF101 and HMDB datasets.",0
"Due to the large number of parameters required by deep architectures and the significant memory requirements, the training of Convolutional Neural Networks (CNNs) on extended video sequences can be computationally expensive. To address this issue, the standard technique is to apply early fusion of video frames, where multiple consecutive frames are combined into a concise representation and then entered as an input sample into the CNN. Recently, a summarization approach was proposed in which a set of consecutive RGB frames is represented by a single dynamic image to capture pixel dynamics. In this paper, we propose an alternative approach using an ordered representation of consecutive optical flow frames, arguing that this approach captures action dynamics more effectively than RGB frames. We provide reasoning for why this representation is superior for action recognition and validate our claims on benchmark datasets, demonstrating that using summaries of flow images leads to significant improvements over RGB frames while achieving accuracy comparable to the state-of-the-art on UCF101 and HMDB datasets.",1
"We propose a framework for Google Map aided UAV navigation in GPS-denied environment. Geo-referenced navigation provides drift-free localization and does not require loop closures. The UAV position is initialized via correlation, which is simple and efficient. We then use optical flow to predict its position in subsequent frames. During pose tracking, we obtain inter-frame translation either by motion field or homography decomposition, and we use HOG features for registration on Google Map. We employ particle filter to conduct a coarse to fine search to localize the UAV. Offline test using aerial images collected by our quadrotor platform shows promising results as our approach eliminates the drift in dead-reckoning, and the small localization error indicates the superiority of our approach as a supplement to GPS.",0
"Our proposed framework utilizes Google Maps to aid in UAV navigation in GPS-deprived environments. By implementing geo-referenced navigation, we can ensure accurate localization without the need for loop closures. To initialize the UAV position, we use a straightforward and efficient correlation method. Optical flow is then utilized to predict the UAV's position in subsequent frames. During pose tracking, we use either motion field or homography decomposition to obtain inter-frame translation and HOG features for registration on Google Maps. To localize the UAV, we apply a particle filter for a coarse-to-fine search. Our approach has been tested offline using aerial images collected by our quadrotor platform, yielding promising results. Our method eliminates dead-reckoning drift and provides superior localization accuracy compared to GPS alone.",1
"Video classification is productive in many practical applications, and the recent deep learning has greatly improved its accuracy. However, existing works often model video frames indiscriminately, but from the view of motion, video frames can be decomposed into salient and non-salient areas naturally. Salient and non-salient areas should be modeled with different networks, for the former present both appearance and motion information, and the latter present static background information. To address this problem, in this paper, video saliency is predicted by optical flow without supervision firstly. Then two streams of 3D CNN are trained individually for raw frames and optical flow on salient areas, and another 2D CNN is trained for raw frames on non-salient areas. For the reason that these three streams play different roles for each class, the weights of each stream are adaptively learned for each class. Experimental results show that saliency-guided modeling and adaptively weighted learning can reinforce each other, and we achieve the state-of-the-art results.",0
"The accuracy of video classification has significantly improved with recent developments in deep learning, making it a valuable tool in various practical applications. However, current methods for modeling video frames do not take into account the salient and non-salient areas of motion present within them. This paper proposes a solution by first predicting the saliency of video frames using optical flow without supervision. Then, two streams of 3D CNN are trained separately for raw frames and optical flow on salient areas, while a 2D CNN is trained for raw frames on non-salient areas. As each stream plays a unique role for each class, the weights of each stream are adaptively learned for each class. Through experimental results, it is shown that saliency-guided modeling and adaptively weighted learning complement each other, resulting in state-of-the-art performance.",1
"Video frame interpolation typically involves two steps: motion estimation and pixel synthesis. Such a two-step approach heavily depends on the quality of motion estimation. This paper presents a robust video frame interpolation method that combines these two steps into a single process. Specifically, our method considers pixel synthesis for the interpolated frame as local convolution over two input frames. The convolution kernel captures both the local motion between the input frames and the coefficients for pixel synthesis. Our method employs a deep fully convolutional neural network to estimate a spatially-adaptive convolution kernel for each pixel. This deep neural network can be directly trained end to end using widely available video data without any difficult-to-obtain ground-truth data like optical flow. Our experiments show that the formulation of video interpolation as a single convolution process allows our method to gracefully handle challenges like occlusion, blur, and abrupt brightness change and enables high-quality video frame interpolation.",0
"Typically, video frame interpolation involves two steps: motion estimation and pixel synthesis, with the quality of motion estimation heavily influencing the overall outcome. However, this paper proposes a robust video frame interpolation method that integrates these two steps into a single process. Our approach involves local convolution over two input frames to synthesize the interpolated frame, with the convolution kernel capturing both local motion and pixel synthesis coefficients. To estimate a spatially-adaptive convolution kernel for each pixel, we use a deep fully convolutional neural network that can be trained end to end using widely available video data, without the need for ground-truth data like optical flow. By formulating video interpolation as a single convolution process, our method can handle challenges such as occlusion, blur, and abrupt brightness change, resulting in high-quality video frame interpolation.",1
"We present a generative method to estimate 3D human motion and body shape from monocular video. Under the assumption that starting from an initial pose optical flow constrains subsequent human motion, we exploit flow to find temporally coherent human poses of a motion sequence. We estimate human motion by minimizing the difference between computed flow fields and the output of an artificial flow renderer. A single initialization step is required to estimate motion over multiple frames. Several regularization functions enhance robustness over time. Our test scenarios demonstrate that optical flow effectively regularizes the under-constrained problem of human shape and motion estimation from monocular video.",0
"Our proposed approach is a generative technique that can determine the 3D motion and body shape of a human from a single video. We assume that optical flow can constrain the human motion sequence, and thus, we use it to identify human poses that are temporally consistent. To estimate human motion, we compare the computed flow fields to the output of an artificial flow renderer and minimize the difference. Only one initialization step is needed to estimate motion for multiple frames, and we use several regularization functions to enhance robustness over time. Our test results demonstrate that optical flow is effective in regulating the challenging problem of estimating human shape and motion from monocular video.",1
"In computer vision most iterative optimization algorithms, both sparse and dense, rely on a coarse and reliable dense initialization to bootstrap their optimization procedure. For example, dense optical flow algorithms profit massively in speed and robustness if they are initialized well in the basin of convergence of the used loss function. The same holds true for methods as sparse feature tracking when initial flow or depth information for new features at arbitrary positions is needed. This makes it extremely important to have techniques at hand that allow to obtain from only very few available measurements a dense but still approximative sketch of a desired 2D structure (e.g. depth maps, optical flow, disparity maps, etc.). The 2D map is regarded as sample from a 2D random process. The method presented here exploits the complete information given by the principal component analysis (PCA) of that process, the principal basis and its prior distribution. The method is able to determine a dense reconstruction from sparse measurement. When facing situations with only very sparse measurements, typically the number of principal components is further reduced which results in a loss of expressiveness of the basis. We overcome this problem and inject prior knowledge in a maximum a posterior (MAP) approach. We test our approach on the KITTI and the virtual KITTI datasets and focus on the interpolation of depth maps for driving scenes. The evaluation of the results show good agreement to the ground truth and are clearly better than results of interpolation by the nearest neighbor method which disregards statistical information.",0
"In computer vision, most iterative optimization algorithms rely on a reliable dense initialization to begin their optimization process. This is especially true for algorithms like dense optical flow, which benefit from being well initialized in the basin of convergence of the used loss function. Sparse feature tracking also requires initial flow or depth information for new features at arbitrary positions. Therefore, it is crucial to have techniques that can obtain a dense but still approximative sketch of a desired 2D structure from only a few available measurements. The presented method uses principal component analysis (PCA) of the 2D random process to determine a dense reconstruction from sparse measurements. When faced with very sparse measurements, the number of principal components is reduced, which limits the expressiveness of the basis. However, this problem is overcome by injecting prior knowledge through a maximum a posterior (MAP) approach. The approach is tested on the KITTI and virtual KITTI datasets with a focus on interpolating depth maps for driving scenes. The results show good agreement with the ground truth and outperform the nearest neighbor method, which disregards statistical information.",1
"Sparse-to-dense interpolation for optical flow is a fundamental phase in the pipeline of most of the leading optical flow estimation algorithms. The current state-of-the-art method for interpolation, EpicFlow, is a local average method based on an edge aware geodesic distance. We propose a new data-driven sparse-to-dense interpolation algorithm based on a fully convolutional network. We draw inspiration from the filling-in process in the visual cortex and introduce lateral dependencies between neurons and multi-layer supervision into our learning process. We also show the importance of the image contour to the learning process. Our method is robust and outperforms EpicFlow on competitive optical flow benchmarks with several underlying matching algorithms. This leads to state-of-the-art performance on the Sintel and KITTI 2012 benchmarks.",0
"Interpolating sparse-to-dense data for optical flow is a crucial step in many optical flow estimation algorithms. While EpicFlow is currently the most advanced interpolation method, it relies on local averaging and edge-aware geodesic distances. Our proposed approach utilizes a fully convolutional network and is data-driven. We drew inspiration from the filling-in process of the visual cortex, introducing lateral dependencies between neurons, multi-layer supervision, and emphasizing the importance of image contours in the learning process. Our method is robust and surpasses EpicFlow on competitive optical flow benchmarks using various matching algorithms. As a result, our approach achieves state-of-the-art performance on the Sintel and KITTI 2012 benchmarks.",1
"Smile is one of the key elements in identifying emotions and present state of mind of an individual. In this work, we propose a cluster of approaches to classify posed and spontaneous smiles using deep convolutional neural network (CNN) face features, local phase quantization (LPQ), dense optical flow and histogram of gradient (HOG). Eulerian Video Magnification (EVM) is used for micro-expression smile amplification along with three normalization procedures for distinguishing posed and spontaneous smiles. Although the deep CNN face model is trained with large number of face images, HOG features outperforms this model for overall face smile classification task. Using EVM to amplify micro-expressions did not have a significant impact on classification accuracy, while the normalizing facial features improved classification accuracy. Unlike many manual or semi-automatic methodologies, our approach aims to automatically classify all smiles into either `spontaneous' or `posed' categories, by using support vector machines (SVM). Experimental results on large UvA-NEMO smile database show promising results as compared to other relevant methods.",0
"The identification of emotions and current mental state of an individual can be determined to a large extent by their smile. This study proposes a range of techniques to differentiate between posed and spontaneous smiles using a deep convolutional neural network (CNN) face features, local phase quantization (LPQ), dense optical flow, and histogram of gradient (HOG). Additionally, Eulerian Video Magnification (EVM) is employed to amplify micro-expressions in smiles, and three normalization procedures are used to distinguish between posed and spontaneous smiles. Although the deep CNN face model is trained on a large number of face images, HOG features outperform this model in the overall face smile classification task. While the amplification of micro-expressions through EVM does not significantly impact classification accuracy, the normalization of facial features improves classification accuracy. Our approach aims to automatically classify all smiles into either 'spontaneous' or 'posed' categories using support vector machines (SVM), unlike many manual or semi-automatic methodologies. Promising results were obtained through experimental testing on a large UvA-NEMO smile database, in comparison to other relevant methods.",1
"Motion detection in video is important for a number of applications and fields. In video surveillance, motion detection is an essential accompaniment to activity recognition for early warning systems. Robotics also has much to gain from motion detection and segmentation, particularly in high speed motion tracking for tactile systems. There are a myriad of techniques for detecting and masking motion in an image. Successful systems have used Gaussian Models to discern background from foreground in an image (motion from static imagery). However, particularly in the case of a moving camera or frame of reference, it is necessary to compensate for the motion of the camera when attempting to discern objects moving in the foreground. For example, it is possible to estimate motion of the camera through optical flow methods or temporal differencing and then compensate for this motion in a background subtraction model. We selection a method by Yi et al. using Dual-Mode Single Gaussian Models which does just this. We implement the technique in Intel's Thread Building Blocks (TBB) and NVIDIA's CUDA libraries. We then compare parallelization improvements with a theoretical analysis of speedups based on the characteristics of our selected model and attributes of both TBB and CUDA. We make our implementation available to the public.",0
"Various applications and fields require motion detection in video. In video surveillance, motion detection is crucial for early warning systems, working alongside activity recognition. Robotics can also benefit from motion detection and segmentation, specifically for high-speed motion tracking in tactile systems. There are numerous techniques for detecting and masking motion in an image, with successful systems utilizing Gaussian Models to differentiate between foreground and background in an image. However, when dealing with a moving camera or frame of reference, it is necessary to account for camera motion when identifying moving objects in the foreground. This can be achieved through optical flow methods or temporal differencing to estimate camera motion and compensate for it in a background subtraction model. We chose to use Yi et al.'s Dual-Mode Single Gaussian Models to accomplish this. Our implementation of this technique is available to the public and was developed using Intel's Thread Building Blocks (TBB) and NVIDIA's CUDA libraries. We compared parallelization improvements with a theoretical analysis of speedups based on the model we selected and the attributes of both TBB and CUDA.",1
"We show that the matching problem that underlies optical flow requires multiple strategies, depending on the amount of image motion and other factors. We then study the implications of this observation on training a deep neural network for representing image patches in the context of descriptor based optical flow. We propose a metric learning method, which selects suitable negative samples based on the nature of the true match. This type of training produces a network that displays multiple strategies depending on the input and leads to state of the art results on the KITTI 2012 and KITTI 2015 optical flow benchmarks.",0
"Our findings reveal that the matching problem essential to optical flow necessitates various approaches, contingent on the degree of image motion and other elements. To this end, we examine the impact of this discovery on the development of a profound neural network that can represent image patches concerning descriptor-based optical flow. Our proposed metric learning approach chooses appropriate negative examples based on the genuine match's characteristics. This training technique yields a network that utilizes multiple strategies depending on the input and yields excellent results on the KITTI 2012 and KITTI 2015 optical flow benchmarks.",1
"We introduce SceneNet RGB-D, expanding the previous work of SceneNet to enable large scale photorealistic rendering of indoor scene trajectories. It provides pixel-perfect ground truth for scene understanding problems such as semantic segmentation, instance segmentation, and object detection, and also for geometric computer vision problems such as optical flow, depth estimation, camera pose estimation, and 3D reconstruction. Random sampling permits virtually unlimited scene configurations, and here we provide a set of 5M rendered RGB-D images from over 15K trajectories in synthetic layouts with random but physically simulated object poses. Each layout also has random lighting, camera trajectories, and textures. The scale of this dataset is well suited for pre-training data-driven computer vision techniques from scratch with RGB-D inputs, which previously has been limited by relatively small labelled datasets in NYUv2 and SUN RGB-D. It also provides a basis for investigating 3D scene labelling tasks by providing perfect camera poses and depth data as proxy for a SLAM system. We host the dataset at http://robotvault.bitbucket.io/scenenet-rgbd.html",0
"The SceneNet RGB-D has been introduced to extend the previous SceneNet work and allow for larger-scale photorealistic rendering of indoor scene trajectories. This dataset offers accurate ground truth for scene understanding problems, including semantic segmentation, instance segmentation, and object detection, as well as for geometric computer vision problems such as optical flow, depth estimation, camera pose estimation, and 3D reconstruction. By utilizing random sampling, an almost limitless variety of scene configurations is possible. The dataset contains 5 million rendered RGB-D images from more than 15,000 trajectories in synthetic layouts featuring physically simulated object poses, random lighting, camera trajectories, and textures. The dataset's size makes it ideal for pre-training data-driven computer vision techniques from scratch with RGB-D inputs, addressing the limitations of smaller labelled datasets, such as NYUv2 and SUN RGB-D. Additionally, the dataset serves as a basis for exploring 3D scene labelling tasks, offering ideal camera poses and depth data as a proxy for a SLAM system. The dataset is available at http://robotvault.bitbucket.io/scenenet-rgbd.html.",1
"In this paper we present a decomposition algorithm for computation of the spatial-temporal optical flow of a dynamic image sequence. We consider several applications, such as the extraction of temporal motion features and motion detection in dynamic sequences under varying illumination conditions, such as they appear for instance in psychological flickering experiments. For the numerical implementation we are solving an integro-differential equation by a fixed point iteration. For comparison purposes we use a standard time dependent optical flow algorithm, which in contrast to our method, constitutes in solving a spatial-temporal differential equation.",0
"Our paper outlines an algorithm for decomposing the spatial-temporal optical flow in a dynamic image sequence. We explore various applications, including the detection of motion and extraction of temporal motion features in sequences that experience changes in illumination, such as those found in psychological flickering experiments. To execute the numerical implementation, we utilize fixed point iteration to solve an integro-differential equation. We also compare our method to a standard time dependent optical flow algorithm, which differs from ours as it solves a spatial-temporal differential equation.",1
"This paper describes the development of a novel algorithm to tackle the problem of real-time video stabilization for unmanned aerial vehicles (UAVs). There are two main components in the algorithm: (1) By designing a suitable model for the global motion of UAV, the proposed algorithm avoids the necessity of estimating the most general motion model, projective transformation, and considers simpler motion models, such as rigid transformation and similarity transformation. (2) To achieve a high processing speed, optical-flow based tracking is employed in lieu of conventional tracking and matching methods used by state-of-the-art algorithms. These two new ideas resulted in a real-time stabilization algorithm, developed over two phases. Stage I considers processing the whole sequence of frames in the video while achieving an average processing speed of 50fps on several publicly available benchmark videos. Next, Stage II undertakes the task of real-time video stabilization using a multi-threading implementation of the algorithm designed in Stage I.",0
"The aim of this paper is to present a new algorithm that addresses the issue of real-time video stabilization for unmanned aerial vehicles (UAVs). The algorithm is composed of two key elements: firstly, a model for the global motion of UAV is designed, which avoids the need to estimate complex motion models such as projective transformation, and instead utilizes simpler motion models like rigid transformation and similarity transformation. Secondly, the algorithm employs optical-flow based tracking, which allows for faster processing speeds than traditional tracking and matching methods used by other algorithms. The algorithm was developed over two stages, with Stage I focusing on processing the entire sequence of frames in the video and achieving an average processing speed of 50fps on multiple benchmark videos. Stage II then utilized multi-threading to implement the algorithm in real-time video stabilization.",1
"Robust visual tracking is a challenging computer vision problem, with many real-world applications. Most existing approaches employ hand-crafted appearance features, such as HOG or Color Names. Recently, deep RGB features extracted from convolutional neural networks have been successfully applied for tracking. Despite their success, these features only capture appearance information. On the other hand, motion cues provide discriminative and complementary information that can improve tracking performance. Contrary to visual tracking, deep motion features have been successfully applied for action recognition and video classification tasks. Typically, the motion features are learned by training a CNN on optical flow images extracted from large amounts of labeled videos.   This paper presents an investigation of the impact of deep motion features in a tracking-by-detection framework. We further show that hand-crafted, deep RGB, and deep motion features contain complementary information. To the best of our knowledge, we are the first to propose fusing appearance information with deep motion features for visual tracking. Comprehensive experiments clearly suggest that our fusion approach with deep motion features outperforms standard methods relying on appearance information alone.",0
"Visual tracking is a complex issue in computer vision, but it has numerous practical applications. Many current techniques employ manually crafted visual features like HOG or Color Names. Recently, deep RGB features derived from convolutional neural networks have been used with success for tracking. However, these features only capture appearance information, whereas motion cues can provide additional and distinctive data that can enhance tracking performance. Deep motion features have been effectively utilized for action recognition and video classification tasks. Typically, these features are learned by training a CNN on optical flow images taken from labeled videos. This study examines the impact of deep motion features in a tracking-by-detection framework. The researchers found that hand-crafted, deep RGB, and deep motion features contain complementary information. This study is the first to propose combining appearance data with deep motion features for visual tracking. Comprehensive experiments show that the fusion approach with deep motion features outperforms standard methods that rely solely on appearance information.",1
"Micro-facial expressions are regarded as an important human behavioural event that can highlight emotional deception. Spotting these movements is difficult for humans and machines, however research into using computer vision to detect subtle facial expressions is growing in popularity. This paper proposes an individualised baseline micro-movement detection method using 3D Histogram of Oriented Gradients (3D HOG) temporal difference method. We define a face template consisting of 26 regions based on the Facial Action Coding System (FACS). We extract the temporal features of each region using 3D HOG. Then, we use Chi-square distance to find subtle facial motion in the local regions. Finally, an automatic peak detector is used to detect micro-movements above the newly proposed adaptive baseline threshold. The performance is validated on two FACS coded datasets: SAMM and CASME II. This objective method focuses on the movement of the 26 face regions. When comparing with the ground truth, the best result was an AUC of 0.7512 and 0.7261 on SAMM and CASME II, respectively. The results show that 3D HOG outperformed for micro-movement detection, compared to state-of-the-art feature representations: Local Binary Patterns in Three Orthogonal Planes and Histograms of Oriented Optical Flow.",0
"Detecting micro-facial expressions is a challenging task for both humans and machines as it can reveal emotional dishonesty. However, recent studies have explored the use of computer vision to identify subtle facial movements. This study proposes a personalized approach for detecting micro-movements using the 3D Histogram of Oriented Gradients (3D HOG) temporal difference method. The method involves creating a face template comprising 26 regions based on the Facial Action Coding System (FACS), extracting temporal features of each region using 3D HOG, using Chi-square distance to find subtle facial motion in local regions, and using an automatic peak detector to identify micro-movements above the adaptive baseline threshold. The effectiveness of the proposed method was tested on two FACS coded datasets: SAMM and CASME II. The results showed that the proposed method outperformed other state-of-the-art feature representations such as Local Binary Patterns in Three Orthogonal Planes and Histograms of Oriented Optical Flow, with an AUC of 0.7512 and 0.7261 on SAMM and CASME II, respectively. This objective method focuses on analyzing the movement of the 26 face regions.",1
"High dynamic range (HDR) image synthesis from multiple low dynamic range (LDR) exposures continues to be actively researched. The extension to HDR video synthesis is a topic of significant current interest due to potential cost benefits. For HDR video, a stiff practical challenge presents itself in the form of accurate correspondence estimation of objects between video frames. In particular, loss of data resulting from poor exposures and varying intensity make conventional optical flow methods highly inaccurate. We avoid exact correspondence estimation by proposing a statistical approach via maximum a posterior (MAP) estimation, and under appropriate statistical assumptions and choice of priors and models, we reduce it to an optimization problem of solving for the foreground and background of the target frame. We obtain the background through rank minimization and estimate the foreground via a novel multiscale adaptive kernel regression technique, which implicitly captures local structure and temporal motion by solving an unconstrained optimization problem. Extensive experimental results on both real and synthetic datasets demonstrate that our algorithm is more capable of delivering high-quality HDR videos than current state-of-the-art methods, under both subjective and objective assessments. Furthermore, a thorough complexity analysis reveals that our algorithm achieves better complexity-performance trade-off than conventional methods.",0
"The creation of high dynamic range (HDR) images from multiple low dynamic range (LDR) exposures is still being actively studied. Recently, there has been a significant interest in extending this technique to HDR video synthesis due to its potential cost savings. However, accurately estimating object correspondence between video frames presents a practical challenge due to data loss caused by poor exposures and varying intensity, making traditional optical flow methods highly inaccurate. To overcome this challenge, we propose a statistical approach using maximum a posterior (MAP) estimation that reduces the problem to an optimization task of solving for the foreground and background of the target frame. We accomplish this by using rank minimization to obtain the background and a novel multiscale adaptive kernel regression technique to estimate the foreground. Our approach captures local structure and temporal motion and produces high-quality HDR videos that outperform current state-of-the-art methods under both subjective and objective assessments. Additionally, our algorithm achieves a better complexity-performance trade-off than conventional methods, as demonstrated by a thorough complexity analysis.",1
