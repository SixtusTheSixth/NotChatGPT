"The dominant paradigm in spatiotemporal action detection is to classify actions using spatiotemporal features learned by 2D or 3D Convolutional Networks. We argue that several actions are characterized by their context, such as relevant objects and actors present in the video. To this end, we introduce an architecture based on self-attention and Graph Convolutional Networks in order to model contextual cues, such as actor-actor and actor-object interactions, to improve human action detection in video. We are interested in achieving this in a weakly-supervised setting, i.e. using as less annotations as possible in terms of action bounding boxes. Our model aids explainability by visualizing the learned context as an attention map, even for actions and objects unseen during training. We evaluate how well our model highlights the relevant context by introducing a quantitative metric based on recall of objects retrieved by attention maps. Our model relies on a 3D convolutional RGB stream, and does not require expensive optical flow computation. We evaluate our models on the DALY dataset, which consists of human-object interaction actions. Experimental results show that our contextualized approach outperforms a baseline action detection approach by more than 2 points in Video-mAP. Code is available at \url{https://github.com/micts/acgcn}",0
"In spatiotemporal action detection, the current approach is to use spatiotemporal features learned by 2D or 3D Convolutional Networks to classify actions. However, we believe that the context of certain actions, including relevant objects and actors in the video, should also be taken into account. To address this, we propose a new architecture that utilizes self-attention and Graph Convolutional Networks to model contextual cues, such as actor-actor and actor-object interactions, with the goal of improving human action detection in video. Our focus is on achieving this in a weakly-supervised setting, which means using as few annotations as possible for action bounding boxes. Our model provides explainability by generating an attention map that visualizes the learned context, even for actions and objects that were not present during training. We evaluate the effectiveness of our model using a quantitative metric based on recall of objects retrieved by attention maps. Our approach relies on a 3D convolutional RGB stream and does not require expensive optical flow computation. We test our model on the DALY dataset, which contains human-object interaction actions, and demonstrate that our contextualized approach outperforms a baseline action detection method by more than 2 points in Video-mAP. Code for our model is available at \url{https://github.com/micts/acgcn}.",1
"We present a learning-based approach for removing unwanted obstructions, such as window reflections, fence occlusions, or adherent raindrops, from a short sequence of images captured by a moving camera. Our method leverages motion differences between the background and obstructing elements to recover both layers. Specifically, we alternate between estimating dense optical flow fields of the two layers and reconstructing each layer from the flow-warped images via a deep convolutional neural network. This learning-based layer reconstruction module facilitates accommodating potential errors in the flow estimation and brittle assumptions, such as brightness consistency. We show that the proposed approach learned from synthetically generated data performs well to real images. Experimental results on numerous challenging scenarios of reflection and fence removal demonstrate the effectiveness of the proposed method.",0
"A technique for eliminating unwelcome obstructions from a brief series of images obtained by a mobile camera is introduced. Such obstructions may include window reflections, fence blockages, or raindrops that have adhered to the lens. Our approach relies on the disparities in motion between the background and the obstructing elements to restore both layers. We accomplish this by alternating between computing dense optical flow fields of the two layers and reconstructing each layer from flow-warped images using a deep convolutional neural network. This learning-based module for layer reconstruction helps to account for potential inaccuracies in flow estimation and fragile assumptions, such as brightness consistency. We demonstrate that the proposed strategy, which has been educated on artificially produced data, performs well on actual images. The efficacy of the proposed method is demonstrated by experimental findings on a variety of challenging reflection and fence removal situations.",1
"Highly complex deep learning models are increasingly integrated into modern cyber-physical systems (CPS), many of which have strict safety requirements. One problem arising from this is that deep learning lacks interpretability, operating as a black box. The reliability of deep learning is heavily impacted by how well the model training data represents runtime test data, especially when the input space dimension is high as natural images. In response, we propose a robust out-of-distribution (OOD) detection framework. Our approach detects unusual movements from driving video in real-time by combining classical optic flow operation with representation learning via variational autoencoder (VAE). We also design a method to locate OOD factors in images. Evaluation on a driving simulation data set shows that our approach is statistically more robust than related works.",0
"Modern cyber-physical systems (CPS) increasingly incorporate highly complex deep learning models, which can pose a challenge due to their lack of interpretability and strict safety requirements. The performance of such models heavily depends on how well the training data represents runtime test data, especially in high-dimensional input spaces like natural images. Therefore, we propose an out-of-distribution (OOD) detection framework that utilizes classical optic flow operation and representation learning via variational autoencoder (VAE) to detect unusual movements in driving video in real-time. We also introduce a method for locating OOD factors in images. Our evaluation on a driving simulation data set shows that our approach is statistically more robust than related works.",1
"Feature pyramids and iterative refinement have recently led to great progress in optical flow estimation. However, downsampling in feature pyramids can cause blending of foreground objects with the background, which will mislead subsequent decisions in the iterative processing. The results are missing details especially in the flow of thin and of small structures. We propose a novel Residual Feature Pyramid Module (RFPM) which retains important details in the feature map without changing the overall iterative refinement design of the optical flow estimation. RFPM incorporates a residual structure between multiple feature pyramids into a downsampling module that corrects the blending of objects across boundaries. We demonstrate how to integrate our module with two state-of-the-art iterative refinement architectures. Results show that our RFPM visibly reduces flow errors and improves state-of-art performance in the clean pass of Sintel, and is one of the top-performing methods in KITTI. According to the particular modular structure of RFPM, we introduce a special transfer learning approach that can dramatically decrease the training time compared to a typical full optical flow training schedule on multiple datasets.",0
"Recent advancements in optical flow estimation have been made possible by feature pyramids and iterative refinement. However, downsampling in feature pyramids can lead to the blending of foreground objects with the background, causing subsequent decisions in iterative processing to be misleading. This results in missing details, particularly in the flow of small and thin structures. To address this issue, we propose a Residual Feature Pyramid Module (RFPM) that retains critical details in the feature map while maintaining the overall iterative refinement design of optical flow estimation. RFPM introduces a residual structure between multiple feature pyramids within a downsampling module, which corrects object blending across boundaries. We demonstrate the integration of our module with two state-of-the-art iterative refinement architectures, showing visible reduction in flow errors and improved performance in the clean pass of Sintel, with RFPM being one of the top-performing methods in KITTI. Our particular modular structure also allows for a special transfer learning approach that significantly decreases training time compared to a full optical flow training schedule on multiple datasets.",1
"Learning deformable 3D objects from 2D images is an extremely ill-posed problem. Existing methods rely on explicit supervision to establish multi-view correspondences, such as template shape models and keypoint annotations, which restricts their applicability on objects ""in the wild"". In this paper, we propose to use monocular videos, which naturally provide correspondences across time, allowing us to learn 3D shapes of deformable object categories without explicit keypoints or template shapes. Specifically, we present DOVE, which learns to predict 3D canonical shape, deformation, viewpoint and texture from a single 2D image of a bird, given a bird video collection as well as automatically obtained silhouettes and optical flows as training data. Our method reconstructs temporally consistent 3D shape and deformation, which allows us to animate and re-render the bird from arbitrary viewpoints from a single image.",0
"The task of learning deformable 3D objects from 2D images is problematic due to its ill-posed nature. Current approaches rely on explicit supervision, such as template shape models and keypoint annotations, to establish multi-view correspondences, which limits their practicality for objects found in unpredictable environments. To address this, we suggest using monocular videos that provide natural correspondences across time, which enables us to learn the 3D shapes of deformable object categories without explicit keypoints or template shapes. To demonstrate this, we introduce DOVE, which predicts the 3D canonical shape, deformation, viewpoint, and texture of a bird from a single 2D image using a bird video collection and automatically obtained silhouettes and optical flows as training data. Our approach generates temporally consistent 3D shape and deformation, allowing us to animate and re-render the bird from any viewpoint using just one image.",1
"Deep Learning-based 2D/3D registration methods are highly robust but often lack the necessary registration accuracy for clinical application. A refinement step using the classical optimization-based 2D/3D registration method applied in combination with Deep Learning-based techniques can provide the required accuracy. However, it also increases the runtime. In this work, we propose a novel Deep Learning driven 2D/3D registration framework that can be used end-to-end for iterative registration tasks without relying on any further refinement step. We accomplish this by learning the update step of the 2D/3D registration framework using Point-to-Plane Correspondences. The update step is learned using iterative residual refinement-based optical flow estimation, in combination with the Point-to-Plane correspondence solver embedded as a known operator. Our proposed method achieves an average runtime of around 8s, a mean re-projection distance error of 0.60 $\pm$ 0.40 mm with a success ratio of 97 percent and a capture range of 60 mm. The combination of high registration accuracy, high robustness, and fast runtime makes our solution ideal for clinical applications.",0
"Although Deep Learning-based 2D/3D registration methods are generally robust, they tend to lack the registration accuracy required for clinical purposes. To address this limitation, a refinement step involving classical optimization-based 2D/3D registration methods is often added, but this can result in increased runtime. This study proposes a novel Deep Learning-driven 2D/3D registration framework that eliminates the need for further refinement steps. The proposed framework uses Point-to-Plane Correspondences to learn the update step of the 2D/3D registration process. This is achieved by combining iterative residual refinement-based optical flow estimation with the Point-to-Plane correspondence solver embedded as a known operator. The proposed method delivers an average runtime of approximately 8 seconds, a mean re-projection distance error of 0.60 $\pm$ 0.40 mm, a success ratio of 97 percent, and a capture range of 60 mm. The combination of high registration accuracy, robustness, and fast runtime makes the proposed solution suitable for clinical applications.",1
"Research on group activity recognition mostly leans on the standard two-stream approach (RGB and Optical Flow) as their input features. Few have explored explicit pose information, with none using it directly to reason about the persons interactions. In this paper, we leverage the skeleton information to learn the interactions between the individuals straight from it. With our proposed method GIRN, multiple relationship types are inferred from independent modules, that describe the relations between the body joints pair-by-pair. Additionally to the joints relations, we also experiment with the previously unexplored relationship between individuals and relevant objects (e.g. volleyball). The individuals distinct relations are then merged through an attention mechanism, that gives more importance to those individuals more relevant for distinguishing the group activity. We evaluate our method in the Volleyball dataset, obtaining competitive results to the state-of-the-art. Our experiments demonstrate the potential of skeleton-based approaches for modeling multi-person interactions.",0
"The majority of research on recognizing group activity relies on the standard two-stream approach, which employs RGB and Optical Flow as input features. While a few studies have examined explicit pose information, none have used it directly to understand interactions between people. This paper proposes a method called GIRN, which uses skeleton information to learn about the interactions between individuals. GIRN utilizes multiple modules to infer different types of relationships between body joints, and also examines the relationship between individuals and relevant objects, such as a volleyball. An attention mechanism is used to merge the distinct relationships between individuals, giving more weight to those that are most relevant for distinguishing the group activity. The proposed approach is evaluated using the Volleyball dataset, and achieves competitive results compared to the state-of-the-art. These experiments demonstrate that skeleton-based approaches have great potential for modeling interactions between multiple individuals.",1
"This presentation introduces a self-supervised learning approach to the synthesis of new video clips from old ones, with several new key elements for improved spatial resolution and realism: It conditions the synthesis process on contextual information for temporal continuity and ancillary information for fine control. The prediction model is doubly autoregressive, in the latent space of an autoencoder for forecasting, and in image space for updating contextual information, which is also used to enforce spatio-temporal consistency through a learnable optical flow module. Adversarial training of the autoencoder in the appearance and temporal domains is used to further improve the realism of its output. A quantizer inserted between the encoder and the transformer in charge of forecasting future frames in latent space (and its inverse inserted between the transformer and the decoder) adds even more flexibility by affording simple mechanisms for handling multimodal ancillary information for controlling the synthesis process (eg, a few sample frames, an audio track, a trajectory in image space) and taking into account the intrinsically uncertain nature of the future by allowing multiple predictions. Experiments with an implementation of the proposed approach give very good qualitative and quantitative results on multiple tasks and standard benchmarks.",0
"In this presentation, a novel approach to self-supervised learning is introduced for generating new video clips from old ones. This approach includes several new features to enhance spatial resolution and realism. By incorporating contextual information for temporal continuity and ancillary information for fine control, the synthesis process is conditioned. The prediction model is doubly autoregressive, operating in the latent space of an autoencoder for forecasting and in image space for updating contextual information. Additionally, a learnable optical flow module is used to enforce spatio-temporal consistency. Adversarial training is employed to further improve the realism of the autoencoder's output in both appearance and temporal domains. Inserting a quantizer between the encoder and transformer and its inverse between the transformer and decoder adds even more flexibility. This allows for simple handling of multimodal ancillary information to control the synthesis process and account for the uncertain nature of the future. The proposed approach is experimentally evaluated on multiple tasks and standard benchmarks, yielding excellent qualitative and quantitative results.",1
"To solve the issue of video dehazing, there are two main tasks to attain: how to align adjacent frames to the reference frame; how to restore the reference frame. Some papers adopt explicit approaches (e.g., the Markov random field, optical flow, deformable convolution, 3D convolution) to align neighboring frames with the reference frame in feature space or image space, they then use various restoration methods to achieve the final dehazing results. In this paper, we propose a progressive alignment and restoration method for video dehazing. The alignment process aligns consecutive neighboring frames stage by stage without using the optical flow estimation. The restoration process is not only implemented under the alignment process but also uses a refinement network to improve the dehazing performance of the whole network. The proposed networks include four fusion networks and one refinement network. To decrease the parameters of networks, three fusion networks in the first fusion stage share the same parameters. Extensive experiments demonstrate that the proposed video dehazing method achieves outstanding performance against the-state-of-art methods.",0
"There are two primary objectives to address when resolving the problem of video dehazing: aligning adjacent frames to a reference frame and restoring the reference frame. Certain studies use explicit techniques, such as Markov random field, optical flow, deformable convolution, and 3D convolution, to align neighboring frames with the reference frame in either feature space or image space. They then apply various restoration methods to produce the final dehazing outcomes. In this study, we introduce a gradual alignment and restoration method for video dehazing. The alignment process stages the alignment of consecutive neighboring frames without relying on optical flow estimation. Furthermore, the restoration process not only takes place under the alignment process but also employs a refinement network to boost the dehazing performance of the entire network. Our proposed networks consist of four fusion networks and one refinement network. To reduce network parameters, the first fusion stage's three fusion networks share the same parameters. Through extensive experimentation, we demonstrate that our proposed video dehazing method surpasses current state-of-the-art approaches.",1
"We propose a method for multi-object tracking and segmentation (MOTS) that does not require fine-tuning or per benchmark hyperparameter selection. The proposed method addresses particularly the data association problem. Indeed, the recently introduced HOTA metric, that has a better alignment with the human visual assessment by evenly balancing detections and associations quality, has shown that improvements are still needed for data association. After creating tracklets using instance segmentation and optical flow, the proposed method relies on a space-time memory network (STM) developed for one-shot video object segmentation to improve the association of tracklets with temporal gaps. To the best of our knowledge, our method, named MeNToS, is the first to use the STM network to track object masks for MOTS. We took the 4th place in the RobMOTS challenge. The project page is https://mehdimiah.com/mentos.html.",0
"Our proposed method for multi-object tracking and segmentation (MOTS) eliminates the need for fine-tuning or selecting hyperparameters per benchmark. Our approach specifically targets the data association issue, which is still a challenge despite the introduction of the HOTA metric that attempts to balance the quality of detections and associations. To tackle this problem, we use a space-time memory network (STM) developed for one-shot video object segmentation that improves the association of tracklets with temporal gaps. After creating tracklets using instance segmentation and optical flow, our method, called MeNToS, utilizes the STM network to track object masks for MOTS. To our knowledge, this is the first time that the STM network has been used for this purpose. Our performance in the RobMOTS challenge was fourth place, and more information about our project can be found at https://mehdimiah.com/mentos.html.",1
"We address the problem of text-guided video temporal grounding, which aims to identify the time interval of certain event based on a natural language description. Different from most existing methods that only consider RGB images as visual features, we propose a multi-modal framework to extract complementary information from videos. Specifically, we adopt RGB images for appearance, optical flow for motion, and depth maps for image structure. While RGB images provide abundant visual cues of certain event, the performance may be affected by background clutters. Therefore, we use optical flow to focus on large motion and depth maps to infer the scene configuration when the action is related to objects recognizable with their shapes. To integrate the three modalities more effectively and enable inter-modal learning, we design a dynamic fusion scheme with transformers to model the interactions between modalities. Furthermore, we apply intra-modal self-supervised learning to enhance feature representations across videos for each modality, which also facilitates multi-modal learning. We conduct extensive experiments on the Charades-STA and ActivityNet Captions datasets, and show that the proposed method performs favorably against state-of-the-art approaches.",0
"Our focus is on identifying the time interval of an event based on natural language descriptions, which is known as text-guided video temporal grounding. Most current methods only consider RGB images as visual features, however, we present a multi-modal framework that extracts complementary information from videos. Our approach involves using RGB images for appearance, optical flow for motion, and depth maps for image structure. Although RGB images offer significant visual cues, they can be affected by background clutters. To address this, we use optical flow to concentrate on large motion and depth maps to deduce the scene configuration when the action relates to objects recognizable by their shapes. Our dynamic fusion scheme with transformers enables inter-modal learning by modeling the interactions between modalities. Furthermore, we apply intra-modal self-supervised learning to enhance feature representations across videos for each modality, which also facilitates multi-modal learning. We evaluate our approach on the Charades-STA and ActivityNet Captions datasets and demonstrate that our method outperforms state-of-the-art approaches.",1
"Dense optical flow estimation is challenging when there are large displacements in a scene with heterogeneous motion dynamics, occlusion, and scene homogeneity. Traditional approaches to handle these challenges include hierarchical and multiresolution processing methods. Learning-based optical flow methods typically use a multiresolution approach with image warping when a broad range of flow velocities and heterogeneous motion is present. Accuracy of such coarse-to-fine methods is affected by the ghosting artifacts when images are warped across multiple resolutions and by the vanishing problem in smaller scene extents with higher motion contrast. Previously, we devised strategies for building compact dense prediction networks guided by the effective receptive field (ERF) characteristics of the network (DDCNet). The DDCNet design was intentionally simple and compact allowing it to be used as a building block for designing more complex yet compact networks. In this work, we extend the DDCNet strategies to handle heterogeneous motion dynamics by cascading DDCNet based sub-nets with decreasing extents of their ERF. Our DDCNet with multiresolution capability (DDCNet-Multires) is compact without any specialized network layers. We evaluate the performance of the DDCNet-Multires network using standard optical flow benchmark datasets. Our experiments demonstrate that DDCNet-Multires improves over the DDCNet-B0 and -B1 and provides optical flow estimates with accuracy comparable to similar lightweight learning-based methods.",0
"When a scene has varying motion dynamics, occlusion, and homogeneity, estimating dense optical flow can be difficult. Traditional methods use hierarchical and multiresolution processing to address these challenges. Learning-based approaches typically use multiresolution methods with image warping to account for different flow velocities and motion patterns. However, these methods can result in ghosting artifacts and vanishing problems, affecting their accuracy. Previously, we developed the DDCNet, a simple and compact network for dense prediction. In this study, we extend the DDCNet to handle heterogeneous motion by cascading sub-nets with decreasing effective receptive field (ERF) sizes. This new DDCNet-Multires network is compact, without the need for specialized layers, and achieves a performance comparable to other lightweight learning-based methods on standard optical flow benchmarks.",1
"This technical report presents our solution to the HACS Temporal Action Localization Challenge 2021, Weakly-Supervised Learning Track. The goal of weakly-supervised temporal action localization is to temporally locate and classify action of interest in untrimmed videos given only video-level labels. We adopt the two-stream consensus network (TSCN) as the main framework in this challenge. The TSCN consists of a two-stream base model training procedure and a pseudo ground truth learning procedure. The base model training encourages the model to predict reliable predictions based on single modality (i.e., RGB or optical flow), based on the fusion of which a pseudo ground truth is generated and in turn used as supervision to train the base models. On the HACS v1.1.1 dataset, without fine-tuning the feature-extraction I3D models, our method achieves 22.20% on the validation set and 21.68% on the testing set in terms of average mAP. Our solution ranked the 2rd in this challenge, and we hope our method can serve as a baseline for future academic research.",0
"Our technical report details our approach to the HACS Temporal Action Localization Challenge 2021, specifically the Weakly-Supervised Learning Track. The objective of this challenge is to locate and classify actions of interest in untrimmed videos using only video-level labels. To achieve this, we utilized the two-stream consensus network (TSCN) as our primary framework. The TSCN comprises a two-stream base model training and a pseudo ground truth learning procedure. The base model training encourages the model to make accurate predictions based on a single modality, and a pseudo ground truth is generated from the fusion of these predictions to train the base models. Our method achieved an average mAP of 22.20% on the validation set and 21.68% on the testing set of the HACS v1.1.1 dataset without fine-tuning the feature-extraction I3D models. Our ranking in this challenge was 2nd, and we hope our approach can serve as a foundation for future academic studies.",1
"Detection of moving objects is a very important task in autonomous driving systems. After the perception phase, motion planning is typically performed in Bird's Eye View (BEV) space. This would require projection of objects detected on the image plane to top view BEV plane. Such a projection is prone to errors due to lack of depth information and noisy mapping in far away areas. CNNs can leverage the global context in the scene to project better. In this work, we explore end-to-end Moving Object Detection (MOD) on the BEV map directly using monocular images as input. To the best of our knowledge, such a dataset does not exist and we create an extended KITTI-raw dataset consisting of 12.9k images with annotations of moving object masks in BEV space for five classes. The dataset is intended to be used for class agnostic motion cue based object detection and classes are provided as meta-data for better tuning. We design and implement a two-stream RGB and optical flow fusion architecture which outputs motion segmentation directly in BEV space. We compare it with inverse perspective mapping of state-of-the-art motion segmentation predictions on the image plane. We observe a significant improvement of 13% in mIoU using the simple baseline implementation. This demonstrates the ability to directly learn motion segmentation output in BEV space. Qualitative results of our baseline and the dataset annotations can be found in https://sites.google.com/view/bev-modnet.",0
"The ability to detect moving objects is crucial for autonomous driving systems. Typically, motion planning is performed in Bird's Eye View (BEV) space after the perception phase. However, projecting objects detected on the image plane to the top view BEV plane can lead to errors due to lack of depth information and noisy mapping in distant areas. CNNs can improve this by utilizing the global context in the scene. This study explores end-to-end Moving Object Detection (MOD) on the BEV map using monocular images as input. The authors create an extended KITTI-raw dataset with annotations of moving object masks in BEV space for five classes. The dataset is intended for class agnostic motion cue based object detection and provides classes as meta-data for better tuning. The authors develop a two-stream RGB and optical flow fusion architecture that outputs motion segmentation directly in BEV space. They compare it with inverse perspective mapping of state-of-the-art motion segmentation predictions on the image plane and observe a significant improvement of 13% in mIoU using the simple baseline implementation. This demonstrates the ability to directly learn motion segmentation output in BEV space. The baseline and dataset annotations can be found in https://sites.google.com/view/bev-modnet.",1
"Dense pixel matching problems such as optical flow and disparity estimation are among the most challenging tasks in computer vision. Recently, several deep learning methods designed for these problems have been successful. A sufficiently larger effective receptive field (ERF) and a higher resolution of spatial features within a network are essential for providing higher-resolution dense estimates. In this work, we present a systemic approach to design network architectures that can provide a larger receptive field while maintaining a higher spatial feature resolution. To achieve a larger ERF, we utilized dilated convolutional layers. By aggressively increasing dilation rates in the deeper layers, we were able to achieve a sufficiently larger ERF with a significantly fewer number of trainable parameters. We used optical flow estimation problem as the primary benchmark to illustrate our network design strategy. The benchmark results (Sintel, KITTI, and Middlebury) indicate that our compact networks can achieve comparable performance in the class of lightweight networks.",0
"Computer vision faces significant challenges with dense pixel matching tasks, such as optical flow and disparity estimation. However, recent deep learning methods have shown success in addressing these challenges. To provide higher-resolution dense estimates, a network must have a larger effective receptive field (ERF) and maintain a higher spatial feature resolution. Our study presents a comprehensive approach to designing network architectures that meet these requirements. Our approach utilizes dilated convolutional layers to achieve a larger ERF by aggressively increasing dilation rates in deeper layers. This allows us to achieve a larger ERF with significantly fewer trainable parameters. We demonstrate the effectiveness of our network design strategy using optical flow estimation as the primary benchmark. Our benchmark results (Sintel, KITTI, and Middlebury) show that our compact networks can achieve comparable performance in the lightweight networks class.",1
"State-of-the-art temporal action detectors to date are based on two-stream input including RGB frames and optical flow. Although combining RGB frames and optical flow boosts performance significantly, optical flow is a hand-designed representation which not only requires heavy computation, but also makes it methodologically unsatisfactory that two-stream methods are often not learned end-to-end jointly with the flow. In this paper, we argue that optical flow is dispensable in high-accuracy temporal action detection and image level data augmentation (ILDA) is the key solution to avoid performance degradation when optical flow is removed. To evaluate the effectiveness of ILDA, we design a simple yet efficient one-stage temporal action detector based on single RGB stream named DaoTAD. Our results show that when trained with ILDA, DaoTAD has comparable accuracy with all existing state-of-the-art two-stream detectors while surpassing the inference speed of previous methods by a large margin and the inference speed is astounding 6668 fps on GeForce GTX 1080 Ti. Code is available at \url{https://github.com/Media-Smart/vedatad}.",0
"The current advanced temporal action detectors rely on two-stream input, which includes RGB frames and optical flow. While the combination of these two inputs significantly enhances performance, optical flow is a representation that is manually designed, requires significant computation, and does not enable two-stream methods to be learned end-to-end jointly with the flow, making it methodologically unsatisfactory. This paper contends that high-accuracy temporal action detection does not require optical flow, and instead advocates for image level data augmentation (ILDA) as a key solution to prevent performance degradation when optical flow is removed. To test the effectiveness of ILDA, the authors designed a simple yet effective one-stage temporal action detector based on a single RGB stream, called DaoTAD. The results demonstrate that when trained with ILDA, DaoTAD is as accurate as all existing state-of-the-art two-stream detectors, while significantly outperforming previous methods in terms of inference speed, with an astounding 6668 fps on GeForce GTX 1080 Ti. The code is available at \url{https://github.com/Media-Smart/vedatad}.",1
"Optical flow estimation is a fundamental problem of computer vision and has many applications in the fields of robot learning and autonomous driving. This paper reveals novel geometric laws of optical flow based on the insight and detailed definition of non-occlusion. Then, two novel loss functions are proposed for the unsupervised learning of optical flow based on the geometric laws of non-occlusion. Specifically, after the occlusion part of the images are masked, the flowing process of pixels is carefully considered and geometric constraints are conducted based on the geometric laws of optical flow. First, neighboring pixels in the first frame will not intersect during the pixel displacement to the second frame. Secondly, when the cluster containing adjacent four pixels in the first frame moves to the second frame, no other pixels will flow into the quadrilateral formed by them. According to the two geometrical constraints, the optical flow non-intersection loss and the optical flow non-blocking loss in the non-occlusion regions are proposed. Two loss functions punish the irregular and inexact optical flows in the non-occlusion regions. The experiments on datasets demonstrated that the proposed unsupervised losses of optical flow based on the geometric laws in non-occlusion regions make the estimated optical flow more refined in detail, and improve the performance of unsupervised learning of optical flow. In addition, the experiments training on synthetic data and evaluating on real data show that the generalization ability of optical flow network is improved by our proposed unsupervised approach.",0
"The estimation of optical flow is a crucial task in computer vision, with various applications in fields such as autonomous driving and robot learning. This study presents original geometric principles of optical flow, derived from a comprehensive definition of non-occlusion. The research then proposes two innovative loss functions for unsupervised optical flow learning, based on the geometric laws of non-occlusion. The study carefully considers the flow of pixels after masking the occlusion part of the images and applies geometric constraints based on the laws of optical flow. The first constraint ensures that neighboring pixels in the first frame do not intersect when they move to the second frame. The second constraint states that no other pixels will flow into the quadrilateral formed by the cluster containing adjacent four pixels in the first frame when it moves to the second frame. The proposed optical flow non-intersection loss and optical flow non-blocking loss punish irregular and inexact optical flows in the non-occlusion regions. The experiments on various datasets illustrate that the proposed unsupervised losses of optical flow based on geometric laws in non-occlusion regions refine the estimated optical flow and enhance the performance of unsupervised learning of optical flow. Moreover, the experiments training on synthetic data and evaluating on real data demonstrate that the proposed unsupervised approach improves the generalization ability of the optical flow network.",1
"Different types of spectroscopies, such as X-ray absorption near edge structure (XANES) and Raman spectroscopy, play a very important role in analyzing the characteristics of different materials. In scientific literature, XANES/Raman data are usually plotted in line graphs which is a visually appropriate way to represent the information when the end-user is a human reader. However, such graphs are not conducive to direct programmatic analysis due to the lack of automatic tools. In this paper, we develop a plot digitizer, named Plot2Spectra, to extract data points from spectroscopy graph images in an automatic fashion, which makes it possible for large scale data acquisition and analysis. Specifically, the plot digitizer is a two-stage framework. In the first axis alignment stage, we adopt an anchor-free detector to detect the plot region and then refine the detected bounding boxes with an edge-based constraint to locate the position of two axes. We also apply scene text detector to extract and interpret all tick information below the x-axis. In the second plot data extraction stage, we first employ semantic segmentation to separate pixels belonging to plot lines from the background, and from there, incorporate optical flow constraints to the plot line pixels to assign them to the appropriate line (data instance) they encode. Extensive experiments are conducted to validate the effectiveness of the proposed plot digitizer, which shows that such a tool could help accelerate the discovery and machine learning of materials properties.",0
"Various spectroscopic techniques, including X-ray absorption near edge structure (XANES) and Raman spectroscopy, are crucial in studying the properties of diverse materials. While line graphs are commonly used to represent XANES/Raman data in scientific literature for human readers, they are not ideal for automated analysis. Therefore, we present Plot2Spectra, a plot digitizer that automatically extracts data from spectroscopy graphs, enabling large-scale data acquisition and analysis. The two-stage framework involves axis alignment, where an anchor-free detector and edge-based constraint locate axis positions, and plot data extraction, where semantic segmentation and optical flow constraints assign pixels to corresponding data instances. Extensive experiments confirm the success of Plot2Spectra, which can facilitate the discovery and machine learning of materials properties.",1
"This article presents a novel approach to incorporate visual cues from video-data from a wide-angle stereo camera system mounted at an urban intersection into the forecast of cyclist trajectories. We extract features from image and optical flow (OF) sequences using 3D convolutional neural networks (3D-ConvNet) and combine them with features extracted from the cyclist's past trajectory to forecast future cyclist positions. By the use of additional information, we are able to improve positional accuracy by about 7.5 % for our test dataset and by up to 22 % for specific motion types compared to a method solely based on past trajectories. Furthermore, we compare the use of image sequences to the use of OF sequences as additional information, showing that OF alone leads to significant improvements in positional accuracy. By training and testing our methods using a real-world dataset recorded at a heavily frequented public intersection and evaluating the methods' runtimes, we demonstrate the applicability in real traffic scenarios. Our code and parts of our dataset are made publicly available.",0
"In this article, a new method is presented for predicting the trajectories of cyclists using visual cues obtained from video data captured by a wide-angle stereo camera system installed at an urban intersection. The approach involves extracting features from image and optical flow sequences using 3D convolutional neural networks (3D-ConvNet) and combining them with the cyclist's past trajectory to forecast future positions. By incorporating additional information, the accuracy of the predictions is improved by approximately 7.5% for the test dataset and up to 22% for specific motion types compared to a method that relies solely on past trajectories. The study also compares the effectiveness of using image sequences versus optical flow sequences as additional information, finding that optical flow alone leads to significant improvements in positional accuracy. The method is evaluated using a real-world dataset recorded at a heavily frequented public intersection, and the runtimes are shown to be applicable for use in real traffic scenarios. The code and some of the dataset are available to the public.",1
"Video frame interpolation is the task of creating an interframe between two adjacent frames along the time axis. So, instead of simply averaging two adjacent frames to create an intermediate image, this operation should maintain semantic continuity with the adjacent frames. Most conventional methods use optical flow, and various tools such as occlusion handling and object smoothing are indispensable. Since the use of these various tools leads to complex problems, we tried to tackle the video interframe generation problem without using problematic optical flow . To enable this , we have tried to use a deep neural network with an invertible structure, and developed an U-Net based Generative Flow which is a modified normalizing flow. In addition, we propose a learning method with a new consistency loss in the latent space to maintain semantic temporal consistency between frames. The resolution of the generated image is guaranteed to be identical to that of the original images by using an invertible network. Furthermore, as it is not a random image like the ones by generative models, our network guarantees stable outputs without flicker. Through experiments, we \sam {confirmed the feasibility of the proposed algorithm and would like to suggest the U-Net based Generative Flow as a new possibility for baseline in video frame interpolation. This paper is meaningful in that it is the world's first attempt to use invertible networks instead of optical flows for video interpolation.",0
"The task of video frame interpolation involves creating an intermediate frame between two adjacent frames along the time axis while maintaining semantic continuity with the surrounding frames. Although conventional methods use optical flow and various tools such as occlusion handling and object smoothing, these lead to complex problems. To address this issue, we developed a new approach using a deep neural network with an invertible structure called U-Net based Generative Flow. We also proposed a learning method with a new consistency loss in the latent space to ensure semantic temporal consistency between frames. The invertible network guarantees that the resolution of the generated image is identical to that of the original images, and stable outputs without flicker are ensured. Our experiments have shown that our proposed algorithm is feasible and we suggest it as a new baseline for video frame interpolation. This paper is significant as it is the first attempt in the world to use invertible networks instead of optical flows for video interpolation.",1
"Multiple human tracking is a fundamental problem for scene understanding. Although both accuracy and speed are required in real-world applications, recent tracking methods based on deep learning have focused on accuracy and require substantial running time. This study aims to improve running speed by performing human detection at a certain frame interval because it accounts for most of the running time. The question is how to maintain accuracy while skipping human detection. In this paper, we propose a method that complements the detection results with optical flow, based on the fact that someone's appearance does not change much between adjacent frames. To maintain the tracking accuracy, we introduce robust interest point selection within human regions and a tracking termination metric calculated by the distribution of the interest points. On the MOT20 dataset in the MOTChallenge, the proposed SDOF-Tracker achieved the best performance in terms of the total running speed while maintaining the MOTA metric. Our code is available at https://anonymous.4open.science/r/sdof-tracker-75AE.",0
"The problem of tracking multiple humans is crucial for understanding scenes. However, current deep learning tracking methods prioritize accuracy over speed, which is essential for real-world applications. To address this issue, this study proposes a method that performs human detection at specific frame intervals to improve running speed. The challenge is to maintain accuracy while skipping human detection. To overcome this challenge, the proposed approach combines detection results with optical flow, which is based on the consistency of a person's appearance in adjacent frames. Additionally, robust interest point selection within human regions and a tracking termination metric calculated by the distribution of interest points are introduced to maintain tracking accuracy. The SDOF-Tracker achieved the best performance in terms of running speed and the MOTA metric on the MOT20 dataset in the MOTChallenge. The code for this approach is available at https://anonymous.4open.science/r/sdof-tracker-75AE.",1
"Moving target detection plays an important role in computer vision. However, traditional algorithms such as frame difference and optical flow usually suffer from low accuracy or heavy computation. Recent algorithms such as deep learning-based convolutional neural networks have achieved high accuracy and real-time performance, but they usually need to know the classes of targets in advance, which limits the practical applications. Therefore, we proposed a model free moving target detection algorithm. This algorithm extracts the moving area through the difference of image features. Then, the color and location probability map of the moving area will be calculated through maximum a posteriori probability. And the target probability map can be obtained through the dot multiply between the two maps. Finally, the optimal moving target area can be solved by stochastic gradient descent on the target probability map. Results show that the proposed algorithm achieves the highest accuracy compared with state-of-the-art algorithms, without needing to know the classes of targets. Furthermore, as the existing datasets are not suitable for moving target detection, we proposed a method for producing evaluation dataset. Besides, we also proved the proposed algorithm can be used to assist target tracking.",0
"Computer vision heavily relies on detecting moving targets. However, conventional methods like frame difference and optical flow often yield low accuracy or require extensive computation. While recent deep learning-based convolutional neural networks have successfully achieved high accuracy and real-time performance, they are limited by their requirement for prior knowledge of target classes, thus limiting practical applications. Therefore, we have presented a model-free moving target detection algorithm that utilizes image feature differences to extract moving areas. Subsequently, a color and location probability map is calculated via maximum a posteriori probability. By multiplying the two maps, the target probability map is obtained. Finally, stochastic gradient descent is employed to derive the optimal moving target area from the target probability map. The proposed algorithm outperforms state-of-the-art algorithms without requiring target class identification. Additionally, we have developed an evaluation dataset and demonstrated that the proposed algorithm is useful in target tracking.",1
"Moving Object Detection (MOD) is a crucial task for the Autonomous Driving pipeline. MOD is usually handled via 2-stream convolutional architectures that incorporates both appearance and motion cues, without considering the inter-relations between the spatial or motion features. In this paper, we tackle this problem through multi-head attention mechanisms, both across the spatial and motion streams. We propose MODETR; a Moving Object DEtection TRansformer network, comprised of multi-stream transformer encoders for both spatial and motion modalities, and an object transformer decoder that produces the moving objects bounding boxes using set predictions. The whole architecture is trained end-to-end using bi-partite loss. Several methods of incorporating motion cues with the Transformer model are explored, including two-stream RGB and Optical Flow (OF) methods, and multi-stream architectures that take advantage of sequence information. To incorporate the temporal information, we propose a new Temporal Positional Encoding (TPE) approach to extend the Spatial Positional Encoding(SPE) in DETR. We explore two architectural choices for that, balancing between speed and time. To evaluate the our network, we perform the MOD task on the KITTI MOD [6] data set. Results show significant 5% mAP of the Transformer network for MOD over the state-of-the art methods. Moreover, the proposed TPE encoding provides 10% mAP improvement over the SPE baseline.",0
"Autonomous Driving pipeline relies heavily on Moving Object Detection (MOD). Typically, MOD is tackled using 2-stream convolutional architectures that incorporate appearance and motion cues, but neglect the inter-connections between spatial or motion features. In this study, we address this issue by employing multi-head attention mechanisms across both the spatial and motion streams. Our proposed solution, MODETR, is a Moving Object Detection Transformer network. It comprises multi-stream transformer encoders for both spatial and motion modalities, and an object transformer decoder that predicts moving object bounding boxes using set predictions. The entire architecture is trained end-to-end using bi-partite loss. We explore various methods of integrating motion cues with the Transformer model, including two-stream RGB and Optical Flow (OF) methods, and multi-stream architectures that leverage sequence information. To incorporate temporal information, we suggest a new Temporal Positional Encoding (TPE) approach, in addition to the Spatial Positional Encoding (SPE) in DETR. We examine two architectural options that strike a balance between speed and time. To evaluate our network, we perform the MOD task on the KITTI MOD [6] dataset. The results show a significant 5% mAP of the Transformer network for MOD over state-of-the-art techniques. Furthermore, the proposed TPE encoding results in a 10% mAP improvement over the SPE baseline.",1
"Transferring image-based object detectors to the domain of video remains challenging under resource constraints. Previous efforts utilised optical flow to allow unchanged features to be propagated, however, the overhead is considerable when working with very slowly changing scenes from applications such as surveillance. In this paper, we propose temporal early exits to reduce the computational complexity of per-frame video object detection. Multiple temporal early exit modules with low computational overhead are inserted at early layers of the backbone network to identify the semantic differences between consecutive frames. Full computation is only required if the frame is identified as having a semantic change to previous frames; otherwise, detection results from previous frames are reused. Experiments on CDnet show that our method significantly reduces the computational complexity and execution of per-frame video object detection up to $34 \times$ compared to existing methods with an acceptable reduction of 2.2\% in mAP.",0
"The task of transferring image-based object detectors to the video domain poses a challenge when faced with limited resources. Previous methods have employed optical flow to propagate unchanged features. However, this approach has proven to be impractical for slowly changing scenes, such as those seen in surveillance applications, due to the high overhead involved. To address this issue, this study proposes the use of temporal early exits to decrease the computational complexity of per-frame video object detection. This involves the insertion of multiple temporal early exit modules with low computational overhead at early layers of the backbone network. These modules identify semantic differences between consecutive frames, and full computation is only required if a frame is found to have a semantic change from previous frames. Otherwise, detection results from earlier frames are reused. Experiments conducted on CDnet demonstrate that this method significantly reduces the computational complexity and execution of per-frame video object detection by up to $34 \times$ in comparison to existing techniques, with an acceptable reduction of 2.2\% in mAP.",1
"Instance-level contrastive learning techniques, which rely on data augmentation and a contrastive loss function, have found great success in the domain of visual representation learning. They are not suitable for exploiting the rich dynamical structure of video however, as operations are done on many augmented instances. In this paper we propose ""Video Cross-Stream Prototypical Contrasting"", a novel method which predicts consistent prototype assignments from both RGB and optical flow views, operating on sets of samples. Specifically, we alternate the optimization process; while optimizing one of the streams, all views are mapped to one set of stream prototype vectors. Each of the assignments is predicted with all views except the one matching the prediction, pushing representations closer to their assigned prototypes. As a result, more efficient video embeddings with ingrained motion information are learned, without the explicit need for optical flow computation during inference. We obtain state-of-the-art results on nearest neighbour video retrieval and action recognition, outperforming previous best by +3.2% on UCF101 using the S3D backbone (90.5% Top-1 acc), and by +7.2% on UCF101 and +15.1% on HMDB51 using the R(2+1)D backbone.",0
"In the field of visual representation learning, instance-level contrastive learning techniques have been successful by utilizing data augmentation and a contrastive loss function. However, due to the fact that these operations are performed on numerous augmented instances, they are not ideal for exploiting the complex dynamic structure of video. In our research, we introduce a new method called ""Video Cross-Stream Prototypical Contrasting"", which predicts consistent prototype assignments from both RGB and optical flow views, working on sets of samples. We alternate the optimization process and map all views to one set of stream prototype vectors while optimizing one of the streams. Each of the assignments is predicted with all views except the one that corresponds to the prediction, resulting in more efficient video embeddings with incorporated motion information, eliminating the need for optical flow computation during inference. Our method outperforms previous approaches in nearest neighbour video retrieval and action recognition, achieving state-of-the-art results on UCF101 and HMDB51 using the S3D and R(2+1)D backbones.",1
"Forecasting the formation and development of clouds is a central element of modern weather forecasting systems. Incorrect clouds forecasts can lead to major uncertainty in the overall accuracy of weather forecasts due to their intrinsic role in the Earth's climate system. Few studies have tackled this challenging problem from a machine learning point-of-view due to a shortage of high-resolution datasets with many historical observations globally. In this paper, we present a novel satellite-based dataset called ``CloudCast''. It consists of 70,080 images with 10 different cloud types for multiple layers of the atmosphere annotated on a pixel level. The spatial resolution of the dataset is 928 x 1530 pixels (3x3 km per pixel) with 15-min intervals between frames for the period 2017-01-01 to 2018-12-31. All frames are centered and projected over Europe. To supplement the dataset, we conduct an evaluation study with current state-of-the-art video prediction methods such as convolutional long short-term memory networks, generative adversarial networks, and optical flow-based extrapolation methods. As the evaluation of video prediction is difficult in practice, we aim for a thorough evaluation in the spatial and temporal domain. Our benchmark models show promising results but with ample room for improvement. This is the first publicly available global-scale dataset with high-resolution cloud types on a high temporal granularity to the authors' best knowledge.",0
"Modern weather forecasting systems rely heavily on accurate predictions of cloud formation and development, as incorrect forecasts can result in significant uncertainty in overall weather predictions due to their crucial role in the Earth's climate system. However, few machine learning studies have tackled this challenging problem, primarily due to a lack of high-resolution datasets with a substantial amount of historical observations worldwide. In this paper, we introduce a new satellite-based dataset called ""CloudCast,"" which includes 70,080 images featuring ten distinct cloud types across multiple atmospheric layers annotated at the pixel level. The dataset boasts a spatial resolution of 928 x 1530 pixels (3x3 km per pixel), with frames taken at 15-minute intervals between January 1, 2017, and December 31, 2018. All frames are centered and projected over Europe. To supplement the dataset, we evaluate current state-of-the-art video prediction methods, including convolutional long short-term memory networks, generative adversarial networks, and optical flow-based extrapolation methods. Since evaluating video prediction is challenging, we aim to conduct a thorough evaluation in the spatial and temporal domains. Our benchmark models show promising results, but there is ample room for improvement. This is the first publicly available high-resolution global dataset with cloud types annotated at a high temporal granularity to our knowledge.",1
"State-of-the-art frame interpolation methods generate intermediate frames by inferring object motions in the image from consecutive key-frames. In the absence of additional information, first-order approximations, i.e. optical flow, must be used, but this choice restricts the types of motions that can be modeled, leading to errors in highly dynamic scenarios. Event cameras are novel sensors that address this limitation by providing auxiliary visual information in the blind-time between frames. They asynchronously measure per-pixel brightness changes and do this with high temporal resolution and low latency. Event-based frame interpolation methods typically adopt a synthesis-based approach, where predicted frame residuals are directly applied to the key-frames. However, while these approaches can capture non-linear motions they suffer from ghosting and perform poorly in low-texture regions with few events. Thus, synthesis-based and flow-based approaches are complementary. In this work, we introduce Time Lens, a novel indicates equal contribution method that leverages the advantages of both. We extensively evaluate our method on three synthetic and two real benchmarks where we show an up to 5.21 dB improvement in terms of PSNR over state-of-the-art frame-based and event-based methods. Finally, we release a new large-scale dataset in highly dynamic scenarios, aimed at pushing the limits of existing methods.",0
"Advanced techniques for frame interpolation involve creating intermediate frames by deducing object movements in an image from consecutive key-frames. If additional information is not available, first-order approximations, such as optical flow, must be used. However, this choice restricts the types of movements that can be modeled, leading to inaccuracies in highly dynamic scenarios. Event cameras are a new type of sensor that overcomes this limitation by providing auxiliary visual data during blind-time between frames. They record per-pixel brightness changes with high temporal resolution and low latency. Event-based frame interpolation techniques typically use a synthesis-based approach, where predicted residuals are directly applied to the key-frames. Nevertheless, these approaches can lead to ghosting and perform poorly in low-texture regions with few events, while flow-based approaches can capture nonlinear motions. As a result, synthesis-based and flow-based approaches are complementary. This study introduces Time Lens, a new approach that combines the strengths of both methods. The proposed method is extensively evaluated on three synthetic and two real benchmarks, demonstrating a maximum improvement of 5.21 dB in terms of PSNR compared to the current state-of-the-art frame-based and event-based methods. Lastly, a new large-scale dataset in highly dynamic scenarios is released to push the boundaries of existing techniques.",1
"Despite the continued successes of computationally efficient deep neural network architectures for video object detection, performance continually arrives at the great trilemma of speed versus accuracy versus computational resources (pick two). Current attempts to exploit temporal information in video data to overcome this trilemma are bottlenecked by the state-of-the-art in object detection models. We present, a technique which performs video object detection through the use of off-the-shelf object detectors alongside existing optical flow based motion estimation techniques in parallel. Through a set of experiments on the benchmark MOT20 dataset, we demonstrate that our approach significantly reduces the baseline latency of any given object detector without sacrificing any accuracy. Further latency reduction, up to 25x lower than the original latency, can be achieved with minimal accuracy loss. MOVEX enables low latency video object detection on common CPU based systems, thus allowing for high performance video object detection beyond the domain of GPU computing. The code is available at https://github.com/juliantrue/movex.",0
"Video object detection using computationally efficient deep neural network architectures often faces the challenge of balancing speed, accuracy, and computational resources. Despite attempts to leverage temporal information in video data to overcome this challenge, the current state-of-the-art in object detection models is a bottleneck. Our approach utilizes off-the-shelf object detectors and optical flow based motion estimation techniques in parallel to perform video object detection. Our experiments on the MOT20 dataset demonstrate that our technique significantly reduces the baseline latency of any given object detector without sacrificing accuracy. Additionally, minimal accuracy loss can achieve further latency reduction, up to 25 times lower than the original latency. With MOVEX, high-performance video object detection can be achieved on CPU-based systems, extending beyond the domain of GPU computing. The code is available at https://github.com/juliantrue/movex.",1
"Video super-resolution (VSR), with the aim to restore a high-resolution video from its corresponding low-resolution version, is a spatial-temporal sequence prediction problem. Recently, Transformer has been gaining popularity due to its parallel computing ability for sequence-to-sequence modeling. Thus, it seems to be straightforward to apply the vision Transformer to solve VSR. However, the typical block design of Transformer with a fully connected self-attention layer and a token-wise feed-forward layer does not fit well for VSR due to the following two reasons. First, the fully connected self-attention layer neglects to exploit the data locality because this layer relies on linear layers to compute attention maps. Second, the token-wise feed-forward layer lacks the feature alignment which is important for VSR since this layer independently processes each of the input token embeddings without any interaction among them. In this paper, we make the first attempt to adapt Transformer for VSR. Specifically, to tackle the first issue, we present a spatial-temporal convolutional self-attention layer with a theoretical understanding to exploit the locality information. For the second issue, we design a bidirectional optical flow-based feed-forward layer to discover the correlations across different video frames and also align features. Extensive experiments on several benchmark datasets demonstrate the effectiveness of our proposed method. The code will be available at https://github.com/caojiezhang/VSR-Transformer.",0
"The problem of video super-resolution (VSR), which aims to enhance the resolution of low-resolution videos, is a challenge in spatial-temporal sequence prediction. The Transformer model has gained popularity for its ability to perform parallel computing for sequence-to-sequence modeling. While it may seem like a natural fit for VSR, the typical Transformer block design with a fully connected self-attention layer and a token-wise feed-forward layer is not suitable for VSR due to two reasons. Firstly, the fully connected self-attention layer does not take into account data locality, relying solely on linear layers to compute attention maps. Secondly, the token-wise feed-forward layer lacks feature alignment, which is crucial for VSR since it processes each input token embedding independently without any interaction between them. This paper presents the first attempt to adapt the Transformer model for VSR. To address the aforementioned issues, the authors propose a spatial-temporal convolutional self-attention layer to exploit locality information and a bidirectional optical flow-based feed-forward layer to discover correlations across different video frames and align features. The proposed method is evaluated on multiple benchmark datasets, and the results show its effectiveness. The code for this research will be available on GitHub at https://github.com/caojiezhang/VSR-Transformer.",1
"Facial expressions vary from the visible to the subtle. In recent years, the analysis of micro-expressions $-$ a natural occurrence resulting from the suppression of one's true emotions, has drawn the attention of researchers with a broad range of potential applications. However, spotting microexpressions in long videos becomes increasingly challenging when intertwined with normal or macro-expressions. In this paper, we propose a shallow optical flow three-stream CNN (SOFTNet) model to predict a score that captures the likelihood of a frame being in an expression interval. By fashioning the spotting task as a regression problem, we introduce pseudo-labeling to facilitate the learning process. We demonstrate the efficacy and efficiency of the proposed approach on the recent MEGC 2020 benchmark, where state-of-the-art performance is achieved on CAS(ME)$^{2}$ with equally promising results on SAMM Long Videos.",0
"The range of facial expressions can vary greatly, from noticeable to subtle. Recently, researchers have taken an interest in analyzing micro-expressions, which occur naturally when true emotions are suppressed. This has many potential applications, but becomes more challenging when trying to identify micro-expressions in longer videos that also contain normal and macro-expressions. In this study, we introduce a shallow optical flow three-stream CNN (SOFTNet) model that predicts the likelihood of a frame being part of an expression interval. We approach this task as a regression problem and use pseudo-labeling to aid the learning process. Our results show that this approach is effective and efficient, achieving state-of-the-art performance on the CAS(ME)$^{2}$ benchmark and promising results on SAMM Long Videos.",1
"Geometric alignment appears in a variety of applications, ranging from domain adaptation, optimal transport, and normalizing flows in machine learning; optical flow and learned augmentation in computer vision and deformable registration within biomedical imaging. A recurring challenge is the alignment of domains whose topology is not the same; a problem that is routinely ignored, potentially introducing bias in downstream analysis. As a first step towards solving such alignment problems, we propose an unsupervised topological difference detection algorithm. The model is based on a conditional variational auto-encoder and detects topological anomalies with regards to a reference alongside the registration step. We consider both a) topological changes in the image under spatial variation and b) unexpected transformations. Our approach is validated on a proxy task of unsupervised anomaly detection in images.",0
"Geometric alignment is present in various fields such as machine learning, computer vision, and biomedical imaging. One of the main difficulties encountered is aligning domains with different topologies, which is often overlooked and may lead to biased analysis. To address this issue, we propose an unsupervised algorithm that detects topological differences using a conditional variational auto-encoder during the registration process. We examine both topological changes caused by spatial variation and unexpected transformations. Our approach is validated through unsupervised anomaly detection in images.",1
"We present DistillFlow, a knowledge distillation approach to learning optical flow. DistillFlow trains multiple teacher models and a student model, where challenging transformations are applied to the input of the student model to generate hallucinated occlusions as well as less confident predictions. Then, a self-supervised learning framework is constructed: confident predictions from teacher models are served as annotations to guide the student model to learn optical flow for those less confident predictions. The self-supervised learning framework enables us to effectively learn optical flow from unlabeled data, not only for non-occluded pixels, but also for occluded pixels. DistillFlow achieves state-of-the-art unsupervised learning performance on both KITTI and Sintel datasets. Our self-supervised pre-trained model also provides an excellent initialization for supervised fine-tuning, suggesting an alternate training paradigm in contrast to current supervised learning methods that highly rely on pre-training on synthetic data. At the time of writing, our fine-tuned models ranked 1st among all monocular methods on the KITTI 2015 benchmark, and outperform all published methods on the Sintel Final benchmark. More importantly, we demonstrate the generalization capability of DistillFlow in three aspects: framework generalization, correspondence generalization and cross-dataset generalization.",0
"Our approach, called DistillFlow, is a technique for learning optical flow through knowledge distillation. We utilize multiple teacher models and a student model, with challenging transformations applied to the student model's input to create hallucinated occlusions and less certain predictions. A self-supervised learning framework is established, where the teacher models' confident predictions serve as annotations to guide the student model in learning optical flow for the less certain predictions. This framework enables us to effectively learn optical flow from unlabeled data, including occluded pixels. DistillFlow sets a new benchmark for unsupervised learning on both the KITTI and Sintel datasets, and our self-supervised pre-trained model provides a strong starting point for supervised fine-tuning. We have achieved 1st place on the KITTI 2015 benchmark and outperformed all other methods on the Sintel Final benchmark. Furthermore, we have demonstrated the generalization capabilities of DistillFlow across framework, correspondence, and cross-dataset contexts.",1
"Neuromorphic sensing and computing hold a promise for highly energy-efficient and high-bandwidth-sensor processing. A major challenge for neuromorphic computing is that learning algorithms for traditional artificial neural networks (ANNs) do not transfer directly to spiking neural networks (SNNs) due to the discrete spikes and more complex neuronal dynamics. As a consequence, SNNs have not yet been successfully applied to complex, large-scale tasks. In this article, we focus on the self-supervised learning problem of optical flow estimation from event-based camera inputs, and investigate the changes that are necessary to the state-of-the-art ANN training pipeline in order to successfully tackle it with SNNs. More specifically, we first modify the input event representation to encode a much smaller time slice with minimal explicit temporal information. Consequently, we make the network's neuronal dynamics and recurrent connections responsible for integrating information over time. Moreover, we reformulate the self-supervised loss function for event-based optical flow to improve its convexity. We perform experiments with various types of recurrent ANNs and SNNs using the proposed pipeline. Concerning SNNs, we investigate the effects of elements such as parameter initialization and optimization, surrogate gradient shape, and adaptive neuronal mechanisms. We find that initialization and surrogate gradient width play a crucial part in enabling learning with sparse inputs, while the inclusion of adaptivity and learnable neuronal parameters can improve performance. We show that the performance of the proposed ANNs and SNNs are on par with that of the current state-of-the-art ANNs trained in a self-supervised manner.",0
"Highly energy-efficient and high-bandwidth-sensor processing can be achieved through neuromorphic sensing and computing. However, the learning algorithms used for traditional artificial neural networks (ANNs) cannot be directly applied to spiking neural networks (SNNs) due to the discrete spikes and complex neuronal dynamics. Therefore, SNNs have not yet been successfully applied to complex, large-scale tasks. This article explores the self-supervised learning problem of optical flow estimation from event-based camera inputs and examines the changes required to the state-of-the-art ANN training pipeline to successfully tackle it with SNNs. The input event representation is modified to encode a smaller time slice with minimal explicit temporal information, making the network's neuronal dynamics and recurrent connections responsible for integrating information over time. The self-supervised loss function for event-based optical flow is reformulated to improve its convexity. Experiments are conducted with various types of recurrent ANNs and SNNs using the proposed pipeline, investigating the effects of elements such as parameter initialization and optimization, surrogate gradient shape, and adaptive neuronal mechanisms. It is found that initialization and surrogate gradient width are crucial in enabling learning with sparse inputs, while the inclusion of adaptivity and learnable neuronal parameters can improve performance. The proposed ANNs and SNNs show a performance on par with the current state-of-the-art ANNs trained in a self-supervised manner.",1
"Non-Rigid Structure-from-Motion (NRSfM) reconstructs a deformable 3D object from the correspondences established between monocular 2D images. Current NRSfM methods lack statistical robustness, which is the ability to cope with correspondence errors.This prevents one to use automatically established correspondences, which are prone to errors, thereby strongly limiting the scope of NRSfM. We propose a three-step automatic pipeline to solve NRSfM robustly by exploiting isometry. Step 1 computes the optical flow from correspondences, step 2 reconstructs each 3D point's normal vector using multiple reference images and integrates them to form surfaces with the best reference and step 3 rejects the 3D points that break isometry in their local neighborhood. Importantly, each step is designed to discard or flag erroneous correspondences. Our contributions include the robustification of optical flow by warp estimation, new fast analytic solutions to local normal reconstruction and their robustification, and a new scale-independent measure of 3D local isometric coherence. Experimental results show that our robust NRSfM method consistently outperforms existing methods on both synthetic and real datasets.",0
"The process of Non-Rigid Structure-from-Motion (NRSfM) involves reconstructing a deformable 3D object through correspondences established between monocular 2D images. However, current NRSfM methods lack statistical robustness, meaning that they struggle to handle errors in correspondence. This has limited the use of automatically established correspondences and therefore restricted the potential of NRSfM. In order to address this issue, we propose a three-step automatic pipeline that utilizes isometry to solve NRSfM robustly. Step 1 involves the computation of optical flow from correspondences, while step 2 reconstructs the normal vector of each 3D point using multiple reference images and integrates them to form surfaces with the best reference. Finally, step 3 involves the rejection of 3D points that break isometry in their local neighborhood. Each step is designed to discard or flag erroneous correspondences. Our contributions to this approach include the robustification of optical flow through warp estimation, new fast analytic solutions for local normal reconstruction and their robustification, and the introduction of a new scale-independent measure of 3D local isometric coherence. Our experimental results show that our approach consistently outperforms existing NRSfM methods on both synthetic and real datasets.",1
"End-to-end trained convolutional neural networks have led to a breakthrough in optical flow estimation. The most recent advances focus on improving the optical flow estimation by improving the architecture and setting a new benchmark on the publicly available MPI-Sintel dataset. Instead, in this article, we investigate how deep neural networks estimate optical flow. A better understanding of how these networks function is important for (i) assessing their generalization capabilities to unseen inputs, and (ii) suggesting changes to improve their performance. For our investigation, we focus on FlowNetS, as it is the prototype of an encoder-decoder neural network for optical flow estimation. Furthermore, we use a filter identification method that has played a major role in uncovering the motion filters present in animal brains in neuropsychological research. The method shows that the filters in the deepest layer of FlowNetS are sensitive to a variety of motion patterns. Not only do we find translation filters, as demonstrated in animal brains, but thanks to the easier measurements in artificial neural networks, we even unveil dilation, rotation, and occlusion filters. Furthermore, we find similarities in the refinement part of the network and the perceptual filling-in process which occurs in the mammal primary visual cortex.",0
"Convolutional neural networks trained end-to-end have revolutionized the estimation of optical flow. While recent developments have concentrated on improving the architecture of these networks and setting new benchmarks using the MPI-Sintel dataset, this article delves into the inner workings of deep neural networks used for optical flow estimation. Understanding their function is key to assessing their ability to generalize and suggesting performance-enhancing changes. The study focuses on the prototype encoder-decoder network, FlowNetS, utilizing a filter identification method that has previously been instrumental in revealing motion filters in animal brains. The method exposes the sensitivity of the deepest layer of FlowNetS to various motion patterns, including dilation, rotation, and occlusion filters, in addition to translation filters found in animal brains. The study also draws parallels between the refinement process in the network and the perceptual filling-in process in the primary visual cortex of mammals.",1
"We present an unsupervised learning approach for optical flow estimation by improving the upsampling and learning of pyramid network. We design a self-guided upsample module to tackle the interpolation blur problem caused by bilinear upsampling between pyramid levels. Moreover, we propose a pyramid distillation loss to add supervision for intermediate levels via distilling the finest flow as pseudo labels. By integrating these two components together, our method achieves the best performance for unsupervised optical flow learning on multiple leading benchmarks, including MPI-SIntel, KITTI 2012 and KITTI 2015. In particular, we achieve EPE=1.4 on KITTI 2012 and F1=9.38% on KITTI 2015, which outperform the previous state-of-the-art methods by 22.2% and 15.7%, respectively.",0
"Our approach to optical flow estimation through unsupervised learning involves enhancing the upsampling and learning process of the pyramid network. To resolve the issue of interpolation blur caused by bilinear upsampling between pyramid levels, we have devised a self-guided upsample module. Furthermore, we have introduced a pyramid distillation loss to provide supervision for intermediate levels by distilling the finest flow as pseudo labels. By combining these two elements, we have achieved the most outstanding results for unsupervised optical flow learning on multiple leading benchmarks, including MPI-SIntel, KITTI 2012 and KITTI 2015. Specifically, we have surpassed the previous state-of-the-art methods by 22.2% and 15.7%, respectively, with EPE=1.4 on KITTI 2012 and F1=9.38% on KITTI 2015.",1
"Estimating geometric elements such as depth, camera motion, and optical flow from images is an important part of the robot's visual perception. We use a joint self-supervised method to estimate the three geometric elements. Depth network, optical flow network and camera motion network are independent of each other but are jointly optimized during training phase. Compared with independent training, joint training can make full use of the geometric relationship between geometric elements and provide dynamic and static information of the scene. In this paper, we improve the joint self-supervision method from three aspects: network structure, dynamic object segmentation, and geometric constraints. In terms of network structure, we apply the attention mechanism to the camera motion network, which helps to take advantage of the similarity of camera movement between frames. And according to attention mechanism in Transformer, we propose a plug-and-play convolutional attention module. In terms of dynamic object, according to the different influences of dynamic objects in the optical flow self-supervised framework and the depth-pose self-supervised framework, we propose a threshold algorithm to detect dynamic regions, and mask that in the loss function respectively. In terms of geometric constraints, we use traditional methods to estimate the fundamental matrix from the corresponding points to constrain the camera motion network. We demonstrate the effectiveness of our method on the KITTI dataset. Compared with other joint self-supervised methods, our method achieves state-of-the-art performance in the estimation of pose and optical flow, and the depth estimation has also achieved competitive results. Code will be available https://github.com/jianfenglihg/Unsupervised_geometry.",0
"The robot's visual perception involves estimating important geometric elements such as camera motion, depth, and optical flow from images. To achieve this, we utilize a joint self-supervised method for estimating these elements. Although the depth network, optical flow network, and camera motion network are independent, they are jointly optimized during training to make full use of the geometric relationship between them and provide both dynamic and static scene information. In this study, we enhance the joint self-supervision method by improving the network structure, applying dynamic object segmentation, and incorporating geometric constraints. We employ the attention mechanism in the camera motion network and introduce a plug-and-play convolutional attention module based on the Transformer model. We propose a threshold algorithm to detect dynamic regions and mask them in the loss function. Additionally, to constrain the camera motion network, we use traditional methods to estimate the fundamental matrix from corresponding points. We demonstrate the effectiveness of our method on the KITTI dataset and show that it surpasses other joint self-supervised techniques, achieving state-of-the-art performance in pose and optical flow estimation, and competitive results in depth estimation. The code for our method is available at https://github.com/jianfenglihg/Unsupervised_geometry.",1
"We present a Python-based renderer built on NVIDIA's OptiX ray tracing engine and the OptiX AI denoiser, designed to generate high-quality synthetic images for research in computer vision and deep learning. Our tool enables the description and manipulation of complex dynamic 3D scenes containing object meshes, materials, textures, lighting, volumetric data (e.g., smoke), and backgrounds. Metadata, such as 2D/3D bounding boxes, segmentation masks, depth maps, normal maps, material properties, and optical flow vectors, can also be generated. In this work, we discuss design goals, architecture, and performance. We demonstrate the use of data generated by path tracing for training an object detector and pose estimator, showing improved performance in sim-to-real transfer in situations that are difficult for traditional raster-based renderers. We offer this tool as an easy-to-use, performant, high-quality renderer for advancing research in synthetic data generation and deep learning.",0
"Our Python-based renderer, which utilizes NVIDIA's OptiX ray tracing engine and the OptiX AI denoiser, aims to produce top-notch synthetic images for computer vision and deep learning research. It empowers users to create and edit intricate dynamic 3D scenes with object meshes, materials, textures, lighting, volumetric data, and backgrounds. Additionally, metadata like 2D/3D bounding boxes, segmentation masks, depth maps, normal maps, material properties, and optical flow vectors can be generated. In this paper, we delve into the renderer's design objectives, architecture, and performance. Our experiments demonstrate that training object detection and pose estimation models with data generated through path tracing yields better results in sim-to-real transfer scenarios that challenge traditional raster-based renderers. Our goal is to provide an accessible, efficient, and superior renderer that furthers synthetic data generation and deep learning research.",1
"Semantic segmentation of aerial videos has been extensively used for decision making in monitoring environmental changes, urban planning, and disaster management. The reliability of these decision support systems is dependent on the accuracy of the video semantic segmentation algorithms. The existing CNN based video semantic segmentation methods have enhanced the image semantic segmentation methods by incorporating an additional module such as LSTM or optical flow for computing temporal dynamics of the video which is a computational overhead. The proposed research work modifies the CNN architecture by incorporating temporal information to improve the efficiency of video semantic segmentation.   In this work, an enhanced encoder-decoder based CNN architecture (UVid-Net) is proposed for UAV video semantic segmentation. The encoder of the proposed architecture embeds temporal information for temporally consistent labelling. The decoder is enhanced by introducing the feature-refiner module, which aids in accurate localization of the class labels. The proposed UVid-Net architecture for UAV video semantic segmentation is quantitatively evaluated on extended ManipalUAVid dataset. The performance metric mIoU of 0.79 has been observed which is significantly greater than the other state-of-the-art algorithms. Further, the proposed work produced promising results even for the pre-trained model of UVid-Net on urban street scene with fine tuning the final layer on UAV aerial videos.",0
"The use of semantic segmentation in aerial videos has become increasingly important in decision making for various applications such as environmental monitoring, urban planning, and disaster management. However, the accuracy of video semantic segmentation algorithms is crucial for reliable decision support systems. Current CNN-based methods for video semantic segmentation have improved image segmentation by incorporating additional modules like LSTM or optical flow, leading to increased computational costs. To address this issue, this research proposes modifying the CNN architecture to include temporal information for efficient video semantic segmentation. The proposed UVid-Net architecture incorporates temporal information in the encoder for consistent labeling and introduces a feature-refiner module in the decoder for accurate class label localization. The performance of UVid-Net was evaluated on the ManipalUAVid dataset, showing a significantly higher mIoU of 0.79 compared to state-of-the-art algorithms. Moreover, the proposed architecture demonstrated promising results even when fine-tuning the final layer on UAV aerial videos after pre-training on urban street scenes.",1
"Recently, DETR and Deformable DETR have been proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance as previous complex hand-crafted detectors. However, their performance on Video Object Detection (VOD) has not been well explored. In this paper, we present TransVOD, an end-to-end video object detection model based on a spatial-temporal Transformer architecture. The goal of this paper is to streamline the pipeline of VOD, effectively removing the need for many hand-crafted components for feature aggregation, e.g., optical flow, recurrent neural networks, relation networks. Besides, benefited from the object query design in DETR, our method does not need complicated post-processing methods such as Seq-NMS or Tubelet rescoring, which keeps the pipeline simple and clean. In particular, we present temporal Transformer to aggregate both the spatial object queries and the feature memories of each frame. Our temporal Transformer consists of three components: Temporal Deformable Transformer Encoder (TDTE) to encode the multiple frame spatial details, Temporal Query Encoder (TQE) to fuse object queries, and Temporal Deformable Transformer Decoder to obtain current frame detection results. These designs boost the strong baseline deformable DETR by a significant margin (3%-4% mAP) on the ImageNet VID dataset. TransVOD yields comparable results performance on the benchmark of ImageNet VID. We hope our TransVOD can provide a new perspective for video object detection. Code will be made publicly available at https://github.com/SJTU-LuHe/TransVOD.",0
"DETR and Deformable DETR have been proposed recently to eliminate the need for hand-designed components in object detection, while maintaining good performance as previous complex detectors. However, their performance in Video Object Detection (VOD) has not been thoroughly investigated. To address this gap, this paper introduces TransVOD, an end-to-end video object detection model based on a spatial-temporal Transformer architecture. The aim is to simplify the VOD pipeline by removing the need for hand-crafted components for feature aggregation such as optical flow, recurrent neural networks, and relation networks. With the object query design in DETR, TransVOD does not require complicated post-processing methods like Seq-NMS or Tubelet rescoring, resulting in a clean and straightforward pipeline. The paper presents a temporal Transformer that aggregates both spatial object queries and feature memories of each frame. TransVOD uses three components: a Temporal Deformable Transformer Encoder (TDTE) to encode multiple frame spatial details, a Temporal Query Encoder (TQE) to fuse object queries, and a Temporal Deformable Transformer Decoder to obtain current frame detection results. These designs enhance the performance of deformable DETR by 3-4% mAP on the ImageNet VID dataset. TransVOD achieves comparable results to the benchmark of ImageNet VID, and the code is available at https://github.com/SJTU-LuHe/TransVOD. The paper offers a new perspective on video object detection.",1
"The technology for Visual Odometry (VO) that estimates the position and orientation of the moving object through analyzing the image sequences captured by on-board cameras, has been well investigated with the rising interest in autonomous driving. This paper studies monocular VO from the perspective of Deep Learning (DL). Unlike most current learning-based methods, our approach, called DeepAVO, is established on the intuition that features contribute discriminately to different motion patterns. Specifically, we present a novel four-branch network to learn the rotation and translation by leveraging Convolutional Neural Networks (CNNs) to focus on different quadrants of optical flow input. To enhance the ability of feature selection, we further introduce an effective channel-spatial attention mechanism to force each branch to explicitly distill related information for specific Frame to Frame (F2F) motion estimation. Experiments on various datasets involving outdoor driving and indoor walking scenarios show that the proposed DeepAVO outperforms the state-of-the-art monocular methods by a large margin, demonstrating competitive performance to the stereo VO algorithm and verifying promising potential for generalization.",0
"The rising interest in autonomous driving has led to extensive research on Visual Odometry (VO), which utilizes on-board cameras to estimate the position and orientation of a moving object by analyzing image sequences. This study focuses on monocular VO using Deep Learning (DL) and proposes a new approach called DeepAVO. Unlike current learning-based methods, DeepAVO exploits the idea that features contribute discriminately to different motion patterns. The proposed method uses a four-branch network that leverages Convolutional Neural Networks (CNNs) to focus on different quadrants of optical flow input to learn rotation and translation. To enhance feature selection, an effective channel-spatial attention mechanism is introduced to explicitly distill related information for specific Frame to Frame (F2F) motion estimation. Experiments on various datasets, including outdoor driving and indoor walking scenarios, demonstrate that DeepAVO outperforms state-of-the-art monocular methods and shows promising potential for generalization, with competitive performance to the stereo VO algorithm.",1
"With the advent of neuromorphic vision sensors such as event-based cameras, a paradigm shift is required for most computer vision algorithms. Among these algorithms, optical flow estimation is a prime candidate for this process considering that it is linked to a neuromorphic vision approach. Usage of optical flow is widespread in robotics applications due to its richness and accuracy. We present a Principal Component Analysis (PCA) approach to the problem of event-based optical flow estimation. In this approach, we examine different regularization methods which efficiently enhance the estimation of the optical flow. We show that the best variant of our proposed method, dedicated to the real-time context of visual odometry, is about two times faster compared to state-of-the-art implementations while significantly improves optical flow accuracy.",0
"Most computer vision algorithms need to be reevaluated with the introduction of neuromorphic vision sensors, like event-based cameras. Optical flow estimation is an ideal candidate for this change, as it is linked to a neuromorphic vision approach, and is widely used in robotics applications due to its richness and accuracy. To tackle the problem of event-based optical flow estimation, we propose a Principal Component Analysis (PCA) approach. Our approach examines various regularization methods that improve the estimation of optical flow efficiently. We demonstrate that our proposed method's best version, specifically designed for the real-time context of visual odometry, is about two times faster than current implementations while significantly enhancing the accuracy of optical flow.",1
"In this work we tackle the task of video-based visual emotion recognition in the wild. Standard methodologies that rely solely on the extraction of bodily and facial features often fall short of accurate emotion prediction in cases where the aforementioned sources of affective information are inaccessible due to head/body orientation, low resolution and poor illumination. We aspire to alleviate this problem by leveraging visual context in the form of scene characteristics and attributes, as part of a broader emotion recognition framework. Temporal Segment Networks (TSN) constitute the backbone of our proposed model. Apart from the RGB input modality, we make use of dense Optical Flow, following an intuitive multi-stream approach for a more effective encoding of motion. Furthermore, we shift our attention towards skeleton-based learning and leverage action-centric data as means of pre-training a Spatial-Temporal Graph Convolutional Network (ST-GCN) for the task of emotion recognition. Our extensive experiments on the challenging Body Language Dataset (BoLD) verify the superiority of our methods over existing approaches, while by properly incorporating all of the aforementioned modules in a network ensemble, we manage to surpass the previous best published recognition scores, by a large margin.",0
"The objective of our study is to improve video-based visual emotion recognition in real-life situations. Traditional approaches that rely on facial and bodily features are often unreliable when these sources of information are inaccessible. Such instances can occur due to low resolution, poor lighting, or head/body orientation. To address this challenge, we propose using visual context, such as scene characteristics and attributes, as part of a broader emotion recognition framework. Our model uses Temporal Segment Networks (TSN) as its foundation, along with dense Optical Flow, to better capture motion. Additionally, we incorporate skeleton-based learning and action-centric data to pre-train a Spatial-Temporal Graph Convolutional Network (ST-GCN) for emotion recognition. Our experiments on the Body Language Dataset (BoLD) demonstrate that our approach outperforms existing methods, and by combining all modules in a network ensemble, we significantly surpass the best published recognition scores.",1
"This paper addresses the challenging unsupervised scene flow estimation problem by jointly learning four low-level vision sub-tasks: optical flow $\textbf{F}$, stereo-depth $\textbf{D}$, camera pose $\textbf{P}$ and motion segmentation $\textbf{S}$. Our key insight is that the rigidity of the scene shares the same inherent geometrical structure with object movements and scene depth. Hence, rigidity from $\textbf{S}$ can be inferred by jointly coupling $\textbf{F}$, $\textbf{D}$ and $\textbf{P}$ to achieve more robust estimation. To this end, we propose a novel scene flow framework named EffiScene with efficient joint rigidity learning, going beyond the existing pipeline with independent auxiliary structures. In EffiScene, we first estimate optical flow and depth at the coarse level and then compute camera pose by Perspective-$n$-Points method. To jointly learn local rigidity, we design a novel Rigidity From Motion (RfM) layer with three principal components: \emph{}{(i)} correlation extraction; \emph{}{(ii)} boundary learning; and \emph{}{(iii)} outlier exclusion. Final outputs are fused based on the rigid map $M_R$ from RfM at finer levels. To efficiently train EffiScene, two new losses $\mathcal{L}_{bnd}$ and $\mathcal{L}_{unc}$ are designed to prevent trivial solutions and to regularize the flow boundary discontinuity. Extensive experiments on scene flow benchmark KITTI show that our method is effective and significantly improves the state-of-the-art approaches for all sub-tasks, i.e. optical flow ($5.19 \rightarrow 4.20$), depth estimation ($3.78 \rightarrow 3.46$), visual odometry ($0.012 \rightarrow 0.011$) and motion segmentation ($0.57 \rightarrow 0.62$).",0
"Efficiently estimating unsupervised scene flow is a complex and challenging task. In this paper, we propose a novel framework, named EffiScene, that jointly learns four low-level vision sub-tasks: optical flow, stereo-depth, camera pose, and motion segmentation. Our approach leverages the inherent geometrical structure shared by rigidity of the scene, object movements, and scene depth. By coupling optical flow, stereo-depth, and camera pose, we achieve more robust estimation of rigidity from motion segmentation. EffiScene utilizes a Rigidity From Motion (RfM) layer with correlation extraction, boundary learning, and outlier exclusion to jointly learn local rigidity. Two new losses, designed to prevent trivial solutions and to regularize flow boundary discontinuity, efficiently train EffiScene. Our experiments on the KITTI benchmark show that EffiScene significantly improves the state-of-the-art approaches for all sub-tasks, including optical flow, depth estimation, visual odometry, and motion segmentation.",1
"We present SMURF, a method for unsupervised learning of optical flow that improves state of the art on all benchmarks by $36\%$ to $40\%$ (over the prior best method UFlow) and even outperforms several supervised approaches such as PWC-Net and FlowNet2. Our method integrates architecture improvements from supervised optical flow, i.e. the RAFT model, with new ideas for unsupervised learning that include a sequence-aware self-supervision loss, a technique for handling out-of-frame motion, and an approach for learning effectively from multi-frame video data while still only requiring two frames for inference.",0
"SMURF is a novel technique for unsupervised learning of optical flow that surpasses the previous best method UFlow, as well as several supervised approaches including PWC-Net and FlowNet2. Our method achieves a remarkable improvement of 36% to 40% on all benchmarks. To achieve this, we have incorporated the RAFT model's architecture enhancements from supervised optical flow while introducing innovative unsupervised learning concepts. These include a self-supervision loss that is sequence-aware, a method for handling out-of-frame motion, and an approach for effectively learning from multi-frame video data while still only requiring two frames for inference.",1
"Video salient object detection (VSOD) aims to locate and segment the most attractive object by exploiting both spatial cues and temporal cues hidden in video sequences. However, spatial and temporal cues are often unreliable in real-world scenarios, such as low-contrast foreground, fast motion, and multiple moving objects. To address these problems, we propose a new framework to adaptively capture available information from spatial and temporal cues, which contains Confidence-guided Adaptive Gate (CAG) modules and Dual Differential Enhancement (DDE) modules. For both RGB features and optical flow features, CAG estimates confidence scores supervised by the IoU between predictions and the ground truths to re-calibrate the information with a gate mechanism. DDE captures the differential feature representation to enrich the spatial and temporal information and generate the fused features. Experimental results on four widely used datasets demonstrate the effectiveness of the proposed method against thirteen state-of-the-art methods.",0
"The objective of Video salient object detection (VSOD) is to locate and segment the most captivating object in a video sequence by utilizing both spatial and temporal cues. However, in real-world scenarios, such as low-contrast foreground, fast motion, and multiple moving objects, these cues are frequently unreliable. To address this issue, we have introduced a novel framework that can adaptively capture information from spatial and temporal cues. This framework includes Confidence-guided Adaptive Gate (CAG) modules and Dual Differential Enhancement (DDE) modules. CAG modules estimate confidence scores for both RGB features and optical flow features, utilizing the intersection over union (IoU) between predictions and ground truths to recalibrate the information with a gate mechanism. DDE captures differential feature representation to enhance the spatial and temporal information and generate fused features. Our experimental results on four commonly used datasets demonstrate the effectiveness of our proposed method over thirteen other state-of-the-art approaches.",1
"Most Video Super-Resolution (VSR) methods enhance a video reference frame by aligning its neighboring frames and mining information on these frames. Recently, deformable alignment has drawn extensive attention in VSR community for its remarkable performance, which can adaptively align neighboring frames with the reference one. However, we experimentally find that deformable alignment methods still suffer from fast motion due to locally loss-driven offset prediction and lack explicit motion constraints. Hence, we propose a Matching-based Flow Estimation (MFE) module to conduct global semantic feature matching and estimate optical flow as coarse offset for each location. And a Flow-guided Deformable Module (FDM) is proposed to integrate optical flow into deformable convolution. The FDM uses the optical flow to warp the neighboring frames at first. And then, the warped neighboring frames and the reference one are used to predict a set of fine offsets for each coarse offset. In general, we propose an end-to-end deep network called Flow-guided Deformable Alignment Network (FDAN), which reaches the state-of-the-art performance on two benchmark datasets while is still competitive in computation and memory consumption.",0
"The majority of Video Super-Resolution (VSR) methods enhance a video reference frame by utilizing neighboring frames to extract information and align the frames. Recently, the VSR community has shown great interest in deformable alignment due to its exceptional performance in adaptively aligning neighboring frames with the reference frame. However, our experiments show that deformable alignment methods are still problematic when dealing with fast-moving images due to locally loss-driven offset prediction and the lack of explicit motion constraints. Consequently, we propose the Matching-based Flow Estimation (MFE) module to conduct global semantic feature matching and estimate optical flow as a coarse offset for each location. Furthermore, we propose the Flow-guided Deformable Module (FDM) that integrates optical flow into deformable convolution. The FDM uses the optical flow to warp the neighboring frames, followed by the warped frames and the reference frame being used to predict a set of fine offsets for each coarse offset. Overall, we propose an end-to-end deep network, called the Flow-guided Deformable Alignment Network (FDAN), which achieves state-of-the-art performance on two benchmark datasets while remaining competitive in computation and memory consumption.",1
"Video Frame Interpolation synthesizes non-existent images between adjacent frames, with the aim of providing a smooth and consistent visual experience. Two approaches for solving this challenging task are optical flow based and kernel-based methods. In existing works, optical flow based methods can provide accurate point-to-point motion description, however, they lack constraints on object structure. On the contrary, kernel-based methods focus on structural alignment, which relies on semantic and apparent features, but tends to blur results. Based on these observations, we propose a structure-motion based iterative fusion method. The framework is an end-to-end learnable structure with two stages. First, interpolated frames are synthesized by structure-based and motion-based learning branches respectively, then, an iterative refinement module is established via spatial and temporal feature integration. Inspired by the observation that audiences have different visual preferences on foreground and background objects, we for the first time propose to use saliency masks in the evaluation processes of the task of video frame interpolation. Experimental results on three typical benchmarks show that the proposed method achieves superior performance on all evaluation metrics over the state-of-the-art methods, even when our models are trained with only one-tenth of the data other methods use.",0
"The process of Video Frame Interpolation involves the creation of non-existent images between neighboring frames to ensure a seamless and consistent visual experience. Optical flow based and kernel-based methods are two approaches used to tackle this complex task. Although optical flow based methods provide accurate point-to-point motion description, they lack constraints on object structure. Conversely, kernel-based methods focus on structural alignment, relying on semantic and apparent features, but tend to produce blurry results. Therefore, we introduce a structure-motion based iterative fusion method that is an end-to-end learnable structure with two stages. First, interpolated frames are synthesized using structure-based and motion-based learning branches, followed by an iterative refinement module via spatial and temporal feature integration. Additionally, we propose using saliency masks in the evaluation processes as audiences have different visual preferences on foreground and background objects. Our experimental results on three typical benchmarks demonstrate that our proposed method outperforms state-of-the-art methods on all evaluation metrics, even with just one-tenth of the data used by other methods in training.",1
"Video anomaly detection is a challenging task because of diverse abnormal events. To this task, methods based on reconstruction and prediction are wildly used in recent works, which are built on the assumption that learning on normal data, anomalies cannot be reconstructed or predicated as good as normal patterns, namely the anomaly result with more errors. In this paper, we propose to discriminate anomalies from normal ones by the duality of normality-granted optical flow, which is conducive to predict normal frames but adverse to abnormal frames. The normality-granted optical flow is predicted from a single frame, to keep the motion knowledge focused on normal patterns. Meanwhile, We extend the appearance-motion correspondence scheme from frame reconstruction to prediction, which not only helps to learn the knowledge about object appearances and correlated motion, but also meets the fact that motion is the transformation between appearances. We also introduce a margin loss to enhance the learning of frame prediction. Experiments on standard benchmark datasets demonstrate the impressive performance of our approach.",0
"Detecting anomalies in videos presents a challenge due to the wide range of abnormal events that can occur. Recent studies have primarily relied on reconstruction and prediction methods, assuming that anomalies cannot be reconstructed or predicted accurately compared to normal patterns. In this paper, we propose using the duality of normality-granted optical flow as a means to distinguish between normal and abnormal frames. Our optical flow prediction is based on a single frame to focus on motion knowledge related to normal patterns. Additionally, we extend the appearance-motion correspondence scheme to prediction and introduce a margin loss to improve frame prediction learning. Our experimental results on benchmark datasets show impressive performance.",1
"Intersections where vehicles are permitted to turn and interact with vulnerable road users (VRUs) like pedestrians and cyclists are among some of the most challenging locations for automated and accurate recognition of road users' behavior. In this paper, we propose a deep conditional generative model for interaction detection at such locations. It aims to automatically analyze massive video data about the continuity of road users' behavior. This task is essential for many intelligent transportation systems such as traffic safety control and self-driving cars that depend on the understanding of road users' locomotion. A Conditional Variational Auto-Encoder based model with Gaussian latent variables is trained to encode road users' behavior and perform probabilistic and diverse predictions of interactions. The model takes as input the information of road users' type, position and motion automatically extracted by a deep learning object detector and optical flow from videos, and generates frame-wise probabilities that represent the dynamics of interactions between a turning vehicle and any VRUs involved. The model's efficacy was validated by testing on real--world datasets acquired from two different intersections. It achieved an F1-score above 0.96 at a right--turn intersection in Germany and 0.89 at a left--turn intersection in Japan, both with very busy traffic flows.",0
"Automated recognition of road users' behavior at intersections where vehicles interact with vulnerable road users (VRUs) such as pedestrians and cyclists is a complex task. Our proposed solution is a deep conditional generative model that analyzes extensive video data to detect interactions accurately. This is essential for intelligent transportation systems, including traffic safety control and self-driving cars, that require an understanding of road users' movements. Our model, a Conditional Variational Auto-Encoder based on Gaussian latent variables, encodes road users' behavior to predict probabilistic and diverse interactions. It utilizes deep learning object detectors and optical flow to extract road users' type, position, and motion, generating frame-wise probabilities that represent the dynamics of interactions between turning vehicles and VRUs. Our model was tested on real-world data from two busy intersections, achieving an F1-score of 0.96 at a right-turn intersection in Germany and 0.89 at a left-turn intersection in Japan.",1
"Remarkable progress has been made in 3D reconstruction of rigid structures from a video or a collection of images. However, it is still challenging to reconstruct nonrigid structures from RGB inputs, due to its under-constrained nature. While template-based approaches, such as parametric shape models, have achieved great success in modeling the ""closed world"" of known object categories, they cannot well handle the ""open-world"" of novel object categories or outlier shapes. In this work, we introduce a template-free approach to learn 3D shapes from a single video. It adopts an analysis-by-synthesis strategy that forward-renders object silhouette, optical flow, and pixel values to compare with video observations, which generates gradients to adjust the camera, shape and motion parameters. Without using a category-specific shape template, our method faithfully reconstructs nonrigid 3D structures from videos of human, animals, and objects of unknown classes. Code will be available at lasr-google.github.io .",0
"Significant advancements have been made in the 3D reconstruction of stiff structures from images or videos. However, reconstructing non-rigid structures from RGB inputs remains challenging due to their under-constrained nature. While parametric shape models have been successful in modeling known object categories, they struggle with novel or outlier shapes. This study introduces a template-free approach to 3D shape learning from a single video, utilizing an analysis-by-synthesis strategy to compare object silhouettes, optical flow, and pixel values with video observations, resulting in gradient generation for camera, shape, and motion parameter adjustments. Without category-specific shape templates, the approach effectively reconstructs non-rigid 3D structures from videos of humans, animals, and objects of unknown categories. The code will be available at lasr-google.github.io.",1
"With the goal of predicting the future rainfall intensity in a local region over a relatively short period time, precipitation nowcasting has been a long-time scientific challenge with great social and economic impact. The radar echo extrapolation approaches for precipitation nowcasting take radar echo images as input, aiming to generate future radar echo images by learning from the historical images. To effectively handle complex and high non-stationary evolution of radar echoes, we propose to decompose the movement into optical flow field motion and morphologic deformation. Following this idea, we introduce Flow-Deformation Network (FDNet), a neural network that models flow and deformation in two parallel cross pathways. The flow encoder captures the optical flow field motion between consecutive images and the deformation encoder distinguishes the change of shape from the translational motion of radar echoes. We evaluate the proposed network architecture on two real-world radar echo datasets. Our model achieves state-of-the-art prediction results compared with recent approaches. To the best of our knowledge, this is the first network architecture with flow and deformation separation to model the evolution of radar echoes for precipitation nowcasting. We believe that the general idea of this work could not only inspire much more effective approaches but also be applied to other similar spatiotemporal prediction tasks",0
"Precipitation nowcasting has been a scientific challenge with significant social and economic impact as it aims to predict future rainfall intensity in a local region over a short period. One approach to this is the use of radar echo extrapolation, where historical images are used to generate future radar echo images. To handle the complex and high non-stationary evolution of radar echoes, we suggest decomposing the movement into optical flow field motion and morphologic deformation. This idea is implemented in the Flow-Deformation Network (FDNet), a neural network with two parallel pathways that model flow and deformation. The flow encoder captures the optical flow field motion, while the deformation encoder distinguishes the change of shape from translational motion. We evaluate the network on two real-world radar echo datasets and achieve state-of-the-art results. This is the first network architecture to use flow and deformation separation to model the evolution of radar echoes for precipitation nowcasting. We believe that this approach can be applied to other similar spatiotemporal prediction tasks and inspire more effective approaches.",1
"In this paper, a novel video classification method is presented that aims to recognize different categories of third-person videos efficiently. Our motivation is to achieve a light model that could be trained with insufficient training data. With this intuition, the processing of the 3-dimensional video input is broken to 1D in temporal dimension on top of the 2D in spatial. The processes related to 2D spatial frames are being done by utilizing pre-trained networks with no training phase. The only step which involves training is to classify the 1D time series resulted from the description of the 2D signals. As a matter of fact, optical flow images are first calculated from consecutive frames and described by pre-trained CNN networks. Their dimension is then reduced using PCA. By stacking the description vectors beside each other, a multi-channel time series is created for each video. Each channel of the time series represents a specific feature and follows it over time. The main focus of the proposed method is to classify the obtained time series effectively. Towards this, the idea is to let the machine learn temporal features. This is done by training a multi-channel one dimensional Convolutional Neural Network (1D-CNN). The 1D-CNN learns the features along the only temporal dimension. Hence, the number of training parameters decreases significantly which would result in the trainability of the method on even smaller datasets. It is illustrated that the proposed method could reach the state-of-the-art results on two public datasets UCF11, jHMDB and competitive results on HMDB51.",0
"This paper introduces a new approach for efficiently recognizing various categories of third-person videos. The goal is to develop a lightweight model that can be trained with limited data. To achieve this, the 3D video input is processed by breaking it down into 1D in the temporal dimension and 2D in the spatial dimension. The 2D processes are performed using pre-trained networks without any training phase, while the 1D time series resulting from the description of the 2D signals is the only step that requires training for classification. Optical flow images are first calculated from consecutive frames and described by pre-trained CNN networks, and their dimension is then reduced using PCA. By stacking the description vectors, a multi-channel time series is generated for each video, with each channel representing a specific feature that is tracked over time. The proposed method's primary focus is to effectively classify the obtained time series by allowing the machine to learn temporal features. This is done through training a multi-channel 1D-CNN that learns features along the temporal dimension, resulting in significantly fewer training parameters. The proposed method achieves state-of-the-art results on the UCF11 and jHMDB datasets and competitive results on the HMDB51 dataset.",1
"The goal of this paper is to self-train a 3D convolutional neural network on an unlabeled video collection for deployment on small-scale video collections. As smaller video datasets benefit more from motion than appearance, we strive to train our network using optical flow, but avoid its computation during inference. We propose the first motion-augmented self-training regime, we call MotionFit. We start with supervised training of a motion model on a small, and labeled, video collection. With the motion model we generate pseudo-labels for a large unlabeled video collection, which enables us to transfer knowledge by learning to predict these pseudo-labels with an appearance model. Moreover, we introduce a multi-clip loss as a simple yet efficient way to improve the quality of the pseudo-labeling, even without additional auxiliary tasks. We also take into consideration the temporal granularity of videos during self-training of the appearance model, which was missed in previous works. As a result we obtain a strong motion-augmented representation model suited for video downstream tasks like action recognition and clip retrieval. On small-scale video datasets, MotionFit outperforms alternatives for knowledge transfer by 5%-8%, video-only self-supervision by 1%-7% and semi-supervised learning by 9%-18% using the same amount of class labels.",0
"The objective of this research is to train a 3D convolutional neural network without labeled data by using an unlabeled video collection. We aim to deploy this network on smaller video datasets that rely more on motion than appearance. To achieve this, we propose a novel self-training approach called MotionFit, which incorporates motion information without the need for optical flow computation during inference. Firstly, we train a motion model on a limited labeled video dataset and use it to generate pseudo-labels for a larger unlabeled dataset. These pseudo-labels are then used to train an appearance model to predict them. To improve the quality of the pseudo-labeling, we use a multi-clip loss, and we also consider the temporal granularity of videos during self-training. This results in a robust motion-augmented representation model that performs better in video downstream tasks such as clip retrieval and action recognition. Compared to other methods, MotionFit outperforms video-only self-supervision, semi-supervised learning, and alternatives for knowledge transfer by 1%-7%, 9%-18%, and 5%-8%, respectively, using the same amount of class labels on small-scale video datasets.",1
"Computing optical flow is a fundamental problem in computer vision. However, deep learning-based optical flow techniques do not perform well for non-rigid movements such as those found in faces, primarily due to lack of the training data representing the fine facial motion. We hypothesize that learning optical flow on face motion data will improve the quality of predicted flow on faces. The aim of this work is threefold: (1) exploring self-supervised techniques to generate optical flow ground truth for face images; (2) computing baseline results on the effects of using face data to train Convolutional Neural Networks (CNN) for predicting optical flow; and (3) using the learned optical flow in micro-expression recognition to demonstrate its effectiveness. We generate optical flow ground truth using facial key-points in the BP4D-Spontaneous dataset. The generated optical flow is used to train the FlowNetS architecture to test its performance on the generated dataset. The performance of FlowNetS trained on face images surpassed that of other optical flow CNN architectures, demonstrating its usefulness. Our optical flow features are further compared with other methods using the STSTNet micro-expression classifier, and the results indicate that the optical flow obtained using this work has promising applications in facial expression analysis.",0
"The computation of optical flow is a crucial issue in the field of computer vision. However, optical flow techniques based on deep learning do not work effectively for non-rigid movements, such as those seen in facial expressions, mainly because of the lack of training data that accurately represents fine facial motion. We propose that training optical flow on facial motion data will enhance the accuracy of predicted flow on faces. The objective of this study is threefold: firstly, to investigate self-supervised techniques for creating optical flow ground truth for facial images; secondly, to compute baseline results on the impact of using facial data to train Convolutional Neural Networks (CNN) for predicting optical flow; and finally, to demonstrate the effectiveness of the learned optical flow in micro-expression recognition. We create optical flow ground truth using facial key-points in the BP4D-Spontaneous dataset. The generated optical flow is used to train the FlowNetS architecture to evaluate its performance on the created dataset. FlowNetS trained on facial images outperforms other optical flow CNN architectures, indicating its utility. Furthermore, we compare our optical flow features with other methods using the STSTNet micro-expression classifier, and the results suggest that the optical flow produced by this study has promising applications in facial expression analysis.",1
"Tremor is a key diagnostic feature of Parkinson's Disease (PD), Essential Tremor (ET), and other central nervous system (CNS) disorders. Clinicians or trained raters assess tremor severity with TETRAS scores by observing patients. Lacking quantitative measures, inter- or intra- observer variabilities are almost inevitable as the distinction between adjacent tremor scores is subtle. Moreover, clinician assessments also require patient visits, which limits the frequency of disease progress evaluation. Therefore it is beneficial to develop an automated assessment that can be performed remotely and repeatably at patients' convenience for continuous monitoring. In this work, we proposed to train a deep neural network (DNN) with rank-consistent ordinal regression using 276 clinical videos from 36 essential tremor patients. The videos are coupled with clinician assessed TETRAS scores, which are used as ground truth labels to train the DNN. To tackle the challenge of limited training data, optical flows are used to eliminate irrelevant background and statistic objects from RGB frames. In addition to optical flows, transfer learning is also applied to leverage pre-trained network weights from a related task of tremor frequency estimate. The approach was evaluated by splitting the clinical videos into training (67%) and testing sets (0.33%). The mean absolute error on TETRAS score of the testing results is 0.45, indicating that most of the errors were from the mismatch of adjacent labels, which is expected and acceptable. The model predications also agree well with clinical ratings. This model is further applied to smart phone videos collected from a PD patient who has an implanted device to turn ""On"" or ""Off"" tremor. The model outputs were consistent with the patient tremor states. The results demonstrate that our trained model can be used as a means to assess and track tremor severity.",0
"Parkinson's Disease (PD), Essential Tremor (ET), and other central nervous system (CNS) disorders are characterized by tremors, making it a key diagnostic feature. Tremor severity is typically assessed by clinicians or trained raters using TETRAS scores. However, due to the lack of quantitative measures, inter- or intra-observer variabilities are almost inevitable, and clinician assessments also require patient visits, which limits the frequency of disease progress evaluation. Therefore, an automated assessment that can be performed remotely and repeatedly at patients' convenience for continuous monitoring would be beneficial. In this study, we propose to train a deep neural network (DNN) with rank-consistent ordinal regression using 276 clinical videos from 36 essential tremor patients. These videos are paired with clinician-assessed TETRAS scores, which serve as ground truth labels for the DNN. To address the limited training data, we use optical flows to eliminate irrelevant background and statistical objects from RGB frames. Additionally, transfer learning is applied to leverage pre-trained network weights from a related task of tremor frequency estimation. We evaluate the approach by splitting the clinical videos into training (67%) and testing sets (0.33%). The model's mean absolute error on the TETRAS score for the testing results is 0.45, indicating that most of the errors were from the mismatch of adjacent labels, which is expected and acceptable. The model predictions also agree well with clinical ratings. We further apply the model to smartphone videos collected from a PD patient who has an implanted device to turn ""On"" or ""Off"" tremor. The model outputs are consistent with the patient's tremor states, demonstrating that our trained model can be used as a means to assess and track tremor severity.",1
"Deep-learning-based video processing has yielded transformative results in recent years. However, the video analytics pipeline is energy-intensive due to high data rates and reliance on complex inference algorithms, which limits its adoption in energy-constrained applications. Motivated by the observation of high and variable spatial redundancy and temporal dynamics in video data streams, we design and evaluate an adaptive-resolution optimization framework to minimize the energy use of multi-task video analytics pipelines. Instead of heuristically tuning the input data resolution of individual tasks, our framework utilizes deep reinforcement learning to dynamically govern the input resolution and computation of the entire video analytics pipeline. By monitoring the impact of varying resolution on the quality of high-dimensional video analytics features, hence the accuracy of video analytics results, the proposed end-to-end optimization framework learns the best non-myopic policy for dynamically controlling the resolution of input video streams to globally optimize energy efficiency. Governed by reinforcement learning, optical flow is incorporated into the framework to minimize unnecessary spatio-temporal redundancy that leads to re-computation, while preserving accuracy. The proposed framework is applied to video instance segmentation which is one of the most challenging computer vision tasks, and achieves better energy efficiency than all baseline methods of similar accuracy on the YouTube-VIS dataset.",0
"In recent times, video processing using deep learning has shown remarkable progress. However, the video analytics pipeline consumes a lot of energy due to high data rates and complex inference algorithms, which makes it unsuitable for energy-constrained applications. To address this issue, we have developed an adaptive-resolution optimization framework that minimizes the energy consumption of multi-task video analytics pipelines. Rather than manually adjusting the input data resolution of individual tasks, our framework utilizes deep reinforcement learning to dynamically control the input resolution and computation of the entire video analytics pipeline. By assessing the impact of varying resolution on the quality of high-dimensional video analytics features and accuracy of video analytics results, our end-to-end optimization framework learns the best policy for controlling the resolution of input video streams to optimize energy efficiency. Optical flow is incorporated into the framework to minimize unnecessary spatio-temporal redundancy that leads to re-computation, while preserving accuracy. We apply the proposed framework to video instance segmentation, one of the most challenging computer vision tasks, and achieve better energy efficiency than all baseline methods of similar accuracy on the YouTube-VIS dataset.",1
"Synthetic datasets play a critical role in pre-training CNN models for optical flow, but they are painstaking to generate and hard to adapt to new applications. To automate the process, we present AutoFlow, a simple and effective method to render training data for optical flow that optimizes the performance of a model on a target dataset. AutoFlow takes a layered approach to render synthetic data, where the motion, shape, and appearance of each layer are controlled by learnable hyperparameters. Experimental results show that AutoFlow achieves state-of-the-art accuracy in pre-training both PWC-Net and RAFT. Our code and data are available at https://autoflow-google.github.io .",0
"Generating synthetic datasets for pre-training CNN models for optical flow is crucial, but it can be a time-consuming task and challenging to adapt to new applications. To simplify this process, we introduce AutoFlow, a straightforward and efficient method for producing optical flow training data that enhances a model's performance on a specific dataset. AutoFlow uses a layered approach to generate synthetic data, with learnable hyperparameters controlling each layer's motion, shape, and appearance. Our experimental results demonstrate that AutoFlow achieves the highest level of accuracy in pre-training both PWC-Net and RAFT. To access our code and data, visit https://autoflow-google.github.io.",1
"Recently, several Space-Time Memory based networks have shown that the object cues (e.g. video frames as well as the segmented object masks) from the past frames are useful for segmenting objects in the current frame. However, these methods exploit the information from the memory by global-to-global matching between the current and past frames, which lead to mismatching to similar objects and high computational complexity. To address these problems, we propose a novel local-to-local matching solution for semi-supervised VOS, namely Regional Memory Network (RMNet). In RMNet, the precise regional memory is constructed by memorizing local regions where the target objects appear in the past frames. For the current query frame, the query regions are tracked and predicted based on the optical flow estimated from the previous frame. The proposed local-to-local matching effectively alleviates the ambiguity of similar objects in both memory and query frames, which allows the information to be passed from the regional memory to the query region efficiently and effectively. Experimental results indicate that the proposed RMNet performs favorably against state-of-the-art methods on the DAVIS and YouTube-VOS datasets.",0
"In recent times, Space-Time Memory based networks have demonstrated that object cues from previous frames can aid in object segmentation in the current frame. However, these methods rely on global-to-global matching, which results in mismatches between similar objects and high computational complexity. To tackle this issue, we propose Regional Memory Network (RMNet), a semi-supervised VOS solution that employs local-to-local matching. RMNet constructs precise regional memory by remembering local regions where target objects appeared in the past frames. Optic flow from the previous frame is used to track and predict query regions in the current frame. Our local-to-local matching reduces ambiguity between similar objects, enabling efficient and effective transfer of information from regional memory to the query region. Our experimental results show that RMNet outperforms state-of-the-art methods on DAVIS and YouTube-VOS datasets.",1
"Despite the significant progress made by deep learning in natural image matting, there has been so far no representative work on deep learning for video matting due to the inherent technical challenges in reasoning temporal domain and lack of large-scale video matting datasets. In this paper, we propose a deep learning-based video matting framework which employs a novel and effective spatio-temporal feature aggregation module (ST-FAM). As optical flow estimation can be very unreliable within matting regions, ST-FAM is designed to effectively align and aggregate information across different spatial scales and temporal frames within the network decoder. To eliminate frame-by-frame trimap annotations, a lightweight interactive trimap propagation network is also introduced. The other contribution consists of a large-scale video matting dataset with groundtruth alpha mattes for quantitative evaluation and real-world high-resolution videos with trimaps for qualitative evaluation. Quantitative and qualitative experimental results show that our framework significantly outperforms conventional video matting and deep image matting methods applied to video in presence of multi-frame temporal information.",0
"Up until now, there has been a lack of deep learning research on video matting due to the technical difficulties of reasoning in the temporal domain and the shortage of large-scale video matting datasets. Despite the successes of deep learning in natural image matting, our proposed framework introduces a novel spatio-temporal feature aggregation module (ST-FAM) to address these challenges. As optical flow estimation can be unreliable within matting regions, ST-FAM aligns and aggregates information across different spatial scales and temporal frames within the network decoder. Additionally, we introduce a lightweight interactive trimap propagation network to eliminate the need for frame-by-frame trimap annotations. Our framework is evaluated using a large-scale video matting dataset with groundtruth alpha mattes for quantitative analysis and high-resolution videos with trimaps for qualitative evaluation. Results demonstrate that our framework surpasses conventional video matting and deep image matting methods when presented with multi-frame temporal information.",1
"We propose a self-supervised approach for training multi-frame video denoising networks. These networks predict frame t from a window of frames around t. Our self-supervised approach benefits from the video temporal consistency by penalizing a loss between the predicted frame t and a neighboring target frame, which are aligned using an optical flow. We use the proposed strategy for online internal learning, where a pre-trained network is fine-tuned to denoise a new unknown noise type from a single video. After a few frames, the proposed fine-tuning reaches and sometimes surpasses the performance of a state-of-the-art network trained with supervision. In addition, for a wide range of noise types, it can be applied blindly without knowing the noise distribution. We demonstrate this by showing results on blind denoising of different synthetic and realistic noises.",0
"Our method involves a self-supervised approach to train networks for multi-frame video denoising. These networks are designed to predict frame t based on a window of frames surrounding it. Our approach utilizes the temporal consistency of the video by imposing a loss penalty between the predicted frame t and a neighboring target frame, which are aligned using optical flow. We employ this strategy for online internal learning, where a pre-trained network is fine-tuned to denoise a new unknown noise type from a single video. Our fine-tuning method achieves comparable, and in some cases better, results than a state-of-the-art network trained with supervision after only a few frames. Additionally, our approach can be applied blindly to a wide range of noise types without prior knowledge of the noise distribution. Our results demonstrate the effectiveness of our approach for denoising different types of synthetic and realistic noises.",1
"We propose an architecture and training scheme to predict video frames by explicitly modeling dis-occlusions and capturing the evolution of semantically consistent regions in the video. The scene layout (semantic map) and motion (optical flow) are decomposed into layers, which are predicted and fused with their context to generate future layouts and motions. The appearance of the scene is warped from past frames using the predicted motion in co-visible regions; dis-occluded regions are synthesized with content-aware inpainting utilizing the predicted scene layout. The result is a predictive model that explicitly represents objects and learns their class-specific motion, which we evaluate on video prediction benchmarks.",0
"Our proposal presents an architecture and training approach designed to predict video frames with a focus on modeling dis-occlusions and tracking the evolution of semantically consistent regions within the footage. This is achieved by breaking down the scene layout, or semantic map, and motion, or optical flow, into layers. These layers are then predicted and merged with their respective contexts to generate future layouts and motions. Additionally, we employ a technique that warps the appearance of the scene from past frames using the predicted motion in co-visible areas, while dis-occluded regions are synthesized using content-aware inpainting based on the predicted scene layout. Our approach results in a predictive model that explicitly represents objects and their respective class-specific motion. We evaluate the efficacy of our model on video prediction benchmarks.",1
"Video segmentation for the human head and shoulders is essential in creating elegant media for videoconferencing and virtual reality applications. The main challenge is to process high-quality background subtraction in a real-time manner and address the segmentation issues under motion blurs, e.g., shaking the head or waving hands during conference video. To overcome the motion blur problem in video segmentation, we propose a novel flow-based encoder-decoder network (FUNet) that combines both traditional Horn-Schunck optical-flow estimation technique and convolutional neural networks to perform robust real-time video segmentation. We also introduce a video and image segmentation dataset: ConferenceVideoSegmentationDataset. Code and pre-trained models are available on our GitHub repository: \url{https://github.com/kuangzijian/Flow-Based-Video-Matting}.",0
"Creating polished media for videoconferencing and virtual reality requires video segmentation of the human head and shoulders. The primary obstacle is achieving high-quality background subtraction in real-time and addressing segmentation challenges caused by motion blurs, such as head shaking or hand waving during conferences. Our solution to overcome motion blur problems in video segmentation is a unique flow-based encoder-decoder network (FUNet) that merges the Horn-Schunck optical-flow estimation technique with convolutional neural networks to achieve robust and real-time video segmentation. We also provide a ConferenceVideoSegmentationDataset for video and image segmentation and offer access to code and pre-trained models on our GitHub repository (\url{https://github.com/kuangzijian/Flow-Based-Video-Matting}).",1
