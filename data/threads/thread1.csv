"We consider the core reinforcement-learning problem of on-policy value function approximation from a batch of trajectory data, and focus on various issues of Temporal Difference (TD) learning and Monte Carlo (MC) policy evaluation. The two methods are known to achieve complementary bias-variance trade-off properties, with TD tending to achieve lower variance but potentially higher bias. In this paper, we argue that the larger bias of TD can be a result of the amplification of local approximation errors. We address this by proposing an algorithm that adaptively switches between TD and MC in each state, thus mitigating the propagation of errors. Our method is based on learned confidence intervals that detect biases of TD estimates. We demonstrate in a variety of policy evaluation tasks that this simple adaptive algorithm performs competitively with the best approach in hindsight, suggesting that learned confidence intervals are a powerful technique for adapting policy evaluation to use TD or MC returns in a data-driven way.",0
"The main focus of our study is the reinforcement-learning problem of approximating on-policy value functions from a set of trajectory data. We examine the issues associated with Temporal Difference (TD) learning and Monte Carlo (MC) policy evaluation, which are known to have complementary bias-variance trade-off properties. TD has lower variance but potentially higher bias compared to MC. Our research indicates that TD's higher bias can be attributed to the amplification of local approximation errors. To mitigate this, we propose an algorithm that switches between TD and MC in each state, utilizing learned confidence intervals to detect TD estimate biases. Our results demonstrate that this adaptive algorithm performs competitively with the best approach in hindsight, indicating that learned confidence intervals can be a valuable tool for adapting policy evaluation to use TD or MC returns in a data-driven manner.",1
"We propose a unified mechanism for achieving coordination and communication in Multi-Agent Reinforcement Learning (MARL), through rewarding agents for having causal influence over other agents' actions. Causal influence is assessed using counterfactual reasoning. At each timestep, an agent simulates alternate actions that it could have taken, and computes their effect on the behavior of other agents. Actions that lead to bigger changes in other agents' behavior are considered influential and are rewarded. We show that this is equivalent to rewarding agents for having high mutual information between their actions. Empirical results demonstrate that influence leads to enhanced coordination and communication in challenging social dilemma environments, dramatically increasing the learning curves of the deep RL agents, and leading to more meaningful learned communication protocols. The influence rewards for all agents can be computed in a decentralized way by enabling agents to learn a model of other agents using deep neural networks. In contrast, key previous works on emergent communication in the MARL setting were unable to learn diverse policies in a decentralized manner and had to resort to centralized training. Consequently, the influence reward opens up a window of new opportunities for research in this area.",0
"Our proposal aims to establish a unified approach to achieve coordination and communication in Multi-Agent Reinforcement Learning (MARL) by rewarding agents for their ability to causally influence the actions of other agents. This assessment is carried out through counterfactual reasoning, where an agent considers alternative actions it could have taken and evaluates their impact on the behavior of other agents. Actions that cause significant changes to the behavior of other agents are deemed influential and rewarded accordingly. We demonstrate that this approach is equivalent to rewarding agents for having high mutual information between their actions. Our experiments reveal that this influence-based approach leads to improved coordination and communication in challenging social dilemma environments, resulting in remarkable learning curves for deep RL agents and meaningful communication protocols. The influence rewards can be computed in a decentralized manner by enabling agents to learn a model of other agents using deep neural networks. This is in contrast to previous works on emergent communication in the MARL setting, which were unable to learn diverse policies in a decentralized fashion and had to rely on centralized training. Therefore, this influence-based reward system creates new possibilities for research in this area.",1
"Efficient exploration is necessary to achieve good sample efficiency for reinforcement learning in general. From small, tabular settings such as gridworlds to large, continuous and sparse reward settings such as robotic object manipulation tasks, exploration through adding an uncertainty bonus to the reward function has been shown to be effective when the uncertainty is able to accurately drive exploration towards promising states. However reward bonuses can still be inefficient since they are non-stationary, which means that we must wait for function approximators to catch up and converge again when uncertainties change. We propose the idea of directed exploration, that is learning a goal-conditioned policy where goals are simply other states, and using that to directly try to reach states with large uncertainty. The goal-conditioned policy is independent of uncertainty and is thus stationary. We show in our experiments how directed exploration is more efficient at exploration and more robust to how the uncertainty is computed than adding bonuses to rewards.",0
"To achieve good sample efficiency in reinforcement learning, efficient exploration is crucial. From small tabular settings like gridworlds to larger, continuous and sparse reward settings like robotic object manipulation tasks, exploration by adding an uncertainty bonus to the reward function has been found effective in driving exploration towards promising states. However, using reward bonuses can still be inefficient as they are non-stationary, requiring function approximators to catch up and converge again when uncertainties change. To overcome this, we propose the concept of directed exploration - learning a goal-conditioned policy where goals are other states, and using it to directly reach states with high uncertainty. The goal-conditioned policy is independent of uncertainty and therefore stationary. Our experiments demonstrate that directed exploration is more efficient in exploration and more robust in how uncertainty is computed than adding bonuses to rewards.",1
"In this work, we describe practical lessons we have learned from successfully using contextual bandits (CBs) to improve key business metrics of the Microsoft Virtual Agent for customer support. While our current use cases focus on single step einforcement learning (RL) and mostly in the domain of natural language processing and information retrieval we believe many of our findings are generally applicable. Through this article, we highlight certain issues that RL practitioners may encounter in similar types of applications as well as offer practical solutions to these challenges.",0
"Our work details the practical insights we gained by employing contextual bandits (CBs) to enhance the vital business metrics of the Microsoft Virtual Agent for customer support. Although our current usage scenarios mainly concentrate on natural language processing and information retrieval single step reinforcement learning (RL), we are convinced that our discoveries have broader relevance. Our article addresses specific problems that RL experts may face in comparable applications and provides effective solutions to overcome these hurdles.",1
"In real-world applications of reinforcement learning (RL), noise from inherent stochasticity of environments is inevitable. However, current policy evaluation algorithms, which plays a key role in many RL algorithms, are either prone to noise or inefficient. To solve this issue, we introduce a novel policy evaluation algorithm, which we call Gap-increasing RetrAce Policy Evaluation (GRAPE). It leverages two recent ideas: (1) gap-increasing value update operators in advantage learning for noise-tolerance and (2) off-policy eligibility trace in Retrace algorithm for efficient learning. We provide detailed theoretical analysis of the new algorithm that shows its efficiency and noise-tolerance inherited from Retrace and advantage learning. Furthermore, our analysis shows that GRAPE's learning is significantly efficient than that of a simple learning-rate-based approach while keeping the same level of noise-tolerance. We applied GRAPE to control problems and obtained experimental results supporting our theoretical analysis.",0
"In reinforcement learning (RL) applications, noise caused by stochastic environments is unavoidable, but current policy evaluation algorithms, which are essential to RL algorithms, are either inefficient or susceptible to noise. To address this issue, we have developed a new policy evaluation algorithm, called Gap-increasing RetrAce Policy Evaluation (GRAPE), which incorporates two recent concepts: gap-increasing value update operators for noise tolerance in advantage learning, and off-policy eligibility trace in Retrace algorithm for efficient learning. We have conducted a detailed theoretical analysis of GRAPE, which demonstrates its efficiency and noise tolerance inherited from Retrace and advantage learning. Our analysis also shows that GRAPE's learning is significantly more efficient than a simple learning-rate-based approach while maintaining the same level of noise tolerance. We have tested GRAPE on control problems and obtained experimental results that support our theoretical analysis.",1
"The growing interest in both the automation of machine learning and deep learning has inevitably led to the development of a wide variety of automated methods for neural architecture search. The choice of the network architecture has proven to be critical, and many advances in deep learning spring from its immediate improvements. However, deep learning techniques are computationally intensive and their application requires a high level of domain knowledge. Therefore, even partial automation of this process helps to make deep learning more accessible to both researchers and practitioners. With this survey, we provide a formalism which unifies and categorizes the landscape of existing methods along with a detailed analysis that compares and contrasts the different approaches. We achieve this via a comprehensive discussion of the commonly adopted architecture search spaces and architecture optimization algorithms based on principles of reinforcement learning and evolutionary algorithms along with approaches that incorporate surrogate and one-shot models. Additionally, we address the new research directions which include constrained and multi-objective architecture search as well as automated data augmentation, optimizer and activation function search.",0
"The increasing interest in automating machine and deep learning has resulted in the creation of various automated techniques for neural architecture search. Selecting the right network architecture is crucial, and enhancements in deep learning mostly stem from its immediate improvements. However, implementing deep learning methods is computationally demanding and requires extensive domain knowledge. As a result, even partial automation of this process can make deep learning more accessible to researchers and professionals. Through our survey, we present a framework that unites and categorizes the existing methods, accompanied by an extensive analysis that compares and contrasts the various approaches. We accomplish this by comprehensively discussing the commonly used architecture search spaces and optimization algorithms based on principles of reinforcement learning and evolutionary algorithms, incorporating surrogate and one-shot models. Additionally, we explore new research directions, such as constrained and multi-objective architecture search, automated data augmentation, optimizer, and activation function search.",1
"Imitation from observation is the framework of learning tasks by observing demonstrated state-only trajectories. Recently, adversarial approaches have achieved significant performance improvements over other methods for imitating complex behaviors. However, these adversarial imitation algorithms often require many demonstration examples and learning iterations to produce a policy that is successful at imitating a demonstrator's behavior. This high sample complexity often prohibits these algorithms from being deployed on physical robots. In this paper, we propose an algorithm that addresses the sample inefficiency problem by utilizing ideas from trajectory centric reinforcement learning algorithms. We test our algorithm and conduct experiments using an imitation task on a physical robot arm and its simulated version in Gazebo and will show the improvement in learning rate and efficiency.",0
"The basis of learning tasks through observation is to imitate what is seen. Lately, adversarial methods have outperformed other techniques in replicating complex behaviors. However, the drawback is that these approaches often require a large number of examples and iterations to develop a successful policy. Consequently, these algorithms are not practical for deployment on physical robots due to their high sample complexity. In this article, we propose a solution to this issue by incorporating trajectory-centric reinforcement learning techniques. Our algorithm is tested through an imitation task on a physical robot arm and its simulated version in Gazebo. We demonstrate the effectiveness of our approach by showing improvements in learning rate and efficiency.",1
"We aim to jointly optimize antenna tilt angle, and vertical and horizontal half-power beamwidths of the macrocells in a heterogeneous cellular network (HetNet). The interactions between the cells, most notably due to their coupled interference render this optimization prohibitively complex. Utilizing a single agent reinforcement learning (RL) algorithm for this optimization becomes quite suboptimum despite its scalability, whereas multi-agent RL algorithms yield better solutions at the expense of scalability. Hence, we propose a compromise algorithm between these two. Specifically, a multi-agent mean field RL algorithm is first utilized in the offline phase so as to transfer information as features for the second (online) phase single agent RL algorithm, which employs a deep neural network to learn users locations. This two-step approach is a practical solution for real deployments, which should automatically adapt to environmental changes in the network. Our results illustrate that the proposed algorithm approaches the performance of the multi-agent RL, which requires millions of trials, with hundreds of online trials, assuming relatively low environmental dynamics, and performs much better than a single agent RL. Furthermore, the proposed algorithm is compact and implementable, and empirically appears to provide a performance guarantee regardless of the amount of environmental dynamics.",0
"Our goal is to optimize the antenna tilt angle, as well as the vertical and horizontal half-power beamwidths of macrocells in a heterogeneous cellular network (HetNet). However, this optimization is complicated by the interactions between cells, particularly the coupled interference. While a single agent reinforcement learning (RL) algorithm is scalable, it is not optimal, and multi-agent RL algorithms are better but less scalable. We propose a compromise algorithm that uses a multi-agent mean field RL algorithm in the offline phase to transfer information as features to the second (online) phase single agent RL algorithm. The latter uses a deep neural network to learn users' locations. This two-step approach is adaptable to environmental changes in the network and is a practical solution for real deployments. Our results show that the proposed algorithm performs nearly as well as the multi-agent RL with millions of trials, with only hundreds of online trials, assuming low environmental dynamics. It also performs better than a single agent RL. The proposed algorithm is compact, implementable, and provides a performance guarantee regardless of the amount of environmental dynamics.",1
"This paper presents novel mixed-type Bayesian optimization (BO) algorithms to accelerate the optimization of a target objective function by exploiting correlated auxiliary information of binary type that can be more cheaply obtained, such as in policy search for reinforcement learning and hyperparameter tuning of machine learning models with early stopping. To achieve this, we first propose a mixed-type multi-output Gaussian process (MOGP) to jointly model the continuous target function and binary auxiliary functions. Then, we propose information-based acquisition functions such as mixed-type entropy search (MT-ES) and mixed-type predictive ES (MT-PES) for mixed-type BO based on the MOGP predictive belief of the target and auxiliary functions. The exact acquisition functions of MT-ES and MT-PES cannot be computed in closed form and need to be approximated. We derive an efficient approximation of MT-PES via a novel mixed-type random features approximation of the MOGP model whose cross-correlation structure between the target and auxiliary functions can be exploited for improving the belief of the global target maximizer using observations from evaluating these functions. We propose new practical constraints to relate the global target maximizer to the binary auxiliary functions. We empirically evaluate the performance of MT-ES and MT-PES with synthetic and real-world experiments.",0
"In this article, we introduce innovative Bayesian optimization (BO) algorithms that combine binary auxiliary information with a target objective function to improve optimization speed. This approach is particularly useful in scenarios where the auxiliary information is easier and cheaper to obtain, such as policy search for reinforcement learning and hyperparameter tuning of machine learning models with early stopping. Our methodology involves using a mixed-type multi-output Gaussian process (MOGP) to jointly model the target function and binary auxiliary functions, followed by the development of information-based acquisition functions like mixed-type entropy search (MT-ES) and mixed-type predictive ES (MT-PES) for mixed-type BO. While the exact acquisition functions for MT-ES and MT-PES cannot be computed, we derive an efficient approximation of MT-PES via a mixed-type random features approximation of the MOGP model. Furthermore, we introduce practical constraints to establish a relationship between the global target maximizer and binary auxiliary functions. Finally, we assess the performance of our algorithms using both synthetic and real-world experiments.",1
"We propose a lifelong learning architecture, the Neural Computer Agent (NCA), where a Reinforcement Learning agent is paired with a predictive model of the environment learned by a Differentiable Neural Computer (DNC). The agent and DNC model are trained in conjunction iteratively. The agent improves its policy in simulations generated by the DNC model and rolls out the policy to the live environment, collecting experiences in new portions or tasks of the environment for further learning. Experiments in two synthetic environments show that DNC models can continually learn from pixels alone to simulate new tasks as they are encountered by the agent, while the agents can be successfully trained to solve the tasks using Proximal Policy Optimization entirely in simulations.",0
"Our proposal is to use the Neural Computer Agent (NCA) as a lifelong learning architecture. This involves pairing a Reinforcement Learning agent with a predictive model of the environment, which is learned by a Differentiable Neural Computer (DNC). The agent and DNC model are trained together in a continuous manner. The agent improves its policy by simulating the DNC model and then applies this policy to the live environment, collecting experiences from new tasks for further learning. Our experiments in two synthetic environments show that DNC models can learn continually from pixels alone and simulate new tasks as they are encountered by the agent. The agents are trained successfully to solve the tasks in simulations using Proximal Policy Optimization.",1
"We propose a method for tackling catastrophic forgetting in deep reinforcement learning that is \textit{agnostic} to the timescale of changes in the distribution of experiences, does not require knowledge of task boundaries, and can adapt in \textit{continuously} changing environments. In our \textit{policy consolidation} model, the policy network interacts with a cascade of hidden networks that simultaneously remember the agent's policy at a range of timescales and regularise the current policy by its own history, thereby improving its ability to learn without forgetting. We find that the model improves continual learning relative to baselines on a number of continuous control tasks in single-task, alternating two-task, and multi-agent competitive self-play settings.",0
"Our proposed solution to address the problem of catastrophic forgetting in deep reinforcement learning is flexible enough to adapt to changes in the distribution of experiences and does not require prior knowledge of task boundaries. Additionally, it can function effectively in environments that undergo continuous changes. Our policy consolidation model involves the policy network interacting with a cascade of hidden networks that remember the agent's policy at various timescales and improve its learning ability without forgetting previous knowledge. Through our experiments on several continuous control tasks, we have observed that our model outperforms baselines in single-task, alternating two-task, and multi-agent competitive self-play settings.",1
"Curriculum learning is often employed in deep reinforcement learning to let the agent progress more quickly towards better behaviors. Numerical methods for curriculum learning in the literature provides only initial heuristic solutions, with little to no guarantee on their quality. We define a new gray-box function that, including a suitable scheduling problem, can be effectively used to reformulate the curriculum learning problem. We propose different efficient numerical methods to address this gray-box reformulation. Preliminary numerical results on a benchmark task in the curriculum learning literature show the viability of the proposed approach.",0
"To facilitate faster progress towards improved behaviors, deep reinforcement learning frequently makes use of curriculum learning. However, the existing literature only offers preliminary heuristic solutions with no assurance of their effectiveness. To address this, we introduce a grey-box function that can be utilized to effectively reformulate the problem of curriculum learning, incorporating an appropriate scheduling problem. We propose multiple efficient numerical methods to tackle this grey-box reformulation. Initial numerical testing on a benchmark task from the curriculum learning literature demonstrates the feasibility of our proposed technique.",1
"Stochastic approximation (SA) is a key method used in statistical learning. Recently, its non-asymptotic convergence analysis has been considered in many papers. However, most of the prior analyses are made under restrictive assumptions such as unbiased gradient estimates and convex objective function, which significantly limit their applications to sophisticated tasks such as online and reinforcement learning. These restrictions are all essentially relaxed in this work. In particular, we analyze a general SA scheme to minimize a non-convex, smooth objective function. We consider update procedure whose drift term depends on a state-dependent Markov chain and the mean field is not necessarily of gradient type, covering approximate second-order method and allowing asymptotic bias for the one-step updates. We illustrate these settings with the online EM algorithm and the policy-gradient method for average reward maximization in reinforcement learning.",0
"The technique of stochastic approximation (SA) is a crucial tool in statistical learning, and has recently been the subject of analysis in multiple studies to determine its non-asymptotic convergence. However, previous analyses have been limited by strict assumptions such as unbiased gradient estimates and convex objective functions, making them unsuitable for complex tasks like online and reinforcement learning. This paper relaxes these restrictions and presents a general SA approach that can minimize a non-convex, smooth objective function. The update procedure is designed with a state-dependent Markov chain drift term, and the mean field is not necessarily of gradient type, thus allowing for approximate second-order methods and asymptotic bias for one-step updates. The online EM algorithm and the policy-gradient method for average reward maximization in reinforcement learning are used to demonstrate these concepts.",1
"Heuristic algorithms such as simulated annealing, Concorde, and METIS are effective and widely used approaches to find solutions to combinatorial optimization problems. However, they are limited by the high sample complexity required to reach a reasonable solution from a cold-start. In this paper, we introduce a novel framework to generate better initial solutions for heuristic algorithms using reinforcement learning (RL), named RLHO. We augment the ability of heuristic algorithms to greedily improve upon an existing initial solution generated by RL, and demonstrate novel results where RL is able to leverage the performance of heuristics as a learning signal to generate better initialization.   We apply this framework to Proximal Policy Optimization (PPO) and Simulated Annealing (SA). We conduct a series of experiments on the well-known NP-complete bin packing problem, and show that the RLHO method outperforms our baselines. We show that on the bin packing problem, RL can learn to help heuristics perform even better, allowing us to combine the best parts of both approaches.",0
"Combinatorial optimization problems are commonly solved using heuristic algorithms like Concorde, simulated annealing, and METIS. However, these algorithms require a high sample complexity to reach a reasonable solution from a cold-start, limiting their effectiveness. To address this issue, we propose a new framework called RLHO that uses reinforcement learning (RL) to generate better initial solutions for heuristic algorithms. By augmenting the ability of heuristic algorithms to improve upon initial solutions generated by RL, we demonstrate how RL can leverage the performance of heuristics as a learning signal to produce better initialization. Our experiments on the bin packing problem show that RLHO outperforms our baselines, and that the combination of RL and heuristics results in even better performance. Specifically, we apply this framework to Proximal Policy Optimization (PPO) and Simulated Annealing (SA) to achieve superior results.",1
"Human ability at solving complex tasks is helped by priors on object and event semantics of their environment. This paper investigates the use of similar prior knowledge for transfer learning in Reinforcement Learning agents. In particular, the paper proposes to use a first-order-logic language grounded in deep neural networks to represent facts about objects and their semantics in the real world. Facts are provided as background knowledge a priori to learning a policy for how to act in the world. The priors are injected with the conventional input in a single agent architecture. As proof-of-concept, the paper tests the system in simple experiments that show the importance of symbolic abstraction and flexible fact derivation. The paper shows that the proposed system can learn to take advantage of both the symbolic layer and the image layer in a single decision selection module.",0
"This paper explores how humans' ability to solve complex tasks is aided by their prior knowledge of object and event semantics in their environment. The authors investigate the potential of utilizing similar prior knowledge for transfer learning in Reinforcement Learning agents. They propose using a first-order-logic language that is grounded in deep neural networks to represent facts about real-world objects and their semantics. These facts are provided as background knowledge prior to learning how to navigate the world. The priors are integrated with conventional input in a single agent architecture. To demonstrate the effectiveness of this approach, the authors conduct simple experiments that highlight the importance of symbolic abstraction and flexible fact derivation. The results indicate that the proposed system can effectively utilize both the symbolic and image layers in a single decision selection module.",1
"We develop a framework for interacting with uncertain environments in reinforcement learning (RL) by leveraging preferences in the form of utility functions. We claim that there is value in considering different risk measures during learning. In this framework, the preference for risk can be tuned by variation of the parameter $\beta$ and the resulting behavior can be risk-averse, risk-neutral or risk-taking depending on the parameter choice. We evaluate our framework for learning problems with model uncertainty. We measure and control for \emph{epistemic} risk using dynamic programming (DP) and policy gradient-based algorithms. The risk-averse behavior is then compared with the behavior of the optimal risk-neutral policy in environments with epistemic risk.",0
"Using utility functions, we have developed a framework for reinforcement learning (RL) that can effectively interact with uncertain environments. Our approach involves considering different risk measures during the learning process. By adjusting the parameter $\beta$, we can tune the level of risk preference and achieve behavior that is either risk-averse, risk-neutral or risk-taking. We have applied our framework to learning problems that involve model uncertainty and have measured and controlled for \emph{epistemic} risk using dynamic programming (DP) and policy gradient-based algorithms. Our results show that the risk-averse behavior is comparable to that of the optimal risk-neutral policy in environments with epistemic risk.",1
"Real-world applications require RL algorithms to act safely. During learning process, it is likely that the agent executes sub-optimal actions that may lead to unsafe/poor states of the system. Exploration is particularly brittle in high-dimensional state/action space due to increased number of low-performing actions. In this work, we consider risk-averse exploration in approximate RL setting. To ensure safety during learning, we propose the distributionally robust policy iteration scheme that provides lower bound guarantee on state-values. Our approach induces a dynamic level of risk to prevent poor decisions and yet preserves the convergence to the optimal policy. Our formulation results in a efficient algorithm that accounts for a simple re-weighting of policy actions in the standard policy iteration scheme. We extend our approach to continuous state/action space and present a practical algorithm, distributionally robust soft actor-critic, that implements a different exploration strategy: it acts conservatively at short-term and it explores optimistically in a long-run. We provide promising experimental results on continuous control tasks.",0
"RL algorithms used in real-world applications must prioritize safety. While learning, the agent may perform sub-optimal actions that lead to unsafe or poor system states. Exploration is especially difficult in high-dimensional state/action spaces because of the large number of low-performing actions. This study introduces risk-averse exploration in the approximate RL setting. To ensure safety during learning, the distributionally robust policy iteration scheme with a lower bound guarantee on state-values is proposed. This approach balances risk dynamically to prevent poor decisions while maintaining convergence to the optimal policy. The algorithm is efficient and involves a simple re-weighting of policy actions in the standard policy iteration scheme. The approach is extended to continuous state/action spaces in the distributionally robust soft actor-critic algorithm. This algorithm explores conservatively in the short-term and optimistically in the long-run. The experimental results on continuous control tasks are promising.",1
"Importance-weighted risk minimization is a key ingredient in many machine learning algorithms for causal inference, domain adaptation, class imbalance, and off-policy reinforcement learning. While the effect of importance weighting is well-characterized for low-capacity misspecified models, little is known about how it impacts over-parameterized, deep neural networks. This work is inspired by recent theoretical results showing that on (linearly) separable data, deep linear networks optimized by SGD learn weight-agnostic solutions, prompting us to ask, for realistic deep networks, for which many practical datasets are separable, what is the effect of importance weighting? We present the surprising finding that while importance weighting impacts models early in training, its effect diminishes over successive epochs. Moreover, while L2 regularization and batch normalization (but not dropout), restore some of the impact of importance weighting, they express the effect via (seemingly) the wrong abstraction: why should practitioners tweak the L2 regularization, and by how much, to produce the correct weighting effect? Our experiments confirm these findings across a range of architectures and datasets.",0
"Many machine learning algorithms rely on importance-weighted risk minimization to achieve optimal results in areas such as causal inference, domain adaptation, class imbalance, and off-policy reinforcement learning. Although the impact of importance weighting on low-capacity misspecified models is well-established, little is known about its effects on over-parameterized deep neural networks. This study was motivated by recent theoretical findings showing that deep linear networks optimized by SGD learn weight-agnostic solutions on (linearly) separable data. The researchers investigated the effect of importance weighting on realistic deep networks, which are separable in many practical datasets. They discovered that while importance weighting has an impact on models early in training, its effect diminishes over successive epochs. The study also found that L2 regularization and batch normalization, but not dropout, restore some of the impact of importance weighting. However, these techniques express the effect via the wrong abstraction, leaving practitioners unsure of how much L2 regularization to use to achieve the desired weighting effect. The experiments conducted in this study confirm these findings across a range of architectures and datasets.",1
"Efficient exploration is an unsolved problem in Reinforcement Learning which is usually addressed by reactively rewarding the agent for fortuitously encountering novel situations. This paper introduces an efficient active exploration algorithm, Model-Based Active eXploration (MAX), which uses an ensemble of forward models to plan to observe novel events. This is carried out by optimizing agent behaviour with respect to a measure of novelty derived from the Bayesian perspective of exploration, which is estimated using the disagreement between the futures predicted by the ensemble members. We show empirically that in semi-random discrete environments where directed exploration is critical to make progress, MAX is at least an order of magnitude more efficient than strong baselines. MAX scales to high-dimensional continuous environments where it builds task-agnostic models that can be used for any downstream task.",0
"The issue of efficient exploration in Reinforcement Learning remains unresolved, and typically, agents are rewarded for coming across new situations by chance. However, this paper proposes a solution to this problem with an active exploration algorithm called Model-Based Active eXploration (MAX). MAX utilizes a group of forward models to plan and observe novel events, optimizing the agent's behavior based on a novelty measure derived from the Bayesian approach to exploration. This measure is estimated by measuring the disagreement between the futures predicted by the ensemble members. Our empirical findings indicate that in semi-random discrete environments where targeted exploration is necessary for progress, MAX is at least ten times more efficient than robust baseline methods. Additionally, MAX can scale to high-dimensional continuous environments, where it creates task-agnostic models that can be applied to any downstream task.",1
"In a discounted reward Markov Decision Process (MDP), the objective is to find the optimal value function, i.e., the value function corresponding to an optimal policy. This problem reduces to solving a functional equation known as the Bellman equation and a fixed point iteration scheme known as the value iteration is utilized to obtain the solution. In literature, a successive over-relaxation based value iteration scheme is proposed to speed-up the computation of the optimal value function. The speed-up is achieved by constructing a modified Bellman equation that ensures faster convergence to the optimal value function. However, in many practical applications, the model information is not known and we resort to Reinforcement Learning (RL) algorithms to obtain optimal policy and value function. One such popular algorithm is Q-learning. In this paper, we propose Successive Over-Relaxation (SOR) Q-learning. We first derive a modified fixed point iteration for SOR Q-values and utilize stochastic approximation to derive a learning algorithm to compute the optimal value function and an optimal policy. We then prove the almost sure convergence of the SOR Q-learning to SOR Q-values. Finally, through numerical experiments, we show that SOR Q-learning is faster compared to the standard Q-learning algorithm.",0
"The goal of a Markov Decision Process (MDP) with discounted rewards is to determine the optimum value function, which corresponds to the best policy. This involves solving the Bellman equation using value iteration. Researchers have developed a modified version of value iteration called Successive Over-Relaxation (SOR) to speed up the computation of the optimal value function. However, in practical applications, the model is often unknown, and Reinforcement Learning (RL) algorithms, such as Q-learning, are used instead. This paper introduces a new algorithm called SOR Q-learning, which uses a modified fixed point iteration and stochastic approximation to determine the optimal value function and policy. The paper proves that SOR Q-learning almost surely converges to SOR Q-values and provides numerical experiments demonstrating its faster performance compared to standard Q-learning.",1
"We present SmartChoices, an approach to making machine learning (ML) a first class citizen in programming languages which we see as one way to lower the entrance cost to applying ML to problems in new domains. There is a growing divide in approaches to building systems: on the one hand, programming leverages human experts to define a system while on the other hand behavior is learned from data in machine learning. We propose to hybridize these two by providing a 3-call API which we expose through an object called SmartChoice. We describe the SmartChoices-interface, how it can be used in programming with minimal code changes, and demonstrate that it is an easy to use but still powerful tool by demonstrating improvements over not using ML at all on three algorithmic problems: binary search, QuickSort, and caches. In these three examples, we replace the commonly used heuristics with an ML model entirely encapsulated within a SmartChoice and thus requiring minimal code changes. As opposed to previous work applying ML to algorithmic problems, our proposed approach does not require to drop existing implementations but seamlessly integrates into the standard software development workflow and gives full control to the software developer over how ML methods are applied. Our implementation relies on standard Reinforcement Learning (RL) methods. To learn faster, we use the heuristic function, which they are replacing, as an initial function. We show how this initial function can be used to speed up and stabilize learning while providing a safety net that prevents performance to become substantially worse -- allowing for a safe deployment in critical applications in real life.",0
"Our approach, called SmartChoices, aims to integrate machine learning (ML) into programming languages to make it more accessible for problem-solving in new domains. Currently, there is a divide in system-building approaches between programming, which relies on human input, and ML, which learns from data. We propose a hybrid solution by introducing the SmartChoice object and a 3-call API that allows for minimal code changes. This tool can be easily used to improve the performance of algorithms, such as binary search, QuickSort, and caches, by replacing commonly used heuristics with an encapsulated ML model within SmartChoice. Unlike previous work, our approach does not require the abandonment of existing implementations and can be seamlessly integrated into the standard software development workflow. Our implementation uses standard Reinforcement Learning (RL) methods and leverages the heuristic function as an initial function to speed up and stabilize the learning process. This approach ensures a safe deployment in critical applications without compromising performance.",1
"We model human decision-making behaviors in a risk-taking task using inverse reinforcement learning (IRL) for the purposes of understanding real human decision making under risk. To the best of our knowledge, this is the first work applying IRL to reveal the implicit reward function in human risk-taking decision making and to interpret risk-prone and risk-averse decision-making policies. We hypothesize that the state history (e.g. rewards and decisions in previous trials) are related to the human reward function, which leads to risk-averse and risk-prone decisions. We design features that reflect these factors in the reward function of IRL and learn the corresponding weight that is interpretable as the importance of features. The results confirm the sub-optimal risk-related decisions of human-driven by the personalized reward function. In particular, the risk-prone person tends to decide based on the current pump number, while the risk-averse person relies on burst information from the previous trial and the average end status. Our results demonstrate that IRL is an effective tool to model human decision-making behavior, as well as to help interpret the human psychological process in risk decision-making.",0
"We utilized inverse reinforcement learning (IRL) to simulate human decision-making behaviors in a risk-taking task in order to gain insight into actual human decision-making under risk. This is the first study that applies IRL to reveal the implicit reward function in human risk-taking decision making and to interpret risk-averse and risk-prone decision-making policies. Our hypothesis is that the state history, such as previous rewards and decisions, is linked to the human reward function, leading to risk-averse and risk-prone decisions. We developed features that reflect these factors in the reward function of IRL and determined the corresponding weight, which is interpretable as the importance of features. Our findings confirm that human-driven sub-optimal risk-related decisions are influenced by personalized reward functions. Specifically, risk-prone individuals tend to base decisions on the current pump number, while risk-averse individuals rely on burst information from the previous trial and the average end status. Our results demonstrate that IRL is an effective tool for modeling human decision-making behavior and to help interpret the human psychological process in risk decision-making.",1
"Curriculum learning in reinforcement learning is used to shape exploration by presenting the agent with increasingly complex tasks. The idea of curriculum learning has been largely applied in both animal training and pedagogy. In reinforcement learning, all previous task sequencing methods have shaped exploration with the objective of reducing the time to reach a given performance level. We propose novel uses of curriculum learning, which arise from choosing different objective functions. Furthermore, we define a general optimization framework for task sequencing and evaluate the performance of popular metaheuristic search methods on several tasks. We show that curriculum learning can be successfully used to: improve the initial performance, take fewer suboptimal actions during exploration, and discover better policies.",0
"The concept of curriculum learning in reinforcement learning involves gradually introducing the agent to more complex tasks in order to guide its exploration. This approach has been utilized in animal training and pedagogy, and previous methods for task sequencing in reinforcement learning have aimed to minimize the time required to achieve a certain level of performance. Our proposal suggests alternative uses of curriculum learning by varying the objective functions, and we also introduce a general optimization framework for task sequencing. Through evaluating metaheuristic search methods on various tasks, we demonstrate the efficacy of curriculum learning in enhancing initial performance, reducing suboptimal actions during exploration, and uncovering superior policies.",1
"Curriculum learning has been successfully used in reinforcement learning to accelerate the learning process, through knowledge transfer between tasks of increasing complexity. Critical tasks, in which suboptimal exploratory actions must be minimized, can benefit from curriculum learning, and its ability to shape exploration through transfer. We propose a task sequencing algorithm maximizing the cumulative return, that is, the return obtained by the agent across all the learning episodes. By maximizing the cumulative return, the agent not only aims at achieving high rewards as fast as possible, but also at doing so while limiting suboptimal actions. We experimentally compare our task sequencing algorithm to several popular metaheuristic algorithms for combinatorial optimization, and show that it achieves significantly better performance on the problem of cumulative return maximization. Furthermore, we validate our algorithm on a critical task, optimizing a home controller for a micro energy grid.",0
"The process of learning through transfer of knowledge between tasks of increasing complexity, known as curriculum learning, has proven successful in reinforcement learning for faster learning. Curriculum learning can be particularly useful for critical tasks where minimizing suboptimal exploratory actions is crucial, as it can shape exploration through transfer. Our proposal is an algorithm for sequencing tasks that maximizes the cumulative return - the return obtained by the agent across all learning episodes. This algorithm aims to not only achieve high rewards quickly, but also limit suboptimal actions. To test our algorithm's effectiveness, we compared it to several popular metaheuristic algorithms for combinatorial optimization and found that it outperformed them in maximizing the cumulative return. Additionally, we validated our algorithm in a real-world application - optimizing a home controller for a micro energy grid.",1
"Continuous action policy search is currently the focus of intensive research, driven both by the recent success of deep reinforcement learning algorithms and the emergence of competitors based on evolutionary algorithms. In this paper, we present a broad survey of policy search methods, providing a unified perspective on very different approaches, including also Bayesian Optimization and directed exploration methods. The main message of this overview is in the relationship between the families of methods, but we also outline some factors underlying sample efficiency properties of the various approaches.",0
"Intense research is currently centered on continuous action policy search, fueled by the success of deep reinforcement learning algorithms and the emergence of evolutionary algorithm-based competitors. This article presents a comprehensive review of policy search methods, offering a unified perspective on diverse approaches, including Bayesian Optimization and directed exploration methods. The primary objective of this overview is to highlight the connection between the different techniques, while also outlining the factors that contribute to the sample efficiency properties of each approach.",1
"Exploration in reinforcement learning (RL) suffers from the curse of dimensionality when the state-action space is large. A common practice is to parameterize the high-dimensional value and policy functions using given features. However existing methods either have no theoretical guarantee or suffer a regret that is exponential in the planning horizon $H$. In this paper, we propose an online RL algorithm, namely the MatrixRL, that leverages ideas from linear bandit to learn a low-dimensional representation of the probability transition model while carefully balancing the exploitation-exploration tradeoff. We show that MatrixRL achieves a regret bound ${O}\big(H^2d\log T\sqrt{T}\big)$ where $d$ is the number of features. MatrixRL has an equivalent kernelized version, which is able to work with an arbitrary kernel Hilbert space without using explicit features. In this case, the kernelized MatrixRL satisfies a regret bound ${O}\big(H^2\widetilde{d}\log T\sqrt{T}\big)$, where $\widetilde{d}$ is the effective dimension of the kernel space. To our best knowledge, for RL using features or kernels, our results are the first regret bounds that are near-optimal in time $T$ and dimension $d$ (or $\widetilde{d}$) and polynomial in the planning horizon $H$.",0
"When the state-action space is large, exploration in reinforcement learning (RL) is hindered by the curse of dimensionality. To overcome this, value and policy functions can be parameterized using given features. However, existing methods have either no theoretical guarantee or experience exponential regret in the planning horizon H. This paper introduces the MatrixRL algorithm, which uses ideas from linear bandit to learn a low-dimensional representation of the probability transition model while balancing the exploitation-exploration tradeoff. MatrixRL achieves a regret bound of O(H^2dlogTsqrt(T)), where d is the number of features. A kernelized version of MatrixRL is also presented, which works with an arbitrary kernel Hilbert space without explicit features. This version has a regret bound of O(H^2tilde{d}logTsqrt(T)), where tilde{d} is the effective dimension of the kernel space. These results are the first regret bounds that are near-optimal in time T and dimension d (or tilde{d}) and polynomial in planning horizon H for RL using features or kernels.",1
"Reinforcement learning algorithms struggle when the reward signal is very sparse. In these cases, naive random exploration methods essentially rely on a random walk to stumble onto a rewarding state. Recent works utilize intrinsic motivation to guide the exploration via generative models, predictive forward models, or discriminative modeling of novelty. We propose EMI, which is an exploration method that constructs embedding representation of states and actions that does not rely on generative decoding of the full observation but extracts predictive signals that can be used to guide exploration based on forward prediction in the representation space. Our experiments show competitive results on challenging locomotion tasks with continuous control and on image-based exploration tasks with discrete actions on Atari. The source code is available at https://github.com/snu-mllab/EMI .",0
"Sparse reward signals pose a challenge for reinforcement learning algorithms, as random exploration methods may rely on chance to reach rewarding states. To address this, recent research has explored intrinsic motivation techniques such as generative models, predictive forward models, and novelty discrimination. Our proposed method, EMI, constructs an embedding representation of states and actions that does not require generative decoding of the full observation. Instead, it extracts predictive signals to guide exploration through forward prediction in the representation space. Our experiments demonstrate competitive results in challenging locomotion tasks with continuous control and image-based exploration tasks with discrete actions on Atari. The source code for EMI is available at https://github.com/snu-mllab/EMI.",1
"Many AI problems, in robotics and other domains, are goal-directed, essentially seeking a trajectory leading to some goal state. In such problems, the way we choose to represent a trajectory underlies algorithms for trajectory prediction and optimization. Interestingly, most all prior work in imitation and reinforcement learning builds on a sequential trajectory representation -- calculating the next state in the trajectory given its predecessors. We propose a different perspective: a goal-conditioned trajectory can be represented by first selecting an intermediate state between start and goal, partitioning the trajectory into two. Then, recursively, predicting intermediate points on each sub-segment, until a complete trajectory is obtained. We call this representation a sub-goal tree, and building on it, we develop new methods for trajectory prediction, learning, and optimization. We show that in a supervised learning setting, sub-goal trees better account for trajectory variability, and can predict trajectories exponentially faster at test time by leveraging a concurrent computation. Then, for optimization, we derive a new dynamic programming equation for sub-goal trees, and use it to develop new planning and reinforcement learning algorithms. These algorithms, which are not based on the standard Bellman equation, naturally account for hierarchical sub-goal structure in a task. Empirical results on motion planning domains show that the sub-goal tree framework significantly improves both accuracy and prediction time.",0
"In various fields, including robotics, many AI problems involve reaching a goal state by following a trajectory. The accuracy of trajectory prediction and optimization algorithms depends on how we represent the trajectory. Previous work in imitation and reinforcement learning has mainly used a sequential representation, where the next state in the trajectory is calculated based on its predecessors. Our approach is different: we propose representing a goal-conditioned trajectory by selecting an intermediate state between the start and goal, dividing the trajectory into two segments, and recursively predicting intermediate points on each sub-segment until a complete trajectory is obtained. This representation is called a sub-goal tree and allows for new methods in trajectory prediction, learning, and optimization. We show that using sub-goal trees in supervised learning results in better trajectory variability and faster prediction at test time by using concurrent computation. In optimization, we derive a new dynamic programming equation for sub-goal trees, which allows us to develop new planning and reinforcement learning algorithms that naturally consider hierarchical sub-goal structures in a task. Empirical results in motion planning domains demonstrate that the sub-goal tree framework significantly improves both accuracy and prediction time.",1
"We examine the question of when and how parametric models are most useful in reinforcement learning. In particular, we look at commonalities and differences between parametric models and experience replay. Replay-based learning algorithms share important traits with model-based approaches, including the ability to plan: to use more computation without additional data to improve predictions and behaviour. We discuss when to expect benefits from either approach, and interpret prior work in this context. We hypothesise that, under suitable conditions, replay-based algorithms should be competitive to or better than model-based algorithms if the model is used only to generate fictional transitions from observed states for an update rule that is otherwise model-free. We validated this hypothesis on Atari 2600 video games. The replay-based algorithm attained state-of-the-art data efficiency, improving over prior results with parametric models.",0
"Our focus is on determining the optimal circumstances and methods for implementing parametric models in reinforcement learning. Our analysis specifically evaluates the similarities and differences between parametric models and experience replay. Although replay-based learning algorithms possess qualities similar to those of model-based approaches, such as the capacity to plan, we examine when each approach is most advantageous and interpret previous research in this regard. Our hypothesis is that if the model is solely utilized to generate hypothetical transitions from observed states for a model-free update rule, replay-based algorithms will be as good as or better than model-based algorithms under certain conditions. Our hypothesis was tested on Atari 2600 video games, where the replay-based algorithm achieved the best data efficiency, surpassing previous results with parametric models.",1
"Value-based reinforcement-learning algorithms provide state-of-the-art results in model-free discrete-action settings, and tend to outperform actor-critic algorithms. We argue that actor-critic algorithms are limited by their need for an on-policy critic. We propose Bootstrapped Dual Policy Iteration (BDPI), a novel model-free reinforcement-learning algorithm for continuous states and discrete actions, with an actor and several off-policy critics. Off-policy critics are compatible with experience replay, ensuring high sample-efficiency, without the need for off-policy corrections. The actor, by slowly imitating the average greedy policy of the critics, leads to high-quality and state-specific exploration, which we compare to Thompson sampling. Because the actor and critics are fully decoupled, BDPI is remarkably stable, and unusually robust to its hyper-parameters. BDPI is significantly more sample-efficient than Bootstrapped DQN, PPO, and ACKTR, on discrete, continuous and pixel-based tasks. Source code: https://github.com/vub-ai-lab/bdpi.",0
"In model-free discrete-action settings, value-based reinforcement-learning algorithms are currently the best performing and surpass actor-critic algorithms. However, the latter is limited due to its reliance on an on-policy critic. To address this issue, we introduce a new model-free reinforcement-learning algorithm called Bootstrapped Dual Policy Iteration (BDPI) for continuous states and discrete actions. BDPI consists of an actor and multiple off-policy critics that are compatible with experience replay, thus ensuring high sample-efficiency without the need for off-policy corrections. By slowly imitating the average greedy policy of the critics, the actor promotes high-quality and state-specific exploration, similar to Thompson sampling. BDPI is remarkably stable and robust to hyper-parameters due to the complete decoupling of the actor and critics. It is also significantly more sample-efficient than Bootstrapped DQN, PPO, and ACKTR in discrete, continuous, and pixel-based tasks. The source code for BDPI can be found at https://github.com/vub-ai-lab/bdpi.",1
"The standard reinforcement learning (RL) formulation considers the expectation of the (discounted) cumulative reward. This is limiting in applications where we are concerned with not only the expected performance, but also the distribution of the performance. In this paper, we introduce micro-objective reinforcement learning --- an alternative RL formalism that overcomes this issue. In this new formulation, a RL task is specified by a set of micro-objectives, which are constructs that specify the desirability or undesirability of events. In addition, micro-objectives allow prior knowledge in the form of temporal abstraction to be incorporated into the global RL objective. The generality of this formalism, and its relations to single/multi-objective RL, and hierarchical RL are discussed.",0
"The traditional approach to reinforcement learning (RL) focuses solely on the expected cumulative reward, which can be restrictive in contexts where performance distribution is also a concern. To address this limitation, we propose a new RL framework called micro-objective reinforcement learning. This approach defines RL tasks based on micro-objectives, which represent the desirability or undesirability of events. Moreover, micro-objectives enable the integration of prior knowledge in the form of temporal abstraction into the overall RL objective. We explore the versatility of this formalism and its connections to single/multi-objective RL and hierarchical RL.",1
"We introduce a unified probabilistic framework for solving sequential decision making problems ranging from Bayesian optimisation to contextual bandits and reinforcement learning. This is accomplished by a probabilistic model-based approach that explains observed data while capturing predictive uncertainty during the decision making process. Crucially, this probabilistic model is chosen to be a Meta-Learning system that allows learning from a distribution of related problems, allowing data efficient adaptation to a target task. As a suitable instantiation of this framework, we explore the use of Neural processes due to statistical and computational desiderata. We apply our framework to a broad range of problem domains, such as control problems, recommender systems and adversarial attacks on RL agents, demonstrating an efficient and general black-box learning approach.",0
"A unified probabilistic framework is presented in this study to address sequential decision making problems, including Bayesian optimization, contextual bandits, and reinforcement learning. This framework utilizes a probabilistic model-based approach that not only explains observed data but also accounts for predictive uncertainty during the decision-making process. To enable data-efficient adaptation to a target task, a Meta-Learning system is chosen as the probabilistic model, which can learn from a distribution of related problems. Neural processes are employed as a suitable instantiation of this framework due to statistical and computational requirements. The framework is applied to various problem domains, such as control problems, recommender systems, and adversarial attacks on RL agents, illustrating an effective and general black-box learning approach.",1
"Multi-agent learning is a promising method to simulate aggregate competitive behaviour in finance. Learning expert agents' reward functions through their external demonstrations is hence particularly relevant for subsequent design of realistic agent-based simulations. Inverse Reinforcement Learning (IRL) aims at acquiring such reward functions through inference, allowing to generalize the resulting policy to states not observed in the past. This paper investigates whether IRL can infer such rewards from agents within real financial stochastic environments: limit order books (LOB). We introduce a simple one-level LOB, where the interactions of a number of stochastic agents and an expert trading agent are modelled as a Markov decision process. We consider two cases for the expert's reward: either a simple linear function of state features; or a complex, more realistic non-linear function. Given the expert agent's demonstrations, we attempt to discover their strategy by modelling their latent reward function using linear and Gaussian process (GP) regressors from previous literature, and our own approach through Bayesian neural networks (BNN). While the three methods can learn the linear case, only the GP-based and our proposed BNN methods are able to discover the non-linear reward case. Our BNN IRL algorithm outperforms the other two approaches as the number of samples increases. These results illustrate that complex behaviours, induced by non-linear reward functions amid agent-based stochastic scenarios, can be deduced through inference, encouraging the use of inverse reinforcement learning for opponent-modelling in multi-agent systems.",0
"The use of multi-agent learning is a promising approach to simulate competitive behaviour in finance. In order to design realistic agent-based simulations, it is important to learn the expert agents' reward functions through their external demonstrations. Inverse Reinforcement Learning (IRL) is a method that aims to acquire these reward functions through inference, allowing for generalization of resulting policies to unobserved states. This research examines whether IRL can infer such rewards from agents in real financial stochastic environments, specifically in limit order books (LOB). A simple one-level LOB is introduced, where the interactions of stochastic agents and an expert trading agent are modeled as a Markov decision process. Two cases for the expert's reward are considered: a simple linear function of state features, or a more realistic non-linear function. The expert agent's strategy is discovered through modeling their latent reward function using linear and Gaussian process (GP) regressors from previous literature, and a Bayesian neural network (BNN) approach. While all three methods can learn the linear case, only the GP-based and BNN methods can discover the non-linear reward case. The BNN IRL algorithm outperforms the other two approaches as the number of samples increases. These results demonstrate that IRL can deduce complex behaviours induced by non-linear reward functions in agent-based stochastic scenarios, encouraging its use for opponent-modelling in multi-agent systems.",1
"Recent efforts in Machine Learning (ML) interpretability have focused on creating methods for explaining black-box ML models. However, these methods rely on the assumption that simple approximations, such as linear models or decision-trees, are inherently human-interpretable, which has not been empirically tested. Additionally, past efforts have focused exclusively on comprehension, neglecting to explore the trust component necessary to convince non-technical experts, such as clinicians, to utilize ML models in practice. In this paper, we posit that reinforcement learning (RL) can be used to learn what is interpretable to different users and, consequently, build their trust in ML models. To validate this idea, we first train a neural network to provide risk assessments for heart failure patients. We then design a RL-based clinical decision-support system (DSS) around the neural network model, which can learn from its interactions with users. We conduct an experiment involving a diverse set of clinicians from multiple institutions in three different countries. Our results demonstrate that ML experts cannot accurately predict which system outputs will maximize clinicians' confidence in the underlying neural network model, and suggest additional findings that have broad implications to the future of research into ML interpretability and the use of ML in medicine.",0
"The recent focus in Machine Learning (ML) interpretability has been on developing methods to explain the workings of black-box ML models. However, these methods assume that simple approximations such as decision-trees or linear models are easily understandable, which has not been empirically tested. Furthermore, previous efforts have ignored the importance of building trust among non-technical experts such as clinicians, who need to be convinced to use ML models in practice. In this article, we propose that reinforcement learning (RL) can be used to determine what is interpretable to different users and thereby establish their trust in ML models. To test this idea, we first train a neural network to assess the risk of heart failure patients. We then develop an RL-based clinical decision-support system (DSS) around the neural network model, which can learn from its interactions with users. We conduct an experiment involving clinicians from different institutions in three countries, and our findings reveal that ML experts are unable to predict which system outputs will enhance clinicians' confidence in the underlying neural network model. Our results have important implications for future research into ML interpretability and the use of ML in medicine.",1
"The utility of learning a dynamics/world model of the environment in reinforcement learning has been shown in a many ways. When using neural networks, however, these models suffer catastrophic forgetting when learned in a lifelong or continual fashion. Current solutions to the continual learning problem require experience to be segmented and labeled as discrete tasks, however, in continuous experience it is generally unclear what a sufficient segmentation of tasks would be. Here we propose a method to continually learn these internal world models through the interleaving of internally generated episodes of past experiences (i.e., pseudo-rehearsal). We show this method can sequentially learn unsupervised temporal prediction, without task labels, in a disparate set of Atari games. Empirically, this interleaving of the internally generated rollouts with the external environment's observations leads to a consistent reduction in temporal prediction loss compared to non-interleaved learning and is preserved over repeated random exposures to various tasks. Similarly, using a network distillation approach, we show that modern policy gradient based reinforcement learning algorithms can use this internal model to continually learn to optimize reward based on the world model's representation of the environment.",0
"Numerous studies have demonstrated the usefulness of acquiring a dynamics or world model of the environment in reinforcement learning. Unfortunately, when utilizing neural networks, these models tend to experience catastrophic forgetting when acquired in a continuous or lifelong manner. Currently, the most popular solutions to the continual learning problem require experience to be divided into discrete tasks, which can be challenging in situations where experience is continuous. Our proposed solution involves interleaving internally generated memories of past experiences (i.e., pseudo-rehearsal) to continually acquire these internal world models. We demonstrate that this method can learn unsupervised temporal prediction in a diverse range of Atari games without task labels. Furthermore, by combining this method with network distillation, we show that modern policy gradient-based reinforcement learning algorithms can continually optimize reward based on the world model's representation of the environment. Our results indicate that interleaving these internally generated rollouts with external observations can consistently reduce temporal prediction loss compared to non-interleaved learning and is maintained even after multiple random exposures to various tasks.",1
"We study Imitation Learning (IL) from Observations alone (ILFO) in large-scale MDPs. While most IL algorithms rely on an expert to directly provide actions to the learner, in this setting the expert only supplies sequences of observations. We design a new model-free algorithm for ILFO, Forward Adversarial Imitation Learning (FAIL), which learns a sequence of time-dependent policies by minimizing an Integral Probability Metric between the observation distributions of the expert policy and the learner. FAIL is the first provably efficient algorithm in ILFO setting, which learns a near-optimal policy with a number of samples that is polynomial in all relevant parameters but independent of the number of unique observations. The resulting theory extends the domain of provably sample efficient learning algorithms beyond existing results, which typically only consider tabular reinforcement learning settings or settings that require access to a near-optimal reset distribution. We also investigate the extension of FAIL in a model-based setting. Finally we demonstrate the efficacy of FAIL on multiple OpenAI Gym control tasks.",0
"Our focus is on Imitation Learning (IL) through Observations alone (ILFO) in large-scale MDPs. Commonly, IL algorithms require an expert to provide actions directly to the learner, but our approach is different: the expert only provides observation sequences. We have developed Forward Adversarial Imitation Learning (FAIL), a new model-free algorithm for ILFO. FAIL learns time-dependent policies by minimizing an Integral Probability Metric between the expert's policy and the learner's observation distributions. It is the first algorithm to be proven efficient in the ILFO setting, learning near-optimal policies with a number of samples that is polynomial in relevant parameters and independent of the number of unique observations. This extends the domain of provably sample-efficient learning algorithms beyond existing results that consider only tabular reinforcement learning or settings requiring access to a near-optimal reset distribution. We have also explored the extension of FAIL in a model-based setting and demonstrated its effectiveness in multiple OpenAI Gym control tasks.",1
"Recent developments in deep reinforcement learning are concerned with creating decision-making agents which can perform well in various complex domains. A particular approach which has received increasing attention is multi-agent reinforcement learning, in which multiple agents learn concurrently to coordinate their actions. In such multi-agent environments, additional learning problems arise due to the continually changing decision-making policies of agents. This paper surveys recent works that address the non-stationarity problem in multi-agent deep reinforcement learning. The surveyed methods range from modifications in the training procedure, such as centralized training, to learning representations of the opponent's policy, meta-learning, communication, and decentralized learning. The survey concludes with a list of open problems and possible lines of future research.",0
"The latest advancements in deep reinforcement learning are focused on developing decision-making agents capable of performing effectively in complex domains. One particular technique that has garnered increased attention is multi-agent reinforcement learning, where multiple agents learn simultaneously to coordinate their actions. In such scenarios, the dynamic decision-making policies of the agents pose additional learning challenges. This research paper examines recent efforts to address the non-stationarity issue in multi-agent deep reinforcement learning. The discussed approaches involve adjustments to the training process, such as centralized training, learning the opponent's policy, meta-learning, communication, and decentralized learning. The study concludes with a list of unresolved issues and potential research avenues.",1
"We focus on the problem of teaching a robot to solve tasks presented sequentially, i.e., in a continual learning scenario. The robot should be able to solve all tasks it has encountered, without forgetting past tasks. We provide preliminary work on applying Reinforcement Learning to such setting, on 2D navigation tasks for a 3 wheel omni-directional robot. Our approach takes advantage of state representation learning and policy distillation. Policies are trained using learned features as input, rather than raw observations, allowing better sample efficiency. Policy distillation is used to combine multiple policies into a single one that solves all encountered tasks.",0
"Our primary concern is teaching a robot to tackle tasks that are presented one after another, without forgetting previous ones, in a continual learning environment. Our focus in this work is on 2D navigation tasks for a 3-wheel omni-directional robot, and we explore the application of Reinforcement Learning for this purpose. To optimize sample efficiency, we employ state representation learning and policy distillation, where policies are trained using learned features instead of raw observations. We combine multiple policies into a single solution for all tasks using policy distillation.",1
"Model-based Reinforcement Learning approaches have the promise of being sample efficient. Much of the progress in learning dynamics models in RL has been made by learning models via supervised learning. But traditional model-based approaches lead to `compounding errors' when the model is unrolled step by step. Essentially, the state transitions that the learner predicts (by unrolling the model for multiple steps) and the state transitions that the learner experiences (by acting in the environment) may not be consistent. There is enough evidence that humans build a model of the environment, not only by observing the environment but also by interacting with the environment. Interaction with the environment allows humans to carry out experiments: taking actions that help uncover true causal relationships which can be used for building better dynamics models. Analogously, we would expect such interactions to be helpful for a learning agent while learning to model the environment dynamics. In this paper, we build upon this intuition by using an auxiliary cost function to ensure consistency between what the agent observes (by acting in the real world) and what it imagines (by acting in the `learned' world). We consider several tasks - Mujoco based control tasks and Atari games - and show that the proposed approach helps to train powerful policies and better dynamics models.",0
"The potential for sample efficiency exists in Model-based Reinforcement Learning approaches. Progress in learning dynamics models has been made through supervised learning, but traditional approaches can result in errors compounding over time. This is due to inconsistencies between the predicted state transitions and the actual state transitions experienced by the learner. Humans are able to build a model of their environment through interaction, which enables them to conduct experiments and discover causal relationships for better model building. Similarly, interaction with the environment can aid a learning agent in modeling its dynamics. This paper uses an auxiliary cost function to ensure consistency between what the agent observes in the real world and what it imagines in the learned world. Through testing on Mujoco based control tasks and Atari games, the proposed approach is shown to effectively train policies and improve dynamics models.",1
"Reinforcement learning has seen great advancements in the past five years. The successful introduction of deep learning in place of more traditional methods allowed reinforcement learning to scale to very complex domains achieving super-human performance in environments like the game of Go or numerous video games. Despite great successes in multiple domains, these new methods suffer from their own issues that make them often inapplicable to the real world problems. Extreme lack of data efficiency, together with huge variance and difficulty in enforcing safety constraints, is one of the three most prominent issues in the field. Usually, millions of data points sampled from the environment are necessary for these algorithms to converge to acceptable policies.   This thesis proposes novel Generative Adversarial Imaginative Reinforcement Learning algorithm. It takes advantage of the recent introduction of highly effective generative adversarial models, and Markov property that underpins reinforcement learning setting, to model dynamics of the real environment within the internal imagination module. Rollouts from the imagination are then used to artificially simulate the real environment in a standard reinforcement learning process to avoid, often expensive and dangerous, trial and error in the real environment. Experimental results show that the proposed algorithm more economically utilises experience from the real environment than the current state-of-the-art Rainbow DQN algorithm, and thus makes an important step towards sample efficient deep reinforcement learning.",0
"In the last five years, reinforcement learning has made significant progress. The replacement of traditional methods with deep learning has allowed the scaling of reinforcement learning to complex domains, achieving super-human performance in environments such as the game of Go and various video games. However, these new methods have their own issues that limit their applicability to real-world problems, including a lack of data efficiency, high variance, and difficulty in enforcing safety constraints. Typically, millions of data points are required for these algorithms to converge to acceptable policies. To address these issues, this thesis proposes a novel Generative Adversarial Imaginative Reinforcement Learning algorithm that leverages highly effective generative adversarial models and the Markov property to model the dynamics of the real environment within the internal imagination module. The algorithm uses rollouts from the imagination to simulate the real environment in a standard reinforcement learning process, reducing the need for expensive and dangerous trial and error in the real environment. Experimental results demonstrate that the proposed algorithm utilizes experience from the real environment more efficiently than the current state-of-the-art Rainbow DQN algorithm, representing a significant step towards sample-efficient deep reinforcement learning.",1
"Efficient exploration is a long-standing problem in sensorimotor learning. Major advances have been demonstrated in noise-free, non-stochastic domains such as video games and simulation. However, most of these formulations either get stuck in environments with stochastic dynamics or are too inefficient to be scalable to real robotics setups. In this paper, we propose a formulation for exploration inspired by the work in active learning literature. Specifically, we train an ensemble of dynamics models and incentivize the agent to explore such that the disagreement of those ensembles is maximized. This allows the agent to learn skills by exploring in a self-supervised manner without any external reward. Notably, we further leverage the disagreement objective to optimize the agent's policy in a differentiable manner, without using reinforcement learning, which results in a sample-efficient exploration. We demonstrate the efficacy of this formulation across a variety of benchmark environments including stochastic-Atari, Mujoco and Unity. Finally, we implement our differentiable exploration on a real robot which learns to interact with objects completely from scratch. Project videos and code are at https://pathak22.github.io/exploration-by-disagreement/",0
"Sensorimotor learning has long struggled with efficient exploration, particularly in stochastic environments. While noise-free, non-stochastic domains like video games and simulation have made progress, they do not scale well to real robotics setups. To address this issue, we propose a new approach inspired by active learning literature, which involves training an ensemble of dynamics models and maximizing the disagreement between them to encourage self-supervised exploration without external rewards. Our method also optimizes the agent's policy in a differentiable manner using the disagreement objective, resulting in sample-efficient exploration. We validate our approach across various benchmark environments, including stochastic-Atari, Mujoco, and Unity, and demonstrate its effectiveness on a real robot learning to interact with objects from scratch. Project videos and code can be found at https://pathak22.github.io/exploration-by-disagreement/.",1
"We propose CAVIA for meta-learning, a simple extension to MAML that is less prone to meta-overfitting, easier to parallelise, and more interpretable. CAVIA partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, only the context parameters are updated, leading to a low-dimensional task representation. We show empirically that CAVIA outperforms MAML for regression, classification, and reinforcement learning. Our experiments also highlight weaknesses in current benchmarks, in that the amount of adaptation needed in some cases is small.",0
"We suggest CAVIA for meta-learning, which is a straightforward modification of MAML. It is less susceptible to meta-overfitting, easier to parallelize, and more straightforward to understand. CAVIA divides the model parameters into two parts: context parameters, which act as extra input to the model and are adjusted for each task, and shared parameters, which are meta-trained and shared across tasks. During testing, only the context parameters are modified, leading to a low-dimensional task representation. We demonstrate through experiments that CAVIA outperforms MAML for regression, classification, and reinforcement learning. Additionally, our experiments reveal flaws in current benchmarks, as some cases require only a small amount of adaptation.",1
"Soft Actor-Critic (SAC) is an off-policy actor-critic deep reinforcement learning (DRL) algorithm based on maximum entropy reinforcement learning. By combining off-policy updates with an actor-critic formulation, SAC achieves state-of-the-art performance on a range of continuous-action benchmark tasks, outperforming prior on-policy and off-policy methods. The off-policy method employed by SAC samples data uniformly from past experience when performing parameter updates. We propose Emphasizing Recent Experience (ERE), a simple but powerful off-policy sampling technique, which emphasizes recently observed data while not forgetting the past. The ERE algorithm samples more aggressively from recent experience, and also orders the updates to ensure that updates from old data do not overwrite updates from new data. We compare vanilla SAC and SAC+ERE, and show that ERE is more sample efficient than vanilla SAC for continuous-action Mujoco tasks. We also consider combining SAC with Priority Experience Replay (PER), a scheme originally proposed for deep Q-learning which prioritizes the data based on temporal-difference (TD) error. We show that SAC+PER can marginally improve the sample efficiency performance of SAC, but much less so than SAC+ERE. Finally, we propose an algorithm which integrates ERE and PER and show that this hybrid algorithm can give the best results for some of the Mujoco tasks.",0
"The Soft Actor-Critic (SAC) algorithm is a deep reinforcement learning (DRL) technique that utilizes maximum entropy reinforcement learning and combines off-policy updates with an actor-critic formulation to achieve superior performance on continuous-action benchmark tasks. SAC samples data uniformly from past experience during parameter updates, but a new off-policy sampling technique called Emphasizing Recent Experience (ERE) has been proposed to improve sample efficiency. ERE prioritizes recently observed data while still considering past experience and orders updates to avoid overwriting new data with old data. SAC+ERE outperforms vanilla SAC on continuous-action Mujoco tasks. Priority Experience Replay (PER), a data prioritization scheme, is also considered for SAC. SAC+PER marginally improves sample efficiency, but not as much as SAC+ERE. A hybrid algorithm that integrates ERE and PER gives the best results for some Mujoco tasks.",1
"To be successful in real-world tasks, Reinforcement Learning (RL) needs to exploit the compositional, relational, and hierarchical structure of the world, and learn to transfer it to the task at hand. Recent advances in representation learning for language make it possible to build models that acquire world knowledge from text corpora and integrate this knowledge into downstream decision making problems. We thus argue that the time is right to investigate a tight integration of natural language understanding into RL in particular. We survey the state of the field, including work on instruction following, text games, and learning from textual domain knowledge. Finally, we call for the development of new environments as well as further investigation into the potential uses of recent Natural Language Processing (NLP) techniques for such tasks.",0
"In order for Reinforcement Learning (RL) to achieve success in real-world tasks, it is necessary to effectively utilize the hierarchical, relational, and compositional structure of the environment, and learn how to transfer it to the task at hand. Recent developments in language representation learning enable the creation of models that can learn about the world through text corpora and incorporate that knowledge into decision-making problems downstream. Therefore, we propose that the integration of natural language understanding into RL should be explored more closely. We examine the current state of the field, including research on instruction-following, text games, and learning from textual domain knowledge. Ultimately, we recommend the creation of new environments and further exploration into the potential uses of recent Natural Language Processing (NLP) techniques in these tasks.",1
"Network slicing promises to provision diversified services with distinct requirements in one infrastructure. Deep reinforcement learning (e.g., deep $\mathcal{Q}$-learning, DQL) is assumed to be an appropriate algorithm to solve the demand-aware inter-slice resource management issue in network slicing by regarding the varying demands and the allocated bandwidth as the environment state and the action, respectively. However, allocating bandwidth in a finer resolution usually implies larger action space, and unfortunately DQL fails to quickly converge in this case. In this paper, we introduce discrete normalized advantage functions (DNAF) into DQL, by separating the $\mathcal{Q}$-value function as a state-value function term and an advantage term and exploiting a deterministic policy gradient descent (DPGD) algorithm to avoid the unnecessary calculation of $\mathcal{Q}$-value for every state-action pair. Furthermore, as DPGD only works in continuous action space, we embed a k-nearest neighbor algorithm into DQL to quickly find a valid action in the discrete space nearest to the DPGD output. Finally, we verify the faster convergence of the DNAF-based DQL through extensive simulations.",0
"Network slicing offers the ability to provide various services with distinct requirements on a single infrastructure. To address the issue of managing inter-slice resources based on demand, deep reinforcement learning (DRL) algorithms, such as deep $\mathcal{Q}$-learning (DQL), have been proposed. However, finer bandwidth allocation leads to a larger action space, which can impede the convergence of DQL. To address this issue, we propose integrating discrete normalized advantage functions (DNAF) into DQL. This involves separating the $\mathcal{Q}$-value function into a state-value function term and an advantage term, and using a deterministic policy gradient descent (DPGD) algorithm to avoid unnecessary calculations. To handle the discrete action space, we incorporate a k-nearest neighbor algorithm into DQL. Our simulations demonstrate that the DNAF-based DQL approach achieves faster convergence than traditional DQL.",1
"Off-policy evaluation (OPE) in both contextual bandits and reinforcement learning allows one to evaluate novel decision policies without needing to conduct exploration, which is often costly or otherwise infeasible. The problem's importance has attracted many proposed solutions, including importance sampling (IS), self-normalized IS (SNIS), and doubly robust (DR) estimates. DR and its variants ensure semiparametric local efficiency if Q-functions are well-specified, but if they are not they can be worse than both IS and SNIS. It also does not enjoy SNIS's inherent stability and boundedness. We propose new estimators for OPE based on empirical likelihood that are always more efficient than IS, SNIS, and DR and satisfy the same stability and boundedness properties as SNIS. On the way, we categorize various properties and classify existing estimators by them. Besides the theoretical guarantees, empirical studies suggest the new estimators provide advantages.",0
"Off-policy evaluation (OPE) is a technique used in both contextual bandits and reinforcement learning that enables the evaluation of new decision policies without the need for exploration, which can be costly or impractical. Proposed solutions to this problem include importance sampling (IS), self-normalized IS (SNIS), and doubly robust (DR) estimates. While DR and its variations can ensure semiparametric local efficiency if Q-functions are well-specified, they may be worse than IS and SNIS if they are not. Moreover, DR lacks the inherent stability and boundedness of SNIS. This paper proposes new OPE estimators based on empirical likelihood that are always more efficient than IS, SNIS, and DR, and possess the same stability and boundedness properties as SNIS. The paper also categorizes various properties and classifies existing estimators based on them. Empirical studies suggest that the new estimators provide benefits in addition to theoretical guarantees.",1
"Access to parallel and distributed computation has enabled researchers and developers to improve algorithms and performance in many applications. Recent research has focused on next generation special purpose systems with multiple kinds of coprocessors, known as heterogeneous system-on-chips (SoC). In this paper, we introduce a method to intelligently schedule--and learn to schedule--a stream of tasks to available processing elements in such a system. We use deep reinforcement learning enabling complex sequential decision making and empirically show that our reinforcement learning system provides for a viable, better alternative to conventional scheduling heuristics with respect to minimizing execution time.",0
"The availability of parallel and distributed computation has facilitated the enhancement of algorithms and performance in various applications. The latest studies have concentrated on the development of specialized systems for the future that contain several types of coprocessors, referred to as heterogeneous system-on-chips (SoC). This article presents a technique for intelligently scheduling a sequence of tasks to available processing elements in such a system, which can also learn to schedule. Our approach utilizes deep reinforcement learning to enable intricate sequential decision-making and we demonstrate through empirical evidence that our reinforcement learning system offers a suitable and superior alternative to traditional scheduling heuristics in terms of minimizing execution time.",1
"Dealing with sparse rewards is a longstanding challenge in reinforcement learning. The recent use of hindsight methods have achieved success on a variety of sparse-reward tasks, but they fail on complex tasks such as stacking multiple blocks with a robot arm in simulation. Curiosity-driven exploration using the prediction error of a learned dynamics model as an intrinsic reward has been shown to be effective for exploring a number of sparse-reward environments. We present a method that combines hindsight with curiosity-driven exploration and curriculum learning in order to solve the challenging sparse-reward block stacking task. We are the first to stack more than two blocks using only sparse reward without human demonstrations.",0
"For a long time, tackling sparse rewards has been a difficult task in reinforcement learning. While hindsight methods have proven to be successful in handling sparse-reward tasks, they do not work well for complex tasks like stacking several blocks with a robot arm in simulation. To explore sparse-reward environments, curiosity-driven exploration that utilizes the prediction error of a learned dynamics model as an intrinsic reward has been effective. To solve the challenging sparse-reward block stacking task, we have developed a method that combines hindsight with curiosity-driven exploration and curriculum learning. Our approach has enabled us to stack more than two blocks using only sparse reward, without any human demonstrations.",1
"Exploration and adaptation to new tasks in a transfer learning setup is a central challenge in reinforcement learning. In this work, we build on the idea of modeling a distribution over policies in a Bayesian deep reinforcement learning setup to propose a transfer strategy. Recent works have shown to induce diversity in the learned policies by maximizing the entropy of a distribution of policies (Bachman et al., 2018; Garnelo et al., 2018) and thus, we postulate that our proposed approach leads to faster exploration resulting in improved transfer learning. We support our hypothesis by demonstrating favorable experimental results on a variety of settings on fully-observable GridWorld and partially observable MiniGrid (Chevalier-Boisvert et al., 2018) environments.",0
"A main obstacle in reinforcement learning is the ability to explore and adjust to novel tasks within a transfer learning framework. In this study, we expand on the concept of developing a probability distribution over policies in a Bayesian deep reinforcement learning framework to introduce a transfer approach. Recent research has demonstrated that increasing the entropy of a policy distribution (Bachman et al., 2018; Garnelo et al., 2018) can enhance diversity in the learned policies and consequently facilitate quicker exploration, leading to better transfer learning outcomes. Our proposed technique is supported by our favorable experimental outcomes in various fully-observable GridWorld and partially observable MiniGrid (Chevalier-Boisvert et al., 2018) environments.",1
"Imagine a patient in critical condition. What and when should be measured to forecast detrimental events, especially under the budget constraints? We answer this question by deep reinforcement learning (RL) that jointly minimizes the measurement cost and maximizes predictive gain, by scheduling strategically-timed measurements. We learn our policy to be dynamically dependent on the patient's health history. To scale our framework to exponentially large action space, we distribute our reward in a sequential setting that makes the learning easier. In our simulation, our policy outperforms heuristic-based scheduling with higher predictive gain and lower cost. In a real-world ICU mortality prediction task (MIMIC3), our policies reduce the total number of measurements by $31\%$ or improve predictive gain by a factor of $3$ as compared to physicians, under the off-policy policy evaluation.",0
"The use of deep reinforcement learning (RL) can help determine what and when to measure a critically ill patient in order to accurately predict negative events, all while considering budget constraints. Our approach involves scheduling strategically-timed measurements that minimize cost and maximize predictive gain, with the policy being dynamically dependent on the patient's health history. To handle the exponentially large action space, we adopted a sequential setting to distribute the reward, which makes learning easier. Our simulation results show that our policy outperforms heuristic-based scheduling in terms of predictive gain and cost. In a real-world ICU mortality prediction task (MIMIC3), our policies were shown to reduce the total number of measurements by 31% or improve predictive gain by a factor of 3 compared to physicians, as evaluated off-policy.",1
"Entropy regularization is commonly used to improve policy optimization in reinforcement learning. It is believed to help with \emph{exploration} by encouraging the selection of more stochastic policies. In this work, we analyze this claim using new visualizations of the optimization landscape based on randomly perturbing the loss function. We first show that even with access to the exact gradient, policy optimization is difficult due to the geometry of the objective function. Then, we qualitatively show that in some environments, a policy with higher entropy can make the optimization landscape smoother, thereby connecting local optima and enabling the use of larger learning rates. This paper presents new tools for understanding the optimization landscape, shows that policy entropy serves as a regularizer, and highlights the challenge of designing general-purpose policy optimization algorithms.",0
"The utilization of entropy regularization is a common technique to enhance policy optimization in reinforcement learning. Its primary purpose is to facilitate exploration by encouraging the selection of policies with greater stochasticity. This study examines this assertion by creating new visualizations of the optimization landscape through randomly perturbing the loss function. The research demonstrates that optimizing policy is challenging due to the objective function's geometry, even when the exact gradient is available. Furthermore, the study qualitatively shows that in specific environments, a higher entropy policy can smooth out the optimization landscape, linking local optima and enabling the use of larger learning rates. This research introduces novel tools for comprehending the optimization landscape, asserts policy entropy as a regularizer, and emphasizes the difficulty of developing policy optimization algorithms that are universally applicable.",1
"This paper considers Safe Policy Improvement (SPI) in Batch Reinforcement Learning (Batch RL): from a fixed dataset and without direct access to the true environment, train a policy that is guaranteed to perform at least as well as the baseline policy used to collect the data. Our approach, called SPI with Baseline Bootstrapping (SPIBB), is inspired by the knows-what-it-knows paradigm: it bootstraps the trained policy with the baseline when the uncertainty is high. Our first algorithm, $\Pi_b$-SPIBB, comes with SPI theoretical guarantees. We also implement a variant, $\Pi_{\leq b}$-SPIBB, that is even more efficient in practice. We apply our algorithms to a motivational stochastic gridworld domain and further demonstrate on randomly generated MDPs the superiority of SPIBB with respect to existing algorithms, not only in safety but also in mean performance. Finally, we implement a model-free version of SPIBB and show its benefits on a navigation task with deep RL implementation called SPIBB-DQN, which is, to the best of our knowledge, the first RL algorithm relying on a neural network representation able to train efficiently and reliably from batch data, without any interaction with the environment.",0
"The focus of this paper is on Safe Policy Improvement (SPI) in Batch Reinforcement Learning (Batch RL). The aim is to train a policy from a fixed dataset without direct access to the true environment, which performs at least as well as the baseline policy used to collect the data. The approach used, called SPI with Baseline Bootstrapping (SPIBB), is based on the knows-what-it-knows paradigm and bootstraps the trained policy with the baseline when uncertainty is high. The first algorithm, called $\Pi_b$-SPIBB, comes with theoretical guarantees, and a more efficient variant, $\Pi_{\leq b}$-SPIBB, has also been implemented. The algorithms have been tested on a stochastic gridworld domain and randomly generated MDPs, demonstrating superiority in safety and mean performance compared to existing algorithms. Finally, a model-free version of SPIBB called SPIBB-DQN has been implemented, which is the first RL algorithm relying on a neural network representation to train efficiently and reliably from batch data without any interaction with the environment.",1
"Integrating model-free and model-based approaches in reinforcement learning has the potential to achieve the high performance of model-free algorithms with low sample complexity. However, this is difficult because an imperfect dynamics model can degrade the performance of the learning algorithm, and in sufficiently complex environments, the dynamics model will almost always be imperfect. As a result, a key challenge is to combine model-based approaches with model-free learning in such a way that errors in the model do not degrade performance. We propose stochastic ensemble value expansion (STEVE), a novel model-based technique that addresses this issue. By dynamically interpolating between model rollouts of various horizon lengths for each individual example, STEVE ensures that the model is only utilized when doing so does not introduce significant errors. Our approach outperforms model-free baselines on challenging continuous control benchmarks with an order-of-magnitude increase in sample efficiency, and in contrast to previous model-based approaches, performance does not degrade in complex environments.",0
"The combination of model-free and model-based approaches in reinforcement learning has the potential to achieve the high performance of model-free algorithms with low sample complexity. However, this is a challenging task due to the imperfect dynamics model, which can degrade the learning algorithm's performance. The dynamics model is almost always imperfect in complex environments, leading to a key challenge of combining model-based approaches with model-free learning to avoid performance degradation. To address this issue, we propose a novel model-based technique called stochastic ensemble value expansion (STEVE). STEVE dynamically interpolates between model rollouts of various horizon lengths for each individual example, ensuring that the model is only utilized when there are no significant errors. Our approach outperforms model-free baselines on challenging continuous control benchmarks, with a ten-fold increase in sample efficiency. Unlike previous model-based approaches, our technique does not suffer from performance degradation in complex environments.",1
"We consider the problem of imitation learning from a finite set of expert trajectories, without access to reinforcement signals. The classical approach of extracting the expert's reward function via inverse reinforcement learning, followed by reinforcement learning is indirect and may be computationally expensive. Recent generative adversarial methods based on matching the policy distribution between the expert and the agent could be unstable during training. We propose a new framework for imitation learning by estimating the support of the expert policy to compute a fixed reward function, which allows us to re-frame imitation learning within the standard reinforcement learning setting. We demonstrate the efficacy of our reward function on both discrete and continuous domains, achieving comparable or better performance than the state of the art under different reinforcement learning algorithms.",0
"Our focus is on the challenge of imitating the behavior of an expert using a limited number of their trajectories, without reinforcement signals. The traditional method of first determining the expert's reward function through inverse reinforcement learning and then using reinforcement learning can be computationally expensive and indirect. Recently, generative adversarial approaches that match the policy distribution of the expert and agent have proven unstable during training. To overcome these issues, we propose a new imitation learning framework that estimates the expert policy's support to determine a fixed reward function. This approach allows us to reframe imitation learning within the standard reinforcement learning setting. Our approach has been tested on discrete and continuous domains, and we have achieved comparable or better performance than the current state-of-the-art under various reinforcement learning algorithms.",1
"Human behavior expression and experience are inherently multi-modal, and characterized by vast individual and contextual heterogeneity. To achieve meaningful human-computer and human-robot interactions, multi-modal models of the users states (e.g., engagement) are therefore needed. Most of the existing works that try to build classifiers for the users states assume that the data to train the models are fully labeled. Nevertheless, data labeling is costly and tedious, and also prone to subjective interpretations by the human coders. This is even more pronounced when the data are multi-modal (e.g., some users are more expressive with their facial expressions, some with their voice). Thus, building models that can accurately estimate the users states during an interaction is challenging. To tackle this, we propose a novel multi-modal active learning (AL) approach that uses the notion of deep reinforcement learning (RL) to find an optimal policy for active selection of the users data, needed to train the target (modality-specific) models. We investigate different strategies for multi-modal data fusion, and show that the proposed model-level fusion coupled with RL outperforms the feature-level and modality-specific models, and the naive AL strategies such as random sampling, and the standard heuristics such as uncertainty sampling. We show the benefits of this approach on the task of engagement estimation from real-world child-robot interactions during an autism therapy. Importantly, we show that the proposed multi-modal AL approach can be used to efficiently personalize the engagement classifiers to the target user using a small amount of actively selected users data.",0
"To enable effective interactions between humans and computers or robots, it is necessary to develop multi-modal models that can accurately determine the state of the user, such as their engagement level. However, creating such models is challenging because human behavior is complex and can vary greatly depending on the individual and the context. Additionally, existing approaches typically require fully labeled data to train the models, which is expensive and subjective. To address these issues, we propose a novel multi-modal active learning (AL) technique that leverages deep reinforcement learning (RL) to identify the optimal data needed to train modality-specific models. Our approach outperforms other AL strategies, such as random sampling and uncertainty sampling, and allows for efficient personalization of engagement classifiers using a small amount of actively selected data. We demonstrate the effectiveness of our approach on real-world child-robot interactions during autism therapy.",1
"We consider the setup of stochastic multi-armed bandits in the case when reward distributions are piecewise i.i.d. and bounded with unknown changepoints. We focus on the case when changes happen simultaneously on all arms, and in stark contrast with the existing literature, we target gap-dependent (as opposed to only gap-independent) regret bounds involving the magnitude of changes $(\Delta^{chg}_{i,g})$ and optimality-gaps ($\Delta^{opt}_{i,g}$). Diverging from previous works, we assume the more realistic scenario that there can be undetectable changepoint gaps and under a different set of assumptions, we show that as long as the compounded delayed detection for each changepoint is bounded there is no need for forced exploration to actively detect changepoints. We introduce two adaptations of UCB-strategies that employ scan-statistics in order to actively detect the changepoints, without knowing in advance the changepoints and also the mean before and after any change. Our first method \UCBLCPD does not know the number of changepoints $G$ or time horizon $T$ and achieves the first time-uniform concentration bound for this setting using the Laplace method of integration. The second strategy \ImpCPD makes use of the knowledge of $T$ to achieve the order optimal regret bound of $\min\big\lbrace O(\sum\limits_{i=1}^{K} \sum\limits_{g=1}^{G}\frac{\log(T/H_{1,g})}{\Delta^{opt}_{i,g}}), O(\sqrt{GT})\big\rbrace$, (where $H_{1,g}$ is the problem complexity) thereby closing an important gap with respect to the lower bound in a specific challenging setting. Our theoretical findings are supported by numerical experiments on synthetic and real-life datasets.",0
"In this study, we examine stochastic multi-armed bandits with piecewise i.i.d. reward distributions that are bounded with unknown changepoints. Specifically, we focus on the scenario where changes occur simultaneously across all arms and aim to derive gap-dependent regret bounds that consider the magnitude of changes and optimality-gaps. Unlike previous works, we consider the possibility of undetectable changepoint gaps and demonstrate that forced exploration is unnecessary if the compounded delayed detection for each changepoint is limited. We propose two adaptations of UCB-strategies that leverage scan-statistics to detect changepoints without prior knowledge of the number of changepoints or time horizon. Our first method, \UCBLCPD, achieves the first time-uniform concentration bound for this setting using the Laplace method of integration, while our second strategy, \ImpCPD, uses knowledge of the time horizon to achieve the optimal regret bound. Our theoretical findings are supported by experiments on both synthetic and real-life datasets.",1
"Figures, such as bar charts, pie charts, and line plots, are widely used to convey important information in a concise format. They are usually human-friendly but difficult for computers to process automatically. In this work, we investigate the problem of figure captioning where the goal is to automatically generate a natural language description of the figure. While natural image captioning has been studied extensively, figure captioning has received relatively little attention and remains a challenging problem. First, we introduce a new dataset for figure captioning, FigCAP, based on FigureQA. Second, we propose two novel attention mechanisms. To achieve accurate generation of labels in figures, we propose Label Maps Attention. To model the relations between figure labels, we propose Relation Maps Attention. Third, we use sequence-level training with reinforcement learning in order to directly optimizes evaluation metrics, which alleviates the exposure bias issue and further improves the models in generating long captions. Extensive experiments show that the proposed method outperforms the baselines, thus demonstrating a significant potential for the automatic captioning of vast repositories of figures.",0
"The use of figures, such as bar charts, pie charts, and line plots, is prevalent in presenting crucial information in a succinct manner. However, these figures are easily understood by humans but pose a challenge for computers to automatically process. This study focuses on the problem of generating a natural language description of a figure, known as figure captioning. This task has not received much attention and remains difficult. Firstly, a new dataset, FigCAP, is introduced for figure captioning based on FigureQA. Secondly, two attention mechanisms are proposed to accurately generate labels in figures: Label Maps Attention and Relation Maps Attention to model the relationships between figure labels. Thirdly, to overcome exposure bias issues and improve the models in generating long captions, sequence-level training with reinforcement learning is employed to directly optimize evaluation metrics. Extensive experiments reveal that the proposed method outperforms the baselines, suggesting huge potential for the automatic captioning of extensive repositories of figures.",1
"The Arcade Learning Environment (ALE) is a popular platform for evaluating reinforcement learning agents. Much of the appeal comes from the fact that Atari games demonstrate aspects of competency we expect from an intelligent agent and are not biased toward any particular solution approach. The challenge of the ALE includes (1) the representation learning problem of extracting pertinent information from raw pixels, and (2) the behavioural learning problem of leveraging complex, delayed associations between actions and rewards. Often, the research questions we are interested in pertain more to the latter, but the representation learning problem adds significant computational expense. We introduce MinAtar, short for miniature Atari, a new set of environments that capture the general mechanics of specific Atari games while simplifying the representational complexity to focus more on the behavioural challenges. MinAtar consists of analogues of five Atari games: Seaquest, Breakout, Asterix, Freeway and Space Invaders. Each MinAtar environment provides the agent with a 10x10xn binary state representation. Each game plays out on a 10x10 grid with n channels corresponding to game-specific objects, such as ball, paddle and brick in the game Breakout. To investigate the behavioural challenges posed by MinAtar, we evaluated a smaller version of the DQN architecture as well as online actor-critic with eligibility traces. With the representation learning problem simplified, we can perform experiments with significantly less computational expense. In our experiments, we use the saved compute time to perform step-size parameter sweeps and more runs than is typical for the ALE. Experiments like this improve reproducibility, and allow us to draw more confident conclusions. We hope that MinAtar can allow researchers to thoroughly investigate behavioural challenges similar to those inherent in the ALE.",0
"The Arcade Learning Environment (ALE) has become a popular tool for evaluating reinforcement learning agents due to the fact that Atari games showcase the competence we expect from intelligent agents without bias towards any particular solution approach. However, the ALE presents two major challenges: the representation learning problem and the behavioural learning problem. While the latter is often the focus of research, the former adds significant computational expense. To address this, we introduce MinAtar, a new set of environments that capture the general mechanics of specific Atari games while simplifying the representational complexity to focus primarily on the behavioural challenges. MinAtar includes analogues of five Atari games, each with a 10x10xn binary state representation, where n corresponds to game-specific objects. By simplifying the representation learning problem, we can perform experiments with significantly less computational expense and allocate the saved compute time to perform step-size parameter sweeps and more runs. This allows for improved reproducibility and more confident conclusions. We hope that MinAtar can provide researchers with a platform to investigate behavioural challenges similar to those in the ALE.",1
"We introduce an off-policy evaluation procedure for highlighting episodes where applying a reinforcement learned (RL) policy is likely to have produced a substantially different outcome than the observed policy. In particular, we introduce a class of structural causal models (SCMs) for generating counterfactual trajectories in finite partially observable Markov Decision Processes (POMDPs). We see this as a useful procedure for off-policy ""debugging"" in high-risk settings (e.g., healthcare); by decomposing the expected difference in reward between the RL and observed policy into specific episodes, we can identify episodes where the counterfactual difference in reward is most dramatic. This in turn can be used to facilitate review of specific episodes by domain experts. We demonstrate the utility of this procedure with a synthetic environment of sepsis management.",0
"The paragraph describes a method for detecting instances where a reinforcement learned policy would have led to a different outcome compared to the observed policy. The approach involves using structural causal models to generate counterfactual trajectories in partially observable Markov Decision Processes. This can be useful in ""debugging"" high-risk situations such as healthcare, where specific episodes can be identified based on the expected difference in reward between the policies. This approach can help domain experts review these episodes and make informed decisions. The effectiveness of this method is demonstrated using a synthetic sepsis management environment.",1
"Deep Reinforcement Learning (DRL) algorithms for continuous action spaces are known to be brittle toward hyperparameters as well as \cut{being}sample inefficient. Soft Actor Critic (SAC) proposes an off-policy deep actor critic algorithm within the maximum entropy RL framework which offers greater stability and empirical gains. The choice of policy distribution, a factored Gaussian, is motivated by \cut{chosen due}its easy re-parametrization rather than its modeling power. We introduce Normalizing Flow policies within the SAC framework that learn more expressive classes of policies than simple factored Gaussians. \cut{We also present a series of stabilization tricks that enable effective training of these policies in the RL setting.}We show empirically on continuous grid world tasks that our approach increases stability and is better suited to difficult exploration in sparse reward settings.",0
"Continuous action space Deep Reinforcement Learning (DRL) algorithms are fragile when it comes to hyperparameters and inefficient in terms of sampling. To address these issues, Soft Actor Critic (SAC) proposes an off-policy deep actor critic algorithm within the maximum entropy RL framework that provides greater stability and empirical gains. The choice of policy distribution is a factored Gaussian, which is easy to re-parametrize. However, we have introduced Normalizing Flow policies within the SAC framework to learn more expressive classes of policies. Empirical evidence from continuous grid world tasks shows that our approach improves stability and is better suited for difficult exploration in sparse reward settings.",1
"Many reinforcement learning (RL) tasks provide the agent with high-dimensional observations that can be simplified into low-dimensional continuous states. To formalize this process, we introduce the concept of a DeepMDP, a parameterized latent space model that is trained via the minimization of two tractable losses: prediction of rewards and prediction of the distribution over next latent states. We show that the optimization of these objectives guarantees (1) the quality of the latent space as a representation of the state space and (2) the quality of the DeepMDP as a model of the environment. We connect these results to prior work in the bisimulation literature, and explore the use of a variety of metrics. Our theoretical findings are substantiated by the experimental result that a trained DeepMDP recovers the latent structure underlying high-dimensional observations on a synthetic environment. Finally, we show that learning a DeepMDP as an auxiliary task in the Atari 2600 domain leads to large performance improvements over model-free RL.",0
"The agent in many RL tasks receives high-dimensional observations that can be simplified into low-dimensional continuous states. To formalize this, we introduce the concept of a DeepMDP, which is a model with a parameterized latent space. It is trained using two losses: predicting rewards and predicting the distribution over next latent states. By optimizing these objectives, we ensure that the DeepMDP is a good model of the environment and that the latent space is a good representation of the state space. We compare our results to previous work in the bisimulation literature and use various metrics. Our experimental results show that a trained DeepMDP can recover the latent structure of high-dimensional observations in a synthetic environment. Additionally, we demonstrate that learning a DeepMDP as an auxiliary task in the Atari 2600 domain leads to significant improvements in performance compared to model-free RL.",1
"Self-supervised learning aims to learn representations from the data itself without explicit manual supervision. Existing efforts ignore a crucial aspect of self-supervised learning - the ability to scale to large amount of data because self-supervision requires no manual labels. In this work, we revisit this principle and scale two popular self-supervised approaches to 100 million images. We show that by scaling on various axes (including data size and problem 'hardness'), one can largely match or even exceed the performance of supervised pre-training on a variety of tasks such as object detection, surface normal estimation (3D) and visual navigation using reinforcement learning. Scaling these methods also provides many interesting insights into the limitations of current self-supervised techniques and evaluations. We conclude that current self-supervised methods are not 'hard' enough to take full advantage of large scale data and do not seem to learn effective high level semantic representations. We also introduce an extensive benchmark across 9 different datasets and tasks. We believe that such a benchmark along with comparable evaluation settings is necessary to make meaningful progress. Code is at: https://github.com/facebookresearch/fair_self_supervision_benchmark.",0
"The goal of self-supervised learning is to learn representations from data without manual supervision. However, current efforts neglect the importance of scalability in self-supervision, as this approach does not require manual labels. This study revisits this principle and scales two popular self-supervised approaches to analyze their effectiveness with 100 million images. The researchers demonstrate that by scaling on various dimensions, including data size and problem 'hardness,' it is possible to match or exceed the performance of supervised pre-training for tasks such as object detection, surface normal estimation (3D), and visual navigation using reinforcement learning. Scaling these methods also offers valuable insight into the limitations of current self-supervised techniques and evaluations. The researchers conclude that current self-supervised methods fail to fully exploit the advantages of large-scale data and do not appear to learn effective high-level semantic representations. They also present a comprehensive benchmark across nine different datasets and tasks, emphasizing the importance of comparable evaluation settings. The code for this study is available at: https://github.com/facebookresearch/fair_self_supervision_benchmark.",1
"Inspired by recent work in attention models for image captioning and question answering, we present a soft attention model for the reinforcement learning domain. This model uses a soft, top-down attention mechanism to create a bottleneck in the agent, forcing it to focus on task-relevant information by sequentially querying its view of the environment. The output of the attention mechanism allows direct observation of the information used by the agent to select its actions, enabling easier interpretation of this model than of traditional models. We analyze different strategies that the agents learn and show that a handful of strategies arise repeatedly across different games. We also show that the model learns to query separately about space and content (`where' vs. `what'). We demonstrate that an agent using this mechanism can achieve performance competitive with state-of-the-art models on ATARI tasks while still being interpretable.",0
"Our soft attention model for the reinforcement learning domain is influenced by recent progress in attention models for image captioning and question answering. The model employs a top-down attention mechanism that is soft, creating a bottleneck in the agent, which helps it concentrate on task-relevant information by sequentially interrogating its environment view. The attention mechanism's output provides a clear understanding of the information used by the agent to select its actions, making it more straightforward to interpret than conventional models. We examine different tactics that agents learn and demonstrate that a few strategies emerge consistently across various games. We also prove that the model learns to query separately about space and content (‘where’ vs. ‘what’). We showcase that an agent utilizing this mechanism can achieve state-of-the-art performance on ATARI tasks while remaining transparent.",1
"Exploration strategy design is one of the challenging problems in reinforcement learning~(RL), especially when the environment contains a large state space or sparse rewards. During exploration, the agent tries to discover novel areas or high reward~(quality) areas. In most existing methods, the novelty and quality in the neighboring area of the current state are not well utilized to guide the exploration of the agent. To tackle this problem, we propose a novel RL framework, called \underline{c}lustered \underline{r}einforcement \underline{l}earning~(CRL), for efficient exploration in RL. CRL adopts clustering to divide the collected states into several clusters, based on which a bonus reward reflecting both novelty and quality in the neighboring area~(cluster) of the current state is given to the agent. Experiments on a continuous control task and several \emph{Atari 2600} games show that CRL can outperform other state-of-the-art methods to achieve the best performance in most cases.",0
"Reinforcement learning (RL) faces the challenging task of designing an effective exploration strategy, particularly when dealing with large state spaces or sparse rewards. In such cases, the agent must seek out novel or high-quality areas within the environment. However, current methods often fail to adequately utilize information about neighboring areas to guide the agent's exploration. To address this issue, we propose a new RL framework called Clustered Reinforcement Learning (CRL). CRL employs clustering to group collected states into clusters, and then offers the agent a bonus reward that reflects both the novelty and quality of neighboring clusters to guide exploration. Our experiments on a continuous control task and several Atari 2600 games demonstrate that CRL outperforms existing methods and achieves superior performance in most cases.",1
"Recent breakthroughs in AI for multi-agent games like Go, Poker, and Dota, have seen great strides in recent years. Yet none of these games address the real-life challenge of cooperation in the presence of unknown and uncertain teammates. This challenge is a key game mechanism in hidden role games. Here we develop the DeepRole algorithm, a multi-agent reinforcement learning agent that we test on The Resistance: Avalon, the most popular hidden role game. DeepRole combines counterfactual regret minimization (CFR) with deep value networks trained through self-play. Our algorithm integrates deductive reasoning into vector-form CFR to reason about joint beliefs and deduce partially observable actions. We augment deep value networks with constraints that yield interpretable representations of win probabilities. These innovations enable DeepRole to scale to the full Avalon game. Empirical game-theoretic methods show that DeepRole outperforms other hand-crafted and learned agents in five-player Avalon. DeepRole played with and against human players on the web in hybrid human-agent teams. We find that DeepRole outperforms human players as both a cooperator and a competitor.",0
"Great advancements in AI for multi-agent games such as Go, Poker and Dota have been made in recent years. However, these games fail to address the real-life challenge of cooperation in the presence of unknown and uncertain teammates, which is a crucial game mechanism in hidden role games. To tackle this issue, we have developed the DeepRole algorithm. It is a multi-agent reinforcement learning agent that we have tested on The Resistance: Avalon, the most popular hidden role game. We have combined counterfactual regret minimization (CFR) with deep value networks that are trained through self-play. Our algorithm incorporates deductive reasoning into vector-form CFR, which enables it to reason about joint beliefs and deduce partially observable actions. We have augmented deep value networks with constraints that yield interpretable representations of win probabilities. These innovations allow DeepRole to scale to the full Avalon game. Empirical game-theoretic methods demonstrate that DeepRole outperforms other hand-crafted and learned agents in five-player Avalon. DeepRole has played with and against human players on the web in hybrid human-agent teams. We have found that DeepRole surpasses human players as both a cooperator and a competitor.",1
"We study the problem of inverse reinforcement learning (IRL) with the added twist that the learner is assisted by a helpful teacher. More formally, we tackle the following algorithmic question: How could a teacher provide an informative sequence of demonstrations to an IRL learner to speed up the learning process? We present an interactive teaching framework where a teacher adaptively chooses the next demonstration based on learner's current policy. In particular, we design teaching algorithms for two concrete settings: an omniscient setting where a teacher has full knowledge about the learner's dynamics and a blackbox setting where the teacher has minimal knowledge. Then, we study a sequential variant of the popular MCE-IRL learner and prove convergence guarantees of our teaching algorithm in the omniscient setting. Extensive experiments with a car driving simulator environment show that the learning progress can be speeded up drastically as compared to an uninformative teacher.",0
"Our focus is on inverse reinforcement learning (IRL), but with the added aspect of a helpful teacher aiding the learner. Specifically, we aim to answer the question of how a teacher can provide demonstrations that are informative and accelerate the IRL learning process. To achieve this, we introduce an interactive teaching framework where the teacher selects the next demonstration based on the learner's current policy. We present teaching algorithms for two scenarios: the omniscient setting where the teacher has complete knowledge of the learner's dynamics, and the blackbox setting where the teacher has minimal knowledge. Further, we explore a sequential variant of the MCE-IRL learner and demonstrate convergence guarantees of our teaching algorithm in the omniscient setting. Our experiments in a car driving simulator environment show that informative teaching can significantly enhance learning progress when compared to uninformative teaching.",1
"We present a method to generate directed acyclic graphs (DAGs) using deep reinforcement learning, specifically deep Q-learning. Generating graphs with specified structures is an important and challenging task in various application fields, however most current graph generation methods produce graphs with undirected edges. We demonstrate that this method is capable of generating DAGs with topology and node types satisfying specified criteria in highly sparse reward environments.",0
"Our study proposes a technique that employs deep Q-learning, a form of deep reinforcement learning, to create directed acyclic graphs (DAGs). The task of generating graphs with predetermined structures is a significant and difficult undertaking in several application domains. Nonetheless, existing graph generation approaches generally yield graphs with undirected edges. Our approach illustrates that it can create DAGs with topology and node types that meet specific requirements in conditions where rewards are highly sparse.",1
"Despite significant recent advances in deep neural networks, training them remains a challenge due to the highly non-convex nature of the objective function. State-of-the-art methods rely on error backpropagation, which suffers from several well-known issues, such as vanishing and exploding gradients, inability to handle non-differentiable nonlinearities and to parallelize weight-updates across layers, and biological implausibility. These limitations continue to motivate exploration of alternative training algorithms, including several recently proposed auxiliary-variable methods which break the complex nested objective function into local subproblems. However, those techniques are mainly offline (batch), which limits their applicability to extremely large datasets, as well as to online, continual or reinforcement learning. The main contribution of our work is a novel online (stochastic/mini-batch) alternating minimization (AM) approach for training deep neural networks, together with the first theoretical convergence guarantees for AM in stochastic settings and promising empirical results on a variety of architectures and datasets.",0
"Despite significant progress in deep neural networks, training them remains a challenge due to the highly non-convex nature of the objective function. The current state-of-the-art method, error backpropagation, has well-known limitations such as vanishing and exploding gradients, inability to handle non-differentiable nonlinearities, and biological implausibility. As a result, alternative training algorithms have been proposed, including auxiliary-variable methods that break the objective function into local subproblems. However, these methods are mainly suitable for batch training and limit their applicability to large datasets and online learning. Our work proposes a novel online alternating minimization approach for training deep neural networks, which includes theoretical convergence guarantees in stochastic settings and promising results on various architectures and datasets.",1
"Model-based reinforcement learning (MBRL) has been proposed as a promising alternative solution to tackle the high sampling cost challenge in the canonical reinforcement learning (RL), by leveraging a learned model to generate synthesized data for policy training purpose. The MBRL framework, nevertheless, is inherently limited by the convoluted process of jointly learning control policy and configuring hyper-parameters (e.g., global/local models, real and synthesized data, etc). The training process could be tedious and prohibitively costly. In this research, we propose an ""reinforcement on reinforcement"" (RoR) architecture to decompose the convoluted tasks into two layers of reinforcement learning. The inner layer is the canonical model-based RL training process environment (TPE), which learns the control policy for the underlying system and exposes interfaces to access states, actions and rewards. The outer layer presents an RL agent, called as AI trainer, to learn an optimal hyper-parameter configuration for the inner TPE. This decomposition approach provides a desirable flexibility to implement different trainer designs, called as ""train the trainer"". In our research, we propose and optimize two alternative trainer designs: 1) a uni-head trainer and 2) a multi-head trainer. Our proposed RoR framework is evaluated for five tasks in the OpenAI gym (i.e., Pendulum, Mountain Car, Reacher, Half Cheetah and Swimmer). Compared to three other baseline algorithms, our proposed Train-the-Trainer algorithm has a competitive performance in auto-tuning capability, with upto 56% expected sampling cost saving without knowing the best parameter setting in advance. The proposed trainer framework can be easily extended to other cases in which the hyper-parameter tuning is costly.",0
"To address the issue of high sampling costs in traditional reinforcement learning (RL), model-based reinforcement learning (MBRL) has been proposed as a viable solution that leverages a learned model to generate synthesized data for policy training. However, the MBRL framework is limited by the complexity of jointly learning control policies and configuring hyper-parameters. This can make the training process tedious and prohibitively expensive. To overcome these challenges, we propose a ""reinforcement on reinforcement"" (RoR) architecture that decomposes the training process into two layers of reinforcement learning. The inner layer involves the canonical model-based RL training process environment (TPE), which learns the control policy for the underlying system and provides access to states, actions, and rewards. The outer layer comprises an RL agent, known as the AI trainer, which learns an optimal hyper-parameter configuration for the inner TPE. This approach offers flexibility to implement different trainer designs, also known as ""train the trainer"". We optimized two alternative trainer designs, namely a uni-head trainer and a multi-head trainer, and evaluated our proposed RoR framework for five tasks in the OpenAI gym. Our Train-the-Trainer algorithm outperformed three other baseline algorithms in terms of auto-tuning capability, achieving up to 56% expected sampling cost savings without prior knowledge of the best parameter settings. This proposed trainer framework can be easily extended to other scenarios that involve costly hyper-parameter tuning.",1
"In this thesis, we draw inspiration from both classical system identification and modern machine learning in order to solve estimation problems for real-world, physical systems. The main approach to estimation and learning adopted is optimization based. Concepts such as regularization will be utilized for encoding of prior knowledge and basis-function expansions will be used to add nonlinear modeling power while keeping data requirements practical. The thesis covers a wide range of applications, many inspired by applications within robotics, but also extending outside this already wide field. Usage of the proposed methods and algorithms are in many cases illustrated in the real-world applications that motivated the research. Topics covered include dynamics modeling and estimation, model-based reinforcement learning, spectral estimation, friction modeling and state estimation and calibration in robotic machining. In the work on modeling and identification of dynamics, we develop regularization strategies that allow us to incorporate prior domain knowledge into flexible, overparameterized models. We make use of classical control theory to gain insight into training and regularization while using flexible tools from modern deep learning. A particular focus of the work is to allow use of modern methods in scenarios where gathering data is associated with a high cost. In the robotics-inspired parts of the thesis, we develop methods that are practically motivated and ensure that they are implementable also outside the research setting. We demonstrate this by performing experiments in realistic settings and providing open-source implementations of all proposed methods and algorithms.",0
"This thesis combines classical system identification and modern machine learning to address estimation problems for physical systems in real-world scenarios, using an optimization-based approach. Regularization is utilized to incorporate prior knowledge, and basis-function expansions allow for nonlinear modeling while maintaining practical data requirements. The applications covered are wide-ranging, including robotics and beyond, with real-world examples provided to illustrate the proposed methods and algorithms. The thesis explores topics such as dynamics modeling and estimation, model-based reinforcement learning, spectral estimation, friction modeling, and state estimation and calibration in robotic machining. The goal is to develop regularization strategies that enable the incorporation of domain knowledge into flexible models, leveraging classical control theory and modern deep learning tools. A key focus is on making these methods practical and cost-effective, even in scenarios where data gathering is challenging. The robotics-inspired components of the thesis prioritize implementability, with experiments conducted in realistic settings and open-source implementations provided for all proposed methods and algorithms.",1
"We present an approach to make molecular optimization more efficient. We infer a hypergraph replacement grammar from the ChEMBL database, count the frequencies of particular rules being used to expand particular nonterminals in other rules, and use these as conditional priors for the policy model. Simulating random molecules from the resulting probabilistic grammar, we show that conditional priors result in a molecular distribution closer to the training set than using equal rule probabilities or unconditional priors. We then treat molecular optimization as a reinforcement learning problem, using a novel modification of the policy gradient algorithm - batch-advantage: using individual rewards minus the batch average reward to weight the log probability loss. The reinforcement learning agent is tasked with building molecules using this grammar, with the goal of maximizing benchmark scores available from the literature. To do so, the agent has policies both to choose the next node in the graph to expand and to select the next grammar rule to apply. The policies are implemented using the Transformer architecture with the partially expanded graph as the input at each step. We show that using the empirical priors as the starting point for a policy eliminates the need for pre-training, and allows us to reach optima faster. We achieve competitive performance on common benchmarks from the literature, such as penalized logP and QED, with only hundreds of training steps on a budget GPU instance.",0
"Our proposed method aims to enhance molecular optimization by using a hypergraph replacement grammar generated from the ChEMBL database. We compute the frequencies of specific rules used to expand nonterminals in other rules, which serve as conditional priors for the policy model. By simulating random molecules from the resultant probabilistic grammar, we demonstrate that the conditional priors result in a molecular distribution that is more similar to the training set compared to using equal rule probabilities or unconditional priors. We treat molecular optimization as a reinforcement learning problem by using a novel modification of the policy gradient algorithm called batch-advantage, which utilizes individual rewards minus the batch average reward to weigh the log probability loss. Our reinforcement learning agent is tasked with constructing molecules using this grammar while maximizing benchmark scores found in the literature. The agent has policies for selecting the next node and grammar rule to apply, implemented using the Transformer architecture with the partially expanded graph as input at each step. We show that using empirical priors as a starting point for the policy eliminates the need for pre-training and leads to faster reaching of optima. We achieve competitive performance on commonly used benchmarks, such as penalized logP and QED, with only a few hundred training steps on a budget GPU instance.",1
"We present RL-VAE, a graph-to-graph variational autoencoder that uses reinforcement learning to decode molecular graphs from latent embeddings. Methods have been described previously for graph-to-graph autoencoding, but these approaches require sophisticated decoders that increase the complexity of training and evaluation (such as requiring parallel encoders and decoders or non-trivial graph matching). Here, we repurpose a simple graph generator to enable efficient decoding and generation of molecular graphs.",0
"RL-VAE, an autoencoder that uses reinforcement learning to decode molecular graphs from latent embeddings, is introduced in this study. While previous methods have outlined graph-to-graph autoencoding, they entail complex decoders that complicate training and evaluation by requiring parallel encoders and decoders or non-trivial graph matching. Instead, we have adapted a basic graph generator to achieve efficient decoding and generation of molecular graphs.",1
We propose reinforcement learning on simple networks consisting of random connections of spiking neurons (both recurrent and feed-forward) that can learn complex tasks with very little trainable parameters. Such sparse and randomly interconnected recurrent spiking networks exhibit highly non-linear dynamics that transform the inputs into rich high-dimensional representations based on past context. The random input representations can be efficiently interpreted by an output (or readout) layer with trainable parameters. Systematic initialization of the random connections and training of the readout layer using Q-learning algorithm enable such small random spiking networks to learn optimally and achieve the same learning efficiency as humans on complex reinforcement learning tasks like Atari games. The spike-based approach using small random recurrent networks provides a computationally efficient alternative to state-of-the-art deep reinforcement learning networks with several layers of trainable parameters. The low-complexity spiking networks can lead to improved energy efficiency in event-driven neuromorphic hardware for complex reinforcement learning tasks.,0
"Our proposal is to use reinforcement learning on uncomplicated networks that are composed of spiking neurons that are randomly connected, both feed-forward and recurrent. These networks require very few trainable parameters, yet they can learn complex tasks. The recurrent spiking networks that are sparsely and randomly interconnected have highly non-linear dynamics that can transform inputs into high-dimensional representations that are rich in past context. The output or readout layer, which can be trained with parameters, can efficiently interpret the random input representations. By systematically initializing random connections and using the Q-learning algorithm to train the readout layer, these small random spiking networks can learn optimally and achieve the same efficiency as humans on challenging reinforcement learning tasks, such as Atari games. Using spike-based approaches on small, random recurrent networks provides a computationally efficient alternative to the deep reinforcement learning networks with multiple trainable parameter layers that are currently available. These low-complexity spiking networks could lead to improved energy efficiency in event-driven neuromorphic hardware for complex reinforcement learning tasks.",1
"Polynomial inequalities lie at the heart of many mathematical disciplines. In this paper, we consider the fundamental computational task of automatically searching for proofs of polynomial inequalities. We adopt the framework of semi-algebraic proof systems that manipulate polynomial inequalities via elementary inference rules that infer new inequalities from the premises. These proof systems are known to be very powerful, but searching for proofs remains a major difficulty. In this work, we introduce a machine learning based method to search for a dynamic proof within these proof systems. We propose a deep reinforcement learning framework that learns an embedding of the polynomials and guides the choice of inference rules, taking the inherent symmetries of the problem as an inductive bias. We compare our approach with powerful and widely-studied linear programming hierarchies based on static proof systems, and show that our method reduces the size of the linear program by several orders of magnitude while also improving performance. These results hence pave the way towards augmenting powerful and well-studied semi-algebraic proof systems with machine learning guiding strategies for enhancing the expressivity of such proof systems.",0
"The significance of polynomial inequalities in various mathematical fields is undeniable. This study focuses on the crucial task of automatically searching for polynomial inequality proofs. Semi-algebraic proof systems are used in this study, which utilize elementary inference rules to deduce new inequalities from premises. Although these proof systems are potent, finding proofs remains a challenge. Therefore, this research proposes a machine learning-based approach to search for dynamic proof within the semi-algebraic proof systems. The deep reinforcement learning framework is adopted to learn polynomial embedding and guide the choice of inference rules based on the problem's inherent symmetries. The results of this approach are compared with linear programming hierarchies based on static proof systems, and it is demonstrated that this method significantly reduces the size of the linear program while improving performance. This study sets the stage for augmenting powerful semi-algebraic proof systems with machine learning techniques to enhance their expressivity.",1
"Visual navigation in complex environments is inefficient with traditional reactive policy or general-purposed recurrent policy. To address the long-term memory issue, this paper proposes a graph attention memory (GAM) architecture consisting of memory construction module, graph attention module and control module. The memory construction module builds the topological graph based on supervised learning by taking the exploration prior. Then, guided attention features are extracted with the graph attention module. Finally, the deep reinforcement learning based control module makes decisions based on visual observations and guided attention features. Detailed convergence analysis of GAM is presented in this paper. We evaluate GAM-based navigation system in two complex 3D environments. Experimental results show that the GAM-based navigation system significantly improves learning efficiency and outperforms all baselines in average success rate.",0
"Conventional reactive or general-purpose recurrent policies are not effective for navigating complex environments visually. To overcome the issue of long-term memory, the proposed solution in this study is a graph attention memory (GAM) architecture, which comprises of a memory construction module, a graph attention module, and a control module. The memory construction module builds a topological graph based on supervised learning using the exploration prior. The graph attention module then extracts guided attention features. Finally, the deep reinforcement learning-based control module makes decisions based on visual observations and guided attention features. The paper provides a detailed analysis of the convergence of the GAM. The GAM-based navigation system is evaluated in two complex 3D environments, and experimental results demonstrate that it considerably improves learning efficiency and outperforms all baselines in average success rate.",1
"The posterior variance of Gaussian processes is a valuable measure of the learning error which is exploited in various applications such as safe reinforcement learning and control design. However, suitable analysis of the posterior variance which captures its behavior for finite and infinite number of training data is missing. This paper derives a novel bound for the posterior variance function which requires only local information because it depends only on the number of training samples in the proximity of a considered test point. Furthermore, we prove sufficient conditions which ensure the convergence of the posterior variance to zero. Finally, we demonstrate that the extension of our bound to an average learning bound outperforms existing approaches.",0
"The use of posterior variance in Gaussian processes is important for assessing learning error and is applied in various fields like safe reinforcement learning and control design. Unfortunately, a comprehensive analysis of the posterior variance that considers its behavior for both finite and infinite training data is absent. To address this, we present a new bound for the posterior variance function that only requires local information, specifically the number of training samples near a test point. Additionally, we establish certain conditions that ensure the posterior variance converges to zero. Lastly, we show that our bound, when extended to an average learning bound, performs better than existing methods.",1
"Autonomous multiple tasks learning is a fundamental capability to develop versatile artificial agents that can act in complex environments. In real-world scenarios, tasks may be interrelated (or ""hierarchical"") so that a robot has to first learn to achieve some of them to set the preconditions for learning other ones. Even though different strategies have been used in robotics to tackle the acquisition of interrelated tasks, in particular within the developmental robotics framework, autonomous learning in this kind of scenarios is still an open question. Building on previous research in the framework of intrinsically motivated open-ended learning, in this work we describe how this question can be addressed working on the level of task selection, in particular considering the multiple interrelated tasks scenario as an MDP where the system is trying to maximise its competence over all the tasks.",0
"Developing versatile artificial agents that can operate in complex environments requires the ability to learn multiple tasks autonomously. In real-world situations, tasks may be hierarchical, meaning that a robot must first learn to accomplish some tasks before it can learn others. While various strategies have been employed in robotics to address the acquisition of interrelated tasks, this remains an open question in the field of autonomous learning. This study builds on previous research in intrinsically motivated open-ended learning and proposes a solution for this challenge. The approach focuses on task selection and treats the multiple interrelated tasks scenario as a Markov Decision Process (MDP) where the system aims to maximize its competency across all tasks.",1
"Many recent successful (deep) reinforcement learning algorithms make use of regularization, generally based on entropy or Kullback-Leibler divergence. We propose a general theory of regularized Markov Decision Processes that generalizes these approaches in two directions: we consider a larger class of regularizers, and we consider the general modified policy iteration approach, encompassing both policy iteration and value iteration. The core building blocks of this theory are a notion of regularized Bellman operator and the Legendre-Fenchel transform, a classical tool of convex optimization. This approach allows for error propagation analyses of general algorithmic schemes of which (possibly variants of) classical algorithms such as Trust Region Policy Optimization, Soft Q-learning, Stochastic Actor Critic or Dynamic Policy Programming are special cases. This also draws connections to proximal convex optimization, especially to Mirror Descent.",0
"Regularization is a common technique used in (deep) reinforcement learning algorithms to improve their success rates. Typically, this involves using entropy or Kullback-Leibler divergence. However, we propose a new theory that extends this approach in two ways. Firstly, we consider a wider range of regularizers, and secondly, we explore the modified policy iteration approach, which encompasses both policy iteration and value iteration. Our theory is based on the regularized Bellman operator and the Legendre-Fenchel transform, which are fundamental concepts in convex optimization. Using this approach, we can analyze the error propagation of various algorithmic schemes, including classical methods such as Trust Region Policy Optimization, Soft Q-learning, Stochastic Actor Critic, and Dynamic Policy Programming. This theory also has connections to proximal convex optimization, specifically Mirror Descent.",1
"Truckload brokerages, a $100 billion/year industry in the U.S., plays the critical role of matching shippers with carriers, often to move loads several days into the future. Brokerages not only have to find companies that will agree to move a load, the brokerage often has to find a price that both the shipper and carrier will agree to. The price not only varies by shipper and carrier, but also by the traffic lanes and other variables such as commodity type. Brokerages have to learn about shipper and carrier response functions by offering a price and observing whether each accepts the quote. We propose a knowledge gradient policy with bootstrap aggregation for high-dimensional contextual settings to guide price experimentation by maximizing the value of information. The learning policy is tested using a carefully calibrated fleet simulator that includes a stochastic lookahead policy that simulates fleet movements, as well as the stochastic modeling of driver assignments and the carrier's load commitment policies with advance booking.",0
"In the United States, the truckload brokerage industry is worth $100 billion annually and plays a crucial role in connecting shippers with carriers for loads that are often transported several days in advance. The brokers not only have to find companies willing to transport the load, but also negotiate a price that satisfies both the shipper and carrier. This price can vary depending on factors such as traffic lanes and commodity type. To determine the appropriate price, brokers must learn about the response functions of both shippers and carriers by offering a quote and observing their acceptance. To maximize the value of information, we suggest using a knowledge gradient policy with bootstrap aggregation for high-dimensional contextual settings to guide price experimentation. This learning policy is tested using a fleet simulator that includes stochastic lookahead policy, driver assignments, and advance booking policies of carriers.",1
"While multi-agent interactions can be naturally modeled as a graph, the environment has traditionally been considered as a black box. We propose to create a shared agent-entity graph, where agents and environmental entities form vertices, and edges exist between the vertices which can communicate with each other. Agents learn to cooperate by exchanging messages along the edges of this graph. Our proposed multi-agent reinforcement learning framework is invariant to the number of agents or entities present in the system as well as permutation invariance, both of which are desirable properties for any multi-agent system representation. We present state-of-the-art results on coverage, formation and line control tasks for multi-agent teams in a fully decentralized framework and further show that the learned policies quickly transfer to scenarios with different team sizes along with strong zero-shot generalization performance. This is an important step towards developing multi-agent teams which can be realistically deployed in the real world without assuming complete prior knowledge or instantaneous communication at unbounded distances.",0
"Traditionally, the environment in multi-agent interactions has been treated as a black box, even though it can be easily represented as a graph. In order to foster cooperation between agents, we suggest creating a shared agent-entity graph where vertices represent agents and environmental entities, and edges enable communication between them. Through message exchange along these edges, agents learn to work together. Our proposed multi-agent reinforcement learning framework is applicable to any number of agents or entities and is permutation invariant, which are both highly desirable qualities in a multi-agent system representation. We demonstrate outstanding results on coverage, formation, and line control tasks for fully decentralized multi-agent teams. Furthermore, we show that learned policies can be easily applied to scenarios with varying team sizes and exhibit strong zero-shot generalization capabilities. This represents a major step towards deploying multi-agent teams in the real world without requiring prior knowledge or instantaneous communication over unlimited distances.",1
"Despite significant progress, deep reinforcement learning (RL) suffers from data-inefficiency and limited generalization. Recent efforts apply meta-learning to learn a meta-learner from a set of RL tasks such that a novel but related task could be solved quickly. Though specific in some ways, different tasks in meta-RL are generally similar at a high level. However, most meta-RL methods do not explicitly and adequately model the specific and shared information among different tasks, which limits their ability to learn training tasks and to generalize to novel tasks. In this paper, we propose to capture the shared information on the one hand and meta-learn how to quickly abstract the specific information about a task on the other hand. Methodologically, we train an SGD meta-learner to quickly optimize a task encoder for each task, which generates a task embedding based on past experience. Meanwhile, we learn a policy which is shared across all tasks and conditioned on task embeddings. Empirical results on four simulated tasks demonstrate that our method has better learning capacity on both training and novel tasks and attains up to 3 to 4 times higher returns compared to baselines.",0
"Although deep reinforcement learning (RL) has made significant progress, it still faces challenges related to limited generalization and data-inefficiency. To address these issues, recent studies have introduced meta-learning, which involves training a meta-learner on a set of RL tasks to enable quick solving of novel, related tasks. However, most meta-RL methods fail to effectively model the specific and shared information among different tasks, which could hinder their ability to learn and generalize to new tasks. This paper proposes a solution that captures both the shared information and specific task information. Specifically, the paper trains an SGD meta-learner to optimize a task encoder for each task, creating a task embedding based on past experiences, and a shared policy that is conditioned on task embeddings. The paper presents empirical results from four simulated tasks, demonstrating that their method has better learning capacity and achieves up to 3 to 4 times higher returns compared to baselines.",1
"One problem in the application of reinforcement learning to real-world problems is the curse of dimensionality on the action space. Macro actions, a sequence of primitive actions, have been studied to diminish the dimensionality of the action space with regard to the time axis. However, previous studies relied on humans defining macro actions or assumed macro actions as repetitions of the same primitive actions. We present Factorized Macro Action Reinforcement Learning (FaMARL) which autonomously learns disentangled factor representation of a sequence of actions to generate macro actions that can be directly applied to general reinforcement learning algorithms. FaMARL exhibits higher scores than other reinforcement learning algorithms on environments that require an extensive amount of search.",0
"The challenge of applying reinforcement learning to real-world problems is the issue of the action space becoming too complex. Researchers have explored the use of macro actions, which involve a series of basic actions, to simplify the action space with respect to time. However, previous studies have relied on human input to define macro actions or have assumed that macro actions are simply repeated basic actions. To address this limitation, our team has developed Factorized Macro Action Reinforcement Learning (FaMARL), which can independently learn a disentangled factor representation of a sequence of actions to generate macro actions that can be used in general reinforcement learning algorithms. FaMARL has shown superior results compared to other reinforcement learning algorithms in environments that require extensive search.",1
"Despite the numerous advances, reinforcement learning remains away from widespread acceptance for autonomous controller design as compared to classical methods due to lack of ability to effectively tackle the reality gap. The reliance on absolute or deterministic reward as a metric for optimization process renders reinforcement learning highly susceptible to changes in problem dynamics. We introduce a novel framework that effectively quantizes the uncertainty of the design space and induces robustness in controllers by switching to a reliability-based optimization routine. The data efficiency of the method is maintained to match reward based optimization methods by employing a model-based approach. We prove the stability of learned neuro-controllers in both static and dynamic environments on classical reinforcement learning tasks such as Cart Pole balancing and Inverted Pendulum.",0
"Despite significant advancements, reinforcement learning has not gained as much popularity as classical methods for autonomous controller design, mainly because it struggles to overcome the reality gap. The use of absolute or deterministic reward as a metric in the optimization process makes reinforcement learning vulnerable to changes in problem dynamics. To address this, we propose a new framework that quantifies the uncertainty of the design space and enhances controller robustness through a reliability-based optimization routine. Our approach maintains the data efficiency of reward-based optimization methods by utilizing a model-based approach. We demonstrate the stability of learned neuro-controllers in both static and dynamic environments on standard reinforcement learning tasks, such as Cart Pole balancing and Inverted Pendulum.",1
"This paper proposes a novel scheme for the watermarking of Deep Reinforcement Learning (DRL) policies. This scheme provides a mechanism for the integration of a unique identifier within the policy in the form of its response to a designated sequence of state transitions, while incurring minimal impact on the nominal performance of the policy. The applications of this watermarking scheme include detection of unauthorized replications of proprietary policies, as well as enabling the graceful interruption or termination of DRL activities by authorized entities. We demonstrate the feasibility of our proposal via experimental evaluation of watermarking a DQN policy trained in the Cartpole environment.",0
"A new method for watermarking Deep Reinforcement Learning (DRL) policies is presented in this paper. The proposed approach involves embedding a distinct identifier into the policy using a designated sequence of state transitions. This watermarking technique has minimal impact on the policy's performance and allows for detecting unauthorized copies of proprietary policies and authorized termination of DRL activities. To validate the feasibility of the proposed scheme, we conducted experiments by watermarking a DQN policy trained in the Cartpole environment.",1
"This paper investigates a class of attacks targeting the confidentiality aspect of security in Deep Reinforcement Learning (DRL) policies. Recent research have established the vulnerability of supervised machine learning models (e.g., classifiers) to model extraction attacks. Such attacks leverage the loosely-restricted ability of the attacker to iteratively query the model for labels, thereby allowing for the forging of a labeled dataset which can be used to train a replica of the original model. In this work, we demonstrate the feasibility of exploiting imitation learning techniques in launching model extraction attacks on DRL agents. Furthermore, we develop proof-of-concept attacks that leverage such techniques for black-box attacks against the integrity of DRL policies. We also present a discussion on potential solution concepts for mitigation techniques.",0
"The purpose of this study is to examine a group of attacks that focus on the confidentiality aspect of security within Deep Reinforcement Learning (DRL) policies. Previous research has shown that supervised machine learning models, such as classifiers, are susceptible to model extraction attacks due to the attacker's ability to repeatedly query the model for labels, which can be used to create a labeled dataset to replicate the original model. The research presented here demonstrates how imitation learning techniques can be used to launch model extraction attacks on DRL agents and provides proof-of-concept attacks that can be used for black-box attacks against the integrity of DRL policies. Additionally, potential solutions for mitigation techniques are discussed.",1
"This paper investigates the resilience and robustness of Deep Reinforcement Learning (DRL) policies to adversarial perturbations in the state space. We first present an approach for the disentanglement of vulnerabilities caused by representation learning of DRL agents from those that stem from the sensitivity of the DRL policies to distributional shifts in state transitions. Building on this approach, we propose two RL-based techniques for quantitative benchmarking of adversarial resilience and robustness in DRL policies against perturbations of state transitions. We demonstrate the feasibility of our proposals through experimental evaluation of resilience and robustness in DQN, A2C, and PPO2 policies trained in the Cartpole environment.",0
"The objective of this research is to examine the ability of Deep Reinforcement Learning (DRL) policies to withstand adversarial perturbations in the state space. Initially, we introduce an approach to differentiate between vulnerabilities that arise from the representation learning of DRL agents and those that result from the sensitivity of DRL policies to distributional shifts in state transitions. Based on this method, we suggest two RL-based techniques to quantitatively evaluate the adversarial resilience and robustness of DRL policies against state transition perturbations. Our proposals are tested through experiments on DQN, A2C, and PPO2 policies trained in the Cartpole environment, demonstrating their feasibility.",1
"Active learning from demonstration allows a robot to query a human for specific types of input to achieve efficient learning. Existing work has explored a variety of active query strategies; however, to our knowledge, none of these strategies directly minimize the performance risk of the policy the robot is learning. Utilizing recent advances in performance bounds for inverse reinforcement learning, we propose a risk-aware active inverse reinforcement learning algorithm that focuses active queries on areas of the state space with the potential for large generalization error. We show that risk-aware active learning outperforms standard active IRL approaches on gridworld, simulated driving, and table setting tasks, while also providing a performance-based stopping criterion that allows a robot to know when it has received enough demonstrations to safely perform a task.",0
"The process of active learning from demonstration enables a robot to seek specific types of input from a human to learn more efficiently. Although various strategies for active queries have been explored, none have been found to directly reduce the risk of the policy being learned. In light of recent advances in performance bounds for inverse reinforcement learning, we propose a new algorithm for risk-aware active inverse reinforcement learning that focuses on areas of the state space that carry the potential for high generalization error. Our study demonstrates that risk-aware active learning outperforms standard active IRL approaches across different tasks, including gridworld, simulated driving, and table setting tasks. Moreover, it provides a performance-based stopping criterion that enables a robot to determine when it has received sufficient demos to safely carry out a task.",1
"In this paper, the problem of describing visual contents of a video sequence with natural language is addressed. Unlike previous video captioning work mainly exploiting the cues of video contents to make a language description, we propose a reconstruction network (RecNet) in a novel encoder-decoder-reconstructor architecture, which leverages both forward (video to sentence) and backward (sentence to video) flows for video captioning. Specifically, the encoder-decoder component makes use of the forward flow to produce a sentence description based on the encoded video semantic features. Two types of reconstructors are subsequently proposed to employ the backward flow and reproduce the video features from local and global perspectives, respectively, capitalizing on the hidden state sequence generated by the decoder. Moreover, in order to make a comprehensive reconstruction of the video features, we propose to fuse the two types of reconstructors together. The generation loss yielded by the encoder-decoder component and the reconstruction loss introduced by the reconstructor are jointly cast into training the proposed RecNet in an end-to-end fashion. Furthermore, the RecNet is fine-tuned by CIDEr optimization via reinforcement learning, which significantly boosts the captioning performance. Experimental results on benchmark datasets demonstrate that the proposed reconstructor can boost the performance of video captioning consistently.",0
"This paper addresses the issue of describing the visual aspects of a video sequence using natural language. Instead of solely relying on video content cues to create a language description, we propose a novel encoder-decoder-reconstructor architecture called RecNet. This architecture utilizes both forward and backward flows for video captioning. The encoder-decoder component produces a sentence description based on the encoded video semantic features, while two types of reconstructors use the backward flow to reproduce the video features from local and global perspectives. To achieve a comprehensive reconstruction of the video features, we fuse the two types of reconstructors. The RecNet is trained end-to-end by casting the generation loss and reconstruction loss into the optimization process. Moreover, we fine-tune the RecNet using CIDEr optimization via reinforcement learning, which significantly improves the captioning performance. Experimental results on benchmark datasets demonstrate that the proposed reconstructor consistently enhances the performance of video captioning.",1
"While off-policy temporal difference (TD) methods have widely been used in reinforcement learning due to their efficiency and simple implementation, their Bayesian counterparts have not been utilized as frequently. One reason is that the non-linear max operation in the Bellman optimality equation makes it difficult to define conjugate distributions over the value functions. In this paper, we introduce a novel Bayesian approach to off-policy TD methods, called as ADFQ, which updates beliefs on state-action values, Q, through an online Bayesian inference method known as Assumed Density Filtering. We formulate an efficient closed-form solution for the value update by approximately estimating analytic parameters of the posterior of the Q-beliefs. Uncertainty measures in the beliefs not only are used in exploration but also provide a natural regularization for the value update considering all next available actions. ADFQ converges to Q-learning as the uncertainty measures of the Q-beliefs decrease and improves common drawbacks of other Bayesian RL algorithms such as computational complexity. We extend ADFQ with a neural network. Our empirical results demonstrate that ADFQ outperforms comparable algorithms on various Atari 2600 games, with drastic improvements in highly stochastic domains or domains with a large action space.",0
"Off-policy temporal difference (TD) methods are commonly used in reinforcement learning due to their simplicity and efficiency. However, their Bayesian counterparts are not as frequently used, partly due to the difficulty in defining conjugate distributions over the value functions caused by the non-linear max operation in the Bellman optimality equation. This paper introduces a novel Bayesian approach called ADFQ, which updates beliefs on state-action values using Assumed Density Filtering. By approximately estimating analytic parameters of the posterior of the Q-beliefs, we formulate an efficient closed-form solution for the value update. Uncertainty measures in the beliefs are used for exploration and regularization for the value update, considering all next available actions. ADFQ converges to Q-learning as the uncertainty measures decrease and improves on the computational complexity of other Bayesian RL algorithms. ADFQ is extended with a neural network, and empirical results show that it outperforms comparable algorithms on various Atari 2600 games, particularly in highly stochastic domains or domains with large action spaces.",1
"Recent results in Reinforcement Learning (RL) have shown that agents with limited training environments are susceptible to a large amount of overfitting across many domains. A key challenge for RL generalization is to quantitatively explain the effects of changing parameters on testing performance. Such parameters include architecture, regularization, and RL-dependent variables such as discount factor and action stochasticity. We provide empirical results that show complex and interdependent relationships between hyperparameters and generalization. We further show that several empirical metrics such as gradient cosine similarity and trajectory-dependent metrics serve to provide intuition towards these results.",0
"In the field of Reinforcement Learning (RL), recent findings reveal that agents who are trained in constrained environments are prone to experiencing significant levels of overfitting in various domains. One of the major obstacles faced by RL generalization is the ability to provide a quantitative explanation of how adjustments to parameters affect testing performance. These parameters comprise of elements such as architecture, regularization, and RL-dependent variables (i.e., discount factor and action stochasticity). Our research presents empirical evidence that demonstrates the intricate and interconnected relationships between hyperparameters and generalization. Furthermore, we demonstrate how various empirical metrics, including gradient cosine similarity and trajectory-dependent metrics, can help provide insight into these results.",1
"Inverse reinforcement learning (IRL) is the problem of finding a reward function that generates a given optimal policy for a given Markov Decision Process. This paper looks at an algorithmic-independent geometric analysis of the IRL problem with finite states and actions. A L1-regularized Support Vector Machine formulation of the IRL problem motivated by the geometric analysis is then proposed with the basic objective of the inverse reinforcement problem in mind: to find a reward function that generates a specified optimal policy. The paper further analyzes the proposed formulation of inverse reinforcement learning with $n$ states and $k$ actions, and shows a sample complexity of $O(n^2 \log (nk))$ for recovering a reward function that generates a policy that satisfies Bellman's optimality condition with respect to the true transition probabilities.",0
"The paper explores Inverse Reinforcement Learning (IRL), which involves identifying a reward function that produces an optimal policy for a given Markov Decision Process. It proposes a geometric analysis of the IRL problem for finite states and actions, which is independent of the algorithm. The paper then introduces a L1-regularized Support Vector Machine formulation of the IRL problem, with the aim of finding a reward function that generates a particular optimal policy. The formulation is analyzed in the context of $n$ states and $k$ actions, and it is demonstrated that a sample complexity of $O(n^2 \log (nk))$ can retrieve a reward function that satisfies Bellman's optimality condition based on the true transition probabilities.",1
"Several recent papers have examined generalization in reinforcement learning (RL), by proposing new environments or ways to add noise to existing environments, then benchmarking algorithms and model architectures on those environments. We discuss subtle conceptual properties of RL benchmarks that are not required in supervised learning (SL), and also properties that an RL benchmark should possess. Chief among them is one we call the principle of unchanged optimality: there should exist a single $\pi$ that is optimal across all train and test tasks. In this work, we argue why this principle is important, and ways it can be broken or satisfied due to subtle choices in state representation or model architecture. We conclude by discussing challenges and future lines of research in theoretically analyzing generalization benchmarks.",0
"In recent studies, researchers have explored generalization in reinforcement learning (RL) by introducing new environments or techniques to introduce noise to existing environments. They then assess the performance of algorithms and model architectures on these environments. In this paper, we delve into the nuanced conceptual characteristics of RL benchmarks that are not necessary in supervised learning (SL). We also highlight the essential properties that an RL benchmark should possess, including the principle of unchanged optimality, which necessitates the existence of a single optimal $\pi$ across all training and testing tasks. We explain why this principle is critical and how it can be realized or violated due to subtle choices in state representation or model architecture. Finally, we discuss the obstacles facing researchers in theoretically analyzing generalization benchmarks and potential avenues for future research.",1
"In this paper, we propose Rogue-Gym, a simple and classic style roguelike game built for evaluating generalization in reinforcement learning (RL). Combined with the recent progress of deep neural networks, RL has successfully trained human-level agents without human knowledge in many games such as those for Atari 2600. However, it has been pointed out that agents trained with RL methods often overfit the training environment, and they work poorly in slightly different environments. To investigate this problem, some research environments with procedural content generation have been proposed. Following these studies, we propose the use of roguelikes as a benchmark for evaluating the generalization ability of RL agents. In our Rogue-Gym, agents need to explore dungeons that are structured differently each time they start a new game. Thanks to the very diverse structures of the dungeons, we believe that the generalization benchmark of Rogue-Gym is sufficiently fair. In our experiments, we evaluate a standard reinforcement learning method, PPO, with and without enhancements for generalization. The results show that some enhancements believed to be effective fail to mitigate the overfitting in Rogue-Gym, although others slightly improve the generalization ability.",0
"The main idea of our paper is to introduce Rogue-Gym, a straightforward and traditional roguelike game, as a means for assessing the generalization capabilities of reinforcement learning (RL). While deep neural networks have enabled RL to train agents that can perform at a human-level in various games such as those for Atari 2600, there is a common issue of RL-trained agents overfitting to their training environments, leading to poor performance in slightly different environments. To address this problem, some studies have proposed research environments that involve procedural content generation. In line with these studies, we suggest that roguelikes can serve as an effective benchmark for evaluating the generalization ability of RL agents. Our Rogue-Gym game requires agents to navigate through randomly structured dungeons each time they play, which ensures a fair and diverse evaluation of their generalization capabilities. We conducted experiments using a standard RL method, PPO, with and without enhancements for generalization, and found that while some enhancements failed to mitigate overfitting in Rogue-Gym, others were able to slightly improve the generalization ability of the agents.",1
"We study how to set channel numbers in a neural network to achieve better accuracy under constrained resources (e.g., FLOPs, latency, memory footprint or model size). A simple and one-shot solution, named AutoSlim, is presented. Instead of training many network samples and searching with reinforcement learning, we train a single slimmable network to approximate the network accuracy of different channel configurations. We then iteratively evaluate the trained slimmable model and greedily slim the layer with minimal accuracy drop. By this single pass, we can obtain the optimized channel configurations under different resource constraints. We present experiments with MobileNet v1, MobileNet v2, ResNet-50 and RL-searched MNasNet on ImageNet classification. We show significant improvements over their default channel configurations. We also achieve better accuracy than recent channel pruning methods and neural architecture search methods.   Notably, by setting optimized channel numbers, our AutoSlim-MobileNet-v2 at 305M FLOPs achieves 74.2% top-1 accuracy, 2.4% better than default MobileNet-v2 (301M FLOPs), and even 0.2% better than RL-searched MNasNet (317M FLOPs). Our AutoSlim-ResNet-50 at 570M FLOPs, without depthwise convolutions, achieves 1.3% better accuracy than MobileNet-v1 (569M FLOPs). Code and models will be available at: https://github.com/JiahuiYu/slimmable_networks",0
"Our research focuses on optimizing channel numbers in neural networks to enhance accuracy while working within limited resources, such as FLOPs, latency, memory footprint, or model size. We have developed a solution called AutoSlim, which is a simple and one-shot approach that eliminates the need to train multiple network samples and conduct reinforcement learning searches. Instead, we train a single slimmable network that approximates the network accuracy of different channel configurations. We then evaluate the trained model and slim the layer with the least accuracy drop. This process enables us to obtain optimized channel configurations under different resource constraints. We conducted experiments using MobileNet v1, MobileNet v2, ResNet-50, and RL-searched MNasNet for ImageNet classification. Our approach demonstrated significant improvements over default channel configurations, as well as other channel pruning and neural architecture search methods. Notably, our AutoSlim-MobileNet-v2 achieved 74.2% top-1 accuracy at 305M FLOPs, which was 2.4% better than default MobileNet-v2 (301M FLOPs) and even 0.2% better than RL-searched MNasNet (317M FLOPs). Our AutoSlim-ResNet-50 achieved 1.3% better accuracy than MobileNet-v1 (569M FLOPs) at 570M FLOPs, without depthwise convolutions. The code and models for our approach will be available at https://github.com/JiahuiYu/slimmable_networks.",1
"Standard reinforcement learning methods aim to master one way of solving a task whereas there may exist multiple near-optimal policies. Being able to identify this collection of near-optimal policies can allow a domain expert to efficiently explore the space of reasonable solutions. Unfortunately, existing approaches that quantify uncertainty over policies are not ultimately relevant to finding policies with qualitatively distinct behaviors. In this work, we formalize the difference between policies as a difference between the distribution of trajectories induced by each policy, which encourages diversity with respect to both state visitation and action choices. We derive a gradient-based optimization technique that can be combined with existing policy gradient methods to now identify diverse collections of well-performing policies. We demonstrate our approach on benchmarks and a healthcare task.",0
"The conventional reinforcement learning techniques focus on mastering a single way of solving a task, but there may be multiple policies that are almost perfect. The identification of these near-optimal policies can aid a domain expert in efficiently exploring practical solutions. However, current methods that measure uncertainty over policies are not suitable for identifying policies with distinct behaviors. This study presents a formalized approach that distinguishes policies based on the distribution of trajectories induced by each policy, promoting diversity in both state visitation and action choices. The researchers developed a gradient-based optimization method that can be integrated with existing policy gradient methods to identify varied sets of well-performing policies. The efficacy of this approach was demonstrated on benchmarks and a healthcare task.",1
"Experience reuse is key to sample-efficient reinforcement learning. One of the critical issues is how the experience is represented and stored. Previously, the experience can be stored in the forms of features, individual models, and the average model, each lying at a different granularity. However, new tasks may require experience across multiple granularities. In this paper, we propose the policy residual representation (PRR) network, which can extract and store multiple levels of experience. PRR network is trained on a set of tasks with a multi-level architecture, where a module in each level corresponds to a subset of the tasks. Therefore, the PRR network represents the experience in a spectrum-like way. When training on a new task, PRR can provide different levels of experience for accelerating the learning. We experiment with the PRR network on a set of grid world navigation tasks, locomotion tasks, and fighting tasks in a video game. The results show that the PRR network leads to better reuse of experience and thus outperforms some state-of-the-art approaches.",0
"Efficient reinforcement learning relies on experience reuse, and the way experience is represented and stored is crucial. Traditionally, experience has been stored as features, individual models, or average models, each at different levels of detail. However, some tasks require experience across multiple levels. In this study, we introduce the policy residual representation (PRR) network, which can extract and store multiple levels of experience. The PRR network is trained on a multi-level architecture, where each module corresponds to a subset of tasks, allowing for a spectrum-like representation of experience. When training on a new task, PRR can provide different levels of experience to speed up learning. We tested the PRR network on grid world navigation, locomotion, and fighting tasks in a video game, and found that it outperforms some state-of-the-art approaches due to its ability to reuse experience efficiently.",1
"Effective medical test suggestions benefit both patients and physicians to conserve time and improve diagnosis accuracy. In this work, we show that an agent can learn to suggest effective medical tests. We formulate the problem as a stage-wise Markov decision process and propose a reinforcement learning method to train the agent. We introduce a new representation of multiple action policy along with the training method of the proposed representation. Furthermore, a new exploration scheme is proposed to accelerate the learning of disease distributions. Our experimental results demonstrate that the accuracy of disease diagnosis can be significantly improved with good medical test suggestions.",0
"The provision of useful medical test recommendations is advantageous for patients and doctors as it saves time and enhances the accuracy of diagnoses. Our research demonstrates that it is possible for an agent to learn to suggest effective medical tests. To achieve this, we use a stage-wise Markov decision process and a reinforcement learning approach to train the agent. We develop a fresh representation of multiple action policies, along with a new training method for our proposed representation. Additionally, we introduce an exploration scheme to expedite the learning of disease distributions. Our experiments reveal that good medical test suggestions can considerably enhance the precision of disease diagnoses.",1
"Most practical recommender systems focus on estimating immediate user engagement without considering the long-term effects of recommendations on user behavior. Reinforcement learning (RL) methods offer the potential to optimize recommendations for long-term user engagement. However, since users are often presented with slates of multiple items - which may have interacting effects on user choice - methods are required to deal with the combinatorics of the RL action space. In this work, we address the challenge of making slate-based recommendations to optimize long-term value using RL. Our contributions are three-fold. (i) We develop SLATEQ, a decomposition of value-based temporal-difference and Q-learning that renders RL tractable with slates. Under mild assumptions on user choice behavior, we show that the long-term value (LTV) of a slate can be decomposed into a tractable function of its component item-wise LTVs. (ii) We outline a methodology that leverages existing myopic learning-based recommenders to quickly develop a recommender that handles LTV. (iii) We demonstrate our methods in simulation, and validate the scalability of decomposed TD-learning using SLATEQ in live experiments on YouTube.",0
"Practical recommender systems typically prioritize estimating immediate user engagement, disregarding the potential long-term effects of recommendations on user behavior. Reinforcement learning (RL) methods present an opportunity to optimize recommendations for long-term user engagement, but the complexity of presenting users with multiple items requires techniques to handle the combinatorial nature of the RL action space. In this study, we address the challenge of using RL to make slate-based recommendations that optimize long-term value. Our contributions include the development of SLATEQ, a decomposition of value-based temporal-difference and Q-learning that makes RL tractable with slates, and a methodology for leveraging existing myopic learning-based recommenders to quickly develop a recommender that handles long-term value. We demonstrate the effectiveness of our methods in simulation and validate their scalability using live experiments on YouTube.",1
"Many potential applications of reinforcement learning in the real world involve interacting with other agents whose numbers vary over time. We propose new neural policy architectures for these multi-agent problems. In contrast to other methods of training an individual, discrete policy for each agent and then enforcing cooperation through some additional inter-policy mechanism, we follow the spirit of recent work on the power of relational inductive biases in deep networks by learning multi-agent relationships at the policy level via an attentional architecture. In our method, all agents share the same policy, but independently apply it in their own context to aggregate the other agents' state information when selecting their next action. The structure of our architectures allow them to be applied on environments with varying numbers of agents. We demonstrate our architecture on a benchmark multi-agent autonomous vehicle coordination problem, obtaining superior results to a full-knowledge, fully-centralized reference solution, and significantly outperforming it when scaling to large numbers of agents.",0
"The utilization of reinforcement learning in real-world situations often involves interacting with varying numbers of other agents. Our proposed solution involves the development of new neural policy architectures that cater to these multi-agent problems. Instead of training individual, separate policies for each agent and then incorporating cooperation through additional inter-policy mechanisms, we adopt the approach of recent research on the effectiveness of relational inductive biases in deep networks. Using an attentional architecture, we learn multi-agent relationships at the policy level, allowing all agents to share the same policy. However, each agent applies the policy independently in their own context to aggregate other agents' state information when selecting their next action. This architecture can be applied to environments with varying numbers of agents. We demonstrate the efficiency of our architecture by solving a multi-agent autonomous vehicle coordination problem and obtaining superior results compared to a full-knowledge, fully-centralized reference solution. Furthermore, our architecture outperforms the reference solution when scaling to larger numbers of agents.",1
"Recent advances in deep reinforcement learning algorithms have shown great potential and success for solving many challenging real-world problems, including Go game and robotic applications. Usually, these algorithms need a carefully designed reward function to guide training in each time step. However, in real world, it is non-trivial to design such a reward function, and the only signal available is usually obtained at the end of a trajectory, also known as the episodic reward or return. In this work, we introduce a new algorithm for temporal credit assignment, which learns to decompose the episodic return back to each time-step in the trajectory using deep neural networks. With this learned reward signal, the learning efficiency can be substantially improved for episodic reinforcement learning. In particular, we find that expressive language models such as the Transformer can be adopted for learning the importance and the dependency of states in the trajectory, therefore providing high-quality and interpretable learned reward signals. We have performed extensive experiments on a set of MuJoCo continuous locomotive control tasks with only episodic returns and demonstrated the effectiveness of our algorithm.",0
"Advanced deep reinforcement learning algorithms have demonstrated success in solving difficult real-world problems, such as Go games and robotics. However, these algorithms typically require a carefully designed reward function to guide training at each time step. In reality, it is challenging to create such a function, and the only available signal is often obtained at the end of a trajectory. This is known as the episodic reward or return. In this study, a new algorithm for temporal credit assignment is presented, which uses deep neural networks to decompose the episodic return into each time step in the trajectory. This learned reward signal improves the efficiency of episodic reinforcement learning. The Transformer language model is used to learn the importance and dependency of states in the trajectory, resulting in high-quality and interpretable learned reward signals. Experiments on a set of MuJoCo continuous locomotive control tasks with only episodic returns demonstrate the effectiveness of the algorithm.",1
"Recent reinforcement learning (RL) approaches have shown strong performance in complex domains such as Atari games, but are often highly sample inefficient. A common approach to reduce interaction time with the environment is to use reward shaping, which involves carefully designing reward functions that provide the agent intermediate rewards for progress towards the goal. However, designing appropriate shaping rewards is known to be difficult as well as time-consuming. In this work, we address this problem by using natural language instructions to perform reward shaping. We propose the LanguagE-Action Reward Network (LEARN), a framework that maps free-form natural language instructions to intermediate rewards based on actions taken by the agent. These intermediate language-based rewards can seamlessly be integrated into any standard reinforcement learning algorithm. We experiment with Montezuma's Revenge from the Atari Learning Environment, a popular benchmark in RL. Our experiments on a diverse set of 15 tasks demonstrate that, for the same number of interactions with the environment, language-based rewards lead to successful completion of the task 60% more often on average, compared to learning without language.",0
"Although recent reinforcement learning (RL) methods have demonstrated impressive results in complex domains like Atari games, they frequently suffer from low sample efficiency. To minimize the time required to interact with the environment, reward shaping is commonly used, which involves designing reward functions that provide the agent with intermediate rewards for making progress towards the objective. However, creating suitable shaping rewards is challenging and time-consuming. Our research addresses this issue by utilizing natural language instructions to conduct reward shaping. We propose the LanguagE-Action Reward Network (LEARN), a framework that translates free-form natural language instructions to intermediate rewards based on the agent's actions. These language-based rewards can be effortlessly integrated into any standard RL algorithm. We tested our approach on Montezuma's Revenge from the Atari Learning Environment, a well-known benchmark in RL. Our experiments on 15 distinct tasks demonstrate that, with the same number of interactions with the environment, language-based rewards resulted in successful task completion 60% more frequently on average than learning without language.",1
"The goal of computational color constancy is to preserve the perceptive colors of objects under different lighting conditions by removing the effect of color casts caused by the scene's illumination. With the rapid development of deep learning based techniques, significant progress has been made in image semantic segmentation. In this work, we exploit the semantic information together with the color and spatial information of the input image in order to remove color casts. We train a convolutional neural network (CNN) model that learns to estimate the illuminant color and gamma correction parameters based on the semantic information of the given image. Experimental results show that feeding the CNN with the semantic information leads to a significant improvement in the results by reducing the error by more than 40%.",0
"The aim of computational color constancy is to eliminate color casts caused by the lighting of a scene, in order to maintain the perceived colors of objects in different conditions. Image semantic segmentation has advanced considerably due to the rapid development of deep learning techniques. This study employs semantic information, as well as color and spatial information, from input images to eliminate color casts. A CNN model is trained to estimate illuminant color and gamma correction parameters based on the semantic information of the image. The experimental findings indicate that supplying the CNN with semantic information results in a significant reduction in error of over 40%.",1
"Designing new molecules with a set of predefined properties is a core problem in modern drug discovery and development. There is a growing need for de-novo design methods that would address this problem. We present MolecularRNN, the graph recurrent generative model for molecular structures. Our model generates diverse realistic molecular graphs after likelihood pretraining on a big database of molecules. We perform an analysis of our pretrained models on large-scale generated datasets of 1 million samples. Further, the model is tuned with policy gradient algorithm, provided a critic that estimates the reward for the property of interest. We show a significant distribution shift to the desired range for lipophilicity, drug-likeness, and melting point outperforming state-of-the-art works. With the use of rejection sampling based on valency constraints, our model yields 100% validity. Moreover, we show that invalid molecules provide a rich signal to the model through the use of structure penalty in our reinforcement learning pipeline.",0
"A fundamental issue in modern drug discovery and development is devising new molecules that have specific properties. To tackle this issue, there is a growing demand for de-novo design methods. Our solution to this problem is MolecularRNN, which is a graph recurrent generative model for molecular structures. By pretraining our model on a vast database of molecules, we are able to produce a diverse range of realistic molecular graphs. We conducted an analysis of our pretrained models on large-scale generated datasets of 1 million samples. Furthermore, we fine-tuned our model using a policy gradient algorithm and a critic to estimate the reward for the targeted property. Our results show that our model outperforms the current state-of-the-art works in achieving a significant distribution shift to the desired range for lipophilicity, drug-likeness, and melting point. We achieved 100% validity by using rejection sampling based on valency constraints. Additionally, we discovered that invalid molecules offer valuable insights to the model through the use of a structure penalty in our reinforcement learning pipeline.",1
"We study data poisoning attacks in the online setting where training items arrive sequentially, and the attacker may perturb the current item to manipulate online learning. Importantly, the attacker has no knowledge of future training items nor the data generating distribution. We formulate online data poisoning attack as a stochastic optimal control problem, and solve it with model predictive control and deep reinforcement learning. We also upper bound the suboptimality suffered by the attacker for not knowing the data generating distribution. Experiments validate our control approach in generating near-optimal attacks on both supervised and unsupervised learning tasks.",0
"Our focus is on analyzing data poisoning attacks in the online environment, where training items are received in a sequential manner. The attacker is able to manipulate the current item to influence online learning and has no insight into future training items or the data generation distribution. We present an approach that frames the online data poisoning attack as a stochastic optimal control problem, utilizing model predictive control and deep reinforcement learning for resolution. We also provide an upper bound for the suboptimality that the attacker would experience due to a lack of understanding of the data generating distribution. Our experiments demonstrate the effectiveness of our control method in generating nearly optimal attacks for both supervised and unsupervised learning tasks.",1
"Model-based reinforcement learning is an appealing framework for creating agents that learn, plan, and act in sequential environments. Model-based algorithms typically involve learning a transition model that takes a state and an action and outputs the next state---a one-step model. This model can be composed with itself to enable predicting multiple steps into the future, but one-step prediction errors can get magnified, leading to unacceptable inaccuracy. This compounding-error problem plagues planning and undermines model-based reinforcement learning. In this paper, we address the compounding-error problem by introducing a multi-step model that directly outputs the outcome of executing a sequence of actions. Novel theoretical and empirical results indicate that the multi-step model is more conducive to efficient value-function estimation, and it yields better action selection compared to the one-step model. These results make a strong case for using multi-step models in the context of model-based reinforcement learning.",0
"The framework of model-based reinforcement learning is attractive for developing agents that can learn, plan, and act in sequential environments. Generally, model-based algorithms involve learning a one-step transition model that takes a state and action as input and produces the next state. However, using this model for predicting several steps into the future can lead to compounding prediction errors, resulting in unacceptable inaccuracies that undermine the model-based reinforcement learning process. To address this issue, this paper proposes a multi-step model that directly predicts the outcome of executing a sequence of actions. Theoretical and empirical evidence suggests that this multi-step model is more effective for efficient value-function estimation and action selection than the one-step model. These findings support the use of multi-step models in the context of model-based reinforcement learning.",1
"AI agents are being developed to support high stakes decision-making processes from driving cars to prescribing drugs, making it increasingly important for human users to understand their behavior. Policy summarization methods aim to convey strengths and weaknesses of such agents by demonstrating their behavior in a subset of informative states. Some policy summarization methods extract a summary that optimizes the ability to reconstruct the agent's policy under the assumption that users will deploy inverse reinforcement learning. In this paper, we explore the use of different models for extracting summaries. We introduce an imitation learning-based approach to policy summarization; we demonstrate through computational simulations that a mismatch between the model used to extract a summary and the model used to reconstruct the policy results in worse reconstruction quality; and we demonstrate through a human-subject study that people use different models to reconstruct policies in different contexts, and that matching the summary extraction model to these can improve performance. Together, our results suggest that it is important to carefully consider user models in policy summarization.",0
"The development of AI agents is aimed at supporting critical decision-making processes such as driving cars and prescribing drugs. Therefore, it is essential for human users to comprehend their behavior. Policy summarization methods have been introduced to exhibit the strengths and weaknesses of such agents by showcasing their behavior in a specific set of informative states. Some methods extract a summary that enhances the ability to reconstruct the agent's policy under the assumption that users will deploy inverse reinforcement learning. In this study, we explore a variety of models for extracting summaries and present an imitation learning-based approach to policy summarization. Our computational simulations demonstrate that a mismatch between the model used to extract a summary and the model used to reconstruct the policy results in poor reconstruction quality. Additionally, our human-subject study shows that people use different models to reconstruct policies in different contexts, and matching the summary extraction model to these can improve performance. Our findings suggest that user models must be carefully considered in policy summarization.",1
"A key impediment to reinforcement learning (RL) in real applications with limited, batch data is defining a reward function that reflects what we implicitly know about reasonable behaviour for a task and allows for robust off-policy evaluation. In this work, we develop a method to identify an admissible set of reward functions for policies that (a) do not diverge too far from past behaviour, and (b) can be evaluated with high confidence, given only a collection of past trajectories. Together, these ensure that we propose policies that we trust to be implemented in high-risk settings. We demonstrate our approach to reward design on synthetic domains as well as in a critical care context, for a reward that consolidates clinical objectives to learn a policy for weaning patients from mechanical ventilation.",0
"One of the main challenges of applying reinforcement learning (RL) to real-world scenarios with limited, batch data is creating a reward function that accurately represents our implicit understanding of reasonable behavior for a given task while also enabling reliable off-policy evaluation. In this study, we introduce a technique for identifying a set of acceptable reward functions for policies that (a) stay consistent with past behavior and (b) can be confidently evaluated using only previous trajectories. This ensures that the policies we propose can be trusted in high-risk situations. We showcase our approach to reward design on both artificial domains and in a critical care context, where we develop a reward that combines clinical objectives to teach a policy for weaning patients off mechanical ventilation.",1
"In this paper we propose a hybrid architecture of actor-critic algorithms for reinforcement learning in parameterized action space, which consists of multiple parallel sub-actor networks to decompose the structured action space into simpler action spaces along with a critic network to guide the training of all sub-actor networks. While this paper is mainly focused on parameterized action space, the proposed architecture, which we call hybrid actor-critic, can be extended for more general action spaces which has a hierarchical structure. We present an instance of the hybrid actor-critic architecture based on proximal policy optimization (PPO), which we refer to as hybrid proximal policy optimization (H-PPO). Our experiments test H-PPO on a collection of tasks with parameterized action space, where H-PPO demonstrates superior performance over previous methods of parameterized action reinforcement learning.",0
"The aim of this paper is to introduce a hybrid actor-critic architecture for reinforcement learning in parameterized action space. The architecture includes multiple parallel sub-actor networks that break down the structured action space into simpler action spaces. Additionally, a critic network is used to guide the training of all sub-actor networks. Though the focus is on parameterized action space, the proposed architecture, known as hybrid actor-critic, can be adapted for more general action spaces that have a hierarchical structure. We present an example of the hybrid actor-critic architecture, based on proximal policy optimization (PPO), which we call hybrid proximal policy optimization (H-PPO). Our experiments involve testing H-PPO on a variety of tasks with parameterized action space, and it outperforms previous methods of parameterized action reinforcement learning.",1
"Deep learning has enabled traditional reinforcement learning methods to deal with high-dimensional problems. However, one of the disadvantages of deep reinforcement learning methods is the limited exploration capacity of learning agents. In this paper, we introduce an approach that integrates human strategies to increase the exploration capacity of multiple deep reinforcement learning agents. We also report the development of our own multi-agent environment called Multiple Tank Defence to simulate the proposed approach. The results show the significant performance improvement of multiple agents that have learned cooperatively with human strategies. This implies that there is a critical need for human intellect teamed with machines to solve complex problems. In addition, the success of this simulation indicates that our multi-agent environment can be used as a testbed platform to develop and validate other multi-agent control algorithms.",0
"The utilization of deep learning has allowed conventional reinforcement learning techniques to handle problems with high dimensions. Nevertheless, a drawback of deep reinforcement learning methods is that the learning agents have limited exploration capacity. In this study, we present an approach that incorporates human strategies to enhance the exploration capacity of multiple deep reinforcement learning agents. Furthermore, we created our own multi-agent environment, called Multiple Tank Defence, to simulate the proposed approach. Our findings reveal that the performance of multiple agents that have learned cooperatively with human strategies has significantly improved. This suggests that the collaboration of human intelligence and machines is crucial in solving intricate problems. Additionally, the triumph of this simulation indicates that our multi-agent environment can serve as a platform for testing and validating other multi-agent control algorithms.",1
"We study the sample complexity of model-based reinforcement learning (henceforth RL) in general contextual decision processes that require strategic exploration to find a near-optimal policy. We design new algorithms for RL with a generic model class and analyze their statistical properties. Our algorithms have sample complexity governed by a new structural parameter called the witness rank, which we show to be small in several settings of interest, including factored MDPs. We also show that the witness rank is never larger than the recently proposed Bellman rank parameter governing the sample complexity of the model-free algorithm OLIVE (Jiang et al., 2017), the only other provably sample-efficient algorithm for global exploration at this level of generality. Focusing on the special case of factored MDPs, we prove an exponential lower bound for a general class of model-free approaches, including OLIVE, which, when combined with our algorithmic results, demonstrates exponential separation between model-based and model-free RL in some rich-observation settings.",0
"Our research delves into the sample complexity of model-based reinforcement learning (abbreviated as RL) in contextual decision processes that require strategic exploration to achieve a policy that is nearly optimal. We have created fresh algorithms for RL that work with a general model class and have assessed their statistical properties. Our algorithms' sample complexity is regulated by a novel structural parameter called the witness rank, which we have found to be small in various settings of interest, such as factored MDPs. We have also proved that the witness rank is never greater than the recently proposed Bellman rank parameter that governs the sample complexity of the model-free OLIVE algorithm (Jiang et al., 2017), the only other algorithm that has been verified to be sample-efficient for global exploration at this level of generality. By concentrating on factored MDPs' particular case, we have shown an exponential lower bound for a general group of model-free approaches, including OLIVE. This, coupled with our algorithmic findings, demonstrates that there is an exponential gap between model-based and model-free RL in certain rich-observation settings.",1
"We study the sample complexity of approximate policy iteration (PI) for the Linear Quadratic Regulator (LQR), building on a recent line of work using LQR as a testbed to understand the limits of reinforcement learning (RL) algorithms on continuous control tasks. Our analysis quantifies the tension between policy improvement and policy evaluation, and suggests that policy evaluation is the dominant factor in terms of sample complexity. Specifically, we show that to obtain a controller that is within $\varepsilon$ of the optimal LQR controller, each step of policy evaluation requires at most $(n+d)^3/\varepsilon^2$ samples, where $n$ is the dimension of the state vector and $d$ is the dimension of the input vector. On the other hand, only $\log(1/\varepsilon)$ policy improvement steps suffice, resulting in an overall sample complexity of $(n+d)^3 \varepsilon^{-2} \log(1/\varepsilon)$. We furthermore build on our analysis and construct a simple adaptive procedure based on $\varepsilon$-greedy exploration which relies on approximate PI as a sub-routine and obtains $T^{2/3}$ regret, improving upon a recent result of Abbasi-Yadkori et al.",0
"In this study, we focus on the sample complexity of approximating the policy iteration (PI) for the Linear Quadratic Regulator (LQR). Our research builds on recent work that uses LQR as a testbed to understand the limitations of reinforcement learning (RL) algorithms on continuous control tasks. Our analysis identifies the conflict between policy improvement and policy evaluation, indicating that policy evaluation has a greater impact on sample complexity. Specifically, we demonstrate that to achieve a controller within $\varepsilon$ of the optimal LQR controller, each policy evaluation step requires no more than $(n+d)^3/\varepsilon^2$ samples, where $n$ represents the state vector's dimension and $d$ the input vector's dimension. On the other hand, only $\log(1/\varepsilon)$ policy improvement steps are necessary, resulting in a total sample complexity of $(n+d)^3 \varepsilon^{-2} \log(1/\varepsilon)$. We also develop a straightforward adaptive procedure based on $\varepsilon$-greedy exploration, which uses approximated PI as a subroutine, leading to $T^{2/3}$ regret. This procedure improves on a recent result of Abbasi-Yadkori et al.",1
"Although reinforcement learning (RL) can provide reliable solutions in many settings, practitioners are often wary of the discrepancies between the RL solution and their status quo procedures. Therefore, they may be reluctant to adapt to the novel way of executing tasks proposed by RL. On the other hand, many real-world problems require relatively small adjustments from the status quo policies to achieve improved performance. Therefore, we propose a student-teacher RL mechanism in which the RL (the ""student"") learns to maximize its reward, subject to a constraint that bounds the difference between the RL policy and the ""teacher"" policy. The teacher can be another RL policy (e.g., trained under a slightly different setting), the status quo policy, or any other exogenous policy. We formulate this problem using a stochastic optimization model and solve it using a primal-dual policy gradient algorithm. We prove that the policy is asymptotically optimal. However, a naive implementation suffers from high variance and convergence to a stochastic optimal policy. With a few practical adjustments to address these issues, our numerical experiments confirm the effectiveness of our proposed method in multiple GridWorld scenarios.",0
"Although reinforcement learning (RL) is a dependable solution in many instances, practitioners may hesitate to implement it due to discrepancies with their existing procedures. However, some real-world problems only require minor modifications to existing policies for improved performance. To address this, we suggest a student-teacher RL mechanism, where the RL (the ""student"") maximizes its reward while adhering to a constraint that limits differences from the ""teacher"" policy. The teacher can be another RL policy, the current policy, or an external policy. We use a stochastic optimization model and a primal-dual policy gradient algorithm to solve this problem, which we show to be asymptotically optimal. However, a basic implementation may suffer from high variance and stochastic optimal policy convergence. We address these issues with practical adjustments and demonstrate the efficacy of our approach in multiple GridWorld scenarios through numerical experiments.",1
"Achieving faster execution with shorter compilation time can enable further diversity and innovation in neural networks. However, the current paradigm of executing neural networks either relies on hand-optimized libraries, traditional compilation heuristics, or very recently, simulated annealing and genetic algorithms. Our work takes a unique approach by formulating compiler optimizations for neural networks as a reinforcement learning problem, whose solution takes fewer steps to converge. This solution, dubbed ReLeASE, comes with a sampling algorithm that leverages clustering to focus the costly samples (real hardware measurements) on representative points, subsuming an entire subspace. Our adaptive sampling not only reduces the number of samples, but also improves the quality of samples for better exploration in shorter time. As such, experimentation with real hardware shows that reinforcement learning with adaptive sampling provides 4.45x speed up in optimization time over AutoTVM, while also improving inference time of the modern deep networks by 5.6%. Further experiments also confirm that our adaptive sampling can even improve AutoTVM's simulated annealing by 4.00x.",0
"Enabling greater diversity and innovation in neural networks can be achieved by achieving faster execution and shorter compilation times. However, current methods of executing neural networks rely on hand-optimized libraries, traditional compilation heuristics, or more recently, simulated annealing and genetic algorithms. Our approach is unique in that it formulates compiler optimizations for neural networks as a reinforcement learning problem, which converges in fewer steps. Our solution, called ReLeASE, includes a sampling algorithm that leverages clustering to focus expensive samples on representative points, covering an entire subspace. Our adaptive sampling reduces the number of samples and improves their quality for better exploration in less time. Experiments with real hardware show that reinforcement learning with adaptive sampling provides a 4.45x speed up in optimization time over AutoTVM, while improving inference time of modern deep networks by 5.6%. Further experiments confirm that adaptive sampling can even improve AutoTVM's simulated annealing by 4.00x.",1
"Latent-state environments with long horizons, such as those faced by recommender systems, pose significant challenges for reinforcement learning (RL). In this work, we identify and analyze several key hurdles for RL in such environments, including belief state error and small action advantage. We develop a general principle of advantage amplification that can overcome these hurdles through the use of temporal abstraction. We propose several aggregation methods and prove they induce amplification in certain settings. We also bound the loss in optimality incurred by our methods in environments where latent state evolves slowly and demonstrate their performance empirically in a stylized user-modeling task.",0
"Reinforcement learning (RL) faces significant challenges in latent-state environments with long horizons, like those encountered by recommender systems. This study examines several obstacles, including belief state error and small action advantage, that hinder RL in such environments. A general principle of advantage amplification is introduced, which can leverage temporal abstraction to overcome these hurdles. We suggest various aggregation methods that induce amplification in specific settings, and we limit the loss in optimality incurred by our strategies in environments where latent state changes slowly. Finally, we demonstrate the effectiveness of our techniques empirically in a user-modeling task.",1
"Understanding generalization in reinforcement learning (RL) is a significant challenge, as many common assumptions of traditional supervised learning theory do not apply. We focus on the special class of reparameterizable RL problems, where the trajectory distribution can be decomposed using the reparametrization trick. For this problem class, estimating the expected return is efficient and the trajectory can be computed deterministically given peripheral random variables, which enables us to study reparametrizable RL using supervised learning and transfer learning theory. Through these relationships, we derive guarantees on the gap between the expected and empirical return for both intrinsic and external errors, based on Rademacher complexity as well as the PAC-Bayes bound. Our bound suggests the generalization capability of reparameterizable RL is related to multiple factors including ""smoothness"" of the environment transition, reward and agent policy function class. We also empirically verify the relationship between the generalization gap and these factors through simulations.",0
"The challenge of comprehending generalization in reinforcement learning (RL) is significant because traditional supervised learning theory assumptions do not apply. Our focus is on reparameterizable RL problems, a unique class where the trajectory distribution can be decomposed using the reparametrization trick. For this class, estimating the expected return is efficient, and the trajectory is deterministically computed given peripheral random variables. This enables us to study reparametrizable RL using supervised learning and transfer learning theory. Using these relationships, we derive guarantees on the gap between the expected and empirical return for intrinsic and external errors. Rademacher complexity and the PAC-Bayes bound are used to create the bound. Our bound suggests that the generalization capability of reparameterizable RL is related to factors such as the ""smoothness"" of the environment transition, reward, and agent policy function class. We also verify this relationship through simulations.",1
"We revisit the stochastic variance-reduced policy gradient (SVRPG) method proposed by Papini et al. (2018) for reinforcement learning. We provide an improved convergence analysis of SVRPG and show that it can find an $\epsilon$-approximate stationary point of the performance function within $O(1/\epsilon^{5/3})$ trajectories. This sample complexity improves upon the best known result $O(1/\epsilon^2)$ by a factor of $O(1/\epsilon^{1/3})$. At the core of our analysis is (i) a tighter upper bound for the variance of importance sampling weights, where we prove that the variance can be controlled by the parameter distance between different policies; and (ii) a fine-grained analysis of the epoch length and batch size parameters such that we can significantly reduce the number of trajectories required in each iteration of SVRPG. We also empirically demonstrate the effectiveness of our theoretical claims of batch sizes on reinforcement learning benchmark tasks.",0
"The stochastic variance-reduced policy gradient (SVRPG) method proposed by Papini et al. (2018) is revisited in the context of reinforcement learning. An improved convergence analysis of SVRPG is provided, demonstrating its ability to find an $\epsilon$-approximate stationary point of the performance function within $O(1/\epsilon^{5/3})$ trajectories, which is a significant improvement over the previous best known result of $O(1/\epsilon^2)$ by a factor of $O(1/\epsilon^{1/3})$. Our analysis centers around (i) establishing a tighter upper bound for the variance of importance sampling weights, which can be controlled by the parameter distance between different policies; and (ii) conducting a fine-grained analysis of the epoch length and batch size parameters to reduce the number of trajectories required in each iteration of SVRPG. Additionally, we provide empirical evidence supporting the effectiveness of our theoretical claims on reinforcement learning benchmark tasks.",1
"A universal rule-based self-learning approach using deep reinforcement learning (DRL) is proposed for the first time to solve nonlinear ordinary differential equations and partial differential equations. The solver consists of a deep neural network-structured actor that outputs candidate solutions, and a critic derived only from physical rules (governing equations and boundary and initial conditions). Solutions in discretized time are treated as multiple tasks sharing the same governing equation, and the current step parameters provide an ideal initialization for the next owing to the temporal continuity of the solutions, which shows a transfer learning characteristic and indicates that the DRL solver has captured the intrinsic nature of the equation. The approach is verified through solving the Schr\""odinger, Navier-Stokes, Burgers', Van der Pol, and Lorenz equations and an equation of motion. The results indicate that the approach gives solutions with high accuracy, and the solution process promises to get faster.",0
"For the first time, a self-learning approach using deep reinforcement learning (DRL) based on universal rules has been proposed to solve nonlinear ordinary differential equations and partial differential equations. The solver consists of an actor with a deep neural network structure that generates candidate solutions, and a critic that is derived solely from physical rules, including governing equations and boundary and initial conditions. Multiple tasks with the same governing equation are treated as solutions in discretized time, and the current step parameters serve as an ideal initialization for the next step due to the temporal continuity of the solutions, indicating a transfer learning characteristic that demonstrates the DRL solver has captured the intrinsic nature of the equation. The approach has been validated by solving various equations, including the Schr\""odinger, Navier-Stokes, Burgers', Van der Pol, and Lorenz equations, as well as an equation of motion. The results demonstrate that the approach provides highly accurate solutions, and the solution process is expected to become faster.",1
"System identification of complex and nonlinear systems is a central problem for model predictive control and model-based reinforcement learning. Despite their complexity, such systems can often be approximated well by a set of linear dynamical systems if broken into appropriate subsequences. This mechanism not only helps us find good approximations of dynamics, but also gives us deeper insight into the underlying system. Leveraging Bayesian inference, Variational Autoencoders and Concrete relaxations, we show how to learn a richer and more meaningful state space, e.g. encoding joint constraints and collisions with walls in a maze, from partial and high-dimensional observations. This representation translates into a gain of accuracy of learned dynamics showcased on various simulated tasks.",0
"The identification of complex and nonlinear systems is a significant challenge for both model predictive control and model-based reinforcement learning. Despite their intricacy, these systems can be well approximated by linear dynamical systems if they are divided into appropriate subsequences. This approach not only enables us to find accurate approximations of the dynamics but also provides a deeper understanding of the underlying system. By utilizing Bayesian inference, Variational Autoencoders, and Concrete relaxations, we demonstrate how to acquire a more comprehensive and meaningful state space that includes joint constraints and wall collisions in a maze from partial and high-dimensional observations. This representation results in improved accuracy of the learned dynamics, as evidenced by various simulated tasks.",1
"In importance sampling (IS)-based reinforcement learning algorithms such as Proximal Policy Optimization (PPO), IS weights are typically clipped to avoid large variance in learning. However, policy update from clipped statistics induces large bias in tasks with high action dimensions, and bias from clipping makes it difficult to reuse old samples with large IS weights. In this paper, we consider PPO, a representative on-policy algorithm, and propose its improvement by dimension-wise IS weight clipping which separately clips the IS weight of each action dimension to avoid large bias and adaptively controls the IS weight to bound policy update from the current policy. This new technique enables efficient learning for high action-dimensional tasks and reusing of old samples like in off-policy learning to increase the sample efficiency. Numerical results show that the proposed new algorithm outperforms PPO and other RL algorithms in various Open AI Gym tasks.",0
"The reinforcement learning algorithms that rely on importance sampling (IS), such as Proximal Policy Optimization (PPO), usually clip IS weights to prevent excessive variance during learning. However, using clipped statistics for policy updates can generate significant bias in tasks with high action dimensions, rendering it difficult to reuse older samples with large IS weights. This study examines PPO, a typical on-policy algorithm, and proposes an enhancement that involves dimension-wise clipping of IS weights. This method clips each action dimension's IS weight separately to minimize bias and adaptively controls the IS weight to restrict policy updates from the existing policy. This approach enables efficient learning in high action-dimensional tasks and facilitates the reuse of old samples, similar to that in off-policy learning, to improve sample efficiency. The experimental outcomes indicate that the proposed algorithm surpasses other RL algorithms and PPO in various Open AI Gym tasks.",1
"Though reinforcement learning has greatly benefited from the incorporation of neural networks, the inability to verify the correctness of such systems limits their use. Current work in explainable deep learning focuses on explaining only a single decision in terms of input features, making it unsuitable for explaining a sequence of decisions. To address this need, we introduce Abstracted Policy Graphs, which are Markov chains of abstract states. This representation concisely summarizes a policy so that individual decisions can be explained in the context of expected future transitions. Additionally, we propose a method to generate these Abstracted Policy Graphs for deterministic policies given a learned value function and a set of observed transitions, potentially off-policy transitions used during training. Since no restrictions are placed on how the value function is generated, our method is compatible with many existing reinforcement learning methods. We prove that the worst-case time complexity of our method is quadratic in the number of features and linear in the number of provided transitions, $O(|F|^2 |tr\_samples|)$. By applying our method to a family of domains, we show that our method scales well in practice and produces Abstracted Policy Graphs which reliably capture relationships within these domains.",0
"The use of neural networks in reinforcement learning has been beneficial, but the inability to verify their accuracy limits their effectiveness. Current work in explainable deep learning only explains individual decisions, which is not sufficient for explaining a sequence of decisions. To address this issue, we introduce Abstracted Policy Graphs, which are Markov chains of abstract states that provide a concise overview of a policy. Our method generates these graphs for deterministic policies using a learned value function and observed transitions. This approach is compatible with various reinforcement learning methods and has a worst-case time complexity of $O(|F|^2 |tr\_samples|)$. Our method has been tested in different domains and produces reliable Abstracted Policy Graphs that capture the relationships within these domains.",1
"It would be desirable for a reinforcement learning (RL) based agent to learn behaviour by merely watching a demonstration. However, defining rewards that facilitate this goal within the RL paradigm remains a challenge. Here we address this problem with Siamese networks, trained to compute distances between observed behaviours and the agent's behaviours. Given a desired motion such Siamese networks can be used to provide a reward signal to an RL agent via the distance between the desired motion and the agent's motion. We experiment with an RNN-based comparator model that can compute distances in space and time between motion clips while training an RL policy to minimize this distance. Through experimentation, we have had also found that the inclusion of multi-task data and an additional image encoding loss helps enforce the temporal consistency. These two components appear to balance reward for matching a specific instance of behaviour versus that behaviour in general. Furthermore, we focus here on a particularly challenging form of this problem where only a single demonstration is provided for a given task -- the one-shot learning setting. We demonstrate our approach on humanoid agents in both 2D with $10$ degrees of freedom (DoF) and 3D with $38$ DoF.",0
"Achieving the goal of having a reinforcement learning (RL) based agent learn behavior through observation alone is desirable, but it poses a challenge within the RL framework to establish rewards that support this objective. To tackle this issue, we propose the use of Siamese networks that can gauge the distance between the agent's and observed behaviors. This approach enables the provision of a reward signal to an RL agent based on the distance between the desired motion and its actual motion. To minimize this distance, we employ an RNN-based comparator model, which computes distances in space and time between motion clips. We discovered that incorporating multi-task data and an extra image encoding loss can help ensure temporal consistency and balance rewards for matching specific instances of behavior versus general behavior. Our focus is on the one-shot learning setting, where only one demonstration is available for a given task, making this problem particularly challenging. We demonstrate our method on humanoid agents in both 2D and 3D with 10 and 38 degrees of freedom, respectively.",1
"We consider a new family of operators for reinforcement learning with the goal of alleviating the negative effects and becoming more robust to approximation or estimation errors. Various theoretical results are established, which include showing on a sample path basis that our family of operators preserve optimality and increase the action gap. Our empirical results illustrate the strong benefits of our family of operators, significantly outperforming the classical Bellman operator and recently proposed operators.",0
"Our objective is to develop a fresh set of operators for reinforcement learning that can mitigate negative consequences and enhance resistance to errors in approximation or estimation. Several theoretical outcomes are demonstrated, such as proving that our operators maintain optimality and widen the action gap on a sample path basis. Our practical findings demonstrate the substantial advantages of our operators, surpassing those of the traditional Bellman operator and recently introduced operators.",1
"The convergence of many reinforcement learning (RL) algorithms with linear function approximation has been investigated extensively but most proofs assume that these methods converge to a unique solution. In this paper, we provide a complete characterization of non-uniqueness issues for a large class of reinforcement learning algorithms, simultaneously unifying many counter-examples to convergence in a theoretical framework. We achieve this by proving a new condition on features that can determine whether the convergence assumptions are valid or non-uniqueness holds. We consider a general class of RL methods, which we call natural algorithms, whose solutions are characterized as the fixed point of a projected Bellman equation (when it exists); notably, bootstrapped temporal difference-based methods such as $TD(\lambda)$ and $GTD(\lambda)$ are natural algorithms. Our main result proves that natural algorithms converge to the correct solution if and only if all the value functions in the approximation space satisfy a certain shape. This implies that natural algorithms are, in general, inherently prone to converge to the wrong solution for most feature choices even if the value function can be represented exactly. Given our results, we show that state aggregation based features are a safe choice for natural algorithms and we also provide a condition for finding convergent algorithms under other feature constructions.",0
"Numerous studies have examined the convergence of several reinforcement learning (RL) algorithms with linear function approximation. However, these studies generally assume that the methods converge to a unique solution. This paper presents a comprehensive analysis of non-uniqueness issues for a broad range of reinforcement learning algorithms. By proving a new condition on features, we can determine whether convergence assumptions are valid or non-uniqueness holds. Our research focuses on a general class of RL methods called natural algorithms, whose solutions are characterized as the fixed point of a projected Bellman equation. Our main finding is that natural algorithms converge to the correct solution only if all the value functions in the approximation space conform to a specific shape. This means that natural algorithms are generally susceptible to converging to the wrong solution for most feature choices, even if the value function can be represented exactly. However, we demonstrate that state aggregation based features are a safe choice for natural algorithms, and we provide a condition for discovering convergent algorithms under other feature constructions.",1
"A fundamental issue in reinforcement learning algorithms is the balance between exploration of the environment and exploitation of information already obtained by the agent. Especially, exploration has played a critical role for both efficiency and efficacy of the learning process. However, Existing works for exploration involve task-agnostic design, that is performing well in one environment, but be ill-suited to another. To the purpose of learning an effective and efficient exploration policy in an automated manner. We formalized a feasible metric for measuring the utility of exploration based on counterfactual ideology. Based on that, We proposed an end-to-end algorithm to learn exploration policy by meta-learning. We demonstrate that our method achieves good results compared to previous works in the high-dimensional control tasks in MuJoCo simulator.",0
"The challenge in reinforcement learning algorithms lies in finding the right balance between exploration and exploitation. Exploration is crucial for an efficient and effective learning process, but current exploration methods are not adaptable to different environments. To address this issue, we developed a metric to measure the usefulness of exploration using counterfactual ideology. Our end-to-end algorithm for meta-learning exploration policy demonstrates good results in high-dimensional control tasks in the MuJoCo simulator, outperforming previous methods. Our aim is to automate the process of learning an effective and efficient exploration policy.",1
"Reinforcement learning in multi-agent scenarios is important for real-world applications but presents challenges beyond those seen in single-agent settings. We present an actor-critic algorithm that trains decentralized policies in multi-agent settings, using centrally computed critics that share an attention mechanism which selects relevant information for each agent at every timestep. This attention mechanism enables more effective and scalable learning in complex multi-agent environments, when compared to recent approaches. Our approach is applicable not only to cooperative settings with shared rewards, but also individualized reward settings, including adversarial settings, as well as settings that do not provide global states, and it makes no assumptions about the action spaces of the agents. As such, it is flexible enough to be applied to most multi-agent learning problems.",0
"Multi-agent scenarios pose unique challenges that go beyond those encountered in single-agent settings, making reinforcement learning crucial for real-world applications. In this study, we introduce an actor-critic algorithm that trains decentralized policies in multi-agent settings. This algorithm employs centrally computed critics that utilize an attention mechanism to select pertinent information for each agent during each timestep. Our approach is more effective and scalable in complex multi-agent environments than recent methods. It can be applied to various learning problems, including cooperative and adversarial settings with individualized rewards, and does not assume any specific action space for the agents. Thus, it is highly flexible and adaptable to most multi-agent learning challenges.",1
"A Budgeted Markov Decision Process (BMDP) is an extension of a Markov Decision Process to critical applications requiring safety constraints. It relies on a notion of risk implemented in the shape of a cost signal constrained to lie below an - adjustable - threshold. So far, BMDPs could only be solved in the case of finite state spaces with known dynamics. This work extends the state-of-the-art to continuous spaces environments and unknown dynamics. We show that the solution to a BMDP is a fixed point of a novel Budgeted Bellman Optimality operator. This observation allows us to introduce natural extensions of Deep Reinforcement Learning algorithms to address large-scale BMDPs. We validate our approach on two simulated applications: spoken dialogue and autonomous driving.",0
"The Budgeted Markov Decision Process (BMDP) is a modified version of the Markov Decision Process that is suitable for applications with safety constraints. It incorporates a risk concept in the form of a cost signal, which must remain below a customizable threshold. Previously, BMDPs could only be solved when the state spaces were finite and the dynamics were known. However, this study expands the current knowledge to encompass continuous space environments with unknown dynamics. The authors demonstrate that the resolution of a BMDP is a fixed point of a new Budgeted Bellman Optimality operator. This discovery enables the creation of expanded versions of Deep Reinforcement Learning algorithms, which are suitable for handling large-scale BMDPs. The authors verify the effectiveness of their approach using two simulated applications: spoken dialogue and autonomous driving.",1
"The performance of a reinforcement learning algorithm can vary drastically during learning because of exploration. Existing algorithms provide little information about the quality of their current policy before executing it, and thus have limited use in high-stakes applications like healthcare. We address this lack of accountability by proposing that algorithms output policy certificates. These certificates bound the sub-optimality and return of the policy in the next episode, allowing humans to intervene when the certified quality is not satisfactory. We further introduce two new algorithms with certificates and present a new framework for theoretical analysis that guarantees the quality of their policies and certificates. For tabular MDPs, we show that computing certificates can even improve the sample-efficiency of optimism-based exploration. As a result, one of our algorithms is the first to achieve minimax-optimal PAC bounds up to lower-order terms, and this algorithm also matches (and in some settings slightly improves upon) existing minimax regret bounds.",0
"Exploration during learning can cause significant variations in the performance of reinforcement learning algorithms. Unfortunately, current algorithms offer little insight into the quality of their policies, making them less useful in high-stakes situations such as healthcare. To address this accountability shortfall, we suggest that algorithms produce policy certificates. These certificates establish the policy's sub-optimality and return in the next episode, enabling individuals to intervene if the certified quality is inadequate. We introduce two new algorithms with certificates and a theoretical analysis framework that guarantees the quality of their policies and certificates. In tabular MDPs, we demonstrate that computing certificates can even increase the sample-efficiency of optimism-based exploration. As a result, our algorithm is the first to achieve minimax-optimal PAC bounds up to lower-order terms, and it matches (and in some situations slightly improves upon) current minimax regret bounds.",1
"Policy gradient methods ignore the potential value of adjusting environment variables: unobservable state features that are randomly determined by the environment in a physical setting, but are controllable in a simulator. This can lead to slow learning, or convergence to suboptimal policies, if the environment variable has a large impact on the transition dynamics. In this paper, we present fingerprint policy optimisation (FPO), which finds a policy that is optimal in expectation across the distribution of environment variables. The central idea is to use Bayesian optimisation (BO) to actively select the distribution of the environment variable that maximises the improvement generated by each iteration of the policy gradient method. To make this BO practical, we contribute two easy-to-compute low-dimensional fingerprints of the current policy. Our experiments show that FPO can efficiently learn policies that are robust to significant rare events, which are unlikely to be observable under random sampling, but are key to learning good policies.",0
"The potential benefits of adjusting environment variables are overlooked by policy gradient methods. These variables are unobservable state features that are randomly determined in a physical setting but can be controlled in a simulator. Neglecting these variables can result in slow learning or suboptimal policies, particularly when they heavily impact the transition dynamics. Our paper proposes a solution called fingerprint policy optimisation (FPO), which aims to find an optimal policy across the distribution of environment variables. FPO uses Bayesian optimisation to select the most effective distribution of environment variables that maximises the policy gradient method's improvement at each iteration. To enable practical implementation of BO, we introduce two low-dimensional fingerprints of the current policy that are easy to compute. Our experiments show that FPO can efficiently learn policies that can withstand significant rare events, which are crucial for developing effective policies but are unlikely to be observed through random sampling.",1
"Value functions are crucial for model-free Reinforcement Learning (RL) to obtain a policy implicitly or guide the policy updates. Value estimation heavily depends on the stochasticity of environmental dynamics and the quality of reward signals. In this paper, we propose a two-step understanding of value estimation from the perspective of future prediction, through decomposing the value function into a reward-independent future dynamics part and a policy-independent trajectory return part. We then derive a practical deep RL algorithm from the above decomposition, consisting of a convolutional trajectory representation model, a conditional variational dynamics model to predict the expected representation of future trajectory and a convex trajectory return model that maps a trajectory representation to its return. Our algorithm is evaluated in MuJoCo continuous control tasks and shows superior results under both common settings and delayed reward settings.",0
"To achieve a policy or guide policy updates in model-free Reinforcement Learning (RL), value functions play a crucial role. The accuracy of value estimation depends heavily on the stochasticity of environmental dynamics and the quality of reward signals. This paper proposes a two-step approach to understanding value estimation in terms of future prediction. The value function is broken down into a reward-independent future dynamics component and a policy-independent trajectory return component. The proposed deep RL algorithm is based on this decomposition, which includes a convolutional trajectory representation model, a conditional variational dynamics model to predict the expected representation of future trajectory, and a convex trajectory return model that maps a trajectory representation to its return. In MuJoCo continuous control tasks, our algorithm outperforms other methods in both common settings and delayed reward settings.",1
"Exploration is an extremely challenging problem in reinforcement learning, especially in high dimensional state and action spaces and when only sparse rewards are available. Effective representations can indicate which components of the state are task relevant and thus reduce the dimensionality of the space to explore. In this work, we take a representation learning viewpoint on exploration, utilizing prior experience to learn effective latent representations, which can subsequently indicate which regions to explore. Prior experience on separate but related tasks help learn representations of the state which are effective at predicting instantaneous rewards. These learned representations can then be used with an entropy-based exploration method to effectively perform exploration in high dimensional spaces by effectively lowering the dimensionality of the search space. We show the benefits of this representation for meta-exploration in a simulated object pushing environment.",0
"Reinforcement learning faces a significant challenge in exploration, particularly when dealing with high-dimensional state and action spaces and limited rewards. The use of efficient representations can help identify task-relevant components of the state and reduce the exploration space. This study adopts a representation learning approach to exploration, leveraging prior experience to develop latent representations that guide the exploration process. By using prior experience from related tasks, effective state representations can be learned, which can predict instantaneous rewards. These learned representations can be combined with an entropy-based exploration strategy to effectively explore high-dimensional spaces by reducing the search space's dimensionality. The study demonstrates the effectiveness of this representation in meta-exploration using a simulated object pushing environment.",1
"Reinforcement Learning agents are expected to eventually perform well. Typically, this takes the form of a guarantee about the asymptotic behavior of an algorithm given some assumptions about the environment. We present an algorithm for a policy whose value approaches the optimal value with probability 1 in all computable probabilistic environments, provided the agent has a bounded horizon. This is known as strong asymptotic optimality, and it was previously unknown whether it was possible for a policy to be strongly asymptotically optimal in the class of all computable probabilistic environments. Our agent, Inquisitive Reinforcement Learner (Inq), is more likely to explore the more it expects an exploratory action to reduce its uncertainty about which environment it is in, hence the term inquisitive. Exploring inquisitively is a strategy that can be applied generally; for more manageable environment classes, inquisitiveness is tractable. We conducted experiments in ""grid-worlds"" to compare the Inquisitive Reinforcement Learner to other weakly asymptotically optimal agents.",0
"The performance of Reinforcement Learning agents is anticipated to improve over time, usually through an assurance of an algorithm's asymptotic behavior, based on certain assumptions about the environment. We introduce a policy algorithm that guarantees a value close to the optimal value with a probability of 1 in all computable probabilistic environments, as long as the agent has a finite horizon. This is referred to as strong asymptotic optimality, which was previously uncertain for policies in the class of all computable probabilistic environments. Our agent, Inquisitive Reinforcement Learner (Inq), is designed to prioritize exploration as it expects exploratory actions to reduce its uncertainty about the environment. This approach, called inquisitiveness, can be applied generally, and is manageable in more specific environment classes. To evaluate Inq's performance, we conducted experiments in ""grid-worlds"", comparing it to other weakly asymptotically optimal agents.",1
"Deep neural networks have achieved impressive results on a wide variety of tasks. However, quantifying uncertainty in the network's output is a challenging task. Bayesian models offer a mathematical framework to reason about model uncertainty. Variational methods have been used for approximating intractable integrals that arise in Bayesian inference for neural networks. In this report, we review the major variational inference concepts pertinent to Bayesian neural networks and compare various approximation methods used in literature. We also talk about the applications of variational bayes in Reinforcement learning and continual learning.",0
"Although deep neural networks have performed remarkably well in diverse tasks, determining the degree of uncertainty in the networks' output is difficult. Bayesian models provide a mathematical structure for addressing model uncertainty. To tackle the arduous integrals that arise in Bayesian inference for neural networks, variational methods have been employed. Our report examines the essential variational inference ideas that relate to Bayesian neural networks and compares the different approximation techniques found in literature. Additionally, we discuss how variational Bayesian methods are used in Reinforcement learning and continual learning.",1
"We examine the problem of adversarial reinforcement learning for multi-agent domains including a rule-based agent. Rule-based algorithms are required in safety-critical applications for them to work properly in a wide range of situations. Hence, every effort is made to find failure scenarios during the development phase. However, as the software becomes complicated, finding failure cases becomes difficult. Especially in multi-agent domains, such as autonomous driving environments, it is much harder to find useful failure scenarios that help us improve the algorithm. We propose a method for efficiently finding failure scenarios; this method trains the adversarial agents using multi-agent reinforcement learning such that the tested rule-based agent fails. We demonstrate the effectiveness of our proposed method using a simple environment and autonomous driving simulator.",0
"Our focus is on adversarial reinforcement learning for multi-agent domains, which includes a rule-based agent. Rule-based algorithms are crucial for safety-critical applications, as they must function effectively in diverse situations. During development, extensive efforts are made to identify possible failure scenarios. However, as software complexity increases, identifying failure cases becomes more challenging, especially in multi-agent domains like autonomous driving environments. To address this issue, we propose an effective method to identify failure scenarios by training adversarial agents using multi-agent reinforcement learning. Our approach aims to make the tested rule-based agent fail. We demonstrate the effectiveness of this approach using a simple environment and an autonomous driving simulator.",1
"Joint replacement is the most common inpatient surgical treatment in the US. We investigate the clinical pathway optimization for knee replacement, which is a sequential decision process from onset to recovery. Based on episodic claims from previous cases, we view the pathway optimization as an intelligence crowdsourcing problem and learn the optimal decision policy from data by imitating the best expert at every intermediate state. We develop a reinforcement learning-based pipeline that uses value iteration, state compression and aggregation learning, kernel representation and cross validation to predict the best treatment policy. It also provides forecast of the clinical pathway under the optimized policy. Empirical validation shows that the optimized policy reduces the overall cost by 7 percent and reduces the excessive cost premium by 33 percent.",0
"In the US, joint replacement is the most commonly performed inpatient surgical procedure. Our research focuses on improving the clinical pathway for knee replacement, which involves a series of decisions from the initial onset of the condition to the recovery phase. By analyzing past cases, we approach this optimization problem as a form of collective intelligence, and use data to learn the best decision-making strategy from the most successful experts at each stage. Our solution involves a reinforcement learning-based process that includes value iteration, state compression, aggregation learning, kernel representation, and cross-validation to determine the optimal treatment plan. It also provides a forecast of the clinical pathway based on the optimized policy. Our empirical analysis shows that this approach can reduce overall costs by 7 percent and decrease excessive cost premiums by 33 percent.",1
"We introduce a new RL problem where the agent is required to generalize to a previously-unseen environment characterized by a subtask graph which describes a set of subtasks and their dependencies. Unlike existing hierarchical multitask RL approaches that explicitly describe what the agent should do at a high level, our problem only describes properties of subtasks and relationships among them, which requires the agent to perform complex reasoning to find the optimal subtask to execute. To solve this problem, we propose a neural subtask graph solver (NSGS) which encodes the subtask graph using a recursive neural network embedding. To overcome the difficulty of training, we propose a novel non-parametric gradient-based policy, graph reward propagation, to pre-train our NSGS agent and further finetune it through actor-critic method. The experimental results on two 2D visual domains show that our agent can perform complex reasoning to find a near-optimal way of executing the subtask graph and generalize well to the unseen subtask graphs. In addition, we compare our agent with a Monte-Carlo tree search (MCTS) method showing that our method is much more efficient than MCTS, and the performance of NSGS can be further improved by combining it with MCTS.",0
"Our research introduces a new problem in RL, where the agent must generalize to an unknown environment characterized by a subtask graph, which outlines a set of subtasks and their dependencies. Unlike current hierarchical multitask RL methods that dictate the agent's actions at a high level, our problem only provides information about the subtasks and their relationships, requiring the agent to use advanced reasoning to determine the optimal subtask to perform. To tackle this challenge, we propose the Neural Subtask Graph Solver (NSGS), which encodes the subtask graph using a recursive neural network embedding. To overcome the difficulty of training, we propose a new non-parametric gradient-based policy called graph reward propagation. This pre-trains our NSGS agent and fine-tunes it using the actor-critic method. Our experimental results on two 2D visual domains demonstrate that our agent performs sophisticated reasoning to identify the best approach to execute the subtask graph and can generalize well to unknown subtask graphs. Moreover, we compare our agent with the Monte-Carlo tree search (MCTS) method, and our approach is significantly more efficient. Finally, we show that the performance of NSGS can be enhanced by combining it with MCTS.",1
"In many finite horizon episodic reinforcement learning (RL) settings, it is desirable to optimize for the undiscounted return - in settings like Atari, for instance, the goal is to collect the most points while staying alive in the long run. Yet, it may be difficult (or even intractable) mathematically to learn with this target. As such, temporal discounting is often applied to optimize over a shorter effective planning horizon. This comes at the risk of potentially biasing the optimization target away from the undiscounted goal. In settings where this bias is unacceptable - where the system must optimize for longer horizons at higher discounts - the target of the value function approximator may increase in variance leading to difficulties in learning. We present an extension of temporal difference (TD) learning, which we call TD($\Delta$), that breaks down a value function into a series of components based on the differences between value functions with smaller discount factors. The separation of a longer horizon value function into these components has useful properties in scalability and performance. We discuss these properties and show theoretic and empirical improvements over standard TD learning in certain settings.",0
"In finite horizon episodic reinforcement learning (RL) scenarios, optimizing for the undiscounted return is often preferred, such as in Atari where the objective is to accumulate points while surviving for a long time. However, it can be challenging to learn with this target, and temporal discounting is frequently employed to optimize over a shorter planning horizon. This approach poses a risk of biasing the optimization target away from the undiscounted goal, making it unsuitable for settings that require optimization over longer horizons with higher discounts. In such situations, increasing the target of the value function approximator may lead to variance and learning difficulties. To address this issue, we propose TD($\Delta$), an extension of temporal difference (TD) learning that breaks down a value function into several components based on the differences between value functions with smaller discount factors. This technique improves scalability and performance, as demonstrated through theoretical and empirical analyses in specific scenarios.",1
"Recent advances in reinforcement learning have proved that given an environment we can learn to perform a task in that environment if we have access to some form of a reward function (dense, sparse or derived from IRL). But most of the algorithms focus on learning a single best policy to perform a given set of tasks. In this paper, we focus on an algorithm that learns to not just perform a task but different ways to perform the same task. As we know when the environment is complex enough there always exists multiple ways to perform a task. We show that using the concept of information maximization it is possible to learn latent codes for discovering multiple ways to perform any given task in an environment.",0
"Recent developments in reinforcement learning have demonstrated that we can acquire the ability to execute a task in an environment if we possess access to some form of a reward function, such as dense, sparse, or derived from IRL. However, most of the algorithms concentrate on discovering a single optimal policy to execute a specific set of tasks. This article instead focuses on an algorithm that learns not only how to execute a task but also various approaches to accomplish the same task. We are aware that in a complex environment, there are always numerous ways to perform a task. We demonstrate that by employing the principle of information maximization, we can learn latent codes that identify multiple ways to perform any given task in an environment.",1
"Reinforcement learning (RL) has had many successes in both ""deep"" and ""shallow"" settings. In both cases, significant hyperparameter tuning is often required to achieve good performance. Furthermore, when nonlinear function approximation is used, non-stationarity in the state representation can lead to learning instability. A variety of techniques exist to combat this --- most notably large experience replay buffers or the use of multiple parallel actors. These techniques come at the cost of moving away from the online RL problem as it is traditionally formulated (i.e., a single agent learning online without maintaining a large database of training examples). Meta-learning can potentially help with both these issues by tuning hyperparameters online and allowing the algorithm to more robustly adjust to non-stationarity in a problem. This paper applies meta-gradient descent to derive a set of step-size tuning algorithms specifically for online RL control with eligibility traces. Our novel technique, Metatrace, makes use of an eligibility trace analogous to methods like $TD(\lambda)$. We explore tuning both a single scalar step-size and a separate step-size for each learned parameter. We evaluate Metatrace first for control with linear function approximation in the classic mountain car problem and then in a noisy, non-stationary version. Finally, we apply Metatrace for control with nonlinear function approximation in 5 games in the Arcade Learning Environment where we explore how it impacts learning speed and robustness to initial step-size choice. Results show that the meta-step-size parameter of Metatrace is easy to set, Metatrace can speed learning, and Metatrace can allow an RL algorithm to deal with non-stationarity in the learning task.",0
"Both ""deep"" and ""shallow"" applications of reinforcement learning (RL) have been successful, but they often require significant hyperparameter tuning for optimal performance. The use of nonlinear function approximation can lead to learning instability due to non-stationarity in the state representation, which can be mitigated through techniques such as large experience replay buffers or multiple parallel actors. However, these approaches deviate from the traditional online RL problem of a single agent learning without a large database of training examples. Meta-learning can potentially address these issues by tuning hyperparameters online and allowing for more robust adjustments to non-stationarity. This paper introduces Metatrace, a novel technique that uses meta-gradient descent to derive step-size tuning algorithms specifically for online RL control with eligibility traces. The approach is tested on both linear and nonlinear function approximation tasks, including the classic mountain car problem and 5 games in the Arcade Learning Environment. Results show that Metatrace is effective at speeding up learning and improving robustness to non-stationarity, and the meta-step-size parameter is easy to set.",1
"Automatic machine learning is an important problem in the forefront of machine learning. The strongest AutoML systems are based on neural networks, evolutionary algorithms, and Bayesian optimization. Recently AlphaD3M reached state-of-the-art results with an order of magnitude speedup using reinforcement learning with self-play. In this work we extend AlphaD3M by using a pipeline grammar and a pre-trained model which generalizes from many different datasets and similar tasks. Our results demonstrate improved performance compared with our earlier work and existing methods on AutoML benchmark datasets for classification and regression tasks. In the spirit of reproducible research we make our data, models, and code publicly available.",0
"The issue of automatic machine learning is currently at the forefront of machine learning research. The most powerful AutoML systems use neural networks, evolutionary algorithms, and Bayesian optimization. AlphaD3M has recently achieved groundbreaking results through the application of reinforcement learning with self-play, resulting in a significant speed increase. Our study builds upon AlphaD3M's findings by incorporating a pipeline grammar and a pre-trained model, which can generalize from multiple datasets and similar tasks. Our results show improved performance compared to our previous research and existing AutoML methods on classification and regression benchmarks. To promote reproducible research, we have made our data, models, and code available to the public.",1
"Recent reinforcement learning algorithms, though achieving impressive results in various fields, suffer from brittle training effects such as regression in results and high sensitivity to initialization and parameters. We claim that some of the brittleness stems from variance differences, i.e. when different environment areas - states and/or actions - have different rewards variance. This causes two problems: First, the ""Boring Areas Trap"" in algorithms such as Q-learning, where moving between areas depends on the current area variance, and getting out of a boring area is hard due to its low variance. Second, the ""Manipulative Consultant"" problem, when value-estimation functions used in DQN and Actor-Critic algorithms influence the agent to prefer boring areas, regardless of the mean rewards return, as they maximize estimation precision rather than rewards. This sheds a new light on how exploration contribute to training, as it helps with both challenges. Cognitive experiments in humans showed that noised reward signals may paradoxically improve performance. We explain this using the two mentioned problems, claiming that both humans and algorithms may share similar challenges. Inspired by this result, we propose the Adaptive Symmetric Reward Noising (ASRN), by which we mean adding Gaussian noise to rewards according to their states' estimated variance, thus avoiding the two problems while not affecting the environment's mean rewards behavior. We conduct our experiments in a Multi Armed Bandit problem with variance differences. We demonstrate that a Q-learning algorithm shows the brittleness effect in this problem, and that the ASRN scheme can dramatically improve the results. We show that ASRN helps a DQN algorithm training process reach better results in an end to end autonomous driving task using the AirSim driving simulator.",0
"Despite achieving impressive outcomes in diverse domains, recent reinforcement learning algorithms suffer from training fragility, including regression in outcomes and high sensitivity to initialization and parameters. Our assertion is that some of this brittleness arises from differences in variance, where distinct environmental areas, i.e., states and/or actions, possess varying rewards variance. This leads to two issues: firstly, the ""Boring Areas Trap"" in algorithms like Q-learning, where transitioning between regions depends on the present area variance, making it challenging to leave a dull area due to its low variance. Secondly, the ""Manipulative Consultant"" problem, where value-estimation functions utilized in DQN and Actor-Critic algorithms influence the agent to prefer boring areas, disregarding the mean rewards return, as they optimize estimation precision rather than rewards. This presents a novel perspective on how exploration contributes to training, as it addresses both challenges. Cognitive studies in humans have demonstrated that noisy reward signals paradoxically enhance performance. The two issues mentioned above explain this, suggesting that humans and algorithms may face similar issues. Our solution to this problem is the Adaptive Symmetric Reward Noising (ASRN), which involves adding Gaussian noise to rewards based on the estimated variance of their states, thus avoiding the two issues while preserving the mean rewards behavior of the environment. We conduct experiments on a Multi Armed Bandit problem with variance differences, demonstrating that the Q-learning algorithm displays the brittleness effect, while the ASRN scheme significantly improves results. Additionally, we show that ASRN aids the training process of a DQN algorithm to achieve better outcomes in an end-to-end autonomous driving task utilizing the AirSim driving simulator.",1
"The optimal predictor for a linear dynamical system (with hidden state and Gaussian noise) takes the form of an autoregressive linear filter, namely the Kalman filter. However, a fundamental problem in reinforcement learning and control theory is to make optimal predictions in an unknown dynamical system. To this end, we take the approach of directly learning an autoregressive filter for time-series prediction under unknown dynamics. Our analysis differs from previous statistical analyses in that we regress not only on the inputs to the dynamical system, but also the outputs, which is essential to dealing with process noise. The main challenge is to estimate the filter under worst case input (in $\mathcal H_\infty$ norm), for which we use an $L^\infty$-based objective rather than ordinary least-squares. For learning an autoregressive model, our algorithm has optimal sample complexity in terms of the rollout length, which does not seem to be attained by naive least-squares.",0
"The Kalman filter is the best predictor for a linear dynamical system with hidden state and Gaussian noise. However, there is a vital issue in reinforcement learning and control theory, which is making accurate predictions in an unknown dynamical system. Therefore, we directly learn an autoregressive filter for time-series prediction without knowing the dynamics. Our approach differs from previous statistical analyses as we regress not only on the inputs but also on the outputs, which is crucial for handling process noise. The significant challenge is to estimate the filter under worst-case input, which we accomplish using an $L^\infty$-based objective instead of ordinary least-squares. Our algorithm has the optimal sample complexity for learning an autoregressive model in terms of the rollout length, which is not achievable through naive least-squares.",1
"Despite recent innovations in network architectures and loss functions, training RNNs to learn long-term dependencies remains difficult due to challenges with gradient-based optimisation methods. Inspired by the success of Deep Neuroevolution in reinforcement learning (Such et al. 2017), we explore the use of gradient-free population-based global optimisation (PBO) techniques -- training RNNs to capture long-term dependencies in time-series data. Testing evolution strategies (ES) and particle swarm optimisation (PSO) on an application in volatility forecasting, we demonstrate that PBO methods lead to performance improvements in general, with ES exhibiting the most consistent results across a variety of architectures.",0
"Training RNNs to capture long-term dependencies in time-series data remains a difficult task, despite advancements in network architectures and loss functions. This is due to challenges with gradient-based optimization methods. To address this issue, we draw inspiration from the success of Deep Neuroevolution in reinforcement learning and explore the use of gradient-free population-based global optimization (PBO) techniques. Through testing evolution strategies (ES) and particle swarm optimization (PSO) on an application in volatility forecasting, we show that PBO methods generally lead to performance improvements. Among the two methods, ES consistently delivers the most promising results across a range of architectures.",1
"Recent work has explored the problem of autonomous navigation by imitating a teacher and learning an end-to-end policy, which directly predicts controls from raw images. However, these approaches tend to be sensitive to mistakes by the teacher and do not scale well to other environments or vehicles. To this end, we propose Observational Imitation Learning (OIL), a novel imitation learning variant that supports online training and automatic selection of optimal behavior by observing multiple imperfect teachers. We apply our proposed methodology to the challenging problems of autonomous driving and UAV racing. For both tasks, we utilize the Sim4CV simulator that enables the generation of large amounts of synthetic training data and also allows for online learning and evaluation. We train a perception network to predict waypoints from raw image data and use OIL to train another network to predict controls from these waypoints. Extensive experiments demonstrate that our trained network outperforms its teachers, conventional imitation learning (IL) and reinforcement learning (RL) baselines and even humans in simulation. The project website is available at https://sites.google.com/kaust.edu.sa/oil/ and a video at https://youtu.be/_rhq8a0qgeg",0
"Recently, researchers have been investigating autonomous navigation through imitation learning, which involves learning an end-to-end policy that predicts controls from raw images by imitating a teacher. However, these approaches are often influenced by errors made by the teacher and do not effectively adapt to different environments or vehicles. As a solution, Observational Imitation Learning (OIL) has been proposed, which is a new variant of imitation learning that facilitates online training and automatic selection of optimal behavior by observing multiple imperfect teachers. The proposed methodology has been applied to the challenging tasks of autonomous driving and UAV racing, utilizing the Sim4CV simulator for generating large amounts of synthetic training data and enabling online learning and evaluation. A perception network has been trained to predict waypoints from raw image data, and OIL has been used to train another network to predict controls from these waypoints. Extensive experiments demonstrate that the trained network outperforms its teachers, conventional imitation learning (IL) and reinforcement learning (RL) baselines, and even humans in simulation. The project website and a video are available at https://sites.google.com/kaust.edu.sa/oil/ and https://youtu.be/_rhq8a0qgeg, respectively.",1
"Despite recent successes in Reinforcement Learning, value-based methods often suffer from high variance hindering performance. In this paper, we illustrate this in a continuous control setting where state of the art methods perform poorly whenever sensor noise is introduced. To overcome this issue, we introduce Recurrent Value Functions (RVFs) as an alternative to estimate the value function of a state. We propose to estimate the value function of the current state using the value function of past states visited along the trajectory. Due to the nature of their formulation, RVFs have a natural way of learning an emphasis function that selectively emphasizes important states. First, we establish RVF's asymptotic convergence properties in tabular settings. We then demonstrate their robustness on a partially observable domain and continuous control tasks. Finally, we provide a qualitative interpretation of the learned emphasis function.",0
"Despite recent progress in Reinforcement Learning, value-based methods often face performance challenges due to high variance. This paper showcases this issue in a continuous control scenario where state of the art methods struggle when sensor noise is present. To combat this problem, we introduce Recurrent Value Functions (RVFs) as an alternative approach to estimate the state's value function. Our proposal involves utilizing the value function of past states visited along the trajectory to estimate the value function of the current state. RVFs have a distinct feature that enables them to learn an emphasis function that selectively highlights important states. We establish RVF's asymptotic convergence abilities in tabular settings and demonstrate their resilience on a partially observable domain and continuous control tasks. Finally, we provide a qualitative interpretation of the learned emphasis function.",1
"In the space of only a few years, deep generative modeling has revolutionized how we think of artificial creativity, yielding autonomous systems which produce original images, music, and text. Inspired by these successes, researchers are now applying deep generative modeling techniques to the generation and optimization of molecules - in our review we found 45 papers on the subject published in the past two years. These works point to a future where such systems will be used to generate lead molecules, greatly reducing resources spent downstream synthesizing and characterizing bad leads in the lab. In this review we survey the increasingly complex landscape of models and representation schemes that have been proposed. The four classes of techniques we describe are recursive neural networks, autoencoders, generative adversarial networks, and reinforcement learning. After first discussing some of the mathematical fundamentals of each technique, we draw high level connections and comparisons with other techniques and expose the pros and cons of each. Several important high level themes emerge as a result of this work, including the shift away from the SMILES string representation of molecules towards more sophisticated representations such as graph grammars and 3D representations, the importance of reward function design, the need for better standards for benchmarking and testing, and the benefits of adversarial training and reinforcement learning over maximum likelihood based training.",0
"Over the past few years, deep generative modeling has transformed our understanding of artificial creativity, leading to the development of self-sufficient systems that create original music, text, and images. As a result of these advancements, researchers are now utilizing deep generative modeling techniques to generate and optimize molecules. In our analysis, we discovered 45 papers published in the past two years on this subject. These works suggest that such systems will be utilized to generate lead molecules, minimizing the resources used in synthesizing and characterizing bad leads in the lab. Our review explores the increasingly intricate landscape of models and representation schemes proposed, including recursive neural networks, autoencoders, generative adversarial networks, and reinforcement learning. We begin by describing the mathematical fundamentals of each technique before drawing connections and comparisons with other techniques and outlining their advantages and disadvantages. Our work highlights several key themes, such as the shift away from SMILES string representation of molecules to more advanced representations like graph grammars and 3D representations, the significance of reward function design, the need for better benchmarking and testing standards, and the benefits of adversarial training and reinforcement learning over maximum likelihood based training.",1
"Phishing is the simplest form of cybercrime with the objective of baiting people into giving away delicate information such as individually recognizable data, banking and credit card details, or even credentials and passwords. This type of simple yet most effective cyber-attack is usually launched through emails, phone calls, or instant messages. The credential or private data stolen are then used to get access to critical records of the victims and can result in extensive fraud and monetary loss. Hence, sending malicious messages to victims is a stepping stone of the phishing procedure. A \textit{phisher} usually setups a deceptive website, where the victims are conned into entering credentials and sensitive information. It is therefore important to detect these types of malicious websites before causing any harmful damages to victims. Inspired by the evolving nature of the phishing websites, this paper introduces a novel approach based on deep reinforcement learning to model and detect malicious URLs. The proposed model is capable of adapting to the dynamic behavior of the phishing websites and thus learn the features associated with phishing website detection.",0
"Cybercrime can take many forms, but phishing is one of the most straightforward and effective methods. Its aim is to trick people into revealing sensitive information, such as bank details, passwords, or personal information. This is usually done through emails, phone calls, or instant messages. Once the attacker has obtained this information, they can gain access to the victim's data and potentially commit fraud or steal money. In order to launch a successful phishing attack, the attacker will often create a fake website designed to look legitimate, where the victim is prompted to enter their details. Detecting these sites before they can cause harm is crucial. This paper proposes a novel approach, using deep reinforcement learning, to model and detect malicious URLs. By adapting to the dynamic nature of phishing websites, the model can learn to recognize the features associated with phishing and prevent potential damage to victims.",1
"Legged locomotion is a challenging task for learning algorithms, especially when the task requires a diverse set of primitive behaviors. To solve these problems, we introduce a hierarchical framework to automatically decompose complex locomotion tasks. A high-level policy issues commands in a latent space and also selects for how long the low-level policy will execute the latent command. Concurrently, the low-level policy uses the latent command and only the robot's on-board sensors to control the robot's actuators. Our approach allows the high-level policy to run at a lower frequency than the low-level one. We test our framework on a path-following task for a dynamic quadruped robot and we show that steering behaviors automatically emerge in the latent command space as low-level skills are needed for this task. We then show efficient adaptation of the trained policy to a different task by transfer of the trained low-level policy. Finally, we validate the policies on a real quadruped robot. To the best of our knowledge, this is the first application of end-to-end hierarchical learning to a real robotic locomotion task.",0
"Learning algorithms face a significant challenge when it comes to legged locomotion, particularly when the task involves a varied set of basic behaviors. To tackle this issue, we have developed a hierarchical framework that can break down complex locomotion tasks automatically. A high-level policy gives commands in a latent space and determines how long the low-level policy should execute these commands. Meanwhile, the low-level policy uses these commands and onboard sensors to control the robot's actuators. Our method enables the high-level policy to operate at a lower frequency than the low-level policy. We have tested this framework on a dynamic quadruped robot in a path-following task, and have found that steering behaviors emerge automatically in the latent command space as low-level skills are required for this task. We have also shown that the trained policy can adapt efficiently to a different task through the transfer of the trained low-level policy. Finally, we have validated the policies on a real quadruped robot. This is the first known instance of end-to-end hierarchical learning being applied to a real robotic locomotion task.",1
"Branch-and-bound (BnB) algorithms are widely used to solve combinatorial problems, and the performance crucially depends on its branching heuristic.In this work, we consider a typical problem of maximum common subgraph (MCS), and propose a branching heuristic inspired from reinforcement learning with a goal of reaching a tree leaf as early as possible to greatly reduce the search tree size.Extensive experiments show that our method is beneficial and outperforms current best BnB algorithm for the MCS.",0
"The use of branch-and-bound (BnB) algorithms is prevalent in solving combinatorial problems, and their efficiency is largely determined by the chosen branching heuristic. Our study is focused on the maximum common subgraph (MCS) problem and introduces a reinforcement learning-inspired branching heuristic aimed at achieving quick access to a tree leaf, leading to a significant reduction in search tree size. Our experiments demonstrate the effectiveness of our approach, which surpasses the current leading BnB algorithm for MCS.",1
"N-discount optimality was introduced as a hierarchical form of policy- and value-function optimality, with Blackwell optimality lying at the top level of the hierarchy Veinott (1969); Blackwell (1962). We formalize notions of myopic discount factors, value functions and policies in terms of Blackwell optimality in MDPs, and we provide a novel concept of regret, called Blackwell regret, which measures the regret compared to a Blackwell optimal policy. Our main analysis focuses on long horizon MDPs with sparse rewards. We show that selecting the discount factor under which zero Blackwell regret can be achieved becomes arbitrarily hard. Moreover, even with oracle knowledge of such a discount factor that can realize a Blackwell regret-free value function, an $\epsilon$-Blackwell optimal value function may not even be gain optimal. Difficulties associated with this class of problems is discussed, and the notion of a policy gap is defined as the difference in expected return between a given policy and any other policy that differs at that state; we prove certain properties related to this gap. Finally, we provide experimental results that further support our theoretical results.",0
"The concept of N-discount optimality is a form of policy and value-function optimality that is hierarchical, with Blackwell optimality at the top level of the hierarchy, according to Veinott (1969) and Blackwell (1962). We define myopic discount factors, value functions, and policies in terms of Blackwell optimality in MDPs, and introduce the concept of Blackwell regret, which measures the regret compared to a Blackwell optimal policy. Our focus is on long horizon MDPs with sparse rewards, and we demonstrate that selecting the discount factor to achieve zero Blackwell regret can be extremely challenging. Even with knowledge of a discount factor that can produce a Blackwell regret-free value function, an $\epsilon$-Blackwell optimal value function may not be the most gain optimal. We explore the difficulties associated with this class of problems and define the policy gap as the difference in expected return between a given policy and any other policy that differs at that state. We present certain properties related to this gap and provide experimental results to support our theoretical findings.",1
"Recent advances in deep reinforcement learning have achieved human-level performance on a variety of real-world applications. However, the current algorithms still suffer from poor gradient estimation with excessive variance, resulting in unstable training and poor sample efficiency. In our paper, we proposed an innovative optimization strategy by utilizing stochastic variance reduced gradient (SVRG) techniques. With extensive experiments on Atari domain, our method outperforms the deep q-learning baselines on 18 out of 20 games.",0
"Deep reinforcement learning has made significant progress in achieving human-level performance in several real-world applications. Nonetheless, the current algorithms encounter difficulties in estimating gradients, leading to unstable training and low sample efficiency. Our study introduces a novel optimization approach utilizing stochastic variance reduced gradient (SVRG) techniques. Through extensive experiments in Atari domain, our method surpasses the deep q-learning baselines in 18 out of 20 games.",1
"The field of reinforcement learning (RL) is facing increasingly challenging domains with combinatorial complexity. For an RL agent to address these challenges, it is essential that it can plan effectively. Prior work has typically utilized an explicit model of the environment, combined with a specific planning algorithm (such as tree search). More recently, a new family of methods have been proposed that learn how to plan, by providing the structure for planning via an inductive bias in the function approximator (such as a tree structured neural network), trained end-to-end by a model-free RL algorithm. In this paper, we go even further, and demonstrate empirically that an entirely model-free approach, without special structure beyond standard neural network components such as convolutional networks and LSTMs, can learn to exhibit many of the characteristics typically associated with a model-based planner. We measure our agent's effectiveness at planning in terms of its ability to generalize across a combinatorial and irreversible state space, its data efficiency, and its ability to utilize additional thinking time. We find that our agent has many of the characteristics that one might expect to find in a planning algorithm. Furthermore, it exceeds the state-of-the-art in challenging combinatorial domains such as Sokoban and outperforms other model-free approaches that utilize strong inductive biases toward planning.",0
"The field of reinforcement learning (RL) is facing complex domains that pose challenges due to their combinatory nature. A successful RL agent must be capable of planning effectively. Prior research has relied on an explicit environment model, coupled with a planning algorithm like tree search. Recently, a new method has emerged that teaches agents how to plan by offering a structure for planning through an inductive bias in the function approximator. This bias is created by using a tree-structured neural network, which is trained using an end-to-end model-free RL algorithm. Our paper goes even further, demonstrating through empirical evidence that a standard neural network without specialized structure, such as convolutional networks and LSTMs, can learn to mimic a model-based planner. We assess the agent's ability to plan by examining its capacity to generalize across a combinatory and irreversible state space, its efficiency in utilizing data, and its ability to allocate additional thinking time. Our findings reveal that our agent possesses many features that one would anticipate in a planning algorithm. Moreover, it surpasses the current state-of-the-art in complex combinatorial domains like Sokoban and outperforms other model-free approaches with strong inductive biases towards planning.",1
"Experience replay (ER) is a fundamental component of off-policy deep reinforcement learning (RL). ER recalls experiences from past iterations to compute gradient estimates for the current policy, increasing data-efficiency. However, the accuracy of such updates may deteriorate when the policy diverges from past behaviors and can undermine the performance of ER. Many algorithms mitigate this issue by tuning hyper-parameters to slow down policy changes. An alternative is to actively enforce the similarity between policy and the experiences in the replay memory. We introduce Remember and Forget Experience Replay (ReF-ER), a novel method that can enhance RL algorithms with parameterized policies. ReF-ER (1) skips gradients computed from experiences that are too unlikely with the current policy and (2) regulates policy changes within a trust region of the replayed behaviors. We couple ReF-ER with Q-learning, deterministic policy gradient and off-policy gradient methods. We find that ReF-ER consistently improves the performance of continuous-action, off-policy RL on fully observable benchmarks and partially observable flow control problems.",0
"Off-policy deep reinforcement learning relies on experience replay (ER) as a crucial element. This involves recalling past experiences to estimate gradients for the current policy, resulting in greater data efficiency. However, the accuracy of these updates may suffer when the policy diverges from past behaviors, adversely affecting the effectiveness of ER. Some algorithms address this issue by adjusting hyper-parameters to slow policy changes, while others enforce similarity between the policy and the experiences in the replay memory. Our novel method, Remember and Forget Experience Replay (ReF-ER), enhances RL algorithms with parameterized policies. ReF-ER skips gradients computed from experiences that are too unlikely with the current policy and regulates policy changes within a trust region of the replayed behaviors. We have coupled ReF-ER with Q-learning, deterministic policy gradient, and off-policy gradient methods and observed consistent improvement in the performance of continuous-action, off-policy RL on fully observable benchmarks and partially observable flow control problems.",1
"Imitation by observation is an approach for learning from expert demonstrations that lack action information, such as videos. Recent approaches to this problem can be placed into two broad categories: training dynamics models that aim to predict the actions taken between states, and learning rewards or features for computing them for Reinforcement Learning (RL). In this paper, we introduce a novel approach that learns values, rather than rewards, directly from observations. We show that by using values, we can significantly speed up RL by removing the need to bootstrap action-values, as compared to sparse-reward specifications.",0
"An approach to learning from expert demonstrations that lack action information, such as videos, is imitation by observation. There are two categories of recent approaches to this problem: training dynamics models to predict actions taken between states, and learning rewards or features for computing them for Reinforcement Learning (RL). Our paper introduces a new approach that learns values instead of rewards directly from observations. We demonstrate that using values can accelerate RL significantly by eliminating the need to bootstrap action-values compared to sparse-reward specifications.",1
